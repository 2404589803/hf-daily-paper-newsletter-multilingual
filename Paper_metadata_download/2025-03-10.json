[
  {
    "paper": {
      "id": "2503.05236",
      "authors": [
        {
          "_id": "67ce37239f9aaaae837f3894",
          "user": {
            "_id": "654c6845bac6e6e49895a5b5",
            "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
            "isPro": false,
            "fullname": "Yibin Wang",
            "user": "CodeGoat24",
            "type": "user"
          },
          "name": "Yibin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:37:51.835Z",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3895",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:24.660Z",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3896",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3897",
          "name": "Cheng Jin",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3898",
          "user": {
            "_id": "64638c4d51fa6e63060521b5",
            "avatarUrl": "/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg",
            "isPro": false,
            "fullname": "JIaqi",
            "user": "Jiaqiwang",
            "type": "user"
          },
          "name": "Jiaqi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:38:17.938Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T08:36:05.000Z",
      "title": "Unified Reward Model for Multimodal Understanding and Generation",
      "summary": "Recent advances in human preference alignment have significantly enhanced\nmultimodal generation and understanding. A key approach is training reward\nmodels to guide preference optimization. However, existing models are often\ntask-specific, limiting their adaptability across diverse visual applications.\nWe also argue that jointly learning to assess multiple tasks may foster a\nsynergistic effect, where improved image understanding enhances image\ngeneration assessment, and refined image evaluation benefits video assessment\nthrough better frame analysis. To this end, this paper proposes UnifiedReward,\nthe first unified reward model for multimodal understanding and generation\nassessment, enabling both pairwise ranking and pointwise scoring, which can be\nemployed for vision model preference alignment. Specifically, (1) we first\ndevelop UnifiedReward on our constructed large-scale human preference dataset,\nincluding both image and video generation/understanding tasks. (2) Then, it is\nutilized to automatically construct high-quality preference pair data based on\nthe vision models, fine-gradually filtering their outputs through pair ranking\nand point sifting. (3) Finally, these data are used for their preference\nalignment through Direct Preference Optimization (DPO). Experimental results\ndemonstrate that joint learning to assess diverse visual tasks can lead to\nsubstantial mutual benefits and we apply our pipeline to both image and video\nunderstanding/generation tasks, significantly improving the performance in each\ndomain.",
      "upvotes": 72,
      "discussionId": "67ce37259f9aaaae837f3948",
      "projectPage": "https://codegoat24.github.io/UnifiedReward/",
      "githubRepo": "https://github.com/CodeGoat24/UnifiedReward"
    },
    "publishedAt": "2025-03-09T22:20:09.137Z",
    "title": "Unified Reward Model for Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05236.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "654c6845bac6e6e49895a5b5",
      "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
      "fullname": "Yibin Wang",
      "name": "CodeGoat24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05179",
      "authors": [
        {
          "_id": "67ce4bff5847e4787a7ebedd",
          "user": {
            "_id": "65f4060754ecda1ecb5797a0",
            "avatarUrl": "/avatars/f8b44524d36b505673cb538fd7895a82.svg",
            "isPro": false,
            "fullname": "Simon Aytes",
            "user": "saytes",
            "type": "user"
          },
          "name": "Simon A. Aytes",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:10.363Z",
          "hidden": false
        },
        {
          "_id": "67ce4bff5847e4787a7ebede",
          "user": {
            "_id": "63036b6c5c70c21d0ea79d48",
            "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
            "isPro": false,
            "fullname": "Jinheon Baek",
            "user": "jinheon",
            "type": "user"
          },
          "name": "Jinheon Baek",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:13.328Z",
          "hidden": false
        },
        {
          "_id": "67ce4bff5847e4787a7ebedf",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T06:57:17.000Z",
      "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching",
      "summary": "Recent advances in large language models have demonstrated remarkable\nreasoning capabilities through Chain of Thought (CoT) prompting, but often at\nthe cost of excessive verbosity in their intermediate outputs, which increases\ncomputational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting\nframework that combines cognitive-inspired reasoning paradigms with linguistic\nconstraints to minimize token usage while preserving reasoning accuracy. SoT is\ndesigned as a flexible framework that can incorporate any custom reasoning\nparadigms based on cognitive science, and we instantiate it with three such\nparadigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each\ntailored to different reasoning tasks and selected dynamically via a\nlightweight routing model. Through comprehensive evaluation across 15 reasoning\ndatasets with multiple languages and multimodal scenarios, we demonstrate that\nSoT achieves token reductions of 76% with negligible accuracy impact. In\ncertain domains like mathematical and multi-hop reasoning, it even improves\naccuracy while using significantly fewer tokens. Our code is publicly\navailable: https://www.github.com/SimonAytes/SoT.",
      "upvotes": 27,
      "discussionId": "67ce4c035847e4787a7ebf4c",
      "projectPage": "https://huggingface.co/saytes/SoT_DistilBERT",
      "githubRepo": "https://github.com/SimonAytes/SoT"
    },
    "publishedAt": "2025-03-09T22:25:52.244Z",
    "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05179.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63036b6c5c70c21d0ea79d48",
      "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
      "fullname": "Jinheon Baek",
      "name": "jinheon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05500",
      "authors": [
        {
          "_id": "67ce9626e5cdfda52b9e8839",
          "user": {
            "_id": "62be186a5f59ff2320e6e32b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
            "isPro": false,
            "fullname": "Nicolas-BZRD",
            "user": "Nicolas-BZRD",
            "type": "user"
          },
          "name": "Nicolas Boizard",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:06.860Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883a",
          "user": {
            "_id": "65fa95405355a52c784633fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fa95405355a52c784633fc/rSfBUHPa7eSAsLd8DuOq4.png",
            "isPro": false,
            "fullname": "Hippolyte Gisserot-Boukhlef",
            "user": "hgissbkh",
            "type": "user"
          },
          "name": "Hippolyte Gisserot-Boukhlef",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:13.176Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883b",
          "user": {
            "_id": "64132452d8a418df415a6ded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64132452d8a418df415a6ded/qkjL5G89uldHUXlCI3n4f.jpeg",
            "isPro": false,
            "fullname": "Duarte Alves",
            "user": "DuarteMRAlves",
            "type": "user"
          },
          "name": "Duarte M. Alves",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:23.055Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883c",
          "name": "André Martins",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883d",
          "user": {
            "_id": "63937b399762cdd66be2a32f",
            "avatarUrl": "/avatars/7aefd888a3c54673d5881dcef61f771b.svg",
            "isPro": false,
            "fullname": "Ayoub Hammal",
            "user": "ayoubhammal",
            "type": "user"
          },
          "name": "Ayoub Hammal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:42.527Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883e",
          "user": {
            "_id": "677bedd522ca8585ede98470",
            "avatarUrl": "/avatars/54bca410c446610f02aca55918c74518.svg",
            "isPro": false,
            "fullname": "Caio Corro",
            "user": "caiocorro",
            "type": "user"
          },
          "name": "Caio Corro",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:42:48.603Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883f",
          "user": {
            "_id": "61efea03a57920a251ec19b8",
            "avatarUrl": "/avatars/f47c8e3cb17a2bf7d43f2c152bb86885.svg",
            "isPro": false,
            "fullname": "Celine Hudelot",
            "user": "CelineH",
            "type": "user"
          },
          "name": "Céline Hudelot",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:51:41.273Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8840",
          "user": {
            "_id": "66f2d6a684a241caac8e16dc",
            "avatarUrl": "/avatars/81acb87c2b07bea938251b40a2139911.svg",
            "isPro": false,
            "fullname": "Emmanuel Malherbe",
            "user": "emmanuelmalherbe",
            "type": "user"
          },
          "name": "Emmanuel Malherbe",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:51:47.996Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8841",
          "name": "Etienne Malaboeuf",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8842",
          "user": {
            "_id": "6708db59caf70ddea8e1355d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6708db59caf70ddea8e1355d/C6T16AdpqoeWCk7Gg9wSH.jpeg",
            "isPro": false,
            "fullname": "Fanny Jourdan",
            "user": "Fannyjrd",
            "type": "user"
          },
          "name": "Fanny Jourdan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:09.223Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8843",
          "user": {
            "_id": "67cafedda972115e89972cd7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P_xComqG9IttvluN-6tyB.png",
            "isPro": false,
            "fullname": "Gabriel Hautreux",
            "user": "GabrielHau",
            "type": "user"
          },
          "name": "Gabriel Hautreux",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:15.512Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8844",
          "user": {
            "_id": "6772bde5c997eeb5550e80ea",
            "avatarUrl": "/avatars/8134a4d9330317e748dc7b33e1bb25f6.svg",
            "isPro": false,
            "fullname": "João Alves",
            "user": "albusonrails",
            "type": "user"
          },
          "name": "João Alves",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:22.630Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8845",
          "user": {
            "_id": "66e2c22d7cc3edd60d725267",
            "avatarUrl": "/avatars/b217c5708c7dba8b1c220f37984ccc1e.svg",
            "isPro": false,
            "fullname": "Kevin El Haddad",
            "user": "kelhad",
            "type": "user"
          },
          "name": "Kevin El-Haddad",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:31.191Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8846",
          "user": {
            "_id": "60f2e021adf471cbdf8bb660",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg",
            "isPro": false,
            "fullname": "Manuel Faysse",
            "user": "manu",
            "type": "user"
          },
          "name": "Manuel Faysse",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:38.114Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8847",
          "user": {
            "_id": "6369394dd322a76e1ea4bdf6",
            "avatarUrl": "/avatars/a4e5ab0167025fbbfc970d54630ce754.svg",
            "isPro": false,
            "fullname": "Maxime Peyrard",
            "user": "peyrardm",
            "type": "user"
          },
          "name": "Maxime Peyrard",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:44.389Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8848",
          "user": {
            "_id": "67b622d2df3a86fbca306c43",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lNlshrl56oaKslArMzSzj.png",
            "isPro": false,
            "fullname": "Nuno  Guerreiro",
            "user": "nunogj",
            "type": "user"
          },
          "name": "Nuno M. Guerreiro",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:52:54.367Z",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8849",
          "name": "Patrick Fernandes",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e884a",
          "name": "Ricardo Rei",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e884b",
          "user": {
            "_id": "644a900e3a619fe72b14af0f",
            "avatarUrl": "/avatars/e2d5dac3d92757ed48e37e126a3464a3.svg",
            "isPro": false,
            "fullname": "Colombo",
            "user": "PierreColombo",
            "type": "user"
          },
          "name": "Pierre Colombo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:41:26.353Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T15:13:58.000Z",
      "title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
      "summary": "General-purpose multilingual vector representations, used in retrieval,\nregression and classification, are traditionally obtained from bidirectional\nencoder models. Despite their wide applicability, encoders have been recently\novershadowed by advances in generative decoder-only models. However, many\ninnovations driving this progress are not inherently tied to decoders. In this\npaper, we revisit the development of multilingual encoders through the lens of\nthese advances, and introduce EuroBERT, a family of multilingual encoders\ncovering European and widely spoken global languages. Our models outperform\nexisting alternatives across a diverse range of tasks, spanning multilingual\ncapabilities, mathematics, and coding, and natively supporting sequences of up\nto 8,192 tokens. We also examine the design decisions behind EuroBERT, offering\ninsights into our dataset composition and training pipeline. We publicly\nrelease the EuroBERT models, including intermediate training checkpoints,\ntogether with our training framework.",
      "upvotes": 26,
      "discussionId": "67ce9627e5cdfda52b9e88a4"
    },
    "publishedAt": "2025-03-10T03:42:45.848Z",
    "title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62be186a5f59ff2320e6e32b/NxwS9WJrRc9D3q9awbn_X.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05500.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62be186a5f59ff2320e6e32b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
      "fullname": "Nicolas-BZRD",
      "name": "Nicolas-BZRD",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02130",
      "authors": [
        {
          "_id": "67cc697fa029f09af72cca01",
          "user": {
            "_id": "6694cc1009326cb83f2d11bb",
            "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
            "isPro": false,
            "fullname": "Zhixuan Lin",
            "user": "zhixuan-lin",
            "type": "user"
          },
          "name": "Zhixuan Lin",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-08T16:00:21.933Z",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca02",
          "user": {
            "_id": "64234eadd654afd6931a288b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/UTU-XcO_ssKpIYr5MBujK.jpeg",
            "isPro": false,
            "fullname": "Evgenii Nikishin",
            "user": "nikishin",
            "type": "user"
          },
          "name": "Evgenii Nikishin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:54:09.945Z",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca03",
          "user": {
            "_id": "66906c4e37eadb9c577984d3",
            "avatarUrl": "/avatars/b81765472942fdf94c0ee885ca62df2d.svg",
            "isPro": false,
            "fullname": "Owen He",
            "user": "littleowen",
            "type": "user"
          },
          "name": "Xu Owen He",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-08T16:00:01.392Z",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca04",
          "name": "Aaron Courville",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T23:35:23.000Z",
      "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
      "summary": "An essential component of modern recurrent sequence models is the forget\ngate. While Transformers do not have an explicit recurrent form, we show that a\nforget gate can be naturally incorporated into Transformers by down-weighting\nthe unnormalized attention scores in a data-dependent way. We name this\nattention mechanism the Forgetting Attention and the resulting model the\nForgetting Transformer (FoX). We show that FoX outperforms the Transformer on\nlong-context language modeling, length extrapolation, and short-context\ndownstream tasks, while performing on par with the Transformer on long-context\ndownstream tasks. Moreover, it is compatible with the FlashAttention algorithm\nand does not require any positional embeddings. Several analyses, including the\nneedle-in-the-haystack test, show that FoX also retains the Transformer's\nsuperior long-context capabilities over recurrent sequence models such as\nMamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that\nincorporates some common architectural components in recurrent sequence models\nand find it significantly improves the performance of both FoX and the\nTransformer. Our code is available at\nhttps://github.com/zhixuan-lin/forgetting-transformer.",
      "upvotes": 12,
      "discussionId": "67cc6981a029f09af72ccac1",
      "githubRepo": "https://github.com/zhixuan-lin/forgetting-transformer"
    },
    "publishedAt": "2025-03-09T22:02:39.842Z",
    "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02130.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6694cc1009326cb83f2d11bb",
      "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
      "fullname": "Zhixuan Lin",
      "name": "zhixuan-lin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05639",
      "authors": [
        {
          "_id": "67ce5ad85847e4787a82242d",
          "user": {
            "_id": "650447dd52ca06fef957f05d",
            "avatarUrl": "/avatars/511c11ac9b3cc7a162bda5e07f6ee0a3.svg",
            "isPro": true,
            "fullname": "Yuxuan BIAN",
            "user": "BianYx",
            "type": "user"
          },
          "name": "Yuxuan Bian",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-10T03:22:04.947Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a82242e",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a82242f",
          "user": {
            "_id": "62d4577bc85b0fcf7fde39bb",
            "avatarUrl": "/avatars/a3a5729e33ae89ce9ba408830db3c835.svg",
            "isPro": false,
            "fullname": "Xuan Ju",
            "user": "juxuan27",
            "type": "user"
          },
          "name": "Xuan Ju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:57:06.955Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822430",
          "user": {
            "_id": "6374a02d0856ac905bfc6113",
            "avatarUrl": "/avatars/2cbe75c9cc818a647ca6e416f129c96f.svg",
            "isPro": false,
            "fullname": "Mingdeng Cao",
            "user": "Ljzycmd",
            "type": "user"
          },
          "name": "Mingdeng Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:56:45.412Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822431",
          "name": "Liangbin Xie",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822432",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T09:56:34.001Z",
          "hidden": false
        },
        {
          "_id": "67ce5ad85847e4787a822433",
          "name": "Qiang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T17:59:46.000Z",
      "title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play\n  Context Control",
      "summary": "Video inpainting, which aims to restore corrupted video content, has\nexperienced substantial progress. Despite these advances, existing methods,\nwhether propagating unmasked region pixels through optical flow and receptive\nfield priors, or extending image-inpainting models temporally, face challenges\nin generating fully masked objects or balancing the competing objectives of\nbackground context preservation and foreground generation in one model,\nrespectively. To address these limitations, we propose a novel dual-stream\nparadigm VideoPainter that incorporates an efficient context encoder\n(comprising only 6% of the backbone parameters) to process masked videos and\ninject backbone-aware background contextual cues to any pre-trained video DiT,\nproducing semantically consistent content in a plug-and-play manner. This\narchitectural separation significantly reduces the model's learning complexity\nwhile enabling nuanced integration of crucial background context. We also\nintroduce a novel target region ID resampling technique that enables any-length\nvideo inpainting, greatly enhancing our practical applicability. Additionally,\nwe establish a scalable dataset pipeline leveraging current vision\nunderstanding models, contributing VPData and VPBench to facilitate\nsegmentation-based inpainting training and assessment, the largest video\ninpainting dataset and benchmark to date with over 390K diverse clips. Using\ninpainting as a pipeline basis, we also explore downstream applications\nincluding video editing and video editing pair data generation, demonstrating\ncompetitive performance and significant practical potential. Extensive\nexperiments demonstrate VideoPainter's superior performance in both any-length\nvideo inpainting and editing, across eight key metrics, including video\nquality, mask region preservation, and textual coherence.",
      "upvotes": 10,
      "discussionId": "67ce5adc5847e4787a822524",
      "projectPage": "https://yxbian23.github.io/project/video-painter/",
      "githubRepo": "https://github.com/TencentARC/VideoPainter"
    },
    "publishedAt": "2025-03-10T04:30:00.983Z",
    "title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650447dd52ca06fef957f05d/VSg-Ti5epQJbVp20s1ILN.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650447dd52ca06fef957f05d",
      "avatarUrl": "/avatars/511c11ac9b3cc7a162bda5e07f6ee0a3.svg",
      "fullname": "Yuxuan BIAN",
      "name": "BianYx",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05592",
      "authors": [
        {
          "_id": "67ce5fd2e5cdfda52b9123a4",
          "user": {
            "_id": "66163dc8c7f45b3f893ff40b",
            "avatarUrl": "/avatars/801043dac0caae90bbca8c9d3e2e203b.svg",
            "isPro": false,
            "fullname": "Song Huatong",
            "user": "XXsongLALA",
            "type": "user"
          },
          "name": "Huatong Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:03:49.730Z",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a5",
          "user": {
            "_id": "61b8405b516a20acdf3b85ff",
            "avatarUrl": "/avatars/3d2eae7c163a80b73260087b05a4230b.svg",
            "isPro": false,
            "fullname": "Jinhao Jiang",
            "user": "Boru",
            "type": "user"
          },
          "name": "Jinhao Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:04:22.446Z",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a6",
          "user": {
            "_id": "6703ac76ea890f0ca5b225eb",
            "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
            "isPro": false,
            "fullname": "Yingqian Min",
            "user": "EliverQ",
            "type": "user"
          },
          "name": "Yingqian Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T09:40:54.171Z",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a7",
          "name": "Jie Chen",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a8",
          "name": "Zhipeng Chen",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a9",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123aa",
          "name": "Lei Fang",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123ab",
          "user": {
            "_id": "64b8c89052b7353d8c6a1013",
            "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
            "isPro": false,
            "fullname": "Ji-Rong Wen",
            "user": "jrwen",
            "type": "user"
          },
          "name": "Ji-Rong Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:04:33.194Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T17:14:44.000Z",
      "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning",
      "summary": "Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose R1-Searcher, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini.",
      "upvotes": 9,
      "discussionId": "67ce5fd3e5cdfda52b912436",
      "githubRepo": "https://github.com/SsmallSong/R1-Searcher"
    },
    "publishedAt": "2025-03-09T23:43:27.151Z",
    "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05592.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05638",
      "authors": [
        {
          "_id": "67ce8388764226f050ad18b3",
          "name": "Mark YU",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b4",
          "user": {
            "_id": "657a7458afbb0117ba15c59f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
            "isPro": false,
            "fullname": "Wenbo Hu",
            "user": "wbhu-tc",
            "type": "user"
          },
          "name": "Wenbo Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:42.681Z",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b5",
          "user": {
            "_id": "64770e86d7cf39f2e937ae9a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64770e86d7cf39f2e937ae9a/pLqGg2z1KzQxCGpMwds-9.jpeg",
            "isPro": false,
            "fullname": "Jinbo Xing",
            "user": "Doubiiu",
            "type": "user"
          },
          "name": "Jinbo Xing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:40.328Z",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b6",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-10T10:06:56.510Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T17:57:53.000Z",
      "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos\n  via Diffusion Models",
      "summary": "We present TrajectoryCrafter, a novel approach to redirect camera\ntrajectories for monocular videos. By disentangling deterministic view\ntransformations from stochastic content generation, our method achieves precise\ncontrol over user-specified camera trajectories. We propose a novel dual-stream\nconditional video diffusion model that concurrently integrates point cloud\nrenders and source videos as conditions, ensuring accurate view transformations\nand coherent 4D content generation. Instead of leveraging scarce multi-view\nvideos, we curate a hybrid training dataset combining web-scale monocular\nvideos with static multi-view datasets, by our innovative double-reprojection\nstrategy, significantly fostering robust generalization across diverse scenes.\nExtensive evaluations on multi-view and large-scale monocular videos\ndemonstrate the superior performance of our method.",
      "upvotes": 8,
      "discussionId": "67ce838a764226f050ad1952",
      "projectPage": "https://trajectorycrafter.github.io/",
      "githubRepo": "https://github.com/TrajectoryCrafter/TrajectoryCrafter"
    },
    "publishedAt": "2025-03-10T02:24:39.763Z",
    "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/lpXbCmGz-upwRVSEUzBjV.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05638.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a7458afbb0117ba15c59f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
      "fullname": "Wenbo Hu",
      "name": "wbhu-tc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05652",
      "authors": [
        {
          "_id": "67ce524ee969bc5fd69c9388",
          "user": {
            "_id": "61e9f5398c237a147a3f4ab5",
            "avatarUrl": "/avatars/afd4ec17cb132b5ab56e50a678c4786d.svg",
            "isPro": false,
            "fullname": "Yunfan Jiang",
            "user": "yunfanj",
            "type": "user"
          },
          "name": "Yunfan Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:07.901Z",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9389",
          "name": "Ruohan Zhang",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938a",
          "name": "Josiah Wong",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938b",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938c",
          "user": {
            "_id": "63509bc859bfa9a85d4220aa",
            "avatarUrl": "/avatars/ca2cc9b87f5ca5cd51606b2f9edf89d0.svg",
            "isPro": false,
            "fullname": "Yanjie Ze",
            "user": "yjze",
            "type": "user"
          },
          "name": "Yanjie Ze",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:05.242Z",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938d",
          "name": "Hang Yin",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938e",
          "name": "Cem Gokmen",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938f",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9390",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9391",
          "name": "Li Fei-Fei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T18:15:21.000Z",
      "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation\n  for Everyday Household Activities",
      "summary": "Real-world household tasks present significant challenges for mobile\nmanipulation robots. An analysis of existing robotics benchmarks reveals that\nsuccessful task performance hinges on three key whole-body control\ncapabilities: bimanual coordination, stable and precise navigation, and\nextensive end-effector reachability. Achieving these capabilities requires\ncareful hardware design, but the resulting system complexity further\ncomplicates visuomotor policy learning. To address these challenges, we\nintroduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for\nwhole-body manipulation in diverse household tasks. Built on a bimanual,\nwheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body\nteleoperation interface for data collection and a novel algorithm for learning\nwhole-body visuomotor policies. We evaluate BRS on five challenging household\ntasks that not only emphasize the three core capabilities but also introduce\nadditional complexities, such as long-range navigation, interaction with\narticulated and deformable objects, and manipulation in confined spaces. We\nbelieve that BRS's integrated robotic embodiment, data collection interface,\nand learning framework mark a significant step toward enabling real-world\nwhole-body manipulation for everyday household tasks. BRS is open-sourced at\nhttps://behavior-robot-suite.github.io/",
      "upvotes": 7,
      "discussionId": "67ce5294e969bc5fd69c9a2c",
      "projectPage": "https://behavior-robot-suite.github.io/",
      "githubRepo": "https://github.com/behavior-robot-suite/brs-algo"
    },
    "publishedAt": "2025-03-09T22:51:04.616Z",
    "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/61e9f5398c237a147a3f4ab5/WD3cV-QLiHRgh4IUOrikN.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05652.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e9f5398c237a147a3f4ab5",
      "avatarUrl": "/avatars/afd4ec17cb132b5ab56e50a678c4786d.svg",
      "fullname": "Yunfan Jiang",
      "name": "yunfanj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05379",
      "authors": [
        {
          "_id": "67ce5f2389663abdbc364495",
          "name": "Jiaxing Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5f2389663abdbc364496",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "67ce5f2389663abdbc364497",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T12:46:42.000Z",
      "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with\n  Reinforcing Learning",
      "summary": "In this work, we present the first application of Reinforcement Learning with\nVerifiable Reward (RLVR) to an Omni-multimodal large language model in the\ncontext of emotion recognition, a task where both visual and audio modalities\nplay crucial roles. We leverage RLVR to optimize the Omni model, significantly\nenhancing its performance in three key aspects: reasoning capability, emotion\nrecognition accuracy, and generalization ability. The introduction of RLVR not\nonly improves the model's overall performance on in-distribution data but also\ndemonstrates superior robustness when evaluated on out-of-distribution\ndatasets. More importantly, the improved reasoning capability enables clear\nanalysis of the contributions of different modalities, particularly visual and\naudio information, in the emotion recognition process. This provides valuable\ninsights into the optimization of multimodal large language models.",
      "upvotes": 6,
      "discussionId": "67ce5f2489663abdbc3644d0"
    },
    "publishedAt": "2025-03-09T23:40:46.906Z",
    "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05379.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04872",
      "authors": [
        {
          "_id": "67ce5deedb623d45a95deb72",
          "user": {
            "_id": "632c30576bcb864974cc40a8",
            "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
            "isPro": false,
            "fullname": "sunlin",
            "user": "lincharliesun",
            "type": "user"
          },
          "name": "Lin Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:47.601Z",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb73",
          "name": "Guangxiang Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb74",
          "name": "Xiaoqi Jian",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb75",
          "name": "Yuhan Wu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb76",
          "name": "Weihong Lin",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb77",
          "name": "Yongfu Zhu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb78",
          "name": "Change Jia",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb79",
          "name": "Linglin Zhang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7a",
          "name": "Jinzhu Wu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7b",
          "name": "Junfeng Ran",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7c",
          "name": "Sai-er Hu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7d",
          "name": "Zihan Jiang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7e",
          "name": "Junting Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7f",
          "name": "Wenrui Liu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb80",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb81",
          "name": "Tong Yang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb82",
          "name": "Xiangzheng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T16:25:53.000Z",
      "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
      "summary": "The challenge of reducing the size of Large Language Models (LLMs) while\nmaintaining their performance has gained significant attention. However,\nexisting methods, such as model distillation and transfer learning, often fail\nto achieve high accuracy. To address this limitation, we introduce the\nBranch-Merge distillation approach, which enhances model compression through\ntwo phases: (1) the Branch Phase, where knowledge from a large teacher model is\nselectively distilled into specialized student models via\ndomain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where\nthese student models are merged to enable cross-domain knowledge transfer and\nimprove generalization. We validate our distillation approach using DeepSeek-R1\nas the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting\nmerged model, TinyR1-32B-Preview, outperforms its counterpart\nDeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics\n(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving\nnear-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge\ndistillation approach provides a scalable solution for creating smaller,\nhigh-performing LLMs with reduced computational cost and time.",
      "upvotes": 5,
      "discussionId": "67ce5df0db623d45a95dec1f"
    },
    "publishedAt": "2025-03-09T23:35:58.424Z",
    "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05447",
      "authors": [
        {
          "_id": "67ceab4132a6585cecad2c36",
          "user": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "isPro": false,
            "fullname": "Weigao Sun",
            "user": "weigao266",
            "type": "user"
          },
          "name": "Weigao Sun",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-10T09:09:22.436Z",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c37",
          "name": "Disen Lan",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c38",
          "name": "Tong Zhu",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c39",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67ceab4132a6585cecad2c3a",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T14:17:45.000Z",
      "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
      "summary": "Linear Sequence Modeling (LSM) like linear attention, state space models and\nlinear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant\narchitectural improvements. In this paper, we introduce Linear-MoE, a\nproduction-level system for modeling and training large-scale models that\nintegrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules\nfor linear-complexity sequence modeling and MoE layers for sparsely activation,\naiming to offer high performance with efficient training. The Linear-MoE system\ncomprises: 1) Modeling subsystem, which provides a unified framework supporting\nall instances of LSM. and 2) Training subsystem, which facilitates efficient\ntraining by incorporating various advanced parallelism technologies,\nparticularly Sequence Parallelism designed for Linear-MoE models. Additionally,\nwe explore hybrid models that combine Linear-MoE layers with standard\nTransformer-MoE layers with its Sequence Parallelism to further enhance model\nflexibility and performance. Evaluations on two model series, A0.3B-2B and\nA1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining\ncompetitive performance on various benchmarks, showcasing its potential as a\nnext-generation foundational model architecture. Code:\nhttps://github.com/OpenSparseLLMs/Linear-MoE.",
      "upvotes": 3,
      "discussionId": "67ceab4232a6585cecad2c82",
      "githubRepo": "https://github.com/OpenSparseLLMs/Linear-MoE"
    },
    "publishedAt": "2025-03-10T05:05:22.522Z",
    "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05447.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01713",
      "authors": [
        {
          "_id": "67c75e18cb29e2a4b0eb0293",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:26:50.367Z",
          "hidden": false
        },
        {
          "_id": "67c75e18cb29e2a4b0eb0294",
          "name": "Guoliang Li",
          "hidden": false
        },
        {
          "_id": "67c75e18cb29e2a4b0eb0295",
          "name": "Jinyang Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T16:25:58.000Z",
      "title": "SAGE: A Framework of Precise Retrieval for RAG",
      "summary": "Retrieval-augmented generation (RAG) has demonstrated significant proficiency\nin conducting question-answering (QA) tasks within a specified corpus.\nNonetheless, numerous failure instances of RAG in QA still exist. These\nfailures are not solely attributable to the limitations of Large Language\nModels (LLMs); instead, they predominantly arise from the retrieval of\ninaccurate information for LLMs due to two limitations: (1) Current RAG methods\nsegment the corpus without considering semantics, making it difficult to find\nrelevant context due to impaired correlation between questions and the\nsegments. (2) There is a trade-off between missing essential context with fewer\ncontext retrieved and getting irrelevant context with more context retrieved.\n  In this paper, we introduce a RAG framework (SAGE), to overcome these\nlimitations. First, to address the segmentation issue without considering\nsemantics, we propose to train a semantic segmentation model. This model is\ntrained to segment the corpus into semantically complete chunks. Second, to\nensure that only the most relevant chunks are retrieved while the irrelevant\nones are ignored, we design a chunk selection algorithm to dynamically select\nchunks based on the decreasing speed of the relevance score, leading to a more\nrelevant selection. Third, to further ensure the precision of the retrieved\nchunks, we propose letting LLMs assess whether retrieved chunks are excessive\nor lacking and then adjust the amount of context accordingly. Experiments show\nthat SAGE outperforms baselines by 61.25% in the quality of QA on average.\nMoreover, by avoiding retrieving noisy context, SAGE lowers the cost of the\ntokens consumed in LLM inference and achieves a 49.41% enhancement in cost\nefficiency on average. Additionally, our work offers valuable insights for\nboosting RAG.",
      "upvotes": 3,
      "discussionId": "67c75e1ccb29e2a4b0eb03a9"
    },
    "publishedAt": "2025-03-10T03:23:51.482Z",
    "title": "SAGE: A Framework of Precise Retrieval for RAG",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01713.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05315",
      "authors": [
        {
          "_id": "67ce6db07110b8bedb3344a7",
          "name": "Saumya Chaturvedi",
          "hidden": false
        },
        {
          "_id": "67ce6db07110b8bedb3344a8",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:45.150Z",
          "hidden": false
        },
        {
          "_id": "67ce6db07110b8bedb3344a9",
          "name": "Laurent Bindschaedler",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T10:50:45.000Z",
      "title": "LoRACode: LoRA Adapters for Code Embeddings",
      "summary": "Code embeddings are essential for semantic code search; however, current\napproaches often struggle to capture the precise syntactic and contextual\nnuances inherent in code. Open-source models such as CodeBERT and UniXcoder\nexhibit limitations in scalability and efficiency, while high-performing\nproprietary systems impose substantial computational costs. We introduce a\nparameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to\nconstruct task-specific adapters for code retrieval. Our approach reduces the\nnumber of trainable parameters to less than two percent of the base model,\nenabling rapid fine-tuning on extensive code corpora (2 million samples in 25\nminutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in\nMean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code\nsearch tasks across multiple programming languages. Distinction in task-wise\nand language-wise adaptation helps explore the sensitivity of code retrieval\nfor syntactical and linguistic variations.",
      "upvotes": 2,
      "discussionId": "67ce6db17110b8bedb3344c5"
    },
    "publishedAt": "2025-03-10T00:51:02.203Z",
    "title": "LoRACode: LoRA Adapters for Code Embeddings",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05132",
      "authors": [
        {
          "_id": "67ce5ec17c6e6ea1cc5649c2",
          "name": "Hengguang Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c3",
          "name": "Xirui Li",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c4",
          "name": "Ruochen Wang",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c5",
          "name": "Minhao Cheng",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c6",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c7",
          "name": "Cho-Jui Hsieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T04:21:47.000Z",
      "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
      "summary": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple\nrule-based incentives can enable autonomous development of complex reasoning in\nlarge language models, characterized by the \"aha moment\", in which the model\nmanifest self-reflection and increased response length during training.\nHowever, attempts to extend this success to multimodal reasoning often failed\nto reproduce these key characteristics. In this report, we present the first\nsuccessful replication of these emergent characteristics for multimodal\nreasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying\nreinforcement learning directly on the SAT dataset, our model achieves 59.47%\naccuracy on CVBench, outperforming the base model by approximately ~30% and\nexceeding both SFT setting by ~2%. In addition, we share our failed attempts\nand insights in attempting to achieve R1-like reasoning using RL with instruct\nmodels. aiming to shed light on the challenges involved. Our key observations\ninclude: (1) applying RL on instruct model often results in trivial reasoning\ntrajectories, and (2) naive length reward are ineffective in eliciting\nreasoning capabilities. The project code is available at\nhttps://github.com/turningpoint-ai/VisualThinker-R1-Zero",
      "upvotes": 2,
      "discussionId": "67ce5ec27c6e6ea1cc564a01"
    },
    "publishedAt": "2025-03-09T23:39:12.374Z",
    "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05132.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04808",
      "authors": [
        {
          "_id": "67ce5c7065b141ae6b0d3957",
          "name": "Stephen Chung",
          "hidden": false
        },
        {
          "_id": "67ce5c7065b141ae6b0d3958",
          "name": "Wenyu Du",
          "hidden": false
        },
        {
          "_id": "67ce5c7065b141ae6b0d3959",
          "name": "Jie Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:53:39.000Z",
      "title": "Learning from Failures in Multi-Attempt Reinforcement Learning",
      "summary": "Recent advancements in reinforcement learning (RL) for large language models\n(LLMs), exemplified by DeepSeek R1, have shown that even a simple\nquestion-answering task can substantially improve an LLM's reasoning\ncapabilities. In this work, we extend this approach by modifying the task into\na multi-attempt setting. Instead of generating a single response per question,\nthe model is given multiple attempts, with feedback provided after incorrect\nresponses. The multi-attempt task encourages the model to refine its previous\nattempts and improve search efficiency. Experimental results show that even a\nsmall LLM trained on a multi-attempt task achieves significantly higher\naccuracy when evaluated with more attempts, improving from 45.6% with 1 attempt\nto 52.5% with 2 attempts on the math benchmark. In contrast, the same LLM\ntrained on a standard single-turn task exhibits only a marginal improvement,\nincreasing from 42.3% to 43.2% when given more attempts during evaluation. The\nresults indicate that, compared to the standard single-turn task, an LLM\ntrained on a multi-attempt task achieves slightly better performance on math\nbenchmarks while also learning to refine its responses more effectively based\non user feedback. Full code is available at\nhttps://github.com/DualityRL/multi-attempt",
      "upvotes": 2,
      "discussionId": "67ce5c7165b141ae6b0d39c6"
    },
    "publishedAt": "2025-03-09T23:29:35.505Z",
    "title": "Learning from Failures in Multi-Attempt Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04808.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6317
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04548",
      "authors": [
        {
          "_id": "67cbff8e4dedec48bdec8a99",
          "name": "Zhipeng Chen",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9a",
          "user": {
            "_id": "6703ac76ea890f0ca5b225eb",
            "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
            "isPro": false,
            "fullname": "Yingqian Min",
            "user": "EliverQ",
            "type": "user"
          },
          "name": "Yingqian Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:02:04.349Z",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9b",
          "name": "Beichen Zhang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9c",
          "name": "Jie Chen",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9d",
          "name": "Jinhao Jiang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9e",
          "name": "Daixuan Cheng",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8a9f",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa0",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa1",
          "name": "Xu Miao",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa2",
          "name": "Yang Lu",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa3",
          "name": "Lei Fang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa4",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "67cbff8e4dedec48bdec8aa5",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T15:34:27.000Z",
      "title": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models",
      "summary": "In this report, we present the third technical report on the development of\nslow-thinking models as part of the STILL project. As the technical pathway\nbecomes clearer, scaling RL training has become a central technique for\nimplementing such reasoning models. We systematically experiment with and\ndocument the effects of various factors influencing RL training, conducting\nexperiments on both base models and fine-tuned models. Specifically, we\ndemonstrate that our RL training approach consistently improves the Qwen2.5-32B\nbase models, enhancing both response length and test accuracy. Furthermore, we\nshow that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already\nachieved a high performance level, it can be further refined through RL\ntraining, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we\nalso explore the use of tool manipulation, finding that it significantly boosts\nthe reasoning performance of large reasoning models. This approach achieves a\nremarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its\neffectiveness in enhancing model capabilities. We release our resources at the\nSTILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
      "upvotes": 1,
      "discussionId": "67cbff8f4dedec48bdec8af3"
    },
    "publishedAt": "2025-03-10T05:23:26.375Z",
    "title": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04548.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6703ac76ea890f0ca5b225eb",
      "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
      "fullname": "Yingqian Min",
      "name": "EliverQ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04504",
      "authors": [
        {
          "_id": "67cb8e882cfa481bcee9455e",
          "user": {
            "_id": "66a07c07b7f0bb64d3b35497",
            "avatarUrl": "/avatars/c38af1ddb7a5b625e26b7ff05957ff7c.svg",
            "isPro": false,
            "fullname": "SunghyunAhn",
            "user": "SkiddieAhn",
            "type": "user"
          },
          "name": "Sunghyun Ahn",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:02:12.798Z",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee9455f",
          "user": {
            "_id": "673d7b70713e4b8db2d5ca94",
            "avatarUrl": "/avatars/b9e89eba62eb939ddd93c1cb91744e93.svg",
            "isPro": false,
            "fullname": "Youngwan Jo",
            "user": "jyy1551",
            "type": "user"
          },
          "name": "Youngwan Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:02:08.710Z",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94560",
          "name": "Kijung Lee",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94561",
          "name": "Sein Kwon",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94562",
          "name": "Inpyo Hong",
          "hidden": false
        },
        {
          "_id": "67cb8e882cfa481bcee94563",
          "name": "Sanghyun Park",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T14:52:34.000Z",
      "title": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM",
      "summary": "Video anomaly detection (VAD) is crucial for video analysis and surveillance\nin computer vision. However, existing VAD models rely on learned normal\npatterns, which makes them difficult to apply to diverse environments.\nConsequently, users should retrain models or develop separate AI models for new\nenvironments, which requires expertise in machine learning, high-performance\nhardware, and extensive data collection, limiting the practical usability of\nVAD. To address these challenges, this study proposes customizable video\nanomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers\nuser-defined text as an abnormal event and detects frames containing a\nspecified event in a video. We effectively implemented AnyAnomaly using a\ncontext-aware visual question answering without fine-tuning the large vision\nlanguage model. To validate the effectiveness of the proposed model, we\nconstructed C-VAD datasets and demonstrated the superiority of AnyAnomaly.\nFurthermore, our approach showed competitive performance on VAD benchmark\ndatasets, achieving state-of-the-art results on the UBnormal dataset and\noutperforming other methods in generalization across all datasets. Our code is\navailable online at github.com/SkiddieAhn/Paper-AnyAnomaly.",
      "upvotes": 0,
      "discussionId": "67cb8e8a2cfa481bcee945cd",
      "githubRepo": "https://github.com/SkiddieAhn/Paper-AnyAnomaly"
    },
    "publishedAt": "2025-03-10T04:06:49.447Z",
    "title": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/TNPrQD3FdFBVVW2pUoEnU.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/cA30QnSF_7AeHciWhCFSN.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/T9EwE4Ea7DrH3XVZ_eJ1g.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/Duuqk_Ph1GNfz6HW2Qv5g.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/66a07c07b7f0bb64d3b35497/-Dze7JwIaBTbCfXUetsCd.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04504.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a07c07b7f0bb64d3b35497",
      "avatarUrl": "/avatars/c38af1ddb7a5b625e26b7ff05957ff7c.svg",
      "fullname": "SunghyunAhn",
      "name": "SkiddieAhn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]