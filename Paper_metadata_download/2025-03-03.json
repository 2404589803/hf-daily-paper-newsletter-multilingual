[
  {
    "paper": {
      "id": "2502.20730",
      "authors": [
        {
          "_id": "67c514aba3d873e41624a082",
          "user": {
            "_id": "63664c8fa2abcdf2fd6425ed",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
            "isPro": false,
            "fullname": "Li Zhuoqun",
            "user": "lzq2021",
            "type": "user"
          },
          "name": "Zhuoqun Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-03T08:07:26.218Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a083",
          "user": {
            "_id": "64a4ceda9a90f701134189b7",
            "avatarUrl": "/avatars/859a189c5d2ae2fcb9aa2d79104fbfe7.svg",
            "isPro": false,
            "fullname": "Haiyang Yu",
            "user": "yhycai",
            "type": "user"
          },
          "name": "Haiyang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-03T09:31:12.493Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a084",
          "user": {
            "_id": "63ef664304b0e373992a2633",
            "avatarUrl": "/avatars/cba554ff88bd8b68ae51bea8ee991d13.svg",
            "isPro": false,
            "fullname": "Xuanang Chen",
            "user": "xuanang",
            "type": "user"
          },
          "name": "Xuanang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:31.384Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a085",
          "user": {
            "_id": "6711c702f858a456b4b9f3a4",
            "avatarUrl": "/avatars/178e9567c3111ab22717c3c0dd003a6a.svg",
            "isPro": false,
            "fullname": "Hongyu  Lin",
            "user": "sanmusunrise",
            "type": "user"
          },
          "name": "Hongyu Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:28:09.791Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a086",
          "user": {
            "_id": "6216496a9b34d2fb49144599",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg",
            "isPro": false,
            "fullname": "Yaojie Lu",
            "user": "luyaojie",
            "type": "user"
          },
          "name": "Yaojie Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:38.957Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a087",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a088",
          "user": {
            "_id": "65e99a77e71555ed193609cf",
            "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
            "isPro": false,
            "fullname": "Xianpei Han",
            "user": "xphan",
            "type": "user"
          },
          "name": "Xianpei Han",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:51.007Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a089",
          "user": {
            "_id": "66641b2fd8e1e34bc621e688",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66641b2fd8e1e34bc621e688/csPETwnx2zCIHSWi9uAi-.png",
            "isPro": false,
            "fullname": "Yongbin Li",
            "user": "Yongbin-Li",
            "type": "user"
          },
          "name": "Yongbin Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:57.561Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a08a",
          "name": "Le Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-28T05:23:10.000Z",
      "title": "DeepSolution: Boosting Complex Engineering Solution Design via\n  Tree-based Exploration and Bi-point Thinking",
      "summary": "Designing solutions for complex engineering challenges is crucial in human\nproduction activities. However, previous research in the retrieval-augmented\ngeneration (RAG) field has not sufficiently addressed tasks related to the\ndesign of complex engineering solutions. To fill this gap, we introduce a new\nbenchmark, SolutionBench, to evaluate a system's ability to generate complete\nand feasible solutions for engineering problems with multiple complex\nconstraints. To further advance the design of complex engineering solutions, we\npropose a novel system, SolutionRAG, that leverages the tree-based exploration\nand bi-point thinking mechanism to generate reliable solutions. Extensive\nexperimental results demonstrate that SolutionRAG achieves state-of-the-art\n(SOTA) performance on the SolutionBench, highlighting its potential to enhance\nthe automation and reliability of complex engineering solution design in\nreal-world applications.",
      "upvotes": 11,
      "discussionId": "67c514aca3d873e41624a10b"
    },
    "publishedAt": "2025-03-02T21:35:24.437Z",
    "title": "DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/y_kT4GP3xgm-5RdguMNV7.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/wDAS_USsxsVHbin1I5CEe.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/4lJgWp9V8pm4vDBUH4I5n.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20730.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63664c8fa2abcdf2fd6425ed",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
      "fullname": "Li Zhuoqun",
      "name": "lzq2021",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.18600",
      "authors": [
        {
          "_id": "67c0a8058589d8ecb79d472b",
          "user": {
            "_id": "6594b1bb57a556fbe162915e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594b1bb57a556fbe162915e/WuYxqbbvaJaT-xsk5KhoT.jpeg",
            "isPro": false,
            "fullname": "Silei Xu",
            "user": "sileixu",
            "type": "user"
          },
          "name": "Silei Xu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-27T18:01:14.543Z",
          "hidden": false
        },
        {
          "_id": "67c0a8058589d8ecb79d472c",
          "name": "Wenhao Xie",
          "hidden": false
        },
        {
          "_id": "67c0a8058589d8ecb79d472d",
          "name": "Lingxiao Zhao",
          "hidden": false
        },
        {
          "_id": "67c0a8058589d8ecb79d472e",
          "user": {
            "_id": "5efd09cf49ed724c8a135868",
            "avatarUrl": "/avatars/af12bc94657979677a9f26183f0c9727.svg",
            "isPro": false,
            "fullname": "Pengcheng He",
            "user": "DeBERTa",
            "type": "user"
          },
          "name": "Pengcheng He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:30:43.479Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T19:36:06.000Z",
      "title": "Chain of Draft: Thinking Faster by Writing Less",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nsolving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT)\nprompting, which emphasizes verbose, step-by-step reasoning. However, humans\ntypically employ a more efficient strategy: drafting concise intermediate\nthoughts that capture only essential information. In this work, we propose\nChain of Draft (CoD), a novel paradigm inspired by human cognitive processes,\nwhere LLMs generate minimalistic yet informative intermediate reasoning outputs\nwhile solving tasks. By reducing verbosity and focusing on critical insights,\nCoD matches or surpasses CoT in accuracy while using as little as only 7.6% of\nthe tokens, significantly reducing cost and latency across various reasoning\ntasks.",
      "upvotes": 6,
      "discussionId": "67c0a8078589d8ecb79d47ed"
    },
    "publishedAt": "2025-03-03T02:35:09.967Z",
    "title": "Chain of Draft: Thinking Faster by Writing Less",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18600.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63da3d7ae697e5898cb86854",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675246771355-noauth.jpeg",
      "fullname": "Talha Rüzgar Akkuş",
      "name": "Q-bert",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 85
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18017",
      "authors": [
        {
          "_id": "67bef5a6070ec160042d99f4",
          "user": {
            "_id": "657429d833e5a4bf5b278615",
            "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
            "isPro": false,
            "fullname": "QiuchenWang",
            "user": "autumncc",
            "type": "user"
          },
          "name": "Qiuchen Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-28T12:15:57.850Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f5",
          "name": "Ruixue Ding",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f6",
          "user": {
            "_id": "64892d31cbda0d1cdb956897",
            "avatarUrl": "/avatars/3cdafe03a8295124636347d15a099aaf.svg",
            "isPro": false,
            "fullname": "Zehui Chen",
            "user": "lovesnowbest",
            "type": "user"
          },
          "name": "Zehui Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:32:18.129Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f7",
          "user": {
            "_id": "65351cbe6141b3927afaed17",
            "avatarUrl": "/avatars/5abf5f2c4ab329e63a7f45c15c9dfb93.svg",
            "isPro": false,
            "fullname": "weiqi wu",
            "user": "vickywu",
            "type": "user"
          },
          "name": "Weiqi Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:32:12.075Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f8",
          "user": {
            "_id": "62e8efb14210d3fe69eacb42",
            "avatarUrl": "/avatars/2feadd75274bf353b910f4679ef72b39.svg",
            "isPro": false,
            "fullname": "Shihang Wang",
            "user": "shihang",
            "type": "user"
          },
          "name": "Shihang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:32:05.679Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f9",
          "user": {
            "_id": "63a091e42fabbbb89991f5ce",
            "avatarUrl": "/avatars/d55485b06461764c36c9edf9d6e8892c.svg",
            "isPro": false,
            "fullname": "pengjun xie",
            "user": "xpjandy",
            "type": "user"
          },
          "name": "Pengjun Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:31:59.813Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99fa",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T09:26:12.000Z",
      "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic\n  Iterative Reasoning Agents",
      "summary": "Understanding information from visually rich documents remains a significant\nchallenge for traditional Retrieval-Augmented Generation (RAG) methods.\nExisting benchmarks predominantly focus on image-based question answering (QA),\noverlooking the fundamental challenges of efficient retrieval, comprehension,\nand reasoning within dense visual documents. To bridge this gap, we introduce\nViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich\ndocuments requiring complex reasoning. Based on it, we identify key limitations\nin current RAG approaches: (i) purely visual retrieval methods struggle to\neffectively integrate both textual and visual features, and (ii) previous\napproaches often allocate insufficient reasoning tokens, limiting their\neffectiveness. To address these challenges, we propose ViDoRAG, a novel\nmulti-agent RAG framework tailored for complex reasoning across visual\ndocuments. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy\nto effectively handle multi-modal retrieval. To further elicit the model's\nreasoning capabilities, we introduce an iterative agent workflow incorporating\nexploration, summarization, and reflection, providing a framework for\ninvestigating test-time scaling in RAG domains. Extensive experiments on\nViDoSeek validate the effectiveness and generalization of our approach.\nNotably, ViDoRAG outperforms existing methods by over 10% on the competitive\nViDoSeek benchmark.",
      "upvotes": 4,
      "discussionId": "67bef5a7070ec160042d9a65"
    },
    "publishedAt": "2025-03-02T22:22:01.895Z",
    "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18017.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657429d833e5a4bf5b278615",
      "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
      "fullname": "QiuchenWang",
      "name": "autumncc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20545",
      "authors": [
        {
          "_id": "67c51b459d5807d6674b3d3c",
          "name": "Kechen Li",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d3d",
          "name": "Wenqi Zhu",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d3e",
          "name": "Coralia Cartis",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d3f",
          "user": {
            "_id": "64bb61e876a6e2efcc728e22",
            "avatarUrl": "/avatars/b0ed1c9f13fd1f2c99d202155001e39b.svg",
            "isPro": false,
            "fullname": "Tianbo Ji",
            "user": "jitianbo",
            "type": "user"
          },
          "name": "Tianbo Ji",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:35:49.782Z",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d40",
          "name": "Shiwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T21:41:43.000Z",
      "title": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers",
      "summary": "Large Language Models (LLMs) have achieved human-level proficiency across\ndiverse tasks, but their ability to perform rigorous mathematical problem\nsolving remains an open challenge. In this work, we investigate a fundamental\nyet computationally intractable problem: determining whether a given\nmultivariate polynomial is nonnegative. This problem, closely related to\nHilbert's Seventeenth Problem, plays a crucial role in global polynomial\noptimization and has applications in various fields. First, we introduce\nSoS-1K, a meticulously curated dataset of approximately 1,000 polynomials,\nalong with expert-designed reasoning instructions based on five progressively\nchallenging criteria. Evaluating multiple state-of-the-art LLMs, we find that\nwithout structured guidance, all models perform only slightly above the random\nguess baseline 50%. However, high-quality reasoning instructions significantly\nimprove accuracy, boosting performance up to 81%. Furthermore, our 7B model,\nSoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3\nand GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation\ntime needed for letters, respectively. Our findings highlight the potential of\nLLMs to push the boundaries of mathematical reasoning and tackle NP-hard\nproblems.",
      "upvotes": 2,
      "discussionId": "67c51b469d5807d6674b3d88"
    },
    "publishedAt": "2025-03-02T22:00:31.796Z",
    "title": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20545.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6260
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20396",
      "authors": [
        {
          "_id": "67c51d36c830dcb76bbb5994",
          "name": "Toru Lin",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5995",
          "name": "Kartik Sachdev",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5996",
          "name": "Linxi Fan",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5997",
          "user": {
            "_id": "65369a95605a07338de78ab0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sGFjOjLT2akN-sn5beVWL.jpeg",
            "isPro": false,
            "fullname": "Jitendra Malik ",
            "user": "jitendra1995",
            "type": "user"
          },
          "name": "Jitendra Malik",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:36:34.177Z",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5998",
          "name": "Yuke Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T18:59:52.000Z",
      "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous\n  Manipulation on Humanoids",
      "summary": "Reinforcement learning has delivered promising results in achieving human- or\neven superhuman-level capabilities across diverse problem domains, but success\nin dexterous robot manipulation remains limited. This work investigates the key\nchallenges in applying reinforcement learning to solve a collection of\ncontact-rich manipulation tasks on a humanoid embodiment. We introduce novel\ntechniques to overcome the identified challenges with empirical validation. Our\nmain contributions include an automated real-to-sim tuning module that brings\nthe simulated environment closer to the real world, a generalized reward design\nscheme that simplifies reward engineering for long-horizon contact-rich\nmanipulation tasks, a divide-and-conquer distillation process that improves the\nsample efficiency of hard-exploration problems while maintaining sim-to-real\nperformance, and a mixture of sparse and dense object representations to bridge\nthe sim-to-real perception gap. We show promising results on three humanoid\ndexterous manipulation tasks, with ablation studies on each technique. Our work\npresents a successful approach to learning humanoid dexterous manipulation\nusing sim-to-real reinforcement learning, achieving robust generalization and\nhigh performance without the need for human demonstration.",
      "upvotes": 1,
      "discussionId": "67c51d39c830dcb76bbb5a1f"
    },
    "publishedAt": "2025-03-02T22:08:44.891Z",
    "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20396.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6260
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20811",
      "authors": [
        {
          "_id": "67c51c198d02783fa3a6249d",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a6249e",
          "name": "Jingyun Hua",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a6249f",
          "user": {
            "_id": "675a69699e086bd6250a36ef",
            "avatarUrl": "/avatars/95c72e3975d1a37f8655a2fe629746ec.svg",
            "isPro": false,
            "fullname": "Weihong Lin",
            "user": "lwher1996",
            "type": "user"
          },
          "name": "Weihong Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:42:30.547Z",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a0",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a1",
          "name": "Fuzheng Zhang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a2",
          "name": "Jianlong Wu",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a3",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a4",
          "name": "Liqiang Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-28T07:53:40.000Z",
      "title": "HAIC: Improving Human Action Understanding and Generation with Better\n  Captions for Multi-modal Large Language Models",
      "summary": "Recent Multi-modal Large Language Models (MLLMs) have made great progress in\nvideo understanding. However, their performance on videos involving human\nactions is still limited by the lack of high-quality data. To address this, we\nintroduce a two-stage data annotation pipeline. First, we design strategies to\naccumulate videos featuring clear human actions from the Internet. Second,\nvideos are annotated in a standardized caption format that uses human\nattributes to distinguish individuals and chronologically details their actions\nand interactions. Through this pipeline, we curate two datasets, namely\nHAICTrain and HAICBench. HAICTrain comprises 126K video-caption pairs\ngenerated by Gemini-Pro and verified for training purposes. Meanwhile,\nHAICBench includes 500 manually annotated video-caption pairs and\n1,400 QA pairs, for a comprehensive evaluation of human action understanding.\nExperimental results demonstrate that training with HAICTrain not only\nsignificantly enhances human understanding abilities across 4 benchmarks, but\ncan also improve text-to-video generation results. Both the HAICTrain and\nHAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.",
      "upvotes": 1,
      "discussionId": "67c51c1b8d02783fa3a62543"
    },
    "publishedAt": "2025-03-02T22:04:15.087Z",
    "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20811.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6260
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20583",
      "authors": [
        {
          "_id": "67c516998d02783fa3a52dc8",
          "user": {
            "_id": "6304ac1a412a1b9d381ca378",
            "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
            "isPro": false,
            "fullname": "Keisuke Kamahori",
            "user": "kamahori",
            "type": "user"
          },
          "name": "Keisuke Kamahori",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-03T08:07:02.986Z",
          "hidden": false
        },
        {
          "_id": "67c516998d02783fa3a52dc9",
          "user": {
            "_id": "62908273c740ebb981a6dba4",
            "avatarUrl": "/avatars/465f50369c367b07670f5209c83d65f2.svg",
            "isPro": false,
            "fullname": "Jungo Kasai",
            "user": "jungok",
            "type": "user"
          },
          "name": "Jungo Kasai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:43:49.097Z",
          "hidden": false
        },
        {
          "_id": "67c516998d02783fa3a52dca",
          "user": {
            "_id": "628c26a8b80bb09700d6af86",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653352051245-noauth.jpeg",
            "isPro": false,
            "fullname": "Noriyuki Kojima",
            "user": "kojimano",
            "type": "user"
          },
          "name": "Noriyuki Kojima",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:43:56.698Z",
          "hidden": false
        },
        {
          "_id": "67c516998d02783fa3a52dcb",
          "user": {
            "_id": "654132fe5a9a913c6c870e79",
            "avatarUrl": "/avatars/2f6807eddef1929c571977e9af35f952.svg",
            "isPro": false,
            "fullname": "Baris Kasikci",
            "user": "kasikci",
            "type": "user"
          },
          "name": "Baris Kasikci",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:44:04.084Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T22:52:21.000Z",
      "title": "LiteASR: Efficient Automatic Speech Recognition with Low-Rank\n  Approximation",
      "summary": "Modern automatic speech recognition (ASR) models, such as OpenAI's Whisper,\nrely on deep encoder-decoder architectures, and their encoders are a critical\nbottleneck for efficient deployment due to high computational intensity. We\nintroduce LiteASR, a low-rank compression scheme for ASR encoders that\nsignificantly reduces inference costs while maintaining transcription accuracy.\nOur approach leverages the strong low-rank properties observed in intermediate\nactivations: by applying principal component analysis (PCA) with a small\ncalibration dataset, we approximate linear transformations with a chain of\nlow-rank matrix multiplications, and further optimize self-attention to work in\nthe reduced dimension. Evaluation results show that our method can compress\nWhisper large-v3's encoder size by over 50%, matching Whisper medium's size\nwith better transcription accuracy, thereby establishing a new Pareto-optimal\nfrontier of efficiency and performance. The code of LiteASR is available at\nhttps://github.com/efeslab/LiteASR.",
      "upvotes": 1,
      "discussionId": "67c516998d02783fa3a52dfd"
    },
    "publishedAt": "2025-03-02T21:48:46.577Z",
    "title": "LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20583.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6304ac1a412a1b9d381ca378",
      "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
      "fullname": "Keisuke Kamahori",
      "name": "kamahori",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.19577",
      "authors": [
        {
          "_id": "67c42356054ae6d1c760b643",
          "user": {
            "_id": "66588b6fd22637bfab498709",
            "avatarUrl": "/avatars/9007f0d3b078bd6193912a5359107f24.svg",
            "isPro": false,
            "fullname": "Hugues Turbé",
            "user": "hturbe",
            "type": "user"
          },
          "name": "Hugues Turbé",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-02T20:15:04.391Z",
          "hidden": false
        },
        {
          "_id": "67c42356054ae6d1c760b644",
          "name": "Mina Bjelogrlic",
          "hidden": false
        },
        {
          "_id": "67c42356054ae6d1c760b645",
          "name": "Gianmarco Mengaldo",
          "hidden": false
        },
        {
          "_id": "67c42356054ae6d1c760b646",
          "name": "Christian Lovis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T21:40:30.000Z",
      "title": "Tell me why: Visual foundation models as self-explainable classifiers",
      "summary": "Visual foundation models (VFMs) have become increasingly popular due to their\nstate-of-the-art performance. However, interpretability remains crucial for\ncritical applications. In this sense, self-explainable models (SEM) aim to\nprovide interpretable classifiers that decompose predictions into a weighted\nsum of interpretable concepts. Despite their promise, recent studies have shown\nthat these explanations often lack faithfulness. In this work, we combine VFMs\nwith a novel prototypical architecture and specialized training objectives. By\ntraining only a lightweight head (approximately 1M parameters) on top of frozen\nVFMs, our approach (ProtoFM) offers an efficient and interpretable solution.\nEvaluations demonstrate that our approach achieves competitive classification\nperformance while outperforming existing models across a range of\ninterpretability metrics derived from the literature. Code is available at\nhttps://github.com/hturbe/proto-fm.",
      "upvotes": 0,
      "discussionId": "67c4235c054ae6d1c760b806"
    },
    "publishedAt": "2025-03-03T04:21:42.563Z",
    "title": "Tell me why: Visual foundation models as self-explainable classifiers",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66588b6fd22637bfab498709/4VG_eDtZKZ4kj1AdG_P14.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19577.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66588b6fd22637bfab498709",
      "avatarUrl": "/avatars/9007f0d3b078bd6193912a5359107f24.svg",
      "fullname": "Hugues Turbé",
      "name": "hturbe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]