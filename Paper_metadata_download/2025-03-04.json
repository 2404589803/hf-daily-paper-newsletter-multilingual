[
  {
    "paper": {
      "id": "2503.01785",
      "authors": [
        {
          "_id": "67c6816614a1bf9855188b8b",
          "user": {
            "_id": "66fe1334ff3ee1f7569fab6d",
            "avatarUrl": "/avatars/6868b1a545028a9b8bbded52490dc093.svg",
            "isPro": false,
            "fullname": "ziyuliu",
            "user": "ziyuliu",
            "type": "user"
          },
          "name": "Ziyu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:57.481Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b8c",
          "user": {
            "_id": "63fda3fced9eead590ff6918",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
            "isPro": false,
            "fullname": "Zeyi Sun",
            "user": "Zery",
            "type": "user"
          },
          "name": "Zeyi Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:35:03.275Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b8d",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:32.723Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b8e",
          "user": {
            "_id": "67c0849ee08c178ef8d4e05c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mQ6VdnjZnRhb0H_waPclo.png",
            "isPro": false,
            "fullname": "Xiaoyi Dong",
            "user": "sweetFruit",
            "type": "user"
          },
          "name": "Xiaoyi Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:25.627Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b8f",
          "user": {
            "_id": "65000bef18830fabea469fdd",
            "avatarUrl": "/avatars/b320c77dfad039d9f9c54127f610d44f.svg",
            "isPro": false,
            "fullname": "Cao Yuhang",
            "user": "yhcao",
            "type": "user"
          },
          "name": "Yuhang Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:19.177Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b90",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:05.281Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b91",
          "user": {
            "_id": "636317ed80c1a705a6eff396",
            "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
            "isPro": false,
            "fullname": "Dahua Lin",
            "user": "lindahua",
            "type": "user"
          },
          "name": "Dahua Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:11:57.087Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b92",
          "user": {
            "_id": "64638c4d51fa6e63060521b5",
            "avatarUrl": "/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg",
            "isPro": false,
            "fullname": "JIaqi",
            "user": "Jiaqiwang",
            "type": "user"
          },
          "name": "Jiaqi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:11:48.889Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T18:16:32.000Z",
      "title": "Visual-RFT: Visual Reinforcement Fine-Tuning",
      "summary": "Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1\nlearns from feedback on its answers, which is especially useful in applications\nwhen fine-tuning data is scarce. Recent open-source work like DeepSeek-R1\ndemonstrates that reinforcement learning with verifiable reward is one key\ndirection in reproducing o1. While the R1-style model has demonstrated success\nin language models, its application in multi-modal domains remains\nunder-explored. This work introduces Visual Reinforcement Fine-Tuning\n(Visual-RFT), which further extends the application areas of RFT on visual\ntasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs)\nto generate multiple responses containing reasoning tokens and final answers\nfor each input, and then uses our proposed visual perception verifiable reward\nfunctions to update the model via the policy optimization algorithm such as\nGroup Relative Policy Optimization (GRPO). We design different verifiable\nreward functions for different perception tasks, such as the Intersection over\nUnion (IoU) reward for object detection. Experimental results on fine-grained\nimage classification, few-shot object detection, reasoning grounding, as well\nas open-vocabulary object detection benchmarks show the competitive performance\nand advanced generalization ability of Visual-RFT compared with Supervised\nFine-tuning (SFT). For example, Visual-RFT improves accuracy by 24.3% over\nthe baseline in one-shot fine-grained image classification with around 100\nsamples. In few-shot object detection, Visual-RFT also exceeds the baseline by\n21.9 on COCO's two-shot setting and 15.4 on LVIS. Our Visual-RFT represents\na paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven\napproach that enhances reasoning and adaptability for domain-specific tasks.",
      "upvotes": 31,
      "discussionId": "67c6816c14a1bf9855188d8c",
      "projectPage": "https://github.com/Liuziyu77/Visual-RFT",
      "githubRepo": "https://github.com/Liuziyu77/Visual-RFT"
    },
    "publishedAt": "2025-03-03T23:29:27.952Z",
    "title": "Visual-RFT: Visual Reinforcement Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01785.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fda3fced9eead590ff6918",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
      "fullname": "Zeyi Sun",
      "name": "Zery",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01774",
      "authors": [
        {
          "_id": "67c694febdab31ec59fea175",
          "user": {
            "_id": "633aaf695df91da9cea92960",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633aaf695df91da9cea92960/9T4y1ru5wt5iKUUqf9_Tt.png",
            "isPro": false,
            "fullname": "Jay Wu",
            "user": "jayw",
            "type": "user"
          },
          "name": "Jay Zhangjie Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:53.874Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea176",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea177",
          "user": {
            "_id": "656e000253703dd78fd072a9",
            "avatarUrl": "/avatars/6702ba8fabe3d08884aa757f90cea333.svg",
            "isPro": false,
            "fullname": "Haithem Turki",
            "user": "hturki",
            "type": "user"
          },
          "name": "Haithem Turki",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:13:26.878Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea178",
          "user": {
            "_id": "658529d61c461dfe88afe8e8",
            "avatarUrl": "/avatars/a22c1b07d28c2662833c462c6537d835.svg",
            "isPro": false,
            "fullname": "Xuanchi Ren",
            "user": "xrenaa",
            "type": "user"
          },
          "name": "Xuanchi Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:13:33.467Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea179",
          "name": "Jun Gao",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea17a",
          "user": {
            "_id": "661ab3da2b14565c7acccf5c",
            "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
            "isPro": false,
            "fullname": "Mike Zheng Shou",
            "user": "AnalMom",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:27:21.825Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea17b",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea17c",
          "user": {
            "_id": "6366cda3361a96184dc22139",
            "avatarUrl": "/avatars/d8a88c84cb5f69e69dd038674a29be89.svg",
            "isPro": false,
            "fullname": "Zan Gojcic",
            "user": "zgojcic",
            "type": "user"
          },
          "name": "Zan Gojcic",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:27:34.034Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea17d",
          "name": "Huan Ling",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T17:58:33.000Z",
      "title": "Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models",
      "summary": "Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D\nreconstruction and novel-view synthesis task. However, achieving photorealistic\nrendering from extreme novel viewpoints remains challenging, as artifacts\npersist across representations. In this work, we introduce Difix3D+, a novel\npipeline designed to enhance 3D reconstruction and novel-view synthesis through\nsingle-step diffusion models. At the core of our approach is Difix, a\nsingle-step image diffusion model trained to enhance and remove artifacts in\nrendered novel views caused by underconstrained regions of the 3D\nrepresentation. Difix serves two critical roles in our pipeline. First, it is\nused during the reconstruction phase to clean up pseudo-training views that are\nrendered from the reconstruction and then distilled back into 3D. This greatly\nenhances underconstrained regions and improves the overall 3D representation\nquality. More importantly, Difix also acts as a neural enhancer during\ninference, effectively removing residual artifacts arising from imperfect 3D\nsupervision and the limited capacity of current reconstruction models. Difix3D+\nis a general solution, a single model compatible with both NeRF and 3DGS\nrepresentations, and it achieves an average 2times improvement in FID score\nover baselines while maintaining 3D consistency.",
      "upvotes": 26,
      "discussionId": "67c69500bdab31ec59fea24d",
      "projectPage": "https://research.nvidia.com/labs/toronto-ai/difix3d"
    },
    "publishedAt": "2025-03-04T00:52:22.204Z",
    "title": "Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01774.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633aaf695df91da9cea92960",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633aaf695df91da9cea92960/9T4y1ru5wt5iKUUqf9_Tt.png",
      "fullname": "Jay Wu",
      "name": "jayw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01743",
      "authors": [
        {
          "_id": "67c67d0dfe135a5f482599bb",
          "name": "Abdelrahman Abouelenin",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599bc",
          "user": {
            "_id": "669ed17498ba26df962584f5",
            "avatarUrl": "/avatars/996c9cf05a4f8e5447552220085157c7.svg",
            "isPro": false,
            "fullname": "Atabak Ashfaq",
            "user": "atabakashfaqMSFT",
            "type": "user"
          },
          "name": "Atabak Ashfaq",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:45:15.511Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599bd",
          "name": "Adam Atkinson",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599be",
          "name": "Hany Awadalla",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599bf",
          "name": "Nguyen Bach",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c0",
          "user": {
            "_id": "6481e690f9ed842838a2b106",
            "avatarUrl": "/avatars/e89a3c8366df504a95dc08a1a412bf3d.svg",
            "isPro": false,
            "fullname": "Jianmin Bao",
            "user": "jianmin-ustc",
            "type": "user"
          },
          "name": "Jianmin Bao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:46:34.578Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c1",
          "user": {
            "_id": "65b9b627e7c838136275a681",
            "avatarUrl": "/avatars/22423f3d9a6c4ee34cad3b0894d27d23.svg",
            "isPro": false,
            "fullname": "Alon Benhaim",
            "user": "alonbenhaim",
            "type": "user"
          },
          "name": "Alon Benhaim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:46:41.117Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c2",
          "user": {
            "_id": "66f81b5b3c7ffa7931b4829a",
            "avatarUrl": "/avatars/a7f34e8e3fd92fdb96affc367b522fbe.svg",
            "isPro": false,
            "fullname": "cai",
            "user": "martincai",
            "type": "user"
          },
          "name": "Martin Cai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:46:47.556Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c3",
          "user": {
            "_id": "659c7ac977ac6f1bf5e63d7e",
            "avatarUrl": "/avatars/86a6efde0d483564a67ed5f344d479a0.svg",
            "isPro": false,
            "fullname": "Vishrav Chaudhary",
            "user": "vishravmsft",
            "type": "user"
          },
          "name": "Vishrav Chaudhary",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:46:56.428Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c4",
          "user": {
            "_id": "66c7a93b92e9f5b19f7533ab",
            "avatarUrl": "/avatars/e26ebf5cf083a3ec09fce24026ecc76e.svg",
            "isPro": false,
            "fullname": "Chen",
            "user": "congcongchen",
            "type": "user"
          },
          "name": "Congcong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:04.205Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c5",
          "user": {
            "_id": "666470a28f5513b0cf11e850",
            "avatarUrl": "/avatars/7beea758882677ad32a12ce56d4d084a.svg",
            "isPro": false,
            "fullname": "Dong Chen",
            "user": "DongChen06",
            "type": "user"
          },
          "name": "Dong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:11.865Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c6",
          "user": {
            "_id": "6567651c6fcc82e5e8c36d4d",
            "avatarUrl": "/avatars/ba3cc037a7688c4f8d967fc6043e540d.svg",
            "isPro": false,
            "fullname": "Dongdong Chen",
            "user": "dongdongchen",
            "type": "user"
          },
          "name": "Dongdong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:18.197Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c7",
          "user": {
            "_id": "669db44d61278f96d8c608a4",
            "avatarUrl": "/avatars/92a493da10c086af5f2af680f4e2c6c6.svg",
            "isPro": false,
            "fullname": "Junkun Chen",
            "user": "shtpgshus",
            "type": "user"
          },
          "name": "Junkun Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:43.236Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c8",
          "user": {
            "_id": "64da876370446182be5b608d",
            "avatarUrl": "/avatars/e412fdc71404ecdf638e416846e3ebfb.svg",
            "isPro": false,
            "fullname": "Weizhu Chen",
            "user": "chenweizhu",
            "type": "user"
          },
          "name": "Weizhu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:51.832Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c9",
          "user": {
            "_id": "662d6b09a47b4da4b23c8b2a",
            "avatarUrl": "/avatars/6770b1d7e25b2cdce04f9904b543d122.svg",
            "isPro": false,
            "fullname": "Yen-Chun Chen",
            "user": "Yen-ChunChen",
            "type": "user"
          },
          "name": "Yen-Chun Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:58.051Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ca",
          "name": "Yi-ling Chen",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599cb",
          "name": "Qi Dai",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599cc",
          "name": "Xiyang Dai",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599cd",
          "user": {
            "_id": "64a8b800b35f48e37dfd20fe",
            "avatarUrl": "/avatars/1e66be9a5238ce86df8b54150520bcc8.svg",
            "isPro": false,
            "fullname": "Ruchao Fan",
            "user": "fanruchao",
            "type": "user"
          },
          "name": "Ruchao Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:40:17.936Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ce",
          "name": "Mei Gao",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599cf",
          "name": "Min Gao",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d0",
          "name": "Amit Garg",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d1",
          "user": {
            "_id": "62cdae333529c21a2283a0a1",
            "avatarUrl": "/avatars/cafc2821e522bbd06d49830e36a073e3.svg",
            "isPro": false,
            "fullname": "Abhishek GOSWAMI",
            "user": "abgoswam",
            "type": "user"
          },
          "name": "Abhishek Goswami",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:49:02.466Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d2",
          "user": {
            "_id": "5f04c4394ec31d33a72116d6",
            "avatarUrl": "/avatars/75d4b9020070e73604b12e5adc1c8201.svg",
            "isPro": false,
            "fullname": "Junheng Hao",
            "user": "jeffhao",
            "type": "user"
          },
          "name": "Junheng Hao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:53:16.356Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d3",
          "user": {
            "_id": "660480db07619487a3718a16",
            "avatarUrl": "/avatars/9c08d541913e57fd79988ef93d5095d4.svg",
            "isPro": false,
            "fullname": "Amr Hendy",
            "user": "amrhendy",
            "type": "user"
          },
          "name": "Amr Hendy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:53:24.716Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d4",
          "name": "Yuxuan Hu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d5",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d6",
          "user": {
            "_id": "6440905e27dc46cca590994c",
            "avatarUrl": "/avatars/0346f8ad17038fba87649a0fc59d64ab.svg",
            "isPro": false,
            "fullname": "Mahmoud Khademi",
            "user": "mkhademi",
            "type": "user"
          },
          "name": "Mahmoud Khademi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:53:53.225Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d7",
          "user": {
            "_id": "662476aec8920ec351b8d3d8",
            "avatarUrl": "/avatars/791e40f53073563680ef18f75b3ea95e.svg",
            "isPro": false,
            "fullname": "Dongwoo Kim",
            "user": "dongwookim-ms",
            "type": "user"
          },
          "name": "Dongwoo Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:54:04.257Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d8",
          "user": {
            "_id": "63f5173bb51da4d61da6c038",
            "avatarUrl": "/avatars/0ee530cf80476aa3985c4d591cd384a1.svg",
            "isPro": false,
            "fullname": "Young Jin Kim",
            "user": "ykim362",
            "type": "user"
          },
          "name": "Young Jin Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:40:19.902Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d9",
          "name": "Gina Lee",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599da",
          "user": {
            "_id": "64004b72330a45b03604303b",
            "avatarUrl": "/avatars/a1fa3fc700173238d0336258b000d934.svg",
            "isPro": false,
            "fullname": "Jinyu Li",
            "user": "FallTraveler",
            "type": "user"
          },
          "name": "Jinyu Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:54:17.115Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599db",
          "name": "Yunsheng Li",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599dc",
          "name": "Chen Liang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599dd",
          "user": {
            "_id": "6464f05e5cdb9ab50f846c98",
            "avatarUrl": "/avatars/3cb2f60a909b59289209ecc7ba75a338.svg",
            "isPro": false,
            "fullname": "Xihui Lin",
            "user": "linxihui",
            "type": "user"
          },
          "name": "Xihui Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:56:29.024Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599de",
          "user": {
            "_id": "62c3a0caf5e2eb44f51de87d",
            "avatarUrl": "/avatars/3c535c5488476b75443666176fcb4c9b.svg",
            "isPro": false,
            "fullname": "Zeqi Lin",
            "user": "linzeqi",
            "type": "user"
          },
          "name": "Zeqi Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:56:38.534Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599df",
          "name": "Mengchen Liu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e0",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e1",
          "user": {
            "_id": "60c790f1accf7da31ed8240d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60c790f1accf7da31ed8240d/YDohCmgf9OUeWqZIs3Thh.jpeg",
            "isPro": false,
            "fullname": "Gilsinia Lopez",
            "user": "lgg",
            "type": "user"
          },
          "name": "Gilsinia Lopez",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:55.169Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e2",
          "name": "Chong Luo",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e3",
          "user": {
            "_id": "66269a329014ef4d10f55d9d",
            "avatarUrl": "/avatars/d4866c32419a7dd07e9aa0660f4bafa9.svg",
            "isPro": false,
            "fullname": "Piyush Madan",
            "user": "PiyushMadan",
            "type": "user"
          },
          "name": "Piyush Madan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:02:38.019Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e4",
          "user": {
            "_id": "65301591944086d1d5fcf656",
            "avatarUrl": "/avatars/250a2e898a4fcbe78feaf6e812851bd6.svg",
            "isPro": false,
            "fullname": "Vadim Mazalovskii",
            "user": "JakeRiley",
            "type": "user"
          },
          "name": "Vadim Mazalov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:02:47.430Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e5",
          "name": "Ali Mousavi",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e6",
          "user": {
            "_id": "649bc84833486cdd77c01c66",
            "avatarUrl": "/avatars/36f4e4bb15c337c4391bfbd234051f4c.svg",
            "isPro": false,
            "fullname": "Nguyen Anh",
            "user": "Anhnguyen",
            "type": "user"
          },
          "name": "Anh Nguyen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:57:52.311Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e7",
          "name": "Jing Pan",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e8",
          "user": {
            "_id": "673b7f70cdc852f69bebfed1",
            "avatarUrl": "/avatars/1efad61a42b948c750c96472a6192de5.svg",
            "isPro": false,
            "fullname": "Daniel Perez-Becker",
            "user": "perezbecker",
            "type": "user"
          },
          "name": "Daniel Perez-Becker",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:09.929Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e9",
          "name": "Jacob Platin",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ea",
          "user": {
            "_id": "65c52dad286bf45e79491697",
            "avatarUrl": "/avatars/01ebc7979273df6e53971ae9835b503f.svg",
            "isPro": false,
            "fullname": "Thomas Portet",
            "user": "thopo",
            "type": "user"
          },
          "name": "Thomas Portet",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:39.865Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599eb",
          "name": "Kai Qiu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ec",
          "user": {
            "_id": "668dcf92835bf7e64bbca904",
            "avatarUrl": "/avatars/416eb3a3c5318a6a45aad87012296470.svg",
            "isPro": false,
            "fullname": "Bo Ren",
            "user": "rosrad",
            "type": "user"
          },
          "name": "Bo Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:40:15.919Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ed",
          "user": {
            "_id": "63815eff4761ddfa00903762",
            "avatarUrl": "/avatars/3419b239d42e091586f1c51b526d88e5.svg",
            "isPro": false,
            "fullname": "Liliang Ren",
            "user": "renll",
            "type": "user"
          },
          "name": "Liliang Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:57:37.996Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ee",
          "name": "Sambuddha Roy",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ef",
          "name": "Ning Shang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f0",
          "user": {
            "_id": "6454c337a13edf669cd5d8ea",
            "avatarUrl": "/avatars/a383a0dda7c2ef6a0d6c3c64651f42ff.svg",
            "isPro": false,
            "fullname": "Yelong Shen",
            "user": "uuu6",
            "type": "user"
          },
          "name": "Yelong Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:00:05.457Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f1",
          "user": {
            "_id": "62743aec8cb70eed79073bc0",
            "avatarUrl": "/avatars/3c8b9a91d898f616265f823ab7d432df.svg",
            "isPro": false,
            "fullname": "Saksham Singhal",
            "user": "sakshamsinghal",
            "type": "user"
          },
          "name": "Saksham Singhal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:03.188Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f2",
          "user": {
            "_id": "678bc6b432ee4968eca9bb6a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/wT-Xa3TYem_EzkZZMyDG0.png",
            "isPro": false,
            "fullname": "Subhojit Som",
            "user": "susom",
            "type": "user"
          },
          "name": "Subhojit Som",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:47.241Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f3",
          "name": "Xia Song",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f4",
          "user": {
            "_id": "64692ad25d701566394fd8da",
            "avatarUrl": "/avatars/d6811ccceb14788bfa0aa10fe4ee1054.svg",
            "isPro": false,
            "fullname": "Tetyana Sych",
            "user": "tesych",
            "type": "user"
          },
          "name": "Tetyana Sych",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:58:27.814Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f5",
          "name": "Praneetha Vaddamanu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f6",
          "name": "Shuohang Wang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f7",
          "name": "Yiming Wang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f8",
          "name": "Zhenghao Wang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f9",
          "name": "Haibin Wu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fa",
          "user": {
            "_id": "61384b860317b0a5c10877d3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1631080954171-61384b860317b0a5c10877d3.jpeg",
            "isPro": false,
            "fullname": "Haoran Xu",
            "user": "haoranxu",
            "type": "user"
          },
          "name": "Haoran Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:56:04.939Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fb",
          "user": {
            "_id": "6398f4b32c20654083f36cde",
            "avatarUrl": "/avatars/4591f514483890997c55e9e6d60bbb0f.svg",
            "isPro": false,
            "fullname": "Weijian Xu",
            "user": "xwjabc",
            "type": "user"
          },
          "name": "Weijian Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:58:36.082Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fc",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fd",
          "name": "Ziyi Yang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fe",
          "user": {
            "_id": "65b01b8a29ae836e9ed5af24",
            "avatarUrl": "/avatars/a8b78a4b54d3f10858c5925521357001.svg",
            "isPro": false,
            "fullname": "Donghan Yu",
            "user": "donghanyu",
            "type": "user"
          },
          "name": "Donghan Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:55:41.798Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ff",
          "name": "Ishmam Zabir",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f48259a00",
          "user": {
            "_id": "63601ee38fb9c2420ffbe45d",
            "avatarUrl": "/avatars/56af091aaff1b42dcfbae84a6ee1e7f7.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Jianwen",
            "type": "user"
          },
          "name": "Jianwen Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:55:12.465Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f48259a01",
          "user": {
            "_id": "62b0009c72043b05d29492b2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
            "isPro": false,
            "fullname": "Li Lyna Zhang",
            "user": "lynazhang",
            "type": "user"
          },
          "name": "Li Lyna Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:55:01.540Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f48259a02",
          "name": "Yunan Zhang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f48259a03",
          "user": {
            "_id": "66ce4c9f864befb39cfc74e9",
            "avatarUrl": "/avatars/ef66398466c470fc1d384c6817d9e461.svg",
            "isPro": false,
            "fullname": "Xiren Zhou",
            "user": "XirenZhou",
            "type": "user"
          },
          "name": "Xiren Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:54:26.629Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T17:05:52.000Z",
      "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language\n  Models via Mixture-of-LoRAs",
      "summary": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable\nlanguage and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language\nmodel trained on high-quality web and synthetic data, significantly\noutperforming recent open-source models of similar size and matching the\nperformance of models twice its size on math and coding tasks requiring complex\nreasoning. This achievement is driven by a carefully curated synthetic data\nrecipe emphasizing high-quality math and coding datasets. Compared to its\npredecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of\n200K tokens to better support multilingual applications, as well as group query\nattention for more efficient long-sequence generation. Phi-4-Multimodal is a\nmultimodal model that integrates text, vision, and speech/audio input\nmodalities into a single model. Its novel modality extension approach leverages\nLoRA adapters and modality-specific routers to allow multiple inference modes\ncombining various modalities without interference. For example, it now ranks\nfirst in the OpenASR leaderboard to date, although the LoRA component of the\nspeech/audio modality has just 460 million parameters. Phi-4-Multimodal\nsupports scenarios involving (vision + language), (vision + speech), and\n(speech/audio) inputs, outperforming larger vision-language and speech-language\nmodels on a wide range of tasks. Additionally, we experiment to further train\nPhi-4-Mini to enhance its reasoning capabilities. Despite its compact\n3.8-billion-parameter size, this experimental version achieves reasoning\nperformance on par with or surpassing significantly larger models, including\nDeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.",
      "upvotes": 22,
      "discussionId": "67c67d0efe135a5f48259a38",
      "projectPage": "https://huggingface.co/microsoft/Phi-4-multimodal-instruct"
    },
    "publishedAt": "2025-03-03T23:15:05.187Z",
    "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01743.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f5173bb51da4d61da6c038",
      "avatarUrl": "/avatars/0ee530cf80476aa3985c4d591cd384a1.svg",
      "fullname": "Young Jin Kim",
      "name": "ykim362",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01496",
      "authors": [
        {
          "_id": "67c6b05f35198d0f397adc98",
          "user": {
            "_id": "66ea643899af9ac3463639b1",
            "avatarUrl": "/avatars/252d470e761a57834dee3dbc60dfefed.svg",
            "isPro": false,
            "fullname": "Disen Lan",
            "user": "landisen",
            "type": "user"
          },
          "name": "Disen Lan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:46.117Z",
          "hidden": false
        },
        {
          "_id": "67c6b05f35198d0f397adc99",
          "user": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "isPro": false,
            "fullname": "Weigao Sun",
            "user": "weigao266",
            "type": "user"
          },
          "name": "Weigao Sun",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-04T08:10:52.130Z",
          "hidden": false
        },
        {
          "_id": "67c6b05f35198d0f397adc9a",
          "user": {
            "_id": "665dc35752ff9daa9ba5a4ed",
            "avatarUrl": "/avatars/df8b01879d97e599b610fa51414d3a18.svg",
            "isPro": false,
            "fullname": "Hu Jiaxi",
            "user": "Jiaxihu2",
            "type": "user"
          },
          "name": "Jiaxi Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:04:18.982Z",
          "hidden": false
        },
        {
          "_id": "67c6b05f35198d0f397adc9b",
          "user": {
            "_id": "65003e857804f04a163328d9",
            "avatarUrl": "/avatars/fe32150aabfde8d283b38ccebcf6982e.svg",
            "isPro": false,
            "fullname": "Jusen Du",
            "user": "JusenK",
            "type": "user"
          },
          "name": "Jusen Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:04:26.432Z",
          "hidden": false
        },
        {
          "_id": "67c6b05f35198d0f397adc9c",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T13:08:00.000Z",
      "title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures",
      "summary": "Transformers with linear recurrent modeling offer linear-time training and\nconstant-memory inference. Despite their demonstrated efficiency and\nperformance, pretraining such non-standard architectures from scratch remains\ncostly and risky. The linearization of large language models (LLMs) transforms\npretrained standard models into linear recurrent structures, enabling more\nefficient deployment. However, current linearization methods typically\nintroduce additional feature map modules that require extensive fine-tuning and\noverlook the gating mechanisms used in state-of-the-art linear recurrent\nmodels. To address these issues, this paper presents Liger, short for\nLinearizing LLMs to gated recurrent structures. Liger is a novel approach for\nconverting pretrained LLMs into gated linear recurrent models without adding\nextra parameters. It repurposes the pretrained key matrix weights to construct\ndiverse gating mechanisms, facilitating the formation of various gated\nrecurrent structures while avoiding the need to train additional components\nfrom scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA),\nLiger restores the performance of the linearized gated recurrent models to\nmatch that of the original LLMs. Additionally, we introduce Liger Attention, an\nintra-layer hybrid attention mechanism, which significantly recovers 93\\% of\nthe Transformer-based LLM at 0.02\\% pre-training tokens during the\nlinearization process, achieving competitive results across multiple\nbenchmarks, as validated on models ranging from 1B to 8B parameters. Code is\navailable at https://github.com/OpenSparseLLMs/Linearization.",
      "upvotes": 12,
      "discussionId": "67c6b06035198d0f397adcc4"
    },
    "publishedAt": "2025-03-04T02:48:58.261Z",
    "title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01496.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01307",
      "authors": [
        {
          "_id": "67c68adc0457c9f809c22df8",
          "user": {
            "_id": "63e6a880f2e9a8f22c5a1630",
            "avatarUrl": "/avatars/53b57690fe052ce6882bbfc87b11567c.svg",
            "isPro": false,
            "fullname": "Kanishk Gandhi",
            "user": "obiwan96",
            "type": "user"
          },
          "name": "Kanishk Gandhi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:35:01.161Z",
          "hidden": false
        },
        {
          "_id": "67c68adc0457c9f809c22df9",
          "user": {
            "_id": "624f9e3d07bd004fb855f5e9",
            "avatarUrl": "/avatars/86a349cd4053bc0317e27e75a51c69fa.svg",
            "isPro": false,
            "fullname": "Ayush Chakravarthy",
            "user": "ayushchakravarthy",
            "type": "user"
          },
          "name": "Ayush Chakravarthy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:04:44.344Z",
          "hidden": false
        },
        {
          "_id": "67c68adc0457c9f809c22dfa",
          "user": {
            "_id": "6511ee845b7e52b0251fdee9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6511ee845b7e52b0251fdee9/hTIwiIYBGOVnIrxtpri83.png",
            "isPro": false,
            "fullname": "Anikait Singh",
            "user": "Asap7772",
            "type": "user"
          },
          "name": "Anikait Singh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:05:05.759Z",
          "hidden": false
        },
        {
          "_id": "67c68adc0457c9f809c22dfb",
          "user": {
            "_id": "61aa15fd8a9625ebfe284286",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61aa15fd8a9625ebfe284286/KaGzIeijcgcN15JErCqft.jpeg",
            "isPro": false,
            "fullname": "nathan lile",
            "user": "nlile",
            "type": "user"
          },
          "name": "Nathan Lile",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:58.582Z",
          "hidden": false
        },
        {
          "_id": "67c68adc0457c9f809c22dfc",
          "user": {
            "_id": "67321274c1f20c742bcf7a8d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ltcQhre6eDRVzn6Vbbyhu.png",
            "isPro": false,
            "fullname": "Noah D. Goodman",
            "user": "ngoodman",
            "type": "user"
          },
          "name": "Noah D. Goodman",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:05:12.186Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T08:46:22.000Z",
      "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four\n  Habits of Highly Effective STaRs",
      "summary": "Test-time inference has emerged as a powerful paradigm for enabling language\nmodels to ``think'' longer and more carefully about complex challenges, much\nlike skilled human experts. While reinforcement learning (RL) can drive\nself-improvement in language models on verifiable tasks, some models exhibit\nsubstantial gains while others quickly plateau. For instance, we find that\nQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game\nof Countdown. This discrepancy raises a critical question: what intrinsic\nproperties enable effective self-improvement? We introduce a framework to\ninvestigate this question by analyzing four key cognitive behaviors --\nverification, backtracking, subgoal setting, and backward chaining -- that both\nexpert human problem solvers and successful language models employ. Our study\nreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama\ninitially lacks them. In systematic experimentation with controlled behavioral\ndatasets, we find that priming Llama with examples containing these reasoning\nbehaviors enables substantial improvements during RL, matching or exceeding\nQwen's performance. Importantly, the presence of reasoning behaviors, rather\nthan correctness of answers, proves to be the critical factor -- models primed\nwith incorrect solutions containing proper reasoning patterns achieve\ncomparable performance to those trained on correct solutions. Finally,\nleveraging continued pretraining with OpenWebMath data, filtered to amplify\nreasoning behaviors, enables the Llama model to match Qwen's self-improvement\ntrajectory. Our findings establish a fundamental relationship between initial\nreasoning behaviors and the capacity for improvement, explaining why some\nlanguage models effectively utilize additional computation while others\nplateau.",
      "upvotes": 9,
      "discussionId": "67c68add0457c9f809c22e31"
    },
    "publishedAt": "2025-03-04T00:09:04.418Z",
    "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01307.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e6a880f2e9a8f22c5a1630",
      "avatarUrl": "/avatars/53b57690fe052ce6882bbfc87b11567c.svg",
      "fullname": "Kanishk Gandhi",
      "name": "obiwan96",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.00714",
      "authors": [
        {
          "_id": "67c6a803025b72f14ccb0939",
          "user": {
            "_id": "6577437552f02732a463d97d",
            "avatarUrl": "/avatars/8eb271ec249fa9b0d97dfe0eace6da88.svg",
            "isPro": false,
            "fullname": "Haoyu Li",
            "user": "Haoyu0529",
            "type": "user"
          },
          "name": "Haoyu Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-04T07:13:08.306Z",
          "hidden": false
        },
        {
          "_id": "67c6a803025b72f14ccb093a",
          "name": "Srikanth Kandula",
          "hidden": false
        },
        {
          "_id": "67c6a803025b72f14ccb093b",
          "name": "Maria Angels de Luis Balaguer",
          "hidden": false
        },
        {
          "_id": "67c6a803025b72f14ccb093c",
          "name": "Aditya Akella",
          "hidden": false
        },
        {
          "_id": "67c6a803025b72f14ccb093d",
          "name": "Venkat Arun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T03:44:31.000Z",
      "title": "Speculative Ad-hoc Querying",
      "summary": "Analyzing large datasets requires responsive query execution, but executing\nSQL queries on massive datasets can be slow. This paper explores whether query\nexecution can begin even before the user has finished typing, allowing results\nto appear almost instantly. We propose SpeQL, a system that leverages Large\nLanguage Models (LLMs) to predict likely queries based on the database schema,\nthe user's past queries, and their incomplete query. Since exact query\nprediction is infeasible, SpeQL speculates on partial queries in two ways: 1)\nit predicts the query structure to compile and plan queries in advance, and 2)\nit precomputes smaller temporary tables that are much smaller than the original\ndatabase, but are still predicted to contain all information necessary to\nanswer the user's final query. Additionally, SpeQL continuously displays\nresults for speculated queries and subqueries in real time, aiding exploratory\nanalysis. A utility/user study showed that SpeQL improved task completion time,\nand participants reported that its speculative display of results helped them\ndiscover patterns in the data more quickly. In the study, SpeQL improves user's\nquery latency by up to 289times and kept the overhead reasonable, at 4$\nper hour.",
      "upvotes": 8,
      "discussionId": "67c6a804025b72f14ccb0994",
      "projectPage": "https://github.com/lihy0529/SpeQL",
      "githubRepo": "https://github.com/lihy0529/SpeQL"
    },
    "publishedAt": "2025-03-04T02:21:00.460Z",
    "title": "Speculative Ad-hoc Querying",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6577437552f02732a463d97d/fEkQ4BZ8Yx_CzsjvHBWFq.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00714.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6577437552f02732a463d97d",
      "avatarUrl": "/avatars/8eb271ec249fa9b0d97dfe0eace6da88.svg",
      "fullname": "Haoyu Li",
      "name": "Haoyu0529",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.00784",
      "authors": [
        {
          "_id": "67c673bcf47209364f0cec96",
          "name": "Kai Lv",
          "hidden": false
        },
        {
          "_id": "67c673bcf47209364f0cec97",
          "name": "Honglin Guo",
          "hidden": false
        },
        {
          "_id": "67c673bcf47209364f0cec98",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "67c673bcf47209364f0cec99",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T08:27:48.000Z",
      "title": "DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with\n  Dynamic Multi-Sequence Drafting",
      "summary": "Large language models (LLMs) exhibit exceptional performance across a wide\nrange of tasks; however, their token-by-token autoregressive generation process\nsignificantly hinders inference speed. Speculative decoding presents a\npromising draft-then-verify framework that reduces generation latency while\nmaintaining output distribution fidelity. Nevertheless, the draft model\nintroduces additional computational overhead, becoming a performance bottleneck\nand increasing the time to first token (TTFT). Previous approaches to mitigate\ndraft model overhead have primarily relied on heuristics and generally failed\nto match the quality of the draft language models. To address these challenges,\nwe propose DuoDecoding, a novel approach that strategically deploys the draft\nand target models on the CPU and GPU respectively, enabling parallel decoding\nwhile preserving draft quality. Our method incorporates a hardware-aware\noptimal draft budget to minimize idle times and employs dynamic multi-sequence\ndrafting to enhance draft quality. Extensive experiments across seven tasks\nshow that DuoDecoding achieves up to 2.61x speedup in generation latency, while\nreducing TTFT to 83% of that in conventional speculative decoding. The Code is\navailable at https://github.com/KaiLv69/DuoDecoding.",
      "upvotes": 7,
      "discussionId": "67c673bdf47209364f0cecb7",
      "githubRepo": "https://github.com/KaiLv69/DuoDecoding"
    },
    "publishedAt": "2025-03-03T22:35:45.299Z",
    "title": "DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00784.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6485d5b300c9cfe5c2470c81",
      "avatarUrl": "/avatars/c29aa81d2add795e8448b99274a04b83.svg",
      "fullname": "Kai",
      "name": "KaiLv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18965",
      "authors": [
        {
          "_id": "67c6bfdf96b9f5fa18c517db",
          "name": "Jiaxin Deng",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517dc",
          "name": "Shiyao Wang",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517dd",
          "name": "Kuo Cai",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517de",
          "name": "Lejian Ren",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517df",
          "name": "Qigen Hu",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517e0",
          "name": "Weifeng Ding",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517e1",
          "name": "Qiang Luo",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517e2",
          "name": "Guorui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T09:25:10.000Z",
      "title": "OneRec: Unifying Retrieve and Rank with Generative Recommender and\n  Iterative Preference Alignment",
      "summary": "Recently, generative retrieval-based recommendation systems have emerged as a\npromising paradigm. However, most modern recommender systems adopt a\nretrieve-and-rank strategy, where the generative model functions only as a\nselector during the retrieval stage. In this paper, we propose OneRec, which\nreplaces the cascaded learning framework with a unified generative model. To\nthe best of our knowledge, this is the first end-to-end generative model that\nsignificantly surpasses current complex and well-designed recommender systems\nin real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder\nstructure, which encodes the user's historical behavior sequences and gradually\ndecodes the videos that the user may be interested in. We adopt sparse\nMixture-of-Experts (MoE) to scale model capacity without proportionally\nincreasing computational FLOPs. 2) a session-wise generation approach. In\ncontrast to traditional next-item prediction, we propose a session-wise\ngeneration, which is more elegant and contextually coherent than point-by-point\ngeneration that relies on hand-crafted rules to properly combine the generated\nresults. 3) an Iterative Preference Alignment module combined with Direct\nPreference Optimization (DPO) to enhance the quality of the generated results.\nUnlike DPO in NLP, a recommendation system typically has only one opportunity\nto display results for each user's browsing request, making it impossible to\nobtain positive and negative samples simultaneously. To address this\nlimitation, We design a reward model to simulate user generation and customize\nthe sampling strategy. Extensive experiments have demonstrated that a limited\nnumber of DPO samples can align user interest preferences and significantly\nimprove the quality of generated results. We deployed OneRec in the main scene\nof Kuaishou, achieving a 1.6\\% increase in watch-time, which is a substantial\nimprovement.",
      "upvotes": 5,
      "discussionId": "67c6bfe396b9f5fa18c518e5"
    },
    "publishedAt": "2025-03-04T03:56:04.503Z",
    "title": "OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18965.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668f5875b5b3081d776e4094",
      "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
      "fullname": "Xiaohuan Zhou",
      "name": "XiaohuanZhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.00501",
      "authors": [
        {
          "_id": "67c6a343ad6b7c2fa29d5e7e",
          "name": "Jia Chen",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e7f",
          "user": {
            "_id": "60c0ed29d8bc072769d78f48",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60c0ed29d8bc072769d78f48/V6q6Tn4kzB46NIbTYw9pQ.jpeg",
            "isPro": false,
            "fullname": "Qian Dong",
            "user": "qian",
            "type": "user"
          },
          "name": "Qian Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:51.762Z",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e80",
          "name": "Haitao Li",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e81",
          "name": "Xiaohui He",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e82",
          "name": "Yan Gao",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e83",
          "name": "Shaosheng Cao",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e84",
          "name": "Yi Wu",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e85",
          "name": "Ping Yang",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e86",
          "name": "Chen Xu",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e87",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e88",
          "name": "Qingyao Ai",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e89",
          "name": "Yiqun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-01T14:15:00.000Z",
      "title": "Qilin: A Multimodal Information Retrieval Dataset with APP-level User\n  Sessions",
      "summary": "User-generated content (UGC) communities, especially those featuring\nmultimodal content, improve user experiences by integrating visual and textual\ninformation into results (or items). The challenge of improving user\nexperiences in complex systems with search and recommendation (S\\&R) services\nhas drawn significant attention from both academia and industry these years.\nHowever, the lack of high-quality datasets has limited the research progress on\nmultimodal S\\&R. To address the growing need for developing better S\\&R\nservices, we present a novel multimodal information retrieval dataset in this\npaper, namely Qilin. The dataset is collected from Xiaohongshu, a popular\nsocial platform with over 300 million monthly active users and an average\nsearch penetration rate of over 70\\%. In contrast to existing datasets,\nQilin offers a comprehensive collection of user sessions with\nheterogeneous results like image-text notes, video notes, commercial notes, and\ndirect answers, facilitating the development of advanced multimodal neural\nretrieval models across diverse task settings. To better model user\nsatisfaction and support the analysis of heterogeneous user behaviors, we also\ncollect extensive APP-level contextual signals and genuine user feedback.\nNotably, Qilin contains user-favored answers and their referred results for\nsearch requests triggering the Deep Query Answering (DQA) module. This allows\nnot only the training \\& evaluation of a Retrieval-augmented Generation (RAG)\npipeline, but also the exploration of how such a module would affect users'\nsearch behavior. Through comprehensive analysis and experiments, we provide\ninteresting findings and insights for further improving S\\&R systems. We hope\nthat Qilin will significantly contribute to the advancement of\nmultimodal content platforms with S\\&R services in the future.",
      "upvotes": 5,
      "discussionId": "67c6a346ad6b7c2fa29d5f88",
      "projectPage": "https://huggingface.co/datasets/THUIR/Qilin",
      "githubRepo": "https://github.com/RED-Search/Qilin/"
    },
    "publishedAt": "2025-03-04T01:56:03.632Z",
    "title": "Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00501.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60c0ed29d8bc072769d78f48",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60c0ed29d8bc072769d78f48/V6q6Tn4kzB46NIbTYw9pQ.jpeg",
      "fullname": "Qian Dong",
      "name": "qian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01370",
      "authors": [
        {
          "_id": "67c691673ff65c55829685a0",
          "name": "Jiantao Lin",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a1",
          "name": "Xin Yang",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a2",
          "name": "Meixi Chen",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a3",
          "name": "Yingjie Xu",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a4",
          "user": {
            "_id": "64049ae20ab5e22719f35103",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg",
            "isPro": false,
            "fullname": "Dongyu Yan",
            "user": "StarYDY",
            "type": "user"
          },
          "name": "Dongyu Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:56.252Z",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a5",
          "name": "Leyi Wu",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a6",
          "name": "Xinli Xu",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a7",
          "name": "Lie XU",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a8",
          "name": "Shunsi Zhang",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a9",
          "name": "Ying-Cong Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:07:19.000Z",
      "title": "Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation",
      "summary": "Diffusion models have achieved great success in generating 2D images.\nHowever, the quality and generalizability of 3D content generation remain\nlimited. State-of-the-art methods often require large-scale 3D assets for\ntraining, which are challenging to collect. In this work, we introduce\nKiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient\nframework for generating, editing, and enhancing 3D objects by repurposing a\nwell-trained 2D image diffusion model for 3D generation. Specifically, we\nfine-tune a diffusion model to generate ''3D Bundle Image'', a tiled\nrepresentation composed of multi-view images and their corresponding normal\nmaps. The normal maps are then used to reconstruct a 3D mesh, and the\nmulti-view images provide texture mapping, resulting in a complete 3D model.\nThis simple method effectively transforms the 3D generation problem into a 2D\nimage generation task, maximizing the utilization of knowledge in pretrained\ndiffusion models. Furthermore, we demonstrate that our Kiss3DGen model is\ncompatible with various diffusion model techniques, enabling advanced features\nsuch as 3D editing, mesh and texture enhancement, etc. Through extensive\nexperiments, we demonstrate the effectiveness of our approach, showcasing its\nability to produce high-quality 3D models efficiently.",
      "upvotes": 4,
      "discussionId": "67c6916b3ff65c5582968702",
      "projectPage": "https://ltt-o.github.io/Kiss3dgen.github.io/",
      "githubRepo": "https://github.com/EnVision-Research/Kiss3DGen"
    },
    "publishedAt": "2025-03-04T01:19:45.715Z",
    "title": "Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6332e2689bf698ce68a22e8c",
      "avatarUrl": "/avatars/c1922acfda2e6d2fe7b03194a404eb10.svg",
      "fullname": "JIANTAO LIN",
      "name": "LTT",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01183",
      "authors": [
        {
          "_id": "67c6a15e21d722b4248bd9c2",
          "name": "Ziqian Ning",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c3",
          "name": "Huakang Chen",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c4",
          "name": "Yuepeng Jiang",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c5",
          "name": "Chunbo Hao",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c6",
          "name": "Guobin Ma",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c7",
          "name": "Shuai Wang",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c8",
          "name": "Jixun Yao",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c9",
          "name": "Lei Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T05:15:34.000Z",
      "title": "DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End\n  Full-Length Song Generation with Latent Diffusion",
      "summary": "Recent advancements in music generation have garnered significant attention,\nyet existing approaches face critical limitations. Some current generative\nmodels can only synthesize either the vocal track or the accompaniment track.\nWhile some models can generate combined vocal and accompaniment, they typically\nrely on meticulously designed multi-stage cascading architectures and intricate\ndata pipelines, hindering scalability. Additionally, most systems are\nrestricted to generating short musical segments rather than full-length songs.\nFurthermore, widely used language model-based methods suffer from slow\ninference speeds. To address these challenges, we propose DiffRhythm, the first\nlatent diffusion-based song generation model capable of synthesizing complete\nsongs with both vocal and accompaniment for durations of up to 4m45s in only\nten seconds, maintaining high musicality and intelligibility. Despite its\nremarkable capabilities, DiffRhythm is designed to be simple and elegant: it\neliminates the need for complex data preparation, employs a straightforward\nmodel structure, and requires only lyrics and a style prompt during inference.\nAdditionally, its non-autoregressive structure ensures fast inference speeds.\nThis simplicity guarantees the scalability of DiffRhythm. Moreover, we release\nthe complete training code along with the pre-trained model on large-scale data\nto promote reproducibility and further research.",
      "upvotes": 3,
      "discussionId": "67c6a16021d722b4248bda37",
      "projectPage": "https://aslp-lab.github.io/DiffRhythm.github.io/",
      "githubRepo": "https://github.com/ASLP-lab/DiffRhythm"
    },
    "publishedAt": "2025-03-04T04:54:04.054Z",
    "title": "DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01183.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "624bebf604abc7ebb01789af",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649143001781-624bebf604abc7ebb01789af.jpeg",
      "fullname": "Apolinário from multimodal AI art",
      "name": "multimodalart",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isMod": false,
      "followerCount": 3862
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18890",
      "authors": [
        {
          "_id": "67c6cbd6e52534aa6ada2e26",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "67c6cbd6e52534aa6ada2e27",
          "name": "Junzhe Shen",
          "hidden": false
        },
        {
          "_id": "67c6cbd6e52534aa6ada2e28",
          "name": "Zixia Jia",
          "hidden": false
        },
        {
          "_id": "67c6cbd6e52534aa6ada2e29",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "67c6cbd6e52534aa6ada2e2a",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-04T09:45:59.571Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T07:10:08.000Z",
      "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens",
      "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
      "upvotes": 2,
      "discussionId": "67c6cbd7e52534aa6ada2e79",
      "githubRepo": "https://github.com/bigai-nlco/TokenSwift"
    },
    "publishedAt": "2025-03-04T04:56:33.061Z",
    "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63a95a6a7930fa8c7dd63d4e/3WZ10b-Ku3GcY1fc1MWx8.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18890.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a95a6a7930fa8c7dd63d4e",
      "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
      "fullname": "Zilong Zheng",
      "name": "zlzheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16779",
      "authors": [
        {
          "_id": "67c65c06e116e361574405e9",
          "user": {
            "_id": "642bdfc65edcc5760cb1ea12",
            "avatarUrl": "/avatars/599b0bbb379b43cd39097c204c946075.svg",
            "isPro": false,
            "fullname": "huang",
            "user": "yxuan",
            "type": "user"
          },
          "name": "Yaxuan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:51:27.582Z",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405ea",
          "name": "Xili Dai",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405eb",
          "name": "Jianan Wang",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405ec",
          "name": "Xianbiao Qi",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405ed",
          "name": "Yixing Yuan",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405ee",
          "name": "Xiangyu Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T02:14:19.000Z",
      "title": "Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain\n  Model",
      "summary": "Room layout estimation from multiple-perspective images is poorly\ninvestigated due to the complexities that emerge from multi-view geometry,\nwhich requires muti-step solutions such as camera intrinsic and extrinsic\nestimation, image matching, and triangulation. However, in 3D reconstruction,\nthe advancement of recent 3D foundation models such as DUSt3R has shifted the\nparadigm from the traditional multi-step structure-from-motion process to an\nend-to-end single-step approach. To this end, we introduce Plane-DUSt3R, a\nnovel method for multi-view room layout estimation leveraging the 3D foundation\nmodel DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes on\na room layout dataset (Structure3D) with a modified objective to estimate\nstructural planes. By generating uniform and parsimonious results, Plane-DUSt3R\nenables room layout estimation with only a single post-processing step and 2D\ndetection results. Unlike previous methods that rely on single-perspective or\npanorama image, Plane-DUSt3R extends the setting to handle multiple-perspective\nimages. Moreover, it offers a streamlined, end-to-end solution that simplifies\nthe process and reduces error accumulation. Experimental results demonstrate\nthat Plane-DUSt3R not only outperforms state-of-the-art methods on the\nsynthetic dataset but also proves robust and effective on in the wild data with\ndifferent image styles such as cartoon.Our code is available at:\nhttps://github.com/justacar/Plane-DUSt3R",
      "upvotes": 2,
      "discussionId": "67c65c0be116e36157440751",
      "githubRepo": "https://github.com/justacar/Plane-DUSt3R"
    },
    "publishedAt": "2025-03-04T04:17:23.806Z",
    "title": "Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16779.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642bdfc65edcc5760cb1ea12",
      "avatarUrl": "/avatars/599b0bbb379b43cd39097c204c946075.svg",
      "fullname": "huang",
      "name": "yxuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01295",
      "authors": [
        {
          "_id": "67c6a8b534aeb86063e94010",
          "user": {
            "_id": "61711f02e0b1ddb56eb9b526",
            "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
            "isPro": false,
            "fullname": "Mingzhe Du",
            "user": "Elfsong",
            "type": "user"
          },
          "name": "Mingzhe Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:49.954Z",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94011",
          "name": "Anh Tuan Luu",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94012",
          "name": "Bin Ji",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94013",
          "name": "Xiaobao Wu",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94014",
          "name": "Dong Huang",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94015",
          "name": "Terry Yue Zhuo",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94016",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94017",
          "name": "See-Kiong Ng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T08:31:16.000Z",
      "title": "CodeArena: A Collective Evaluation Platform for LLM Code Generation",
      "summary": "Large Language Models (LLMs) have reshaped code generation by synergizing\ntheir exceptional comprehension of natural language and programming syntax,\nthereby substantially boosting developer productivity. These advancements have\nprompted numerous efforts to quantitatively evaluate their coding capabilities.\nHowever, persistent challenges, such as benchmark leakage, data dissipation,\nand limited system accessibility, continue to impede a timely and accurate\nassessment. To address these limitations, we introduce CodeArena, an online\nevaluation framework tailored for LLM code generation. The key innovation is a\ncollective evaluation mechanism, which dynamically recalibrates individual\nmodel scores based on the holistic performance of all participating models,\nmitigating score biases caused by widespread benchmark leakage. In addition,\nCodeArena ensures open access to all submitted solutions and test cases and\nprovides automation-friendly APIs to streamline the code evaluation workflow.\nOur main contributions are: (1) a collective evaluation system for unbiased\nassessment, (2) a public repository of solutions and test cases, and (3)\nautomation-ready APIs for seamless integration.",
      "upvotes": 2,
      "discussionId": "67c6a8b634aeb86063e9406a"
    },
    "publishedAt": "2025-03-04T02:16:25.633Z",
    "title": "CodeArena: A Collective Evaluation Platform for LLM Code Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61711f02e0b1ddb56eb9b526",
      "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
      "fullname": "Mingzhe Du",
      "name": "Elfsong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01739",
      "authors": [
        {
          "_id": "67c68f7828a037872c5ce5bb",
          "name": "Wenhao Wang",
          "hidden": false
        },
        {
          "_id": "67c68f7828a037872c5ce5bc",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T17:00:36.000Z",
      "title": "VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video\n  Generation",
      "summary": "Text-to-video generative models convert textual prompts into dynamic visual\ncontent, offering wide-ranging applications in film production, gaming, and\neducation. However, their real-world performance often falls short of user\nexpectations. One key reason is that these models have not been trained on\nvideos related to some topics users want to create. In this paper, we propose\nVideoUFO, the first Video dataset specifically curated to align with Users'\nFOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1)\nminimal (0.29%) overlap with existing video datasets, and (2) videos\nsearched exclusively via YouTube's official API under the Creative Commons\nlicense. These two attributes provide future researchers with greater freedom\nto broaden their training sources. The VideoUFO comprises over 1.09 million\nvideo clips, each paired with both a brief and a detailed caption\n(description). Specifically, through clustering, we first identify 1,291\nuser-focused topics from the million-scale real text-to-video prompt dataset,\nVidProM. Then, we use these topics to retrieve videos from YouTube, split the\nretrieved videos into clips, and generate both brief and detailed captions for\neach clip. After verifying the clips with specified topics, we are left with\nabout 1.09 million video clips. Our experiments reveal that (1) current 16\ntext-to-video models do not achieve consistent performance across all\nuser-focused topics; and (2) a simple model trained on VideoUFO outperforms\nothers on worst-performing topics. The dataset is publicly available at\nhttps://huggingface.co/datasets/WenhaoWang/VideoUFO under the CC BY 4.0\nLicense.",
      "upvotes": 2,
      "discussionId": "67c68f7a28a037872c5ce60d"
    },
    "publishedAt": "2025-03-04T00:29:56.570Z",
    "title": "VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01739.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b32a4429a410b7f6b06710",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b32a4429a410b7f6b06710/VzgvmnlYZWuifZTkIkCxy.jpeg",
      "fullname": "Wenhao Wang",
      "name": "WenhaoWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01807",
      "authors": [
        {
          "_id": "67c67ff6dec55d10cb10fc9e",
          "user": {
            "_id": "62608fc2ffe8827cb1d89f9f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png",
            "isPro": false,
            "fullname": "Hamish Ivison",
            "user": "hamishivi",
            "type": "user"
          },
          "name": "Hamish Ivison",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:40:13.649Z",
          "hidden": false
        },
        {
          "_id": "67c67ff6dec55d10cb10fc9f",
          "name": "Muru Zhang",
          "hidden": false
        },
        {
          "_id": "67c67ff6dec55d10cb10fca0",
          "name": "Faeze Brahman",
          "hidden": false
        },
        {
          "_id": "67c67ff6dec55d10cb10fca1",
          "name": "Pang Wei Koh",
          "hidden": false
        },
        {
          "_id": "67c67ff6dec55d10cb10fca2",
          "name": "Pradeep Dasigi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T18:37:26.000Z",
      "title": "Large-Scale Data Selection for Instruction Tuning",
      "summary": "Selecting high-quality training data from a larger pool is a crucial step\nwhen instruction-tuning language models, as carefully curated datasets often\nproduce models that outperform those trained on much larger, noisier datasets.\nAutomated data selection approaches for instruction-tuning are typically tested\nby selecting small datasets (roughly 10k samples) from small pools (100-200k\nsamples). However, popular deployed instruction-tuned models often train on\nhundreds of thousands to millions of samples, subsampled from even larger data\npools. We present a systematic study of how well data selection methods scale\nto these settings, selecting up to 2.5M samples from pools of up to 5.8M\nsamples and evaluating across 7 diverse tasks. We show that many recently\nproposed methods fall short of random selection in this setting (while using\nmore compute), and even decline in performance when given access to larger\npools of data to select over. However, we find that a variant of\nrepresentation-based data selection (RDS+), which uses weighted mean pooling of\npretrained LM hidden states, consistently outperforms more complex methods\nacross all settings tested -- all whilst being more compute-efficient. Our\nfindings highlight that the scaling properties of proposed automated selection\nmethods should be more closely examined. We release our code, data, and models\nat https://github.com/hamishivi/automated-instruction-selection.",
      "upvotes": 2,
      "discussionId": "67c67ff9dec55d10cb10fcef"
    },
    "publishedAt": "2025-03-03T23:44:06.105Z",
    "title": "Large-Scale Data Selection for Instruction Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62608fc2ffe8827cb1d89f9f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png",
      "fullname": "Hamish Ivison",
      "name": "hamishivi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01063",
      "authors": [
        {
          "_id": "67c6b72b7aad9a016ae60797",
          "name": "David Noever",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T23:59:52.000Z",
      "title": "AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond\n  Human Understanding",
      "summary": "This paper investigates the potential for large language models (LLMs) to\ndevelop private tonal languages for machine-to-machine (M2M) communication.\nInspired by cryptophasia in human twins (affecting up to 50% of twin births)\nand natural tonal languages like Mandarin and Vietnamese, we implement a\nprecise character-to-frequency mapping system that encodes the full ASCII\ncharacter set (32-126) using musical semitones. Each character is assigned a\nunique frequency, creating a logarithmic progression beginning with space (220\nHz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves,\nwith higher characters deliberately mapped to ultrasonic frequencies beyond\nhuman perception (>20 kHz). Our implemented software prototype demonstrates\nthis encoding through visualization, auditory playback, and ABC musical\nnotation, allowing for analysis of information density and transmission speed.\nTesting reveals that tonal encoding can achieve information rates exceeding\nhuman speech while operating partially outside human perceptual boundaries.\nThis work responds directly to concerns about AI systems catastrophically\ndeveloping private languages within the next five years, providing a concrete\nprototype software example of how such communication might function and the\ntechnical foundation required for its emergence, detection, and governance.",
      "upvotes": 0,
      "discussionId": "67c6b72c7aad9a016ae607bb"
    },
    "publishedAt": "2025-03-04T03:20:03.380Z",
    "title": "AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond Human Understanding",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63136a82e29fb2e86d5e5bdd/mgIPjnhtUaGLR2Iv4ViL6.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01063.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63136a82e29fb2e86d5e5bdd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63136a82e29fb2e86d5e5bdd/pFZDuQtzfUStovbwwZGvn.png",
      "fullname": "David Noever",
      "name": "dnoever",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.00729",
      "authors": [
        {
          "_id": "67c6ab3ec0b62d612c54ddf5",
          "user": {
            "_id": "6628c6107751d297d7025a71",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628c6107751d297d7025a71/S1rm5VIwV2Uxfv8GetKMU.jpeg",
            "isPro": false,
            "fullname": "Lei Mingcong",
            "user": "SP4595",
            "type": "user"
          },
          "name": "Mingcong Lei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:48.061Z",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddf6",
          "name": "Ge Wang",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddf7",
          "name": "Yiming Zhao",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddf8",
          "name": "Zhixin Mai",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddf9",
          "name": "Qing Zhao",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfa",
          "name": "Yao Guo",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfb",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfc",
          "name": "Shuguang Cui",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfd",
          "name": "Yatong Han",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfe",
          "name": "Jinke Ren",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T04:50:59.000Z",
      "title": "CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic\n  Environments",
      "summary": "Large Language Models (LLMs) exhibit remarkable capabilities in the\nhierarchical decomposition of complex tasks through semantic reasoning.\nHowever, their application in embodied systems faces challenges in ensuring\nreliable execution of subtask sequences and achieving one-shot success in\nlong-term task completion. To address these limitations in dynamic\nenvironments, we propose Closed-Loop Embodied Agent (CLEA) -- a novel\narchitecture incorporating four specialized open-source LLMs with functional\ndecoupling for closed-loop task management. The framework features two core\ninnovations: (1) Interactive task planner that dynamically generates executable\nsubtasks based on the environmental memory, and (2) Multimodal execution critic\nemploying an evaluation framework to conduct a probabilistic assessment of\naction feasibility, triggering hierarchical re-planning mechanisms when\nenvironmental perturbations exceed preset thresholds. To validate CLEA's\neffectiveness, we conduct experiments in a real environment with manipulable\nobjects, using two heterogeneous robots for object search, manipulation, and\nsearch-manipulation integration tasks. Across 12 task trials, CLEA outperforms\nthe baseline model, achieving a 67.3% improvement in success rate and a 52.8%\nincrease in task completion rate. These results demonstrate that CLEA\nsignificantly enhances the robustness of task planning and execution in dynamic\nenvironments.",
      "upvotes": 0,
      "discussionId": "67c6ab42c0b62d612c54df71",
      "projectPage": "https://sp4595.github.io/CLEA/",
      "githubRepo": "https://github.com/SP4595/CLEA-Closed-Loop-Embodied-Agent"
    },
    "publishedAt": "2025-03-04T02:27:17.351Z",
    "title": "CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic Environments",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00729.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6628c6107751d297d7025a71",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628c6107751d297d7025a71/S1rm5VIwV2Uxfv8GetKMU.jpeg",
      "fullname": "Lei Mingcong",
      "name": "SP4595",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]