[
  {
    "paper": {
      "id": "2511.10647",
      "authors": [
        {
          "_id": "6916d5069a1d7af6ca2f01d5",
          "name": "Haotong Lin",
          "hidden": false
        },
        {
          "_id": "6916d5069a1d7af6ca2f01d6",
          "name": "Sili Chen",
          "hidden": false
        },
        {
          "_id": "6916d5069a1d7af6ca2f01d7",
          "name": "Junhao Liew",
          "hidden": false
        },
        {
          "_id": "6916d5069a1d7af6ca2f01d8",
          "name": "Donny Y. Chen",
          "hidden": false
        },
        {
          "_id": "6916d5069a1d7af6ca2f01d9",
          "name": "Zhenyu Li",
          "hidden": false
        },
        {
          "_id": "6916d5069a1d7af6ca2f01da",
          "name": "Guang Shi",
          "hidden": false
        },
        {
          "_id": "6916d5069a1d7af6ca2f01db",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "6916d5069a1d7af6ca2f01dc",
          "name": "Bingyi Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-13T18:59:53.000Z",
      "submittedOnDailyAt": "2025-11-14T06:16:02.269Z",
      "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
      "submittedOnDailyBy": {
        "_id": "63a369d98c0c89dcae3b8329",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
        "isPro": true,
        "fullname": "Adina Yakefu",
        "user": "AdinaY",
        "type": "user"
      },
      "summary": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.",
      "upvotes": 2,
      "discussionId": "6916d5069a1d7af6ca2f01dd",
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-11-13T13:59:53.000Z",
    "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
    "summary": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.10647.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1177
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  }
]