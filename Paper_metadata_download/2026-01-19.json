[
  {
    "paper": {
      "id": "2601.08521",
      "authors": [
        {
          "_id": "69674059c5e371f6b235d1d8",
          "name": "Fengkai Yang",
          "hidden": false
        },
        {
          "_id": "69674059c5e371f6b235d1d9",
          "user": {
            "_id": "6969715fb2636f5f23a9a8c5",
            "avatarUrl": "/avatars/5e75043891ee41bb980f71fb9e3a33ab.svg",
            "isPro": false,
            "fullname": "Zherui Chen",
            "user": "chenzherui007",
            "type": "user"
          },
          "name": "Zherui Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-16T10:33:53.078Z",
          "hidden": false
        },
        {
          "_id": "69674059c5e371f6b235d1da",
          "name": "Xiaohan Wang",
          "hidden": false
        },
        {
          "_id": "69674059c5e371f6b235d1db",
          "name": "Xiaodong Lu",
          "hidden": false
        },
        {
          "_id": "69674059c5e371f6b235d1dc",
          "name": "Jiajun Chai",
          "hidden": false
        },
        {
          "_id": "69674059c5e371f6b235d1dd",
          "name": "Guojun Yin",
          "hidden": false
        },
        {
          "_id": "69674059c5e371f6b235d1de",
          "name": "Wei Lin",
          "hidden": false
        },
        {
          "_id": "69674059c5e371f6b235d1df",
          "name": "Shuai Ma",
          "hidden": false
        },
        {
          "_id": "69674059c5e371f6b235d1e0",
          "name": "Fuzhen Zhuang",
          "hidden": false
        },
        {
          "_id": "69674059c5e371f6b235d1e1",
          "name": "Deqing Wang",
          "hidden": false
        },
        {
          "_id": "69674059c5e371f6b235d1e2",
          "name": "Yaodong Yang",
          "hidden": false
        },
        {
          "_id": "69674059c5e371f6b235d1e3",
          "name": "Jianxin Li",
          "hidden": false
        },
        {
          "_id": "69674059c5e371f6b235d1e4",
          "user": {
            "_id": "68345345f4bbf856e2d708e2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg",
            "isPro": false,
            "fullname": "Yikun Ban",
            "user": "Yikunb",
            "type": "user"
          },
          "name": "Yikun Ban",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-16T10:33:50.655Z",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-13T13:03:15.000Z",
      "submittedOnDailyAt": "2026-01-19T00:20:58.837Z",
      "title": "Your Group-Relative Advantage Is Biased",
      "submittedOnDailyBy": {
        "_id": "68345345f4bbf856e2d708e2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg",
        "isPro": false,
        "fullname": "Yikun Ban",
        "user": "Yikunb",
        "type": "user"
      },
      "summary": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.\n  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.",
      "upvotes": 83,
      "discussionId": "6967405ac5e371f6b235d1e5",
      "ai_summary": "Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method.",
      "ai_keywords": [
        "Reinforcement Learning from Verifier Rewards",
        "group-based methods",
        "GRPO",
        "advantage estimation",
        "bias correction",
        "History-Aware Adaptive Difficulty Weighting",
        "mathematical reasoning benchmarks"
      ]
    },
    "publishedAt": "2026-01-13T08:03:15.000Z",
    "title": "Your Group-Relative Advantage Is Biased",
    "summary": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.\n  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08521.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "68345345f4bbf856e2d708e2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg",
      "fullname": "Yikun Ban",
      "name": "Yikunb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.10355",
      "authors": [
        {
          "_id": "6969a11632f0333869ff9390",
          "user": {
            "_id": "65647e2b50a80d26dbfdf49c",
            "avatarUrl": "/avatars/aff0de9f9e4ed322e05d7f832c3c060d.svg",
            "isPro": false,
            "fullname": "Xu Zhihao",
            "user": "naiweizi",
            "type": "user"
          },
          "name": "Zhihao Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-16T10:30:47.984Z",
          "hidden": false
        },
        {
          "_id": "6969a11632f0333869ff9391",
          "name": "Rumei Li",
          "hidden": false
        },
        {
          "_id": "6969a11632f0333869ff9392",
          "name": "Jiahuan Li",
          "hidden": false
        },
        {
          "_id": "6969a11632f0333869ff9393",
          "name": "Rongxiang Weng",
          "hidden": false
        },
        {
          "_id": "6969a11632f0333869ff9394",
          "name": "Jingang Wang",
          "hidden": false
        },
        {
          "_id": "6969a11632f0333869ff9395",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "6969a11632f0333869ff9396",
          "name": "Xiting Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-15T12:58:46.000Z",
      "submittedOnDailyAt": "2026-01-19T00:30:06.659Z",
      "title": "Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text",
      "submittedOnDailyBy": {
        "_id": "65647e2b50a80d26dbfdf49c",
        "avatarUrl": "/avatars/aff0de9f9e4ed322e05d7f832c3c060d.svg",
        "isPro": false,
        "fullname": "Xu Zhihao",
        "user": "naiweizi",
        "type": "user"
      },
      "summary": "Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.",
      "upvotes": 26,
      "discussionId": "6969a11632f0333869ff9397",
      "ai_summary": "A text-based data synthesis approach generates multi-turn tool-use trajectories for large language models, achieving improved performance and reduced computational costs through a specialized trajectory synthesizer.",
      "ai_keywords": [
        "large language models",
        "multi-turn interactions",
        "tool-use data",
        "text corpora",
        "data synthesis pipeline",
        "relevance filtering",
        "workflow extraction",
        "trajectory grounding",
        "complexity refinement",
        "supervised fine-tuning",
        "trajectory synthesizer",
        "BFCL V3 Multi-turn benchmark",
        "τ-bench",
        "inference latency"
      ],
      "organization": {
        "_id": "68b28d79a176a9beb30d2049",
        "name": "meituan-longcat",
        "fullname": "LongCat",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
      }
    },
    "publishedAt": "2026-01-15T07:58:46.000Z",
    "title": "Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text",
    "summary": "Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10355.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65647e2b50a80d26dbfdf49c",
      "avatarUrl": "/avatars/aff0de9f9e4ed322e05d7f832c3c060d.svg",
      "fullname": "Xu Zhihao",
      "name": "naiweizi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68b28d79a176a9beb30d2049",
      "name": "meituan-longcat",
      "fullname": "LongCat",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.11496",
      "authors": [
        {
          "_id": "696de89f3f1837bfb8970ab3",
          "name": "Eilam Shapira",
          "hidden": false
        },
        {
          "_id": "696de89f3f1837bfb8970ab4",
          "name": "Roi Reichart",
          "hidden": false
        },
        {
          "_id": "696de89f3f1837bfb8970ab5",
          "name": "Moshe Tennenholtz",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-16T18:18:03.000Z",
      "submittedOnDailyAt": "2026-01-19T06:58:50.740Z",
      "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents",
      "submittedOnDailyBy": {
        "_id": "64802fb6c57f629056c59966",
        "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
        "isPro": false,
        "fullname": "Eilam Shapira",
        "user": "EilamSha",
        "type": "user"
      },
      "summary": "The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the \"Poisoned Apple\" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.",
      "upvotes": 22,
      "discussionId": "696de8a03f1837bfb8970ab6",
      "organization": {
        "_id": "6393322be2364bc1eea56e45",
        "name": "Technion",
        "fullname": "Technion Israel institute of technology",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
      }
    },
    "publishedAt": "2026-01-16T13:18:03.000Z",
    "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents",
    "summary": "The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the \"Poisoned Apple\" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11496.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64802fb6c57f629056c59966",
      "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
      "fullname": "Eilam Shapira",
      "name": "EilamSha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6393322be2364bc1eea56e45",
      "name": "Technion",
      "fullname": "Technion Israel institute of technology",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.08430",
      "authors": [
        {
          "_id": "696b31133f1837bfb8970653",
          "name": "Sunzhu Li",
          "hidden": false
        },
        {
          "_id": "696b31133f1837bfb8970654",
          "name": "Jiale Zhao",
          "hidden": false
        },
        {
          "_id": "696b31133f1837bfb8970655",
          "name": "Miteto Wei",
          "hidden": false
        },
        {
          "_id": "696b31133f1837bfb8970656",
          "name": "Huimin Ren",
          "hidden": false
        },
        {
          "_id": "696b31133f1837bfb8970657",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "696b31133f1837bfb8970658",
          "name": "Jingwen Yang",
          "hidden": false
        },
        {
          "_id": "696b31133f1837bfb8970659",
          "name": "Shunyu Liu",
          "hidden": false
        },
        {
          "_id": "696b31133f1837bfb897065a",
          "name": "Kaike Zhang",
          "hidden": false
        },
        {
          "_id": "696b31133f1837bfb897065b",
          "name": "Wei Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-13T10:56:39.000Z",
      "submittedOnDailyAt": "2026-01-19T00:15:22.448Z",
      "title": "RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation",
      "submittedOnDailyBy": {
        "_id": "67375a6ae6b1d15ff5359a54",
        "avatarUrl": "/avatars/9d32d9e3bfb43b8d001c6ddeae720ec5.svg",
        "isPro": false,
        "fullname": "Zela",
        "user": "vzl123",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale (sim110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.",
      "upvotes": 22,
      "discussionId": "696b31143f1837bfb897065c",
      "projectPage": "https://huggingface.co/datasets/sojuL/RubricHub_v1",
      "githubRepo": "https://github.com/teqkilla/RubricHub",
      "githubRepoAddedBy": "user",
      "ai_summary": "RLVR has advanced reasoning capabilities but struggles with open-ended generation due to lack of ground truth; this work proposes an automated rubric generation framework and dataset to improve performance in health reasoning benchmarks.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "rubric-based evaluation",
        "principle-guided synthesis",
        "multi-model aggregation",
        "difficulty evolution",
        "RubricHub",
        "Rubric-based Rejection Sampling Fine-Tuning",
        "Reinforcement Learning"
      ],
      "githubStars": 13
    },
    "publishedAt": "2026-01-13T05:56:39.000Z",
    "title": "RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale (sim110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08430.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67375a6ae6b1d15ff5359a54",
      "avatarUrl": "/avatars/9d32d9e3bfb43b8d001c6ddeae720ec5.svg",
      "fullname": "Zela",
      "name": "vzl123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.11000",
      "authors": [
        {
          "_id": "696dc5773f1837bfb8970a3b",
          "user": {
            "_id": "6309bfdab8d7b3889319b588",
            "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg",
            "isPro": false,
            "fullname": "SunZX",
            "user": "Jeryi",
            "type": "user"
          },
          "name": "Zhongxiang Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-19T09:22:05.638Z",
          "hidden": false
        },
        {
          "_id": "696dc5773f1837bfb8970a3c",
          "name": "Yi Zhan",
          "hidden": false
        },
        {
          "_id": "696dc5773f1837bfb8970a3d",
          "name": "Chenglei Shen",
          "hidden": false
        },
        {
          "_id": "696dc5773f1837bfb8970a3e",
          "name": "Weijie Yu",
          "hidden": false
        },
        {
          "_id": "696dc5773f1837bfb8970a3f",
          "name": "Xiao Zhang",
          "hidden": false
        },
        {
          "_id": "696dc5773f1837bfb8970a40",
          "name": "Ming He",
          "hidden": false
        },
        {
          "_id": "696dc5773f1837bfb8970a41",
          "name": "Jun Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-16T05:20:10.000Z",
      "submittedOnDailyAt": "2026-01-19T04:22:37.620Z",
      "title": "When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs",
      "submittedOnDailyBy": {
        "_id": "6309bfdab8d7b3889319b588",
        "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg",
        "isPro": false,
        "fullname": "SunZX",
        "user": "Jeryi",
        "type": "user"
      },
      "summary": "Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.",
      "upvotes": 19,
      "discussionId": "696dc5773f1837bfb8970a42",
      "ai_summary": "Personalized large language models can generate false information aligned with user history instead of factual truth, but a new method called FPPS helps maintain both factual accuracy and personalized responses while preserving existing personalization effects.",
      "ai_keywords": [
        "personalized large language models",
        "factual reasoning",
        "personalization-induced hallucinations",
        "representational entanglement",
        "Factuality-Preserving Personalized Steering",
        "PFQABench",
        "inference-time approach",
        "factual accuracy",
        "personalized performance"
      ],
      "organization": {
        "_id": "622177ac43826d6f261f8208",
        "name": "RUC",
        "fullname": "Renmin University of China",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
      }
    },
    "publishedAt": "2026-01-16T00:20:10.000Z",
    "title": "When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs",
    "summary": "Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11000.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6309bfdab8d7b3889319b588",
      "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg",
      "fullname": "SunZX",
      "name": "Jeryi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "622177ac43826d6f261f8208",
      "name": "RUC",
      "fullname": "Renmin University of China",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.11404",
      "authors": [
        {
          "_id": "696d92733f1837bfb897095c",
          "user": {
            "_id": "68ff818830c48bce91a89b2c",
            "avatarUrl": "/avatars/1bdabe82a217a737316df34e3ba14537.svg",
            "isPro": false,
            "fullname": "Linqing Zhong",
            "user": "Linqing94482664",
            "type": "user"
          },
          "name": "Linqing Zhong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-19T09:22:25.917Z",
          "hidden": false
        },
        {
          "_id": "696d92733f1837bfb897095d",
          "name": "Yi Liu",
          "hidden": false
        },
        {
          "_id": "696d92733f1837bfb897095e",
          "name": "Yifei Wei",
          "hidden": false
        },
        {
          "_id": "696d92733f1837bfb897095f",
          "name": "Ziyu Xiong",
          "hidden": false
        },
        {
          "_id": "696d92733f1837bfb8970960",
          "name": "Maoqing Yao",
          "hidden": false
        },
        {
          "_id": "696d92733f1837bfb8970961",
          "name": "Si Liu",
          "hidden": false
        },
        {
          "_id": "696d92733f1837bfb8970962",
          "user": {
            "_id": "646ec9b135f55eb49e405faa",
            "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
            "isPro": false,
            "fullname": "Guanghui Ren",
            "user": "sundrops",
            "type": "user"
          },
          "name": "Guanghui Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-19T09:22:18.181Z",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-16T16:17:06.000Z",
      "submittedOnDailyAt": "2026-01-19T05:00:58.489Z",
      "title": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models",
      "submittedOnDailyBy": {
        "_id": "646ec9b135f55eb49e405faa",
        "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
        "isPro": false,
        "fullname": "Guanghui Ren",
        "user": "sundrops",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.",
      "upvotes": 14,
      "discussionId": "696d92743f1837bfb8970963",
      "ai_summary": "Vision-Language-Action models are enhanced by incorporating action-space reasoning through a structured sequence of coarse action intents, improving manipulation task performance in both simulation and real-world environments.",
      "ai_keywords": [
        "Vision-Language-Model",
        "action space",
        "action chain-of-thought",
        "ACoT",
        "Explicit Action Reasoner",
        "Implicit Action Reasoner",
        "coarse action intents",
        "multimodal input",
        "policy learning"
      ],
      "organization": {
        "_id": "676fc7c31c48eff17fac3135",
        "name": "agibot-world",
        "fullname": "AgiBot World",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e57309b78bc92221ce3b70/ewI1QvFVMDgSsShQeSvlX.png"
      }
    },
    "publishedAt": "2026-01-16T11:17:06.000Z",
    "title": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models",
    "summary": "Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11404.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646ec9b135f55eb49e405faa",
      "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
      "fullname": "Guanghui Ren",
      "name": "sundrops",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "676fc7c31c48eff17fac3135",
      "name": "agibot-world",
      "fullname": "AgiBot World",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e57309b78bc92221ce3b70/ewI1QvFVMDgSsShQeSvlX.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.11037",
      "authors": [
        {
          "_id": "696d98a63f1837bfb8970965",
          "name": "Shiyu Liu",
          "hidden": false
        },
        {
          "_id": "696d98a63f1837bfb8970966",
          "name": "Yongjing Yin",
          "hidden": false
        },
        {
          "_id": "696d98a63f1837bfb8970967",
          "name": "Jianhao Yan",
          "hidden": false
        },
        {
          "_id": "696d98a63f1837bfb8970968",
          "name": "Yunbo Tang",
          "hidden": false
        },
        {
          "_id": "696d98a63f1837bfb8970969",
          "name": "Qinggang Zhang",
          "hidden": false
        },
        {
          "_id": "696d98a63f1837bfb897096a",
          "name": "Bei Li",
          "hidden": false
        },
        {
          "_id": "696d98a63f1837bfb897096b",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "696d98a63f1837bfb897096c",
          "name": "Jingang Wang",
          "hidden": false
        },
        {
          "_id": "696d98a63f1837bfb897096d",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "696d98a63f1837bfb897096e",
          "name": "Jinsong Su",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-16T07:06:58.000Z",
      "submittedOnDailyAt": "2026-01-19T00:06:37.086Z",
      "title": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.",
      "upvotes": 12,
      "discussionId": "696d98a63f1837bfb897096f",
      "githubRepo": "https://github.com/Liushiyu-0709/BAPO-Reliable-Search",
      "githubRepoAddedBy": "user",
      "ai_summary": "Reinforcement learning framework for agentic search that improves reliability by teaching agents to recognize reasoning limits and respond appropriately when evidence is insufficient.",
      "ai_keywords": [
        "reinforcement learning",
        "agentic search",
        "large language models",
        "boundary-aware policy optimization",
        "group-based boundary-aware reward",
        "adaptive reward modulator",
        "I DON'T KNOW response",
        "reasoning limits",
        "reliability enhancement"
      ],
      "githubStars": 4
    },
    "publishedAt": "2026-01-16T02:06:58.000Z",
    "title": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search",
    "summary": "RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11037.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 210,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.09195",
      "authors": [
        {
          "_id": "6969ca6832f0333869ff94c0",
          "name": "Tao Liu",
          "hidden": false
        },
        {
          "_id": "6969ca6832f0333869ff94c1",
          "user": {
            "_id": "6621cea88850e38ffbb1854f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg",
            "isPro": false,
            "fullname": "Taki WU",
            "user": "taki555",
            "type": "user"
          },
          "name": "Taiqiang Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-19T09:23:59.501Z",
          "hidden": false
        },
        {
          "_id": "6969ca6832f0333869ff94c2",
          "name": "Runming Yang",
          "hidden": false
        },
        {
          "_id": "6969ca6832f0333869ff94c3",
          "name": "Shaoning Sun",
          "hidden": false
        },
        {
          "_id": "6969ca6832f0333869ff94c4",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "6969ca6832f0333869ff94c5",
          "name": "Yujiu Yang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6621cea88850e38ffbb1854f/pPOt4b6-MlXHcGoicirKU.png"
      ],
      "publishedAt": "2026-01-14T05:50:40.000Z",
      "submittedOnDailyAt": "2026-01-19T00:19:47.118Z",
      "title": "ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection",
      "submittedOnDailyBy": {
        "_id": "6621cea88850e38ffbb1854f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg",
        "isPro": false,
        "fullname": "Taki WU",
        "user": "taki555",
        "type": "user"
      },
      "summary": "Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.",
      "upvotes": 8,
      "discussionId": "6969ca6932f0333869ff94c6",
      "githubRepo": "https://github.com/Utaotao/ProFit",
      "githubRepoAddedBy": "user",
      "ai_summary": "Supervised fine-tuning with multiple references addresses overfitting to non-core expressions by masking low-probability tokens based on their semantic importance.",
      "ai_keywords": [
        "supervised fine-tuning",
        "Large Language Models",
        "one-to-many nature",
        "token probability",
        "semantic importance",
        "ProFit",
        "surface-level overfitting"
      ],
      "githubStars": 8,
      "organization": {
        "_id": "66f55d53853f0506904d1922",
        "name": "IIGroup",
        "fullname": "Tsinghua IIGroup",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62579c55b98dcaa7e0de285d/A1SKeBEvaODFnkAZusICK.png"
      }
    },
    "publishedAt": "2026-01-14T00:50:40.000Z",
    "title": "ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection",
    "summary": "Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6621cea88850e38ffbb1854f/pPOt4b6-MlXHcGoicirKU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09195.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6621cea88850e38ffbb1854f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg",
      "fullname": "Taki WU",
      "name": "taki555",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66f55d53853f0506904d1922",
      "name": "IIGroup",
      "fullname": "Tsinghua IIGroup",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62579c55b98dcaa7e0de285d/A1SKeBEvaODFnkAZusICK.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.10909",
      "authors": [
        {
          "_id": "696d9afc3f1837bfb897099b",
          "name": "Chuqiao Li",
          "hidden": false
        },
        {
          "_id": "696d9afc3f1837bfb897099c",
          "name": "Xianghui Xie",
          "hidden": false
        },
        {
          "_id": "696d9afc3f1837bfb897099d",
          "name": "Yong Cao",
          "hidden": false
        },
        {
          "_id": "696d9afc3f1837bfb897099e",
          "name": "Andreas Geiger",
          "hidden": false
        },
        {
          "_id": "696d9afc3f1837bfb897099f",
          "name": "Gerard Pons-Moll",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/CVN5Pk94HIT--aBIwgWo9.mp4"
      ],
      "publishedAt": "2026-01-15T23:50:07.000Z",
      "submittedOnDailyAt": "2026-01-19T00:16:39.946Z",
      "title": "FrankenMotion: Part-level Human Motion Generation and Composition",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.",
      "upvotes": 6,
      "discussionId": "696d9afc3f1837bfb89709a0",
      "projectPage": "https://coral79.github.io/frankenmotion/",
      "githubRepo": "https://github.com/Coral79/FrankenMotion-Code",
      "githubRepoAddedBy": "user",
      "ai_summary": "A diffusion-based framework generates human motion from text prompts with fine-grained part-level control using a newly constructed dataset with atomic, temporally-aware annotations.",
      "ai_keywords": [
        "diffusion-based",
        "part-aware motion generation",
        "large language models",
        "temporally-aware part-level text annotations",
        "atomic motion annotations",
        "temporally-structured textual prompts",
        "motion generation"
      ],
      "githubStars": 13
    },
    "publishedAt": "2026-01-15T18:50:07.000Z",
    "title": "FrankenMotion: Part-level Human Motion Generation and Composition",
    "summary": "Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/CVN5Pk94HIT--aBIwgWo9.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10909.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 210,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.10825",
      "authors": [
        {
          "_id": "696d99753f1837bfb8970978",
          "name": "Junsol Kim",
          "hidden": false
        },
        {
          "_id": "696d99753f1837bfb8970979",
          "name": "Shiyang Lai",
          "hidden": false
        },
        {
          "_id": "696d99753f1837bfb897097a",
          "name": "Nino Scherrer",
          "hidden": false
        },
        {
          "_id": "696d99753f1837bfb897097b",
          "name": "Blaise Agüera y Arcas",
          "hidden": false
        },
        {
          "_id": "696d99753f1837bfb897097c",
          "name": "James Evans",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-15T19:52:33.000Z",
      "submittedOnDailyAt": "2026-01-19T00:09:55.018Z",
      "title": "Reasoning Models Generate Societies of Thought",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.",
      "upvotes": 4,
      "discussionId": "696d99753f1837bfb897097d",
      "ai_summary": "Reasoning models demonstrate enhanced performance through multi-agent-like interactions that create diverse cognitive perspectives and improve problem-solving through structured social organization.",
      "ai_keywords": [
        "large language models",
        "reasoning models",
        "chains of thought",
        "multi-agent-like interactions",
        "perspective diversity",
        "personality traits",
        "domain expertise",
        "mechanistic interpretability",
        "conversational behaviors",
        "reinforcement learning",
        "agent organization",
        "collective intelligence",
        "wisdom of crowds"
      ]
    },
    "publishedAt": "2026-01-15T14:52:33.000Z",
    "title": "Reasoning Models Generate Societies of Thought",
    "summary": "Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10825.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 210,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.11516",
      "authors": [
        {
          "_id": "696d99c23f1837bfb897097f",
          "name": "János Kramár",
          "hidden": false
        },
        {
          "_id": "696d99c23f1837bfb8970980",
          "name": "Joshua Engels",
          "hidden": false
        },
        {
          "_id": "696d99c23f1837bfb8970981",
          "name": "Zheng Wang",
          "hidden": false
        },
        {
          "_id": "696d99c23f1837bfb8970982",
          "name": "Bilal Chughtai",
          "hidden": false
        },
        {
          "_id": "696d99c23f1837bfb8970983",
          "name": "Rohin Shah",
          "hidden": false
        },
        {
          "_id": "696d99c23f1837bfb8970984",
          "name": "Neel Nanda",
          "hidden": false
        },
        {
          "_id": "696d99c23f1837bfb8970985",
          "name": "Arthur Conmy",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-16T18:54:29.000Z",
      "submittedOnDailyAt": "2026-01-19T00:11:12.659Z",
      "title": "Building Production-Ready Probes For Gemini",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.\n  We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.\n  These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.",
      "upvotes": 2,
      "discussionId": "696d99c33f1837bfb8970986",
      "ai_summary": "Activation probes for language model misuse mitigation face challenges with long-context generalization, requiring new architectures and diverse training for robust performance across production shifts.",
      "ai_keywords": [
        "activation probes",
        "language model",
        "misuse mitigation",
        "context length",
        "probe architecture",
        "cyber-offensive domain",
        "multi-turn conversations",
        "jailbreaks",
        "red teaming",
        "AlphaEvolve",
        "automated AI safety research"
      ]
    },
    "publishedAt": "2026-01-16T13:54:29.000Z",
    "title": "Building Production-Ready Probes For Gemini",
    "summary": "Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.\n  We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.\n  These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11516.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 210,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.11087",
      "authors": [
        {
          "_id": "696da79a3f1837bfb89709ea",
          "name": "Qiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "696da79a3f1837bfb89709eb",
          "name": "Biao Gong",
          "hidden": false
        },
        {
          "_id": "696da79a3f1837bfb89709ec",
          "name": "Shuai Tan",
          "hidden": false
        },
        {
          "_id": "696da79a3f1837bfb89709ed",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "696da79a3f1837bfb89709ee",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "696da79a3f1837bfb89709ef",
          "name": "Xing Zhu",
          "hidden": false
        },
        {
          "_id": "696da79a3f1837bfb89709f0",
          "name": "Yuyuan Li",
          "hidden": false
        },
        {
          "_id": "696da79a3f1837bfb89709f1",
          "name": "Kelu Yao",
          "hidden": false
        },
        {
          "_id": "696da79a3f1837bfb89709f2",
          "name": "Chunhua Shen",
          "hidden": false
        },
        {
          "_id": "696da79a3f1837bfb89709f3",
          "name": "Changqing Zou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-16T08:40:10.000Z",
      "submittedOnDailyAt": "2026-01-19T01:13:46.189Z",
      "title": "PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models",
      "submittedOnDailyBy": {
        "_id": "644fcbea4f7316588267dc80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
        "isPro": false,
        "fullname": "Biao Gong",
        "user": "BiaoGong",
        "type": "user"
      },
      "summary": "Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.",
      "upvotes": 2,
      "discussionId": "696da79a3f1837bfb89709f4",
      "ai_summary": "A physics-aware reinforcement learning paradigm is introduced for video generation that enforces physical collision rules directly in high-dimensional spaces, ensuring strict application of physics knowledge rather than treating it as conditional constraints.",
      "ai_keywords": [
        "reinforcement learning",
        "video generation",
        "physical collision rules",
        "high-dimensional spaces",
        "physics-aware",
        "Mimicry-Discovery Cycle",
        "PhysRVGBench"
      ]
    },
    "publishedAt": "2026-01-16T03:40:10.000Z",
    "title": "PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models",
    "summary": "Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11087.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644fcbea4f7316588267dc80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
      "fullname": "Biao Gong",
      "name": "BiaoGong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.09255",
      "authors": [
        {
          "_id": "696a064a844a787c4fdea458",
          "user": {
            "_id": "63710c2bafbe42caa5a4dfc8",
            "avatarUrl": "/avatars/33fd1695466bff2b14c3ab42793826ad.svg",
            "isPro": false,
            "fullname": "zhaoyibo",
            "user": "zjuyb",
            "type": "user"
          },
          "name": "Yibo Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-16T10:28:24.372Z",
          "hidden": false
        },
        {
          "_id": "696a064a844a787c4fdea459",
          "name": "Hengjia Li",
          "hidden": false
        },
        {
          "_id": "696a064a844a787c4fdea45a",
          "name": "Xiaofei He",
          "hidden": false
        },
        {
          "_id": "696a064a844a787c4fdea45b",
          "name": "Boxi Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-14T07:41:56.000Z",
      "submittedOnDailyAt": "2026-01-19T05:31:10.396Z",
      "title": "PhyRPR: Training-Free Physics-Constrained Video Generation",
      "submittedOnDailyBy": {
        "_id": "63710c2bafbe42caa5a4dfc8",
        "avatarUrl": "/avatars/33fd1695466bff2b14c3ab42793826ad.svg",
        "isPro": false,
        "fullname": "zhaoyibo",
        "user": "zjuyb",
        "type": "user"
      },
      "summary": "Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,PhyRPR:Phy\\uline{Reason}--Phy\\uline{Plan}--Phy\\uline{Refine}, which decouples physical understanding from visual synthesis. Specifically, PhyReason uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; PhyPlan deterministically synthesizes a controllable coarse motion scaffold; and PhyRefine injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.",
      "upvotes": 1,
      "discussionId": "696a064a844a787c4fdea45c",
      "ai_summary": "A three-stage pipeline decouples physical reasoning from visual synthesis in video generation, improving physical plausibility and motion controllability through distinct phases of reasoning, planning, and refinement.",
      "ai_keywords": [
        "diffusion models",
        "video generation",
        "physical reasoning",
        "motion scaffolding",
        "latent fusion strategy"
      ]
    },
    "publishedAt": "2026-01-14T02:41:56.000Z",
    "title": "PhyRPR: Training-Free Physics-Constrained Video Generation",
    "summary": "Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,PhyRPR:Phy\\uline{Reason}--Phy\\uline{Plan}--Phy\\uline{Refine}, which decouples physical understanding from visual synthesis. Specifically, PhyReason uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; PhyPlan deterministically synthesizes a controllable coarse motion scaffold; and PhyRefine injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09255.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63710c2bafbe42caa5a4dfc8",
      "avatarUrl": "/avatars/33fd1695466bff2b14c3ab42793826ad.svg",
      "fullname": "zhaoyibo",
      "name": "zjuyb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.11354",
      "authors": [
        {
          "_id": "696d99113f1837bfb8970971",
          "name": "Weiyi Wang",
          "hidden": false
        },
        {
          "_id": "696d99113f1837bfb8970972",
          "name": "Xinchi Chen",
          "hidden": false
        },
        {
          "_id": "696d99113f1837bfb8970973",
          "name": "Jingjing Gong",
          "hidden": false
        },
        {
          "_id": "696d99113f1837bfb8970974",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "696d99113f1837bfb8970975",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-16T15:02:41.000Z",
      "submittedOnDailyAt": "2026-01-19T00:08:23.333Z",
      "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.",
      "upvotes": 0,
      "discussionId": "696d99113f1837bfb8970976",
      "githubRepo": "https://github.com/Mtrya/astro-reason",
      "githubRepoAddedBy": "user",
      "ai_summary": "Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.",
      "ai_keywords": [
        "agentic Large Language Models",
        "generalist planners",
        "symbolic environments",
        "weakly grounded environments",
        "physics-constrained domains",
        "Space Planning Problems",
        "heterogeneous objectives",
        "physical constraints",
        "long-horizon decision-making",
        "scheduling regimes",
        "ground station communication",
        "agile Earth observation",
        "agent-oriented interaction protocol",
        "specialized solvers",
        "diagnostic testbed"
      ],
      "githubStars": 1
    },
    "publishedAt": "2026-01-16T10:02:41.000Z",
    "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems",
    "summary": "Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11354.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 210,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.11227",
      "authors": [
        {
          "_id": "696df8e03f1837bfb8970af6",
          "user": {
            "_id": "639ae8dfb49b726255975f86",
            "avatarUrl": "/avatars/3361477fb2de29eaea5484696b2721c6.svg",
            "isPro": false,
            "fullname": "xushaoyang",
            "user": "beiweixiaoxu",
            "type": "user"
          },
          "name": "Shaoyang Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-19T09:27:18.477Z",
          "hidden": false
        },
        {
          "_id": "696df8e03f1837bfb8970af7",
          "name": "Wenxuan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-16T12:14:16.000Z",
      "submittedOnDailyAt": "2026-01-19T07:01:26.528Z",
      "title": "Language of Thought Shapes Output Diversity in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "639ae8dfb49b726255975f86",
        "avatarUrl": "/avatars/3361477fb2de29eaea5484696b2721c6.svg",
        "isPro": false,
        "fullname": "xushaoyang",
        "user": "beiweixiaoxu",
        "type": "user"
      },
      "summary": "Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.",
      "upvotes": 0,
      "discussionId": "696df8e13f1837bfb8970af8",
      "githubRepo": "https://github.com/iNLP-Lab/Multilingual-LoT-Diversity",
      "githubRepoAddedBy": "user",
      "ai_summary": "Controlling the language of thought in large language models increases output diversity by leveraging distinct thinking spaces across different languages, with mixed-language sampling providing superior results.",
      "ai_keywords": [
        "language of thought",
        "thinking space",
        "multilingual thinking",
        "Single-Language Sampling",
        "Mixed-Language Sampling",
        "output diversity",
        "linguistic heterogeneity",
        "pluralistic alignment"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "68b82daee976083ccd80824b",
        "name": "iNLP-Lab",
        "fullname": "iNLP Lab @ SUTD",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60dff6ae19a362a8c27862aa/3Ukf0b4f546tJM84zynTz.png"
      }
    },
    "publishedAt": "2026-01-16T07:14:16.000Z",
    "title": "Language of Thought Shapes Output Diversity in Large Language Models",
    "summary": "Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11227.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639ae8dfb49b726255975f86",
      "avatarUrl": "/avatars/3361477fb2de29eaea5484696b2721c6.svg",
      "fullname": "xushaoyang",
      "name": "beiweixiaoxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68b82daee976083ccd80824b",
      "name": "iNLP-Lab",
      "fullname": "iNLP Lab @ SUTD",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60dff6ae19a362a8c27862aa/3Ukf0b4f546tJM84zynTz.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.07812",
      "authors": [
        {
          "_id": "696df8ea3f1837bfb8970b04",
          "name": "Anurag Das",
          "hidden": false
        },
        {
          "_id": "696df8ea3f1837bfb8970b05",
          "name": "Adrian Bulat",
          "hidden": false
        },
        {
          "_id": "696df8ea3f1837bfb8970b06",
          "name": "Alberto Baldrati",
          "hidden": false
        },
        {
          "_id": "696df8ea3f1837bfb8970b07",
          "name": "Ioannis Maniadis Metaxas",
          "hidden": false
        },
        {
          "_id": "696df8ea3f1837bfb8970b08",
          "name": "Bernt Schiele",
          "hidden": false
        },
        {
          "_id": "696df8ea3f1837bfb8970b09",
          "name": "Georgios Tzimiropoulos",
          "hidden": false
        },
        {
          "_id": "696df8ea3f1837bfb8970b0a",
          "name": "Brais Martinez",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-12T18:45:13.000Z",
      "submittedOnDailyAt": "2026-01-19T07:00:02.929Z",
      "title": "More Images, More Problems? A Controlled Analysis of VLM Failure Modes",
      "submittedOnDailyBy": {
        "_id": "64a6e0923987f4dd3c37087d",
        "avatarUrl": "/avatars/88bc86ddce1b38a012b07c81f5c61183.svg",
        "isPro": false,
        "fullname": "Adrian Bulat",
        "user": "adrianb1",
        "type": "user"
      },
      "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.",
      "upvotes": 0,
      "discussionId": "696df8ea3f1837bfb8970b0b",
      "ai_summary": "Large Vision Language Models exhibit significant limitations in multi-image understanding and reasoning, which are revealed through a new benchmark and addressed via procedural data generation and attention masking techniques.",
      "ai_keywords": [
        "Large Vision Language Models",
        "multi-image capabilities",
        "benchmark",
        "diagnostic experiments",
        "cross-image aggregation",
        "attention-masking scheme",
        "procedural data-generation strategy"
      ],
      "organization": {
        "_id": "686df54910a52f2c2cf03c06",
        "name": "SamsungResearch",
        "fullname": "Samsung Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"
      }
    },
    "publishedAt": "2026-01-12T13:45:13.000Z",
    "title": "More Images, More Problems? A Controlled Analysis of VLM Failure Modes",
    "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07812.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a6e0923987f4dd3c37087d",
      "avatarUrl": "/avatars/88bc86ddce1b38a012b07c81f5c61183.svg",
      "fullname": "Adrian Bulat",
      "name": "adrianb1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "686df54910a52f2c2cf03c06",
      "name": "SamsungResearch",
      "fullname": "Samsung Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"
    },
    "isAuthorParticipating": false
  }
]