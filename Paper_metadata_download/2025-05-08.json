[
  {
    "paper": {
      "id": "2505.02567",
      "authors": [
        {
          "_id": "681c7895c7211b7efbc49f17",
          "name": "Xinjie Zhang",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f18",
          "name": "Jintao Guo",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f19",
          "user": {
            "_id": "66ab4c8a1703f12f49583c6d",
            "avatarUrl": "/avatars/59c77d4556edc049bb410e180813d5e3.svg",
            "isPro": false,
            "fullname": "zss",
            "user": "Suikong",
            "type": "user"
          },
          "name": "Shanshan Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T10:07:03.107Z",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1a",
          "name": "Minghao Fu",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1b",
          "name": "Lunhao Duan",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1c",
          "user": {
            "_id": "636f4c6b5d2050767e4a1491",
            "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
            "isPro": false,
            "fullname": "Guo-Hua Wang",
            "user": "Flourish",
            "type": "user"
          },
          "name": "Guo-Hua Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:04.757Z",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1d",
          "name": "Qing-Guo Chen",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1e",
          "name": "Zhao Xu",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1f",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f20",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658a8a837959448ef5500ce5/DwtDeTTjSrZoe2r4sL-a4.png"
      ],
      "publishedAt": "2025-05-05T11:18:03.000Z",
      "submittedOnDailyAt": "2025-05-08T07:57:47.854Z",
      "title": "Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities",
      "submittedOnDailyBy": {
        "_id": "658a8a837959448ef5500ce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
        "isPro": false,
        "fullname": "Shiyin Lu",
        "user": "runninglsy",
        "type": "user"
      },
      "summary": "Recent years have seen remarkable progress in both multimodal understanding\nmodels and image generation models. Despite their respective successes, these\ntwo domains have evolved independently, leading to distinct architectural\nparadigms: While autoregressive-based architectures have dominated multimodal\nunderstanding, diffusion-based models have become the cornerstone of image\ngeneration. Recently, there has been growing interest in developing unified\nframeworks that integrate these tasks. The emergence of GPT-4o's new\ncapabilities exemplifies this trend, highlighting the potential for\nunification. However, the architectural differences between the two domains\npose significant challenges. To provide a clear overview of current efforts\ntoward unification, we present a comprehensive survey aimed at guiding future\nresearch. First, we introduce the foundational concepts and recent advancements\nin multimodal understanding and text-to-image generation models. Next, we\nreview existing unified models, categorizing them into three main architectural\nparadigms: diffusion-based, autoregressive-based, and hybrid approaches that\nfuse autoregressive and diffusion mechanisms. For each category, we analyze the\nstructural designs and innovations introduced by related works. Additionally,\nwe compile datasets and benchmarks tailored for unified models, offering\nresources for future exploration. Finally, we discuss the key challenges facing\nthis nascent field, including tokenization strategy, cross-modal attention, and\ndata. As this area is still in its early stages, we anticipate rapid\nadvancements and will regularly update this survey. Our goal is to inspire\nfurther research and provide a valuable reference for the community. The\nreferences associated with this survey are available on GitHub\n(https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
      "upvotes": 33,
      "discussionId": "681c7896c7211b7efbc49f76",
      "githubRepo": "https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models",
      "ai_keywords": [
        "autoregressive-based architectures",
        "diffusion-based models",
        "unified frameworks",
        "GPT-4o",
        "multimodal understanding",
        "text-to-image generation models",
        "diffusion-based",
        "autoregressive-based",
        "hybrid approaches",
        "cross-modal attention"
      ]
    },
    "publishedAt": "2025-05-05T07:18:03.000Z",
    "title": "Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities",
    "summary": "Recent years have seen remarkable progress in both multimodal understanding\nmodels and image generation models. Despite their respective successes, these\ntwo domains have evolved independently, leading to distinct architectural\nparadigms: While autoregressive-based architectures have dominated multimodal\nunderstanding, diffusion-based models have become the cornerstone of image\ngeneration. Recently, there has been growing interest in developing unified\nframeworks that integrate these tasks. The emergence of GPT-4o's new\ncapabilities exemplifies this trend, highlighting the potential for\nunification. However, the architectural differences between the two domains\npose significant challenges. To provide a clear overview of current efforts\ntoward unification, we present a comprehensive survey aimed at guiding future\nresearch. First, we introduce the foundational concepts and recent advancements\nin multimodal understanding and text-to-image generation models. Next, we\nreview existing unified models, categorizing them into three main architectural\nparadigms: diffusion-based, autoregressive-based, and hybrid approaches that\nfuse autoregressive and diffusion mechanisms. For each category, we analyze the\nstructural designs and innovations introduced by related works. Additionally,\nwe compile datasets and benchmarks tailored for unified models, offering\nresources for future exploration. Finally, we discuss the key challenges facing\nthis nascent field, including tokenization strategy, cross-modal attention, and\ndata. As this area is still in its early stages, we anticipate rapid\nadvancements and will regularly update this survey. Our goal is to inspire\nfurther research and provide a valuable reference for the community. The\nreferences associated with this survey are available on GitHub\n(https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658a8a837959448ef5500ce5/DwtDeTTjSrZoe2r4sL-a4.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02567.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "658a8a837959448ef5500ce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
      "fullname": "Shiyin Lu",
      "name": "runninglsy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04588",
      "authors": [
        {
          "_id": "681c15ab84d0a008fcdb1ee8",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1ee9",
          "name": "Zile Qiao",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eea",
          "user": {
            "_id": "66224557c61c7fbd98099079",
            "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
            "isPro": false,
            "fullname": "GJ",
            "user": "SpaceProduct",
            "type": "user"
          },
          "name": "Jiayan Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:01.834Z",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eeb",
          "name": "Xuanbo Fan",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eec",
          "name": "Yingyan Hou",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eed",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eee",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eef",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1ef0",
          "name": "Yan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T17:30:22.000Z",
      "submittedOnDailyAt": "2025-05-08T00:54:07.103Z",
      "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
      "submittedOnDailyBy": {
        "_id": "66224557c61c7fbd98099079",
        "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
        "isPro": false,
        "fullname": "GJ",
        "user": "SpaceProduct",
        "type": "user"
      },
      "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms.",
      "upvotes": 23,
      "discussionId": "681c15ac84d0a008fcdb1f21",
      "projectPage": "https://alibaba-nlp.github.io/ZeroSearch/",
      "githubRepo": "https://github.com/Alibaba-nlp/ZeroSearch",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "large language models (LLMs)",
        "search capabilities",
        "live search engines",
        "real-world environments",
        "document quality",
        "noise",
        "instability",
        "training process",
        "API costs",
        "rollouts",
        "search requests",
        "ZeroSearch",
        "lightweight supervised fine-tuning",
        "retrieval module",
        "relevant documents",
        "noisy documents",
        "query",
        "curriculum-based rollout strategy",
        "reasoning ability",
        "retrieval scenarios",
        "base models",
        "instruction-tuned models",
        "parameter sizes",
        "RL algorithms"
      ]
    },
    "publishedAt": "2025-05-07T13:30:22.000Z",
    "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
    "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04588.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66224557c61c7fbd98099079",
      "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
      "fullname": "GJ",
      "name": "SpaceProduct",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04512",
      "authors": [
        {
          "_id": "681c546817fc8222efed5318",
          "name": "Teng Hu",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed5319",
          "name": "Zhentao Yu",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531a",
          "name": "Zhengguang Zhou",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531b",
          "name": "Sen Liang",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531c",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531d",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531e",
          "name": "Qinglin Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T15:33:18.000Z",
      "submittedOnDailyAt": "2025-05-08T05:21:39.978Z",
      "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io.",
      "upvotes": 8,
      "discussionId": "681c546e17fc8222efed54ce",
      "ai_keywords": [
        "LLaVA",
        "text-image fusion module",
        "image ID enhancement module",
        "temporal concatenation",
        "modality-specific condition injection mechanisms",
        "AudioNet module",
        "spatial cross-attention",
        "video-driven injection module",
        "latent-compressed conditional video",
        "patchify-based feature-alignment network",
        "ID consistency",
        "text-video alignment",
        "controllable video generation"
      ]
    },
    "publishedAt": "2025-05-07T11:33:18.000Z",
    "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation",
    "summary": "Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04512.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 50
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04622",
      "authors": [
        {
          "_id": "681c03418ff29a163ef5f370",
          "name": "Jingwen Ye",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f371",
          "user": {
            "_id": "64c903957b4d0d947ce86bc6",
            "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
            "isPro": false,
            "fullname": "Yuze He",
            "user": "hyz317",
            "type": "user"
          },
          "name": "Yuze He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:10.350Z",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f372",
          "name": "Yanning Zhou",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f373",
          "name": "Yiqin Zhu",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f374",
          "user": {
            "_id": "6441491c5d600fb0951cd872",
            "avatarUrl": "/avatars/d98892f3b52d87c2328201efa9897110.svg",
            "isPro": false,
            "fullname": "Kaiwen Xiao",
            "user": "loktarxiao",
            "type": "user"
          },
          "name": "Kaiwen Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:12.445Z",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f375",
          "name": "Yong-Jin Liu",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f376",
          "name": "Wei Yang",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f377",
          "name": "Xiao Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T17:59:46.000Z",
      "submittedOnDailyAt": "2025-05-08T05:41:14.360Z",
      "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer",
      "submittedOnDailyBy": {
        "_id": "64c903957b4d0d947ce86bc6",
        "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
        "isPro": false,
        "fullname": "Yuze He",
        "user": "hyz317",
        "type": "user"
      },
      "summary": "Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io",
      "upvotes": 7,
      "discussionId": "681c03468ff29a163ef5f4d7",
      "projectPage": "https://primitiveanything.github.io/",
      "githubRepo": "https://github.com/PrimitiveAnything/PrimitiveAnything",
      "ai_keywords": [
        "shape primitive abstraction",
        "geometric elements",
        "human visual cognition",
        "computer vision",
        "graphics",
        "3D content generation",
        "geometric optimization",
        "semantic understanding",
        "category-specific datasets",
        "primitive assembly generation task",
        "shape-conditioned primitive transformer",
        "auto-regressive generation",
        "ambiguity-free parameterization scheme",
        "human-crafted abstractions",
        "high-quality primitive assemblies",
        "human perception",
        "geometric fidelity",
        "3D applications",
        "user-generated content (UGC)"
      ]
    },
    "publishedAt": "2025-05-07T13:59:46.000Z",
    "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer",
    "summary": "Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04622.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c903957b4d0d947ce86bc6",
      "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
      "fullname": "Yuze He",
      "name": "hyz317",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.04364",
      "authors": [
        {
          "_id": "681c189c791c72783efe5a94",
          "user": {
            "_id": "6205fefd3f1dc8a642d70b10",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
            "isPro": false,
            "fullname": "Kai Ruan",
            "user": "6cf",
            "type": "user"
          },
          "name": "Kai Ruan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:58:58.134Z",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a95",
          "name": "Mowen Huang",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a96",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a97",
          "name": "Hao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T12:32:01.000Z",
      "submittedOnDailyAt": "2025-05-08T01:06:26.256Z",
      "title": "Benchmarking LLMs' Swarm intelligence",
      "submittedOnDailyBy": {
        "_id": "6205fefd3f1dc8a642d70b10",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
        "isPro": false,
        "fullname": "Kai Ruan",
        "user": "6cf",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench.",
      "upvotes": 7,
      "discussionId": "681c189e791c72783efe5b2d",
      "githubRepo": "https://github.com/x66ccff/swarmbench",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Multi-Agent Systems (MAS)",
        "swarm intelligence",
        "decentralized coordination",
        "spatio-temporal information",
        "SwarmBench",
        "foundational MAS coordination tasks",
        "2D grid environment",
        "local sensory input",
        "local communication",
        "coordination effectiveness",
        "emergent group dynamics",
        "zero-shot setting",
        "robust planning",
        "strategy formation",
        "uncertainty",
        "decentralized scenarios",
        "Embodied MAS"
      ]
    },
    "publishedAt": "2025-05-07T08:32:01.000Z",
    "title": "Benchmarking LLMs' Swarm intelligence",
    "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04364.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6205fefd3f1dc8a642d70b10",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
      "fullname": "Kai Ruan",
      "name": "6cf",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04528",
      "authors": [
        {
          "_id": "681c5152c7211b7efbba4b73",
          "user": {
            "_id": "641aef7b1911d3be67425338",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641aef7b1911d3be67425338/CmCbWWB6NxkAaus59q31w.jpeg",
            "isPro": false,
            "fullname": "Qi Liu",
            "user": "purewhite42",
            "type": "user"
          },
          "name": "Qi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:58:55.624Z",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b74",
          "name": "Xinhao Zheng",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b75",
          "name": "Renqiu Xia",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b76",
          "name": "Xingzhi Qi",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b77",
          "name": "Qinxiang Cao",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b78",
          "name": "Junchi Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T16:02:14.000Z",
      "submittedOnDailyAt": "2025-05-08T05:14:50.449Z",
      "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving",
      "submittedOnDailyBy": {
        "_id": "65b7ae76768464877cdb2e39",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
        "isPro": false,
        "fullname": "Renqiu Xia",
        "user": "renqiux0302",
        "type": "user"
      },
      "summary": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving.",
      "upvotes": 5,
      "discussionId": "681c5153c7211b7efbba4bb4",
      "githubRepo": "https://github.com/Purewhite2019/formal_problem_solving_main",
      "ai_keywords": [
        "Markov decision process",
        "FPS (Formal Problem-Solving)",
        "FTP (formal theorem proving)",
        "D-FPS (Deductive FPS)",
        "FormalMath500",
        "MiniF2F-Solving",
        "PutnamBench-Solving",
        "RPE (Restricted Propositional Equivalence)"
      ]
    },
    "publishedAt": "2025-05-07T12:02:14.000Z",
    "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving",
    "summary": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04528.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b7ae76768464877cdb2e39",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
      "fullname": "Renqiu Xia",
      "name": "renqiux0302",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03912",
      "authors": [
        {
          "_id": "681c549cb322a2fe864c8b0d",
          "name": "Can Cui",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b0e",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b0f",
          "name": "Wenxuan Song",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b10",
          "name": "Shuanghao Bai",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b11",
          "name": "Xinyang Tong",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b12",
          "name": "Zirui Ge",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b13",
          "name": "Runze Suo",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b14",
          "name": "Wanqi Zhou",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b15",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b16",
          "name": "Bofang Jia",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b17",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b18",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b19",
          "name": "Donglin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T18:35:07.000Z",
      "submittedOnDailyAt": "2025-05-08T05:23:33.004Z",
      "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "Dual-system VLA (Vision-Language-Action) architectures have become a hot\ntopic in embodied intelligence research, but there is a lack of sufficient\nopen-source work for further performance analysis and optimization. To address\nthis problem, this paper will summarize and compare the structural designs of\nexisting dual-system architectures, and conduct systematic empirical\nevaluations on the core design elements of existing dual-system architectures.\nUltimately, it will provide a low-cost open-source model for further\nexploration. Of course, this project will continue to update with more\nexperimental conclusions and open-source models with improved performance for\neveryone to choose from. Project page: https://openhelix-robot.github.io/.",
      "upvotes": 3,
      "discussionId": "681c549eb322a2fe864c8b6e"
    },
    "publishedAt": "2025-05-06T14:35:07.000Z",
    "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation",
    "summary": "Dual-system VLA (Vision-Language-Action) architectures have become a hot\ntopic in embodied intelligence research, but there is a lack of sufficient\nopen-source work for further performance analysis and optimization. To address\nthis problem, this paper will summarize and compare the structural designs of\nexisting dual-system architectures, and conduct systematic empirical\nevaluations on the core design elements of existing dual-system architectures.\nUltimately, it will provide a low-cost open-source model for further\nexploration. Of course, this project will continue to update with more\nexperimental conclusions and open-source models with improved performance for\neveryone to choose from. Project page: https://openhelix-robot.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03912.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03418",
      "authors": [
        {
          "_id": "681c4d5b5971460af345032a",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032b",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032c",
          "name": "Junwei Su",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032d",
          "name": "Yuchen Tian",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032e",
          "name": "Yuqi Zhu",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032f",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450330",
          "name": "Lanning Wei",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450331",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450332",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T10:53:58.000Z",
      "submittedOnDailyAt": "2025-05-08T04:51:36.213Z",
      "title": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification.",
      "upvotes": 2,
      "discussionId": "681c4d5f5971460af3450465",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Chain-of-Thought (CoT) reasoning",
        "knowledge augmentation",
        "verification techniques",
        "software engineering",
        "mathematical reasoning and proving",
        "data analysis and modeling",
        "scientific research",
        "multi-step reasoning",
        "domain knowledge integration",
        "result verification"
      ]
    },
    "publishedAt": "2025-05-06T06:53:58.000Z",
    "title": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey",
    "summary": "Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03418.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03821",
      "authors": [
        {
          "_id": "681c7a3829ba66a745217db5",
          "user": {
            "_id": "63caf7ce9f78909f9f81eb72",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
            "isPro": true,
            "fullname": "Gracjan Goral",
            "user": "Gracjan",
            "type": "user"
          },
          "name": "Gracjan Gral",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:02.558Z",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db6",
          "name": "Alicja Ziarko",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db7",
          "name": "Piotr Mio",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db8",
          "name": "Micha Nauman",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db9",
          "name": "Maciej Woczyk",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217dba",
          "name": "Micha Kosiski",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63caf7ce9f78909f9f81eb72/b8OMQONPXPyBtcitD1Yg7.jpeg"
      ],
      "publishedAt": "2025-05-03T00:10:41.000Z",
      "submittedOnDailyAt": "2025-05-08T08:19:59.040Z",
      "title": "Beyond Recognition: Evaluating Visual Perspective Taking in Vision\n  Language Models",
      "submittedOnDailyBy": {
        "_id": "63caf7ce9f78909f9f81eb72",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
        "isPro": true,
        "fullname": "Gracjan Goral",
        "user": "Gracjan",
        "type": "user"
      },
      "summary": "We investigate the ability of Vision Language Models (VLMs) to perform visual\nperspective taking using a novel set of visual tasks inspired by established\nhuman tests. Our approach leverages carefully controlled scenes, in which a\nsingle humanoid minifigure is paired with a single object. By systematically\nvarying spatial configurations - such as object position relative to the\nhumanoid minifigure and the humanoid minifigure's orientation - and using both\nbird's-eye and surface-level views, we created 144 unique visual tasks. Each\nvisual task is paired with a series of 7 diagnostic questions designed to\nassess three levels of visual cognition: scene understanding, spatial\nreasoning, and visual perspective taking. Our evaluation of several\nstate-of-the-art models, including GPT-4-Turbo, GPT-4o,\nLlama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that\nwhile they excel in scene understanding, the performance declines significantly\non spatial reasoning and further deteriorates on perspective-taking. Our\nanalysis suggests a gap between surface-level object recognition and the deeper\nspatial and perspective reasoning required for complex visual tasks, pointing\nto the need for integrating explicit geometric representations and tailored\ntraining protocols in future VLM development.",
      "upvotes": 2,
      "discussionId": "681c7a3e29ba66a745217f0c"
    },
    "publishedAt": "2025-05-02T20:10:41.000Z",
    "title": "Beyond Recognition: Evaluating Visual Perspective Taking in Vision\n  Language Models",
    "summary": "We investigate the ability of Vision Language Models (VLMs) to perform visual\nperspective taking using a novel set of visual tasks inspired by established\nhuman tests. Our approach leverages carefully controlled scenes, in which a\nsingle humanoid minifigure is paired with a single object. By systematically\nvarying spatial configurations - such as object position relative to the\nhumanoid minifigure and the humanoid minifigure's orientation - and using both\nbird's-eye and surface-level views, we created 144 unique visual tasks. Each\nvisual task is paired with a series of 7 diagnostic questions designed to\nassess three levels of visual cognition: scene understanding, spatial\nreasoning, and visual perspective taking. Our evaluation of several\nstate-of-the-art models, including GPT-4-Turbo, GPT-4o,\nLlama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that\nwhile they excel in scene understanding, the performance declines significantly\non spatial reasoning and further deteriorates on perspective-taking. Our\nanalysis suggests a gap between surface-level object recognition and the deeper\nspatial and perspective reasoning required for complex visual tasks, pointing\nto the need for integrating explicit geometric representations and tailored\ntraining protocols in future VLM development.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63caf7ce9f78909f9f81eb72/b8OMQONPXPyBtcitD1Yg7.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63caf7ce9f78909f9f81eb72",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
      "fullname": "Gracjan Goral",
      "name": "Gracjan",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.00358",
      "authors": [
        {
          "_id": "68154d77c8ab88a66b8d81a7",
          "name": "Albert Ge",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81a8",
          "name": "Tzu-Heng Huang",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81a9",
          "name": "John Cooper",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81aa",
          "name": "Avi Trost",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ab",
          "name": "Ziyi Chu",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ac",
          "name": "Satya Sai Srinath Namburi GNVV",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ad",
          "name": "Ziyang Cai",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ae",
          "name": "Kendall Park",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81af",
          "name": "Nicholas Roberts",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81b0",
          "name": "Frederic Sala",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T07:08:19.000Z",
      "submittedOnDailyAt": "2025-05-08T05:38:42.650Z",
      "title": "R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training",
      "submittedOnDailyBy": {
        "_id": "650263c89a612aa33a018383",
        "avatarUrl": "/avatars/a1415a4696da98ffebc1ac798cbab052.svg",
        "isPro": false,
        "fullname": "Albert Ge",
        "user": "albertge",
        "type": "user"
      },
      "summary": "Data mixing strategies have successfully reduced the costs involved in\ntraining language models. While promising, such methods suffer from two flaws.\nFirst, they rely on predetermined data domains (e.g., data sources, task\ntypes), which may fail to capture critical semantic nuances, leaving\nperformance on the table. Second, these methods scale with the number of\ndomains in a computationally prohibitive way. We address these challenges via\nR&B, a framework that re-partitions training data based on semantic similarity\n(Regroup) to create finer-grained domains, and efficiently optimizes the data\ncomposition (Balance) by leveraging a Gram matrix induced by domain gradients\nobtained throughout training. Unlike prior works, it removes the need for\nadditional compute to obtain evaluation information such as losses or\ngradients. We analyze this technique under standard regularity conditions and\nprovide theoretical insights that justify R&B's effectiveness compared to\nnon-adaptive mixing approaches. Empirically, we demonstrate the effectiveness\nof R&B on five diverse datasets ranging from natural language to reasoning and\nmultimodal tasks. With as little as 0.01% additional compute overhead, R&B\nmatches or exceeds the performance of state-of-the-art data mixing strategies.",
      "upvotes": 2,
      "discussionId": "68154d78c8ab88a66b8d820c",
      "ai_keywords": [
        "semantic similarity",
        "Gram matrix",
        "domain gradients"
      ]
    },
    "publishedAt": "2025-05-01T03:08:19.000Z",
    "title": "R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training",
    "summary": "Data mixing strategies have successfully reduced the costs involved in\ntraining language models. While promising, such methods suffer from two flaws.\nFirst, they rely on predetermined data domains (e.g., data sources, task\ntypes), which may fail to capture critical semantic nuances, leaving\nperformance on the table. Second, these methods scale with the number of\ndomains in a computationally prohibitive way. We address these challenges via\nR&B, a framework that re-partitions training data based on semantic similarity\n(Regroup) to create finer-grained domains, and efficiently optimizes the data\ncomposition (Balance) by leveraging a Gram matrix induced by domain gradients\nobtained throughout training. Unlike prior works, it removes the need for\nadditional compute to obtain evaluation information such as losses or\ngradients. We analyze this technique under standard regularity conditions and\nprovide theoretical insights that justify R&B's effectiveness compared to\nnon-adaptive mixing approaches. Empirically, we demonstrate the effectiveness\nof R&B on five diverse datasets ranging from natural language to reasoning and\nmultimodal tasks. With as little as 0.01% additional compute overhead, R&B\nmatches or exceeds the performance of state-of-the-art data mixing strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00358.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650263c89a612aa33a018383",
      "avatarUrl": "/avatars/a1415a4696da98ffebc1ac798cbab052.svg",
      "fullname": "Albert Ge",
      "name": "albertge",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03570",
      "authors": [
        {
          "_id": "681b518bf497fd5e45b55eeb",
          "user": {
            "_id": "667ed2bf12e48bee0e972ccc",
            "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
            "isPro": false,
            "fullname": "Mariya Davydova",
            "user": "mariya-davydova",
            "type": "user"
          },
          "name": "Mariya Davydova",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:24.254Z",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eec",
          "name": "Daniel Jeffries",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eed",
          "name": "Patrick Barker",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eee",
          "name": "Arturo Mrquez Flores",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eef",
          "name": "Sinad Ryan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/fe49DABHt5h8Uw-_GCzaB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/yfxA_jM6W5khwa_hHL_mF.png"
      ],
      "publishedAt": "2025-05-06T14:29:47.000Z",
      "submittedOnDailyAt": "2025-05-08T07:36:52.978Z",
      "title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents",
      "submittedOnDailyBy": {
        "_id": "667ed2bf12e48bee0e972ccc",
        "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
        "isPro": false,
        "fullname": "Mariya Davydova",
        "user": "mariya-davydova",
        "type": "user"
      },
      "summary": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\nease of use, extensibility, comprehensive coverage of test cases, and automated\nvalidation. We divide the tasks in increasing levels of complexity, from basic\nprecision clicking to multistep, multiapplication tests requiring dexterity,\nprecision, and clear thinking from the agent. In version one of the benchmark,\npresented here, we have calibrated the complexity of the benchmark test cases\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\ndo not achieve results higher than 50%, while the average white collar worker\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\nmanually, but we also introduce an automated validation mechanism that has an\naverage error rate less than 2%. Therefore, this benchmark presents solid\nground for fully automated measuring of progress, capabilities and the\neffectiveness of GUI-navigation AI agents over the short and medium-term\nhorizon. The source code of the benchmark is available at\nhttps://github.com/agentsea/osuniverse.",
      "upvotes": 1,
      "discussionId": "681b518cf497fd5e45b55f0f",
      "projectPage": "https://agentsea.github.io/osuniverse/",
      "githubRepo": "https://github.com/agentsea/osuniverse"
    },
    "publishedAt": "2025-05-06T10:29:47.000Z",
    "title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents",
    "summary": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\nease of use, extensibility, comprehensive coverage of test cases, and automated\nvalidation. We divide the tasks in increasing levels of complexity, from basic\nprecision clicking to multistep, multiapplication tests requiring dexterity,\nprecision, and clear thinking from the agent. In version one of the benchmark,\npresented here, we have calibrated the complexity of the benchmark test cases\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\ndo not achieve results higher than 50%, while the average white collar worker\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\nmanually, but we also introduce an automated validation mechanism that has an\naverage error rate less than 2%. Therefore, this benchmark presents solid\nground for fully automated measuring of progress, capabilities and the\neffectiveness of GUI-navigation AI agents over the short and medium-term\nhorizon. The source code of the benchmark is available at\nhttps://github.com/agentsea/osuniverse.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/fe49DABHt5h8Uw-_GCzaB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/yfxA_jM6W5khwa_hHL_mF.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03570.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667ed2bf12e48bee0e972ccc",
      "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
      "fullname": "Mariya Davydova",
      "name": "mariya-davydova",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02393",
      "authors": [
        {
          "_id": "681c423f198e1dea5c26f2f4",
          "user": {
            "_id": "6445e9bd1cfc9ae6bb40985c",
            "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
            "isPro": false,
            "fullname": "Evan Jeong",
            "user": "Eavn",
            "type": "user"
          },
          "name": "Sungheon Jeong",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-08T09:38:37.142Z",
          "hidden": false
        },
        {
          "_id": "681c423f198e1dea5c26f2f5",
          "user": {
            "_id": "646b57c6e5abcbf6709fabf6",
            "avatarUrl": "/avatars/e9749acf7866eeaf017f0a43351794fc.svg",
            "isPro": false,
            "fullname": "Jihong Park",
            "user": "Paper9795",
            "type": "user"
          },
          "name": "Jihong Park",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-08T05:36:09.797Z",
          "hidden": false
        },
        {
          "_id": "681c423f198e1dea5c26f2f6",
          "name": "Mohsen Imani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:33:20.000Z",
      "submittedOnDailyAt": "2025-05-08T04:12:55.976Z",
      "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection",
      "submittedOnDailyBy": {
        "_id": "6445e9bd1cfc9ae6bb40985c",
        "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
        "isPro": false,
        "fullname": "Evan Jeong",
        "user": "Eavn",
        "type": "user"
      },
      "summary": "Most existing video anomaly detectors rely solely on RGB frames, which lack\nthe temporal resolution needed to capture abrupt or transient motion cues, key\nindicators of anomalous events. To address this limitation, we propose\nImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that\nsynthesizes event representations directly from RGB videos and fuses them with\nimage features through a principled, uncertainty-aware process. The system (i)\nmodels heavy-tailed sensor noise with a Student`s-t likelihood, deriving\nvalue-level inverse-variance weights via a Laplace approximation; (ii) applies\nKalman-style frame-wise updates to balance modalities over time; and (iii)\niteratively refines the fused latent state to erase residual cross-modal noise.\nWithout any dedicated event sensor or frame-level labels, IEF-VAD sets a new\nstate of the art across multiple real-world anomaly detection benchmarks. These\nfindings highlight the utility of synthetic event representations in\nemphasizing motion cues that are often underrepresented in RGB frames, enabling\naccurate and robust video understanding across diverse applications without\nrequiring dedicated event sensors. Code and models are available at\nhttps://github.com/EavnJeong/IEF-VAD.",
      "upvotes": 1,
      "discussionId": "681c4243198e1dea5c26f3cd",
      "githubRepo": "https://github.com/EavnJeong/IEF-VAD",
      "ai_keywords": [
        "Image-Event Fusion",
        "Video Anomaly Detection",
        "event representations",
        "Student`s-t likelihood",
        "Laplace approximation",
        "Kalman-style frame-wise updates",
        "fused latent state",
        "cross-modal noise"
      ]
    },
    "publishedAt": "2025-05-05T02:33:20.000Z",
    "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection",
    "summary": "Most existing video anomaly detectors rely solely on RGB frames, which lack\nthe temporal resolution needed to capture abrupt or transient motion cues, key\nindicators of anomalous events. To address this limitation, we propose\nImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that\nsynthesizes event representations directly from RGB videos and fuses them with\nimage features through a principled, uncertainty-aware process. The system (i)\nmodels heavy-tailed sensor noise with a Student`s-t likelihood, deriving\nvalue-level inverse-variance weights via a Laplace approximation; (ii) applies\nKalman-style frame-wise updates to balance modalities over time; and (iii)\niteratively refines the fused latent state to erase residual cross-modal noise.\nWithout any dedicated event sensor or frame-level labels, IEF-VAD sets a new\nstate of the art across multiple real-world anomaly detection benchmarks. These\nfindings highlight the utility of synthetic event representations in\nemphasizing motion cues that are often underrepresented in RGB frames, enabling\naccurate and robust video understanding across diverse applications without\nrequiring dedicated event sensors. Code and models are available at\nhttps://github.com/EavnJeong/IEF-VAD.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02393.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6445e9bd1cfc9ae6bb40985c",
      "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
      "fullname": "Evan Jeong",
      "name": "Eavn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]