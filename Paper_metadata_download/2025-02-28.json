[
  {
    "paper": {
      "id": "2502.19613",
      "authors": [
        {
          "_id": "67c12987505a88e4a185e0d7",
          "name": "Wei Xiong",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0d8",
          "name": "Hanning Zhang",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0d9",
          "name": "Chenlu Ye",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0da",
          "name": "Lichang Chen",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0db",
          "name": "Nan Jiang",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0dc",
          "name": "Tong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T23:01:16.000Z",
      "title": "Self-rewarding correction for mathematical reasoning",
      "summary": "We study self-rewarding reasoning large language models (LLMs), which can\nsimultaneously generate step-by-step reasoning and evaluate the correctness of\ntheir outputs during the inference time-without external feedback. This\nintegrated approach allows a single model to independently guide its reasoning\nprocess, offering computational advantages for model deployment. We\nparticularly focus on the representative task of self-correction, where models\nautonomously detect errors in their responses, revise outputs, and decide when\nto terminate iterative refinement loops. To enable this, we propose a\ntwo-staged algorithmic framework for constructing self-rewarding reasoning\nmodels using only self-generated data. In the first stage, we employ sequential\nrejection sampling to synthesize long chain-of-thought trajectories that\nincorporate both self-rewarding and self-correction mechanisms. Fine-tuning\nmodels on these curated data allows them to learn the patterns of\nself-rewarding and self-correction. In the second stage, we further enhance the\nmodels' ability to assess response accuracy and refine outputs through\nreinforcement learning with rule-based signals. Experiments with Llama-3 and\nQwen-2.5 demonstrate that our approach surpasses intrinsic self-correction\ncapabilities and achieves performance comparable to systems that rely on\nexternal reward models.",
      "upvotes": 42,
      "discussionId": "67c12989505a88e4a185e115"
    },
    "publishedAt": "2025-02-27T22:15:54.222Z",
    "title": "Self-rewarding correction for mathematical reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19613.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "643e59806db6ba8c5ee123f3",
      "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
      "fullname": "Wei Xiong",
      "name": "weqweasdas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20395",
      "authors": [
        {
          "_id": "67c12b5def9af74902537b98",
          "name": "Zhongyang Li",
          "hidden": false
        },
        {
          "_id": "67c12b5def9af74902537b99",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67c12b5def9af74902537b9a",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T18:59:32.000Z",
      "title": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts",
      "summary": "In large multimodal models (LMMs), the perception of non-language modalities\n(e.g., visual representations) is usually not on par with the large language\nmodels (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on\nchallenging downstream tasks. This weakness has been recently mitigated by\nreplacing the vision encoder with a mixture-of-experts (MoE), which provides\nrich, multi-granularity, and diverse representations required by diverse\ndownstream tasks. The performance of multimodal MoE largely depends on its\nrouter, which reweights and mixes the representations of different experts for\neach input. However, we find that the end-to-end trained router does not always\nproduce the optimal routing weights for every test sample. To bridge the gap,\nwe propose a novel and efficient method \"Re-Routing in Test-Time(R2-T2) that\nlocally optimizes the vector of routing weights in test-time by moving it\ntoward those vectors of the correctly predicted samples in a neighborhood of\nthe test sample. We propose three R2-T2 strategies with different optimization\nobjectives and neighbor-search spaces. R2-T2 consistently and greatly improves\nstate-of-the-art LMMs' performance on challenging benchmarks of diverse tasks,\nwithout training any base-model parameters.",
      "upvotes": 20,
      "discussionId": "67c12b5eef9af74902537c00"
    },
    "publishedAt": "2025-02-27T22:27:24.486Z",
    "title": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/PaZkWIhqZBRCSfBA-k4OX.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/FASlyPDiSb9VHZaeWMj9H.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/kGeIJVMDDAbIassiuYIb2.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/Tw2Bf_RsFTPARKLJWIlKM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20395.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20082",
      "authors": [
        {
          "_id": "67c12b6d25c74ee5b6e2ce8e",
          "name": "Ning Shang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce8f",
          "name": "Li Lyna Zhang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce90",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce91",
          "name": "Gaokai Zhang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce92",
          "name": "Gilsinia Lopez",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce93",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce94",
          "name": "Weizhu Chen",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce95",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T13:41:07.000Z",
      "title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
      "summary": "LongRoPE2 is a novel approach that extends the effective context window of\npre-trained large language models (LLMs) to the target length, while preserving\nthe performance on the original shorter context window. This is achieved by\nthree contributions: (1) a hypothesis that insufficient training in higher RoPE\ndimensions contributes to the persistent out-of-distribution (OOD) issues\nobserved in existing methods; (2) an effective RoPE rescaling algorithm that\nadopts evolutionary search guided by \"needle-driven\" perplexity to address the\ninsufficient training problem; (3) a mixed context window training approach\nthat fine-tunes model weights to adopt rescaled RoPE for long-context sequences\nwhile preserving the short-context performance with the original RoPE.\nExtensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks\nvalidate the hypothesis and demonstrate the effectiveness of LongRoPE2.\nRemarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context\nlength while retaining over 98.5% of short-context performance, using only 10B\ntokens -- 80x fewer than Meta's approach, which fails to reach the target\neffective context length. Code will be available at\nhttps://github.com/microsoft/LongRoPE.",
      "upvotes": 19,
      "discussionId": "67c12b6e25c74ee5b6e2ceb5"
    },
    "publishedAt": "2025-02-27T22:22:53.713Z",
    "title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20082.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b0009c72043b05d29492b2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
      "fullname": "Li Lyna Zhang",
      "name": "lynazhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 27
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.19634",
      "authors": [
        {
          "_id": "67c12bf3505a88e4a1866a01",
          "name": "Jiazhen Pan",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a02",
          "user": {
            "_id": "631b9ff5824f2502e3557c7e",
            "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
            "isPro": false,
            "fullname": "liu",
            "user": "che111",
            "type": "user"
          },
          "name": "Che Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-28T09:28:38.598Z",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a03",
          "name": "Junde Wu",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a04",
          "name": "Fenglin Liu",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a05",
          "name": "Jiayuan Zhu",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a06",
          "name": "Hongwei Bran Li",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a07",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a08",
          "name": "Cheng Ouyang",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a09",
          "name": "Daniel Rueckert",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T23:57:34.000Z",
      "title": "MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language\n  Models (VLMs) via Reinforcement Learning",
      "summary": "Reasoning is a critical frontier for advancing medical image analysis, where\ntransparency and trustworthiness play a central role in both clinician trust\nand regulatory approval. Although Medical Visual Language Models (VLMs) show\npromise for radiological tasks, most existing VLMs merely produce final answers\nwithout revealing the underlying reasoning. To address this gap, we introduce\nMedVLM-R1, a medical VLM that explicitly generates natural language reasoning\nto enhance transparency and trustworthiness. Instead of relying on supervised\nfine-tuning (SFT), which often suffers from overfitting to training\ndistributions and fails to foster genuine reasoning, MedVLM-R1 employs a\nreinforcement learning framework that incentivizes the model to discover\nhuman-interpretable reasoning paths without using any reasoning references.\nDespite limited training data (600 visual question answering samples) and model\nparameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI,\nCT, and X-ray benchmarks, outperforming larger models trained on over a million\nsamples. It also demonstrates robust domain generalization under\nout-of-distribution tasks. By unifying medical image analysis with explicit\nreasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable\nAI in clinical practice.",
      "upvotes": 17,
      "discussionId": "67c12bf4505a88e4a1866a35"
    },
    "publishedAt": "2025-02-28T04:36:05.045Z",
    "title": "MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19634.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631b9ff5824f2502e3557c7e",
      "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
      "fullname": "liu",
      "name": "che111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20238",
      "authors": [
        {
          "_id": "67c15306333e2f71f01c8e35",
          "name": "Guizhen Chen",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e36",
          "name": "Weiwen Xu",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e37",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e38",
          "name": "Hou Pong Chan",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e39",
          "name": "Chaoqun Liu",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e3a",
          "name": "Lidong Bing",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e3b",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e3c",
          "name": "Anh Tuan Luu",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e3d",
          "name": "Yu Rong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T16:23:25.000Z",
      "title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through\n  Reflective Puzzle Solving",
      "summary": "Many challenging reasoning tasks require not just rapid, intuitive responses,\nbut a more deliberate, multi-step approach. Recent progress in large language\nmodels (LLMs) highlights an important shift from the \"System 1\" way of quick\nreactions to the \"System 2\" style of reflection-and-correction problem solving.\nHowever, current benchmarks heavily rely on the final-answer accuracy, leaving\nmuch of a model's intermediate reasoning steps unexamined. This fails to assess\nthe model's ability to reflect and rectify mistakes within the reasoning\nprocess. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark\nfor fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be\ndecomposed into atomic steps, making it ideal for rigorous validation of\nintermediate correctness. Building on this, we introduce two tasks: state\nchecking, and state transition, for a comprehensive evaluation of how models\nassess the current situation and plan the next move. To support broader\nresearch, we also provide a puzzle training set aimed at enhancing performance\non general mathematical tasks. We show that models trained on our state\nchecking and transition data demonstrate gains in math reasoning by up to 5.1%\non GSM8K.",
      "upvotes": 13,
      "discussionId": "67c15307333e2f71f01c8ebc"
    },
    "publishedAt": "2025-02-28T01:14:11.268Z",
    "title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20238.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e85b3edb3767299865e0e3",
      "avatarUrl": "/avatars/fdbe121535dea940edd2766161393485.svg",
      "fullname": "Chen",
      "name": "Guizhen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16645",
      "authors": [
        {
          "_id": "67c12e60d8247a49b805694f",
          "name": "Chenlong Wang",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056950",
          "name": "Zhaoyang Chu",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056951",
          "user": {
            "_id": "669096da35cddb688a352ca8",
            "avatarUrl": "/avatars/d01f34d99d89447d27c0fd43734ae6d9.svg",
            "isPro": false,
            "fullname": "zxiang",
            "user": "zx10086",
            "type": "user"
          },
          "name": "Zhengxiang Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-28T09:28:33.569Z",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056952",
          "user": {
            "_id": "6743e9d4303e7ce5b9d13e9b",
            "avatarUrl": "/avatars/cdaf150380e9c8916547185b968a2670.svg",
            "isPro": false,
            "fullname": "xy",
            "user": "yxy0807",
            "type": "user"
          },
          "name": "Xuyi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-28T09:28:31.564Z",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056953",
          "name": "Kaiyue Qiu",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056954",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056955",
          "name": "Zhou Zhao",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056956",
          "name": "Xuanhua Shi",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056957",
          "name": "Dongping Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T16:46:18.000Z",
      "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code\n  Evolution at Scale",
      "summary": "Large Language Models (LLMs) have exhibited exceptional performance in\nsoftware engineering yet face challenges in adapting to continually evolving\ncode knowledge, particularly regarding the frequent updates of third-party\nlibrary APIs. This limitation, stemming from static pre-training datasets,\noften results in non-executable code or implementations with suboptimal safety\nand efficiency. To this end, this paper introduces CODESYNC, a data engine for\nidentifying outdated code patterns and collecting real-time code knowledge\nupdates from Python third-party libraries. Building upon CODESYNC, we develop\nCODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay\nsynchronized with code evolution, which covers real-world updates for 220 APIs\nfrom six Python libraries. Our benchmark offers 3,300 test cases across three\nevaluation tasks and an update-aware instruction tuning dataset consisting of\n2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs\nreveal that they struggle with dynamic code evolution, even with the support of\nadvanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe\nthat our benchmark can offer a strong foundation for the development of more\neffective methods for real-time code knowledge updating in the future. The\nexperimental code and dataset are publicly available at:\nhttps://github.com/Lucky-voyage/Code-Sync.",
      "upvotes": 12,
      "discussionId": "67c12e61d8247a49b805698f"
    },
    "publishedAt": "2025-02-27T23:04:14.619Z",
    "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16645.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16944",
      "authors": [
        {
          "_id": "67be807e8a5a805423137ca2",
          "name": "Chenghua Huang",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca3",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca4",
          "user": {
            "_id": "669dcf6200970c3b27aafa5d",
            "avatarUrl": "/avatars/bb9ed5ff86326fdaeb184c6b0e40f74f.svg",
            "isPro": false,
            "fullname": "kaikai yang",
            "user": "keanudicap",
            "type": "user"
          },
          "name": "Fangkai Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:17:46.382Z",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca5",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca6",
          "name": "Zhixu Li",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca7",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca8",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca9",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137caa",
          "name": "Qi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T08:11:33.000Z",
      "title": "Lean and Mean: Decoupled Value Policy Optimization with Global Value\n  Guidance",
      "summary": "Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human\nFeedback (RLHF) is essential for aligning large language models (LLMs) with\nhuman preferences. It requires joint training of an actor and critic with a\npretrained, fixed reward model for guidance. This approach increases\ncomputational complexity and instability due to actor-critic interdependence.\nAdditionally, PPO lacks access to true environment rewards in LLM tasks,\nlimiting its adaptability. Under such conditions, pretraining a value model or\na reward model becomes equivalent, as both provide fixed supervisory signals\nwithout new ground-truth feedback. To address these issues, we propose\nDecoupled Value Policy Optimization (DVPO), a lean framework that\nreplaces traditional reward modeling with a pretrained global value model\n(GVM). The GVM is conditioned on policy trajectories and predicts token-level\nreturn-to-go estimates. By decoupling value model from policy training (via\nfrozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence,\nreducing GPU memory usage by 40\\% and training time by 35\\% compared to\nconventional RLHF. Experiments across benchmarks show DVPO outperforms\nefficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in\nperformance.",
      "upvotes": 8,
      "discussionId": "67be807e8a5a805423137cc2"
    },
    "publishedAt": "2025-02-28T01:55:41.427Z",
    "title": "Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16944.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669dcf6200970c3b27aafa5d",
      "avatarUrl": "/avatars/bb9ed5ff86326fdaeb184c6b0e40f74f.svg",
      "fullname": "kaikai yang",
      "name": "keanudicap",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20321",
      "authors": [
        {
          "_id": "67c13c68d8247a49b808fdac",
          "name": "Chuofan Ma",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdad",
          "name": "Yi Jiang",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdae",
          "name": "Junfeng Wu",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdaf",
          "name": "Jihan Yang",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdb0",
          "name": "Xin Yu",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdb1",
          "name": "Zehuan Yuan",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdb2",
          "name": "Bingyue Peng",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdb3",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T17:47:01.000Z",
      "title": "UniTok: A Unified Tokenizer for Visual Generation and Understanding",
      "summary": "The representation disparity between visual generation and understanding\nimposes a critical gap in integrating these capabilities into a single\nframework. To bridge this gap, we introduce UniTok, a discrete visual tokenizer\nthat encodes fine-grained details for generation while also capturing\nhigh-level semantics for understanding. Despite recent studies have shown that\nthese objectives could induce loss conflicts in training, we reveal that the\nunderlying bottleneck stems from limited representational capacity of discrete\ntokens. We address this by introducing multi-codebook quantization, which\ndivides vector quantization with several independent sub-codebooks to expand\nthe latent feature space, while avoiding training instability caused by\noverlarge codebooks. Our method significantly raises the upper limit of unified\ndiscrete tokenizers to match or even surpass domain-specific continuous\ntokenizers. For instance, UniTok achieves a remarkable rFID of 0.38 (versus\n0.87 for SD-VAE) and a zero-shot accuracy of 78.6% (versus 76.2% for CLIP) on\nImageNet. Our code is available at https://github.com/FoundationVision/UniTok.",
      "upvotes": 8,
      "discussionId": "67c13c6ad8247a49b8090003"
    },
    "publishedAt": "2025-02-27T23:34:45.416Z",
    "title": "UniTok: A Unified Tokenizer for Visual Generation and Understanding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20321.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6344dcb1cd37e44d9ed46508",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344dcb1cd37e44d9ed46508/J92UKSxKR3iziD2WJfih4.jpeg",
      "fullname": "Yi Jiang",
      "name": "JiangYi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20126",
      "authors": [
        {
          "_id": "67c14524af5eaa8dd062a216",
          "name": "Sotiris Anagnostidis",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a217",
          "name": "Gregor Bachmann",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a218",
          "name": "Yeongmin Kim",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a219",
          "name": "Jonas Kohler",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21a",
          "name": "Markos Georgopoulos",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21b",
          "name": "Artsiom Sanakoyeu",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21c",
          "name": "Yuming Du",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21d",
          "name": "Albert Pumarola",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21e",
          "name": "Ali Thabet",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21f",
          "name": "Edgar Sch√∂nfeld",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T14:16:56.000Z",
      "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality\n  Samples with Less Compute",
      "summary": "Despite their remarkable performance, modern Diffusion Transformers are\nhindered by substantial resource requirements during inference, stemming from\nthe fixed and large amount of compute needed for each denoising step. In this\nwork, we revisit the conventional static paradigm that allocates a fixed\ncompute budget per denoising iteration and propose a dynamic strategy instead.\nOur simple and sample-efficient framework enables pre-trained DiT models to be\nconverted into flexible ones -- dubbed FlexiDiT -- allowing them to\nprocess inputs at varying compute budgets. We demonstrate how a single\nflexible model can generate images without any drop in quality, while\nreducing the required FLOPs by more than 40\\% compared to their static\ncounterparts, for both class-conditioned and text-conditioned image generation.\nOur method is general and agnostic to input and conditioning modalities. We\nshow how our approach can be readily extended for video generation, where\nFlexiDiT models generate samples with up to 75\\% less compute without\ncompromising performance.",
      "upvotes": 6,
      "discussionId": "67c14529af5eaa8dd062a38c"
    },
    "publishedAt": "2025-02-28T00:10:30.864Z",
    "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6246
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.19587",
      "authors": [
        {
          "_id": "67c13aa6a43d7939d60eb02e",
          "name": "Lola Le Breton",
          "hidden": false
        },
        {
          "_id": "67c13aa6a43d7939d60eb02f",
          "name": "Quentin Fournier",
          "hidden": false
        },
        {
          "_id": "67c13aa6a43d7939d60eb030",
          "name": "Mariam El Mezouar",
          "hidden": false
        },
        {
          "_id": "67c13aa6a43d7939d60eb031",
          "name": "Sarath Chandar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T22:00:22.000Z",
      "title": "NeoBERT: A Next-Generation BERT",
      "summary": "Recent innovations in architecture, pre-training, and fine-tuning have led to\nthe remarkable in-context learning and reasoning abilities of large\nauto-regressive language models such as LLaMA and DeepSeek. In contrast,\nencoders like BERT and RoBERTa have not seen the same level of progress despite\nbeing foundational for many downstream NLP applications. To bridge this gap, we\nintroduce NeoBERT, a next-generation encoder that redefines the capabilities of\nbidirectional models by integrating state-of-the-art advancements in\narchitecture, modern data, and optimized pre-training methodologies. NeoBERT is\ndesigned for seamless adoption: it serves as a plug-and-play replacement for\nexisting base models, relies on an optimal depth-to-width ratio, and leverages\nan extended context length of 4,096 tokens. Despite its compact 250M parameter\nfootprint, it achieves state-of-the-art results on the massive MTEB benchmark,\noutperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under\nidentical fine-tuning conditions. In addition, we rigorously evaluate the\nimpact of each modification on GLUE and design a uniform fine-tuning and\nevaluation framework for MTEB. We release all code, data, checkpoints, and\ntraining scripts to accelerate research and real-world adoption.",
      "upvotes": 5,
      "discussionId": "67c13aa7a43d7939d60eb065"
    },
    "publishedAt": "2025-02-28T03:27:32.294Z",
    "title": "NeoBERT: A Next-Generation BERT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19587.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6317233cc92fd6fee317e030",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png",
      "fullname": "Tom Aarsen",
      "name": "tomaarsen",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 1591
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20307",
      "authors": [
        {
          "_id": "67c1460201cef6d4b9b9ac73",
          "name": "Xiuli Bi",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac74",
          "name": "Jianfei Yuan",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac75",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac76",
          "name": "Yong Zhang",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac77",
          "name": "Xiaodong Cun",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac78",
          "name": "Chi-Man Pun",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac79",
          "name": "Bin Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T17:33:51.000Z",
      "title": "Mobius: Text to Seamless Looping Video Generation via Latent Shift",
      "summary": "We present Mobius, a novel method to generate seamlessly looping videos from\ntext descriptions directly without any user annotations, thereby creating new\nvisual materials for the multi-media presentation. Our method repurposes the\npre-trained video latent diffusion model for generating looping videos from\ntext prompts without any training. During inference, we first construct a\nlatent cycle by connecting the starting and ending noise of the videos. Given\nthat the temporal consistency can be maintained by the context of the video\ndiffusion model, we perform multi-frame latent denoising by gradually shifting\nthe first-frame latent to the end in each step. As a result, the denoising\ncontext varies in each step while maintaining consistency throughout the\ninference process. Moreover, the latent cycle in our method can be of any\nlength. This extends our latent-shifting approach to generate seamless looping\nvideos beyond the scope of the video diffusion model's context. Unlike previous\ncinemagraphs, the proposed method does not require an image as appearance,\nwhich will restrict the motions of the generated results. Instead, our method\ncan produce more dynamic motion and better visual quality. We conduct multiple\nexperiments and comparisons to verify the effectiveness of the proposed method,\ndemonstrating its efficacy in different scenarios. All the code will be made\navailable.",
      "upvotes": 5,
      "discussionId": "67c1460501cef6d4b9b9addf"
    },
    "publishedAt": "2025-02-28T00:14:01.841Z",
    "title": "Mobius: Text to Seamless Looping Video Generation via Latent Shift",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20307.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6246
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20127",
      "authors": [
        {
          "_id": "67c12de08cd49ca63e230b99",
          "user": {
            "_id": "654da66fb36f85a025bc24b6",
            "avatarUrl": "/avatars/e5542856ab4bf1845e8f546b5f17cd99.svg",
            "isPro": false,
            "fullname": "Zexiong Ma",
            "user": "mizersy",
            "type": "user"
          },
          "name": "Zexiong Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-28T09:28:35.503Z",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9a",
          "name": "Chao Peng",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9b",
          "name": "Pengfei Gao",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9c",
          "name": "Xiangxin Meng",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9d",
          "name": "Yanzhen Zou",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9e",
          "name": "Bing Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T14:19:45.000Z",
      "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
      "summary": "Mainstream issue-resolving frameworks predominantly rely on commercial\nmodels, leading to high costs and privacy concerns. Existing training\napproaches for issue resolving struggle with poor generalization and fail to\nfully leverage open-source development resources. We propose Subtask-oriented\nReinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue\nresolving capability of LLMs. We decomposes issue resolving into structured\nsubtasks: file localization, function localization, line localization, and code\nedit generation. SoRFT consists of two training stages: (1) rejection-sampled\nsupervised fine-tuning, Chain of Thought (CoT) data is filtered using\nground-truth before fine-tuning the LLM, and (2) rule-based reinforcement\nlearning, which leverages PPO with ground-truth based rewards. We evaluate the\nSoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving\nstate-of-the-art (SOTA) performance among open-source models (e.g., resolve\n21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental\nresults demonstrate that SoRFT significantly enhances issue-resolving\nperformance, improves model generalization, and provides a cost-efficient\nalternative to commercial models.",
      "upvotes": 5,
      "discussionId": "67c12de08cd49ca63e230bd1"
    },
    "publishedAt": "2025-02-27T22:38:04.562Z",
    "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20127.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654da66fb36f85a025bc24b6",
      "avatarUrl": "/avatars/e5542856ab4bf1845e8f546b5f17cd99.svg",
      "fullname": "Zexiong Ma",
      "name": "mizersy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20172",
      "authors": [
        {
          "_id": "67c17b8f60206395233b7e46",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "67c17b8f60206395233b7e47",
          "name": "Shuai Bai",
          "hidden": false
        },
        {
          "_id": "67c17b8f60206395233b7e48",
          "name": "Wenhao Chai",
          "hidden": false
        },
        {
          "_id": "67c17b8f60206395233b7e49",
          "name": "Weichu Xie",
          "hidden": false
        },
        {
          "_id": "67c17b8f60206395233b7e4a",
          "name": "Haozhe Zhao",
          "hidden": false
        },
        {
          "_id": "67c17b8f60206395233b7e4b",
          "name": "Leon Vinci",
          "hidden": false
        },
        {
          "_id": "67c17b8f60206395233b7e4c",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "67c17b8f60206395233b7e4d",
          "name": "Baobao Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T15:08:39.000Z",
      "title": "Multimodal Representation Alignment for Image Generation: Text-Image\n  Interleaved Control Is Easier Than You Think",
      "summary": "The field of advanced text-to-image generation is witnessing the emergence of\nunified frameworks that integrate powerful text encoders, such as CLIP and T5,\nwith Diffusion Transformer backbones. Although there have been efforts to\ncontrol output images with additional conditions, like canny and depth map, a\ncomprehensive framework for arbitrary text-image interleaved control is still\nlacking. This gap is especially evident when attempting to merge concepts or\nvisual elements from multiple images in the generation process. To mitigate the\ngap, we conducted preliminary experiments showing that large multimodal models\n(LMMs) offer an effective shared representation space, where image and text can\nbe well-aligned to serve as a condition for external diffusion models. Based on\nthis discovery, we propose Dream Engine, an efficient and unified framework\ndesigned for arbitrary text-image interleaved control in image generation\nmodels. Building on powerful text-to-image models like SD3.5, we replace the\noriginal text-only encoders by incorporating versatile multimodal information\nencoders such as QwenVL. Our approach utilizes a two-stage training paradigm,\nconsisting of joint text-image alignment and multimodal interleaved instruction\ntuning. Our experiments demonstrate that this training method is effective,\nachieving a 0.69 overall score on the GenEval benchmark, and matching the\nperformance of state-of-the-art text-to-image models like SD3.5 and FLUX.",
      "upvotes": 4,
      "discussionId": "67c17b9160206395233b7e9c"
    },
    "publishedAt": "2025-02-28T04:02:19.534Z",
    "title": "Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20172.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 31
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.19735",
      "authors": [
        {
          "_id": "67c1438fd7ffcd1cab1fc412",
          "user": {
            "_id": "6727998d4fc2e4f7cc0c85d3",
            "avatarUrl": "/avatars/ac18eaadd606f7fae64996502f393cf2.svg",
            "isPro": false,
            "fullname": "he",
            "user": "boommmmm",
            "type": "user"
          },
          "name": "Minggui He",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-28T05:03:12.675Z",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc413",
          "name": "Yilun Liu",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc414",
          "name": "Shimin Tao",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc415",
          "name": "Yuanchang Luo",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc416",
          "name": "Hongyong Zeng",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc417",
          "name": "Chang Su",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc418",
          "name": "Li Zhang",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc419",
          "name": "Hongxia Ma",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41a",
          "name": "Daimeng Wei",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41b",
          "name": "Weibin Meng",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41c",
          "name": "Hao Yang",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41d",
          "name": "Boxing Chen",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41e",
          "name": "Osamu Yoshie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T03:57:00.000Z",
      "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning\n  Learning",
      "summary": "Despite recent breakthroughs in reasoning-enhanced large language models\n(LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine\ntranslation (MT), where human translators naturally employ structured,\nmulti-layered reasoning chain-of-thoughts (CoTs), is yet underexplored.\nExisting methods either design a fixed CoT tailored for a specific MT sub-task\n(e.g., literature translation), or rely on synthesizing CoTs unaligned with\nhumans and supervised fine-tuning (SFT) prone to catastrophic forgetting,\nlimiting their adaptability to diverse translation scenarios. This paper\nintroduces R1-Translator (R1-T1), a novel framework to achieve inference-time\nreasoning for general MT via reinforcement learning (RL) with human-aligned\nCoTs comprising six common patterns. Our approach pioneers three innovations:\n(1) extending reasoning-based translation beyond MT sub-tasks to six languages\nand diverse tasks (e.g., legal/medical domain adaptation, idiom resolution);\n(2) formalizing six expert-curated CoT templates that mirror hybrid human\nstrategies like context-aware paraphrasing and back translation; and (3)\nenabling self-evolving CoT discovery and anti-forgetting adaptation through RL\nwith KL-constrained rewards. Experimental results indicate a steady translation\nperformance improvement in 21 languages and 80 translation directions on\nFlores-101 test set, especially on the 15 languages unseen from training, with\nits general multilingual abilities preserved compared with plain SFT.",
      "upvotes": 3,
      "discussionId": "67c14390d7ffcd1cab1fc479"
    },
    "publishedAt": "2025-02-28T00:03:34.893Z",
    "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6246
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.19459",
      "authors": [
        {
          "_id": "67c185f46a31b8fe77434551",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "67c185f46a31b8fe77434552",
          "name": "Baoxiong Jia",
          "hidden": false
        },
        {
          "_id": "67c185f46a31b8fe77434553",
          "name": "Ruijie Lu",
          "hidden": false
        },
        {
          "_id": "67c185f46a31b8fe77434554",
          "name": "Junfeng Ni",
          "hidden": false
        },
        {
          "_id": "67c185f46a31b8fe77434555",
          "name": "Song-Chun Zhu",
          "hidden": false
        },
        {
          "_id": "67c185f46a31b8fe77434556",
          "name": "Siyuan Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T10:25:32.000Z",
      "title": "Building Interactable Replicas of Complex Articulated Objects via\n  Gaussian Splatting",
      "summary": "Building articulated objects is a key challenge in computer vision. Existing\nmethods often fail to effectively integrate information across different object\nstates, limiting the accuracy of part-mesh reconstruction and part dynamics\nmodeling, particularly for complex multi-part articulated objects. We introduce\nArtGS, a novel approach that leverages 3D Gaussians as a flexible and efficient\nrepresentation to address these issues. Our method incorporates canonical\nGaussians with coarse-to-fine initialization and updates for aligning\narticulated part information across different object states, and employs a\nskinning-inspired part dynamics modeling module to improve both part-mesh\nreconstruction and articulation learning. Extensive experiments on both\nsynthetic and real-world datasets, including a new benchmark for complex\nmulti-part objects, demonstrate that ArtGS achieves state-of-the-art\nperformance in joint parameter estimation and part mesh reconstruction. Our\napproach significantly improves reconstruction quality and efficiency,\nespecially for multi-part articulated objects. Additionally, we provide\ncomprehensive analyses of our design choices, validating the effectiveness of\neach component to highlight potential areas for future improvement.",
      "upvotes": 1,
      "discussionId": "67c185f66a31b8fe774345d2"
    },
    "publishedAt": "2025-02-28T04:47:08.197Z",
    "title": "Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19459.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c7a33121bd95f80ed74652",
      "avatarUrl": "/avatars/7dd59afea785a2bff0ec2b757abd474e.svg",
      "fullname": "Siyuan Huang",
      "name": "thuhsy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]