[
  {
    "paper": {
      "id": "2505.23747",
      "authors": [
        {
          "_id": "68391565d762b7c617b1ba81",
          "name": "Diankun Wu",
          "hidden": false
        },
        {
          "_id": "68391565d762b7c617b1ba82",
          "name": "Fangfu Liu",
          "hidden": false
        },
        {
          "_id": "68391565d762b7c617b1ba83",
          "name": "Yi-Hsin Hung",
          "hidden": false
        },
        {
          "_id": "68391565d762b7c617b1ba84",
          "name": "Yueqi Duan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6505a02f9310ce8c400edc63/eb5xv9-rlab_DrHLk4BMq.mp4"
      ],
      "publishedAt": "2025-05-29T17:59:04.000Z",
      "submittedOnDailyAt": "2025-05-30T00:56:58.237Z",
      "title": "Spectro-MLLM: Mejora de la capacidad de MLLM para el entendimiento espectral basado en visión",
      "submittedOnDailyBy": {
        "_id": "6505a02f9310ce8c400edc63",
        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
        "isPro": false,
        "fullname": "Fangfu Liu",
        "user": "Liuff23",
        "type": "user"
      },
      "summary": "El reciente desarrollo de los modelos de múltiples imágenes y modelos de lenguaje (MLLM) ha mejorado significativamente el rendimiento de tareas visuales bidimensionales. Sin embargo, el aumento de la conocida espacialidad es un problema complejo. Los actuales MLLM 3D utilizan datos 3D o 2.5D adicionales para incluir la reconocción espacial, pero no son útiles para casos donde solo se tiene entrada 2D (gráficos, imágenes, videos). En este artículo, se presenta un nuevo marco de trabajo llamado \"Spatial-MLLM\" para hacer inferencias espaciales a partir de observaciones 2D. Una diferencia con los MLLM simples de video es que utiliza un encoder visual basado en CLIP para optimizar la comprensión del significado. Nuestra principal idea es que la fuerte estructuración previa de un modelo de generalización visual puede ser desbloqueada. Específicamente, proponemos una arquitectura de doble encoder que utiliza un encoder visuo-2D entrenado para extraer características significativas y un encoder espacial inicializado con el modelo de generalización visual para extraer características estructuradas 3D. Posteriormente, se integran estas características en tokens visuales. Además, se propone una estrategia de muestreo de frames para la reconocimiento espacial que selecciona frames con información espacial durante el proceso de inferencia. De esta manera, se asegura que el modelo se concentre en frames necesarios para la inferencia espacial. Además de mejorar la arquitectura, se construye el dataset Spatial-MLLM-120k y se entrena el modelo utilizando sub-provisión y GRPO. Los resultados de validación con diferentes conjuntos de datos de la vida real muestran que nuestro MLLM espacial logra el mejor rendimiento en tareas de comprensión y inferencia espaciales. Página del proyecto: https://diankun-wu.github.io/Spatial-MLLM/",
      "upvotes": 39,
      "discussionId": "68391566d762b7c617b1bae5",
      "projectPage": "https://diankun-wu.github.io/Spatial-MLLM/",
      "githubRepo": "https://github.com/diankun-wu/Spatial-MLLM",
      "ai_summary": "Spatial-MLLM improves spatial reasoning in multimodal large language models using a dual-encoder architecture with pretrained 2D and 3D structure encoders, achieving state-of-the-art performance on visual spatial tasks.",
      "ai_keywords": [
        "spatial-mllm",
        "dual-encoder architecture",
        "visual geometry foundation model",
        "CLIP-based visual encoders",
        "semantic features",
        "3D structure features",
        "unified visual tokens",
        "space-aware frame sampling",
        "supervised fine-tuning",
        "GRPO",
        "spatial understanding",
        "spatial reasoning"
      ]
    },
    "publishedAt": "2025-05-29T13:59:04.000Z",
    "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial\n  Intelligence",
    "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6505a02f9310ce8c400edc63/eb5xv9-rlab_DrHLk4BMq.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23747.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6505a02f9310ce8c400edc63",
      "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
      "fullname": "Fangfu Liu",
      "name": "Liuff23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23621",
      "authors": [
        {
          "_id": "68391925d73e6015a1b0f305",
          "name": "Zheyuan Yang",
          "hidden": false
        },
        {
          "_id": "68391925d73e6015a1b0f306",
          "name": "Lyuhao Chen",
          "hidden": false
        },
        {
          "_id": "68391925d73e6015a1b0f307",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "68391925d73e6015a1b0f308",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:43.117Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:28:50.000Z",
      "submittedOnDailyAt": "2025-05-30T01:04:47.042Z",
      "title": "Escalado en la inferencia de tablas",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "En este estudio, se presenta una investigación inicial para probar la escalabilidad en el proceso de inferencia. Se desarrollan dos estrategias posteriores a la entrenamiento para evaluar si es posible escalar en el proceso de inferencia: la desventilación desde DeepSeek-R1 y la compensación de aprendizaje reforzado con justificación (RLVR). En la desventilación, se utiliza un conjunto de datos de trazas de inferencia generadas por DeepSeek-R1 para ajustar un modelo de LLM a Table-R1-SFT. En RLVR, se propone una función de recompensa relacionada con la tarea y se aplica el algoritmo GRPO para obtener el modelo Table-R1-Zero. Los modelos de la serie Table-R1 se evalúan en diferentes tareas de inferencia de tablas, y particularmente, el modelo Table-R1-Zero compara o supera el rendimiento de GPT-4.1 y DeepSeek-R1 utilizando solo un modelo de LLM de 7B parámetros. Además, muestra un fuerte rendimiento de generalización en conjuntos de datos fuera de la área. A través de pruebas de eliminación ampliada y análisis cualitativo, se descubre la ajuste de instrucciones, la elección de la estructura del modelo, la generalización entre tareas y la aparición de habilidades importantes de inferencia de tablas durante el entrenamiento RL.",
      "upvotes": 38,
      "discussionId": "68391928d73e6015a1b0f3a8",
      "githubRepo": "https://github.com/Table-R1/Table-R1",
      "ai_summary": "Two post-training strategies, distillation and RLVR, enable inference-time scaling in table reasoning tasks, resulting in a model (Table-R1-Zero) that matches GPT-4.1's performance using fewer parameters and shows strong generalization.",
      "ai_keywords": [
        "distillation",
        "reinforcement learning",
        "verifiable rewards",
        "RLVR",
        "reasoning traces",
        "DeepSeek-R1",
        "LLMs",
        "Table-R1-SFT",
        "GRPO",
        "Table-R1-Zero",
        "short-form QA",
        "fact verification",
        "free-form QA",
        "instruction tuning",
        "model architecture choices",
        "cross-task generalization",
        "table reasoning skills"
      ]
    },
    "publishedAt": "2025-05-29T12:28:50.000Z",
    "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
    "summary": "In this work, we present the first study to explore inference-time scaling on\ntable reasoning tasks. We develop and evaluate two post-training strategies to\nenable inference-time scaling: distillation from frontier model reasoning\ntraces and reinforcement learning with verifiable rewards (RLVR). For\ndistillation, we introduce a large-scale dataset of reasoning traces generated\nby DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For\nRLVR, we propose task-specific verifiable reward functions and apply the GRPO\nalgorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series\nmodels across diverse table reasoning tasks, including short-form QA, fact\nverification, and free-form QA. Notably, the Table-R1-Zero model matches or\nexceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a\n7B-parameter LLM. It also demonstrates strong generalization to out-of-domain\ndatasets. Extensive ablation and qualitative analyses reveal the benefits of\ninstruction tuning, model architecture choices, and cross-task generalization,\nas well as emergence of essential table reasoning skills during RL training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22653",
      "authors": [
        {
          "_id": "6838bb282b382ba50bdcddc4",
          "name": "Ang Lv",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc5",
          "user": {
            "_id": "6622443b9b0614a760dd8123",
            "avatarUrl": "/avatars/acb6c1c9c429af1112530dcf76a8e420.svg",
            "isPro": false,
            "fullname": "Ruobing Xie",
            "user": "Ruobing-Xie",
            "type": "user"
          },
          "name": "Ruobing Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:45.319Z",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc6",
          "name": "Xingwu Sun",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc7",
          "name": "Zhanhui Kang",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc8",
          "name": "Rui Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:59:03.000Z",
      "submittedOnDailyAt": "2025-05-30T00:44:54.555Z",
      "title": "Dentro de Deng Rui se esconde una profundidad de sabiduría: un recompensa confusa para el razonamiento sobre el significado del aprendizaje.",
      "submittedOnDailyBy": {
        "_id": "64b8ca3c5067873176d4b436",
        "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg",
        "isPro": false,
        "fullname": "AngLv",
        "user": "AngLv",
        "type": "user"
      },
      "summary": "Recientemente, los estudios sobre la computación lógica de los modelos de lenguaje de aprendizaje posterior (LLMs) en aprendizaje reforzado (RL) se centran en tareas que pueden ser validadas, como el resolución de problemas matemáticos. En contraste, nuestro trabajo investiga el impacto de los modelos de recompensa en los LLMs en la realidad. Hemos encontrado que los LLMs tienen una fuerte capacidad para enfrentar ruidos fuertes en las recompensas. Por ejemplo, al invertir directamente el 40% de la salida de la función de recompensa para tareas matemáticas, el modelo Qwen-2.5-7B logra converger rápidamente, aumentando la precisión en el 5% a 72% en comparación con la precisión sin ruido de recompensa. Sorprendentemente, cuando se proporciona recompensa solo por la aparición de frases de razonamiento (patrón de razonamiento de recompensa, RPR), el modelo logra alcanzar una precisión del 70% o más, aunque no se verifica la exactitud del respuesta. Comparado con modelos que utilizan recompensas estrictas y precisas, el rendimiento bajo se mejoró. Reconocemos que lo más importante para los resultados finales es el proceso de razonamiento más que la razón final. Combinamos RPR con modelos de ruido de recompensa, lo que ajusta los modelos de ruido de recompensa, mitiga la posibilidad de campos potenciales negativos y mejora el rendimiento de tareas abiertas en los LLMs. Estos hallazgos mejoran las capacidades básicas del modelo durante el proceso de entrenamiento y ofrecen consejos sobre el desarrollo de tecnologías posteriores. Nuestro código y script están disponibles en https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.",
      "upvotes": 36,
      "discussionId": "6838bb2a2b382ba50bdcde1b",
      "githubRepo": "https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason",
      "ai_summary": "LLMs exhibit robustness to reward noise during post-training and achieve high performance using reasoning pattern rewards (RPR) in conjunction with noisy reward models.",
      "ai_keywords": [
        "large language models (LLMs)",
        "post-training",
        "reinforcement learning (RL)",
        "reward noise",
        "reward models",
        "rapid convergence",
        "reasoning pattern reward (RPR)",
        "false negatives",
        "open-ended tasks"
      ]
    },
    "publishedAt": "2025-05-28T13:59:03.000Z",
    "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason",
    "summary": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22653.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b8ca3c5067873176d4b436",
      "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg",
      "fullname": "AngLv",
      "name": "AngLv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23693",
      "authors": [
        {
          "_id": "68390e95b85141ce6c11b50f",
          "user": {
            "_id": "64dc29d9b5d625e0e9a6ecb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
            "isPro": false,
            "fullname": "Tingyu Song",
            "user": "songtingyu",
            "type": "user"
          },
          "name": "Tingyu Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:12.429Z",
          "hidden": false
        },
        {
          "_id": "68390e95b85141ce6c11b510",
          "user": {
            "_id": "66e83ec5deb449d8d856e78d",
            "avatarUrl": "/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg",
            "isPro": false,
            "fullname": "Tongyan Hu",
            "user": "entropyhu",
            "type": "user"
          },
          "name": "Tongyan Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:15.154Z",
          "hidden": false
        },
        {
          "_id": "68390e95b85141ce6c11b511",
          "name": "Guo Gan",
          "hidden": false
        },
        {
          "_id": "68390e95b85141ce6c11b512",
          "name": "Yilun Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64dc29d9b5d625e0e9a6ecb9/ZnHMXas2Khv3qS0RIvYR2.png"
      ],
      "publishedAt": "2025-05-29T17:31:13.000Z",
      "submittedOnDailyAt": "2025-05-30T00:24:42.566Z",
      "title": "VF-Eval: Evaluación de estructuras multimodal de LLMs para generar retroalimentación sobre vídeos de AIGC",
      "submittedOnDailyBy": {
        "_id": "64dc29d9b5d625e0e9a6ecb9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
        "isPro": false,
        "fullname": "Tingyu Song",
        "user": "songtingyu",
        "type": "user"
      },
      "summary": "MLLMs están realizando muchos estudios sobre preguntas de películas recientes. Sin embargo, actualmente la evaluación se centra en películas naturales y ignora contenidos de producción artificial (AIGC) de películas sintéticas. Por otro lado, se evalúa la calidad de películas generadas con MLLMs, aunque el conocimiento de MLLMs sobre películas de AIGC no ha sido ampliamente investigado. En este contexto, proponemos un nuevo criterio de evaluación llamado VF-Eval, que introduce cuatro tareas: validación de coherencia, reconocimiento de errores, detección de tipos de errores y evaluación de razones, para evaluar con precisión la capacidad de entender películas de AIGC. Mediante VF-Eval, evaluamos 13 MLLMs más recientes y, aunque el modelo GPT-4.1 destacó por su rendimiento, no logró alcanzar un desempeño consistentemente excelente en todos los casos. Esto resalta claramente la dificultad del nuevo criterio de evaluación. Además, mediante experimentos RePrompt, investigamos cómo VF-Eval puede aplicarse de manera práctica en la generación de películas, demostrando que MLLMs pueden ser mejorados con retroalimentación humana, lo que puede ser útil en la generación de películas.",
      "upvotes": 34,
      "discussionId": "68390e96b85141ce6c11b55c",
      "githubRepo": "https://github.com/SighingSnow/VF-EVAL",
      "ai_summary": "A new benchmark, VF-Eval, evaluates the capabilities of MLLMs in interpreting AI-generated content videos across four tasks, highlighting challenges and demonstrating benefits in video generation through human feedback alignment.",
      "ai_keywords": [
        "MLLMs",
        "video question answering",
        "synthetic videos",
        "AI-generated content (AIGC)",
        "VF-Eval",
        "coherence validation",
        "error awareness",
        "error type detection",
        "reasoning evaluation",
        "GPT-4.1",
        "RePrompt"
      ]
    },
    "publishedAt": "2025-05-29T13:31:13.000Z",
    "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC\n  Videos",
    "summary": "MLLMs have been widely studied for video question answering recently.\nHowever, most existing assessments focus on natural videos, overlooking\nsynthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in\nvideo generation rely on MLLMs to evaluate the quality of generated videos, but\nthe capabilities of MLLMs on interpreting AIGC videos remain largely\nunderexplored. To address this, we propose a new benchmark, VF-Eval, which\nintroduces four tasks-coherence validation, error awareness, error type\ndetection, and reasoning evaluation-to comprehensively evaluate the abilities\nof MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that\neven the best-performing model, GPT-4.1, struggles to achieve consistently good\nperformance across all tasks. This highlights the challenging nature of our\nbenchmark. Additionally, to investigate the practical applications of VF-Eval\nin improving video generation, we conduct an experiment, RePrompt,\ndemonstrating that aligning MLLMs more closely with human feedback can benefit\nvideo generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64dc29d9b5d625e0e9a6ecb9/ZnHMXas2Khv3qS0RIvYR2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23693.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dc29d9b5d625e0e9a6ecb9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
      "fullname": "Tingyu Song",
      "name": "songtingyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23762",
      "authors": [
        {
          "_id": "68391353d8c153d346e1ddb5",
          "user": {
            "_id": "637f347a52229c639211bee8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg",
            "isPro": false,
            "fullname": "Chenyu Yang",
            "user": "cyyang822",
            "type": "user"
          },
          "name": "Chenyu Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:52.043Z",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb6",
          "name": "Shiqian Su",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb7",
          "name": "Shi Liu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb8",
          "name": "Xuan Dong",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb9",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddba",
          "name": "Weijie Su",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbb",
          "name": "Xuehui Wang",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbc",
          "name": "Zhaoyang Liu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbd",
          "name": "Jinguo Zhu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbe",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbf",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddc0",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddc1",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddc2",
          "name": "Jifeng Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:51.000Z",
      "submittedOnDailyAt": "2025-05-30T00:41:52.864Z",
      "title": "ZeroGUI: Automatización de la aprendizaje de GUI en línea basada en el costo de cero de Himañko",
      "submittedOnDailyBy": {
        "_id": "637f347a52229c639211bee8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg",
        "isPro": false,
        "fullname": "Chenyu Yang",
        "user": "cyyang822",
        "type": "user"
      },
      "summary": "El rápido desarrollo de los grandes modelos de lenguaje de visión (VLMs) ha impulsado el desarrollo de agentes de interfaz gráfica basados en predicciones, dotandoles de la capacidad de reconocer y manipular la interfaz gráfica de usuario (GUI) para satisfacer automáticamente las instrucciones del usuario. Sin embargo, el enfoque actual utiliza generalmente un marco de aprendizaje offline, y presenta dos limitaciones clave: 1) el esfuerzo de anotación manual de alta calidad para ajustes de elementos y sublexis de acciones, y 2) una adaptación limitada a entornos dinámicos interactivos. Para superar estas limitaciones, se propone ZeroGUI, un marco de aprendizaje en línea flexible. ZeroGUI incluye: 1) la generación automática de tareas basada en VLMs, creando objetivos de entrenamiento diversos en el entorno actual, 2) la evaluación automática de recompensa basada en VLMs, evaluando la éxito de tareas creadas por el usuario sin necesidad de funciones de evaluación, y 3) un aprendizaje en línea en dos etapas, caracterizado por su interacción continua con el entorno de GUI. Los experimentos con los dos avanzados agentes de GUI, UI-TARS y Aguvis, muestran que ZeroGUI mejoró significativamente el rendimiento en los entornos OSWorld y AndroidLab. El código está disponible en https://github.com/OpenGVLab/ZeroGUI.",
      "upvotes": 33,
      "discussionId": "68391354d8c153d346e1de1a",
      "githubRepo": "https://github.com/OpenGVLab/ZeroGUI",
      "ai_summary": "ZeroGUI is an online learning framework that uses Vision-Language Models for task generation and reward estimation, enhancing GUI Agents' performance with minimal human intervention.",
      "ai_keywords": [
        "Vision-Language Models",
        "GUI Agents",
        "element grounding",
        "action supervision",
        "offline learning",
        "automatic task generation",
        "automatic reward estimation",
        "reinforcement learning",
        "OSWorld",
        "AndroidLab"
      ]
    },
    "publishedAt": "2025-05-29T13:59:51.000Z",
    "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
    "summary": "The rapid advancement of large Vision-Language Models (VLMs) has propelled\nthe development of pure-vision-based GUI Agents, capable of perceiving and\noperating Graphical User Interfaces (GUI) to autonomously fulfill user\ninstructions. However, existing approaches usually adopt an offline learning\nframework, which faces two core limitations: (1) heavy reliance on high-quality\nmanual annotations for element grounding and action supervision, and (2)\nlimited adaptability to dynamic and interactive environments. To address these\nlimitations, we propose ZeroGUI, a scalable, online learning framework for\nautomating GUI Agent training at Zero human cost. Specifically, ZeroGUI\nintegrates (i) VLM-based automatic task generation to produce diverse training\ngoals from the current environment state, (ii) VLM-based automatic reward\nestimation to assess task success without hand-crafted evaluation functions,\nand (iii) two-stage online reinforcement learning to continuously interact with\nand learn from GUI environments. Experiments on two advanced GUI Agents\n(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance\nacross OSWorld and AndroidLab environments. The code is available at\nhttps://github.com/OpenGVLab/ZeroGUI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23762.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f347a52229c639211bee8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg",
      "fullname": "Chenyu Yang",
      "name": "cyyang822",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23604",
      "authors": [
        {
          "_id": "68390da8f527444e97c4ab95",
          "name": "Guangtao Zeng",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab96",
          "user": {
            "_id": "6553c985a7aded0380b5f928",
            "avatarUrl": "/avatars/36109d6f536d2b34d98822b88eac9608.svg",
            "isPro": false,
            "fullname": "Maohao Shen",
            "user": "maohaos2",
            "type": "user"
          },
          "name": "Maohao Shen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T01:45:13.275Z",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab97",
          "name": "Delin Chen",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab98",
          "name": "Zhenting Qi",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab99",
          "name": "Subhro Das",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9a",
          "name": "Dan Gutfreund",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9b",
          "name": "David Cox",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9c",
          "name": "Gregory Wornell",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9d",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9e",
          "name": "Zhang-Wei Hong",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9f",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:15:36.000Z",
      "submittedOnDailyAt": "2025-05-30T00:43:27.169Z",
      "title": "Satori-SWE: Ejemplo de un avanzado horario de pruebas eficaz para el evolucionamiento de la ingeniería de software",
      "submittedOnDailyBy": {
        "_id": "60ad0de755f970745d4ec28d",
        "avatarUrl": "/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg",
        "isPro": true,
        "fullname": "GtZeng",
        "user": "chaoscodes",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje (LMs) muestran un buen desempeño en marcos de referencia de codificación estándar, pero en tareas de desarrollo de software reales, especialmente en SWE-Bench, en la resolución de problemas de GitHub, presentan dificultades. Esto es particularmente cierto para modelos con menos de 100B parámetros. En términos prácticos, los modelos pequeños son preferidos debido a que su costo de cálculo es más bajo, aunque su mejora en rendimiento es más difícil. El enfoque actual se basa principalmente en la micro-fine-tuning (SFT) con datos de alta calidad, lo cual es costoso desde el punto de vista de escala. Una alternativa es el escalado durante la prueba, donde se generan múltiples salidas y se calculan puntuaciones mediante valoración para elegir la mejor opción. Este enfoque es efectivo, pero su aplicación es limitada debido a la necesidad de muestras excesivas y los altos costos de valoración. Proponemos un método eficiente de muestreo, el escalado evolutivo (EvoScale), que trata las salidas como parte de un proceso evolutivo. Utilizando cambios súbitos y selecciones, EvoScale mejora recurrentemente las salidas y moviliza la distribución de las salidas hacia el área de altas puntuaciones, reduciendo el número de muestras necesarias para encontrar la respuesta correcta. Para reducir el sobrecarga de muestreos repetidos y la sobrecarga de selección, se utiliza aprendizaje por refuerzo (RL) para auto-evolucionar el modelo. Durante la inferencia, el modelo no depende de valoraciones externas, sino que se entrena con puntuaciones auto-mejoradas y mejora su generación en cada iteración. En SWE-Bench-Verified, el EvaScale evaluado demostró que nuestro modelo de 32B, Satori-SWE-32B, puede superar el desempeño de modelos con más de 100B parámetros. Los códigos, datos y modelos están completamente abiertos.",
      "upvotes": 18,
      "discussionId": "68390da9f527444e97c4abdf",
      "projectPage": "https://satori-reasoning.github.io/blog/satori-swe/",
      "ai_summary": "EvoScale, an evolutionary and reinforcement learning-based method, enhances small language models' performance on real-world software engineering tasks by iteratively improving and refining outputs.",
      "ai_keywords": [
        "supervised fine-tuning",
        "test-time scaling",
        "Evolutionary Test-Time Scaling",
        "EvoScale",
        "reinforcement learning",
        "SWE-Bench-Verified"
      ]
    },
    "publishedAt": "2025-05-29T12:15:36.000Z",
    "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software\n  Engineering",
    "summary": "Language models (LMs) perform well on standardized coding benchmarks but\nstruggle with real-world software engineering tasks such as resolving GitHub\nissues in SWE-Bench, especially when model parameters are less than 100B. While\nsmaller models are preferable in practice due to their lower computational\ncost, improving their performance remains challenging. Existing approaches\nprimarily rely on supervised fine-tuning (SFT) with high-quality data, which is\nexpensive to curate at scale. An alternative is test-time scaling: generating\nmultiple outputs, scoring them using a verifier, and selecting the best one.\nAlthough effective, this strategy often requires excessive sampling and costly\nscoring, limiting its practical application. We propose Evolutionary Test-Time\nScaling (EvoScale), a sample-efficient method that treats generation as an\nevolutionary process. By iteratively refining outputs via selection and\nmutation, EvoScale shifts the output distribution toward higher-scoring\nregions, reducing the number of samples needed to find correct solutions. To\nreduce the overhead from repeatedly sampling and selection, we train the model\nto self-evolve using reinforcement learning (RL). Rather than relying on\nexternal verifiers at inference time, the model learns to self-improve the\nscores of its own generations across iterations. Evaluated on\nSWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or\nexceed the performance of models with over 100B parameters while using a few\nsamples. Code, data, and models will be fully open-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23604.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60ad0de755f970745d4ec28d",
      "avatarUrl": "/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg",
      "fullname": "GtZeng",
      "name": "chaoscodes",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23359",
      "authors": [
        {
          "_id": "683909a29deef11aa625817c",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa625817d",
          "user": {
            "_id": "62cd7aca7a036fc9941bb2b0",
            "avatarUrl": "/avatars/17a4d27af0243fd7dccf06066f671461.svg",
            "isPro": false,
            "fullname": "kun ouyang",
            "user": "RUBBISHLIKE",
            "type": "user"
          },
          "name": "Kun Ouyang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:22.290Z",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa625817e",
          "name": "Haoning Wu",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa625817f",
          "name": "Yi Liu",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258180",
          "name": "Lin Sui",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258181",
          "name": "Xinhao Li",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258182",
          "name": "Yan Zhong",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258183",
          "name": "Y. Charles",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258184",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258185",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6489761dcaea79f577897f98/48SxCF6GVyMdnPR7vZSgC.mp4"
      ],
      "publishedAt": "2025-05-29T11:33:43.000Z",
      "submittedOnDailyAt": "2025-05-30T01:03:01.950Z",
      "title": "VideoReasonBench: Benchmark de Video: ¿Las MLLMs pueden realizar lógica de razones centrada en la visión para videos complejos?",
      "submittedOnDailyBy": {
        "_id": "6489761dcaea79f577897f98",
        "avatarUrl": "/avatars/8f56dc9c08dc2b672555602d68509a03.svg",
        "isPro": false,
        "fullname": "Yuanxin Liu",
        "user": "lyx97",
        "type": "user"
      },
      "summary": "Recientes estudios indican que la lógica de cadenas largas de puntos continuas (CoT) mejora significativamente el rendimiento de grandes modelos de lenguaje (LLMs) en tareas complejas. Sin embargo, este beneficio aún no se ha visto en el dominio de la comprensión de imágenes. Esto se debe a que los marcadores de evaluación actuales no tienen profundidad lógica. Recientes intentos han propuesto marcadores de evaluación para la lógica de imágenes, pero estos trabajos generalmente se basan en conocimiento y no en contenido visual. Para llenar esta brecha, se presenta VideoReasonBench, un marcador de evaluación que evalúa lógicas visuales complejas. Cada imagen en VideoReasonBench describe el orden de operaciones que pueden ser vistas en alguna parte de la imagen, asegurando una riqueza visual y una alta complejidad lógica. Las preguntas se evalúan mediante una tecnología de lógica de imágenes en tres etapas: memoria de información visual, inferencia de contenido potencial y predicción de información más allá de la imagen. En esta configuración, los modelos deben precisamente recordar varias operaciones de la imagen y ejecutar la lógica de paso a paso para obtener una respuesta final precisa. Al utilizar VideoReasonBench, revisamos 18 de los modelos de lenguaje más avanzados (MLLMs) y observamos que muchos modelos muestran bajos rendimientos en lógicas de imágenes complejas. Por ejemplo, GPT-4o alcanza una precisión de 6.9%, mientras que Gemini-2.5-Pro, con mejora de la memoria, supera a otros modelos con una precisión de 56.0%. La investigación sobre \"escalado en el test\" muestra que, en comparación con casi todos los marcadores de evaluación en imágenes actuales, VideoReasonBench muestra un rendimiento significativamente superior a los modelos de memoria de escala.",
      "upvotes": 18,
      "discussionId": "683909a39deef11aa62581c2",
      "projectPage": "https://llyx97.github.io/video_reason_bench/",
      "githubRepo": "https://github.com/llyx97/video_reason_bench",
      "ai_summary": "A new benchmark, VideoReasonBench, evaluates complex vision-centric video reasoning, finding that extended thinking budgets are crucial for improved performance compared to existing benchmarks.",
      "ai_keywords": [
        "long chain-of-thought reasoning",
        "large language models",
        "video understanding",
        "VideoReasonBench",
        "vision-centric",
        "complex video reasoning",
        "latent state",
        "visual reasoning",
        "step-by-step reasoning",
        "multimodal language models",
        "thinking-enhanced models",
        "test-time scaling"
      ]
    },
    "publishedAt": "2025-05-29T07:33:43.000Z",
    "title": "VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video\n  Reasoning?",
    "summary": "Recent studies have shown that long chain-of-thought (CoT) reasoning can\nsignificantly enhance the performance of large language models (LLMs) on\ncomplex tasks. However, this benefit is yet to be demonstrated in the domain of\nvideo understanding, since most existing benchmarks lack the reasoning depth\nrequired to demonstrate the advantages of extended CoT chains. While recent\nefforts have proposed benchmarks aimed at video reasoning, the tasks are often\nknowledge-driven and do not rely heavily on visual content. To bridge this gap,\nwe introduce VideoReasonBench, a benchmark designed to evaluate vision-centric,\ncomplex video reasoning. To ensure visual richness and high reasoning\ncomplexity, each video in VideoReasonBench depicts a sequence of fine-grained\noperations on a latent state that is only visible in part of the video. The\nquestions evaluate three escalating levels of video reasoning skills: recalling\nobserved visual information, inferring the content of latent states, and\npredicting information beyond the video. Under such task setting, models have\nto precisely recall multiple operations in the video, and perform step-by-step\nreasoning to get correct final answers for these questions. Using\nVideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal\nLLMs (MLLMs), finding that most perform poorly on complex video reasoning,\ne.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced\nGemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our\ninvestigations on \"test-time scaling\" further reveal that extended thinking\nbudget, while offering none or minimal benefits on existing video benchmarks,\nis essential for improving the performance on VideoReasonBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6489761dcaea79f577897f98/48SxCF6GVyMdnPR7vZSgC.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23359.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "6489761dcaea79f577897f98",
      "avatarUrl": "/avatars/8f56dc9c08dc2b672555602d68509a03.svg",
      "fullname": "Yuanxin Liu",
      "name": "lyx97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23716",
      "authors": [
        {
          "_id": "6839217e95fedc63bb4ae475",
          "name": "Lihan Jiang",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae476",
          "user": {
            "_id": "65de9c6cf68c3d3bac330509",
            "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
            "isPro": false,
            "fullname": "Yucheng Mao",
            "user": "matthewmao",
            "type": "user"
          },
          "name": "Yucheng Mao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:34.206Z",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae477",
          "name": "Linning Xu",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae478",
          "name": "Tao Lu",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae479",
          "name": "Kerui Ren",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47a",
          "name": "Yichen Jin",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47b",
          "name": "Xudong Xu",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47c",
          "name": "Mulin Yu",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47d",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47e",
          "name": "Feng Zhao",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47f",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae480",
          "name": "Bo Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:49:56.000Z",
      "submittedOnDailyAt": "2025-05-30T01:42:17.635Z",
      "title": "Feed-forward 3D Gaussian Splatting sin límites de tiempo",
      "submittedOnDailyBy": {
        "_id": "64a4ce8118f4e2529546daef",
        "avatarUrl": "/avatars/6d88aa68eccfa07d2009df405f957fd7.svg",
        "isPro": false,
        "fullname": "Jiang Lihan",
        "user": "lhjiang",
        "type": "user"
      },
      "summary": "AnySplat es una red neuronal de vanguardia para la síntesis visual en colecciones de imágenes no medidas. En contraste con los flujos de rendición neural existentes, que requieren ajustes de la posición de la cámara y la optimización de cada plano, y con los métodos recientes de vanguardia que cargan una gran cantidad de cálculos para manejar visuales complejos, nuestro modelo predice todo en un solo paso de propagación hacia adelante. Para cada plano de la imagen de entrada, se puede obtener la forma y el aspecto del plano, incluyendo un gaussiano 3D, y también el ángulo interno y el aspecto de la cámara correspondiente. Esta disección integrada permite escalar fácilmente conjuntos de datos de múltiples perspectivas capturadas por la cámara, como si no hubiera descripción de la posición. En evaluaciones amplias de 0 shot, AnySplat logra la calidad de un estándar de referencia con interés en la posición, tanto para imágenes poco abundantes como para múltiples perspectivas, y supera las limitaciones de acceso a la posición. Además, en comparación con los campos neurales basados en optimización, nuestro modelo reduce significativamente el retraso de renderización y permite la síntesis de nuevas perspectivas en tiempo real en cualquier configuración de captura. Página del proyecto: https://city-super.github.io/anysplat/",
      "upvotes": 17,
      "discussionId": "6839217f95fedc63bb4ae4d0",
      "projectPage": "https://city-super.github.io/anysplat/",
      "ai_summary": "AnySplat is a feed forward network that performs novel view synthesis without camera poses, using 3D Gaussian primitives and unified design for efficiency and quality across sparse and dense datasets.",
      "ai_keywords": [
        "feed forward network",
        "novel view synthesis",
        "uncalibrated image collections",
        "3D Gaussian primitives",
        "camera intrinsics",
        "camera extrinsics",
        "neural rendering pipelines",
        "per scene optimization",
        "zero shot evaluations",
        "pose aware baselines",
        "pose free approaches",
        "rendering latency",
        "optimization based neural fields"
      ]
    },
    "publishedAt": "2025-05-29T13:49:56.000Z",
    "title": "AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views",
    "summary": "We introduce AnySplat, a feed forward network for novel view synthesis from\nuncalibrated image collections. In contrast to traditional neural rendering\npipelines that demand known camera poses and per scene optimization, or recent\nfeed forward methods that buckle under the computational weight of dense views,\nour model predicts everything in one shot. A single forward pass yields a set\nof 3D Gaussian primitives encoding both scene geometry and appearance, and the\ncorresponding camera intrinsics and extrinsics for each input image. This\nunified design scales effortlessly to casually captured, multi view datasets\nwithout any pose annotations. In extensive zero shot evaluations, AnySplat\nmatches the quality of pose aware baselines in both sparse and dense view\nscenarios while surpassing existing pose free approaches. Moreover, it greatly\nreduce rendering latency compared to optimization based neural fields, bringing\nreal time novel view synthesis within reach for unconstrained capture\nsettings.Project page: https://city-super.github.io/anysplat/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23716.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a4ce8118f4e2529546daef",
      "avatarUrl": "/avatars/6d88aa68eccfa07d2009df405f957fd7.svg",
      "fullname": "Jiang Lihan",
      "name": "lhjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23660",
      "authors": [
        {
          "_id": "683906adf85de1fc563957d8",
          "user": {
            "_id": "648ac3d53470b17ccc90deaf",
            "avatarUrl": "/avatars/35b678baa7a91959f40f704c5de2a3e1.svg",
            "isPro": false,
            "fullname": "Ziteng Gao",
            "user": "sebgao",
            "type": "user"
          },
          "name": "Ziteng Gao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T01:15:28.731Z",
          "hidden": false
        },
        {
          "_id": "683906adf85de1fc563957d9",
          "user": {
            "_id": "63a55320ce5763e06f78519c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671779060549-noauth.jpeg",
            "isPro": false,
            "fullname": "Mike Shou",
            "user": "mikeshou",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T01:15:28.731Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:09:25.000Z",
      "submittedOnDailyAt": "2025-05-30T01:28:22.938Z",
      "title": "D-AR: Difusión por modelo de regresión automática",
      "submittedOnDailyBy": {
        "_id": "648ac3d53470b17ccc90deaf",
        "avatarUrl": "/avatars/35b678baa7a91959f40f704c5de2a3e1.svg",
        "isPro": false,
        "fullname": "Ziteng Gao",
        "user": "sebgao",
        "type": "user"
      },
      "summary": "En este artículo, se presenta el Diffusion via Autoregressive models (D-AR). Este es un nuevo paradigma que reconstruye el proceso de difusión de imágenes como una predicción de siguiente token en el formato estándar de buena anotación. Primero, se diseña un tokenizador que convierte las imágenes en columnas de tokens discretos. Estos tokens pueden ser interpretados en diferentes posiciones en el espacio de píxeles. Según las características de la difusión, estos tokens se ordenan naturalmente en el orden de la corteza. Esto es directamente adecuado para el modelado de anotación. Por lo tanto, se aplica la predicción de siguiente token estándar a estos tokens, sin cambiar la máscara causal y la estrategia de entrenamiento/predicción. Este generación de tokens anotados secuenciales refleja directamente el proceso de difusión en el espacio de imágenes. Es decir, cuando el modelo anota un aumento de token, estos tokens pueden ser interpretados directamente en modo de flujo. Nuestro pipeline soporta previsualizaciones continuas al crear subconjuntos de tokens y permite la síntesis controlada sin ejemplos. En el benchmark estándar de ImageNet, utilizando un Llama de 775M y 256 tokens discretos, se alcanzó una FID de 2.09. Esperamos que nuestra investigación contribuya a la investigación futura de arquitecturas de anotación unificada en síntesis visual. Los códigos y modelos están disponibles en https://github.com/showlab/D-AR.",
      "upvotes": 17,
      "discussionId": "683906b0f85de1fc5639586b",
      "githubRepo": "https://github.com/showlab/D-AR",
      "ai_summary": "Diffusion via Autoregressive models (D-AR) recasts the image diffusion process as a standard autoregressive task, achieving high-quality image generation with consistent previews and layout control using a large language model backbone.",
      "ai_keywords": [
        "Diffusion via Autoregressive models",
        "autoregressive procedure",
        "next-token-prediction",
        "tokenizer",
        "discrete tokens",
        "diffusion denoising",
        "coarse-to-fine order",
        "autoregressive modeling",
        "autoregressive token generation",
        "FID",
        "Llama backbone"
      ]
    },
    "publishedAt": "2025-05-29T13:09:25.000Z",
    "title": "D-AR: Diffusion via Autoregressive Models",
    "summary": "This paper presents Diffusion via Autoregressive models (D-AR), a new\nparadigm recasting the image diffusion process as a vanilla autoregressive\nprocedure in the standard next-token-prediction fashion. We start by designing\nthe tokenizer that converts images into sequences of discrete tokens, where\ntokens in different positions can be decoded into different diffusion denoising\nsteps in the pixel space. Thanks to the diffusion properties, these tokens\nnaturally follow a coarse-to-fine order, which directly lends itself to\nautoregressive modeling. Therefore, we apply standard next-token prediction on\nthese tokens, without modifying any underlying designs (either causal masks or\ntraining/inference strategies), and such sequential autoregressive token\ngeneration directly mirrors the diffusion procedure in image space. That is,\nonce the autoregressive model generates an increment of tokens, we can directly\ndecode these tokens into the corresponding diffusion denoising step in the\nstreaming manner. Our pipeline naturally reveals several intriguing properties,\nfor example, it supports consistent previews when generating only a subset of\ntokens and enables zero-shot layout-controlled synthesis. On the standard\nImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone\nwith 256 discrete tokens. We hope our work can inspire future research on\nunified autoregressive architectures of visual synthesis, especially with large\nlanguage models. Code and models will be available at\nhttps://github.com/showlab/D-AR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23660.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648ac3d53470b17ccc90deaf",
      "avatarUrl": "/avatars/35b678baa7a91959f40f704c5de2a3e1.svg",
      "fullname": "Ziteng Gao",
      "name": "sebgao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23646",
      "authors": [
        {
          "_id": "683946c845636acda08ed401",
          "name": "Zijun Yao",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed402",
          "name": "Yantao Liu",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed403",
          "name": "Yanxu Chen",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed404",
          "name": "Jianhui Chen",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed405",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed406",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed407",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed408",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:53:41.000Z",
      "submittedOnDailyAt": "2025-05-30T04:31:18.157Z",
      "title": "¿Por qué este modelo tiene interés por Hollings?",
      "submittedOnDailyBy": {
        "_id": "62e25e2247678ea5ce1b1786",
        "avatarUrl": "/avatars/1bb32e7597a9b1c89c434cbf550b5382.svg",
        "isPro": false,
        "fullname": "Yantao",
        "user": "RicardoL1u",
        "type": "user"
      },
      "summary": "Los nuevos modelos de lógica de gran escala (LRMs) han demostrado un potente rendimiento al resolver tareas complejas gracias a su capacidad para generar largas cadenas de razonamiento (CoT). Estos LRMs se han desarrollado principalmente a través de aprendizaje posterior en tareas lógicas formales, lo que hace que su capacidad lógica sea incierta en tareas de exploración de hechos. Por ejemplo, DeepSeek-R1 ha mostrado mejoras en el benchmark de exploración de hechos SimpleQA, mientras que OpenAI-o3 presenta observaciones más estrictas sobre sus fallos. Esta diferencia ha generado una cuestión de investigación natural: ¿se vuelven más vulnerables los modelos de lógica a las fallas? Este artículo aborda esta cuestión desde tres perspectivas: (1) Primero, se realiza una evaluación general de las fallas en los LRMs. El análisis muestra que los LRMs pueden reducir las fallas mediante ajustes de subconjuntos en un inicio frío y RL con recompensas verificables, pero otros métodos de aprendizaje, como aprendizaje por refuerzo, pueden generar fallas más complejas. (2) Se analiza la influencia de las fallas en los comportamientos de los LRMs. Se caracterizan dos comportamientos cognitivos que afectan directamente a la verdad: la Repetición de Errores, que involucra una repetición lógica de errores potenciales, y el Desajuste entre Pensamiento y Respuesta, que se refiere a respuestas finales que no se alinean con el proceso de razonamiento CoT. (3) Además, se investiga la estructura de las fallas en los LRMs, separándolas de la incertidumbre del modelo. La mayoría de los casos, el aumento de las fallas en los LRMs está asociado con la desbalance entre la incertidumbre del modelo y la precisión factual. Este artículo proporciona una comprensión inicial de las fallas en los LRMs.",
      "upvotes": 17,
      "discussionId": "683946c845636acda08ed42a",
      "ai_summary": "Large reasoning models exhibit varying susceptibility to hallucination depending on post-training pipelines, revealing critical cognitive behaviors and uncertainty misalignment as contributing factors.",
      "ai_keywords": [
        "large reasoning models",
        "chain-of-thought",
        "post-training",
        "formal reasoning tasks",
        "fact-seeking tasks",
        "DeepSeek-R1",
        "OpenAI-o3",
        "hallucination",
        "cold start supervised fine-tuning",
        "verifiable reward RL",
        "distillation",
        "Flaw Repetition",
        "Think-Answer Mismatch",
        "model uncertainty",
        "factual accuracy"
      ]
    },
    "publishedAt": "2025-05-29T12:53:41.000Z",
    "title": "Are Reasoning Models More Prone to Hallucination?",
    "summary": "Recently evolved large reasoning models (LRMs) show powerful performance in\nsolving complex tasks with long chain-of-thought (CoT) reasoning capability. As\nthese LRMs are mostly developed by post-training on formal reasoning tasks,\nwhether they generalize the reasoning capability to help reduce hallucination\nin fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1\nreports increased performance on SimpleQA, a fact-seeking benchmark, while\nOpenAI-o3 observes even severer hallucination. This discrepancy naturally\nraises the following research question: Are reasoning models more prone to\nhallucination? This paper addresses the question from three perspectives. (1)\nWe first conduct a holistic evaluation for the hallucination in LRMs. Our\nanalysis reveals that LRMs undergo a full post-training pipeline with cold\nstart supervised fine-tuning (SFT) and verifiable reward RL generally alleviate\ntheir hallucination. In contrast, both distillation alone and RL training\nwithout cold start fine-tuning introduce more nuanced hallucinations. (2) To\nexplore why different post-training pipelines alters the impact on\nhallucination in LRMs, we conduct behavior analysis. We characterize two\ncritical cognitive behaviors that directly affect the factuality of a LRM: Flaw\nRepetition, where the surface-level reasoning attempts repeatedly follow the\nsame underlying flawed logic, and Think-Answer Mismatch, where the final answer\nfails to faithfully match the previous CoT process. (3) Further, we investigate\nthe mechanism behind the hallucination of LRMs from the perspective of model\nuncertainty. We find that increased hallucination of LRMs is usually associated\nwith the misalignment between model uncertainty and factual accuracy. Our work\nprovides an initial understanding of the hallucination in LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23646.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e25e2247678ea5ce1b1786",
      "avatarUrl": "/avatars/1bb32e7597a9b1c89c434cbf550b5382.svg",
      "fullname": "Yantao",
      "name": "RicardoL1u",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22914",
      "authors": [
        {
          "_id": "6839533ae7922379b361b3cb",
          "name": "Maksim Kolodiazhnyi",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3cc",
          "name": "Denis Tarasov",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3cd",
          "user": {
            "_id": "67d5a331eab66ce9cb01bae4",
            "avatarUrl": "/avatars/3ed437c874889e8a6db66c3ef88a60c0.svg",
            "isPro": false,
            "fullname": "DMITRII ZHEMCHUZHNIKOV",
            "user": "zhemchuzhnikov",
            "type": "user"
          },
          "name": "Dmitrii Zhemchuzhnikov",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T06:42:03.218Z",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3ce",
          "name": "Alexander Nikulin",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3cf",
          "name": "Ilya Zisman",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3d0",
          "name": "Anna Vorontsova",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3d1",
          "name": "Anton Konushin",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3d2",
          "name": "Vladislav Kurenkov",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3d3",
          "name": "Danila Rukhovich",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T22:32:31.000Z",
      "submittedOnDailyAt": "2025-05-30T05:25:03.367Z",
      "title": "Aplicamos aprendizaje por refuerzo en la listado de tarjetas multimodal.",
      "submittedOnDailyBy": {
        "_id": "665b10fb270e47e678f2ddf1",
        "avatarUrl": "/avatars/1bc7a9211acf767f7bfca998c24315a0.svg",
        "isPro": false,
        "fullname": "max",
        "user": "maksimko123",
        "type": "user"
      },
      "summary": "El apoyo a diseño de computadoras (CAD) desempeña un papel crucial en las ciencias de la ingeniería y en la ingeniería mecánica, permitiendo la creación de modelos 3D precisos y ajustables. El democratización del acceso a aplicaciones de diseño CAD se ha logrado mediante el uso de datos de sensores y de usuarios en la reconstrucción de CAD. Sin embargo, los métodos actuales generalmente se centran en un solo modelo de entrada, como puntos, imágenes o texto, lo que limita la generalización y la robustez. En este contexto, se propone un modelo de reconstrucción de CAD que procese simultáneamente tres tipos de modelos de entrada, aprovechando el desarrollo reciente de modelos de lenguaje visual (VLM). Para ello, se utiliza una pipeline de dos etapas que combina la regulación regular de datos de procesos grandes (SFT) y la optimización mediante aprendizaje por refuerzo (RL) con retroalimentación en línea. Además, se explora por primera vez la optimización de RL en tareas de CAD, demostrando que el algoritmo de preferencia relativa grupal (GRPO), que actúa como referencia en RL en línea, supera los algoritmos offline. En el DeepCAD benchmark, el modelo SFT es capaz de manejar de manera eficiente a los tres tipos de modelos de entrada. Lo más importante es que, después de la optimización RL, DeepCAD ha evolucionado hacia nuevos óptimos en tres conjuntos de datos complejos.",
      "upvotes": 16,
      "discussionId": "6839533be7922379b361b418",
      "ai_summary": "A multi-modal CAD reconstruction model leveraging vision-language models and reinforcement learning achieves state-of-the-art performance across various datasets, including real-world ones.",
      "ai_keywords": [
        "vision-language models",
        "multi-modal",
        "large language model",
        "supervised fine-tuning",
        "reinforcement learning",
        "Group Relative Preference Optimization",
        "DeepCAD benchmark"
      ]
    },
    "publishedAt": "2025-05-28T18:32:31.000Z",
    "title": "cadrille: Multi-modal CAD Reconstruction with Online Reinforcement\n  Learning",
    "summary": "Computer-Aided Design (CAD) plays a central role in engineering and\nmanufacturing, making it possible to create precise and editable 3D models.\nUsing a variety of sensor or user-provided data as inputs for CAD\nreconstruction can democratize access to design applications. However, existing\nmethods typically focus on a single input modality, such as point clouds,\nimages, or text, which limits their generalizability and robustness. Leveraging\nrecent advances in vision-language models (VLM), we propose a multi-modal CAD\nreconstruction model that simultaneously processes all three input modalities.\nInspired by large language model (LLM) training paradigms, we adopt a two-stage\npipeline: supervised fine-tuning (SFT) on large-scale procedurally generated\ndata, followed by reinforcement learning (RL) fine-tuning using online\nfeedback, obtained programatically. Furthermore, we are the first to explore RL\nfine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such\nas Group Relative Preference Optimization (GRPO) outperform offline\nalternatives. In the DeepCAD benchmark, our SFT model outperforms existing\nsingle-modal approaches in all three input modalities simultaneously. More\nimportantly, after RL fine-tuning, cadrille sets new state-of-the-art on three\nchallenging datasets, including a real-world one.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22914.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "665b10fb270e47e678f2ddf1",
      "avatarUrl": "/avatars/1bc7a9211acf767f7bfca998c24315a0.svg",
      "fullname": "max",
      "name": "maksimko123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20088",
      "authors": [
        {
          "_id": "683947458987f50a5ec45a01",
          "name": "Nitay Calderon",
          "hidden": false
        },
        {
          "_id": "683947458987f50a5ec45a02",
          "name": "Liat Ein-Dor",
          "hidden": false
        },
        {
          "_id": "683947458987f50a5ec45a03",
          "name": "Roi Reichart",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62d6a0c18faee0ac953c51fa/C8RHn7j61J4qQlZvXd52v.png"
      ],
      "publishedAt": "2025-05-26T15:01:56.000Z",
      "submittedOnDailyAt": "2025-05-30T04:25:05.621Z",
      "title": "Descripción de diversos intereses",
      "submittedOnDailyBy": {
        "_id": "62d6a0c18faee0ac953c51fa",
        "avatarUrl": "/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg",
        "isPro": false,
        "fullname": "Nitay Calderon",
        "user": "nitay",
        "type": "user"
      },
      "summary": "Los métodos de ajuste, especialmente la influencia humana, LLM-as-a-Judge (LaaJ), modelos de recompensa y otros son utilizados de manera central para ajustar y evaluar modelos de lenguaje grandes (LLMs). Sin embargo, los conceptos que lideran esta influencia aún no se han comprendido completamente. En este artículo, proponemos una completa y automática metodología para generar conceptos locales y globales de la influencia en diversas áreas. Nuestro método identifica conceptos que permiten distinguir respuestas rechazadas de las aceptadas y los representa en vectores basados en conceptos. Para modelizar la relación entre conceptos e influencia, proponemos un modelo de regresión multidominio de hipervías abiertos que captura efectos generales y específicos. Este modelo es utilizado para explicar 12 técnicas en un conjunto de datos complejos y variados, extendidos a 8 áreas difíciles. Nuestro método logra un rendimiento de predicción de influencia fuerte que supera los estándares, al mismo tiempo que proporciona explicaciones. Además, evaluamos nuestra explicación en dos configuraciones de aplicación. Primero, utilizando la explicación de LaaJ, se puede guiar la salida de un LLM para obtener respuestas que el juez prefiera consistentemente. Luego, presentando conceptos a LaaJ, se mejora la predicción de su influencia. Estos resultados apoyan un nuevo paradigma de explicabilidad en la era de los LLMs.",
      "upvotes": 15,
      "discussionId": "683947488987f50a5ec45a95",
      "ai_summary": "A new automated method using concept-based vectors and a Hierarchical Multi-Domain Regression model improves preference explanations and predictions for large language models.",
      "ai_keywords": [
        "LLM-as-a-Judge",
        "reward models",
        "large language models",
        "concept-based explanations",
        "domain-general effects",
        "domain-specific effects",
        "Hierarchical Multi-Domain Regression model",
        "preference prediction",
        "explainability"
      ]
    },
    "publishedAt": "2025-05-26T11:01:56.000Z",
    "title": "Multi-Domain Explainability of Preferences",
    "summary": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated method for generating\nlocal and global concept-based explanations of preferences across multiple\ndomains. Our method utilizes an LLM to identify concepts that distinguish\nbetween chosen and rejected responses, and to represent them with concept-based\nvectors. To model the relationships between concepts and preferences, we\npropose a white-box Hierarchical Multi-Domain Regression model that captures\nboth domain-general and domain-specific effects. To evaluate our method, we\ncurate a dataset spanning eight challenging and diverse domains and explain\ntwelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two application-driven settings. First,\nguiding LLM outputs with concepts from LaaJ explanations yields responses that\nthose judges consistently prefer. Second, prompting LaaJs with concepts\nexplaining humans improves their preference predictions. Together, our work\nestablishes a new paradigm for explainability in the era of LLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62d6a0c18faee0ac953c51fa/C8RHn7j61J4qQlZvXd52v.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d6a0c18faee0ac953c51fa",
      "avatarUrl": "/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg",
      "fullname": "Nitay Calderon",
      "name": "nitay",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23380",
      "authors": [
        {
          "_id": "683906f81bf7a7a94309c5a5",
          "name": "Weijia Mao",
          "hidden": false
        },
        {
          "_id": "683906f81bf7a7a94309c5a6",
          "name": "Zhenheng Yang",
          "hidden": false
        },
        {
          "_id": "683906f81bf7a7a94309c5a7",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T12:00:15.000Z",
      "submittedOnDailyAt": "2025-05-30T02:41:07.217Z",
      "title": "UniRL: Única Unimodal Model para Supervisado y Reinforcement Learning para Mejorar la Autonomía",
      "submittedOnDailyBy": {
        "_id": "63f320ee0be81bdc5d8ecb88",
        "avatarUrl": "/avatars/9d08cff6ed23a51887c869947bc03228.svg",
        "isPro": false,
        "fullname": "Mao Weijia",
        "user": "benzweijia",
        "type": "user"
      },
      "summary": "Un modelo de lenguaje de gran escala como Show-o y Janus logran un desempeño fuerte en ambas tareas: generación y comprensión. Sin embargo, estos modelos generalmente se basan en grandes conjuntos de datos, lo que requiere una gran cantidad de cálculos en la fase de entrenamiento previo. Además, se han propuesto diversos métodos de entrenamiento posterior, pero estos suelen depender de datos externos o estar limitados a una customización específica para tareas. En este artículo, presentamos UniRL, una aproximación de mejora automática de entrenamiento posterior. Nuestro enfoque hace que el modelo genere imágenes a partir de prompts y que estos sean utilizados como datos de entrenamiento en cada iteración. Además, estas dos tareas pueden fortalecerse mutuamente: las imágenes generadas pueden ser utilizadas en la comprensión y los resultados de la comprensión pueden controlar la generación. Utilizamos la micro-fine-tuning (SFT) y la optimización relativa de grupos (GRPO) para optimizar el modelo. UniRL ofrece tres principales ventajas: (1) no depende de datos de imágenes externos, dejando que todos los ejemplos de entrenamiento sean generados por el modelo durante el entrenamiento; (2) mejora el rendimiento de las tareas individuales del modelo, reduciendo la desigualdad entre generación y comprensión; (3) en la fase de entrenamiento posterior, solo se necesitan unos pocos pasos adicionales de entrenamiento. UniRL se evaluó en Show-o y Janus, obteniendo un puntaje GenEval de 0.77 para Show-o y 0.65 para Janus. El código y el modelo están disponibles en https://github.com/showlab/UniRL.",
      "upvotes": 14,
      "discussionId": "683906f91bf7a7a94309c5dd",
      "ai_summary": "UniRL is a self-improving post-training method for unified multimodal large language models that uses generated images as training data, enhancing both generation and understanding tasks without external data.",
      "ai_keywords": [
        "Unified multimodal large language models",
        "Show-o",
        "Janus",
        "self-improving post-training",
        "prompts",
        "training data",
        "supervised fine-tuning",
        "Group Relative Policy Optimization",
        "GenEval score"
      ]
    },
    "publishedAt": "2025-05-29T08:00:15.000Z",
    "title": "UniRL: Self-Improving Unified Multimodal Models via Supervised and\n  Reinforcement Learning",
    "summary": "Unified multimodal large language models such as Show-o and Janus have\nachieved strong performance across both generation and understanding tasks.\nHowever, these models typically rely on large-scale datasets and require\nsubstantial computation during the pretraining stage. In addition, several\npost-training methods have been proposed, but they often depend on external\ndata or are limited to task-specific customization. In this work, we introduce\nUniRL, a self-improving post-training approach. Our approach enables the model\nto generate images from prompts and use them as training data in each\niteration, without relying on any external image data. Moreover, it enables the\ntwo tasks to enhance each other: the generated images are used for\nunderstanding, and the understanding results are used to supervise generation.\nWe explore supervised fine-tuning (SFT) and Group Relative Policy Optimization\n(GRPO) to optimize the models. UniRL offers three key advantages: (1) it\nrequires no external image data, as all training samples are generated by the\nmodel itself during training; (2) it not only improves individual task\nperformance, but also reduces the imbalance between generation and\nunderstanding; and (3) it requires only several additional training steps\nduring the post-training stage. We evaluate UniRL on top of Show-o and Janus,\nachieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and\nmodels will be released in https://github.com/showlab/UniRL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23380.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f320ee0be81bdc5d8ecb88",
      "avatarUrl": "/avatars/9d08cff6ed23a51887c869947bc03228.svg",
      "fullname": "Mao Weijia",
      "name": "benzweijia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23419",
      "authors": [
        {
          "_id": "68391574f85de1fc563cd890",
          "name": "Linghao Zhang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd891",
          "name": "Shilin He",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd892",
          "name": "Chaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd893",
          "name": "Yu Kang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd894",
          "name": "Bowen Li",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd895",
          "name": "Chengxing Xie",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd896",
          "name": "Junhao Wang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd897",
          "name": "Maoquan Wang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd898",
          "name": "Yufan Huang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd899",
          "name": "Shengyu Fu",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89a",
          "name": "Elsie Nallipogu",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89b",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89c",
          "name": "Yingnong Dang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89d",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89e",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T13:09:44.000Z",
      "submittedOnDailyAt": "2025-05-30T00:49:19.429Z",
      "title": "SWE-bench Servicio ha comenzado!",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "En la tarea de resolución de problemas, la generación de patches para corregir errores reales en el mundo real se ha convertido en un importante marco de evaluación de las capacidades de los grandes modelos de lenguaje (LLMs). Sin embargo, SWE-bench y sus variantes no han sido actualizados desde su lanzamiento inicial, cubren un conjunto reducido de repositorios, y requieren esfuerzos manuales en la construcción de instancias y configuración del entorno, lo que impide la escalabilidad y puede llevar a sobreajuste y contaminación de datos. En este artículo, se presenta SWE-bench-Live, un marco de evaluación dinámico que supera estos desafíos. La primera versión se lanzó con 1,319 tareas basadas en problemas reales del GitHub desde 2024, y cubre 93 repositorios. Cada tarea incluye una imagen de documentación adicional para garantizar reproducibilidad. El núcleo de este marco es un flujo de caracterización automático que automatiza todas las etapas desde la creación de instancias hasta la configuración del entorno, eliminando esfuerzos manuales y permitiendo escalabilidad y actualizaciones continuas. En SWE-bench-Live, se evalúan los últimos frameworks de agentes y LLMs, y se muestran diferencias significativas en comparación con marcos estáticos. Para comprender estas diferencias, se realizó un análisis detallado de la fuente de repositorios, la naturaleza de los problemas y la dificultad de las tareas. Al proporcionar un nuevo, diverso y ejecutable marco de evaluación basado en la actividad dinámica de repositorios, SWE-bench-Live permite una evaluación estricta y resistente a la contaminación de los LLMs y agentes, y fomenta la evaluación en entornos dinámicos y reales de desarrollo de software.",
      "upvotes": 13,
      "discussionId": "68391574f85de1fc563cd8d4",
      "ai_summary": "SWE-bench-Live is a continuously updatable benchmark for evaluating LLMs in issue resolution, featuring live GitHub issues and automated curation to ensure scalability and contamination resistance.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "SWE-bench",
        "SWE-bench-Live",
        "live-updatable benchmark",
        "GitHub issues",
        "Docker image",
        "automated curation pipeline",
        "reproducible execution",
        "performance evaluation",
        "repository origin",
        "issue recency",
        "task difficulty"
      ]
    },
    "publishedAt": "2025-05-29T09:09:44.000Z",
    "title": "SWE-bench Goes Live!",
    "summary": "The issue-resolving task, where a model generates patches to fix real-world\nbugs, has emerged as a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs). While SWE-bench and its variants have become\nstandard in this domain, they suffer from key limitations: they have not been\nupdated since their initial releases, cover a narrow set of repositories, and\ndepend heavily on manual effort for instance construction and environment\nsetup. These factors hinder scalability and introduce risks of overfitting and\ndata contamination. In this work, we present SWE-bench-Live, a\nlive-updatable benchmark designed to overcome these challenges. Our\ninitial release consists of 1,319 tasks derived from real GitHub issues created\nsince 2024, spanning 93 repositories. Each task is accompanied by a dedicated\nDocker image to ensure reproducible execution. Central to our benchmark is\n\\method, an automated curation pipeline that streamlines the entire process\nfrom instance creation to environment setup, removing manual bottlenecks and\nenabling scalability and continuous updates. We evaluate a range of\nstate-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a\nsubstantial performance gap compared to static benchmarks like SWE-bench, even\nunder controlled evaluation conditions. To better understand this discrepancy,\nwe perform detailed analyses across repository origin, issue recency, and task\ndifficulty. By providing a fresh, diverse, and executable benchmark grounded in\nlive repository activity, SWE-bench-Live facilitates rigorous,\ncontamination-resistant evaluation of LLMs and agents in dynamic, real-world\nsoftware development settings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22618",
      "authors": [
        {
          "_id": "683931e1b6280677f75edf09",
          "name": "Chengyue Wu",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0a",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0b",
          "user": {
            "_id": "64cf5e81a2e7f9ff61eb3a0c",
            "avatarUrl": "/avatars/cbaa11daec9d4113bf7de93fe9b9ee86.svg",
            "isPro": false,
            "fullname": "scxue",
            "user": "Cauthyyy",
            "type": "user"
          },
          "name": "Shuchen Xue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:19.453Z",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0c",
          "user": {
            "_id": "650dac79b959b0e1d41d7378",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dac79b959b0e1d41d7378/mzbN0MFk3k8b94FQ40I7L.jpeg",
            "isPro": false,
            "fullname": "Zhijian Liu",
            "user": "zhijianliu",
            "type": "user"
          },
          "name": "Zhijian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:22.124Z",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0d",
          "name": "Shizhe Diao",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0e",
          "name": "Ligeng Zhu",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0f",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf10",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf11",
          "name": "Enze Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:39:15.000Z",
      "submittedOnDailyAt": "2025-05-30T02:50:04.196Z",
      "title": "Fast-dLLM: Mejora de la velocidad de difusión de LLM sin entrenamiento mediante caché de KV y verificación paralela",
      "submittedOnDailyBy": {
        "_id": "633bd54b00732349209a18fe",
        "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
        "isPro": false,
        "fullname": "Shizhe Diao",
        "user": "shizhediao",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grandes basados en difusión (Diffusion LLMs) tienen una capacidad de análisis paralela excepcional y se especializan en la generación de respuestas no automáticas. Sin embargo, porque no tienen un cache Key-Value (KV), la velocidad de inferencia real de los Diffusion LLMs abierto-código es más lenta que los modelos de respuestas automáticas, y al mismo tiempo, la calidad se reduce al interpretar múltiples tokens. Para resolver estos problemas, introducimos una nueva estructura de cache de bloques aproximados adecuados para modelos bidireccionales de difusión, lo que minimiza la pérdida de rendimiento y permite el reutilización del cache. Además, identificamos la causa fundamental de la pérdida de calidad en la análisis paralelo: la destrucción de la relación de dependencia entre tokens debido a la asumida independencia condicional. En respuesta, proponemos una estrategia de análisis paralelo basada en confianza, que mitiga la violación de estas relaciones y mantiene la calidad de la generación. Las experimentos en varios marcos de evaluación de modelos de lenguaje grande como LLaDA y Dream muestran que se minimiza la pérdida de precisión y se acerca el rendimiento a los modelos de respuestas automáticas, adaptando así los LLMs basados en difusión a su aplicación real.",
      "upvotes": 11,
      "discussionId": "683931e2b6280677f75edf32",
      "ai_summary": "A novel block-wise approximate KV Cache and confidence-aware parallel decoding strategy improve the inference speed of diffusion-based large language models without significant quality loss.",
      "ai_keywords": [
        "diffusion-based large language models (Diffusion LLMs)",
        "non-autoregressive text generation",
        "parallel decoding",
        "Key-Value (KV) Cache",
        "bidirectional diffusion models",
        "token dependencies",
        "confidence-aware parallel decoding",
        "LLaDA",
        "Dream models",
        "LLM benchmarks"
      ]
    },
    "publishedAt": "2025-05-28T13:39:15.000Z",
    "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
    "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to 27.6times\nthroughput improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633bd54b00732349209a18fe",
      "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
      "fullname": "Shizhe Diao",
      "name": "shizhediao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23606",
      "authors": [
        {
          "_id": "6839189f4a3a71a917b0514e",
          "user": {
            "_id": "656724074f6ec72017754d33",
            "avatarUrl": "/avatars/e61de248f6f53719b2375077340dd033.svg",
            "isPro": false,
            "fullname": "QingyuShi",
            "user": "QingyuShi",
            "type": "user"
          },
          "name": "Qingyu Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:45.160Z",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b0514f",
          "user": {
            "_id": "63fccdac93b993a4ebd7789a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
            "isPro": false,
            "fullname": "Jinbin Bai",
            "user": "BryanW",
            "type": "user"
          },
          "name": "Jinbin Bai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:47.122Z",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05150",
          "name": "Zhuoran Zhao",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05151",
          "name": "Wenhao Chai",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05152",
          "name": "Kaidong Yu",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05153",
          "name": "Jianzong Wu",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05154",
          "name": "Shuangyong Song",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05155",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05156",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05157",
          "name": "Xuelong Li",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05158",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:15:48.000Z",
      "submittedOnDailyAt": "2025-05-30T01:06:53.607Z",
      "title": "Muddit: Es más efectivo generar imágenes desde el contexto que liberar el futuro con un modelo de dispersión distribuida de manera uniforme.",
      "submittedOnDailyBy": {
        "_id": "63fccdac93b993a4ebd7789a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
        "isPro": false,
        "fullname": "Jinbin Bai",
        "user": "BryanW",
        "type": "user"
      },
      "summary": "El módulo de generación integrado tiene como objetivo procesar diferentes tareas como la generación de texto, la generación de imágenes y la inferencia de lenguaje visual, utilizando una arquitectura y un paradigma de decodificación único. El módulo de generación automática secuencial es lento en la inferencia secuencial, mientras que el módulo de generación no secuencial no automatizado sufre dificultades en la generalización debido a un aprendizaje previo limitado. Presentamos Muddit, un transformador de diérencial discreto que permite la generación rápida y paralela de datos de modelos de texto y imágenes. A diferencia de los módulos de diérencial discreto integrados existentes, Muddit integra un texto town image backbone con fuerte capacidad de procesamiento de imágenes y un decodificador ligero de texto, lo que permite una arquitectura flexible y alta calidad para la generación de múltiples modelos. A través de los resultados de experimentos, Muddit logra los mejores rendimientos en calidad y eficiencia comparado con los mejores módulos de generación automática secuencial. Este artículo eleva la posibilidad de integrar un diérencial discreto con fuerte capacidad de procesamiento de imágenes como una arquitectura eficiente y flexible para generación integrada.",
      "upvotes": 10,
      "discussionId": "683918a14a3a71a917b051ea",
      "githubRepo": "https://github.com/M-E-AGI-Lab/Muddit",
      "ai_summary": "Muddit, a unified discrete diffusion transformer, achieves fast and high-quality generation across text and image modalities by integrating pretrained visual priors with a lightweight text decoder.",
      "ai_keywords": [
        "unified generation models",
        "autoregressive models",
        "non-autoregressive models",
        "discrete diffusion",
        "diffusion transformer",
        "pretrained backbones",
        "flexible generation",
        "multimodal generation",
        "text generation",
        "image generation",
        "vision-language reasoning",
        "quality",
        "efficiency",
        "visual priors",
        "scalable backbone"
      ]
    },
    "publishedAt": "2025-05-29T12:15:48.000Z",
    "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified\n  Discrete Diffusion Model",
    "summary": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23606.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63fccdac93b993a4ebd7789a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
      "fullname": "Jinbin Bai",
      "name": "BryanW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23585",
      "authors": [
        {
          "_id": "683916a7a2d6f83cf12552dd",
          "name": "Yaru Hao",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552de",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552df",
          "name": "Xun Wu",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552e0",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552e1",
          "name": "Zewen Chi",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552e2",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T15:58:04.000Z",
      "submittedOnDailyAt": "2025-05-30T00:53:45.844Z",
      "title": "El línea de base de recompensa más adecuada para el aprendizaje por refuerzo en proyectos de política añadida",
      "submittedOnDailyBy": {
        "_id": "5df85abada6d0311fd3d5408",
        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
        "isPro": false,
        "fullname": "Li Dong",
        "user": "unilm",
        "type": "user"
      },
      "summary": "Los algoritmos de aprendizaje por refuerzo desempeñan un papel fundamental en la alineación de grandes modelos de lenguaje y las preferencias humanas, así como en el mejoramiento de la capacidad de razonamiento. Sin embargo, actualmente, estos algoritmos enfrentan desafíos debido a la instabilidad del entrenamiento debido a la baja rigidez de las restricciones de política y a la baja eficiencia de la red de cálculo propiciada por modelos auxiliares. En este artículo, se propone un nuevo algoritmo de aprendizaje por refuerzo simplificado llamado \"On-Policy RL with Optimal reward baseline (OPO)\" para resolver estos problemas. OPO enfatiza la importancia de entrenar una política precisa para estabilizar el proceso de entrenamiento experimental y mejorar la exploración. Además, OPO introduce una línea de base de recompensa óptima teóricamente para minimizar la varianza. Se evaluó OPO en un marco de referencia matemático. Los resultados muestran que OPO logra un rendimiento superior y estabilidad del entrenamiento sin necesidad de incluir modelos adicionales o términos de normalización. Además, OPO implementa menos movimientos de política y mayores eventos de salida de impacto, y induce respuestas más diversas sin repetir. Estos resultados demuestran que OPO ofrece una nueva dirección para un aprendizaje por refuerzo estable y efectivo en modelos de lenguaje grandes y tareas de razonamiento. La implementación está disponible en https://github.com/microsoft/LMOps/tree/main/opo.",
      "upvotes": 9,
      "discussionId": "683916a8a2d6f83cf1255310",
      "githubRepo": "https://github.com/microsoft/LMOps/tree/main/opo",
      "ai_summary": "A novel reinforcement learning algorithm, OPO, improves training stability and performance in large language model alignment and reasoning by emphasizing exact on-policy training and using an optimal reward baseline.",
      "ai_keywords": [
        "reinforcement learning",
        "on-policy constraints",
        "computational efficiency",
        "auxiliary models",
        "optimal reward baseline",
        "gradient variance",
        "policy shifts",
        "output entropy",
        "mathematical reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-05-29T11:58:04.000Z",
    "title": "On-Policy RL with Optimal Reward Baseline",
    "summary": "Reinforcement learning algorithms are fundamental to align large language\nmodels with human preferences and to enhance their reasoning capabilities.\nHowever, current reinforcement learning algorithms often suffer from training\ninstability due to loose on-policy constraints and computational inefficiency\ndue to auxiliary models. In this work, we propose On-Policy RL with Optimal\nreward baseline (OPO), a novel and simplified reinforcement learning algorithm\ndesigned to address these challenges. OPO emphasizes the importance of exact\non-policy training, which empirically stabilizes the training process and\nenhances exploration. Moreover, OPO introduces the optimal reward baseline that\ntheoretically minimizes gradient variance. We evaluate OPO on mathematical\nreasoning benchmarks. The results demonstrate its superior performance and\ntraining stability without additional models or regularization terms.\nFurthermore, OPO achieves lower policy shifts and higher output entropy,\nencouraging more diverse and less repetitive responses. These results highlight\nOPO as a promising direction for stable and effective reinforcement learning in\nlarge language model alignment and reasoning tasks. The implementation is\nprovided at https://github.com/microsoft/LMOps/tree/main/opo.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23585.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5df85abada6d0311fd3d5408",
      "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
      "fullname": "Li Dong",
      "name": "unilm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22421",
      "authors": [
        {
          "_id": "68391dbb0653b6a3441a7f7e",
          "user": {
            "_id": "6311d9ee04f842f79916158c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PvOYgDEpe0x5ExLyj1BK-.png",
            "isPro": false,
            "fullname": "chen",
            "user": "antonio-c",
            "type": "user"
          },
          "name": "Anthony Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:38.940Z",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f7f",
          "name": "Wenzhao Zheng",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f80",
          "user": {
            "_id": "647068944be5cf1f33491cb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Aqudl2PTSKPAylBhlccWr.png",
            "isPro": false,
            "fullname": "Yida Wang",
            "user": "wangyida",
            "type": "user"
          },
          "name": "Yida Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:36.769Z",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f81",
          "name": "Xueyang Zhang",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f82",
          "name": "Kun Zhan",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f83",
          "name": "Peng Jia",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f84",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f85",
          "name": "Shanghang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T14:46:51.000Z",
      "submittedOnDailyAt": "2025-05-30T01:26:00.575Z",
      "title": "GeoDrive: Modelo de mundo de conducción 3D con información de geometría y control preciso de acciones",
      "submittedOnDailyBy": {
        "_id": "6311d9ee04f842f79916158c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PvOYgDEpe0x5ExLyj1BK-.png",
        "isPro": false,
        "fullname": "chen",
        "user": "antonio-c",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los modelos mundiales ha impulsado una innovadora influencia en la simulación de entornos dinámicos, permitiendo a los sistemas predecir el estado futuro y evaluar acciones potenciales. En el contexto de la conducción automática, esta capacidad ha permitido a los vehículos predecir el comportamiento de otros usuarios de la carretera, planificar acciones frente a riesgos, acelerar la entrenamiento por simulación y adaptarse a nuevos escenarios, mejorando así la seguridad y la confianza. Los métodos actuales presentan limitaciones, como la necesidad de mantener una fuerte consistencia 3D dejemntonico o la acumulación de artefactos en el proceso de bloqueo. En respuesta a esto, presentamos GeoDrive, un modelo mundial que fortalece las condiciones 3D dejemntonico, mejorando la comprensión espacial y el control de acciones, con el objetivo de realizar modelado de escenarios más realistas, adaptables y confiables para la conducción automática seguro. Específicamente, extraemos una representación 3D a partir de los frames de entrada y obtenemos un renderizado 2D basado en la trayectoria de un vehículo personalizado. Proponemos un módulo de edición dinámico durante el entrenamiento para facilitar la modelación dinámica y mejorar el renderizado al editar la posición del vehículo. Las experimentaciones extendidas muestran que nuestro método supera significativamente a los modelos existentes en términos de precisión de acciones y comprensión espacial 3D, demostrando su capacidad para realizar modelado de escenarios más realistas, adaptables y confiables en la conducción automática segura. Además, nuestro modelo puede generalizarse a nuevas trayectorias y ofrece funciones de edición de objetos y control de trayectorias de objetos, permitiendo escenarios de interacción editados.",
      "upvotes": 9,
      "discussionId": "68391dbc0653b6a3441a7fd1",
      "githubRepo": "https://github.com/antonioo-c/GeoDrive",
      "ai_summary": "GeoDrive integrates robust 3D geometry into driving world models to improve spatial understanding and action controllability in autonomous navigation, enhancing safety and reliability.",
      "ai_keywords": [
        "world models",
        "dynamic environment simulation",
        "autonomous driving",
        "3D geometric consistency",
        "occlusion handling",
        "3D representation",
        "2D rendering",
        "ego-car trajectory",
        "dynamic editing module",
        "action accuracy",
        "3D spatial awareness",
        "scene modeling",
        "object editing",
        "object trajectory control"
      ]
    },
    "publishedAt": "2025-05-28T10:46:51.000Z",
    "title": "GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action\n  Control",
    "summary": "Recent advancements in world models have revolutionized dynamic environment\nsimulation, allowing systems to foresee future states and assess potential\nactions. In autonomous driving, these capabilities help vehicles anticipate the\nbehavior of other road users, perform risk-aware planning, accelerate training\nin simulation, and adapt to novel scenarios, thereby enhancing safety and\nreliability. Current approaches exhibit deficiencies in maintaining robust 3D\ngeometric consistency or accumulating artifacts during occlusion handling, both\ncritical for reliable safety assessment in autonomous navigation tasks. To\naddress this, we introduce GeoDrive, which explicitly integrates robust 3D\ngeometry conditions into driving world models to enhance spatial understanding\nand action controllability. Specifically, we first extract a 3D representation\nfrom the input frame and then obtain its 2D rendering based on the\nuser-specified ego-car trajectory. To enable dynamic modeling, we propose a\ndynamic editing module during training to enhance the renderings by editing the\npositions of the vehicles. Extensive experiments demonstrate that our method\nsignificantly outperforms existing models in both action accuracy and 3D\nspatial awareness, leading to more realistic, adaptable, and reliable scene\nmodeling for safer autonomous driving. Additionally, our model can generalize\nto novel trajectories and offers interactive scene editing capabilities, such\nas object editing and object trajectory control.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22421.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6311d9ee04f842f79916158c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PvOYgDEpe0x5ExLyj1BK-.png",
      "fullname": "chen",
      "name": "antonio-c",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23758",
      "authors": [
        {
          "_id": "683925243a3289061eda69ee",
          "user": {
            "_id": "65454d7c117ecae648892170",
            "avatarUrl": "/avatars/83a7091a24bf86801176ca85234b417a.svg",
            "isPro": false,
            "fullname": "Yusuf Dalva",
            "user": "ydalva",
            "type": "user"
          },
          "name": "Yusuf Dalva",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:26.107Z",
          "hidden": false
        },
        {
          "_id": "683925243a3289061eda69ef",
          "name": "Hidir Yesiltepe",
          "hidden": false
        },
        {
          "_id": "683925243a3289061eda69f0",
          "name": "Pinar Yanardag",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:46.000Z",
      "submittedOnDailyAt": "2025-05-30T01:56:15.021Z",
      "title": "LoRAShop: Generación y edición de imágenes multi-concepto sin entrenamiento con la forma del flujo de lance para corrección.",
      "submittedOnDailyBy": {
        "_id": "65454d7c117ecae648892170",
        "avatarUrl": "/avatars/83a7091a24bf86801176ca85234b417a.svg",
        "isPro": false,
        "fullname": "Yusuf Dalva",
        "user": "ydalva",
        "type": "user"
      },
      "summary": "LoRAShop es el primer marco de trabajo para edición de imágenes multi-concepto utilizando modelos LoRA. LoRAShop se basa en una importante descubrimiento sobre patrones de interacción de características dentro de transformadores de difusión de estilo Flux: las características de transformadores propias de un concepto se activan en áreas espacialmente coherentes en el inicio del procesamiento de ruido. Este hallazgo se utiliza para calcular un máscara lineal de Rayton para cada concepto y para mezclar pesos LoRA que corresponden solo a las áreas que definen dicho concepto. Así, la edición final integra de manera sinérgica múltiples temas y estilos en el escalado original, manteniendo el contexto global, la iluminación y los detalles. Nuestras experimentos muestran que LoRAShop proporciona un mejor mantenimiento de salidas en comparación con la línea base. Al eliminar el entrenamiento retrasado y las restricciones externas, LoRAShop transforma modelos personalizados de dipóreo en una herramienta práctica \"Photoshop con LoRA\", abriendo nuevas rutas para reedición visual creativa y rápida interacción.",
      "upvotes": 8,
      "discussionId": "683925263a3289061eda6a62",
      "projectPage": "https://lorashop.github.io/",
      "ai_summary": "LoRAShop, a framework for multi-concept image editing with LoRA models, leverages spatially coherent feature activation in Flux-style diffusion transformers to achieve seamless integration of multiple subjects or styles while preserving global context and identity.",
      "ai_keywords": [
        "Flux-style diffusion transformers",
        "concept-specific transformer features",
        "denoising process",
        "disentangled latent mask",
        "LoRA models",
        "identity preservation",
        "personalized diffusion models",
        "compositional visual storytelling",
        "rapid creative iteration"
      ]
    },
    "publishedAt": "2025-05-29T13:59:46.000Z",
    "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with\n  Rectified Flow Transformers",
    "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23758.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65454d7c117ecae648892170",
      "avatarUrl": "/avatars/83a7091a24bf86801176ca85234b417a.svg",
      "fullname": "Yusuf Dalva",
      "name": "ydalva",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23735",
      "authors": [
        {
          "_id": "6839158b56bcc85d9f92199b",
          "name": "Ali Behrouz",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199c",
          "name": "Zeman Li",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199d",
          "name": "Praneeth Kacham",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199e",
          "name": "Majid Daliri",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199f",
          "name": "Yuan Deng",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f9219a0",
          "name": "Peilin Zhong",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f9219a1",
          "name": "Meisam Razaviyayn",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f9219a2",
          "name": "Vahab Mirrokni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:57:16.000Z",
      "submittedOnDailyAt": "2025-05-30T00:49:34.783Z",
      "title": "ATLAS: Aprendizaje que recuerda el contexto óptimo durante el tiempo de prueba",
      "submittedOnDailyBy": {
        "_id": "65cccd5134a5d74cbaa9446c",
        "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
        "isPro": false,
        "fullname": "Ali Behrouz",
        "user": "AliBehrouz",
        "type": "user"
      },
      "summary": "Transformers ha sido establecido como el modelo básico más popular para modelar secuencias. Principalmente, se caracteriza por su eficiencia en tareas de recuperación en contexto y su capacidad de aprendizaje de escalabilidad. Sin embargo, la memoria bidimensional y la complejidad temporal limitan su aplicación en secuencias largas, lo que ha llevado a los investigadores a buscar mejoras efectivas, como los recuerdos de largo plazo en redes neuronales recurrentes modernas. Recientemente, el éxito en tareas de flujo hacia abajo, así como en tareas que requieren comprensión de contextos largos y secuencias extensas, han demostrado la necesidad de estas mejoras. Hemos identificado que estas limitaciones surgen de tres aspectos: 1. La capacidad de memoria está limitada por la estructura de la entrada y la mapeo de características. 2. La actualización de la memoria es online y se optimiza únicamente para el último input. 3. La gestión de memoria de tamaño fijo tiene una baja expresividad. Para mejorar estos aspectos, hemos optimizado la memoria basándonos en tokens actuales y pasados, y hemos propuesto un módulo de memoria de largo plazo de alta capacidad para superar la característica online de modelos de memoria de largo plazo. ATLAS. Basándonos en esta observación, hemos propuesto DeepTransformers, una nueva familia de arquitecturas similares a Transformers. Estas arquitecturas parten de una generalización rigurosa de la arquitectura de Transformers. En base a resultados experimentales de modelado de lenguaje, inferencia de conocimiento general, énfasis en llamadas y comprensión de contextos largos, ATLAS supera el rendimiento de Transformers y de los recientes modelos recurrentes lineales. ATLAS mejora el rendimiento de Titans en tareas de contextos largos y aumenta la precisión en el benchmark BABILong en una longitud de contexto de 10M con un aumento del 80%.",
      "upvotes": 8,
      "discussionId": "6839158c56bcc85d9f9219fc",
      "ai_summary": "A new long-term memory module called ATLAS addresses limitations of Transformers in handling long contexts by optimizing memory based on current and past inputs, leading to improved performance in long-context understanding tasks.",
      "ai_keywords": [
        "Transformers",
        "sequence modeling",
        "in-context retrieval",
        "memory capacity",
        "online nature",
        "fixed-size memory",
        "long-term memory module",
        "DeepTransformers",
        "language modeling",
        "common-sense reasoning",
        "recall-intensive tasks",
        "long-context understanding",
        "ATLAS",
        "Titans",
        "BABILong benchmark"
      ]
    },
    "publishedAt": "2025-05-29T13:57:16.000Z",
    "title": "ATLAS: Learning to Optimally Memorize the Context at Test Time",
    "summary": "Transformers have been established as the most popular backbones in sequence\nmodeling, mainly due to their effectiveness in in-context retrieval tasks and\nthe ability to learn at scale. Their quadratic memory and time complexity,\nhowever, bound their applicability in longer sequences and so has motivated\nresearchers to explore effective alternative architectures such as modern\nrecurrent neural networks (a.k.a long-term recurrent memory module). Despite\ntheir recent success in diverse downstream tasks, they struggle in tasks that\nrequires long context understanding and extrapolation to longer sequences. We\nobserve that these shortcomings come from three disjoint aspects in their\ndesign: (1) limited memory capacity that is bounded by the architecture of\nmemory and feature mapping of the input; (2) online nature of update, i.e.,\noptimizing the memory only with respect to the last input; and (3) less\nexpressive management of their fixed-size memory. To enhance all these three\naspects, we present ATLAS, a long-term memory module with high capacity that\nlearns to memorize the context by optimizing the memory based on the current\nand past tokens, overcoming the online nature of long-term memory models.\nBuilding on this insight, we present a new family of Transformer-like\narchitectures, called DeepTransformers, that are strict generalizations of the\noriginal Transformer architecture. Our experimental results on language\nmodeling, common-sense reasoning, recall-intensive, and long-context\nunderstanding tasks show that ATLAS surpasses the performance of Transformers\nand recent linear recurrent models. ATLAS further improves the long context\nperformance of Titans, achieving +80\\% accuracy in 10M context length of\nBABILong benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cccd5134a5d74cbaa9446c",
      "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
      "fullname": "Ali Behrouz",
      "name": "AliBehrouz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23416",
      "authors": [
        {
          "_id": "68393d075711daf8cc919c04",
          "user": {
            "_id": "62e21907eda17fc126a15210",
            "avatarUrl": "/avatars/097879cf1eafa5e77bde419c7172fac6.svg",
            "isPro": false,
            "fullname": "Jang-Hyun Kim",
            "user": "Jang-Hyun",
            "type": "user"
          },
          "name": "Jang-Hyun Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:14.736Z",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c05",
          "user": {
            "_id": "60eb9074cc720726777d22a2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60eb9074cc720726777d22a2/aI-pHJRmnYOfC5fH7fFzD.jpeg",
            "isPro": false,
            "fullname": "Jinuk Kim",
            "user": "jusjinuk",
            "type": "user"
          },
          "name": "Jinuk Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:17.350Z",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c06",
          "name": "Sangwoo Kwon",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c07",
          "name": "Jae W. Lee",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c08",
          "name": "Sangdoo Yun",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c09",
          "name": "Hyun Oh Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T13:05:47.000Z",
      "submittedOnDailyAt": "2025-05-30T03:37:32.573Z",
      "title": "KVzip: Utiliza la reconstrucción de contexto en el KV caché compresión querías extrañas.",
      "submittedOnDailyBy": {
        "_id": "62e21907eda17fc126a15210",
        "avatarUrl": "/avatars/097879cf1eafa5e77bde419c7172fac6.svg",
        "isPro": false,
        "fullname": "Jang-Hyun Kim",
        "user": "Jang-Hyun",
        "type": "user"
      },
      "summary": "Transformadores basados de grandes modelos de lenguaje (LLMs) almacenan el contexto en pares de clave-valor (KV) durante la inferencia. Cuando el largo del contexto aumenta, el tamaño de la caché de KV se expande, lo que aumenta el sobrecarga de memoria y la demora en atención. En este artículo, se presenta KVzip, un método para eliminar la caché de KV sin relación a la consulta y para reutilizar la caché de KV compresa de manera efectiva con diferentes consultas. KVzip utiliza un modelo básico de LLM para reconstruir el contexto original a partir de pares de KV compresos y excluye pares con baja importancia. Según evaluaciones experimentales, KVzip reduce el tamaño de la caché de KV en un 3 a 4 veces y reduce aproximadamente en un 2 veces la demora de decisión de FlashAttention. Resuelve problemas de evaluación, búsqueda y comprensión de código. Las evaluaciones incluyen modelos como LLaMA3.1-8B, Qwen2.5-14B, Gemma3-12B, y el largo de contexto llega a 170K tokens. KVzip es notablemente superior en escenarios multi-consulta, manteniendo una eficiencia de 90% de los buckets de caché, en comparación con métodos de eliminación de KV que dependen de la consulta.",
      "upvotes": 8,
      "discussionId": "68393d075711daf8cc919c2b",
      "githubRepo": "https://github.com/snu-mllab/KVzip",
      "ai_summary": "KVzip, a query-agnostic KV cache eviction method for transformer-based LLMs, reduces KV cache size and decoding latency while maintaining performance across various tasks and models.",
      "ai_keywords": [
        "Transformer-based large language models (LLMs)",
        "KV cache",
        "context length",
        "query-agnostic",
        "KVzip",
        "query-aware",
        "KV eviction",
        "attention latency",
        "FlashAttention",
        "question-answering",
        "retrieval",
        "reasoning",
        "code comprehension",
        "LLaMA3.1-8B",
        "Qwen2.5-14B",
        "Gemma3-12B"
      ]
    },
    "publishedAt": "2025-05-29T09:05:47.000Z",
    "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
    "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4times and FlashAttention decoding latency by approximately\n2times, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23416.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e21907eda17fc126a15210",
      "avatarUrl": "/avatars/097879cf1eafa5e77bde419c7172fac6.svg",
      "fullname": "Jang-Hyun Kim",
      "name": "Jang-Hyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23559",
      "authors": [
        {
          "_id": "68390eebc260f5fa36eaa27c",
          "user": {
            "_id": "66554507e6ea63012f35824c",
            "avatarUrl": "/avatars/b82de75bd60890e7bb524fc3754b131c.svg",
            "isPro": false,
            "fullname": "Kunlun_Zhu",
            "user": "Leozkl",
            "type": "user"
          },
          "name": "Kunlun Zhu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T01:50:37.811Z",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa27d",
          "name": "Jiaxun Zhang",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa27e",
          "name": "Ziheng Qi",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa27f",
          "name": "Nuoxing Shang",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa280",
          "user": {
            "_id": "65d188a4aa309d842e438ef1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
            "isPro": false,
            "fullname": "Zijia Liu",
            "user": "m-serious",
            "type": "user"
          },
          "name": "Zijia Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:08.570Z",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa281",
          "user": {
            "_id": "638d601b5e14c2f38678fb3a",
            "avatarUrl": "/avatars/98d6151ec4261741eb4bd406685c07b5.svg",
            "isPro": false,
            "fullname": "韩沛煊",
            "user": "HakHan",
            "type": "user"
          },
          "name": "Peixuan Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:06.300Z",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa282",
          "name": "Yue Su",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa283",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa284",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T15:35:58.000Z",
      "submittedOnDailyAt": "2025-05-30T00:27:19.661Z",
      "title": "SafeScientist: Descubrimiento de un Agente de Modelo de Lenguaje para la Reconocimiento del Riesgo en la Ciencia",
      "submittedOnDailyBy": {
        "_id": "64c090a9f613170e7be93d2f",
        "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
        "isPro": false,
        "fullname": "KunlunZhu",
        "user": "KunlunZhu",
        "type": "user"
      },
      "summary": "El reciente desarrollo de los agentes de modelos de lenguaje grandes (LLM) ha acelerado significativamente la automatización de la investigación científica, pero también ha despertado preocupaciones importantes en términos de ética y seguridad. Para abordar estos desafíos de manera sistemática, presentamos un marco de trabajo innovador llamado \"SafeScientist\", diseñado para elevar la responsabilidad ética y la seguridad en la investigación científica dirigida por IA. SafeScientist rechaza activamente tareas éticamente inapropiadas o de alto riesgo, y en todo el proceso de investigación se enfoca en la seguridad. Para lograr una supervisión general de la seguridad, hemos integrado una serie de estructuras defensivas que incluyen monitoreo de prompts, monitoreo de colaboración entre agentes, monitoreo de uso de herramientas y componentes de evaluación ética. Como complemento de SafeScientist, proponemos un nuevo benchmark llamado \"SciSafetyBench\" para evaluar la seguridad de la IA en el contexto científico. Este benchmark consta de 240 trabajos científicos de alto riesgo en 6 áreas, 30 herramientas científicas diseñadas y 120 trabajos de riesgo relacionados con las herramientas. Los experimentos extendidos han mejorado la seguridad en un 35% más que los frameworks tradicionales de IA científico, logrando mejoras sin dañar la calidad de los resultados científicos. Además, hemos validado rigurosamente la robustez de los sistemas de seguridad frente a diferentes métodos de ataque y confirmado el efecto de nuestro enfoque integral. Los códigos y datos están disponibles en https://github.com/ulab-uiuc/SafeScientist. Advertencia: este artículo incluye ejemplos de datos que pueden contener errores o pérdidas.",
      "upvotes": 7,
      "discussionId": "68390eedc260f5fa36eaa319",
      "ai_summary": "SafeScientist is an AI framework that enhances safety in AI-driven scientific research through multiple defensive mechanisms and is validated using the SciSafetyBench benchmark.",
      "ai_keywords": [
        "SafeScientist",
        "AI scientist framework",
        "ethical responsibility",
        "prompt monitoring",
        "agent-collaboration monitoring",
        "tool-use monitoring",
        "ethical reviewer component",
        "SciSafetyBench",
        "benchmark",
        "scientific tasks",
        "scientific tools",
        "safety performance",
        "adversarial attack methods"
      ]
    },
    "publishedAt": "2025-05-29T11:35:58.000Z",
    "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
    "summary": "Recent advancements in large language model (LLM) agents have significantly\naccelerated scientific discovery automation, yet concurrently raised critical\nethical and safety concerns. To systematically address these challenges, we\nintroduce SafeScientist, an innovative AI scientist framework\nexplicitly designed to enhance safety and ethical responsibility in AI-driven\nscientific exploration. SafeScientist proactively refuses ethically\ninappropriate or high-risk tasks and rigorously emphasizes safety throughout\nthe research process. To achieve comprehensive safety oversight, we integrate\nmultiple defensive mechanisms, including prompt monitoring, agent-collaboration\nmonitoring, tool-use monitoring, and an ethical reviewer component.\nComplementing SafeScientist, we propose SciSafetyBench, a novel\nbenchmark specifically designed to evaluate AI safety in scientific contexts,\ncomprising 240 high-risk scientific tasks across 6 domains, alongside 30\nspecially designed scientific tools and 120 tool-related risk tasks. Extensive\nexperiments demonstrate that SafeScientist significantly improves safety\nperformance by 35\\% compared to traditional AI scientist frameworks, without\ncompromising scientific output quality. Additionally, we rigorously validate\nthe robustness of our safety pipeline against diverse adversarial attack\nmethods, further confirming the effectiveness of our integrated approach. The\ncode and data will be available at https://github.com/ulab-uiuc/SafeScientist.\nred{Warning: this paper contains example data that may be offensive\nor harmful.}",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23559.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c090a9f613170e7be93d2f",
      "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
      "fullname": "KunlunZhu",
      "name": "KunlunZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22961",
      "authors": [
        {
          "_id": "68390b2ea26b4142b0578d05",
          "user": {
            "_id": "638d601b5e14c2f38678fb3a",
            "avatarUrl": "/avatars/98d6151ec4261741eb4bd406685c07b5.svg",
            "isPro": false,
            "fullname": "韩沛煊",
            "user": "HakHan",
            "type": "user"
          },
          "name": "Peixuan Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:17.916Z",
          "hidden": false
        },
        {
          "_id": "68390b2ea26b4142b0578d06",
          "user": {
            "_id": "65d188a4aa309d842e438ef1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
            "isPro": false,
            "fullname": "Zijia Liu",
            "user": "m-serious",
            "type": "user"
          },
          "name": "Zijia Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:20.240Z",
          "hidden": false
        },
        {
          "_id": "68390b2ea26b4142b0578d07",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T01:03:41.000Z",
      "submittedOnDailyAt": "2025-05-30T00:34:06.330Z",
      "title": "ToMAP: Trialing Open-minded LLM Persuaders Based on the Theory of Mind",
      "submittedOnDailyBy": {
        "_id": "65d188a4aa309d842e438ef1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
        "isPro": false,
        "fullname": "Zijia Liu",
        "user": "m-serious",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) demuestran el potencial esperado en persuasión, pero la investigación sobre el entrenamiento de los persuasores de LLMs está todavía en estados iniciales. En particular, la humanidad tiene la capacidad de modelar de manera dinámica y dinámicamente los pensamientos de los otros, pero los actuales LLMs encuentran dificultades para entender la Teoría de la Mente (ToM) y su lógica, y su percepción de la diversidad y la cognición está limitada. Para resolver estas limitaciones, introducimos la Teoría de la Mente Augmented Persuader (ToMAP), proponiendo un nuevo enfoque que incluye dos módulos de Teoría de la Mente para reconocer y analizar el estado de la mente del interlocutor. Específicamente, inducimos a los persuasores a dirigirse hacia posibles opiniones contrarias sobre un punto central, y combinamos un codificador de texto y un clasificador MLP entrenado para predecir la posición actual del interlocutor frente a esas opiniones contrarias. Diseñamos un esquema de reentrenamiento ajustado para que los persuasores aprendan a analizar la información relacionada con el interlocutor y a generar argumentos más efectivos. Los experimentos muestran que ToMAP, si tiene 3B parámetros, supera a GPT-4o y otros grandes modelos de base en múltiples modelos de persuasión y diferentes corpus, demostrando un beneficio relativo de 39.4%. En particular, ToMAP muestra una conexión compleja de razones, reduce la repetición durante el entrenamiento y genera argumentos más diversos y efectivos. Además, su capacidad para reconocer a los interlocutores es adecuada para largas conversaciones y permite tomar estrategias más racionales y con mayor reconocimiento de los interlocutores. Estos resultados destacan la eficacia de nuestro método y revelan la posibilidad de desarrollar un lenguaje grande con potencial de persuasión. El código está disponible en la siguiente URL: https://github.com/ulab-uiuc/ToMAP.",
      "upvotes": 7,
      "discussionId": "68390b30a26b4142b0578d58",
      "githubRepo": "https://github.com/ulab-uiuc/ToMAP",
      "ai_summary": "ToMAP enhances LLM persuaders with Theory of Mind modules, improving opponent awareness and argument quality.",
      "ai_keywords": [
        "large language models",
        "theory of mind (ToM)",
        "reinforcement learning",
        "text encoder",
        "MLP classifier",
        "opponent awareness",
        "reasoning chains",
        "effective arguments",
        "logical strategies"
      ]
    },
    "publishedAt": "2025-05-28T21:03:41.000Z",
    "title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind",
    "summary": "Large language models (LLMs) have shown promising potential in persuasion,\nbut existing works on training LLM persuaders are still preliminary. Notably,\nwhile humans are skilled in modeling their opponent's thoughts and opinions\nproactively and dynamically, current LLMs struggle with such Theory of Mind\n(ToM) reasoning, resulting in limited diversity and opponent awareness. To\naddress this limitation, we introduce Theory of Mind Augmented Persuader\n(ToMAP), a novel approach for building more flexible persuader agents by\nincorporating two theory of mind modules that enhance the persuader's awareness\nand analysis of the opponent's mental state. Specifically, we begin by\nprompting the persuader to consider possible objections to the target central\nclaim, and then use a text encoder paired with a trained MLP classifier to\npredict the opponent's current stance on these counterclaims. Our carefully\ndesigned reinforcement learning schema enables the persuader learns how to\nanalyze opponent-related information and utilize it to generate more effective\narguments. Experiments show that the ToMAP persuader, while containing only 3B\nparameters, outperforms much larger baselines, like GPT-4o, with a relative\ngain of 39.4% across multiple persuadee models and diverse corpora. Notably,\nToMAP exhibits complex reasoning chains and reduced repetition during training,\nwhich leads to more diverse and effective arguments. The opponent-aware feature\nof ToMAP also makes it suitable for long conversations and enables it to employ\nmore logical and opponent-aware strategies. These results underscore our\nmethod's effectiveness and highlight its potential for developing more\npersuasive language agents. Code is available at:\nhttps://github.com/ulab-uiuc/ToMAP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22961.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d188a4aa309d842e438ef1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
      "fullname": "Zijia Liu",
      "name": "m-serious",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20755",
      "authors": [
        {
          "_id": "6836a4401314d4ac39a526f3",
          "user": {
            "_id": "6838043c11b8b14a21f6ecd8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kzuNcUqFpRbM4rlHEfbrx.png",
            "isPro": false,
            "fullname": "Yifei Wang",
            "user": "smallAI",
            "type": "user"
          },
          "name": "Yifei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:43:51.221Z",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f4",
          "name": "Weimin Bai",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f5",
          "name": "Colin Zhang",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f6",
          "name": "Debing Zhang",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f7",
          "name": "Weijian Luo",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f8",
          "name": "He Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6838043c11b8b14a21f6ecd8/y_4g5XJdwhTEjDjAMTQaS.png"
      ],
      "publishedAt": "2025-05-27T05:55:45.000Z",
      "submittedOnDailyAt": "2025-05-30T02:37:16.654Z",
      "title": "Uni-Instruct: Modelo Unificado de Dif-Fachion\nInstrucción de Dif-Fachion",
      "submittedOnDailyBy": {
        "_id": "6838043c11b8b14a21f6ecd8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kzuNcUqFpRbM4rlHEfbrx.png",
        "isPro": false,
        "fullname": "Yifei Wang",
        "user": "smallAI",
        "type": "user"
      },
      "summary": "En este artículo, se integran 10 o más métodos de diferenciación de una sola etapa existentes, como Diff-Instruct, DMD, SIM, SiD y f-distill, en un marco teórico llamado \"Uni-Instruct\". Uni-Instruct se basa en la teoría de la extensión de la familia de f-divergencias propuesta por nosotros. A continuación, se presenta una teoría importante para superar los problemas de cálculo de la f-divergencia extendida, y se derivan una igualdad y una pérdida calculable que permiten entrenar eficazmente modelos de una sola etapa de diferenciación de un nivel. La nueva integración de Uni-Instruct proporciona una contribución teórica para entender los métodos existentes a un alto nivel y realiza un rendimiento de generación de un nivel de diferenciación de un solo paso más avanzado. En el benchmark de generación CIFAR10, se alcanzan nuevos récords de Frechet Inception Distance (FID) de 1.46 en generación absoluta y 1.38 en generación condicional. En el benchmark de generación ImageNet-64×64, se mejora significativamente la diferenciación de 79 pasos de instructor y se alcanza un nuevo FID de generación de un nivel de diferenciación de un solo paso más avanzado de 1.02. Además, Uni-Instruct puede aplicarse a una amplia gama de tareas, como la generación 3D y la generación 3D a partir de texto. En la generación 3D, se muestra un ligeravante superioridad en cuanto a calidad y diversidad en comparación con los métodos anteriores. La contribución teórica y experimental sólida de Uni-Instruct se espera que sea útil para la propagación del conocimiento en el diseño de modelos de diferenciación de un nivel de generación y diferenciación en el futuro.",
      "upvotes": 6,
      "discussionId": "6836a4441314d4ac39a52803",
      "ai_summary": "Uni-Instruct unifies and enhances one-step diffusion distillation methods through a novel diffusion expansion theory, achieving state-of-the-art performance in unconditional and conditional image generation and text-to-3D generation.",
      "ai_keywords": [
        "Uni-Instruct",
        "diffusion expansion theory",
        "f-divergence",
        "one-step diffusion models",
        "Frechet Inception Distance (FID)",
        "CIFAR10",
        "ImageNet-64x64",
        "text-to-3D generation",
        "SDS",
        "VSD"
      ]
    },
    "publishedAt": "2025-05-27T01:55:45.000Z",
    "title": "Uni-Instruct: One-step Diffusion Model through Unified Diffusion\n  Divergence Instruction",
    "summary": "In this paper, we unify more than 10 existing one-step diffusion distillation\napproaches, such as Diff-Instruct, DMD, SIM, SiD, f-distill, etc, inside a\ntheory-driven framework which we name the \\emph{Uni-Instruct}.\nUni-Instruct is motivated by our proposed diffusion expansion theory of the\nf-divergence family. Then we introduce key theories that overcome the\nintractability issue of the original expanded f-divergence, resulting in an\nequivalent yet tractable loss that effectively trains one-step diffusion models\nby minimizing the expanded f-divergence family. The novel unification\nintroduced by Uni-Instruct not only offers new theoretical contributions that\nhelp understand existing approaches from a high-level perspective but also\nleads to state-of-the-art one-step diffusion generation performances. On the\nCIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet\nInception Distance (FID) values of \\emph{1.46} for unconditional\ngeneration and \\emph{1.38} for conditional generation. On the\nImageNet-64times 64 generation benchmark, Uni-Instruct achieves a new SoTA\none-step generation FID of \\emph{1.02}, which outperforms its 79-step\nteacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35).\nWe also apply Uni-Instruct on broader tasks like text-to-3D generation. For\ntext-to-3D generation, Uni-Instruct gives decent results, which slightly\noutperforms previous methods, such as SDS and VSD, in terms of both generation\nquality and diversity. Both the solid theoretical and empirical contributions\nof Uni-Instruct will potentially help future studies on one-step diffusion\ndistillation and knowledge transferring of diffusion models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6838043c11b8b14a21f6ecd8/y_4g5XJdwhTEjDjAMTQaS.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20755.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6838043c11b8b14a21f6ecd8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kzuNcUqFpRbM4rlHEfbrx.png",
      "fullname": "Yifei Wang",
      "name": "smallAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23742",
      "authors": [
        {
          "_id": "683917d0ecf59de6ef345e76",
          "name": "Yufan Deng",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e77",
          "name": "Xun Guo",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e78",
          "name": "Yuanyang Yin",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e79",
          "name": "Jacob Zhiyuan Fang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7a",
          "name": "Yiding Yang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7b",
          "name": "Yizhi Wang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7c",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:49.399Z",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7d",
          "name": "Angtian Wang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7e",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7f",
          "name": "Haibin Huang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e80",
          "name": "Chongyang Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:58:15.000Z",
      "submittedOnDailyAt": "2025-05-30T00:58:48.386Z",
      "title": "MAGREF: Creación de videos con referencias arbitrarias en el guía de marco de generación de videos",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "La generación de vídeo ha experimentado un gran avance a través del desarrollo de modelos de aprendizaje profundo, especialmente con la aparición de enfoques basados en la convolución. Sin embargo, la generación de vídeo basada en múltiples propietarios encuentra grandes desafíos para mantener la coherencia y la calidad de generación alta. En este artículo, proponemos un marco de trabajo unitario denominado MAGREF, que permite la síntesis de vídeo multi-propietario basado en diferentes imágenes de referencia y pistas de texto, incluyendo un guia con máscaras. En particular, (1) proponemos una estructura dinámica de máscaras para diferentes áreas, lo que permite que el modelo pueda adaptarse flexiblemente a temas variados, como personas, objetos y fondos, sin necesidad de cambios estructurales. Además, (2) presentamos una estructura de combinación de canales por píxeles, que permite mantener mejores características de filtro en la dimensión de canales. Nuestro modelo puede expandirse de entrenamiento de propietarios únicos a escenarios complejos multi-propietarios, ofreciendo la mejor calidad de generación de vídeo actualmente disponible, y superando tanto los límites de código abierto como de líneas de base comerciales. Para evaluar nuestro trabajo, presentamos un conjunto de benchmarks de vídeo multi-propietario. Los experimentos distribuidos demuestran la efectividad de nuestro enfoque y abren una ruta para la síntesis de vídeo de alta calidad, controlable y escalable. Los códigos y modelos pueden encontrarse en la siguiente URL: https://github.com/MAGREF-Video/MAGREF",
      "upvotes": 5,
      "discussionId": "683917d2ecf59de6ef345efd",
      "projectPage": "https://magref-video.github.io/magref.github.io/",
      "githubRepo": "https://github.com/MAGREF-Video/MAGREF",
      "ai_summary": "MAGREF is a unified framework for video generation that uses masked guidance and dynamic masking for coherent multi-subject synthesis from diverse references and text prompts.",
      "ai_keywords": [
        "diffusion-based approaches",
        "multi-subject consistency",
        "unified framework",
        "masked guidance",
        "region-aware dynamic masking",
        "pixel-wise channel concatenation",
        "appearance features",
        "multi-subject video benchmark",
        "scalable",
        "controllable",
        "high-fidelity video synthesis"
      ]
    },
    "publishedAt": "2025-05-29T13:58:15.000Z",
    "title": "MAGREF: Masked Guidance for Any-Reference Video Generation",
    "summary": "Video generation has made substantial strides with the emergence of deep\ngenerative models, especially diffusion-based approaches. However, video\ngeneration based on multiple reference subjects still faces significant\nchallenges in maintaining multi-subject consistency and ensuring high\ngeneration quality. In this paper, we propose MAGREF, a unified framework for\nany-reference video generation that introduces masked guidance to enable\ncoherent multi-subject video synthesis conditioned on diverse reference images\nand a textual prompt. Specifically, we propose (1) a region-aware dynamic\nmasking mechanism that enables a single model to flexibly handle various\nsubject inference, including humans, objects, and backgrounds, without\narchitectural changes, and (2) a pixel-wise channel concatenation mechanism\nthat operates on the channel dimension to better preserve appearance features.\nOur model delivers state-of-the-art video generation quality, generalizing from\nsingle-subject training to complex multi-subject scenarios with coherent\nsynthesis and precise control over individual subjects, outperforming existing\nopen-source and commercial baselines. To facilitate evaluation, we also\nintroduce a comprehensive multi-subject video benchmark. Extensive experiments\ndemonstrate the effectiveness of our approach, paving the way for scalable,\ncontrollable, and high-fidelity multi-subject video synthesis. Code and model\ncan be found at: https://github.com/MAGREF-Video/MAGREF",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23742.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 55
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23754",
      "authors": [
        {
          "_id": "68391c6dc4405ad056a9ff79",
          "name": "Ziyin Zhang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7a",
          "name": "Jiahao Xu",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7b",
          "name": "Zhiwei He",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7c",
          "name": "Tian Liang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7d",
          "name": "Qiuzhi Liu",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7e",
          "name": "Yansi Li",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7f",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff80",
          "name": "Zhengwen Liang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff81",
          "name": "Zhuosheng Zhang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff82",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff83",
          "name": "Zhaopeng Tu",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff84",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff85",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:39.000Z",
      "submittedOnDailyAt": "2025-05-30T01:18:32.068Z",
      "title": "DeepTheorem: Teoremas y inferencia de modelos de lenguaje natural y aprendizaje por refuerzo\n\n(Note: \"Teorema\" es la traducción adecuada de \"theorem\" en español, pero si prefieres \"teorema\" para mantener el significado original de un concepto matemático, por favor avísame.)",
      "submittedOnDailyBy": {
        "_id": "660399710f1fc2f16de18072",
        "avatarUrl": "/avatars/c22a749cc45db693c2d9ea877c7cace4.svg",
        "isPro": false,
        "fullname": "Jiahao Xu",
        "user": "Jiahao004",
        "type": "user"
      },
      "summary": "El resumen es una importante prueba de los grandes modelos de lenguaje (LLMs) para evaluar sus capacidades complejas de inferencia. Sin embargo, los métodos tradicionales de pruebas automáticas de demostración (ATP) no se ajustan a las fortalezas de los LLMs en obtener conocimientos no formales en naturaleza en el tiempo de predicción. En este artículo, se propone un marco de demostración de teoremas no formales \"DeepTheorem\" para mejorar las capacidades teóricas matemáticas de los LLMs utilizando naturaleza. DeepTheorem construye un gran conjunto de datos de prueba que incluye 121K trabajos de alta calidad de teoremas no formales del nivel del IMO, anotados con precisión en exactitud, dificultad y categorías de tema, y también se han construido formas variadas de teoremas que pueden ser demostrados. Se propone una nueva estrategia de aprendizaje reforzado (RL-Zero) y se utilizan formas variadas de teoremas que pueden ser demostrados para impulsar una fuerte inferencia matemática. Además, se presentan resultados detallados para evaluar la exactitud de la demostración y la calidad de los pasos de inferencia. A través de un análisis de experimentos extendidos, DeepTheorem alcanza niveles de precisión y calidad de inferencia de nivel actual comparados con los conjuntos de datos existentes y protocolos de aprendizaje supervisado. Nuestros hallazgos claramente demuestran que DeepTheorem explora los fundamentos de la demostración automática de teoremas no formales y la exploración matemática.",
      "upvotes": 4,
      "discussionId": "68391c6ec4405ad056a9ffb2",
      "ai_summary": "DeepTheorem enhances LLM theorem-proving through a large-scale natural language dataset and a tailored reinforcement learning strategy, achieving state-of-the-art results in informal theorem proving.",
      "ai_keywords": [
        "DeepTheorem",
        "automated theorem proving",
        "ATP",
        "natural language",
        "informal theorem-proving",
        "large-scale benchmark dataset",
        "IMO-level theorems",
        "reinforcement learning",
        "RL-Zero",
        "verified theorem variants",
        "proof correctness",
        "reasoning quality"
      ]
    },
    "publishedAt": "2025-05-29T13:59:39.000Z",
    "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement Learning",
    "summary": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23754.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "660399710f1fc2f16de18072",
      "avatarUrl": "/avatars/c22a749cc45db693c2d9ea877c7cace4.svg",
      "fullname": "Jiahao Xu",
      "name": "Jiahao004",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23387",
      "authors": [
        {
          "_id": "68397697a14020a996c30112",
          "name": "Mingzhe Du",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30113",
          "name": "Luu Tuan Tuan",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30114",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30115",
          "name": "Yuhao Qing",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30116",
          "name": "Dong Huang",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30117",
          "name": "Xinyi He",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30118",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30119",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c3011a",
          "name": "See-kiong Ng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T12:14:29.000Z",
      "submittedOnDailyAt": "2025-05-30T07:47:46.606Z",
      "title": "Afterburner: Métodos de Optimización Eficiente para Códigos Automáticamente Mejorados por Aprendizaje Profundo",
      "submittedOnDailyBy": {
        "_id": "61711f02e0b1ddb56eb9b526",
        "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
        "isPro": true,
        "fullname": "Mingzhe Du",
        "user": "Elfsong",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) generan soluciones funcionalmente correctas, pero presentan problemas de eficiencia del código y desempeñan un papel crucial en la implementación real del aprendizaje automático. En este artículo, se introduce un marco de optimización iterativo basado en retroalimentación de rendimiento de ejecución empírica en un sandbox de ejecución para resolver este problema. Los LLMs modifican el código iterativamente basándose en retroalimentación de rendimiento de ejecución empírico en el sandbox de ejecución. Se revisaron tres estrategias de entrenamiento: Entrenamiento Supervisado Fino-Tuning (SFT), Optimización Directa de Preferencias (DPO), y Optimización de Políticas Relativas de Grupo (GRPO). A través de los resultados de nuestro conjunto de datos Venus y el marco de referencia APPS, SFT y DPO alcanzan rápidamente mejoras en la eficiencia. En contraste, GRPO optimiza continuamente la eficiencia del código utilizando aprendizaje por refuerzo (RL) y retroalimentación de ejecución, mejorando significativamente los resultados de pass@1 (47% a 62%) y la probabilidad de obtener resultados mejores que las sugerencias humanas en términos de eficiencia (31% a 45%). Nuestra investigación demuestra efectivamente la mejora de la eficiencia del código en tiempos de prueba y critica críticamente la posibilidad de que los LLMs puedan mejorar autonómicamente la eficiencia del código.",
      "upvotes": 4,
      "discussionId": "68397698a14020a996c30176",
      "ai_summary": "A novel test-time iterative optimization framework using reinforcement learning continuously enhances code efficiency generated by large language models.",
      "ai_keywords": [
        "Large Language Models",
        "iterative optimization",
        "closed-loop system",
        "empirical performance feedback",
        "Supervised Fine-Tuning",
        "Direct Preference Optimization",
        "Group Relative Policy Optimization",
        "reinforcement learning",
        "pass@1",
        "Venus dataset",
        "APPS benchmark"
      ]
    },
    "publishedAt": "2025-05-29T08:14:29.000Z",
    "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code\n  Efficiency Optimization",
    "summary": "Large Language Models (LLMs) generate functionally correct solutions but\noften fall short in code efficiency, a critical bottleneck for real-world\ndeployment. In this paper, we introduce a novel test-time iterative\noptimization framework to address this, employing a closed-loop system where\nLLMs iteratively refine code based on empirical performance feedback from an\nexecution sandbox. We explore three training strategies: Supervised Fine-Tuning\n(SFT), Direct Preference Optimization (DPO), and Group Relative Policy\nOptimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark\nshow that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO,\nusing reinforcement learning (RL) with execution feedback, continuously\noptimizes code performance, significantly boosting both pass@1 (from 47% to\n62%) and the likelihood of outperforming human submissions in efficiency (from\n31% to 45%). Our work demonstrates effective test-time code efficiency\nimprovement and critically reveals the power of RL in teaching LLMs to truly\nself-improve code efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61711f02e0b1ddb56eb9b526",
      "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
      "fullname": "Mingzhe Du",
      "name": "Elfsong",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21114",
      "authors": [
        {
          "_id": "68366f4410fa22cd420ae295",
          "user": {
            "_id": "66615c855fd9d736e670e0a9",
            "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
            "isPro": false,
            "fullname": "wangshuai",
            "user": "wangsssssss",
            "type": "user"
          },
          "name": "Shuai Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T08:58:12.608Z",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae296",
          "name": "Zexian Li",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae297",
          "name": "Qipeng zhang",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae298",
          "user": {
            "_id": "649e7693a83143427691769c",
            "avatarUrl": "/avatars/d04f7b3d417423abaa053375212da21f.svg",
            "isPro": false,
            "fullname": "Tianhui Song",
            "user": "sthuihui",
            "type": "user"
          },
          "name": "Tianhui Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T08:58:10.255Z",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae299",
          "name": "Xubin Li",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae29a",
          "name": "Tiezheng Ge",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae29b",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae29c",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T12:33:43.000Z",
      "submittedOnDailyAt": "2025-05-30T05:34:57.709Z",
      "title": "Exploración de Métodos Diferenciables para Sampling Rápido Distribuido",
      "submittedOnDailyBy": {
        "_id": "66615c855fd9d736e670e0a9",
        "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
        "isPro": false,
        "fullname": "wangshuai",
        "user": "wangsssssss",
        "type": "user"
      },
      "summary": "Los modelos de difusión muestran una gran calidad de generación, pero requieren muchas evaluaciones de función. Recientemente, se ha desarrollado un nuevo solver basado en ecuaciones diferenciales (ODE) para manejar eficientemente la retrodifusión. Sin embargo, estos solvers utilizan métodos multi-paso como Adams y dependen simplemente de un interpolador de Lagrange en función de t. Nosotros consideramos que un interpolador de Lagrange en función de t no es adecuado para modelos de difusión y que representa un espacio de búsqueda estrecho constituido por pasos de tiempo y coeficientes de solver. Basándonos en este análisis, proponemos un nuevo algoritmo de búsqueda de solvers diferenciables y identificamos un solver más optimizado. Como ejemplo, aplicando el solver explorado, los modelos rectified-flow de SiT-XL/2 y FlowDCN-XL/2 alcanzan un FID de 2.40 y 2.35 en ImageNet256 en 10 pasos, mientras que el DiT-XL/2 del modelo DDPM alcanza un FID de 2.33 en 10 pasos. En particular, nuestro solver explorado muestra una ventaja significativa sobre los solvers tradicionales. Además, nuestro solver explorado muestra una generalización a diferentes estructuras de modelo, resoluciones y tamaños de modelo.",
      "upvotes": 4,
      "discussionId": "68366f4a10fa22cd420ae43f",
      "githubRepo": "https://github.com/MCG-NJU/NeuralSolver",
      "ai_summary": "Researchers propose a novel differentiable solver search algorithm that optimizes the computational efficiency and quality of diffusion models for image generation tasks.",
      "ai_keywords": [
        "diffusion models",
        "ODE-based solvers",
        "Adams-like multistep methods",
        "t-related Lagrange interpolation",
        "differentiable solver search algorithm",
        "rectified-flow models",
        "SiT-XL/2",
        "FlowDCN-XL/2",
        "DDPM",
        "DiT-XL/2",
        "FID scores",
        "ImageNet256",
        "computational efficiency",
        "generality"
      ]
    },
    "publishedAt": "2025-05-27T08:33:43.000Z",
    "title": "Differentiable Solver Search for Fast Diffusion Sampling",
    "summary": "Diffusion models have demonstrated remarkable generation quality but at the\ncost of numerous function evaluations. Recently, advanced ODE-based solvers\nhave been developed to mitigate the substantial computational demands of\nreverse-diffusion solving under limited sampling steps. However, these solvers,\nheavily inspired by Adams-like multistep methods, rely solely on t-related\nLagrange interpolation. We show that t-related Lagrange interpolation is\nsuboptimal for diffusion model and reveal a compact search space comprised of\ntime steps and solver coefficients. Building on our analysis, we propose a\nnovel differentiable solver search algorithm to identify more optimal solver.\nEquipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and\nFlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256\nwith only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of\n2.33 with only 10 steps. Notably, our searched solver outperforms traditional\nsolvers by a significant margin. Moreover, our searched solver demonstrates\ngenerality across various model architectures, resolutions, and model sizes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21114.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66615c855fd9d736e670e0a9",
      "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
      "fullname": "wangshuai",
      "name": "wangsssssss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17818",
      "authors": [
        {
          "_id": "6837b52b1233747046cfa5df",
          "name": "Daeun Kyung",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e0",
          "name": "Hyunseung Chung",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e1",
          "name": "Seongsu Bae",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e2",
          "name": "Jiho Kim",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e3",
          "name": "Jae Ho Sohn",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e4",
          "name": "Taerim Kim",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e5",
          "name": "Soo Kyung Kim",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e6",
          "name": "Edward Choi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645cd00f5ebf379fd6d7a4c1/LP4mF9N1AGEXPzwV9jmbE.png"
      ],
      "publishedAt": "2025-05-23T12:34:48.000Z",
      "submittedOnDailyAt": "2025-05-30T01:25:38.142Z",
      "title": "PatientSim: Simulador Drove que realiza la interacción real entre el médico profesional y el paciente.",
      "submittedOnDailyBy": {
        "_id": "645cd00f5ebf379fd6d7a4c1",
        "avatarUrl": "/avatars/be8375ed32ac234919e26a2450bf9d38.svg",
        "isPro": false,
        "fullname": "Daeun Kyung",
        "user": "dek924",
        "type": "user"
      },
      "summary": "La conversación médica con pacientes es una serie de conversaciones que se desarrollan en varios contextos y que se adaptan a diferentes posiciones del paciente. En estas condiciones, para entrenar o evaluar un modelo de lenguaje médico como el LLM, es necesario un sistema de interacción real con pacientes. Sin embargo, los simuladores actuales no reflejan la gama completa de posiciones que se pueden ver en la práctica clínica. En este sentido, presentamos PatientSim. PatientSim genera posiciones realistas y diversas basadas en escenarios clínicos y funciona basado en conocimientos médicos. PatientSim funciona de dos maneras: 1) Utiliza datos de MIMIC-ED y MIMIC-IV, que incluyen perfiles clínicos, síntomas y enfermedades, obtenidos de la realidad mundial. 2) Define posiciones a través de cuatro ejes: características, habilidades lingüísticas, nivel de memoria de historia clínica y nivel de confusión cognitiva, generando 37 combinaciones únicas. Evaluamos la precisión real y la coincidencia de posiciones con 8 modelos de LLM. El mejor modelo abierto fue Llama 3.3, que demostró la robustez del marco de trabajo para 4 médicos. PatientSim es un plataforma abierta que puede ser personalizada y se puede adaptar según las necesidades de entrenamiento. Ofrece un entorno considerando la privacidad, permitiendo una evaluación potente de sistemas de diálogo médicos a través de la representación de diferentes pacientes, y también demostrando resultados esperados como instrumentos educativos en el cuidado de la salud.",
      "upvotes": 4,
      "discussionId": "6837b52c1233747046cfa614",
      "ai_summary": "PatientSim generates diverse and realistic patient personas using clinical data to evaluate LLMs in medical dialogue settings.",
      "ai_keywords": [
        "clinical profiles",
        "personas",
        "personality",
        "language proficiency",
        "medical history recall level",
        "cognitive confusion level",
        "patient simulator",
        "factual accuracy",
        "persona consistency",
        "privacy-compliant"
      ]
    },
    "publishedAt": "2025-05-23T08:34:48.000Z",
    "title": "PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient\n  Interactions",
    "summary": "Doctor-patient consultations require multi-turn, context-aware communication\ntailored to diverse patient personas. Training or evaluating doctor LLMs in\nsuch settings requires realistic patient interaction systems. However, existing\nsimulators often fail to reflect the full range of personas seen in clinical\npractice. To address this, we introduce PatientSim, a patient simulator that\ngenerates realistic and diverse patient personas for clinical scenarios,\ngrounded in medical expertise. PatientSim operates using: 1) clinical profiles,\nincluding symptoms and medical history, derived from real-world data in the\nMIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:\npersonality, language proficiency, medical history recall level, and cognitive\nconfusion level, resulting in 37 unique combinations. We evaluated eight LLMs\nfor factual accuracy and persona consistency. The top-performing open-source\nmodel, Llama 3.3, was validated by four clinicians to confirm the robustness of\nour framework. As an open-source, customizable platform, PatientSim provides a\nreproducible and scalable solution that can be customized for specific training\nneeds. Offering a privacy-compliant environment, it serves as a robust testbed\nfor evaluating medical dialogue systems across diverse patient presentations\nand shows promise as an educational tool for healthcare.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645cd00f5ebf379fd6d7a4c1/LP4mF9N1AGEXPzwV9jmbE.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17818.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645cd00f5ebf379fd6d7a4c1",
      "avatarUrl": "/avatars/be8375ed32ac234919e26a2450bf9d38.svg",
      "fullname": "Daeun Kyung",
      "name": "dek924",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23745",
      "authors": [
        {
          "_id": "683910d50df60182c0dd5b62",
          "user": {
            "_id": "6649fb62a460da1da20f66d0",
            "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
            "isPro": false,
            "fullname": "Hao Dong",
            "user": "hdong51",
            "type": "user"
          },
          "name": "Hao Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:56.719Z",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b63",
          "name": "Moru Liu",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b64",
          "name": "Jian Liang",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b65",
          "name": "Eleni Chatzi",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b66",
          "name": "Olga Fink",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:01.000Z",
      "submittedOnDailyAt": "2025-05-30T00:32:10.298Z",
      "title": "¿Se puede confiar en la predicción del modelo de lenguaje visuo-lingüístico?",
      "submittedOnDailyBy": {
        "_id": "6649fb62a460da1da20f66d0",
        "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
        "isPro": false,
        "fullname": "Hao Dong",
        "user": "hdong51",
        "type": "user"
      },
      "summary": "Visión-Lenguaje Modelos (VLMs) muestran una capacidad potente para corresponder precisamente a la visión y el texto, lo que los puede utilizar en una amplia gama de aplicaciones. Sin embargo, en casos de 0-shot y aprendizaje transfer, los VLMs pueden hacer clasificaciones erróneas fácilmente, con confianza pero con predicciones incorrectas. Estas limitaciones representan un gran riesgo en regiones seguras donde los errores pueden tener resultados graves. En este estudio, se presenta un marco de trabajo que se puede aplicar sin entrenamiento, llamado TrustVLM, y se propone una solución para el desafío importante de evaluar si las predicciones de un VLM son confiables. Detectamos errores entre modelos y, basándonos en que ciertos conceptos se expresan claramente en el espacio de embeddings de imagenes, proponemos una nueva función de puntuación de confianza. Se realizaron evaluaciones rigurosas utilizando 17 conjuntos de datos diferentes, 4 arquitecturas y 2 VLMs, demostrando mejoras del 51.87% en AURC, 9.14% en AUROC y 32.42% en FPR95, superando los estándares actuales y alcanzando nuevas performances. Al mejorar la confianza del modelo sin entrenamiento, TrustVLM abre el camino para el manejo seguro de VLMs en aplicaciones reales. El código está disponible en https://github.com/EPFL-IMOS/TrustVLM.",
      "upvotes": 3,
      "discussionId": "683910d70df60182c0dd5bd3",
      "githubRepo": "https://github.com/EPFL-IMOS/TrustVLM",
      "ai_summary": "TrustVLM enhances the reliability of Vision-Language Models by estimating prediction trustworthiness without retraining, improving misclassification detection in multimodal tasks.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "modality gap",
        "image embedding space",
        "confidence-scoring function",
        "AURC",
        "AUROC",
        "FPR95"
      ]
    },
    "publishedAt": "2025-05-29T13:59:01.000Z",
    "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
    "summary": "Vision-Language Models (VLMs) have demonstrated strong capabilities in\naligning visual and textual modalities, enabling a wide range of applications\nin multimodal understanding and generation. While they excel in zero-shot and\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\noften yielding confident yet incorrect predictions. This limitation poses a\nsignificant risk in safety-critical domains, where erroneous predictions can\nlead to severe consequences. In this work, we introduce TrustVLM, a\ntraining-free framework designed to address the critical challenge of\nestimating when VLM's predictions can be trusted. Motivated by the observed\nmodality gap in VLMs and the insight that certain concepts are more distinctly\nrepresented in the image embedding space, we propose a novel confidence-scoring\nfunction that leverages this space to improve misclassification detection. We\nrigorously evaluate our approach across 17 diverse datasets, employing 4\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\ncompared to existing baselines. By improving the reliability of the model\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\nVLMs in real-world applications. The code will be available at\nhttps://github.com/EPFL-IMOS/TrustVLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6649fb62a460da1da20f66d0",
      "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
      "fullname": "Hao Dong",
      "name": "hdong51",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23253",
      "authors": [
        {
          "_id": "6839325ddbf608133c740556",
          "name": "Yixun Liang",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c740557",
          "name": "Kunming Luo",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c740558",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c740559",
          "name": "Rui Chen",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c74055a",
          "name": "Hongyu Yan",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c74055b",
          "name": "Weiyu Li",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c74055c",
          "name": "Jiarui Liu",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c74055d",
          "name": "Ping Tan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647d8f65becb41a272907e7a/xUWTQ8gs16XOP8nrS-KNI.jpeg"
      ],
      "publishedAt": "2025-05-29T08:58:41.000Z",
      "submittedOnDailyAt": "2025-05-30T03:02:50.614Z",
      "title": "UniTEX: 3D forma de textura de alta calidad general generada",
      "submittedOnDailyBy": {
        "_id": "647d8f65becb41a272907e7a",
        "avatarUrl": "/avatars/f891a1986a9361b23b7c83c23031cf26.svg",
        "isPro": false,
        "fullname": "yixunliang",
        "user": "lyxun",
        "type": "user"
      },
      "summary": "UniTEX es un nuevo marco de trabajo de creación de texturas 3D en dos etapas. Este marco se utiliza para generar texturas de alta calidad en los recursos 3D. El método actual de acceso consiste en proyectar imágenes multipuntos generadas y ajustar las texturas a la forma 3D mediante un UV mapeo basado en UV, pero este método genera problemas relacionados con la topología. Para resolver esto, se propone tratar las texturas en un espacio funcional 3D unificado, evitando los límites de la proyección UV. Específicamente, se propone primero la textura funcional (TF) para listar texturas en el espacio 3D. Esta función utiliza una representación continua y volumétrica basada en pequeños bloques para mapear los valores de textura. Para predecir esta función de textura a partir de imágenes y geometría, se utiliza un modelo grande de textura basado en Transformers (LTM). Además, para mejorar la calidad de las texturas y utilizar fuertes proyecciones 2D, se desarrolla una estrategia avanzada basada en LoRA para aplicar DiTs de manera eficiente, lo que permite la síntesis de texturas de alta calidad en el primer paso. Los experimentos muestran que UniTEX implementa una calidad visual avanzada y una consistencia de textura en comparación con el método actual, ofreciendo una solución escalable y generalizable para la generación automática de texturas 3D. El código está disponible en la siguiente URL: https://github.com/YixunLiang/UniTEX.",
      "upvotes": 3,
      "discussionId": "6839325fdbf608133c7405dc",
      "githubRepo": "https://github.com/YixunLiang/UniTEX",
      "ai_summary": "UniTEX generates high-quality, consistent 3D textures by using Texture Functions and adapting Diffusion Transformers directly from images and geometry without UV mapping.",
      "ai_keywords": [
        "Texture Functions",
        "transformer-based Large Texturing Model",
        "LoRA",
        "Diffusion Transformers"
      ]
    },
    "publishedAt": "2025-05-29T04:58:41.000Z",
    "title": "UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes",
    "summary": "We present UniTEX, a novel two-stage 3D texture generation framework to\ncreate high-quality, consistent textures for 3D assets. Existing approaches\npredominantly rely on UV-based inpainting to refine textures after reprojecting\nthe generated multi-view images onto the 3D shapes, which introduces challenges\nrelated to topological ambiguity. To address this, we propose to bypass the\nlimitations of UV mapping by operating directly in a unified 3D functional\nspace. Specifically, we first propose that lifts texture generation into 3D\nspace via Texture Functions (TFs)--a continuous, volumetric representation that\nmaps any 3D point to a texture value based solely on surface proximity,\nindependent of mesh topology. Then, we propose to predict these TFs directly\nfrom images and geometry inputs using a transformer-based Large Texturing Model\n(LTM). To further enhance texture quality and leverage powerful 2D priors, we\ndevelop an advanced LoRA-based strategy for efficiently adapting large-scale\nDiffusion Transformers (DiTs) for high-quality multi-view texture synthesis as\nour first stage. Extensive experiments demonstrate that UniTEX achieves\nsuperior visual quality and texture integrity compared to existing approaches,\noffering a generalizable and scalable solution for automated 3D texture\ngeneration. Code will available in: https://github.com/YixunLiang/UniTEX.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647d8f65becb41a272907e7a/xUWTQ8gs16XOP8nrS-KNI.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d8f65becb41a272907e7a",
      "avatarUrl": "/avatars/f891a1986a9361b23b7c83c23031cf26.svg",
      "fullname": "yixunliang",
      "name": "lyxun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18087",
      "authors": [
        {
          "_id": "68351b91037632c04211688d",
          "user": {
            "_id": "6837b9a63ed37b18326c7fff",
            "avatarUrl": "/avatars/7aaf5e1ff783aad45946c937ac36bdd8.svg",
            "isPro": false,
            "fullname": "Hyungyung Lee",
            "user": "ttumyche",
            "type": "user"
          },
          "name": "Hyungyung Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:44:18.221Z",
          "hidden": false
        },
        {
          "_id": "68351b91037632c04211688e",
          "name": "Geon Choi",
          "hidden": false
        },
        {
          "_id": "68351b91037632c04211688f",
          "name": "Jung-Oh Lee",
          "hidden": false
        },
        {
          "_id": "68351b91037632c042116890",
          "name": "Hangyul Yoon",
          "hidden": false
        },
        {
          "_id": "68351b91037632c042116891",
          "name": "Hyuk Gi Hong",
          "hidden": false
        },
        {
          "_id": "68351b91037632c042116892",
          "name": "Edward Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T16:44:21.000Z",
      "submittedOnDailyAt": "2025-05-30T00:18:09.180Z",
      "title": "CXReasonBench: CXReasonBench es un marco de referencia para evaluar la teoría de diagnóstico estructural de las imágenes de rayos X de pares.",
      "submittedOnDailyBy": {
        "_id": "6837b9a63ed37b18326c7fff",
        "avatarUrl": "/avatars/7aaf5e1ff783aad45946c937ac36bdd8.svg",
        "isPro": false,
        "fullname": "Hyungyung Lee",
        "user": "ttumyche",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los grandes modelos de lenguaje visuo-lingüístico (LVLMs) ha permitido aplicaciones esperadas en el trabajo médico, como la generación de informes y la respuesta a preguntas de visualización. Sin embargo, los actuales benchmarks se centran principalmente en las respuestas finales de diagnóstico, proporcionando una visión limitada sobre si los modelos realizan inferencias clínicamente significativas. En respuesta a esto, presentamos CheXStruct y CXReasonBench, un pipila de estructurado y un benchmark basado en el dataset MIMIC-CXR-JPG públicamente disponible. CheXStruct extrae automáticamente los pasos intermedios de inferencia directamente de las imágenes de radiografías de cuerpo. Por ejemplo, se realizan tareas como la segmentación de áreas anatómicas, la extracción de marcadores anatómicos, la medición de diagnóstico, el cálculo de índices de diagnóstico y la aplicación de umbrales clínicos. CXReasonBench utiliza este pipila para evaluar si los modelos realizan inferencias clínicamente justificadas y si han aprendido de las guías estructuradas, así como para evaluar con precisión diferentes aspectos de la inferencia diagnóstica. Este benchmark está constituido por 12 trabajos de diagnóstico y 1,200 casos, proporcionando 18,988 pares de preguntas y respuestas, y apoya evaluaciones multi-rutina y multi-etapa, incluyendo la normalización visual por selección de áreas anatómicas y la medición de diagnóstico. Además, 10 de los LVLMs más potentes evaluados fallan en la conexión entre la inferencia estructurada, la generalización, el conocimiento abstracto y la interpretación visual anatómica basada. El código está disponible en https://github.com/ttumyche/CXReasonBench.",
      "upvotes": 3,
      "discussionId": "68351b97037632c0421169d0",
      "githubRepo": "https://github.com/ttumyche/CXReasonBench",
      "ai_summary": "CheXStruct and CXReasonBench evaluate Large Vision-Language Models in clinical diagnosis by assessing structured reasoning, visual grounding, and generalization using the MIMIC-CXR-JPG dataset.",
      "ai_keywords": [
        "Large Vision-Language Models",
        "LVLMs",
        "CheXStruct",
        "CXReasonBench",
        "MIMIC-CXR-JPG",
        "structured reasoning",
        "visual question answering",
        "medical tasks",
        "report generation",
        "anatomical regions",
        "diagnostic measurements",
        "diagnostic indices",
        "clinical thresholds",
        "visual grounding",
        "diagnostic reasoning",
        "multi-path",
        "multi-stage evaluation"
      ]
    },
    "publishedAt": "2025-05-23T12:44:21.000Z",
    "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic\n  Reasoning in Chest X-rays",
    "summary": "Recent progress in Large Vision-Language Models (LVLMs) has enabled promising\napplications in medical tasks, such as report generation and visual question\nanswering. However, existing benchmarks focus mainly on the final diagnostic\nanswer, offering limited insight into whether models engage in clinically\nmeaningful reasoning. To address this, we present CheXStruct and CXReasonBench,\na structured pipeline and benchmark built on the publicly available\nMIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of\nintermediate reasoning steps directly from chest X-rays, such as segmenting\nanatomical regions, deriving anatomical landmarks and diagnostic measurements,\ncomputing diagnostic indices, and applying clinical thresholds. CXReasonBench\nleverages this pipeline to evaluate whether models can perform clinically valid\nreasoning steps and to what extent they can learn from structured guidance,\nenabling fine-grained and transparent assessment of diagnostic reasoning. The\nbenchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,\neach paired with up to 4 visual inputs, and supports multi-path, multi-stage\nevaluation including visual grounding via anatomical region selection and\ndiagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with\nstructured reasoning and generalization, often failing to link abstract\nknowledge with anatomically grounded visual interpretation. The code is\navailable at https://github.com/ttumyche/CXReasonBench",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18087.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6837b9a63ed37b18326c7fff",
      "avatarUrl": "/avatars/7aaf5e1ff783aad45946c937ac36bdd8.svg",
      "fullname": "Hyungyung Lee",
      "name": "ttumyche",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14321",
      "authors": [
        {
          "_id": "68396605ac00da416d094bbf",
          "name": "Bo Feng",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc0",
          "name": "Zhengfeng Lai",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc1",
          "name": "Shiyu Li",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc2",
          "name": "Zizhen Wang",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc3",
          "name": "Simon Wang",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc4",
          "name": "Ping Huang",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc5",
          "name": "Meng Cao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T13:07:55.000Z",
      "submittedOnDailyAt": "2025-05-30T06:32:58.958Z",
      "title": "Video LLM benchmarks: Deconstructing knowledge, spatial awareness, or real-time series understanding?",
      "submittedOnDailyBy": {
        "_id": "66b5295f83425904fa7a1a6a",
        "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
        "isPro": false,
        "fullname": "Zhengfeng Lai",
        "user": "jefflai",
        "type": "user"
      },
      "summary": "Los actuales marcadores de rendimiento para el entendimiento de imágenes confunden las preguntas basadas en conocimiento y las basadas en imágenes simples, y no distinguen claramente la capacidad lógica temporal del modelo, lo que hace que la diferencia entre diferentes modelos de entendimiento de imágenes sea casi invisible. Hemos identificado dos principales límites en los marcadores de rendimiento que impiden determinar claramente si un modelo entiende el contenido dinámico de una imagen: 1) el fuerte principio lingüístico, que sugiere que el modelo no necesita ver la imagen para responder a una pregunta, y 2) la invarianza al desorden, que implica que el rendimiento del modelo se mantenga constante incluso cuando los frame de la imagen se desordenen en cualquier secuencia temporal. Para resolver estos problemas, proponemos una nueva pipeline automatizada llamada VBenchComp, y clasificamos las preguntas en diferentes dominios: LLM-Answerable, Semantic y Temporal. En particular, las preguntas de LLM-Answerable no requieren ver la imagen, las de Semantic pueden responderse incluso si los frame están desordenados, y las de Temporal requieren un entendimiento de la secuencia temporal correcta de los frames. Las demás preguntas se etiquetan como \"Otros\". De esta manera, se puede evaluar de manera clara las diferentes capacidades de los modelos de LLM de imágenes. Nuestro análisis revela los pequeños debilidades ocultas de los modelos en los marcadores tradicionales, ofrece consejos y recomendaciones para el diseño de marcadores de rendimiento, y proporciona una guía para evaluar de manera más precisa los modelos de LLM de imágenes.",
      "upvotes": 3,
      "discussionId": "68396607ac00da416d094c6c",
      "ai_summary": "VBenchComp, an automated pipeline, categorizes video LLM questions into different domains to evaluate temporal reasoning and isolate model weaknesses beyond overall scores.",
      "ai_keywords": [
        "video understanding",
        "LLM-Answerable",
        "Semantic",
        "Temporal"
      ]
    },
    "publishedAt": "2025-05-20T09:07:55.000Z",
    "title": "Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or\n  True Temporal Understanding?",
    "summary": "Existing video understanding benchmarks often conflate knowledge-based and\npurely image-based questions, rather than clearly isolating a model's temporal\nreasoning ability, which is the key aspect that distinguishes video\nunderstanding from other modalities. We identify two major limitations that\nobscure whether higher scores truly indicate stronger understanding of the\ndynamic content in videos: (1) strong language priors, where models can answer\nquestions without watching the video; and (2) shuffling invariance, where\nmodels maintain similar performance on certain questions even when video frames\nare temporally shuffled. To alleviate these issues, we propose VBenchComp, an\nautomated pipeline that categorizes questions into different domains:\nLLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions\ncan be answered without viewing the video; Semantic questions remain answerable\neven when the video frames are shuffled; and Temporal questions require\nunderstanding the correct temporal order of frames. The rest of the questions\nare labeled as Others. This can enable fine-grained evaluation of different\ncapabilities of a video LLM. Our analysis reveals nuanced model weaknesses that\nare hidden by traditional overall scores, and we offer insights and\nrecommendations for designing future benchmarks that more accurately assess\nvideo LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14321.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b5295f83425904fa7a1a6a",
      "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
      "fullname": "Zhengfeng Lai",
      "name": "jefflai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23761",
      "authors": [
        {
          "_id": "6839107ed762b7c617b0731b",
          "user": {
            "_id": "67578bf874cf42cdedbf00df",
            "avatarUrl": "/avatars/5981300ee0025c876973f94c81a85b19.svg",
            "isPro": false,
            "fullname": "Yunjae Won",
            "user": "yunjae-won",
            "type": "user"
          },
          "name": "Yunjae Won",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:04.071Z",
          "hidden": false
        },
        {
          "_id": "6839107ed762b7c617b0731c",
          "name": "Hyunji Lee",
          "hidden": false
        },
        {
          "_id": "6839107ed762b7c617b0731d",
          "name": "Hyeonbin Hwang",
          "hidden": false
        },
        {
          "_id": "6839107ed762b7c617b0731e",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:50.000Z",
      "submittedOnDailyAt": "2025-05-30T05:38:24.883Z",
      "title": "\"Información Gradual: Optimización de Preferencias desde la Perspectiva de la Teoría de la Información\"",
      "submittedOnDailyBy": {
        "_id": "67578bf874cf42cdedbf00df",
        "avatarUrl": "/avatars/5981300ee0025c876973f94c81a85b19.svg",
        "isPro": false,
        "fullname": "Yunjae Won",
        "user": "yunjae-won",
        "type": "user"
      },
      "summary": "La optimización directa (DPO) es uno de los métodos de aprendizaje por observación, se ha establecido como el método estándar para ajustar modelos de lenguaje a las preferencias humanas. A pesar de sus éxitos experimentales, la teoría explicativa de la parametrización de la recompensa proporcional a la lograr no es perfecta. En este estudio, se utiliza la distribución de información de la varianza (DID) para completar estas deficiencias: se refiere a la distribución de las secuencias de tokens que detectan la información obtenida en el actualización de la política. Primero, si las etiquetas de preferencia incluyen la DID y se necesita convertir la política de referencia en la política objetivo, la recompensa proporcional a la lograr en DPO se muestra como la forma más adecuada para el aprendizaje de la política objetivo bajo la optimización de preferencias. Este resultado proporciona una natural representación cerrada de la distribución óptima de muestreo para las respuestas rechazadas. Además, las condiciones en las que la preferencia incluye la DID están estrechamente relacionadas con el sesgo de índice en las funciones de recompensa monotónica. Este sesgo de índice ha sido ampliamente utilizado en la optimización de preferencias pero no había sido reconocido anteriormente. Finalmente, se analiza la entropía de la DID, explicando cómo la información de baja entropía fortalece la distribución de la política y que la información de alta entropía causa efectos de ruido, y se interpreta el fenómeno de movimiento de la similitud de logaritmo. Se verifican los hallazgos teóricos a través de experimentos sintéticos y se expandieron a conjuntos de datos de instrucciones reales. Los resultados muestran que el aprendizaje de la información de baja entropía es importante para la resolución de problemas relacionados con la información general, mientras que el aprendizaje de la información de alta entropía es beneficioso para la resolución de problemas relacionados con la información específica. En general, este estudio proporciona una perspectiva coherente sobre los objetivos de la DPO, la estructura de los datos de preferencia y las acciones de política que resultan.",
      "upvotes": 2,
      "discussionId": "6839107fd762b7c617b07385",
      "ai_summary": "Theoretical analysis of Direct Preference Optimization (DPO) reveals that log-ratio reward parameterization is optimal for learning target policy via preference optimization, linked to log-margin ordered policies, and explains policy reinforcement and smoothing based on differential information entropy.",
      "ai_keywords": [
        "Direct Preference Optimization",
        "DPO",
        "log-ratio reward parameterization",
        "Differential Information Distribution",
        "DID",
        "token sequences",
        "policy updates",
        "preference labels",
        "target policy",
        "preference optimization",
        "log-margin ordered policies",
        "entropy",
        "differential information entropy",
        "log-likelihood displacement",
        "instruction-following datasets",
        "knowledge-intensive question answering"
      ]
    },
    "publishedAt": "2025-05-29T13:59:50.000Z",
    "title": "Differential Information: An Information-Theoretic Perspective on\n  Preference Optimization",
    "summary": "Direct Preference Optimization (DPO) has become a standard technique for\naligning language models with human preferences in a supervised manner. Despite\nits empirical success, the theoretical justification behind its log-ratio\nreward parameterization remains incomplete. In this work, we address this gap\nby utilizing the Differential Information Distribution (DID): a distribution\nover token sequences that captures the information gained during policy\nupdates. First, we show that when preference labels encode the differential\ninformation required to transform a reference policy into a target policy, the\nlog-ratio reward in DPO emerges as the uniquely optimal form for learning the\ntarget policy via preference optimization. This result naturally yields a\nclosed-form expression for the optimal sampling distribution over rejected\nresponses. Second, we find that the condition for preferences to encode\ndifferential information is fundamentally linked to an implicit assumption\nregarding log-margin ordered policies-an inductive bias widely used in\npreference optimization yet previously unrecognized. Finally, by analyzing the\nentropy of the DID, we characterize how learning low-entropy differential\ninformation reinforces the policy distribution, while high-entropy differential\ninformation induces a smoothing effect, which explains the log-likelihood\ndisplacement phenomenon. We validate our theoretical findings in synthetic\nexperiments and extend them to real-world instruction-following datasets. Our\nresults suggest that learning high-entropy differential information is crucial\nfor general instruction-following, while learning low-entropy differential\ninformation benefits knowledge-intensive question answering. Overall, our work\npresents a unifying perspective on the DPO objective, the structure of\npreference data, and resulting policy behaviors through the lens of\ndifferential information.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23761.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67578bf874cf42cdedbf00df",
      "avatarUrl": "/avatars/5981300ee0025c876973f94c81a85b19.svg",
      "fullname": "Yunjae Won",
      "name": "yunjae-won",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23625",
      "authors": [
        {
          "_id": "683924d1ac00da416df936dd",
          "name": "Chao Huang",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936de",
          "name": "Yuesheng Ma",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936df",
          "name": "Junxuan Huang",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e0",
          "name": "Susan Liang",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e1",
          "name": "Yunlong Tang",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e2",
          "name": "Jing Bi",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e3",
          "name": "Wenqiang Liu",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e4",
          "name": "Nima Mesgarani",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e5",
          "name": "Chenliang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:31:45.000Z",
      "submittedOnDailyAt": "2025-05-30T01:54:58.337Z",
      "title": "ZeroSep: Puede separar cualquier cosa en el voz sin entrenarlo.",
      "submittedOnDailyBy": {
        "_id": "67257ee0938e718957c9c100",
        "avatarUrl": "/avatars/db1f792ee1a5d860ea4e98c11a016a1b.svg",
        "isPro": false,
        "fullname": "Chao Huang",
        "user": "ChaoHuangCS",
        "type": "user"
      },
      "summary": "La separación de sonidos es una tecnología básica que permite a los dispositivos comprender ambientes acústicos complejos, formando la base para múltiples aplicaciones sonoras. El enfoque actual de aprendizaje profundo con subgrupos, aunque potente, está limitado por la necesidad de datos estandarizados y específicos para tareas, así como por problemas de generalización ante la gran variabilidad de entornos acústicos reales y la característica de conjuntos abiertos. Basándonos en el éxito del Fundamental Model de generación, investigamos si es posible superar estas limitaciones. Como resultado, hemos encontrado un modelo de separación de sonidos guiado por texto que puede aplicarse a cualquier tarea, logrando separación de sonidos de 0 shot. Este método se llama ZeroSep y funciona invertiendo el espacio potencial del modelo de separación para retroalimentar el sonido mixto y guiando el proceso de desnivelado mediante condiciones de texto para recuperar fuentes individuales. No es necesario entrenar o fine-tunar el modelo para tareas específicas; ZeroSep puede reutilizar el modelo de generación para tareas de separación de sonidos y apoyar escenarios abiertos a través de un juego de texto rico. ZeroSep es compatible con varios modelos de separación de sonidos guiados por texto y ofrece un excelente rendimiento en benchmarks de separación de sonidos, superando en este proceso el enfoque con subgrupos.",
      "upvotes": 2,
      "discussionId": "683924d6ac00da416df937f6",
      "ai_summary": "ZeroSep, a text-guided audio diffusion model, achieves zero-shot source separation through pre-trained models and text conditioning, outperforming supervised methods on various benchmarks.",
      "ai_keywords": [
        "audio source separation",
        "supervised deep learning",
        "generative foundation models",
        "text-guided audio diffusion models",
        "zero-shot source separation",
        "latent space",
        "denoising process",
        "discriminative separation task",
        "textual priors",
        "open-set scenarios"
      ]
    },
    "publishedAt": "2025-05-29T12:31:45.000Z",
    "title": "ZeroSep: Separate Anything in Audio with Zero Training",
    "summary": "Audio source separation is fundamental for machines to understand complex\nacoustic environments and underpins numerous audio applications. Current\nsupervised deep learning approaches, while powerful, are limited by the need\nfor extensive, task-specific labeled data and struggle to generalize to the\nimmense variability and open-set nature of real-world acoustic scenes. Inspired\nby the success of generative foundation models, we investigate whether\npre-trained text-guided audio diffusion models can overcome these limitations.\nWe make a surprising discovery: zero-shot source separation can be achieved\npurely through a pre-trained text-guided audio diffusion model under the right\nconfiguration. Our method, named ZeroSep, works by inverting the mixed audio\ninto the diffusion model's latent space and then using text conditioning to\nguide the denoising process to recover individual sources. Without any\ntask-specific training or fine-tuning, ZeroSep repurposes the generative\ndiffusion model for a discriminative separation task and inherently supports\nopen-set scenarios through its rich textual priors. ZeroSep is compatible with\na variety of pre-trained text-guided audio diffusion backbones and delivers\nstrong separation performance on multiple separation benchmarks, surpassing\neven supervised methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67257ee0938e718957c9c100",
      "avatarUrl": "/avatars/db1f792ee1a5d860ea4e98c11a016a1b.svg",
      "fullname": "Chao Huang",
      "name": "ChaoHuangCS",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22918",
      "authors": [
        {
          "_id": "68393b9ef0458c8bcb652d78",
          "user": {
            "_id": "6814e98abbe5b8a5d92fc335",
            "avatarUrl": "/avatars/83c1963280bc94fdc7a2b21f6e0f1c59.svg",
            "isPro": false,
            "fullname": "Ruichen Chen",
            "user": "crc5577",
            "type": "user"
          },
          "name": "Ruichen Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T05:01:23.240Z",
          "hidden": false
        },
        {
          "_id": "68393b9ef0458c8bcb652d79",
          "user": {
            "_id": "661c391720b47b0daddfcc5a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RwvDJRtlEEPch27xId9Cb.png",
            "isPro": false,
            "fullname": "Keith G. Mills",
            "user": "kgmills",
            "type": "user"
          },
          "name": "Keith G. Mills",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T06:54:23.854Z",
          "hidden": false
        },
        {
          "_id": "68393b9ef0458c8bcb652d7a",
          "user": {
            "_id": "64ac49ccb7d86b40fd60a8dd",
            "avatarUrl": "/avatars/e9f5482cffdd1d5917523a496a3805f0.svg",
            "isPro": false,
            "fullname": "Liyao Jiang",
            "user": "LiyaoJiang",
            "type": "user"
          },
          "name": "Liyao Jiang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T05:01:59.348Z",
          "hidden": false
        },
        {
          "_id": "68393b9ef0458c8bcb652d7b",
          "name": "Chao Gao",
          "hidden": false
        },
        {
          "_id": "68393b9ef0458c8bcb652d7c",
          "name": "Di Niu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T22:39:12.000Z",
      "submittedOnDailyAt": "2025-05-30T06:23:07.403Z",
      "title": "Retención: Generación de Memoria Visible Ultraesparsa para Re-Shaping Estadístico de Atención",
      "submittedOnDailyBy": {
        "_id": "6814e98abbe5b8a5d92fc335",
        "avatarUrl": "/avatars/83c1963280bc94fdc7a2b21f6e0f1c59.svg",
        "isPro": false,
        "fullname": "Ruichen Chen",
        "user": "crc5577",
        "type": "user"
      },
      "summary": "Los Transformadores de Difusión (DiT) han adquirido la posición de modelo estándar en la generación de contenido visual de alta calidad (por ejemplo, películas e imágenes). Una de las principales desventajas es la estructura de atención que aumenta cuadraticamente con la resolución y la longitud de la película. Una manera razonable de reducir esta carga es utilizar atención esparsa, pero actualmente, las tecnologías no pueden mantener la calidad a niveles muy altos de esparsidad sin incurrir en sobrecargas computacionales. Para resolver estos problemas, proponemos Re-ttention. Re-ttention utiliza la extensibilidad temporal de los modelos de difusión para superar la normalización escalar probabilística dentro de la estructura de atención y lograr una atención muy esparsa. Concretamente, Re-ttention ajusta los puntajes de atención para mantener la calidad de todos los escalados cuadráticos utilizando la historia de la distribución softmax predecida. En los resultados de experimentos con modelos T2V/T2I, como CogVideoX y PixArt DiTs, Re-ttention requiere solo el 3.1% de los tokens en la inferencia, superando a métodos modernos como FastDiTAttn, Sparse VideoGen y MInference. Además, medimos la reducción de la escala latina desde GPUs H100 y demostramos que se puede lograr una reducción de la escala latina de más del 45% y un 92% de la auto-atención latina con un costo ignorable. El código está disponible en el siguiente enlace.",
      "upvotes": 2,
      "discussionId": "68393ba3f0458c8bcb652e60",
      "ai_summary": "Re-ttention uses temporal redundancy in diffusion models to enable high sparse attention in visual generation, maintaining quality with minimal computational overhead.",
      "ai_keywords": [
        "Diffusion Transformers",
        "sparse attention",
        "visual generation models",
        "probabilistic normalization shift",
        "Re-ttention",
        "attention scores",
        "softmax distribution",
        "latency reduction",
        "H100 GPU"
      ]
    },
    "publishedAt": "2025-05-28T18:39:12.000Z",
    "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical\n  Reshape",
    "summary": "Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. % To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. % Experimental\nresults on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate\nthat Re-ttention requires as few as 3.1\\% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference. Further, we measure latency to show that our method can attain over\n45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU\nat negligible overhead cost.\n  Code available online here:\nhttps://github.com/cccrrrccc/Re-ttention{https://github.com/cccrrrccc/Re-ttention}",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22918.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6814e98abbe5b8a5d92fc335",
      "avatarUrl": "/avatars/83c1963280bc94fdc7a2b21f6e0f1c59.svg",
      "fullname": "Ruichen Chen",
      "name": "crc5577",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22765",
      "authors": [
        {
          "_id": "68394fb7f1e62a44f53ecf82",
          "user": {
            "_id": "658436f5c73f74776b19198a",
            "avatarUrl": "/avatars/3f1d76af6fc0405d663c9294318fe83e.svg",
            "isPro": false,
            "fullname": "Iddo Yosha",
            "user": "iyosha",
            "type": "user"
          },
          "name": "Iddo Yosha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:52:57.738Z",
          "hidden": false
        },
        {
          "_id": "68394fb7f1e62a44f53ecf83",
          "user": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "isPro": false,
            "fullname": "Gallil Maimon",
            "user": "gallilmaimon",
            "type": "user"
          },
          "name": "Gallil Maimon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:52:41.076Z",
          "hidden": false
        },
        {
          "_id": "68394fb7f1e62a44f53ecf84",
          "name": "Yossi Adi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T18:32:56.000Z",
      "submittedOnDailyAt": "2025-05-30T05:20:21.736Z",
      "title": "Estréstest: ¿Puede tu modelo de lenguaje aceptar estrés?",
      "submittedOnDailyBy": {
        "_id": "66b9bc2dacdbc1d0b39c3b50",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
        "isPro": false,
        "fullname": "Gallil Maimon",
        "user": "gallilmaimon",
        "type": "user"
      },
      "summary": "El contexto-centrico es un énfasis en una palabra específica de una frase de conversación para destacar la énfasis o la contraposición de una idea o para introducir nueva información. Esto se utiliza principalmente para indicar un propósito potencial. El desarrollo reciente de los modelos de lenguaje de voz para escuchar (SLMs) ha permitido utilizar toda la rica información del señal de voz para abordar tareas de procesamiento de voz. El contexto-centrico desempeña un papel importante en la formación del significado del contexto y en la intención del interlocutor, pero estos modelos suelen ser poco considerados en su evaluación y desarrollo. En este estudio, hemos introducido el benchmark StressTest, diseñado para evaluar la capacidad de modelos que interpretan frases de conversación basándose en patrones contexto-centricos. Hemos evaluado el rendimiento de los SLMs y comparado sus capacidades generales, encontrando que algunos muestran bajos rendimientos. Para superar estas limitaciones, hemos propuesto un nuevo proceso de generación de datos sintéticos y hemos creado un conjunto de entrenamiento, Stress17k, que simula cambios en los símbolos para representar cambios en el significado. Usando este conjunto de datos, hemos optimizado los modelos para que coincidan con el habla real y facilitar la fine-tuning efectivo de los SLMs. Los resultados muestran que nuestro modelo fine-tuned, StressSLM, supera a los modelos actuales en tareas de contexto-centrico y detección de significado significativamente. Los códigos, modelos, datos y ejemplos de voz están disponibles en pages.cs.huji.ac.il/adiyoss-lab/stresstest.",
      "upvotes": 2,
      "discussionId": "68394fb8f1e62a44f53ecfa4",
      "projectPage": "https://pages.cs.huji.ac.il/adiyoss-lab/stresstest/",
      "githubRepo": "https://github.com/slp-rl/StressTest",
      "ai_summary": "A StressTest benchmark and synthetic Stress17k dataset are introduced to improve speech-aware language models' ability to interpret sentence stress in spoken language.",
      "ai_keywords": [
        "speech-aware language models",
        "spoken question answering",
        "sentence stress",
        "benchmark",
        "synthetic data generation pipeline",
        "audio reasoning",
        "sentence stress reasoning and detection tasks"
      ]
    },
    "publishedAt": "2025-05-28T14:32:56.000Z",
    "title": "StressTest: Can YOUR Speech LM Handle the Stress?",
    "summary": "Sentence stress refers to emphasis, placed on specific words within a spoken\nutterance to highlight or contrast an idea, or to introduce new information. It\nis often used to imply an underlying intention that is not explicitly stated.\nRecent advances in speech-aware language models (SLMs) have enabled direct\nprocessing of audio, allowing models to bypass transcription and access the\nfull richness of the speech signal and perform audio reasoning tasks such as\nspoken question answering. Despite the crucial role of sentence stress in\nshaping meaning and speaker intent, it remains largely overlooked in evaluation\nand development of such models. In this work, we address this gap by\nintroducing StressTest, a benchmark specifically designed to evaluate a model's\nability to distinguish between interpretations of spoken sentences based on the\nstress pattern. We assess the performance of several leading SLMs and find\nthat, despite their overall capabilities, they perform poorly on such tasks. To\novercome this limitation, we propose a novel synthetic data generation\npipeline, and create Stress17k, a training set that simulates change of meaning\nimplied by stress variation. Then, we empirically show that optimizing models\nwith this synthetic dataset aligns well with real-world recordings and enables\neffective finetuning of SLMs. Results suggest, that our finetuned model,\nStresSLM, significantly outperforms existing models on both sentence stress\nreasoning and detection tasks. Code, models, data, and audio samples -\npages.cs.huji.ac.il/adiyoss-lab/stresstest.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22765.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b9bc2dacdbc1d0b39c3b50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
      "fullname": "Gallil Maimon",
      "name": "gallilmaimon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20282",
      "authors": [
        {
          "_id": "683889f78c8e3b72170f6412",
          "user": {
            "_id": "641ddac5be3bd3a5a06ed4a4",
            "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
            "isPro": false,
            "fullname": "zitian gao",
            "user": "zgao3186",
            "type": "user"
          },
          "name": "Zitian Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:47.238Z",
          "hidden": false
        },
        {
          "_id": "683889f78c8e3b72170f6413",
          "name": "Lynx Chen",
          "hidden": false
        },
        {
          "_id": "683889f78c8e3b72170f6414",
          "name": "Joey Zhou",
          "hidden": false
        },
        {
          "_id": "683889f78c8e3b72170f6415",
          "name": "Bryan Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T17:58:30.000Z",
      "submittedOnDailyAt": "2025-05-30T01:51:16.066Z",
      "title": "1 Minimización de Entropía de Pick",
      "submittedOnDailyBy": {
        "_id": "641ddac5be3bd3a5a06ed4a4",
        "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
        "isPro": false,
        "fullname": "zitian gao",
        "user": "zgao3186",
        "type": "user"
      },
      "summary": "Se entrenó 13,440 modelos de lenguaje de texto y se logró mejorar el rendimiento mediante la minimización de la entropía con un conjunto de datos sin etiquetas único y 10 etapas de optimización, utilizando un aprendizaje de retroalimentación basado en reglas con decenas de datos y una recompensa rigurosamente diseñada. Estos resultados sorprendentes han impulsado una reflexión sobre el paradigma posterior al entrenamiento de modelos de lenguaje de gran escala. El código está disponible en https://github.com/zitian-gao/one-shot-em.",
      "upvotes": 2,
      "discussionId": "683889f78c8e3b72170f643d",
      "projectPage": "https://www.notion.so/One-shot-Entropy-Minimization-202606db813b80639773f850f39246a5",
      "githubRepo": "https://github.com/zitian-gao/one-shot-em",
      "ai_summary": "Entropy minimization with one sample and minimal optimization achieves significant performance improvements for large language models.",
      "ai_keywords": [
        "large language models",
        "entropy minimization",
        "unlabeled data",
        "optimization",
        "rule-based reinforcement learning",
        "post-training paradigms"
      ]
    },
    "publishedAt": "2025-05-26T13:58:30.000Z",
    "title": "One-shot Entropy Minimization",
    "summary": "We trained 13,440 large language models and found that entropy minimization\nrequires only a single unlabeled data and 10 steps optimization to achieve\nperformance improvements comparable to or even greater than those obtained\nusing thousands of data and carefully designed rewards in rule-based\nreinforcement learning. This striking result may prompt a rethinking of\npost-training paradigms for large language models. Our code is avaliable at\nhttps://github.com/zitian-gao/one-shot-em.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20282.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641ddac5be3bd3a5a06ed4a4",
      "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
      "fullname": "zitian gao",
      "name": "zgao3186",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19360",
      "authors": [
        {
          "_id": "68392279896eb9ceb71fac39",
          "name": "Manan Suri",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3a",
          "user": {
            "_id": "65c16444d4c3b8dff2f0d78d",
            "avatarUrl": "/avatars/4ed764c1657bd260d2a12ba61c111062.svg",
            "isPro": false,
            "fullname": "Puneet Mathur",
            "user": "puneetm",
            "type": "user"
          },
          "name": "Puneet Mathur",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T03:14:02.414Z",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3b",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3c",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:28.558Z",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3d",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3e",
          "name": "Dinesh Manocha",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T23:17:32.000Z",
      "submittedOnDailyAt": "2025-05-30T01:44:05.816Z",
      "title": "ChartLens: Visualización Micro de Gráficos con Atributos de Visualización",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "La mejora de las funciones de los modelos de lenguaje multimodal (MLLMs) como DamoDali permite mejorar tareas variadas, como la comprensión de grafos. Sin embargo, estos modelos sufren desafíos debido a la falta de acuerdo entre las visualizaciones de datos y las frases generadas, un fenómeno conocido como \"hallucinations\". Para resolver esto, introducimos \"Atribución de Visualización Posterior\". Esta técnica permite identificar elementos gráficos detallados para justificar respuestas relacionadas con grafos. Además, proponemos un nuevo algoritmo de responsabilidad gráfica \"ChartLens\", que utiliza un enfoque basado en particiones para identificar objetos gráficos y realizar tareas de responsabilidad visualizada con los MLLMs. Además, presentamos un nuevo benchmark llamado \"ChartVA-Eval\", que incluye gráficos de finanzas, políticas y economía, y se caracteriza por registros de responsabilidad detallados. Según nuestra evaluación, ChartLens mejora los registros de responsabilidad en un 26-66%.",
      "upvotes": 2,
      "discussionId": "6839227a896eb9ceb71fac99",
      "ai_summary": "ChartLens enhances multimodal language models with fine-grained visual attributions, improving the accuracy of chart understanding by 26-66%.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "chart understanding",
        "hallucinations",
        "Post-Hoc Visual Attribution",
        "ChartLens",
        "segmentation-based techniques",
        "set-of-marks prompting",
        "ChartVA-Eval",
        "synthetic charts",
        "real-world charts",
        "fine-grained attribution annotations"
      ]
    },
    "publishedAt": "2025-05-25T19:17:32.000Z",
    "title": "ChartLens: Fine-grained Visual Attribution in Charts",
    "summary": "The growing capabilities of multimodal large language models (MLLMs) have\nadvanced tasks like chart understanding. However, these models often suffer\nfrom hallucinations, where generated text sequences conflict with the provided\nvisual data. To address this, we introduce Post-Hoc Visual Attribution for\nCharts, which identifies fine-grained chart elements that validate a given\nchart-associated response. We propose ChartLens, a novel chart attribution\nalgorithm that uses segmentation-based techniques to identify chart objects and\nemploys set-of-marks prompting with MLLMs for fine-grained visual attribution.\nAdditionally, we present ChartVA-Eval, a benchmark with synthetic and\nreal-world charts from diverse domains like finance, policy, and economics,\nfeaturing fine-grained attribution annotations. Our evaluations show that\nChartLens improves fine-grained attributions by 26-66%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19360.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19286",
      "authors": [
        {
          "_id": "6839220cf85de1fc56402b3d",
          "name": "Utkarsh Sahu",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b3e",
          "name": "Zhisheng Qi",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b3f",
          "name": "Yongjia Lei",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b40",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b41",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:31.804Z",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b42",
          "name": "Nesreen K. Ahmed",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b43",
          "user": {
            "_id": "637c6d95a8716d642050b50f",
            "avatarUrl": "/avatars/0955a10113807348f24db968c7bd7c7a.svg",
            "isPro": false,
            "fullname": "Mahantesh Halappanavar",
            "user": "mhalappa",
            "type": "user"
          },
          "name": "Mahantesh M Halappanavar",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T05:25:34.615Z",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b44",
          "name": "Yao Ma",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b45",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T19:34:15.000Z",
      "submittedOnDailyAt": "2025-05-30T01:42:17.692Z",
      "title": "Verificación de patrones estructurales desde la perspectiva visual del grafo",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande están ampliamente investigados en acceso a conocimiento, posibilidades de edición, inferencia y explicabilidad, pero poco se ha centrado en patrones estructurales. Esta carencia ha impulsado a que analicemos estos patrones desde una perspectiva grafica. Enfatizamos la estructura gráfica de los modelos de lenguaje grande, analizando la información de los tripletos y entidades, así como las características y relaciones estructurales de los nodos. Además, encontramos patrones de homopia. Entidades topológicamente cercanas muestran un nivel de conocimiento similar. Con esto, desarrollamos un modelo de aprendizaje automático basado en grafos para estimar el conocimiento de entidades a partir de sus nodos adyacentes. Este modelo puede realizar una validación de conocimiento con tripletos no conocidos por el modelo de lenguaje grande. Los resultados de los experimentos muestran que un ajuste con estos tripletos muestra altos rendimientos.",
      "upvotes": 2,
      "discussionId": "6839220ef85de1fc56402ba7",
      "ai_summary": "The study explores the structural patterns of knowledge in large language models from a graph perspective, uncovering knowledge homophily and developing models for graph machine learning to estimate entity knowledge.",
      "ai_keywords": [
        "large language models",
        "graph perspective",
        "triplet",
        "entity",
        "node degree",
        "knowledge homophily",
        "graph machine learning",
        "knowledge checking",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-05-25T15:34:15.000Z",
    "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large\n  Language Models",
    "summary": "Large language models have been extensively studied as neural knowledge bases\nfor their knowledge access, editability, reasoning, and explainability.\nHowever, few works focus on the structural patterns of their knowledge.\nMotivated by this gap, we investigate these structural patterns from a graph\nperspective. We quantify the knowledge of LLMs at both the triplet and entity\nlevels, and analyze how it relates to graph structural properties such as node\ndegree. Furthermore, we uncover the knowledge homophily, where topologically\nclose entities exhibit similar levels of knowledgeability, which further\nmotivates us to develop graph machine learning models to estimate entity\nknowledge based on its local neighbors. This model further enables valuable\nknowledge checking by selecting triplets less known to LLMs. Empirical results\nshow that using selected triplets for fine-tuning leads to superior\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19286.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23183",
      "authors": [
        {
          "_id": "683962e4cba8ce4f5ebced9c",
          "user": {
            "_id": "5e7749883d77a72421292d07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
            "isPro": false,
            "fullname": "Gabriele Sarti",
            "user": "gsarti",
            "type": "user"
          },
          "name": "Gabriele Sarti",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T07:54:16.127Z",
          "hidden": false
        },
        {
          "_id": "683962e4cba8ce4f5ebced9d",
          "name": "Vilém Zouhar",
          "hidden": false
        },
        {
          "_id": "683962e4cba8ce4f5ebced9e",
          "name": "Malvina Nissim",
          "hidden": false
        },
        {
          "_id": "683962e4cba8ce4f5ebced9f",
          "name": "Arianna Bisazza",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/1j5bJLYubeP6s5cGuTNzs.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/I2PXFiD8jN4odhmi5vrG6.png"
      ],
      "publishedAt": "2025-05-29T07:20:36.000Z",
      "submittedOnDailyAt": "2025-05-30T06:23:03.812Z",
      "title": "La traducción al español de la frase proporcionada es:\n\n\"Visión para los etiquetadores de (Dis)agreement a través de la traducción automática de herramientas de evaluación de calidad a nivel de palabra sin subbiejas\"",
      "submittedOnDailyBy": {
        "_id": "5e7749883d77a72421292d07",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
        "isPro": false,
        "fullname": "Gabriele Sarti",
        "user": "gsarti",
        "type": "user"
      },
      "summary": "La evaluación de calidad a nivel de palabra (WQE) tiene como objetivo la detección automática de errores mínimos en los resultados de traducción automática. Esto permite múltiples aplicaciones, como el apoyo a la traducción y posterior corrección. La tecnología actual de WQE es costosa debido a la ejecución de modelos de lenguaje grandes y la entrenamiento con muchos datos etiquetados humanos. En este artículo, se revisa una alternativa eficiente utilizando los avances recientes en la evaluación de explicatividad y incertidumbre de modelos de lenguaje, con el objetivo de identificar errores de traducción a partir de las funciones internas del modelo. Se evaluó en 14 mercados en 12 direcciones y se cuantificó el impacto de la variación en los etiquetadores humanos sobre el rendimiento de los métricas utilizando varios conjuntos de etiquetadores humanos. Este estudio revela la posibilidad de desarrollar métricas no manuales, los problemas de incertidumbre en los etiquetadores humanos y las debilidades de la evaluación por segundo notador.",
      "upvotes": 1,
      "discussionId": "683962e6cba8ce4f5ebcedfc",
      "githubRepo": "https://github.com/gsarti/labl",
      "ai_summary": "Evaluation of word-level quality estimation techniques leverages model interpretability and uncertainty quantification to identify translation errors with a focus on the impact of label variation and the performance of supervised versus unsupervised metrics.",
      "ai_keywords": [
        "language model interpretability",
        "uncertainty quantification",
        "word-level quality estimation",
        "translation errors",
        "human label variation",
        "unsupervised metrics",
        "supervised methods",
        "single-annotator evaluation practices"
      ]
    },
    "publishedAt": "2025-05-29T03:20:36.000Z",
    "title": "Unsupervised Word-level Quality Estimation for Machine Translation\n  Through the Lens of Annotators (Dis)agreement",
    "summary": "Word-level quality estimation (WQE) aims to automatically identify\nfine-grained error spans in machine-translated outputs and has found many uses,\nincluding assisting translators during post-editing. Modern WQE techniques are\noften expensive, involving prompting of large language models or ad-hoc\ntraining on large amounts of human-labeled data. In this work, we investigate\nefficient alternatives exploiting recent advances in language model\ninterpretability and uncertainty quantification to identify translation errors\nfrom the inner workings of translation models. In our evaluation spanning 14\nmetrics across 12 translation directions, we quantify the impact of human label\nvariation on metric performance by using multiple sets of human labels. Our\nresults highlight the untapped potential of unsupervised metrics, the\nshortcomings of supervised methods when faced with label uncertainty, and the\nbrittleness of single-annotator evaluation practices.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/1j5bJLYubeP6s5cGuTNzs.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/I2PXFiD8jN4odhmi5vrG6.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23183.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e7749883d77a72421292d07",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
      "fullname": "Gabriele Sarti",
      "name": "gsarti",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 225
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22943",
      "authors": [
        {
          "_id": "683912ee831cc04b0a3c0c6c",
          "user": {
            "_id": "64bb081c01f1983a863654dc",
            "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
            "isPro": false,
            "fullname": "Jaewoo Ahn",
            "user": "ahnpersie",
            "type": "user"
          },
          "name": "Jaewoo Ahn",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:54.580Z",
          "hidden": false
        },
        {
          "_id": "683912ee831cc04b0a3c0c6d",
          "name": "Heeseung Yun",
          "hidden": false
        },
        {
          "_id": "683912ee831cc04b0a3c0c6e",
          "name": "Dayoon Ko",
          "hidden": false
        },
        {
          "_id": "683912ee831cc04b0a3c0c6f",
          "name": "Gunhee Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T23:45:55.000Z",
      "submittedOnDailyAt": "2025-05-30T00:40:41.829Z",
      "title": "¿Los LLMs pueden engañar CLIP? Marca de referencia para evaluar la estructura estructural adversa de las representaciones de domó del aprendizaje previo mediante actualización de texto.",
      "submittedOnDailyBy": {
        "_id": "64bb081c01f1983a863654dc",
        "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
        "isPro": false,
        "fullname": "Jaewoo Ahn",
        "user": "ahnpersie",
        "type": "user"
      },
      "summary": "Los modelos de representación multimodal preentrenados como clip demostraron capacidades sorprendentes, pero también muestran vulnerabilidades estructurales y frecuentemente se comportan de manera contraria a la intuición. Presentamos el criterio MAC (Adversarial Crafting Multimodal), utilizando modelos de lenguaje grandes (LLMs) para generar ejemplos de texto falso que explotan estas vulnerabilidades y evaluamos el éxito de la ataque y la diversidad de los ejemplos por grupos. Proponemos un enfoque de aprendizaje autosupervisado mediante el entrenamiento con ejemplos rechazados para mejorar el método 0-shot, incrementando la diversidad de los ejemplos falsos mediante un filtrado que promueve esta diversidad. Nuestro enfoque, que ha demostrado excelentes resultados, incluye modelos de lenguaje de pequeño tamaño como Llama-3.1-8B, y muestra claramente las vulnerabilidades estructurales de representaciones multimodales variadas como imágenes, videos y sonidos.",
      "upvotes": 1,
      "discussionId": "683912ef831cc04b0a3c0cc1",
      "githubRepo": "https://github.com/ahnjaewoo/MAC",
      "ai_summary": "A benchmark using deceptive text samples to evaluate compositional vulnerabilities in multimodal representations is introduced, and a self-training approach improves zero-shot methods by enhancing attack success and sample diversity.",
      "ai_keywords": [
        "Multimodal Adversarial Compositionality (MAC)",
        "multimodal representations",
        "large language models (LLMs)",
        "rejection-sampling fine-tuning",
        "diversity-promoting filtering",
        "images",
        "videos",
        "audios"
      ]
    },
    "publishedAt": "2025-05-28T19:45:55.000Z",
    "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of\n  Pre-trained Multimodal Representation via Text Updates",
    "summary": "While pre-trained multimodal representations (e.g., CLIP) have shown\nimpressive capabilities, they exhibit significant compositional vulnerabilities\nleading to counterintuitive judgments. We introduce Multimodal Adversarial\nCompositionality (MAC), a benchmark that leverages large language models (LLMs)\nto generate deceptive text samples to exploit these vulnerabilities across\ndifferent modalities and evaluates them through both sample-wise attack success\nrate and group-wise entropy-based diversity. To improve zero-shot methods, we\npropose a self-training approach that leverages rejection-sampling fine-tuning\nwith diversity-promoting filtering, which enhances both attack success rate and\nsample diversity. Using smaller language models like Llama-3.1-8B, our approach\ndemonstrates superior performance in revealing compositional vulnerabilities\nacross various multimodal representations, including images, videos, and\naudios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22943.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64bb081c01f1983a863654dc",
      "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
      "fullname": "Jaewoo Ahn",
      "name": "ahnpersie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22810",
      "authors": [
        {
          "_id": "68395c867f983113faf29005",
          "name": "Zhoufaran Yang",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf29006",
          "name": "Yan Shu",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf29007",
          "name": "Zhifei Yang",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf29008",
          "name": "Yan Zhang",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf29009",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900a",
          "name": "Keyang Lu",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900b",
          "name": "Gangyan Zeng",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900c",
          "name": "Shaohui Liu",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900d",
          "name": "Yu Zhou",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900e",
          "name": "Nicu Sebe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T19:39:35.000Z",
      "submittedOnDailyAt": "2025-05-30T05:52:32.917Z",
      "title": "VidText: Método para evaluar completamente el texto de video",
      "submittedOnDailyBy": {
        "_id": "65c4f99b27736b5b86c2cbda",
        "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
        "isPro": false,
        "fullname": "Yan Shu",
        "user": "sy1998",
        "type": "user"
      },
      "summary": "Texto visual incorporado tiene una rica información semántica y es importante para la comprensión de todo el video y para la inferencia de detalles específicos de comportamientos humanos. Sin embargo, actualmente los marcos de referencia para la comprensión de videos ignoran principalmente la información textual y los marcos de referencia especializados en OCR están limitados a imágenes estáticas, lo que restringe la capacidad de interactuar entre texto y contexto visual dinámico. Para remediar esto, se propone el nuevo marco de referencia VidText. VidText presenta las siguientes características: 1) Un amplio rango de escenarios realistas, soporte para contenido multilingüe y incluye diversos tipos de texto en video que aparecen de manera natural. 2) Un marco de evaluación en diferentes niveles (video, clip y instancia) para evaluar capacidades de resumen global y búsqueda local. 3) Introduce tareas de reconocimiento y inferencia cruzada de modalidades que expanden el rango de inferencia de texto y contexto visual. Experimentos extendidos con 18 de los modelos más avanzados de multimodalidad (LMMs) claramente demuestran que los modelos actuales enfrentan muchas desafíos y tienen mucho potencial para mejoras. Un análisis profundo se centra en los causas propias del modelo, como la resolución de entrada y la capacidad de OCR, y en el impacto de la utilización de información auxiliar y estrategias de inferencia de cadena de pensamiento. Se espera que VidText llene el espacio de los marcos de referencia actuales de comprensión de video y sea la base para la investigación futura de inferencia multimodal en entornos dinámicos.",
      "upvotes": 1,
      "discussionId": "68395c897f983113faf290eb",
      "ai_summary": "VidText is a new benchmark that evaluates video text understanding across various tasks, covering global summarization and local retrieval, and highlights challenges for current multimodal models.",
      "ai_keywords": [
        "LMMs",
        "video text perception",
        "cross-modal reasoning",
        "Chain-of-Thought reasoning"
      ]
    },
    "publishedAt": "2025-05-28T15:39:35.000Z",
    "title": "VidText: Towards Comprehensive Evaluation for Video Text Understanding",
    "summary": "Visual texts embedded in videos carry rich semantic information, which is\ncrucial for both holistic video understanding and fine-grained reasoning about\nlocal human actions. However, existing video understanding benchmarks largely\noverlook textual information, while OCR-specific benchmarks are constrained to\nstatic images, limiting their ability to capture the interaction between text\nand dynamic visual contexts. To address this gap, we propose VidText, a new\nbenchmark designed for comprehensive and in-depth evaluation of video text\nunderstanding. VidText offers the following key features: 1) It covers a wide\nrange of real-world scenarios and supports multilingual content, encompassing\ndiverse settings where video text naturally appears. 2) It introduces a\nhierarchical evaluation framework with video-level, clip-level, and\ninstance-level tasks, enabling assessment of both global summarization and\nlocal retrieval capabilities. 3) The benchmark also introduces a set of paired\nperception reasoning tasks, ranging from visual text perception to cross-modal\nreasoning between textual and visual information. Extensive experiments on 18\nstate-of-the-art Large Multimodal Models (LMMs) reveal that current models\nstruggle across most tasks, with significant room for improvement. Further\nanalysis highlights the impact of both model-intrinsic factors, such as input\nresolution and OCR capability, and external factors, including the use of\nauxiliary information and Chain-of-Thought reasoning strategies. We hope\nVidText will fill the current gap in video understanding benchmarks and serve\nas a foundation for future research on multimodal reasoning with video text in\ndynamic environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22810.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c4f99b27736b5b86c2cbda",
      "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
      "fullname": "Yan Shu",
      "name": "sy1998",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22126",
      "authors": [
        {
          "_id": "683944793e5dd928f04e8431",
          "name": "Yifan Chang",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8432",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8433",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8434",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8435",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8436",
          "name": "S. Kevin Zhou",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8437",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T08:51:01.000Z",
      "submittedOnDailyAt": "2025-05-30T04:18:26.192Z",
      "title": "SridBench: Método de dibujo de imágenes de investigación científica como modelo de prueba",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "Recientemente, el campo de la generación de imágenes está experimentando un rápido avance liderado por la IA. Los modelos de difusión iniciales priorizaban la calidad visual, mientras que los nuevos modelos de difusión multimodal han evolucionado a través de mejoras en la integración lógica, la comprensión del significado y la configuración estructural. La generación de gráficos de explicaciones científicas es un ejemplo de esta evolución, ya que, en contraste con la síntesis de imágenes generales, requiere una interpretación precisa de los contenidos técnicos y la transformación de ideas abstractas en visualizaciones claras y estándarizadas. Esta tarea requiere mucha conocida y labor, puede tomar tiempo y necesita herramientas especializadas. Automatizar estas actividades de manera controlada e inteligente ofrece valor práctico, pero actualmente no existen estándares para evaluar esta área de la IA. Para remediar esto, presentamos SridBench, el primer estándar para la generación de gráficos de explicaciones científicas. Compuesto por 1,120 muestras seleccionadas de artículos científicos avanzados en 13 áreas de la ciencia natural y de la ciencia de la computación, cada muestra es evaluada en 6 aspectos, incluyendo la fidelidad semántica y la precisión estructural. Los resultados de los experimentos muestran claramente que modelos superiores como GPT-4o-image superan al humano en la claridad de texto/imágen y en la precisión científica. Estos hallazgos subrayan la necesidad de habilidades de generación visual para la integración lógica avanzada.",
      "upvotes": 1,
      "discussionId": "6839447c3e5dd928f04e84c1",
      "ai_summary": "The introduction of SridBench, a benchmark for scientific figure generation, reveals that current top-tier models, such as GPT-4o-image, fall short in semantic and structural accuracy compared to human performance, underscoring the need for more advanced multimodal reasoning-driven visual generation.",
      "ai_keywords": [
        "diffusion models",
        "multimodal models",
        "GPT-4o-image",
        "semantic understanding",
        "structural composition",
        "scientific illustration generation",
        "SridBench",
        "semantic fidelity",
        "structural accuracy"
      ]
    },
    "publishedAt": "2025-05-28T04:51:01.000Z",
    "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of\n  Image Generation Model",
    "summary": "Recent years have seen rapid advances in AI-driven image generation. Early\ndiffusion models emphasized perceptual quality, while newer multimodal models\nlike GPT-4o-image integrate high-level reasoning, improving semantic\nunderstanding and structural composition. Scientific illustration generation\nexemplifies this evolution: unlike general image synthesis, it demands accurate\ninterpretation of technical content and transformation of abstract ideas into\nclear, standardized visuals. This task is significantly more\nknowledge-intensive and laborious, often requiring hours of manual work and\nspecialized tools. Automating it in a controllable, intelligent manner would\nprovide substantial practical value. Yet, no benchmark currently exists to\nevaluate AI on this front. To fill this gap, we introduce SridBench, the first\nbenchmark for scientific figure generation. It comprises 1,120 instances\ncurated from leading scientific papers across 13 natural and computer science\ndisciplines, collected via human experts and MLLMs. Each sample is evaluated\nalong six dimensions, including semantic fidelity and structural accuracy.\nExperimental results reveal that even top-tier models like GPT-4o-image lag\nbehind human performance, with common issues in text/visual clarity and\nscientific correctness. These findings highlight the need for more advanced\nreasoning-driven visual generation capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20199",
      "authors": [
        {
          "_id": "683919611186f2cbf3ed2267",
          "name": "Pengxiang Li",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed2268",
          "name": "Shilin Yan",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed2269",
          "name": "Joey Tsai",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226a",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226b",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226c",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226d",
          "name": "Xiaowei Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T16:40:22.000Z",
      "submittedOnDailyAt": "2025-05-30T01:05:23.884Z",
      "title": "Adaptive Clase-Libre Guiada para Mascaras de Baja Confiabilidad Dinámica",
      "submittedOnDailyBy": {
        "_id": "64245f2c089d5fae56b4549a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
        "isPro": false,
        "fullname": "Pengxiang Li",
        "user": "pengxiang",
        "type": "user"
      },
      "summary": "La Clasificación de Filtro de Condición (CFG) mejora significativamente la controlabilidad de los modelos generativos al utilizar predicciones condicionales e incondicionales en la interfaz. Sin embargo, el CFG estándar utiliza entradas incondicionales estáticas debido a que la incertidumbre del modelo cambia dinámicamente en el proceso de generación iterativo, lo cual no es óptimo. Presentamos un nuevo método llamado Adaptativo Clasificador Filtro de Condición (A-CFG), que utiliza la confianza instantánea de las predicciones del modelo para convertir entradas incondicionales en un modelo. En cada paso del modelo de lenguaje de dispersión (masked) iterativo, el A-CFG identifica los tokens que muestran baja confianza en la secuencia generada actual. Estos tokens son temporalmente re-mascarados dinámicamente y localmente para crear entradas incondicionales. De esta manera, el impacto de la modificación del CFG se concentra precisamente en las áreas inciertas, proporcionando una guía más efectiva. Integramos A-CFG en los modelos de lenguaje de dispersión (masked) más recientes y demostramos su eficacia. Según los resultados de experimentos en diferentes benchmarks de generación de lenguaje, A-CFG ha mejorado significativamente sobre el CFG estándar, obteniendo, por ejemplo, un ganancia de 3.9 puntos en GPQA. Nuestro estudio revela que ajustar mecanismos de guía en relación con la incertidumbre del modelo durante la generación iterativa es fundamental.",
      "upvotes": 1,
      "discussionId": "683919611186f2cbf3ed2292",
      "ai_summary": "Adaptive Classifier-Free Guidance (A-CFG) dynamically adjusts the guidance in masked diffusion language models by focusing on areas of low model confidence, leading to significant improvements in language generation performance.",
      "ai_keywords": [
        "Classifier-Free Guidance (CFG)",
        "Adaptive Classifier-Free Guidance (A-CFG)",
        "masked diffusion language model",
        "predictive confidence",
        "GPQA"
      ]
    },
    "publishedAt": "2025-05-26T12:40:22.000Z",
    "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
    "summary": "Classifier-Free Guidance (CFG) significantly enhances controllability in\ngenerative models by interpolating conditional and unconditional predictions.\nHowever, standard CFG often employs a static unconditional input, which can be\nsuboptimal for iterative generation processes where model uncertainty varies\ndynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel\nmethod that tailors the unconditional input by leveraging the model's\ninstantaneous predictive confidence. At each step of an iterative (masked)\ndiffusion language model, A-CFG identifies tokens in the currently generated\nsequence for which the model exhibits low confidence. These tokens are\ntemporarily re-masked to create a dynamic, localized unconditional input. This\nfocuses CFG's corrective influence precisely on areas of ambiguity, leading to\nmore effective guidance. We integrate A-CFG into a state-of-the-art masked\ndiffusion language model and demonstrate its efficacy. Experiments on diverse\nlanguage generation benchmarks show that A-CFG yields substantial improvements\nover standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work\nhighlights the benefit of dynamically adapting guidance mechanisms to model\nuncertainty in iterative generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20199.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64245f2c089d5fae56b4549a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
      "fullname": "Pengxiang Li",
      "name": "pengxiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19236",
      "authors": [
        {
          "_id": "68392fc4b6280677f75e6194",
          "user": {
            "_id": "5fbdf878485ef14d9a960f4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634724388998-5fbdf878485ef14d9a960f4d.jpeg",
            "isPro": false,
            "fullname": "Qian Cao",
            "user": "Aman",
            "type": "user"
          },
          "name": "Qian Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:24.101Z",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6195",
          "name": "Xiting Wang",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6196",
          "name": "Yuzhuo Yuan",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6197",
          "name": "Yahui Liu",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6198",
          "name": "Fang Luo",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6199",
          "name": "Ruihua Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T17:25:23.000Z",
      "submittedOnDailyAt": "2025-05-30T02:42:27.745Z",
      "title": "Evaluación de creatividad textual en diversas áreas: conjuntos de datos y evaluadores de modelos de lenguaje de gran escala",
      "submittedOnDailyBy": {
        "_id": "5fbdf878485ef14d9a960f4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634724388998-5fbdf878485ef14d9a960f4d.jpeg",
        "isPro": false,
        "fullname": "Qian Cao",
        "user": "Aman",
        "type": "user"
      },
      "summary": "La evaluación de creatividad es una situación muy difícil en los grandes modelos de lenguaje (LLMs). Actualmente, las evaluaciones son ineficientes y costosas, se basan en juicios humanos y no se conectan con la mejora de la creatividad de la máquina. Los métodos de automatización van desde pruebas psicológicas hasta métodos heurísticos o basados en prompting, son generalizables pero no coinciden con la evaluación humana. Para resolver estos problemas, este artículo propone dos nuevos marcos de comparación para mejorar la coincidencia de la evaluación en un contexto común. Se presenta CreataSet, un conjunto de datos de gran escala que incluye 100K+ pares de respuestas creativas de nivel humano y 1M+ de pares de respuestas creativas sintéticas, y se extiende a tareas de dominios abiertos. Mediante el entrenamiento en CreataSet, se desarrolla CrEval, un evaluador basado en modelos de lenguaje (LLMs). CrEval muestra un desempeño sorprendentemente excelente en comparación con los métodos actuales, y señala la importancia fundamental de un entrenamiento de evaluadores con alta intensidad, incluyendo datos generados y sintéticos. CrEval muestra también su utilidad práctica. Se anunciará pronto. Esta investigación se apoyará con esta contribución.",
      "upvotes": 1,
      "discussionId": "68392fc5b6280677f75e61e6",
      "projectPage": "https://creval-creative-evaluation.github.io/",
      "ai_summary": "A novel pairwise-comparison framework using CreataSet dataset trains CrEval, an LLM-based evaluator that significantly improves the assessment of textual creativity aligned with human judgments.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "automated methods",
        "heuristic-based approaches",
        "prompting-based approaches",
        "pairwise-comparison framework",
        "shared contextual instructions",
        "CreataSet",
        "synthetic creative instruction-response pairs",
        "open-domain tasks",
        "human-level instructions",
        "CrEval",
        "human judgments",
        "robust evaluators",
        "creativity enhancement"
      ]
    },
    "publishedAt": "2025-05-25T13:25:23.000Z",
    "title": "Evaluating Text Creativity across Diverse Domains: A Dataset and Large\n  Language Model Evaluator",
    "summary": "Creativity evaluation remains a challenging frontier for large language\nmodels (LLMs). Current evaluations heavily rely on inefficient and costly human\njudgments, hindering progress in enhancing machine creativity. While automated\nmethods exist, ranging from psychological testing to heuristic- or\nprompting-based approaches, they often lack generalizability or alignment with\nhuman judgment. To address these issues, in this paper, we propose a novel\npairwise-comparison framework for assessing textual creativity, leveraging\nshared contextual instructions to improve evaluation consistency. We introduce\nCreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic\ncreative instruction-response pairs spanning diverse open-domain tasks. Through\ntraining on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval\ndemonstrates remarkable superiority over existing methods in alignment with\nhuman judgments. Experimental results underscore the indispensable significance\nof integrating both human-generated and synthetic data in training highly\nrobust evaluators, and showcase the practical utility of CrEval in boosting the\ncreativity of LLMs. We will release all data, code, and models publicly soon to\nsupport further research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19236.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fbdf878485ef14d9a960f4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634724388998-5fbdf878485ef14d9a960f4d.jpeg",
      "fullname": "Qian Cao",
      "name": "Aman",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  }
]