[
  {
    "paper": {
      "id": "2502.19613",
      "authors": [
        {
          "_id": "67c12987505a88e4a185e0d7",
          "name": "Wei Xiong",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0d8",
          "name": "Hanning Zhang",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0d9",
          "name": "Chenlu Ye",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0da",
          "name": "Lichang Chen",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0db",
          "name": "Nan Jiang",
          "hidden": false
        },
        {
          "_id": "67c12987505a88e4a185e0dc",
          "name": "Tong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T23:01:16.000Z",
      "title": "La corrección de auto-compensación en inferencia matemática",
      "summary": "Estamos realizando investigaciones sobre grandes modelos de lenguaje (LLMs) en la teoría de la auto-compensación lógica. Estos modelos generan lógicas de manera gradual y pueden evaluar la precisión de sus salidas. Esta metodología integral permite que un solo modelo guíe el procesamiento lógico de manera independiente y ofrezca ventajas computacionales. En particular, nos centramos en trabajos de ajuste automático representativos. En este contexto, el modelo detecta automáticamente errores en las respuestas, modifica las salidas y decide finalizar los ciclos de entrenamiento iterativos. Para lograrlo, proponemos un marco de algoritmos de dos etapas para construir modelos de auto-compensación lógica. En la primera etapa, utilizamos muestreo secuencial para sintetizar proyectos lógicos extensos combinando estructuras de auto-compensación y ajuste automático. Esta finejación basada en datos permite que el modelo aprenda patrones de auto-compensación y ajuste automático. En la segunda etapa, utilizamos aprendizaje por refuerzo con señales basadas en reglas para mejorar la precisión de la salida del modelo y su capacidad de entrenamiento. Según los resultados de experimentos con Llama-3 y Qwen-2.5, nuestro enfoque supera la capacidad de ajuste automático única y alcanza significativas mejoras en comparación con sistemas que dependen de modelos de compensación externa.",
      "upvotes": 42,
      "discussionId": "67c12989505a88e4a185e115"
    },
    "publishedAt": "2025-02-27T22:15:54.222Z",
    "title": "Self-rewarding correction for mathematical reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19613.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "643e59806db6ba8c5ee123f3",
      "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
      "fullname": "Wei Xiong",
      "name": "weqweasdas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20395",
      "authors": [
        {
          "_id": "67c12b5def9af74902537b98",
          "name": "Zhongyang Li",
          "hidden": false
        },
        {
          "_id": "67c12b5def9af74902537b99",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67c12b5def9af74902537b9a",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T18:59:32.000Z",
      "title": "R2-T2: Procesador Mixto de Modelos para Rearranque de R2 en Test Scenarios",
      "summary": "En los grandes modelos multimodales (LMMs), el reconocimiento de modalidades no lingüísticas (por ejemplo, representaciones visuales) no supera la potente capacidad de inferencia de los grandes modelos de lenguaje (LLMs), lo que limita el rendimiento de los LMMs en tareas de trabajo de bajo nivel. Recientemente, para mitigar esta debilidad, se sustituyó al encoder visual por un conjunto de expertos (MoE) para proporcionar a las tareas de trabajo de bajo nivel una representación rica, multiescala y diversificada. El rendimiento de los MoE multimodales depende significativamente del router, que re-escala y mezcla la entrada para cada representación de experto diferente. Sin embargo, descubrimos que un router entrenado end-to-end no genera pesos de router óptimos para los muestras de prueba. Para remediar esto, proponemos una nueva y eficiente metodología llamada \"Router Reentrenamiento en Tiempo de Prueba (R2-T2)\". Esta metodología optimiza los vectores de peso de router localmente moviendo vectores que realizan buenas predicciones en el área vecina de las muestras de prueba. R2-T2 propone tres estrategias según diferentes objetivos de optimización y espacios de búsqueda de vecinos. R2-T2 continuamente y significativamente mejora el rendimiento de los más recientes LMMs en los desafíos de las pruebas de referencia, sin necesidad de entrenar los parámetros básicos del modelo.",
      "upvotes": 20,
      "discussionId": "67c12b5eef9af74902537c00"
    },
    "publishedAt": "2025-02-27T22:27:24.486Z",
    "title": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/PaZkWIhqZBRCSfBA-k4OX.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/FASlyPDiSb9VHZaeWMj9H.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/kGeIJVMDDAbIassiuYIb2.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/Tw2Bf_RsFTPARKLJWIlKM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20395.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20082",
      "authors": [
        {
          "_id": "67c12b6d25c74ee5b6e2ce8e",
          "name": "Ning Shang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce8f",
          "name": "Li Lyna Zhang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce90",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce91",
          "name": "Gaokai Zhang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce92",
          "name": "Gilsinia Lopez",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce93",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce94",
          "name": "Weizhu Chen",
          "hidden": false
        },
        {
          "_id": "67c12b6d25c74ee5b6e2ce95",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T13:41:07.000Z",
      "title": "LongRoPE2: Errores de aproximación en escalado del contexto de la ventana de LLM",
      "summary": "LongRoPE2 es un nuevo enfoque que permite expandir la ventana de contexto larga mientras mantiene la integridad de la ventana de contexto larga en modelos de lenguaje grandes y previamente entrenados (LLMs). Para lograr esto, se presentan tres contribuciones clave: 1) la hipótesis de que el poco entrenamiento en dimensiones RoPE de alto índice, no visibles con los métodos actuales, es la causa de problemas de distribución fuera de la distribución (OOD); 2) un algoritmo efectivo de escalado de RoPE que guia la búsqueda de cálculo evolutivo en la incertidumbre de la estructura de nodos; 3) un enfoque de entrenamiento de ventana de contexto mixta que mantiene el rendimiento en contextos cortos utilizando RoPE original, mientras escala el RoPE para secuencias de contexto largas para ajustar los pesos del modelo. Los experimentos en los benchmarks de LLaMA3-8B y Phi3-mini-3.8B apoyan estas hipótesis y demuestran la eficacia de LongRoPE2. En particular, LongRoPE2 puede expandir LLaMA3-8B a una longitud de contexto válida de 128K, manteniendo un rendimiento en contextos cortos del 98.5% o más, utilizando solo 80 veces menos de 10B tokens que la metodología de Meta. El código está disponible en https://github.com/microsoft/LongRoPE.",
      "upvotes": 19,
      "discussionId": "67c12b6e25c74ee5b6e2ceb5"
    },
    "publishedAt": "2025-02-27T22:22:53.713Z",
    "title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20082.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b0009c72043b05d29492b2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
      "fullname": "Li Lyna Zhang",
      "name": "lynazhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 27
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.19634",
      "authors": [
        {
          "_id": "67c12bf3505a88e4a1866a01",
          "name": "Jiazhen Pan",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a02",
          "user": {
            "_id": "631b9ff5824f2502e3557c7e",
            "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
            "isPro": false,
            "fullname": "liu",
            "user": "che111",
            "type": "user"
          },
          "name": "Che Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-28T09:28:38.598Z",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a03",
          "name": "Junde Wu",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a04",
          "name": "Fenglin Liu",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a05",
          "name": "Jiayuan Zhu",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a06",
          "name": "Hongwei Bran Li",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a07",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a08",
          "name": "Cheng Ouyang",
          "hidden": false
        },
        {
          "_id": "67c12bf3505a88e4a1866a09",
          "name": "Daniel Rueckert",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T23:57:34.000Z",
      "title": "MedVLM-R1: Mejora de la capacidad lógica médica mediante el aprendizaje de relaciones en modelos de lenguaje visuolingüístico (VLMs)",
      "summary": "La teoría es un paso importante en el desarrollo de la análisis de imágenes médicas y desempeña un papel crucial en la confianza de los médicos y la aprobación regulatoria, proporcionando transparencia y confianza. Sin embargo, los modelos de lenguaje visual médico (VLMs) muestran buenos resultados en tareas radiológicas, pero muchos de ellos se centran únicamente en presentar una respuesta final sin explicar claramente los motivos. Para llenar esta brecha, presentamos MedVLM-R1, un modelo que explica los razones en lenguaje natural para aumentar la transparencia y confianza. Este modelo evita la sobreajuste y el fracaso por falta de motivos verdaderos, utilizando un enfoque de aprendizaje por refuerzo para encontrar rutas de razones comprensibles para los humanos. Con un conjunto de datos limitado (600 ejemplos de respuestas a preguntas de imágenes) y parámetros del modelo (2B), MedVLM-R1 mejora la precisión en el 55.11% a 78.22% en benchmarks de MRI, CT y radiografías, superando modelos grandes entrenados con más de un millón de ejemplos. Además, muestra una fuerte capacidad de extensión doméstica y muestra un gran rendimiento en tareas fuera de su distribución. Al integrar la análisis de imágenes médicas con razones claras, MedVLM-R1 demuestra un paso importante en la confianza y comprensión de la IA en el tratamiento.",
      "upvotes": 17,
      "discussionId": "67c12bf4505a88e4a1866a35"
    },
    "publishedAt": "2025-02-28T04:36:05.045Z",
    "title": "MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19634.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631b9ff5824f2502e3557c7e",
      "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
      "fullname": "liu",
      "name": "che111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20238",
      "authors": [
        {
          "_id": "67c15306333e2f71f01c8e35",
          "name": "Guizhen Chen",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e36",
          "name": "Weiwen Xu",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e37",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e38",
          "name": "Hou Pong Chan",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e39",
          "name": "Chaoqun Liu",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e3a",
          "name": "Lidong Bing",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e3b",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e3c",
          "name": "Anh Tuan Luu",
          "hidden": false
        },
        {
          "_id": "67c15306333e2f71f01c8e3d",
          "name": "Yu Rong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T16:23:25.000Z",
      "title": "FINEREASON: Evaluación y mejora a través de la reflexión lógica crítica de los métodos de resolución de problemas de los LLMs",
      "summary": "Varios motivos de tareas difíciles requieren una reacción rápida, pero necesitan un enfoque más cuidadoso y estratificado. El reciente desarrollo de los modelos de lenguaje grande (LLMs) ha mostrado un cambio importante desde la forma de respuesta rápida, conocida como 'System 1', hacia un estilo de resolución de problemas que involucra reflexión y corrección, conocido como 'System 2'. Sin embargo, los actuales benchmarks se centran principalmente en la precisión final de la respuesta, sin investigar significativamente los pasos intermedios del modelo. Esto limita la evaluación de la capacidad de los modelos para reflexionar y corregir errores durante el proceso de razonamiento. Para complementar esto, presentamos 'FINEREASON', una lógica de perdida adecuada para la capacidad de razonamiento de los LLMs. Cada perdida puede ser decomposida en pasos individuales, y está optimizada para la validación de la precisión intermedia. Así, presentamos dos tareas: estado de chequeo y movimiento de estado, para evaluar el estado actual y planificar el próximo paso. Proporcionamos un conjunto de entrenamiento de perdidas para expandir el entrenamiento de perdidas y mejorar el rendimiento en tareas matemáticas generales. Mostramos que nuestros modelos entrenados, a través de datos de estado de chequeo y movimiento, obtienen un mejoramiento del 5.1% en la razonamiento matemático en el conjunto GSM8K.",
      "upvotes": 13,
      "discussionId": "67c15307333e2f71f01c8ebc"
    },
    "publishedAt": "2025-02-28T01:14:11.268Z",
    "title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20238.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e85b3edb3767299865e0e3",
      "avatarUrl": "/avatars/fdbe121535dea940edd2766161393485.svg",
      "fullname": "Chen",
      "name": "Guizhen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16645",
      "authors": [
        {
          "_id": "67c12e60d8247a49b805694f",
          "name": "Chenlong Wang",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056950",
          "name": "Zhaoyang Chu",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056951",
          "user": {
            "_id": "669096da35cddb688a352ca8",
            "avatarUrl": "/avatars/d01f34d99d89447d27c0fd43734ae6d9.svg",
            "isPro": false,
            "fullname": "zxiang",
            "user": "zx10086",
            "type": "user"
          },
          "name": "Zhengxiang Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-28T09:28:33.569Z",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056952",
          "user": {
            "_id": "6743e9d4303e7ce5b9d13e9b",
            "avatarUrl": "/avatars/cdaf150380e9c8916547185b968a2670.svg",
            "isPro": false,
            "fullname": "xy",
            "user": "yxy0807",
            "type": "user"
          },
          "name": "Xuyi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-28T09:28:31.564Z",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056953",
          "name": "Kaiyue Qiu",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056954",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056955",
          "name": "Zhou Zhao",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056956",
          "name": "Xuanhua Shi",
          "hidden": false
        },
        {
          "_id": "67c12e60d8247a49b8056957",
          "name": "Dongping Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T16:46:18.000Z",
      "title": "CODESYNC: Sincronización Dinámica de Código y Modelos de Lenguaje de Gran Escala\nEvolución Escalable",
      "summary": "Los modelos de lenguaje grandes (LLMs) muestran un excelente rendimiento en el desarrollo de software, pero enfrentan dificultades al adaptarse a las actualizaciones frecuentes de las API de terceros. Esta limitación está asociada con la utilización de conjuntos de datos de entrenamiento estáticos, lo que generalmente lleva a implementaciones de códigos ejecutables, inseguras y poco eficientes. En este contexto, este artículo presenta CODESYNC, un motor de datos. CODESYNC identifica las actualizaciones de la capacidad de código de terceros bibliotecas de Python en función del tiempo y recopila actualizaciones de código en tiempo real. Con CODESYNC, se desarrolló CODESYNCBENCH, un marco de prueba detallado, que evalúa la capacidad de los LLMs para adaptarse a la evolución de código que incluye 220 actualizaciones de API en el mundo real. Este marco de prueba proporciona 3,300 casos de prueba y incluye tres tareas de evaluación, así como un conjunto de datos de entrenamiento de supervisado interesante compuesto por 2,200 muestras. Las experimentaciones ampliadas en 14 de los más avanzados LLMs revelan que estos enfrentan dificultades con la evolución dinámica de código, y aún con el apoyo de métodos de actualización de conocimiento dinámico (como DPO, ORPO y SimPO) siguen enfrentando desafíos. Creemos que este marco de prueba proporciona una sólida base para el desarrollo de métodos válidos para la actualización en tiempo real de código. Los códigos experimentales y conjuntos de datos están disponibles en la siguiente URL.\nhttps://github.com/Lucky-voyage/Code-Sync",
      "upvotes": 12,
      "discussionId": "67c12e61d8247a49b805698f"
    },
    "publishedAt": "2025-02-27T23:04:14.619Z",
    "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16645.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16944",
      "authors": [
        {
          "_id": "67be807e8a5a805423137ca2",
          "name": "Chenghua Huang",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca3",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca4",
          "user": {
            "_id": "669dcf6200970c3b27aafa5d",
            "avatarUrl": "/avatars/bb9ed5ff86326fdaeb184c6b0e40f74f.svg",
            "isPro": false,
            "fullname": "kaikai yang",
            "user": "keanudicap",
            "type": "user"
          },
          "name": "Fangkai Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:17:46.382Z",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca5",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca6",
          "name": "Zhixu Li",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca7",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca8",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137ca9",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "67be807e8a5a805423137caa",
          "name": "Qi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T08:11:33.000Z",
      "title": "Thin and Thick: Global Values Guidelines for Optimized Decomposed Value Policies",
      "summary": "La Optimización de Políticas Proximas (PPO) basada en Aprendizaje por Refuerzo con Retroalimentación de Humanos (RLHF) es esencial para ajustar los grandes modelos de lenguaje (LLMs) a las preferencias humanas. Para ello, es necesario realizar una entrenamiento conjunto de un jugador que aprende con un modelo de recompensa preentrenado y un evaluador. Esta metodología aumenta la complejidad computacional y las características inestables debido a la relación de interdependencia entre el jugador y el evaluador. Además, PPO está limitado en su adaptabilidad porque no puede acceder a las recompensas reales en las tareas de los LLMs. En estas condiciones, el aprendizaje previo de modelos de valor o recompensa resulta insuficiente, ya que ambos proporcionan solo señales de audiencia fija y no nuevos feedbacks reales. Para resolver estos problemas, se propone el Decoupled Value Policy Optimization (DVPO). DVPO es un marco hermoso que reemplaza el modelo de recompensa previo por un modelo de valor global preentrenado (GVM). El GVM es condicional a las trazas de política y estima el retorno a la condición token a token. Al separar el entrenamiento de modelo de valor y política, DVPO fija el GVM y utiliza una función de entrenamiento de RL, lo que elimina la relación de interdependencia entre el jugador y el evaluador, reduce el uso de memoria GPU en un 40% y el tiempo de entrenamiento en un 35%. Los experimentos de referencia muestran que DVPO supera métodos eficientes de RLHF (por ejemplo, DPO) y compite con el PPO más avanzado en términos de rendimiento.",
      "upvotes": 8,
      "discussionId": "67be807e8a5a805423137cc2"
    },
    "publishedAt": "2025-02-28T01:55:41.427Z",
    "title": "Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16944.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669dcf6200970c3b27aafa5d",
      "avatarUrl": "/avatars/bb9ed5ff86326fdaeb184c6b0e40f74f.svg",
      "fullname": "kaikai yang",
      "name": "keanudicap",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20321",
      "authors": [
        {
          "_id": "67c13c68d8247a49b808fdac",
          "name": "Chuofan Ma",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdad",
          "name": "Yi Jiang",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdae",
          "name": "Junfeng Wu",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdaf",
          "name": "Jihan Yang",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdb0",
          "name": "Xin Yu",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdb1",
          "name": "Zehuan Yuan",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdb2",
          "name": "Bingyue Peng",
          "hidden": false
        },
        {
          "_id": "67c13c68d8247a49b808fdb3",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T17:47:01.000Z",
      "title": "UniTok: Visión generación y comprensión integrados en el tokenizador",
      "summary": "La diferencia entre la generación visual y la representación de comprensión es que estas funciones crean un importante vacío que debe ser llenado para integrarlas en un solo marco. Para llenar este vacío, presentamos UniTok. UniTok es un tokenizador visual distribuido que codifica información detallada necesaria para la generación, mientras que proporciona un contexto alto nivel para la comprensión. Los recientes estudios han demostrado que este objetivo puede causar conflictos de pérdida durante el entrenamiento, pero hemos descubierto que la raíz de este conflicto es la limitación de la representación de los tokens distribuidos. Para resolver esto, hemos introducido la multi-codebook offshoring. Este método evita la instabilidad de entrenamiento causada por códigobooks excesivos, expandiendo el espacio de características potenciales al distribuir la vector quantización en varios sub-codebooks independientes. Nuestro método puede elevar significativamente la capacidad de un tokenizador distribuido uniforme y competir, o incluso superar, a tokenizadores continuos de dominio específico. Por ejemplo, UniTok ha logrado un rFID sorprendente de 0.38 en ImageNet (en comparación con 0.87 de SD-VAE) y una precisión de 0-shot de 78.6%, que supera al 76.2% del CLIP. Nuestro código está disponible en https://github.com/FoundationVision/UniTok.",
      "upvotes": 8,
      "discussionId": "67c13c6ad8247a49b8090003"
    },
    "publishedAt": "2025-02-27T23:34:45.416Z",
    "title": "UniTok: A Unified Tokenizer for Visual Generation and Understanding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20321.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6344dcb1cd37e44d9ed46508",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344dcb1cd37e44d9ed46508/J92UKSxKR3iziD2WJfih4.jpeg",
      "fullname": "Yi Jiang",
      "name": "JiangYi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20126",
      "authors": [
        {
          "_id": "67c14524af5eaa8dd062a216",
          "name": "Sotiris Anagnostidis",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a217",
          "name": "Gregor Bachmann",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a218",
          "name": "Yeongmin Kim",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a219",
          "name": "Jonas Kohler",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21a",
          "name": "Markos Georgopoulos",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21b",
          "name": "Artsiom Sanakoyeu",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21c",
          "name": "Yuming Du",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21d",
          "name": "Albert Pumarola",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21e",
          "name": "Ali Thabet",
          "hidden": false
        },
        {
          "_id": "67c14524af5eaa8dd062a21f",
          "name": "Edgar Schönfeld",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T14:16:56.000Z",
      "title": "FlexiDiT: Tu Diffusion Transformer puede generar muestras de alta calidad con poco cálculo.",
      "summary": "Los modernos Transformers de Difusión celebran excelentes resultados en inferencia, pero requieren una gran cantidad fija de recursos computacionales debido a la fijación y el tamaño de la cantidad de cálculos necesarias en cada etapa de eliminación de ruido. En este artículo, se reevalúa la forma de asignación de cajas de cálculo fija basada en el paradigma estático y se propone una estrategia dinámica. Mediante nuestro sencillo y amortiguable marco, se transforman modelos DiT preentrenados y se convierten en modelos flexibles llamados FlexiDiT. Estos modelos pueden procesar entradas en cajas de cálculo variables. Demostramos que, en la generación de imágenes condicionadas a la clase y al contexto, se puede reducir en más del 40% de los FLOPs en comparación con modelos estáticos. Nuestro método es un método general que no depende de la entrada ni del modelo condicionado y se puede fácilmente expandir para la generación de videos. El modelo FlexiDiT puede reducir el 75% del cálculo sin perjudicar el rendimiento.",
      "upvotes": 6,
      "discussionId": "67c14529af5eaa8dd062a38c"
    },
    "publishedAt": "2025-02-28T00:10:30.864Z",
    "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6246
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.19587",
      "authors": [
        {
          "_id": "67c13aa6a43d7939d60eb02e",
          "name": "Lola Le Breton",
          "hidden": false
        },
        {
          "_id": "67c13aa6a43d7939d60eb02f",
          "name": "Quentin Fournier",
          "hidden": false
        },
        {
          "_id": "67c13aa6a43d7939d60eb030",
          "name": "Mariam El Mezouar",
          "hidden": false
        },
        {
          "_id": "67c13aa6a43d7939d60eb031",
          "name": "Sarath Chandar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T22:00:22.000Z",
      "title": "NeoBERT: Siglo de BERT",
      "summary": "Las últimas innovaciones en arquitectura, predicción y manipulación han despertado admiración al demostrar el poder cerebral y la capacidad de inferencia de modelos grandes como LLaMA y DeepSeek. Por otro lado, los encoderes de BERT y RoBERTa, que forman la base de muchas aplicaciones NLP, no han mostrado el mismo nivel de progreso. Para cerrar esta brecha, presentamos NeoBERT. NeoBERT es un encoder del siguiente generación que redefine las capacidades de modelos bidireccionales mediante la integración de los últimos avances en arquitectura, datos y predicción. NeoBERT está diseñado para ser introducido fácilmente como un plug-in y paquete de modelos base existentes. Utiliza una proporción óptima de profundidad y ancho, así como una longitud de contexto extendida de 4,096 tokens. Incluye un pequeño modelo de 250M parámetros que registra los resultados más avanzados en el benchmark MTEB, superando a BERT large, RoBERTa large, NomicBERT y ModernBERT bajo las mismas condiciones de manipulación. Además, ha realizado una evaluación estricta en GLUE y diseñado un marco de trabajo continuo de manipulación y evaluación para MTEB. Todo el código, datos, checkpoints y scripts de manipulación están públicos para fomentar la investigación y la introducción en la realidad.",
      "upvotes": 5,
      "discussionId": "67c13aa7a43d7939d60eb065"
    },
    "publishedAt": "2025-02-28T03:27:32.294Z",
    "title": "NeoBERT: A Next-Generation BERT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19587.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6317233cc92fd6fee317e030",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png",
      "fullname": "Tom Aarsen",
      "name": "tomaarsen",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 1591
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20307",
      "authors": [
        {
          "_id": "67c1460201cef6d4b9b9ac73",
          "name": "Xiuli Bi",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac74",
          "name": "Jianfei Yuan",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac75",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac76",
          "name": "Yong Zhang",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac77",
          "name": "Xiaodong Cun",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac78",
          "name": "Chi-Man Pun",
          "hidden": false
        },
        {
          "_id": "67c1460201cef6d4b9b9ac79",
          "name": "Bin Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T17:33:51.000Z",
      "title": "Mobius: Generación de vídeos por vínculos no indirectos a partir de texto por transformación potencial",
      "summary": "Mobius introduce a new method for generating transparent loop animations that include visual feedback from text descriptions. This approach provides new visual materials for various media expressions. Our method repurposes a pre-trained video potential diffusion model to generate loop animations from text prompts. During inference, noise at the start and end of the video is connected to build a potential cycle. To maintain temporal consistency, the potential value of the first frame is moved to the last frame for denoising processing on each frame. This ensures that the inference process maintains consistency as noise cases change. Additionally, our method's potential cycle can be of any length, surpassing the model's case. This allows Mobius to generate transparent loop animations that surpass previous methods. Unlike previous cinemagraphs, the proposed method does not require limiting images to appear. This results in more dynamic and visually superior generated actions. The effectiveness of the proposed method is demonstrated through several experiments and comparisons. All code is now available.",
      "upvotes": 5,
      "discussionId": "67c1460501cef6d4b9b9addf"
    },
    "publishedAt": "2025-02-28T00:14:01.841Z",
    "title": "Mobius: Text to Seamless Looping Video Generation via Latent Shift",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20307.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6246
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20127",
      "authors": [
        {
          "_id": "67c12de08cd49ca63e230b99",
          "user": {
            "_id": "654da66fb36f85a025bc24b6",
            "avatarUrl": "/avatars/e5542856ab4bf1845e8f546b5f17cd99.svg",
            "isPro": false,
            "fullname": "Zexiong Ma",
            "user": "mizersy",
            "type": "user"
          },
          "name": "Zexiong Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-28T09:28:35.503Z",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9a",
          "name": "Chao Peng",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9b",
          "name": "Pengfei Gao",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9c",
          "name": "Xiangxin Meng",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9d",
          "name": "Yanzhen Zou",
          "hidden": false
        },
        {
          "_id": "67c12de08cd49ca63e230b9e",
          "name": "Bing Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T14:19:45.000Z",
      "title": "SoRFT: Solución de Problemas en la Regulación de Fortalecimiento para Subtareas",
      "summary": "El marco principal para la resolución de problemas es basado principalmente en modelos comerciales, lo que implica altos costos y preocupaciones de privacidad. El enfoque actual para la entrenamiento de problemas de resolución se ve afectado por una pérdida de capacidad de generalización y una insuficiencia en el uso de recursos de desarrollo abierto. Proponemos un nuevo enfoque de entrenamiento para mejorar la capacidad de resolución de problemas de modelos de lenguaje de máquina (LLM), llamado Subtask-oriented Reinforced Fine-Tuning (SoRFT). Se decomponen los problemas en sub-tareas estructuradas: ubicación del archivo, ubicación de la función, ubicación de la línea, generación de ediciones de código. SoRFT está configurado en dos etapas de entrenamiento: (1) ajuste micro con muestras rechazadas y observaciones, utilizando datos de CoT (Chain of Thought) filtrados según la realidad para ajustar el LLM. (2) Entrenamiento de aprendizaje reforzado basado en reglas, utilizando PPO para aplicar recompensas basadas en la realidad. Los modelos entrenados con SoRFT fueron evaluados en SWE-Bench Verified y SWE-Bench Lite, y lograron los mejores resultados entre los modelos abierto-source (por ejemplo, SoRFT-Qwen-7B resolvió el 21.4% de los problemas en SWE-Bench Verified). Los resultados de los experimentos muestran que SoRFT mejora significativamente la capacidad de resolución de problemas, mejora la capacidad de generalización del modelo y proporciona una alternativa más rentable en términos de costo a los modelos comerciales.",
      "upvotes": 5,
      "discussionId": "67c12de08cd49ca63e230bd1"
    },
    "publishedAt": "2025-02-27T22:38:04.562Z",
    "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20127.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654da66fb36f85a025bc24b6",
      "avatarUrl": "/avatars/e5542856ab4bf1845e8f546b5f17cd99.svg",
      "fullname": "Zexiong Ma",
      "name": "mizersy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20172",
      "authors": [
        {
          "_id": "67c17b8f60206395233b7e46",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "67c17b8f60206395233b7e47",
          "name": "Shuai Bai",
          "hidden": false
        },
        {
          "_id": "67c17b8f60206395233b7e48",
          "name": "Wenhao Chai",
          "hidden": false
        },
        {
          "_id": "67c17b8f60206395233b7e49",
          "name": "Weichu Xie",
          "hidden": false
        },
        {
          "_id": "67c17b8f60206395233b7e4a",
          "name": "Haozhe Zhao",
          "hidden": false
        },
        {
          "_id": "67c17b8f60206395233b7e4b",
          "name": "Leon Vinci",
          "hidden": false
        },
        {
          "_id": "67c17b8f60206395233b7e4c",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "67c17b8f60206395233b7e4d",
          "name": "Baobao Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T15:08:39.000Z",
      "title": "La regulación de expresiones multimodal y la generación de imágenes: no deberías dudar en que el control cruzado entre texto e imagen no sea tan difícil.",
      "summary": "En el campo de la generación de imágenes en texto avanzado, se ha iniciado el desarrollo de un marco integrado que combina un potente codificador de texto (por ejemplo, CLIP, T5) y un transformer de difusión con bonus. Además, existen efectos para controlar la imagen generada con condiciones adicionales como mapas de Canny o Depth, pero no existe un único marco de diseño para controlar la intersección arbitraria de texto y imágenes. En particular, cuando se integran conceptos o elementos visuales extraídos de varias imágenes durante el proceso de generación, este vacío es evidente. Para remediar esto, hemos realizado experimentos preliminares con modelos de gran escala multimodal (LMMs) para proporcionar una representación común que permita a las imágenes y textos responder mejor y a utilizar condiciones de modelos de difusión externos. Basándonos en estas observaciones, proponemos un marco integrado eficiente para modelos de generación de imágenes que se utilizan para el control cruzado de texto y imágenes. Basado en modelos de imágenes generadas de texto fuertes (por ejemplo, SD3.5), reemplazamos solo el codificador de texto original y integramos diferentes codificadores multimodales como QwenVL. Nuestro enfoque utiliza un patrón de entrenamiento de dos etapas para la colaboración entre la correspondencia común de texto y imágenes y el control cruzado multimodal. Según los resultados de los experimentos, este método de entrenamiento es efectivo y alcanza un puntaje general de 0.69 en el benchmark GenEval, mostrando un rendimiento equivalente a los modelos de texto a imágenes más avanzados (por ejemplo, SD3.5, FLUX).",
      "upvotes": 4,
      "discussionId": "67c17b9160206395233b7e9c"
    },
    "publishedAt": "2025-02-28T04:02:19.534Z",
    "title": "Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20172.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 31
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.19735",
      "authors": [
        {
          "_id": "67c1438fd7ffcd1cab1fc412",
          "user": {
            "_id": "6727998d4fc2e4f7cc0c85d3",
            "avatarUrl": "/avatars/ac18eaadd606f7fae64996502f393cf2.svg",
            "isPro": false,
            "fullname": "he",
            "user": "boommmmm",
            "type": "user"
          },
          "name": "Minggui He",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-28T05:03:12.675Z",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc413",
          "name": "Yilun Liu",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc414",
          "name": "Shimin Tao",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc415",
          "name": "Yuanchang Luo",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc416",
          "name": "Hongyong Zeng",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc417",
          "name": "Chang Su",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc418",
          "name": "Li Zhang",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc419",
          "name": "Hongxia Ma",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41a",
          "name": "Daimeng Wei",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41b",
          "name": "Weibin Meng",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41c",
          "name": "Hao Yang",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41d",
          "name": "Boxing Chen",
          "hidden": false
        },
        {
          "_id": "67c1438fd7ffcd1cab1fc41e",
          "name": "Osamu Yoshie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T03:57:00.000Z",
      "title": "R1-T1: Motivo y aprendizaje para la mejora de la capacidad de traducción completa de los modelos de lenguaje grandes (LLM)",
      "summary": "Recientemente, el desarrollo de modelos lógicos como DeepSeek-R1 ha llevado a que en la traducción automática (MT) se ha introducido la lógica en la inferencia, pero los estudios que apliquen estructuras lógicas multiniveles, como las CoTs (Continuos de Teoría de la Lógica) que los humanos traductores utilizan naturalmente, aún son insuficientes. Los métodos actuales diseñan CoTs fijos para sub-tareas específicas de MT o utilizan CoTs sintéticos que incluyen la asimetría con respecto a los humanos, con el riesgo de olvido catástrofico debido a la normalización de aprendizaje (SFT). En este artículo, se presenta un nuevo marco llamado R1-Translator (R1-T1), que aplica un aprendizaje por refuerzo (RL) para lograr la lógica en la inferencia de MT utilizando CoTs que se adaptan a los humanos. Nuestro enfoque introduce tres innovaciones: (1) la traducción basada en lógica que se extiende más allá de las sub-tareas de MT a 6 idiomas y diversas tareas (por ejemplo, aplicaciones en campos legales o médicos, y la resolución de vocabulario); (2) los seis expertos formalizan templates de CoT personalizados y crean estrategias humanas; (3) se utiliza una recompensa basada en la restricción de KL para facilitar la evolución automática de los CoT y la adaptación a la olvidación. Los resultados de los experimentos muestran un mejoramiento en la calidad de traducción estable en 21 idiomas y 80 direcciones de traducción en el conjunto de prueba Flores-101, manteniendo especialmente la capacidad multilingüe general en 15 idiomas que no fueron vistos durante el entrenamiento, y demostrando también su efectividad frente a la SFT normalizada.",
      "upvotes": 3,
      "discussionId": "67c14390d7ffcd1cab1fc479"
    },
    "publishedAt": "2025-02-28T00:03:34.893Z",
    "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6246
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.19459",
      "authors": [
        {
          "_id": "67c185f46a31b8fe77434551",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "67c185f46a31b8fe77434552",
          "name": "Baoxiong Jia",
          "hidden": false
        },
        {
          "_id": "67c185f46a31b8fe77434553",
          "name": "Ruijie Lu",
          "hidden": false
        },
        {
          "_id": "67c185f46a31b8fe77434554",
          "name": "Junfeng Ni",
          "hidden": false
        },
        {
          "_id": "67c185f46a31b8fe77434555",
          "name": "Song-Chun Zhu",
          "hidden": false
        },
        {
          "_id": "67c185f46a31b8fe77434556",
          "name": "Siyuan Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T10:25:32.000Z",
      "title": "Aplicación del método de dispersión de Gauss en el diseño de graficas computacionales para la construcción de solicitudes de interacción entre objetos dinámicos complejos",
      "summary": "ArtGS es un nuevo enfoque que utiliza representaciones eficientes y flexibles de 3D Gauss para resolver los problemas de análisis de la estructura de objetos conectados. Los métodos actuales no pueden procesar información coherentemente entre diferentes estados de los objetos, lo que limita la precisión de la reconfiguración de la malla y la modelación dinámica de los componentes, especialmente en estructuras complejas de múltiples componentes. Nuestro enfoque utiliza la densidad y el actualizamiento de algoritmos para procesar coherentemente la información de objetos conectados, mejorando la modelación dinámica de los componentes mediante el skinning, y mejorando tanto la reconfiguración de la malla como el aprendizaje del algoritmo. Mediante experimentos distribuidos que incluyen conjuntos de datos sintéticos y reales, incluyendo un nuevo benchmark para estructuras complejas de múltiples componentes, ArtGS logra el mejor desempeño en el cálculo de parámetros de conexión y en la reconfiguración de la malla. Nuestro enfoque mejora significativamente la calidad y eficiencia de la reconfiguración en estructuras de múltiples componentes. Además, proporcionamos un análisis detallado de las decisiones de diseño, prueban el efecto de cada elemento y revelan las posibilidades de futuras mejoras.",
      "upvotes": 1,
      "discussionId": "67c185f66a31b8fe774345d2"
    },
    "publishedAt": "2025-02-28T04:47:08.197Z",
    "title": "Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19459.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c7a33121bd95f80ed74652",
      "avatarUrl": "/avatars/7dd59afea785a2bff0ec2b757abd474e.svg",
      "fullname": "Siyuan Huang",
      "name": "thuhsy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]