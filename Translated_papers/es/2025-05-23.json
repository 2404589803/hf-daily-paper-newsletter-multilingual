[
  {
    "paper": {
      "id": "2505.16938",
      "authors": [
        {
          "_id": "682fe3a565bac3ec3556fc6c",
          "name": "NovelSeek Team",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc6d",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc6e",
          "name": "Shiyang Feng",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc6f",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc70",
          "name": "Jiakang Yuan",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc71",
          "name": "Zhiyin Yu",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc72",
          "name": "Xiaohan He",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc73",
          "name": "Songtao Huang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc74",
          "name": "Shaowei Hou",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc75",
          "name": "Zheng Nie",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc76",
          "name": "Zhilong Wang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc77",
          "name": "Jinyao Liu",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc78",
          "name": "Runmin Ma",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc79",
          "name": "Tianshuo Peng",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7a",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7b",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7c",
          "name": "Shufei Zhang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7d",
          "name": "Xiaosong Wang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7e",
          "name": "Yilan Zhang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7f",
          "name": "Meng Li",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc80",
          "name": "Zhongying Tu",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc81",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc82",
          "name": "Wangli Ouyang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc83",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc84",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:27:43.000Z",
      "submittedOnDailyAt": "2025-05-23T01:25:34.477Z",
      "title": "Nubelsh: Agente científico que divide - Construcción de un sistema cerrado desde la hipótesis hasta la prueba de la misma",
      "submittedOnDailyBy": {
        "_id": "643dfd235aafbdca3a5792c0",
        "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg",
        "isPro": false,
        "fullname": "Bo Zhang",
        "user": "BoZhang",
        "type": "user"
      },
      "summary": "La inteligencia artificial (IA) está acelerando los cambios en el paradigma de la investigación científica y fortaleciendo la eficiencia de la misma, liderando la innovación. Se presenta el Nueral Sketch. Nueral Sketch es un marco de trabajo integrado y cerrado multi-agente que realiza traducciones automáticas en diversas áreas de la investigación científica, permitiendo a los investigadores resolver problemas complejos con una velocidad y precisión sin precedentes. Nueral Sketch destaca por tres principales ventajas: 1) Escalabilidad: muestra diversidad en 12 tareas de investigación científica y genera innovaciones para mejorar el rendimiento de códigos de referencia. 2) Interactividad: proporciona interacción entre el feedback de expertos humanos desde terminales automatizados y múltiples agentes, facilitando la integración continua de conocimientos de expertos en una conversación. 3) Eficiencia: reduce significativamente el esfuerzo humano y ofrece ganancias de rendimiento sorprendentes en diversas áreas científicas. Por ejemplo, la predicción de producción de reacciones se mejoró del 27.6% a 35.4% en 12 horas, la predicción de actividad en enzimas del 0.52 a 0.79 en 4 horas, y la segmentación semántica 2D del 78.8% a 81.0% en 30 horas.",
      "upvotes": 70,
      "discussionId": "682fe3a865bac3ec3556fd21"
    },
    "publishedAt": "2025-05-22T13:27:43.000Z",
    "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification",
    "summary": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16938.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "643dfd235aafbdca3a5792c0",
      "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg",
      "fullname": "Bo Zhang",
      "name": "BoZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16410",
      "authors": [
        {
          "_id": "682fd6045e83dc325675312b",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312c",
          "name": "Yifei Chen",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312d",
          "name": "Xiaoxi Li",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312e",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312f",
          "name": "Hongjin Qian",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753130",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753131",
          "name": "Hangyu Mao",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753132",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753133",
          "name": "Zhicheng Dou",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753134",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T09:00:19.000Z",
      "submittedOnDailyAt": "2025-05-23T00:31:41.669Z",
      "title": "Tool-Star: Aprendizaje por Reforzamiento para el Soporte de Verificador de Funciones Multifuncionales en LLM en Modo Ciego",
      "submittedOnDailyBy": {
        "_id": "61cd4b833dd34ba1985e0753",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
        "isPro": false,
        "fullname": "KABI",
        "user": "dongguanting",
        "type": "user"
      },
      "summary": "Recientemente, los Modelos de Lenguaje de Largo Alcance (LLMs) han demostrado habilidades lógicas impresionantes a través de grandes escenarios de Aprendizaje por Reforzamiento (RL). Sin embargo, la colaboración efectiva de múltiples herramientas y la fortalecimiento de la lógica mediante RL en los LLMs representa un desafío abierto. En este artículo, se presenta un marco de trabajo basado en RL llamado Tool-Star, cuyo objetivo es que los LLMs puedan llamar automáticamente a diversas herramientas externas en diferentes etapas de la lógica. Tool-Star integra seis herramientas y adopta un diseño sistemático para la síntesis y entrenamiento de datos. Para abordar la escasez de datos de uso de herramientas, se propone un sistema de síntesis de datos lógicos que incluye herramientas generales, combinando el uso de Prompts y Hints para generar procesos de uso de herramientas automáticamente y escalables. Además, se utilizan procesos de normalización de calidad y clasificación de niveles de dificultad para eliminar muestras de baja calidad y organizar el conjunto de datos de manera creciente desde la dificultad. Se propone un marco de entrenamiento en dos etapas, incluyendo un ajuste frío y una lógica de recompensa heurística para la autoevaluación de múltiples herramientas. Los análisis experimentales en 10 o más benchmarks lógicos difíciles demuestran la eficacia y eficiencia de Tool-Star. El código está disponible en: https://github.com/dongguanting/Tool-Star.",
      "upvotes": 37,
      "discussionId": "682fd6055e83dc3256753187",
      "projectPage": "https://github.com/dongguanting/Tool-Star/",
      "githubRepo": "https://github.com/dongguanting/Tool-Star/",
      "ai_summary": "Tool-Star, an RL-based framework, enables LLMs to autonomously use multiple tools for stepwise reasoning, leveraging data synthesis and hierarchical reward design.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "large-scale reinforcement learning",
        "RL",
        "multi-tool collaborative reasoning",
        "tool-use data",
        "tool-integrated reasoning",
        "tool-invocation feedback",
        "multi-tool self-critic",
        "hierarchical reward design"
      ]
    },
    "publishedAt": "2025-05-22T05:00:19.000Z",
    "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement\n  Learning",
    "summary": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61cd4b833dd34ba1985e0753",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
      "fullname": "KABI",
      "name": "dongguanting",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14810",
      "authors": [
        {
          "_id": "682ea2b450671dc82688b8ad",
          "user": {
            "_id": "640ad17a1ee054d66a74783e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640ad17a1ee054d66a74783e/u0PjIkyC-9HkGzEyUQ7JN.jpeg",
            "isPro": false,
            "fullname": "Tingchen Fu",
            "user": "TingchenFu",
            "type": "user"
          },
          "name": "Tingchen Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:44.217Z",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8ae",
          "name": "Jiawei Gu",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8af",
          "user": {
            "_id": "63f3502a520c14618925825a",
            "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
            "isPro": false,
            "fullname": "Yafu Li",
            "user": "yaful",
            "type": "user"
          },
          "name": "Yafu Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-22T04:06:13.396Z",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8b0",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8b1",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T18:18:01.000Z",
      "submittedOnDailyAt": "2025-05-23T00:49:30.349Z",
      "title": "Scaling Region, Loss of Control: Evaluation of Instruction Following in Large-Scale Logical Models",
      "submittedOnDailyBy": {
        "_id": "64cb54da1af278541d663708",
        "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
        "isPro": false,
        "fullname": "Xiaoye Qu",
        "user": "Xiaoye08",
        "type": "user"
      },
      "summary": "Seguir instrucciones es crucial para ajustar los modelos de lenguaje grandes (LLMs) a las intenciones del usuario. Los modelos más recientes han demostrado un excelente rendimiento en problemas matemáticos complejos, pero su capacidad para seguir instrucciones en lenguaje natural aún no ha sido suficientemente investigada. En este artículo, se presenta \"MathIF\", un marco de referencia especializado para evaluar el seguimiento de instrucciones en tareas de razonamiento matemático. Los análisis experimentales revelaron claramente que mantener la controlabilidad (Controllability) al mejorar la capacidad de razonamiento es un desafío. Además, los modelos con alta capacidad de razonamiento tienden a tener dificultades para seguir las instrucciones del usuario. Limitar la longitud de las secuencias o entrenarlos con aprendizaje por refuerzo en razonamiento puede disminuir el rendimiento del seguimiento de instrucciones, especialmente cuando las secuencias de texto son largas. Además, la recuperación parcial del seguimiento de instrucciones con simples interferencias puede costear la capacidad de razonamiento, revelando una contraposición fundamental en el paradigma actual de entrenamiento de LLMs y subrayando la necesidad de modelos de razonamiento para seguir instrucciones. Los códigos y datos están disponibles en https://github.com/TingchenFu/MathIF.",
      "upvotes": 37,
      "discussionId": "682ea2b550671dc82688b8e2",
      "githubRepo": "https://github.com/TingchenFu/MathIF",
      "ai_summary": "An empirical analysis of MathIF identifies a tension between enhancing reasoning capacity and maintaining instruction adherence in large language models.",
      "ai_keywords": [
        "instruction-following",
        "reasoning-oriented models",
        "benchmarks",
        "chains-of-thought",
        "reinforcement learning",
        "instruction adherence"
      ]
    },
    "publishedAt": "2025-05-20T14:18:01.000Z",
    "title": "Scaling Reasoning, Losing Control: Evaluating Instruction Following in\n  Large Reasoning Models",
    "summary": "Instruction-following is essential for aligning large language models (LLMs)\nwith user intent. While recent reasoning-oriented models exhibit impressive\nperformance on complex mathematical problems, their ability to adhere to\nnatural language instructions remains underexplored. In this work, we introduce\nMathIF, a dedicated benchmark for evaluating instruction-following in\nmathematical reasoning tasks. Our empirical analysis reveals a consistent\ntension between scaling up reasoning capacity and maintaining controllability,\nas models that reason more effectively often struggle to comply with user\ndirectives. We find that models tuned on distilled long chains-of-thought or\ntrained with reasoning-oriented reinforcement learning often degrade in\ninstruction adherence, especially when generation length increases.\nFurthermore, we show that even simple interventions can partially recover\nobedience, though at the cost of reasoning performance. These findings\nhighlight a fundamental tension in current LLM training paradigms and motivate\nthe need for more instruction-aware reasoning models. We release the code and\ndata at https://github.com/TingchenFu/MathIF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14810.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb54da1af278541d663708",
      "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
      "fullname": "Xiaoye Qu",
      "name": "Xiaoye08",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16707",
      "authors": [
        {
          "_id": "682fdd77e3102e71872d9b00",
          "name": "Yongliang Wu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b01",
          "name": "Zonghui Li",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b02",
          "name": "Xinting Hu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b03",
          "name": "Xinyu Ye",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b04",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b05",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b06",
          "name": "Wenbo Zhu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b07",
          "name": "Bernt Schiele",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b08",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b09",
          "name": "Xu Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T14:08:59.000Z",
      "submittedOnDailyAt": "2025-05-23T00:59:23.402Z",
      "title": "KRIS-Bench: Marco de Prueba para Modelos de Edición de Imágenes Inteligentes de la Futura",
      "submittedOnDailyBy": {
        "_id": "66f6bc97980d52c75c300511",
        "avatarUrl": "/avatars/f7c23c4b09701580b533212ec9b6e306.svg",
        "isPro": false,
        "fullname": "Yongliang",
        "user": "Liang0223",
        "type": "user"
      },
      "summary": "El desarrollo reciente de modelos generativos multimodal ha logrado significativas avances en la edición de imágenes según indicaciones. Sin embargo, estos modelos no solo deben crear salidas visualmente adecuadas; también deben ejecutar tareas de edición lógica basadas en conocimientos. Este artículo presenta KRIS-Bench, un marco de referencia para evaluar modelos basados en conocimientos lógicos en sistemas de edición de imágenes. KRIS-Bench, inspirado en teorías educativas, clasifica las tareas de edición en tres tipos básicos de conocimientos: factuales, conceptuales y procesuales. Basándose en estos nombres técnicos, se diseñó un total de 22 trabajos representativos en 7 dimensiones lógicas, y se lanzó una colección de 1,267 instancias etiquetadas de alta calidad. Se propone un nuevo métrico para evaluar la razonabilidad del conocimiento y se ajustan sugerencias de conocimiento para mejorar el estudio humano. Los resultados de experimentos con los 10 modelos más avanzados muestran claramente una gran variación en el rendimiento lógico, subrayando la necesidad de un marco de referencia centrado en conocimientos y promoviendo el desarrollo de sistemas de edición de imágenes basados en el cerebro.",
      "upvotes": 33,
      "discussionId": "682fdd79e3102e71872d9b79",
      "projectPage": "https://yongliang-wu.github.io/kris_bench_project_page/",
      "githubRepo": "https://github.com/mercurystraw/Kris_Bench",
      "ai_summary": "KRIS-Bench assesses generative models' knowledge-based reasoning in image editing through a taxonomy of editing tasks and a Knowledge Plausibility metric.",
      "ai_keywords": [
        "multi-modal generative models",
        "instruction-based image editing",
        "knowledge-based reasoning",
        "KRIS-Bench",
        "cognitive assessment",
        "foundational knowledge types",
        "Factual",
        "Conceptual",
        "Procedural",
        "reasoning dimensions",
        "Knowledge Plausibility metric"
      ]
    },
    "publishedAt": "2025-05-22T10:08:59.000Z",
    "title": "KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models",
    "summary": "Recent advances in multi-modal generative models have enabled significant\nprogress in instruction-based image editing. However, while these models\nproduce visually plausible outputs, their capacity for knowledge-based\nreasoning editing tasks remains under-explored. In this paper, we introduce\nKRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), a\ndiagnostic benchmark designed to assess models through a cognitively informed\nlens. Drawing from educational theory, KRIS-Bench categorizes editing tasks\nacross three foundational knowledge types: Factual, Conceptual, and Procedural.\nBased on this taxonomy, we design 22 representative tasks spanning 7 reasoning\ndimensions and release 1,267 high-quality annotated editing instances. To\nsupport fine-grained evaluation, we propose a comprehensive protocol that\nincorporates a novel Knowledge Plausibility metric, enhanced by knowledge hints\nand calibrated through human studies. Empirical results on 10 state-of-the-art\nmodels reveal significant gaps in reasoning performance, highlighting the need\nfor knowledge-centric benchmarks to advance the development of intelligent\nimage editing systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16707.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66f6bc97980d52c75c300511",
      "avatarUrl": "/avatars/f7c23c4b09701580b533212ec9b6e306.svg",
      "fullname": "Yongliang",
      "name": "Liang0223",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15966",
      "authors": [
        {
          "_id": "682fe6bd5f80e910085b5116",
          "name": "Alex Su",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b5117",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b5118",
          "name": "Weimin Ren",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b5119",
          "name": "Fangzhen Lin",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b511a",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:08:46.964Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/pPkAUFC7KKDS7Q9fhZKmM.png"
      ],
      "publishedAt": "2025-05-21T19:35:08.000Z",
      "submittedOnDailyAt": "2025-05-23T01:39:51.922Z",
      "title": "Pixel Reasoner: Apoya la teoría de la razón en el espacio de píxeles utilizando la neurona de corte de Kaical.",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "La pensamiento racional en cadena ha traído significativas mejoras en el rendimiento de los grandes modelos de lenguaje (LLMs) en diversas áreas. Sin embargo, este proceso de inferencia se desarrolla solo en el espacio de texto, lo que limita la efectividad en tareas de densidad visual. Para resolver esta limitación, proponemos el concepto de inferencia en el espacio de píxeles. En este nuevo marco, los modelos de lenguaje visual (VLMs) adquieren operaciones de inferencia visual, como amplificación y selección de cuadros. Estas operaciones permiten a los VLMs verificar y preguntar sobre pruebas visuales directamente, lo que mejora la verdad de la inferencia en tareas visuales. Además, incorporar esta capacidad de inferencia en el espacio de píxeles en los VLMs plantea desafíos claros, como la desigualdad inicial de capacidad del modelo y la resistencia a nuevas operaciones en el espacio de píxeles. Para abordar estos desafíos, utilizamos un método de entrenamiento en dos etapas. En la primera etapa, se realiza ajuste guiado por trayectorias de inferencia sintética para que el modelo se familiarice con nuevas operaciones visuales. En la segunda etapa, el aprendizaje por refuerzo (RL) utiliza un plan de recompensa impulsada por curiosidad para equilibrar el exploración entre inferencia en el espacio de píxeles y en el texto. A través de estas operaciones visuales, los VLMs pueden interactuar con complejos entradas visuales y, por ejemplo, recopilar información activamente en imágenes o videos ricas en contenido. Demostramos que estas metodologías mejoran significativamente el rendimiento de los VLMs en diferentes criterios de prueba de inferencia visual. Nuestro modelo de 700 millones de parámetros, \\model, alcanzó una precisión del 84% en los criterios V*, del 74% en TallyQA-Complex y del 84% en InfographicsVQA, lo que representa la precisión más alta hasta el día de hoy de un modelo abierto. Estos resultados subrayan la importancia de la inferencia en el espacio de píxeles y la efectividad de nuestro marco.",
      "upvotes": 25,
      "discussionId": "682fe6bf5f80e910085b51ae",
      "ai_summary": "Introducing pixel-space reasoning in Vision-Language Models (VLMs) through visual operations like zoom-in and select-frame enhances their performance on visual tasks.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "pixel-space reasoning",
        "visual reasoning operations",
        "zoom-in",
        "select-frame",
        "reinforcement learning",
        "RL",
        "curiosity-driven reward scheme",
        "V* bench",
        "TallyQA-Complex",
        "InfographicsVQA"
      ]
    },
    "publishedAt": "2025-05-21T15:35:08.000Z",
    "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with\n  Curiosity-Driven Reinforcement Learning",
    "summary": "Chain-of-thought reasoning has significantly improved the performance of\nLarge Language Models (LLMs) across various domains. However, this reasoning\nprocess has been confined exclusively to textual space, limiting its\neffectiveness in visually intensive tasks. To address this limitation, we\nintroduce the concept of reasoning in the pixel-space. Within this novel\nframework, Vision-Language Models (VLMs) are equipped with a suite of visual\nreasoning operations, such as zoom-in and select-frame. These operations enable\nVLMs to directly inspect, interrogate, and infer from visual evidences, thereby\nenhancing reasoning fidelity for visual tasks. Cultivating such pixel-space\nreasoning capabilities in VLMs presents notable challenges, including the\nmodel's initially imbalanced competence and its reluctance to adopt the newly\nintroduced pixel-space operations. We address these challenges through a\ntwo-phase training approach. The first phase employs instruction tuning on\nsynthesized reasoning traces to familiarize the model with the novel visual\noperations. Following this, a reinforcement learning (RL) phase leverages a\ncuriosity-driven reward scheme to balance exploration between pixel-space\nreasoning and textual reasoning. With these visual operations, VLMs can\ninteract with complex visual inputs, such as information-rich images or videos\nto proactively gather necessary information. We demonstrate that this approach\nsignificantly improves VLM performance across diverse visual reasoning\nbenchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on\nTallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy\nachieved by any open-source model to date. These results highlight the\nimportance of pixel-space reasoning and the effectiveness of our framework.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/pPkAUFC7KKDS7Q9fhZKmM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15966.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 38
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16175",
      "authors": [
        {
          "_id": "682fd91a1ffb93faf139d288",
          "name": "Benjamin Schneider",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d289",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d28a",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d28b",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d28c",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:26:50.000Z",
      "submittedOnDailyAt": "2025-05-23T00:43:01.292Z",
      "title": "¡Claro! Aquí tienes la traducción al español:\n\n**QUICK VIDEO: Comprensión Real-Time de Vídeos de Duración Prolongada Mediante Algoritmos de Sistema**\n**Code Sheep**",
      "submittedOnDailyBy": {
        "_id": "62567c86d444a9b5a0ec51c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
        "isPro": false,
        "fullname": "Dongfu Jiang",
        "user": "DongfuJiang",
        "type": "user"
      },
      "summary": "La comprensión de vídeos ha surgido como una habilidad importante en aplicaciones reales. Por ejemplo, se utiliza en videoeditado, resumenes de reuniones, análisis de clases educativas y transmisiones deportivas, entre otros campos. Sin embargo, en los modelos de lenguaje de vídeo (LLM), dos problemas principales limitan su ejecución: el alto consumo de cálculos y la imposibilidad de ejecución. 1) El decodificado secuencial de vídeo, en el proceso de convertir el flujo de bytes de vídeo en frames RGB, puede llevar aproximadamente un minuto para un vídeo de una hora. 2) Para la inferencia de LLM, se necesita cargar anticipadamente al GPU memoria para 100,000 tokens, lo que aumenta el tiempo de carga y la utilización de memoria. Para resolver estos problemas, se propone el sistema algoritmo QuickVideo, que busca acelerar significativamente la comprensión de vídeos y apoyar aplicaciones en tiempo real. QuickVideo está constituido por tres innovaciones clave: QuickDecoder, que divide el vídeo en intervalos correspondientes a frames clave, y mejora la velocidad en un 2-3 veces mediante un decodificador CPU paralelizado; QuickPrefill, que utiliza caching de KV para una lectura anticipada eficiente de memoria, permitiendo soportar más frames con menos memoria GPU; y la combinación de decodificación de vídeo CPU y inferencia en GPU mediante overlay de tiempo. Estos componentes reducen aproximadamente el tiempo de inferencia a un minuto para un vídeo largo y permiten una comprensión de vídeo de alta calidad escalable, incluso en dispositivos con hardware limitado. Las pruebas muestran que QuickVideo demostra una capacidad de escalabilidad en tiempo y frecuencia de muestreo, y es capaz de procesar vídeos de larga duración de manera práctica.",
      "upvotes": 24,
      "discussionId": "682fd91b1ffb93faf139d2d0",
      "ai_summary": "QuickVideo accelerates long-video understanding by combining a parallelized video decoder, memory-efficient prefilling, and overlapping video decoding with inference, enabling real-time performance.",
      "ai_keywords": [
        "QuickDecoder",
        "parallelized CPU-based video decoder",
        "keyframe-aligned intervals",
        "QuickPrefill",
        "memory-efficient prefilling",
        "KV-cache pruning",
        "overlapping scheme"
      ]
    },
    "publishedAt": "2025-05-21T23:26:50.000Z",
    "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
    "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16175.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62567c86d444a9b5a0ec51c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
      "fullname": "Dongfu Jiang",
      "name": "DongfuJiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17022",
      "authors": [
        {
          "_id": "682ffa9a6e906040a3bb7160",
          "name": "Chengqi Duan",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7161",
          "name": "Rongyao Fang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7162",
          "name": "Yuqing Wang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7163",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7164",
          "name": "Linjiang Huang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7165",
          "name": "Xingyu Zeng",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7166",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7167",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:58.000Z",
      "submittedOnDailyAt": "2025-05-23T03:06:29.578Z",
      "title": "GoT-R1: La liberación de la capacidad teórica de generación de visión para MLLM se logra mediante el aprendizaje por refuerzo.",
      "submittedOnDailyBy": {
        "_id": "64a2b496e2e19de17db7de65",
        "avatarUrl": "/avatars/241448ca487833d6cc5d57bb1fdb6ee5.svg",
        "isPro": false,
        "fullname": "Duan Chengqi",
        "user": "gogoduan",
        "type": "user"
      },
      "summary": "Los modelos de generación visual han experimentado un desarrollo sorprendente para generar imágenes realistas a partir de prompts de texto, pero en los prompts complejos que especifican múltiples objetos y relaciones espaciales y propiedades precisas, se enfrentan a dificultades. Para procesar estos prompts de manera efectiva, es necesario una lógica clara tanto en el contenido literario como en la disposición espacial. Presentamos un marco de entrenamiento para fortalecer la lógica en la generación visual, lo cual mejora la generación de imágenes. Este marco se basa en el enfoque de la cadena de pensamiento para generación (CoT), permitiendo una lógica literaria y espacial en los prompts complejos. Para lograrlo, proponemos un marco de recompensas multi-puntaje en dos etapas utilizando MLLM para evaluar el proceso de lógica y el output final. De esta manera, se puede apoyar efectivamente toda la cadena de generación. El sistema de recompensas evalúa de manera integral la coincidencia literaria, la precisión espacial y la calidad visual. Los resultados de los experimentos muestran una notable mejora en las tareas de composición que incluyen relaciones espaciales y propiedades precisas, como observado en el benchmark T2I-CompBench. GoT-R1 logró un éxito notable en la capacidad de lógica compleja, permitiendo que el proceso de generación visual se dirija hacia la creación de imágenes de alto nivel. Publicamos el código y modelos pre-entrenados en https://github.com/gogoduan/GoT-R1 para fomentar futuras investigaciones.",
      "upvotes": 21,
      "discussionId": "682ffa9b6e906040a3bb71ba",
      "githubRepo": "https://github.com/gogoduan/GoT-R1",
      "ai_summary": "GoT-R1 enhances visual generation by using reinforcement learning to improve semantic-spatial reasoning, outperforming existing models on complex compositional tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "Generation Chain-of-Thought",
        "MLLMs",
        "dual-stage multi-dimensional reward framework",
        "semantic alignment",
        "spatial accuracy",
        "visual quality",
        "T2I-CompBench"
      ]
    },
    "publishedAt": "2025-05-22T13:59:58.000Z",
    "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation\n  with Reinforcement Learning",
    "summary": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a2b496e2e19de17db7de65",
      "avatarUrl": "/avatars/241448ca487833d6cc5d57bb1fdb6ee5.svg",
      "fullname": "Duan Chengqi",
      "name": "gogoduan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16933",
      "authors": [
        {
          "_id": "682fe37bb998c9f79463b563",
          "name": "Zebin You",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b564",
          "name": "Shen Nie",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b565",
          "name": "Xiaolu Zhang",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b566",
          "name": "Jun Hu",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b567",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b568",
          "name": "Zhiwu Lu",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b569",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b56a",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:23:26.000Z",
      "submittedOnDailyAt": "2025-05-23T01:26:20.847Z",
      "title": "LLaDA-V: Modelo de Difusión de Lenguajes de Grandes Escalas y Entrenamiento de Instrucciones Visuales",
      "submittedOnDailyBy": {
        "_id": "624f909eac5dd186b01ac3f5",
        "avatarUrl": "/avatars/0aafdb1cbb492fda52a0303031cc6c14.svg",
        "isPro": false,
        "fullname": "Zebin You",
        "user": "yyyou",
        "type": "user"
      },
      "summary": "En este estudio, se presenta LLaDA-V, que es un modelo de múltiples modelos y lenguaje basado en la distribución completa. Este modelo se distingue al separarse del paradigma de auto-regressive predominante en los enfoques actuales de múltiples modelos, integrando la instrucción visual de entrenamiento y el modelo de distribución con mascara. Basado en LLaDA (un modelo de distribución de lenguaje representativo), LLaDA-V incluye un encoder visual y un conjunto de conectores MLP, proyectando características visuales en el espacio de embedding lingüístico para facilitar un alineamiento efectivo de múltiples modelos. A través de una investigación experimental, se obtuvieron los siguientes resultados interesantes: primero, LLaDA-V muestra resultados aceptables en comparación con modelos de lenguaje como LLaMA3-8B y Qwen2-7B en tareas simples de caracteres, pero mejores en términos de rendimiento de múltiples modelos. Con datos de instrucción equivalentes, LLaDA-V presenta una competencia alta en tareas de múltiples modelos comparado con LLaMA3-V y es adecuado para escalas de datos. Además, reduce la diferencia de rendimiento con Qwen2-VL, demostrando la eficiencia de esta estructura. Segundo, al comparar la combinación actual de auto-regressive y distribución completa de MLLM, LLaDA-V logra los mejores resultados en la comprensión de múltiples modelos. Nuestros hallazgos muestran que los modelos de distribución de lenguaje son adecuados en contextos de múltiples modelos y deben ser considerados en futuras investigaciones. La página del proyecto y el código están disponibles en la URL proporcionada.",
      "upvotes": 20,
      "discussionId": "682fe37cb998c9f79463b5ae",
      "ai_summary": "A diffusion-based Multimodal Large Language Model (LLaDA-V) with integrated visual instruction tuning performs competitively on multimodal tasks and outperforms existing models in multimodal understanding.",
      "ai_keywords": [
        "diffusion-based",
        "Multimodal Large Language Model (MLLM)",
        "visual instruction tuning",
        "masked diffusion models",
        "autoregressive paradigms",
        "vision encoder",
        "MLP connector",
        "language embedding space",
        "multimodal performance",
        "LLaDA",
        "LLaMA3-8B",
        "Qwen2-7B",
        "LLaMA3-V",
        "Qwen2-VL",
        "multimodal understanding",
        "hybrid autoregressive-diffusion"
      ]
    },
    "publishedAt": "2025-05-22T13:23:26.000Z",
    "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning",
    "summary": "In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large\nLanguage Model (MLLM) that integrates visual instruction tuning with masked\ndiffusion models, representing a departure from the autoregressive paradigms\ndominant in current multimodal approaches. Built upon LLaDA, a representative\nlarge language diffusion model, LLaDA-V incorporates a vision encoder and MLP\nconnector that projects visual features into the language embedding space,\nenabling effective multimodal alignment. Our empirical investigation reveals\nseveral intriguing results: First, LLaDA-V demonstrates promising multimodal\nperformance despite its language model being weaker on purely textual tasks\nthan counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same\ninstruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal\ntasks with better data scalability. It also narrows the performance gap to\nQwen2-VL, suggesting the effectiveness of its architecture for multimodal\ntasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal\nunderstanding compared to existing hybrid autoregressive-diffusion and purely\ndiffusion-based MLLMs. Our findings suggest that large language diffusion\nmodels show promise in multimodal contexts and warrant further investigation in\nfuture research. Project page and codes:\nhttps://ml-gsai.github.io/LLaDA-V-demo/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16933.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "624f909eac5dd186b01ac3f5",
      "avatarUrl": "/avatars/0aafdb1cbb492fda52a0303031cc6c14.svg",
      "fullname": "Zebin You",
      "name": "yyyou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16925",
      "authors": [
        {
          "_id": "68302d01b85e3ed6a61e6476",
          "user": {
            "_id": "67a33f0e36fccbd55d6e8f7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a33f0e36fccbd55d6e8f7f/o_2UU5bxtnPmPdKo-jg7x.png",
            "isPro": false,
            "fullname": "Igor Udovichenko",
            "user": "i-udovichenko",
            "type": "user"
          },
          "name": "Igor Udovichenko",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T08:16:34.263Z",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e6477",
          "name": "Olivier Croissant",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e6478",
          "name": "Anita Toleutaeva",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e6479",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e647a",
          "name": "Alexander Korotin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:18:07.000Z",
      "submittedOnDailyAt": "2025-05-23T06:41:43.892Z",
      "title": "Risco evasivo de aprendizaje reforzado con la pérdida etàkra-sitoscópica",
      "submittedOnDailyBy": {
        "_id": "67a33f0e36fccbd55d6e8f7f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a33f0e36fccbd55d6e8f7f/o_2UU5bxtnPmPdKo-jg7x.png",
        "isPro": false,
        "fullname": "Igor Udovichenko",
        "user": "i-udovichenko",
        "type": "user"
      },
      "summary": "El aprendizaje por refuerzo de evasión de riesgo se aplica en muchos ámbitos de alto riesgo. A diferencia de la aprendizaje por refuerzo clásico, el objetivo no es maximizar la utilidad sino minimizar el riesgo al seleccionar políticas, y a veces permiten temporalmente la utilidad esperada. Esta tendencia puede ser modelada por la teoría de la utilidad. Nos centramos en un caso especial de la función de utilidad exponencial, de la cual obtenemos la ecuación de Bellman y podemos aplicar algoritmos de aprendizaje por refuerzo sin cambios significativos. Sin embargo, estos métodos sufren de instabilidad numérica debido a la necesidad de cálculos exponenciales. Para resolver esto, introducimos una función de pérdida numéricamente estable y matemáticamente justificada basada en la divergencia de Itakura-Saito en el aprendizaje de funciones de valor de estado y de acción. Se evaluó teóricamente y experimentalmente comparando con sus alternativas. En la sección de experimentos, se revisaron varios escenarios financieros, incluyendo algunos donde ya se conocía la solución analítica, demostrando que la función de pérdida propuesta es superior a las alternativas.",
      "upvotes": 16,
      "discussionId": "68302d02b85e3ed6a61e64db",
      "ai_summary": "Proposed Itakura-Saito divergence-based loss function enhances numerical stability in risk-averse reinforcement learning using exponential utility functions.",
      "ai_keywords": [
        "reinforcement learning",
        "risk-averse",
        "utility theory",
        "exponential utility function",
        "Bellman equations",
        "numerical instability",
        "Itakura-Saito divergence",
        "state-value functions",
        "action-value functions"
      ]
    },
    "publishedAt": "2025-05-22T13:18:07.000Z",
    "title": "Risk-Averse Reinforcement Learning with Itakura-Saito Loss",
    "summary": "Risk-averse reinforcement learning finds application in various high-stakes\nfields. Unlike classical reinforcement learning, which aims to maximize\nexpected returns, risk-averse agents choose policies that minimize risk,\noccasionally sacrificing expected value. These preferences can be framed\nthrough utility theory. We focus on the specific case of the exponential\nutility function, where we can derive the Bellman equations and employ various\nreinforcement learning algorithms with few modifications. However, these\nmethods suffer from numerical instability due to the need for exponent\ncomputation throughout the process. To address this, we introduce a numerically\nstable and mathematically sound loss function based on the Itakura-Saito\ndivergence for learning state-value and action-value functions. We evaluate our\nproposed loss function against established alternatives, both theoretically and\nempirically. In the experimental section, we explore multiple financial\nscenarios, some with known analytical solutions, and show that our loss\nfunction outperforms the alternatives.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a33f0e36fccbd55d6e8f7f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a33f0e36fccbd55d6e8f7f/o_2UU5bxtnPmPdKo-jg7x.png",
      "fullname": "Igor Udovichenko",
      "name": "i-udovichenko",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15270",
      "authors": [
        {
          "_id": "682e907a24b2bb08885b94dc",
          "user": {
            "_id": "682e8e6d007cd8c2f2cd0afd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/az0B5USOPlLR3vq872JeN.png",
            "isPro": false,
            "fullname": "Chenyu Zheng",
            "user": "ChenyuZheng",
            "type": "user"
          },
          "name": "Chenyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:10.939Z",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94dd",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94de",
          "name": "Rongzhen Wang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94df",
          "name": "Wei Huang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e0",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e1",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e2",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e3",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T08:49:03.000Z",
      "submittedOnDailyAt": "2025-05-23T00:59:19.168Z",
      "title": "Utilizando el μP para expandir de manera eficiente los Transformers de Difusión",
      "submittedOnDailyBy": {
        "_id": "682e8e6d007cd8c2f2cd0afd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/az0B5USOPlLR3vq872JeN.png",
        "isPro": false,
        "fullname": "Chenyu Zheng",
        "user": "ChenyuZheng",
        "type": "user"
      },
      "summary": "Los Transformers de Difusión aparecen como la base de los modelos generadores visuales, sin embargo, su escalabilidad es limitada por el alto costo asociado con la ajuste de parámetros de gran escala. Recientemente, la Maximal Update Parametrization (muP) fue propuesta por la bergevent frange-mineur, permitiendo un ajuste de parámetros estable desde pequeños modelos hasta grandes modelos de lenguaje, reduciendo significativamente los costos de ajuste. Sin embargo, no se sabe si la versión de muP de bergevent frange-mineur puede aplicarse a los Transformers de Difusión. En este artículo, se generaliza la muP para los Transformers de Difusión y prueba su efectividad a través de experimentos de gran escala. Primero, se demuestra rigurosamente que los modelos de difusión transformers principales (DiT, U-ViT, PixArt-alpha, MMDiT) con muP coinciden con la muP de bergevent frange-mineur, y que se pueden aplicar directamente los métodos de muP existentes. Estos resultados se utilizan para probar la fuerte aplicabilidad del ajuste de parámetros de DiT-muP. En particular, se demuestra que DiT-XL-2-muP con un aprendizaje de razón converge más rápido que el original DiT-XL-2, alcanzando un rendimiento 2.9 veces más rápido. Finalmente, se verifica la efectividad de la muP en la generación de imágenes a partir de texto para PixArt-alpha y MMDiT. PixArt-alpha escala de 0.04B a 0.61B, mientras que MMDiT escala de 0.18B a 18B. Ambos modelos con muP superan los modelos de referencia con un costo de ajuste reducido. Solo se necesita el 5.5% del costo de un solo entrenamiento para PixArt-alpha y el 3% del consumo de un experto para MMDiT-18B. Estos resultados establecen la muP como un marco racional y eficiente para la escalabilidad de los Transformers de Difusión.",
      "upvotes": 16,
      "discussionId": "682e907b24b2bb08885b952c",
      "projectPage": "https://github.com/ML-GSAI/Scaling-Diffusion-Transformers-muP",
      "githubRepo": "https://github.com/ML-GSAI/Scaling-Diffusion-Transformers-muP",
      "ai_summary": "Maximal Update Parametrization (μP) is extended to diffusion Transformers, demonstrating efficient hyperparameter transferability and reduced tuning costs across various models and tasks.",
      "ai_keywords": [
        "Diffusion Transformers",
        "Maximal Update Parametrization",
        "μP",
        "hyperparameter tuning",
        "DiT",
        "U-ViT",
        "PixArt-α",
        "MMDiT",
        "text-to-image generation",
        "transfer learning",
        "convergence",
        "training run"
      ]
    },
    "publishedAt": "2025-05-21T04:49:03.000Z",
    "title": "Scaling Diffusion Transformers Efficiently via μP",
    "summary": "Diffusion Transformers have emerged as the foundation for vision generative\nmodels, but their scalability is limited by the high cost of hyperparameter\n(HP) tuning at large scales. Recently, Maximal Update Parametrization (muP)\nwas proposed for vanilla Transformers, which enables stable HP transfer from\nsmall to large language models, and dramatically reduces tuning costs. However,\nit remains unclear whether muP of vanilla Transformers extends to diffusion\nTransformers, which differ architecturally and objectively. In this work, we\ngeneralize standard muP to diffusion Transformers and validate its\neffectiveness through large-scale experiments. First, we rigorously prove that\nmuP of mainstream diffusion Transformers, including DiT, U-ViT,\nPixArt-alpha, and MMDiT, aligns with that of the vanilla Transformer,\nenabling the direct application of existing muP methodologies. Leveraging\nthis result, we systematically demonstrate that DiT-muP enjoys robust HP\ntransferability. Notably, DiT-XL-2-muP with transferred learning rate\nachieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we\nvalidate the effectiveness of muP on text-to-image generation by scaling\nPixArt-alpha from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,\nmodels under muP outperform their respective baselines while requiring small\ntuning cost, only 5.5% of one training run for PixArt-alpha and 3% of\nconsumption by human experts for MMDiT-18B. These results establish muP as a\nprincipled and efficient framework for scaling diffusion Transformers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "682e8e6d007cd8c2f2cd0afd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/az0B5USOPlLR3vq872JeN.png",
      "fullname": "Chenyu Zheng",
      "name": "ChenyuZheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14604",
      "authors": [
        {
          "_id": "682f34b52b9fdc24ae9de371",
          "name": "Haoran Zhao",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de372",
          "name": "Yuchen Yan",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de373",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de374",
          "name": "Haolei Xu",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de375",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de376",
          "name": "Kaitao Song",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de377",
          "name": "Jian Shao",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de378",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de379",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de37a",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T16:53:40.000Z",
      "submittedOnDailyAt": "2025-05-23T03:36:02.584Z",
      "title": "LLMs proponen reducir la presión de pensamiento excesivo a través de las limitaciones propias, permitiendo que funcionen de manera más libre.",
      "submittedOnDailyBy": {
        "_id": "5e1058e9fcf41d740b69966d",
        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
        "isPro": false,
        "fullname": "Yongliang Shen",
        "user": "tricktreat",
        "type": "user"
      },
      "summary": "Los modelos lógicos de discurso (LRMs) como OpenAI o1 y DeepSeek-R1 son ejemplos. Estos incrementan la longitud de la pensamiento para lograr excelencias en diversas tareas y mejoran significativamente su capacidad de razonamiento. Sin embargo, este aumento de rendimiento se ve afectado por un aumento significativo de razones redundantes y por sobrecargas computacionales y problemas de suposiciones excesivas en el proceso de generación. La mayoría de los métodos actuales dependen de la intervención externa para resolver este problema de suposiciones excesivas. Sin embargo, estas metodologías generalmente dependen de la regulación externa. En este artículo, se propone un nuevo marco que permita que los modelos regulen automáticamente el proceso de razonamiento. Este marco evita la dependencia de la regulación externa y permite que los modelos controlen el proceso de razonamiento de manera autónoma para resolver el problema de suposiciones excesivas. Usando criterios de medición para identificar suposiciones excesivas basadas en respuestas estándar, se diseña un método sistemático para identificar razones redundantes y se identifican y se entrenan señales de entrenamiento para cuantificar precisamente las etapas innecesarias dentro del rastro de razonamiento. Con esta base, se desarrolla una estrategia completa para la construcción de datos con razones adaptativas y se introduce una estructura innovadora de \"frena\" para que el modelo aprenda a detenerse en el momento adecuado, sin necesidad de restricciones. Las experimentos en los benchmarks matemáticos (AIME, AMC, MATH500, GSM8K) muestran que este método reduce el consumo de tokens en al menos el 60% y mantiene una precisión relativamente alta sin restricciones.",
      "upvotes": 16,
      "discussionId": "682f34b62b9fdc24ae9de3be",
      "ai_summary": "A novel Self-Braking Tuning framework reduces overthinking and unnecessary computational overhead in large reasoning models by enabling the model to self-regulate its reasoning process.",
      "ai_keywords": [
        "large reasoning models",
        "self-braking tuning",
        "overthinking",
        "reasoning capabilities",
        "mathematical benchmarks",
        "adaptive reasoning lengths",
        "braking prompt mechanism"
      ]
    },
    "publishedAt": "2025-05-20T12:53:40.000Z",
    "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning",
    "summary": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14604.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e1058e9fcf41d740b69966d",
      "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
      "fullname": "Yongliang Shen",
      "name": "tricktreat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16916",
      "authors": [
        {
          "_id": "682fdcfc2c98b5e99660561e",
          "name": "Xuankun Rong",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e99660561f",
          "name": "Wenke Huang",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605620",
          "name": "Jian Liang",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605621",
          "name": "Jinhe Bi",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605622",
          "name": "Xun Xiao",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605623",
          "name": "Yiming Li",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605624",
          "name": "Bo Du",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605625",
          "name": "Mang Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:11:58.000Z",
      "submittedOnDailyAt": "2025-05-23T00:58:25.177Z",
      "title": "Post-processing de multi-model pitching sin guías externas",
      "submittedOnDailyBy": {
        "_id": "66c014820836dd7a55be3fde",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OuqAxEFq1Ny2CHbl9HOm.jpeg",
        "isPro": false,
        "fullname": "Xuankun Rong",
        "user": "XuankunRong",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje multimodal de DeepMind (MLLMs) se aplican a tareas generales utilizando conjuntos de datos proporcionados por el usuario en la configuración de FINCH (Fine-Tuning as a Service). Esta flexibilidad exige un mínimo de esfuerzo para insertar backdoors en los MLLMs, lo que aumenta los riesgos de seguridad. En este artículo, observamos que los backdoors Trigger generan una concentración atípica de atención en áreas de la red que no deberían recibir atención y destruyen el procesamiento multimodal. Basándonos en esta observación, proponemos un marco de filtrado de datos \"Believe Your Eyes (BYE)\" que utiliza patrones de entropía de atención para identificar y filtrar muestras con backdoors. BYE funciona mediante tres etapas: 1) Extracción de mapas de atención usando modelos de FINCH, 2) Cálculo de puntuaciones de entropía y profilado de capas sensibles mediante separación de modalidades, y 3) Clustering no-subjuntivo para eliminar muestras sospechosas. A diferencia de otros mecanismos de protección, BYE no requiere la limpieza de subjuntivo, etiquetas auxiliares o modificaciones del modelo. Se ha validado la eficacia de BYE en experimentos con diversos conjuntos de datos, modelos y tipos de backdoors: mantiene el rendimiento de tareas limpios mientras alcanza un porcentaje de éxito de ataque cercano a 0, proporcionando una solución generalizable y potente para mitigar el riesgo de backdoors en MLLMs.",
      "upvotes": 14,
      "discussionId": "682fdcfd2c98b5e99660567b",
      "ai_summary": "A novel defense framework, Believe Your Eyes (BYE), identifies and filters backdoor samples in fine-tuned multimodal large language models by analyzing attention entropy patterns, preventing trigger activation without requiring additional labels or model changes.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "fine-tuning-as-a-service",
        "FTaaS",
        "backdoors",
        "attention collapse",
        "attention entropy",
        "self-supervised",
        "bimodal separation",
        "unsupervised clustering",
        "clean-task performance"
      ]
    },
    "publishedAt": "2025-05-22T13:11:58.000Z",
    "title": "Backdoor Cleaning without External Guidance in MLLM Fine-tuning",
    "summary": "Multimodal Large Language Models (MLLMs) are increasingly deployed in\nfine-tuning-as-a-service (FTaaS) settings, where user-submitted datasets adapt\ngeneral-purpose models to downstream tasks. This flexibility, however,\nintroduces serious security risks, as malicious fine-tuning can implant\nbackdoors into MLLMs with minimal effort. In this paper, we observe that\nbackdoor triggers systematically disrupt cross-modal processing by causing\nabnormal attention concentration on non-semantic regions--a phenomenon we term\nattention collapse. Based on this insight, we propose Believe Your Eyes (BYE),\na data filtering framework that leverages attention entropy patterns as\nself-supervised signals to identify and filter backdoor samples. BYE operates\nvia a three-stage pipeline: (1) extracting attention maps using the fine-tuned\nmodel, (2) computing entropy scores and profiling sensitive layers via bimodal\nseparation, and (3) performing unsupervised clustering to remove suspicious\nsamples. Unlike prior defenses, BYE equires no clean supervision, auxiliary\nlabels, or model modifications. Extensive experiments across various datasets,\nmodels, and diverse trigger types validate BYE's effectiveness: it achieves\nnear-zero attack success rates while maintaining clean-task performance,\noffering a robust and generalizable solution against backdoor threats in MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16916.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c014820836dd7a55be3fde",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OuqAxEFq1Ny2CHbl9HOm.jpeg",
      "fullname": "Xuankun Rong",
      "name": "XuankunRong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14684",
      "authors": [
        {
          "_id": "682f474c9ee0bb0cc953b885",
          "name": "Haolei Xu",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b886",
          "name": "Yuchen Yan",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b887",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b888",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b889",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88a",
          "name": "Shengpei Jiang",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88b",
          "name": "Kaitao Song",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88c",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88d",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88e",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:59:31.000Z",
      "submittedOnDailyAt": "2025-05-23T03:43:43.765Z",
      "title": "Cerrar el espacio para crear un salto de pensamiento que mejore la Tuning de la Cadena de Pensamiento.",
      "submittedOnDailyBy": {
        "_id": "64098738342c26884c792c93",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
        "isPro": false,
        "fullname": "Yuchen Yan",
        "user": "yanyc",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) han logrado impresionantes avances en tareas matemáticas utilizando la lógica de CoT (Chain-of-Thought). Sin embargo, los conjuntos de datos de CoT actuales de matemáticas permiten a los expertos saltar pasos intermedios utilizando \"saltos de pensamiento\" (Thought Leaps), lo que tiene un impacto negativo en el aprendizaje y la generalización del modelo. Proponemos la tarea de CoT Thought Leap Bridge, con el objetivo de detectar automáticamente los saltos y generar etapas de lógica intermedias insuficientes para recuperar la completitud y coherencia de la CoT. Para promover esto, basamos nuestro trabajo en el conjunto de datos estructurado ScaleQuestMath y construimos un conjunto de datos de entrenamiento especializado, entrenando el CoT-Bridge. A través de experimentos detallados en marcos de referencia de lógica matemática, observamos que un modelo fino con el conjunto de datos ajustado supera a los modelos entrenados con el conjunto original, con una mejora del 5.87% en NuminaMath. Nuestro enfoque optimiza efectivamente la combinación de datos (+3.02%), proporciona un mejor inicio para la aprendizaje por refuerzo y funciona junto con los métodos de optimización existentes, utilizando módulos de plug-in y play-in. Además, el CoT-Bridge mejora la generalización en tareas de lógica matemática en dominios de perturbación y fortalece la completitud de la lógica, demostrando una amplia gama de beneficios.",
      "upvotes": 14,
      "discussionId": "682f474d9ee0bb0cc953b8c7",
      "projectPage": "https://zju-real.github.io/CoT-Bridge/",
      "githubRepo": "https://github.com/ZJU-REAL/Mind-the-Gap",
      "ai_summary": "A model for detecting and generating missing intermediate steps in mathematical Chain-of-Thought reasoning improves performance and generalization on mathematical and logical reasoning tasks.",
      "ai_keywords": [
        "Chain-of-Thought",
        "reasoning",
        "thought leaps",
        "ScaleQM+",
        "ScaleQuestMath",
        "NuminaMath",
        "distilled data",
        "reinforcement learning",
        "generalization",
        "logical reasoning tasks"
      ]
    },
    "publishedAt": "2025-05-20T13:59:31.000Z",
    "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
    "summary": "Large language models (LLMs) have achieved remarkable progress on\nmathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14684.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64098738342c26884c792c93",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
      "fullname": "Yuchen Yan",
      "name": "yanyc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16990",
      "authors": [
        {
          "_id": "682fdd034640a9db4d1cc04d",
          "name": "Runpeng Yu",
          "hidden": false
        },
        {
          "_id": "682fdd034640a9db4d1cc04e",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "682fdd034640a9db4d1cc04f",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:55:04.000Z",
      "submittedOnDailyAt": "2025-05-23T01:04:28.755Z",
      "title": "DINPL: Verificación de Modelo de Dispersión Distribuída y Modelo de Lenguaje en Paralelo",
      "submittedOnDailyBy": {
        "_id": "635364b3c41f548fe39db945",
        "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
        "isPro": false,
        "fullname": "Runpeng Yu",
        "user": "rp-yu",
        "type": "user"
      },
      "summary": "En este estudio, se propone el primer modelo de lenguaje de DMLLM (Modelo de Lenguaje de Distribución de Dipénsión) llamado Dimple. El enfoque único de entrenamiento de la distribución de diépension tiene problemas graves como instabilidad de entrenamiento, desempeño óptimo y sesgos de longitud. Para enfrentar estos problemas, se diseñó un nuevo patrón de entrenamiento que combina un primer paso automático de regresión y un segundo paso de diépension. Con este enfoque, se construyó el modelo Dimple-7B y se entrenó utilizando una pipeline de entrenamiento similar a LLaVA-NEXT. Dimple-7B demostró un desempeño superior al de LLaVA-NEXT en un 3.9%, demostrando que DMLLM puede alcanzar el mismo rendimiento que los modelos de regresión automática. Para mejorar la eficiencia de la inferencia, se propone un decoding stereotype llamado \"confident decoding\", que ajusta dinamicamente la cantidad de tokens generados en cada etapa para reducir significativamente la cantidad de iteraciones de generación. En contraste con los modelos de regresión automática, donde la cantidad de iteraciones bidireccionales es igual a la longitud de la respuesta, el confiente decoding puede reducir la cantidad de iteraciones necesarias en Dimple hasta tres veces la longitud de la respuesta. Además, se re-implementó la tecnología de predicción de los modelos de regresión automática para evitar que afecte significativamente el rendimiento en muchos test de evaluación de versión, proporcionando un aumento de velocidad del 1.5 a 7 veces. Además, se utiliza una estructura de perfil que permite controlar con precisión las respuestas de Dimple, lo que permite la generación de respuestas estructuradas basadas en comandos o en programación de cadenas, así como un control micro de la forma y longitud de las respuestas. En resumen, este estudio demuestra la posibilidad y los puntos fuertes de los DMLLM, así como la mejora en la eficiencia de la inferencia y el control de las respuestas. El código y el modelo están disponibles en https://github.com/yu-rp/Dimple.",
      "upvotes": 12,
      "discussionId": "682fdd044640a9db4d1cc0a1",
      "githubRepo": "https://github.com/yu-rp/Dimple",
      "ai_summary": "Dimple, a Discrete Diffusion Multimodal Large Language Model, achieves performance comparable to autoregressive models through a hybrid training approach and enhances inference efficiency with confident decoding and structure priors.",
      "ai_keywords": [
        "Discrete Diffusion Multimodal Large Language Model",
        "DMLLM",
        "autoregressive phase",
        "diffusion phase",
        "confident decoding",
        "prefilling technique",
        "structure priors"
      ]
    },
    "publishedAt": "2025-05-22T13:55:04.000Z",
    "title": "Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel\n  Decoding",
    "summary": "In this work, we propose Dimple, the first Discrete Diffusion Multimodal\nLarge Language Model (DMLLM). We observe that training with a purely discrete\ndiffusion approach leads to significant training instability, suboptimal\nperformance, and severe length bias issues. To address these challenges, we\ndesign a novel training paradigm that combines an initial autoregressive phase\nwith a subsequent diffusion phase. This approach yields the Dimple-7B model,\ntrained on the same dataset and using a similar training pipeline as\nLLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%,\ndemonstrating that DMLLM can achieve performance comparable to that of\nautoregressive models. To improve inference efficiency, we propose a decoding\nstrategy termed confident decoding, which dynamically adjusts the number of\ntokens generated at each step, significantly reducing the number of generation\niterations. In autoregressive models, the number of forward iterations during\ngeneration equals the response length. With confident decoding, however, the\nnumber of iterations needed by Dimple is even only text{response\nlength}{3}. We also re-implement the prefilling technique in autoregressive\nmodels and demonstrate that it does not significantly impact performance on\nmost benchmark evaluations, while offering a speedup of 1.5x to 7x.\nAdditionally, we explore Dimple's capability to precisely control its response\nusing structure priors. These priors enable structured responses in a manner\ndistinct from instruction-based or chain-of-thought prompting, and allow\nfine-grained control over response format and length, which is difficult to\nachieve in autoregressive models. Overall, this work validates the feasibility\nand advantages of DMLLM and enhances its inference efficiency and\ncontrollability. Code and models are available at\nhttps://github.com/yu-rp/Dimple.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635364b3c41f548fe39db945",
      "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
      "fullname": "Runpeng Yu",
      "name": "rp-yu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16864",
      "authors": [
        {
          "_id": "682fe14abafb480b9595da32",
          "name": "Yuechen Zhang",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da33",
          "name": "Jinbo Xing",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da34",
          "name": "Bin Xia",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da35",
          "name": "Shaoteng Liu",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da36",
          "name": "Bohao Peng",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da37",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da38",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da39",
          "name": "Eric Lo",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da3a",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T16:21:32.000Z",
      "submittedOnDailyAt": "2025-05-23T01:22:45.264Z",
      "title": "Efectivos métodos de generación de vídeos para el entrenamiento sin supervisión: método de separación dinámica de tokens",
      "submittedOnDailyBy": {
        "_id": "6418554a0956be7233a1023e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
        "isPro": false,
        "fullname": "zhang yuechen",
        "user": "julianjuaner",
        "type": "user"
      },
      "summary": "El modelo Transformer de difusión vídeo (DiT) presenta una calidad de generación sorprendente, pero su implementación práctica está estrictamente limitada por los altos requerimientos computacionales. Esta ineficiencia se basa en dos problemas principales: la complejidad cuadrática de la atención automática con respecto a la longitud de tokens y las características multi-escala de los modelos de difusión. Para resolver estos limitaciones, proponemos una innovadora arquitectura de inferencia llamada \"Jenga\". Esta arquitectura combina la separación dinámica de la atención y la generación evolutiva de la resolución. Nuestro enfoque se basa en dos principales insights: (1) en el primer paso de denoising, no es necesario tener potenciales de alta resolución, y (2) en los pasos posteriores, no es necesaria una atención densa. Jenga introduce una estructura de atención bloque-wise que selecciona la interacción entre tokens asociados de manera dinámica utilizando una curva de moldeado en el espacio 3D, y aumenta gradualmente la resolución potencial durante el proceso de generación. A través de los resultados experimentales, Jenga logra un significativo aumento de velocidad en muchos modelos de difusión vídeo recientes, manteniendo una calidad de generación relativamente alta (aumento de velocidad del 8.83 en VBench con un descenso en rendimiento del 0.01%). Como un plato de pan y mantequilla, Jenga permite a los modelos que no requieren reentrenamiento reducir drásticamente su tiempo de inferencia y facilita la generación de vídeo de alta calidad en hardware moderno. Código: https://github.com/dvlab-research/Jenga",
      "upvotes": 11,
      "discussionId": "682fe14ebafb480b9595db1c",
      "projectPage": "https://julianjuaner.github.io/projects/jenga/",
      "githubRepo": "https://github.com/dvlab-research/Jenga",
      "ai_summary": "Jenga, a novel inference pipeline for video Diffusion Transformer models, combines dynamic attention carving and progressive resolution generation to significantly speed up video generation while maintaining high quality.",
      "ai_keywords": [
        "video Diffusion Transformer (DiT)",
        "self-attention",
        "diffusion models",
        "dynamic attention carving",
        "progressive resolution generation",
        "block-wise attention",
        "3D space-filling curves"
      ]
    },
    "publishedAt": "2025-05-22T12:21:32.000Z",
    "title": "Training-Free Efficient Video Generation via Dynamic Token Carving",
    "summary": "Despite the remarkable generation quality of video Diffusion Transformer\n(DiT) models, their practical deployment is severely hindered by extensive\ncomputational requirements. This inefficiency stems from two key challenges:\nthe quadratic complexity of self-attention with respect to token length and the\nmulti-step nature of diffusion models. To address these limitations, we present\nJenga, a novel inference pipeline that combines dynamic attention carving with\nprogressive resolution generation. Our approach leverages two key insights: (1)\nearly denoising steps do not require high-resolution latents, and (2) later\nsteps do not require dense attention. Jenga introduces a block-wise attention\nmechanism that dynamically selects relevant token interactions using 3D\nspace-filling curves, alongside a progressive resolution strategy that\ngradually increases latent resolution during generation. Experimental results\ndemonstrate that Jenga achieves substantial speedups across multiple\nstate-of-the-art video diffusion models while maintaining comparable generation\nquality (8.83times speedup with 0.01\\% performance drop on VBench). As a\nplug-and-play solution, Jenga enables practical, high-quality video generation\non modern hardware by reducing inference time from minutes to seconds --\nwithout requiring model retraining. Code:\nhttps://github.com/dvlab-research/Jenga",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6418554a0956be7233a1023e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
      "fullname": "zhang yuechen",
      "name": "julianjuaner",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16181",
      "authors": [
        {
          "_id": "682fddbd2b4a4d1ce53c5afb",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Reza Taesiri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T03:18:33.876Z",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5afc",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Brandon Collins",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T02:32:10.307Z",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5afd",
          "user": {
            "_id": "668c8e8c142f9b26a49f03cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668c8e8c142f9b26a49f03cc/YNmPCrlsi6iwSeNfh1iID.png",
            "isPro": false,
            "fullname": "Logan Bolton",
            "user": "loganbolton",
            "type": "user"
          },
          "name": "Logan Bolton",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T02:33:14.731Z",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5afe",
          "name": "Viet Dac Lai",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5aff",
          "name": "Franck Dernoncourt",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5b00",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5b01",
          "name": "Anh Totti Nguyen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:35:15.000Z",
      "submittedOnDailyAt": "2025-05-23T01:00:45.248Z",
      "title": "Entendido. Aquí tienes la traducción al español:\n\n**Entender las capacidades de la inteligencia artificial generativa en el trabajo de edición de imágenes diarias**\n\n**Nota:** Aunque no se pide agregar explicaciones o textos adicionales, se ha proporcionado una versión de traducción más detallada para adaptarse a los hábitos de lectura de los lectores coreanos.",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "Los AI generados (GenAI) tienen un gran potencial para automatizar las tareas de edición de imágenes diarias, y este potencial se ha enfatizado aún más después de la publicación de GPT-4o el 25 de marzo de 2025. Sin embargo, ¿qué son los temas de edición más solicitados? ¿Qué tipos de tareas de edición (por ejemplo, eliminar o estilizar un tema) desean los usuarios? ¿Prefieren ediciones precisas y predictibles o preferen ediciones de alto nivel de creatividad? ¿Nos ayudaría comprender las características de las solicitudes reales y las ediciones adaptativas realizadas por profesionales de la fotografía libres, para mejorar los editores basados en AI y determinar qué tipos de solicitudes pueden ser procesadas exitosamente por un editor de AI? Este artículo ha analizado 83k solicitudes y 305k ediciones de PSR-wizard recopiladas en la comunidad Reddit durante los últimos 12 años (de 2013 a 2025) para estudiar estos temas de manera única. Según evaluaciones humanas, los mejores editores de AI (incluyendo GPT-4o, Gemini-2.0-Flash y SeedEdit) pueden manejar aproximadamente el 33% de las solicitudes. Interesantemente, los editores de AI realizan mejores ediciones precisas en tareas abiertas que en tareas con bajo nivel de creatividad. A menudo, los editores de AI encuentran difícil mantener la identidad de personajes y animales, lo que los hace menos solicitados. Por otro lado, los jueces de modelos de lenguaje (VLM, por ejemplo, o1) muestran un tendencia más pronunciada hacia el editor de AI. Los códigos y ejemplos de calidad se pueden encontrar en https://psrdataset.github.io.",
      "upvotes": 11,
      "discussionId": "682fddc32b4a4d1ce53c5c60",
      "projectPage": "https://psrdataset.github.io",
      "ai_summary": "Analysis of 83k image editing requests reveals that AI editors, including GPT-4o, struggle with low-creativity tasks and precise editing, while performing better on open-ended tasks, and human and VLM judges differ in their preferences for AI versus human edits.",
      "ai_keywords": [
        "GPT-4o",
        "Gemini-2.0-Flash",
        "SeedEdit",
        "VLM judges"
      ]
    },
    "publishedAt": "2025-05-21T23:35:15.000Z",
    "title": "Understanding Generative AI Capabilities in Everyday Image Editing Tasks",
    "summary": "Generative AI (GenAI) holds significant promise for automating everyday image\nediting tasks, especially following the recent release of GPT-4o on March 25,\n2025. However, what subjects do people most often want edited? What kinds of\nediting actions do they want to perform (e.g., removing or stylizing the\nsubject)? Do people prefer precise edits with predictable outcomes or highly\ncreative ones? By understanding the characteristics of real-world requests and\nthe corresponding edits made by freelance photo-editing wizards, can we draw\nlessons for improving AI-based editors and determine which types of requests\ncan currently be handled successfully by AI editors? In this paper, we present\na unique study addressing these questions by analyzing 83k requests from the\npast 12 years (2013-2025) on the Reddit community, which collected 305k\nPSR-wizard edits. According to human ratings, approximately only 33% of\nrequests can be fulfilled by the best AI editors (including GPT-4o,\nGemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on\nlow-creativity requests that require precise editing than on more open-ended\ntasks. They often struggle to preserve the identity of people and animals, and\nfrequently make non-requested touch-ups. On the other side of the table, VLM\njudges (e.g., o1) perform differently from human judges and may prefer AI edits\nmore than human edits. Code and qualitative examples are available at:\nhttps://psrdataset.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16181.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15952",
      "authors": [
        {
          "_id": "682fe833f39f561d1d8cd5d1",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Reza Taesiri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T03:18:36.072Z",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d2",
          "name": "Abhijay Ghildyal",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d3",
          "name": "Saman Zadtootaghaj",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d4",
          "name": "Nabajeet Barman",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d5",
          "user": {
            "_id": "644feede17b6189cda58575d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WVo1Ah7xmHEeBOUQpkYgS.png",
            "isPro": false,
            "fullname": "Cor-Paul",
            "user": "corpaul",
            "type": "user"
          },
          "name": "Cor-Paul Bezemer",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:15:06.040Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T19:08:38.000Z",
      "submittedOnDailyAt": "2025-05-23T01:45:28.315Z",
      "title": "VideoGameQA-Bench: Evaluación de la Garantía de Calidad del Modelo de Lenguaje Visual de los Juegos de Video",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Los videojuegos actualmente generan los ingresos más altos en el sector de la industria de entretenimiento y la optimización del proceso de desarrollo de juegos es fundamental para el crecimiento continuo de la industria. El desarrollo reciente de los modelos de lenguaje visual (VLMs) ha demostrado la posibilidad de automatizar y mejorar diferentes áreas del desarrollo de juegos, especialmente en el campo de la garantía de calidad (QA), donde las oportunidades de automatización actuales están limitadas. Para evaluar precisamente el rendimiento de VLMs en tareas de QA de juegos y evaluar su eficiencia en procesar escenarios reales, es necesario un marco de referencia estándar. Los actuales marcos de referencia no responden a las especificidades de este campo, por lo que se presenta VideoGameQA-Bench, un marco de referencia detallado que cubre una amplia gama de actividades de QA de juegos. Este marco de referencia incluye pruebas de unidad visual, pruebas de reinicio visual, tareas de nodos de la pila de ejecución, detección de sorpresas, informes de errores en imágenes y videos de los juegos, entre otras actividades de QA de juegos. Los códigos y datos están disponibles en la siguiente URL: https://asgaardlab.github.io/videogameqa-bench/",
      "upvotes": 11,
      "discussionId": "682fe83af39f561d1d8cd7e5",
      "projectPage": "https://asgaardlab.github.io/videogameqa-bench/",
      "ai_summary": "A benchmark called VideoGameQA-Bench is introduced to assess Vision-Language Models in video game quality assurance tasks.",
      "ai_keywords": [
        "Vision-Language Models",
        "VideoGameQA-Bench",
        "visual unit testing",
        "visual regression testing",
        "needle-in-a-haystack tasks",
        "glitch detection",
        "bug report generation"
      ]
    },
    "publishedAt": "2025-05-21T15:08:38.000Z",
    "title": "VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game\n  Quality Assurance",
    "summary": "With video games now generating the highest revenues in the entertainment\nindustry, optimizing game development workflows has become essential for the\nsector's sustained growth. Recent advancements in Vision-Language Models (VLMs)\noffer considerable potential to automate and enhance various aspects of game\ndevelopment, particularly Quality Assurance (QA), which remains one of the\nindustry's most labor-intensive processes with limited automation options. To\naccurately evaluate the performance of VLMs in video game QA tasks and\ndetermine their effectiveness in handling real-world scenarios, there is a\nclear need for standardized benchmarks, as existing benchmarks are insufficient\nto address the specific requirements of this domain. To bridge this gap, we\nintroduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array\nof game QA activities, including visual unit testing, visual regression\ntesting, needle-in-a-haystack tasks, glitch detection, and bug report\ngeneration for both images and videos of various games. Code and data are\navailable at: https://asgaardlab.github.io/videogameqa-bench/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 83
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17018",
      "authors": [
        {
          "_id": "682fe2b865bac3ec3556c016",
          "name": "Kaixuan Fan",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c017",
          "name": "Kaituo Feng",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c018",
          "name": "Haoming Lyu",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c019",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c01a",
          "name": "Xiangyu Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:53.000Z",
      "submittedOnDailyAt": "2025-05-23T01:28:23.338Z",
      "title": "SophiaVL-R1: Consideraciones para la recompensa para fortalecer los cálculos de modelos de comprensión de texto",
      "submittedOnDailyBy": {
        "_id": "67079840a9bcb7459b8d2a46",
        "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
        "isPro": false,
        "fullname": "Kaituo Feng",
        "user": "KaituoFeng",
        "type": "user"
      },
      "summary": "Recientemente, los avances han demostrado el éxito de la fortaleza de los modelos de lenguaje multimodal (MLLMs) en la capacidad lógica a través del aprendizaje por refuerzo basado en reglas (RL). Sin embargo, este paradigma presenta una limitación en la observación del proceso de pensamiento que conduce a la deducción de resultados finales, lo que puede limitar la capacidad de los modelos para aprender estrategias lógicas óptimas. Para complementar esto, proponemos SophiaVL-R1, un intento de agregar señales de recompensa al proceso de pensamiento en este paradigma. Para lograrlo, primero entrenamos un modelo de recompensa del pensamiento y evaluamos la calidad del proceso de pensamiento del modelo entero. La recompensa del pensamiento puede ser menos confiable para ciertos ejemplos, por lo que proponemos el método Trust-GRPO y asignamos pesos de confianza durante el entrenamiento. Estos pesos se calculan comparando la recompensa del pensamiento con respuestas correctas y incorrectas, ayudando a mitigar el impacto potencial de las recompensas del pensamiento no confiables. Además, diseñamos una estrategia de entrenamiento de relajación para reducir gradualmente la recompensa del pensamiento a medida que el tiempo pasa, lo que permite que el modelo se adapte a la mayor dependencia de las recompensas de resultados basados en reglas precisas en las fases de entrenamiento posteriores. Los resultados de las pruebas muestran que nuestro SophiaVL-R1 supera a series de modelos lógicos en diferentes benchmarks como MathVisita y MMMU, demostrando una fuerte capacidad lógica y generalización. En particular, nuestro SophiaVL-R1-7B supera a LLaVA-OneVision-72B, que tiene 10 veces más parámetros, en la capacidad lógica y generalización. Todo el código, modelos y conjuntos de datos están disponibles en https://github.com/kxfan2002/SophiaVL-R1.",
      "upvotes": 10,
      "discussionId": "682fe2b965bac3ec3556c066",
      "projectPage": "https://github.com/kxfan2002/SophiaVL-R1",
      "githubRepo": "https://github.com/kxfan2002/SophiaVL-R1",
      "ai_summary": "An enhanced multimodal language model incorporates thinking process rewards to improve reasoning and generalization, achieving superior performance on benchmarks compared to larger models.",
      "ai_keywords": [
        "multimodal large language models",
        "rule-based reinforcement learning",
        "reward signals",
        "thinking reward model",
        "Trust-GRPO method",
        "thinking reward comparison",
        "annealing training strategy",
        "reasoning MLLMs",
        "MathVisita",
        "MMMU"
      ]
    },
    "publishedAt": "2025-05-22T13:59:53.000Z",
    "title": "SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward",
    "summary": "Recent advances have shown success in eliciting strong reasoning abilities in\nmultimodal large language models (MLLMs) through rule-based reinforcement\nlearning (RL) with outcome rewards. However, this paradigm typically lacks\nsupervision over the thinking process leading to the final outcome.As a result,\nthe model may learn sub-optimal reasoning strategies, which can hinder its\ngeneralization ability. In light of this, we propose SophiaVL-R1, as an attempt\nto add reward signals for the thinking process in this paradigm. To achieve\nthis, we first train a thinking reward model that evaluates the quality of the\nentire thinking process. Given that the thinking reward may be unreliable for\ncertain samples due to reward hacking, we propose the Trust-GRPO method, which\nassigns a trustworthiness weight to the thinking reward during training. This\nweight is computed based on the thinking reward comparison of responses leading\nto correct answers versus incorrect answers, helping to mitigate the impact of\npotentially unreliable thinking rewards. Moreover, we design an annealing\ntraining strategy that gradually reduces the thinking reward over time,\nallowing the model to rely more on the accurate rule-based outcome reward in\nlater training stages. Experiments show that our SophiaVL-R1 surpasses a series\nof reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU),\ndemonstrating strong reasoning and generalization capabilities. Notably, our\nSophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite\nthe latter having 10 times more parameters. All code, models, and datasets are\nmade publicly available at https://github.com/kxfan2002/SophiaVL-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17018.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67079840a9bcb7459b8d2a46",
      "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
      "fullname": "Kaituo Feng",
      "name": "KaituoFeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17012",
      "authors": [
        {
          "_id": "682fde942f8f73559fcbc5da",
          "name": "Haoning Wu",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5db",
          "name": "Xiao Huang",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5dc",
          "name": "Yaohui Chen",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5dd",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5de",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5df",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632c7a0d1d303f5f9acf01b8/3GQcp-q-BBj5bX_DRmB6S.jpeg"
      ],
      "publishedAt": "2025-05-22T17:59:03.000Z",
      "submittedOnDailyAt": "2025-05-23T01:12:41.090Z",
      "title": "Spectral Score: Una forma de evaluar la comprensión del espectro de Damocles de manera uniforme",
      "submittedOnDailyBy": {
        "_id": "632c7a0d1d303f5f9acf01b8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
        "isPro": false,
        "fullname": "Haoning Wu",
        "user": "haoningwu",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje multimodal de difusión (MLLMs) han logrado un éxito impresionante en tareas de respuesta a consultas, pero su capacidad para comprender espacios ha sido poco investigada. Este artículo estudia principalmente los siguientes problemas: ¿tienen actualmente los MLLMs la capacidad para entender y comprender espacios 3D? Concretamente, este artículo presenta los siguientes contribuciones: (i) introducimos VGBench, un estándar especialmente diseñado para evaluar la percepción geométrica visual de los MLLMs, que incluye diversos aspectos como la estimación de la posición y movimiento de la cámara; (ii) proponemos SpatialScore, el más amplio y diverso estándar de comprensión espacial multimodal hasta el día de hoy, que integra datos de 11 datasetes existentes diferentes al VGBench; este estándar incluye 28,000 muestras y aborda diversas tareas de comprensión espacial, modalidades y formatos de preguntas y respuestas, además de un subconjunto desafiante llamado SpatialScore-Hard; (iii) desarrollamos SpatialAgent, un nuevo sistema de agente multimodal multiagente que combina 9 herramientas profesionales y soporta el patrón de inferencia Plan-Execute y ReAct; (iv) realizamos una amplia evaluación que revela los desafíos continuos en la inferencia espacial y demuestra la efectividad de SpatialAgent. Creo que SpatialScore podría convertirse en un estándar estricto para el próximo avance de los MLLMs.",
      "upvotes": 9,
      "discussionId": "682fde952f8f73559fcbc616",
      "projectPage": "https://haoningwu3639.github.io/SpatialScore/",
      "githubRepo": "https://github.com/haoningwu3639/SpatialScore/",
      "ai_summary": "SpatialScore benchmarks multimodal large language models for 3D spatial understanding, revealing challenges and showcasing the effectiveness of SpatialAgent with specialized tools.",
      "ai_keywords": [
        "Multimodal large language models",
        "MLLMs",
        "VGBench",
        "SpatialScore",
        "spatial understanding",
        "visual geometry perception",
        "camera pose",
        "motion estimation",
        "multi-agent system",
        "SpatialAgent",
        "Plan-Execute",
        "ReAct reasoning paradigms"
      ]
    },
    "publishedAt": "2025-05-22T13:59:03.000Z",
    "title": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial\n  Understanding",
    "summary": "Multimodal large language models (MLLMs) have achieved impressive success in\nquestion-answering tasks, yet their capabilities for spatial understanding are\nless explored. This work investigates a critical question: do existing MLLMs\npossess 3D spatial perception and understanding abilities? Concretely, we make\nthe following contributions in this paper: (i) we introduce VGBench, a\nbenchmark specifically designed to assess MLLMs for visual geometry perception,\ne.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most\ncomprehensive and diverse multimodal spatial understanding benchmark to date,\nintegrating VGBench with relevant data from the other 11 existing datasets.\nThis benchmark comprises 28K samples across various spatial understanding\ntasks, modalities, and QA formats, along with a carefully curated challenging\nsubset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent\nsystem incorporating 9 specialized tools for spatial understanding, supporting\nboth Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive\nevaluations to reveal persistent challenges in spatial reasoning while\ndemonstrating the effectiveness of SpatialAgent. We believe SpatialScore will\noffer valuable insights and serve as a rigorous benchmark for the next\nevolution of MLLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632c7a0d1d303f5f9acf01b8/3GQcp-q-BBj5bX_DRmB6S.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632c7a0d1d303f5f9acf01b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
      "fullname": "Haoning Wu",
      "name": "haoningwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16839",
      "authors": [
        {
          "_id": "682fd5758d2fd6fc7cd5c9f7",
          "name": "Shufan Li",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9f8",
          "name": "Konstantinos Kallidromitis",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9f9",
          "name": "Hritik Bansal",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fa",
          "name": "Akash Gokul",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fb",
          "name": "Yusuke Kato",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fc",
          "name": "Kazuki Kozuka",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fd",
          "name": "Jason Kuen",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fe",
          "name": "Zhe Lin",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9ff",
          "name": "Kai-Wei Chang",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5ca00",
          "name": "Aditya Grover",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6310531914aa81e1044363ed/85JcNXnpZ6f0XvO4vZ5_s.gif"
      ],
      "publishedAt": "2025-05-22T16:07:12.000Z",
      "submittedOnDailyAt": "2025-05-23T00:30:19.437Z",
      "title": "LaViDa: Modelo de lenguaje de difusión grande para la comprensión multimodal",
      "submittedOnDailyBy": {
        "_id": "6310531914aa81e1044363ed",
        "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
        "isPro": false,
        "fullname": "Shufan Li",
        "user": "jacklishufan",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje similar al mundo actual (VLMs) pueden resolver varias tareas. En escenarios muy populares, los VLMs necesitan características como inferencia rápida y control de la generación (por ejemplo, limitar la salida a un formato específico). Sin embargo, modelos VLMs de recuperación automática (AR) como LLaVA tienen dificultades en estos aspectos. Modelos de descripción discreta (DMs) permiten la interpretación paralela, lo que facilita la inferencia rápida, y utilizan el contexto para controlar la generación. Aunque DMs son efectivos solo en entornos de lenguaje, su capacidad para tareas multifásicas ha sido poco investigada. Presentamos una familia de VLMs basada en DMs. Agregamos un encoder visual a los DMs y combinamos la parte combinada con un ciclo de comandos para construir LaViDa. Para enfrentar estos desafíos, LaViDa introduce nuevas tecnologías como mascaras de interpolación, caché de prefijo KV y desplazamiento temporal. Los experimentos muestran que LaViDa presenta un excelente desempeño en benchmarks como MMMU comparado con VLMs de AR, y ofrece las ventajas únicas de los DMs. El código y los modelos están disponibles en la versión de cámara.",
      "upvotes": 9,
      "discussionId": "682fd5768d2fd6fc7cd5ca3c",
      "projectPage": " https://homepage.jackli.org/projects/lavida/index.html",
      "githubRepo": "https://github.com/jacklishufan/LaViDa",
      "ai_summary": "LaViDa, a family of vision-language models built on discrete diffusion models, offers competitive performance on multimodal benchmarks with advantages in speed, controllability, and bidirectional reasoning.",
      "ai_keywords": [
        "autoregressive (AR) VLMs",
        "discrete diffusion models (DMs)",
        "parallel decoding",
        "bidirectional context",
        "text-infilling",
        "multimodal instruction following",
        "complementary masking",
        "prefix KV cache",
        "timestep shifting",
        "MMMU",
        "COCO captioning",
        "Constrained Poem Completion",
        "Open-LLaVa-Next-8B",
        "CIDEr"
      ]
    },
    "publishedAt": "2025-05-22T12:07:12.000Z",
    "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
    "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6310531914aa81e1044363ed/85JcNXnpZ6f0XvO4vZ5_s.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16839.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310531914aa81e1044363ed",
      "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
      "fullname": "Shufan Li",
      "name": "jacklishufan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14625",
      "authors": [
        {
          "_id": "682fdb318df2d5446a1cf30b",
          "name": "Zhangchen Xu",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30c",
          "name": "Yuetai Li",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30d",
          "name": "Fengqing Jiang",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30e",
          "name": "Bhaskar Ramasubramanian",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30f",
          "name": "Luyao Niu",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf310",
          "name": "Bill Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf311",
          "name": "Radha Poovendran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:16:44.000Z",
      "submittedOnDailyAt": "2025-05-23T00:50:16.785Z",
      "title": "TinyV: La reducción del overfitting en la verificación contribuye al mejoramiento de la RL en los LLM.",
      "submittedOnDailyBy": {
        "_id": "653df1323479e9ebbe3eb6cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
        "isPro": true,
        "fullname": "Zhangchen Xu",
        "user": "zhangchenxu",
        "type": "user"
      },
      "summary": "El aprendizaje por refuerzo (RL) ha adquirido una posición dominante como una potente herramienta para mejorar la capacidad lógica de grandes modelos de lenguaje (LLMs) utilizando señales de recompensa. Sin embargo, el éxito del RL depende de la confianza en las señales de recompensa, lo cual se provee por los verificadores. En este artículo, se exponen y analizan los problemas amplios de los \"falsos negativos\" (false negatives) en que los verificadores rechazan respuestas correctas del modelo. Un estudio detallado sobre el conjunto de datos Big-Math-RL-Verified revela que más del 38% de las respuestas generadas por el modelo se encuentran ante el rechazo de los verificadores, que no pueden reconocer la respuesta correcta. Esta \"falsa negación\" se demuestra tanto experimentalmente como teóricamente, y se muestra que esta señaliza un gradiente informativo que se roba al modelo, deteriorando su entrenamiento y retrasando la convergencia. Para mitigar esto, se propone un verificador basado en LLM llamado tinyV. Este verificador fortalece los métodos basados en reglas actuales, reconoce dinamicamente los posibles \"falsos negativos\" y recupera respuestas correctas, permitiendo una estimación más precisa de la recompensa. La integración de TinyV en varios marcadores de lógica matemática ha aumentado la tasa de aceptación en más del 10% y acelerado la convergencia. Nuestro hallazgo subraya la importancia de resolver los \"falsos negativos\" de los verificadores y proporciona una aproximación útil para mejorar la fine-tuning basado en RL de los LLMs. Nuestro código está disponible en https://github.com/uw-nsl/TinyV.",
      "upvotes": 9,
      "discussionId": "682fdb328df2d5446a1cf377",
      "githubRepo": "https://github.com/uw-nsl/TinyV",
      "ai_summary": "TinyV, a lightweight LLM-based verifier, improves RL training of large language models by addressing false negatives from existing rule-based verifiers, enhancing reward accuracy and convergence speed.",
      "ai_keywords": [
        "Reinforcement Learning",
        "large language models",
        "policies",
        "reward signals",
        "verifiers",
        "false negatives",
        "Big-Math-RL-Verified dataset",
        "gradient signals",
        "convergence",
        "TinyV",
        "rule-based methods",
        "math-reasoning benchmarks",
        "pass rates"
      ]
    },
    "publishedAt": "2025-05-20T13:16:44.000Z",
    "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning",
    "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "zhangchenxu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16151",
      "authors": [
        {
          "_id": "682fd61601208348fffaa62e",
          "name": "Hongchen Wei",
          "hidden": false
        },
        {
          "_id": "682fd61601208348fffaa62f",
          "name": "Zhenzhong Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T02:51:12.000Z",
      "submittedOnDailyAt": "2025-05-23T00:34:39.659Z",
      "title": "Teoría de la inteligencia no entrenada y la función de retroalimentación en MLLM",
      "submittedOnDailyBy": {
        "_id": "63f96e99ade090bc87bc2f81",
        "avatarUrl": "/avatars/0dd0807e5b2cec011e97c8d6a3c61bae.svg",
        "isPro": false,
        "fullname": "hcwei",
        "user": "hcwei",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los Reasoning LLMs (como DeepSeek-R1 y OpenAI-o1) ha demostrado una capacidad de inferencia atractiva basada en aprendizaje reforzado. Sin embargo, extender estas capacidades a los Multimodal LLMs (MLLMs) ha sido difícil debido a los altos costos de retrenamiento y la escasez de conjuntos de datos multimodales de alta calidad y verificables. En este artículo, presentamos el modelo FRANK. Este modelo ofrece inferencia y capacidades de reflexión sin necesidad de actualizaciones ∇ ni adicionales de estratería, y es un MLLM análogo a r1 pero con costos de entrenamiento ceros. Nuestra principal idea es separar la inferencia y la justificación en las capas decodificadoras del MLLM. En particular, observamos que las capas decodificadoras superficiales distribuyen mucha atención sobre los tokens visuales, mientras que las capas profundas se centran en el significado literal. Esta observación proporciona una estrategia gradual para combinar los pesos de un MLLM con un modelo especializado en justificaciones, promoviendo la integración de la capacidad de justificación en las capas profundas y la preservación de la visión en las capas superficiales mediante una estructura de fusión de tipo \"tiro cerrado\" por capas. Los experimentos en difíciles benchmarks de inferencia multimodal muestran los efectos de nuestro enfoque. En el benchmark MMMU, nuestro modelo FRANK-38B alcanza una precisión de 69.2, superando significativamente a un modelo de referencia fuerte como InternVL2.5-38B en +5.3 y también a los modelos de perfil como el GPT-4o. Para más información, consulte nuestro sitio web de proyecto en la siguiente URL: http://iip.whu.edu.cn/frank/index.html",
      "upvotes": 6,
      "discussionId": "682fd61701208348fffaa654",
      "ai_summary": "The FRANK Model enhances multimodal LLMs with reasoning and reflection abilities without retraining, using a hierarchical weight merging approach that merges visual-pretrained and reasoning-specialized models.",
      "ai_keywords": [
        "Reasoning LLMs",
        "Multimodal LLMs (MLLMs)",
        "FRANK Model",
        "reinforcement learning",
        "multimodal reasoning datasets",
        "hierarchical weight merging",
        "Taylor-derived closed-form fusion mechanism",
        "MMMU benchmark",
        "visual tokens",
        "textual semantics",
        "deep decoder layers",
        "shallow decoder layers"
      ]
    },
    "publishedAt": "2025-05-21T22:51:12.000Z",
    "title": "Training-Free Reasoning and Reflection in MLLMs",
    "summary": "Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have\nshowcased impressive reasoning capabilities via reinforcement learning.\nHowever, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by\nthe prohibitive costs of retraining and the scarcity of high-quality,\nverifiable multimodal reasoning datasets. This paper introduces FRANK Model, a\ntraining-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning\nand reflection abilities, without any gradient updates or extra supervision.\nOur key insight is to decouple perception and reasoning across MLLM decoder\nlayers. Specifically, we observe that compared to the deeper decoder layers,\nthe shallow decoder layers allocate more attention to visual tokens, while the\ndeeper decoder layers concentrate on textual semantics. This observation\nmotivates a hierarchical weight merging approach that combines a\nvisual-pretrained MLLM with a reasoning-specialized LLM. To this end, we\npropose a layer-wise, Taylor-derived closed-form fusion mechanism that\nintegrates reasoning capacity into deep decoder layers while preserving visual\ngrounding in shallow decoder layers. Extensive experiments on challenging\nmultimodal reasoning benchmarks demonstrate the effectiveness of our approach.\nOn the MMMU benchmark, our model FRANK-38B achieves an accuracy of 69.2,\noutperforming the strongest baseline InternVL2.5-38B by +5.3, and even\nsurpasses the proprietary GPT-4o model. Our project homepage is at:\nhttp://iip.whu.edu.cn/frank/index.html",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16151.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63f96e99ade090bc87bc2f81",
      "avatarUrl": "/avatars/0dd0807e5b2cec011e97c8d6a3c61bae.svg",
      "fullname": "hcwei",
      "name": "hcwei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15879",
      "authors": [
        {
          "_id": "682fecc3fd3719dbe6fbb84b",
          "name": "Yue Fan",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84c",
          "name": "Xuehai He",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84d",
          "name": "Diji Yang",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84e",
          "name": "Kaizhi Zheng",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84f",
          "name": "Ching-Chen Kuo",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb850",
          "name": "Yuting Zheng",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb851",
          "name": "Sravana Jyothi Narayanaraju",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb852",
          "name": "Xinze Guan",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb853",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:54:49.000Z",
      "submittedOnDailyAt": "2025-05-23T02:05:02.804Z",
      "title": "GRIT: Método de dirección de MLLM que se imagina con imágenes",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "Recientes estudios han demostrado la eficacia de la construcción de modelos explicativos utilizando aprendizaje por refuerzo (RL). Sin embargo, el desarrollo de mecanismos para explicar tareas de lenguaje visual está en curso, y los modelos de razonamiento visual abiertos de código actual generalmente generan contenidos de razonamiento en un simple lenguaje natural, lo que limita la integración explícita de información visual. Esto restringe la capacidad de generar una secuencia clara de razonamientos. En respuesta a esto, proponemos la Introducción de una base de razonamiento básico (GRIT) que utiliza imágenes y texto. GRIT proporciona un nuevo enfoque para el entrenamiento de modelos MLLM que piensan con imágenes. GRIT introduce un paradigma básico de razonamiento y se enfoca en que el modelo genere secuencias de razonamientos que cruzan lenguaje natural y coordenadas de caja delimitadora, que especifican las regiones de la imagen que el modelo referencia durante el proceso de razonamiento. Además, GRIT utiliza un enfoque de aprendizaje por refuerzo basado en el algoritmo GRPO, conocido como GRPO-GR. GRPO-GR utiliza una fuerte recompensa que se centra en la precisión de la respuesta final y en el formato de salida de la razonamiento básico, lo que permite probar la secuencia de razonamientos sin necesidad de etiquetas de caja delimitadora explícitas. Con esta base, GRIT logra una alta eficiencia en la utilización de datos, necesitando solo 20 tuplas de imágenes-preguntas-respuestas en los conjuntos de datos actuales. Las evaluaciones detalladas muestran que GRIT entrena eficazmente a los modelos MLLM para generar secuencias de razonamientos y integra exitosamente la capacidad de razonamiento y la razonamiento básico.",
      "upvotes": 6,
      "discussionId": "682fecc4fd3719dbe6fbb8ac",
      "projectPage": "https://grounded-reasoning.github.io",
      "githubRepo": "https://github.com/eric-ai-lab/GRIT",
      "ai_summary": "A novel method called GRIT enhances visual reasoning in MLLMs by generating reasoning chains that integrate both natural language and bounding box coordinates, guided by a reinforcement learning approach for high data efficiency.",
      "ai_keywords": [
        "Reinforcement Learning",
        "RL",
        "MLLMs",
        "reasoning chains",
        "interleave natural language",
        "bounding box coordinates",
        "GRPO-GR",
        "GRPO",
        "grounded reasoning output",
        "reasoning and grounding abilities"
      ]
    },
    "publishedAt": "2025-05-21T13:54:49.000Z",
    "title": "GRIT: Teaching MLLMs to Think with Images",
    "summary": "Recent studies have demonstrated the efficacy of using Reinforcement Learning\n(RL) in building reasoning models that articulate chains of thoughts prior to\nproducing final answers. However, despite ongoing advances that aim at enabling\nreasoning for vision-language tasks, existing open-source visual reasoning\nmodels typically generate reasoning content with pure natural language, lacking\nexplicit integration of visual information. This limits their ability to\nproduce clearly articulated and visually grounded reasoning chains. To this\nend, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method\nfor training MLLMs to think with images. GRIT introduces a grounded reasoning\nparadigm, in which models generate reasoning chains that interleave natural\nlanguage and explicit bounding box coordinates. These coordinates point to\nregions of the input image that the model consults during its reasoning\nprocess. Additionally, GRIT is equipped with a reinforcement learning approach,\nGRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused\non the final answer accuracy and format of the grounded reasoning output, which\neliminates the need for data with reasoning chain annotations or explicit\nbounding box labels. As a result, GRIT achieves exceptional data efficiency,\nrequiring as few as 20 image-question-answer triplets from existing datasets.\nComprehensive evaluations demonstrate that GRIT effectively trains MLLMs to\nproduce coherent and visually grounded reasoning chains, showing a successful\nunification of reasoning and grounding abilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16944",
      "authors": [
        {
          "_id": "683023848d2fd6fc7ce9b99a",
          "name": "Yunjia Qi",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99b",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99c",
          "name": "Xiaozhi Wang",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99d",
          "name": "Amy Xin",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99e",
          "name": "Youfeng Liu",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99f",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b9a0",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b9a1",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:31:10.000Z",
      "submittedOnDailyAt": "2025-05-23T05:58:53.468Z",
      "title": "AGENTIF: Scenario de prueba de cumplimiento de instrucciones para modelos de lenguaje de gran escala",
      "submittedOnDailyBy": {
        "_id": "6556cf2cee35f7d8bcf13bb3",
        "avatarUrl": "/avatars/0a945054cb4732c2ee7a5502c42628bf.svg",
        "isPro": false,
        "fullname": "Qi Yunjia",
        "user": "Kikkk",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) han demostrado altas capacidades para el desarrollo de sistemas de agentes efectivos y realistas. El progreso de la investigación busca desarrollar agentes basados en LLMs que cumplan con las necesidades disponibles, pero se han presentado nuevos desafíos. Los escenarios de agentes incluyen sistemas de comandos largos y restricciones complejas, así como especificaciones detalladas de herramientas. La adecuación a las instrucciones es crucial en el uso de los sistemas de agentes, pero la investigación sobre la capacidad de los LLMs para seguir estas instrucciones de manera confiable es insuficiente. En este artículo, se presenta \"AgentIF\", el primer benchmark para evaluar de manera sistemática la capacidad de seguimiento de instrucciones de los LLMs en escenarios de agentes. AgentIF cuenta con tres características principales: 1. Se ha construido a partir de 50 sistemas de agentes reales. 2. Los comandos son largos, con una longitud media de más de 1,723 caracteres y un máximo de 15,630 caracteres. 3. Son complejos, con un promedio de 11.9 restricciones por instrucción y varios tipos de restricciones, como especificaciones de herramientas y condiciones. Para la construcción de AgentIF, se recopilaron 707 instrucciones anotadas por humanos de 50 tareas de agentes. Se anotaron las restricciones relacionadas con cada instrucción y se incluyeron criterios de evaluación que incluyen evaluaciones basadas en código, basadas en LLMs y híbridas de código-LLMs. Usando AgentIF, se evaluaron sistemáticamente los avances en los LLMs actuales. Los modelos actuales muestran un rendimiento bajo, especialmente en el tratamiento de estructuras de restricciones complejas y especificaciones de herramientas. Además, se realizaron análisis de errores y experimentos analíticos sobre la longitud de los comandos y las restricciones meta, proporcionando descubrimientos sobre los modos de fallo de los LLMs actuales. El objetivo es publicar el código y los datos para apoyar futuras investigaciones.",
      "upvotes": 5,
      "discussionId": "683023858d2fd6fc7ce9b9e4",
      "githubRepo": "https://github.com/THU-KEG/AgentIF",
      "ai_summary": "A new benchmark, AgentIF, evaluates Large Language Models' ability to follow complex instructions in realistic agentic scenarios, revealing performance limitations in handling constraints and tool specifications.",
      "ai_keywords": [
        "AgentIF",
        "Large Language Models (LLMs)",
        "agentic applications",
        "system prompts",
        "tool specifications",
        "instruction following",
        "constraint structures",
        "error analysis"
      ]
    },
    "publishedAt": "2025-05-22T13:31:10.000Z",
    "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in\n  Agentic Scenarios",
    "summary": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16944.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6556cf2cee35f7d8bcf13bb3",
      "avatarUrl": "/avatars/0a945054cb4732c2ee7a5502c42628bf.svg",
      "fullname": "Qi Yunjia",
      "name": "Kikkk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16854",
      "authors": [
        {
          "_id": "682fdd8691757629e1d58e16",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "682fdd8691757629e1d58e17",
          "name": "Kevin Qinghong Lin",
          "hidden": false
        },
        {
          "_id": "682fdd8691757629e1d58e18",
          "name": "James Cheng",
          "hidden": false
        },
        {
          "_id": "682fdd8691757629e1d58e19",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T16:13:29.000Z",
      "submittedOnDailyAt": "2025-05-23T01:09:25.197Z",
      "title": "Modelo Visuo-lingüístico utilizando el aprendizaje Renos para la Teoría de Razones Selectivas",
      "submittedOnDailyBy": {
        "_id": "64440be5af034cdfd69ca3a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
        "isPro": false,
        "fullname": "Qinghong (Kevin) Lin",
        "user": "KevinQHLin",
        "type": "user"
      },
      "summary": "La enseñanza por refuerzo (RL) ha demostrado ser una estrategia efectiva para mejorar la lógica de los modelos de lenguaje visual (VLMs). La Grupo Relatively Optimal (GRPO) es un método reciente y importante que promueve la generación completa de trazas lógicas antes de que el modelo responda, con el objetivo de optimizar la eficiencia de la lógica en términos de cantidad de tokens y costos de cálculo. Se ha investigado cómo modelar el proceso de pensamiento humano, aprendiendo a evitar la lógica en problemas simples y pensar con cuidado solo cuando sea necesario. Se ha buscado también cómo los VLMs puedan decidir desde el principio si es necesaria la lógica. Para lograr esto, se propone TON (Think or Not), que es una estrategia de entrenamiento en dos etapas: (i) una etapa de fine-tuning normalizado (SFT) que incluye la operación 'thought dropout', introduciendo un formato think-or-not para que las trazas lógicas puedan ser reemplazadas por pensamientos aleatorios, proporcionando un enfriamiento selectivo de la lógica, y (ii) en la etapa de GRPO, se decide si el modelo puede pensar libremente y se busca maximizar las recompensas de resultados relacionados con la tarea. Los resultados de los experimentos muestran que TON puede reducir en un 90% el tiempo de completación frente a GRPO, mejorando la eficiencia sin perder la calidad. Mediante evaluaciones en diferentes niveles de dificultad de tareas visuales lingüísticas, se ha confirmado experiencialmente que los modelos aprenden a saltar gradualmente las etapas lógicas innecesarias durante el entrenamiento. Estos hallazgos iluminan el camino hacia un enfoque de aprendizaje por refuerzo que orienta los patrones lógicos de manera similar a la humana. El código está disponible en https://github.com/kokolerk/TON.",
      "upvotes": 5,
      "discussionId": "682fdd8791757629e1d58e77",
      "projectPage": "https://huggingface.co/collections/kolerk/ton-682ad9038395c21e228a645b",
      "githubRepo": "https://github.com/kokolerk/TON",
      "ai_summary": "TON, a two-stage training strategy combining supervised fine-tuning with thought dropout and Group Relative Policy Optimization, reduces unnecessary reasoning steps in vision-language models without sacrificing performance.",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "vision-language models (VLMs)",
        "Group Relative Policy Optimization (GRPO)",
        "thought dropout",
        "selective reasoning"
      ]
    },
    "publishedAt": "2025-05-22T12:13:29.000Z",
    "title": "Think or Not? Selective Reasoning via Reinforcement Learning for\n  Vision-Language Models",
    "summary": "Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across diverse vision-language tasks-covering\na range of reasoning difficulties under both 3B and 7B models-consistently\nreveal that the model progressively learns to bypass unnecessary reasoning\nsteps as training advances. These findings shed light on the path toward\nhuman-like reasoning patterns in reinforcement learning approaches. Our code is\navailable at https://github.com/kokolerk/TON.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16854.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64440be5af034cdfd69ca3a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
      "fullname": "Qinghong (Kevin) Lin",
      "name": "KevinQHLin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 31
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16400",
      "authors": [
        {
          "_id": "682fe6644640a9db4d1f31d9",
          "name": "Yang Chen",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31da",
          "user": {
            "_id": "67d75b0117c2acac528f47b6",
            "avatarUrl": "/avatars/619aacd1a619aab64de3499ac3ee2229.svg",
            "isPro": false,
            "fullname": "Zhuolin Yang",
            "user": "zhuoliny",
            "type": "user"
          },
          "name": "Zhuolin Yang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:07:17.257Z",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31db",
          "name": "Zihan Liu",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31dc",
          "name": "Chankyu Lee",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31dd",
          "name": "Peng Xu",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31de",
          "name": "Mohammad Shoeybi",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31df",
          "name": "Bryan Catanzaro",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31e0",
          "user": {
            "_id": "663ee43bfeeb49803537da98",
            "avatarUrl": "/avatars/17c3e9c435cc36fb04b4589e6176a243.svg",
            "isPro": false,
            "fullname": "Wei Ping",
            "user": "wping",
            "type": "user"
          },
          "name": "Wei Ping",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:07:17.257Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T08:50:47.000Z",
      "submittedOnDailyAt": "2025-05-23T01:38:02.331Z",
      "title": "AceReason-Nemotron: Entendimiento de la Matemática y Código a través del Aprendizaje Renor",
      "submittedOnDailyBy": {
        "_id": "62bc9d90e81dfd65cced9316",
        "avatarUrl": "/avatars/05df14cd1fdbc7d6a80d2960a05a94f0.svg",
        "isPro": false,
        "fullname": "Yang Chen",
        "user": "ychenNLP",
        "type": "user"
      },
      "summary": "En el desarrollo de modelos lógicos para el aprendizaje reforzado (RL) de gran escala reciente, la detección de entrenamiento aún es incierta. Los detalles de implementación de los modelos líderes, como el DeepSeek-R1, su recuperación de datos y la detección de entrenamiento RL, generalmente son omitidos. Además, recientes estudios han demostrado que la traducción es más efectiva que el RL para pequeños modelos. En este trabajo, se muestra que el RL de gran escala puede significativamente mejorar la capacidad lógica de pequeños o modelos de tamaño intermedio, y que puede lograr resultados mejores que los modelos basados en traducción más avanzados. Se construye el proceso de entrenamiento de RL, se realizan pruebas de dispersión y se propone un enfoque sencillo y efectivo. Se entrena primero con un perro que solo tiene matemáticas, y luego con un perro que solo tiene código. En particular, el RL con matemáticas solo muestra que puede mejorar significativamente los marcadores de matemáticas de modelos fuertes de traducción (por ejemplo, +14.6%/+17.2% en el AIME 2025 para modelos de 7B/14B) y las tareas de lógica de código (por ejemplo, +6.8%/+5.8% en LiveCodeBench para modelos de 7B/14B). Además, el entrenamiento de RL con código extendido muestra que puede mejorar el rendimiento de los marcadores de código sin necesidad de resultados matemáticos o con desacatos. Se desarrolla un potente pipeline de recuperación de datos para recopilar difíciles perros que incluyen respuestas de alta calidad y casos de prueba, lo que permite probar el RL basado en pruebas y permite la detección de RL en ambos campos. Finalmente, se establece el retroalimento de las principales experimentaciones y se incluye el efecto de la mejora del aprendizaje de la cadena de respuestas creciente y la estabilización del actualizamiento de parámetros en línea. El RL desarrolla las capacidades lógicas básicas obtenidas durante el aprendizaje previo y la normalización (por ejemplo, la traducción), supera las limitaciones de la capacidad lógica del modelo y puede resolver problemas que anteriormente no habían sido resueltos.",
      "upvotes": 5,
      "discussionId": "682fe6654640a9db4d1f3229",
      "ai_summary": "Large-scale reinforcement learning enhances reasoning capabilities in small and mid-sized models more effectively than distillation, achieving superior results in both math and code benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "DeepSeek-R1",
        "data curation",
        "distillation",
        "math-only prompts",
        "code-only prompts",
        "AIME 2025",
        "LiveCodeBench",
        "curriculum learning",
        "on-policy parameter updates"
      ]
    },
    "publishedAt": "2025-05-22T04:50:47.000Z",
    "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through\n  Reinforcement Learning",
    "summary": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16400.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bc9d90e81dfd65cced9316",
      "avatarUrl": "/avatars/05df14cd1fdbc7d6a80d2960a05a94f0.svg",
      "fullname": "Yang Chen",
      "name": "ychenNLP",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16192",
      "authors": [
        {
          "_id": "68302db3b73ef22aebdce9c2",
          "name": "Chaoya Jiang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c3",
          "name": "Yongrui Heng",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c4",
          "name": "Wei Ye",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c5",
          "name": "Han Yang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c6",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c7",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c8",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c9",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9ca",
          "name": "Shikun Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:50:13.000Z",
      "submittedOnDailyAt": "2025-05-23T06:42:34.763Z",
      "title": "VLM-R^3: Reconocimiento de áreas, teoría y mejora a través de la expansión de múltiples modelos continuos.",
      "submittedOnDailyBy": {
        "_id": "645b10e80c73ea27d13f7aca",
        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
        "isPro": false,
        "fullname": "xuhaiyang",
        "user": "xhyandwyy",
        "type": "user"
      },
      "summary": "Recientemente, los modelos de lenguaje y visión basados en razonamiento (MLLM) han logrado un cierto nivel de éxito en la generación de largas oraciones. Sin embargo, en tareas complejas, es necesario procesar de manera dinámica los áreas visuales de las imágenes y reanalizarlas, y ajustar precisamente la razón a la contexto, lo cual es difícil con evidencias visuales. Presentamos el marco de trabajo VLM-R^3 (Visual Language Model with Region Recognition and Reasoning) para modelos de lenguaje visual, que aborda el reconocimiento de regiones y la razón. Este marco proporciona a los MLLM las siguientes capacidades: (i) determinar si se necesita más evidencia visual, (ii) decidir en qué parte de la imagen se debe aplicar la razón, y (iii) conectar los contenidos de las subimágenes relacionadas en una secuencia de razones continuas. El esencial de nuestro método es la selección de regiones informativas, la configuración de transformaciones adecuadas (por ejemplo, extracción, expansión) y la optimización de políticas de fortalecimiento basadas en condiciones de regiones (R-GRPO) para integrar estos resultados en las etapas posteriores de razonamiento. Para iniciar estas políticas, hemos construido un corpus VLIR (Visual Language Interactive Reasoning) basado en directorios y al nivel de etapas del contexto de razonamiento, que incluye interacciones visuales interactivas. Este corpus, extendiendo pruebas como MathVista, ScienceQA y otros benchmarks, ha permitido que VLM-R^3 establezca un nuevo nivel de rendimiento en 0 shot y con configuraciones de características, especialmente demostrando grandes avances en problemas que requieren razones espaciales complejas y detalles visuales precisos.",
      "upvotes": 4,
      "discussionId": "68302db5b73ef22aebdcea32",
      "ai_summary": "VLM-R3 enhances multi-modal language models with region recognition and reasoning, achieving state-of-the-art performance on visual question answering tasks through region-conditioned reinforcement policy optimization.",
      "ai_keywords": [
        "VLM-R3",
        "Region-Conditioned Reinforcement Policy Optimization (R-GRPO)",
        "Visuo-Lingual Interleaved Rationale (VLIR) corpus",
        "MathVista",
        "ScienceQA"
      ]
    },
    "publishedAt": "2025-05-21T23:50:13.000Z",
    "title": "VLM-R^3: Region Recognition, Reasoning, and Refinement for Enhanced\n  Multimodal Chain-of-Thought",
    "summary": "Recently, reasoning-based MLLMs have achieved a degree of success in\ngenerating long-form textual reasoning chains. However, they still struggle\nwith complex tasks that necessitate dynamic and iterative focusing on and\nrevisiting of visual regions to achieve precise grounding of textual reasoning\nin visual evidence. We introduce VLM-R^3 (Visual\nLanguage Model with Region Recognition and\nReasoning), a framework that equips an MLLM with the ability to (i)\ndecide when additional visual evidence is needed, (ii) determine\nwhere to ground within the image, and (iii) seamlessly weave the\nrelevant sub-image content back into an interleaved chain-of-thought. The core\nof our method is Region-Conditioned Reinforcement Policy Optimization\n(R-GRPO), a training paradigm that rewards the model for selecting informative\nregions, formulating appropriate transformations (e.g.\\ crop, zoom), and\nintegrating the resulting visual context into subsequent reasoning steps. To\nbootstrap this policy, we compile a modest but carefully curated Visuo-Lingual\nInterleaved Rationale (VLIR) corpus that provides step-level supervision on\nregion selection and textual justification. Extensive experiments on MathVista,\nScienceQA, and other benchmarks show that VLM-R^3 sets a new state of the art\nin zero-shot and few-shot settings, with the largest gains appearing on\nquestions demanding subtle spatial reasoning or fine-grained visual cue\nextraction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16192.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15963",
      "authors": [
        {
          "_id": "682fdcc0087ea62f1663df96",
          "name": "Shujun Liu",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df97",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df98",
          "name": "Zejun Li",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df99",
          "name": "Jianxiang Wang",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df9a",
          "name": "Cheng Zeng",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df9b",
          "name": "Zhongyu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T19:26:09.000Z",
      "submittedOnDailyAt": "2025-05-23T00:57:50.603Z",
      "title": "OViP: Lenguaje de Visión Online de Preferencias de Aprendizaje",
      "submittedOnDailyBy": {
        "_id": "6534c1fc23e0af0e0d7e8ebd",
        "avatarUrl": "/avatars/d65237d82aea19328f28b5a0cc93b271.svg",
        "isPro": false,
        "fullname": "Siyuan Wang",
        "user": "Siyuanyuan",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje visuolingüísticos (LVLMs) son vulnerables al hallazgo y a la generación de contenido que no coincide con la entrada visual. Los últimos enfoques proponen la optimización preferencial directa (DPO) para mitigar el hallazgo, pero frecuentemente utilizan muestras negativas predefinidas o editadas aleatoriamente, lo que no refleja efectivamente los errores del modelo y limita la eficacia del entrenamiento. En este estudio, se propone un marco de entrenamiento de preferencia visuolingüística en línea (OViP) que construye datos de entrenamiento dinámicos basados en las salidas que el modelo ha hallado. Identificando las diferencias significativas entre pares de respuestas y utilizando modelos de difusión para sintetizar imágenes negativas, OViP genera señales de subobjetos relacionadas en tiempo real. Esta forma de entrenamiento activa permite ajustar de manera adaptativa el lenguaje y la preferencia visual. Además, se mejora los protocolos de evaluación para mejorar la comprensión del hallazgo y el trade-off entre expresividad y hallazgo. En experimentos de hallazgo y de benchmark generales, OViP muestra su efectividad en reducir el hallazgo mientras mantiene la capacidad de diversidad esencial.",
      "upvotes": 4,
      "discussionId": "682fdcc1087ea62f1663dfcb",
      "ai_summary": "OViP dynamically generates contrastive training data using a diffusion model to reduce hallucinations in large vision-language models while maintaining their multi-modal capabilities.",
      "ai_keywords": [
        "large vision-language models",
        "hallucination",
        "multi-modal Direct Preference Optimization",
        "OViP",
        "diffusion model",
        "contrastive training",
        "semantic differences",
        "failure-driven training"
      ]
    },
    "publishedAt": "2025-05-21T15:26:09.000Z",
    "title": "OViP: Online Vision-Language Preference Learning",
    "summary": "Large vision-language models (LVLMs) remain vulnerable to hallucination,\noften generating content misaligned with visual inputs. While recent approaches\nadvance multi-modal Direct Preference Optimization (DPO) to mitigate\nhallucination, they typically rely on predefined or randomly edited negative\nsamples that fail to reflect actual model errors, limiting training efficacy.\nIn this work, we propose an Online Vision-language Preference Learning (OViP)\nframework that dynamically constructs contrastive training data based on the\nmodel's own hallucinated outputs. By identifying semantic differences between\nsampled response pairs and synthesizing negative images using a diffusion\nmodel, OViP generates more relevant supervision signals in real time. This\nfailure-driven training enables adaptive alignment of both textual and visual\npreferences. Moreover, we refine existing evaluation protocols to better\ncapture the trade-off between hallucination suppression and expressiveness.\nExperiments on hallucination and general benchmarks demonstrate that OViP\neffectively reduces hallucinations while preserving core multi-modal\ncapabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15963.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6534c1fc23e0af0e0d7e8ebd",
      "avatarUrl": "/avatars/d65237d82aea19328f28b5a0cc93b271.svg",
      "fullname": "Siyuan Wang",
      "name": "Siyuanyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11711",
      "authors": [
        {
          "_id": "682e0d9540c6417d9962227a",
          "user": {
            "_id": "6255a34d7dacca56ac2b04e4",
            "avatarUrl": "/avatars/3e7751aa6ef7c880e3e36ac995c9a191.svg",
            "isPro": false,
            "fullname": "sagnik mukherjee",
            "user": "sagnikM",
            "type": "user"
          },
          "name": "Sagnik Mukherjee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:26.467Z",
          "hidden": false
        },
        {
          "_id": "682e0d9540c6417d9962227b",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "682e0d9540c6417d9962227c",
          "name": "Dilek Hakkani-Tur",
          "hidden": false
        },
        {
          "_id": "682e0d9540c6417d9962227d",
          "name": "Hao Peng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T21:42:28.000Z",
      "submittedOnDailyAt": "2025-05-23T00:08:00.344Z",
      "title": "En el aprendizaje por refuerzo, se ajustan pequeños subredes de grandes modelos de lenguaje.",
      "submittedOnDailyBy": {
        "_id": "6255a34d7dacca56ac2b04e4",
        "avatarUrl": "/avatars/3e7751aa6ef7c880e3e36ac995c9a191.svg",
        "isPro": false,
        "fullname": "sagnik mukherjee",
        "user": "sagnikM",
        "type": "user"
      },
      "summary": "El aprendizaje por refuerzo (RL) trae un gran aumento en la calidad de los tareas de modelos de lenguaje grandes (LLMs) y en la concordancia con los valores humanos. Lo sorprendente es que este aumento significativo se logra actualizando solo un pequeño porcentaje de los parámetros, entre 5% y 30%, sin que se modifiquen efectivamente los demás. Este fenómeno se denomina esparsidad de la actualización de parámetros en RL, y se observa en 10 diferentes familias de LLMs y en 7 algoritmos de RL ampliamente utilizados (como PPO, GRPO, DPO). Esta esparsidad puede surgir debido a la normalización explícita de la esparsidad o a las restricciones arquitectónicas. La precisión se recupera únicamente a través de la ajuste de subredes, y se puede lograr un rendimiento comparable al ajuste completo de la red. Las semillas aleatorias, los datos de entrenamiento y las redes de RL superponen más que se esperaba. La análisis muestra que esta esparsidad implica actualizaciones aproximadamente esparsas en todas las matrices de parámetros, en lugar de actualizaciones en capas específicas. Además, la mayoría de las actualizaciones de parámetros son actualizaciones aproximadamente completas en todas las dimensiones, y las actualizaciones de RL actualizan un pequeño conjunto de parámetros que cubren aproximadamente todas las dimensiones posibles. Esta esparsidad sugiere que el aprendizaje de datos cercanos a la distribución de políticas y la creación de modelos que se parezcan a modelos previamente entrenados (como la normalización KL y el clipping de gradientes) tienen un impacto limitado.",
      "upvotes": 4,
      "discussionId": "682e0d9540c6417d996222d7",
      "ai_summary": "Reinforcement learning improves large language models with minimal parameter updates, affecting only a small subnetwork without explicit sparsity techniques.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "parameter update sparsity",
        "PPO",
        "GRPO",
        "DPO",
        "policy distribution",
        "KL regularization",
        "gradient clipping"
      ]
    },
    "publishedAt": "2025-05-16T17:42:28.000Z",
    "title": "Reinforcement Learning Finetunes Small Subnetworks in Large Language\n  Models",
    "summary": "Reinforcement learning (RL) yields substantial improvements in large language\nmodels (LLMs) downstream task performance and alignment with human values.\nSurprisingly, such large gains result from updating only a small subnetwork\ncomprising just 5 percent to 30 percent of the parameters, with the rest\neffectively unchanged. We refer to this phenomenon as parameter update sparsity\ninduced by RL. It is observed across all 7 widely used RL algorithms (e.g.,\nPPO, GRPO, DPO) and all 10 LLMs from different families in our experiments.\nThis sparsity is intrinsic and occurs without any explicit sparsity promoting\nregularizations or architectural constraints. Finetuning the subnetwork alone\nrecovers the test accuracy, and, remarkably, produces a model nearly identical\nto the one obtained via full finetuning. The subnetworks from different random\nseeds, training data, and even RL algorithms show substantially greater overlap\nthan expected by chance. Our analysis suggests that this sparsity is not due to\nupdating only a subset of layers, instead, nearly all parameter matrices\nreceive similarly sparse updates. Moreover, the updates to almost all parameter\nmatrices are nearly full-rank, suggesting RL updates a small subset of\nparameters that nevertheless span almost the full subspaces that the parameter\nmatrices can represent. We conjecture that the this update sparsity can be\nprimarily attributed to training on data that is near the policy distribution,\ntechniques that encourage the policy to remain close to the pretrained model,\nsuch as the KL regularization and gradient clipping, have limited impact.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11711.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6255a34d7dacca56ac2b04e4",
      "avatarUrl": "/avatars/3e7751aa6ef7c880e3e36ac995c9a191.svg",
      "fullname": "sagnik mukherjee",
      "name": "sagnikM",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16186",
      "authors": [
        {
          "_id": "683005352b4a4d1ce546568b",
          "name": "Kaiwen Zhou",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568c",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568d",
          "name": "Gaowen Liu",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568e",
          "name": "Jayanth Srinivasa",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568f",
          "name": "Aosong Feng",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce5465690",
          "name": "Dawn Song",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce5465691",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/Qu6Z3hnjamfkpCZ9DuRkT.png"
      ],
      "publishedAt": "2025-05-22T03:46:03.000Z",
      "submittedOnDailyAt": "2025-05-23T03:51:00.196Z",
      "title": "SafeKey: Fortalece los momentos críticos de insights de insta por razones de seguridad.",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "Los modelos de inferencia a gran escala (LRMs) están introduciendo un nuevo paradigma en la era actual, proporcionando respuestas con razones explícitas y logrando un impresionante mejoramiento en tareas complejas. Sin embargo, presentan un alto riesgo de seguridad frente a consultas perjudiciales y ataques adversarios. Las últimas tentativas para mejorar la seguridad de los LRMs, como el ajuste micro con control de normas (SFT), aumentan la seguridad, pero reducen la capacidad de los modelos para responder a consultas que antes no habían visto. Se investiga en profundidad la generación de LRMs, se identifican los momentos de 'ahá' que activan la seguridad y se reconoce que estos momentos son claves para la generación de respuestas seguras. Este 'ahá' momento aparece generalmente como 'palabras clave' en el proceso de comprensión de la consulta y indica si el modelo puede seguir la consulta de manera segura. Basándose en esta perspectiva, se propone SafeKey, un método que incluye dos objetivos complementarios para mejorar la activación de la seguridad en las palabras clave: 1) fortalecer los señales de seguridad en las representaciones internas del modelo antes de las palabras clave con un 'header de seguridad doble paso', y 2) mejorar la comprensión de la consulta con el modelado de mascaras de consulta. Los experimentos en varios marcos de referencia de seguridad muestran que nuestro método mejora significativamente la capacidad de expansión de seguridad frente a ataques de 'frenos de pinchazo' y consultas perjudiciales fuera de la distribución, reduciendo el promedio de porcentaje de daños a un 9.6% y manteniendo la capacidad general. El análisis demuestra cómo SafeKey mejora la seguridad, modificando la atención interna y mejorando la calidad de las representaciones ocultas.",
      "upvotes": 3,
      "discussionId": "683005362b4a4d1ce54656b1",
      "ai_summary": "SafeKey enhances the safety of large reasoning models by focusing on activating a safety aha moment in the key sentence through dual-path safety head and query-mask modeling, thereby improving generalization to harmful prompts.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRMs",
        "explicit reasoning",
        "safety risks",
        "adversarial attacks",
        "supervised fine-tuning",
        "SFT",
        "safety aha moment",
        "key sentence",
        "query understanding process",
        "Dual-Path Safety Head",
        "Query-Mask Modeling",
        "safety generalization",
        "jailbreak attacks",
        "out-of-distribution harmful prompts",
        "average harmfulness rate",
        "hidden representations"
      ]
    },
    "publishedAt": "2025-05-21T23:46:03.000Z",
    "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning",
    "summary": "Large Reasoning Models (LRMs) introduce a new generation paradigm of\nexplicitly reasoning before answering, leading to remarkable improvements in\ncomplex tasks. However, they pose great safety risks against harmful queries\nand adversarial attacks. While recent mainstream safety efforts on LRMs,\nsupervised fine-tuning (SFT), improve safety performance, we find that\nSFT-aligned models struggle to generalize to unseen jailbreak prompts. After\nthorough investigation of LRMs' generation, we identify a safety aha moment\nthat can activate safety reasoning and lead to a safe response. This aha moment\ntypically appears in the `key sentence', which follows models' query\nunderstanding process and can indicate whether the model will proceed safely.\nBased on these insights, we propose SafeKey, including two complementary\nobjectives to better activate the safety aha moment in the key sentence: (1) a\nDual-Path Safety Head to enhance the safety signal in the model's internal\nrepresentations before the key sentence, and (2) a Query-Mask Modeling\nobjective to improve the models' attention on its query understanding, which\nhas important safety hints. Experiments across multiple safety benchmarks\ndemonstrate that our methods significantly improve safety generalization to a\nwide range of jailbreak attacks and out-of-distribution harmful prompts,\nlowering the average harmfulness rate by 9.6\\%, while maintaining general\nabilities. Our analysis reveals how SafeKey enhances safety by reshaping\ninternal attention and improving the quality of hidden representations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/Qu6Z3hnjamfkpCZ9DuRkT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16186.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15517",
      "authors": [
        {
          "_id": "682e8bc5b38184d0edcd1671",
          "user": {
            "_id": "66d2af23f040611f7cea1b1b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d2af23f040611f7cea1b1b/vCToO_XgrLD0nfR8rzjZd.jpeg",
            "isPro": false,
            "fullname": "Kaiyuan Eric Chen",
            "user": "keplerccc",
            "type": "user"
          },
          "name": "Kaiyuan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:21.981Z",
          "hidden": false
        },
        {
          "_id": "682e8bc5b38184d0edcd1672",
          "name": "Shuangyu Xie",
          "hidden": false
        },
        {
          "_id": "682e8bc5b38184d0edcd1673",
          "name": "Zehan Ma",
          "hidden": false
        },
        {
          "_id": "682e8bc5b38184d0edcd1674",
          "name": "Ken Goldberg",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66d2af23f040611f7cea1b1b/f5gpZzVb5pLF6IXMt0_xZ.png"
      ],
      "publishedAt": "2025-05-21T13:42:52.000Z",
      "submittedOnDailyAt": "2025-05-23T03:39:07.197Z",
      "title": "Robo2VLM: Conjunto de datos de manipulación de robots en el exterior para respuestas visuales a preguntas",
      "submittedOnDailyBy": {
        "_id": "66d2af23f040611f7cea1b1b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d2af23f040611f7cea1b1b/vCToO_XgrLD0nfR8rzjZd.jpeg",
        "isPro": false,
        "fullname": "Kaiyuan Eric Chen",
        "user": "keplerccc",
        "type": "user"
      },
      "summary": "La Visión de Langue Longue-Mot (VLM) obtiene conocimientos de la realidad y capacidades lógicas generales a partir de corpus de imagen-texto a escala de internet. Estos modelos fortalecen la comprensión de escenarios y la planificación de tareas, así como apoyan las políticas motoras visuales basadas en datos de trayectos de robots. Estamos revisando un enfoque contrario - fortaleciendo y evaluando VLMs con datos de trayectos de robots ricos, verdaderos y diversos. En este artículo, presentamos Robo2VLM, un marco de trabajo para generar conjuntos de datos de respuesta a preguntas visuales (VQA) para VLMs. Cuando una persona proporciona datos de trayectos de robots teleoperados, Robo2VLM obtiene datos reales de sensores no visuales y no explicativos. Con estos sensores, se dividen los datos de trayectos de robots en secuencias de etapas. En cada etapa, Robo2VLM comprende el escenario y la interacción, identificando el robot, el objetivo de la tarea y las características 3D de los objetos objetivos. Utilizando estas características, se generan templates de preguntas VQA lógicas basadas en espacio y interacción. Robo2VLM-1 proporciona un conjunto de datos a partir de grandes cantidades de datos de trayectos de robots. Este conjunto incluye 176k datos de trayectos de robots verdaderos, 463 tipos de escenarios y 3,396 tipos de tareas de movimiento del robot, registrando 684,710 preguntas. Los resultados muestran que Robo2VLM-1 benchmarkea y mejora las capacidades lógicas espaciales y de interacción de VLMs.",
      "upvotes": 2,
      "discussionId": "682e8bc6b38184d0edcd16bc",
      "githubRepo": "https://github.com/KeplerC/robo2VLM",
      "ai_summary": "Robo2VLM, a framework for generating Visual Question Answering datasets using robot trajectory data, enhances and evaluates Vision-Language Models by leveraging sensory modalities and 3D property understanding.",
      "ai_keywords": [
        "Visual-Language Models",
        "Visual Question Answering",
        "VQA",
        "robot trajectory data",
        "end-effector pose",
        "gripper aperture",
        "force sensing",
        "manipulation phases",
        "3D properties",
        "task goal",
        "target object",
        "spatial reasoning",
        "goal-conditioned reasoning",
        "interaction reasoning",
        "Robo2VLM-1"
      ]
    },
    "publishedAt": "2025-05-21T09:42:52.000Z",
    "title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot\n  Manipulation Datasets",
    "summary": "Vision-Language Models (VLMs) acquire real-world knowledge and general\nreasoning ability through Internet-scale image-text corpora. They can augment\nrobotic systems with scene understanding and task planning, and assist\nvisuomotor policies that are trained on robot trajectory data. We explore the\nreverse paradigm - using rich, real, multi-modal robot trajectory data to\nenhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual\nQuestion Answering (VQA) dataset generation framework for VLMs. Given a human\ntele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual\nand non-descriptive sensory modalities, such as end-effector pose, gripper\naperture, and force sensing. Based on these modalities, it segments the robot\ntrajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses\nscene and interaction understanding to identify 3D properties of the robot,\ntask goal, and the target object. The properties are used to generate\nrepresentative VQA queries - images with textural multiple-choice questions -\nbased on spatial, goal-conditioned, and interaction reasoning question\ntemplates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710\nquestions covering 463 distinct scenes and 3,396 robotic manipulation tasks\nfrom 176k real robot trajectories. Results suggest that Robo2VLM-1 can\nbenchmark and improve VLM capabilities in spatial and interaction reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66d2af23f040611f7cea1b1b/f5gpZzVb5pLF6IXMt0_xZ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15517.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d2af23f040611f7cea1b1b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d2af23f040611f7cea1b1b/vCToO_XgrLD0nfR8rzjZd.jpeg",
      "fullname": "Kaiyuan Eric Chen",
      "name": "keplerccc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17019",
      "authors": [
        {
          "_id": "683041f868160a3c0e525cae",
          "name": "Chenhao Zhang",
          "hidden": false
        },
        {
          "_id": "683041f868160a3c0e525caf",
          "name": "Yazhe Niu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:53.000Z",
      "submittedOnDailyAt": "2025-05-23T08:11:12.176Z",
      "title": "\"Dejen que los Androides Sueñen de Ovinos Eléctricos: Un Marco de Comprensión y Reacción de Implicaciones de Imágenes Similar a la Humana\"",
      "submittedOnDailyBy": {
        "_id": "647daf00cfca67bc50f9a99f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647daf00cfca67bc50f9a99f/8Snmk1V6lZ8ecdTW3POfm.jpeg",
        "isPro": false,
        "fullname": "Chenhao(Leo) Zhang",
        "user": "MING-ZCH",
        "type": "user"
      },
      "summary": "Entender la metáfora de las imágenes es un problema importante en sistemas de IA, ya que los modelos actuales tienen dificultades para comprender los significados culturales, emocionales y contextuales que se encuentran en el contenido de las imágenes. Los modelos de lenguaje multimodal (MLLMs) muestran excelente rendimiento en tareas básicas como Respuesta a Preguntas Visuales (VQA), pero su comprensión de los significados de las imágenes está limitada por defectos contextuales, lo que inspira el propuesto de un nuevo marco llamado \"Let Androids Dream (LAD)\". LAD utiliza un marco de tres etapas para abordar estos defectos: (1) la conversión de información visual en una representación de texto multinivel llamada \"Percepción\", (2) la resolución indirecta de los defectos y la integración de conocimientos contextuales en la etapa llamada \"Busqueda\", y (3) la generación explícita de la significación contextual de las imágenes en la etapa llamada \"Razonamiento\". El marco utilizando el modelo GPT-4o-mini ligero ha demostrado un rendimiento superior a los 15+MLLM en el benchmark de significado de imágenes en inglés, ha mostrado un gran mejoramiento en el benchmark chino, ha mostrado un rendimiento comparable al de GPT-4o en preguntas de múltiples respuestas, y ha mejorado en el 36.7% en las preguntas abiertas. Este proyecto proporciona nuevas guías sobre cómo la IA puede comprender el significado de las imágenes, y está disponible para uso público (https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep).",
      "upvotes": 1,
      "discussionId": "683041f968160a3c0e525cf2",
      "githubRepo": "https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep",
      "ai_summary": "LAD, a three-stage framework using GPT-4o-mini, achieves state-of-the-art performance in image implication understanding and reasoning tasks across different languages and question types.",
      "ai_keywords": [
        "Visual Question Answering (VQA)",
        "image implication understanding",
        "reasoning",
        "multimodal large language models (MLLMs)",
        "cross-domain knowledge",
        "context-alignment",
        "Multiple-Choice Question (MCQ)",
        "Open-Style Question (OSQ)"
      ]
    },
    "publishedAt": "2025-05-22T13:59:53.000Z",
    "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication\n  Understanding and Reasoning Framework",
    "summary": "Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17019.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647daf00cfca67bc50f9a99f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647daf00cfca67bc50f9a99f/8Snmk1V6lZ8ecdTW3POfm.jpeg",
      "fullname": "Chenhao(Leo) Zhang",
      "name": "MING-ZCH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16612",
      "authors": [
        {
          "_id": "6830410d37efd0e958fcaf9c",
          "name": "Daniel Scalena",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcaf9d",
          "user": {
            "_id": "5e7749883d77a72421292d07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
            "isPro": false,
            "fullname": "Gabriele Sarti",
            "user": "gsarti",
            "type": "user"
          },
          "name": "Gabriele Sarti",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T09:35:27.088Z",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcaf9e",
          "name": "Arianna Bisazza",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcaf9f",
          "name": "Elisabetta Fersini",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcafa0",
          "name": "Malvina Nissim",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/NW7x7zlN8SRZybdNR_ad1.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/Y5HbXIQzjGZjHUu99IGGT.png"
      ],
      "publishedAt": "2025-05-22T12:47:16.000Z",
      "submittedOnDailyAt": "2025-05-23T08:07:20.310Z",
      "title": "Este texto ya está en coreano, por lo tanto, no se necesita traducción.",
      "submittedOnDailyBy": {
        "_id": "5e7749883d77a72421292d07",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
        "isPro": false,
        "fullname": "Gabriele Sarti",
        "user": "gsarti",
        "type": "user"
      },
      "summary": "Los sistemas de traducción basados en modelos de lenguaje grandes de alta calidad (LLMs) han facilitado la producción de traducciones de portales que reflejan teorías de estilo. Sin embargo, en casos donde las demandas de estilo son imprecisas o difíciles de transmitir, estos sistemas enfrentan desafíos. Hemos revisado diversas estrategias para la portalización y individualización de la generación de LLMs, y nos centramos en áreas de traducción literaria complejas. Presentamos estrategias y la intervención en la inferencia para controlar la generación de modelos para estilos individuales, y proponemos un marco relativamente autónomo utilizando conceptos potenciales extraídos de codificadores automáticos escasos. Nuestros resultados muestran que el control fortalece la individualización mientras mantiene la calidad de la traducción. Además, revisamos el impacto de la influencia de la representación de LLMs y se observa que las capas de modelos relacionadas con la individualización reciben la misma influencia, lo que indica que operan bajo la misma estructura.",
      "upvotes": 1,
      "discussionId": "6830410d37efd0e958fcafd5",
      "ai_summary": "Strategies including prompting and contrastive frameworks using latent concepts from sparse autoencoders effectively personalize LLM translations in low-resource settings while maintaining quality.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "literary translation",
        "prompting strategies",
        "inference-time interventions",
        "steering",
        "contrastive framework",
        "latent concepts",
        "sparse autoencoders",
        "personalization properties",
        "translation quality",
        "multi-shot prompting"
      ]
    },
    "publishedAt": "2025-05-22T08:47:16.000Z",
    "title": "Steering Large Language Models for Machine Translation Personalization",
    "summary": "High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/NW7x7zlN8SRZybdNR_ad1.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/Y5HbXIQzjGZjHUu99IGGT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16612.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e7749883d77a72421292d07",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
      "fullname": "Gabriele Sarti",
      "name": "gsarti",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 224
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16170",
      "authors": [
        {
          "_id": "68301340cc0d12cd873342e1",
          "user": {
            "_id": "65c0de12efbb14b39c97f78e",
            "avatarUrl": "/avatars/18485f79427a35bd9e19f71b67c88dce.svg",
            "isPro": false,
            "fullname": "Yuqing Yang",
            "user": "ayyyq",
            "type": "user"
          },
          "name": "Yuqing Yang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T06:35:23.129Z",
          "hidden": false
        },
        {
          "_id": "68301340cc0d12cd873342e2",
          "name": "Robin Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:16:00.000Z",
      "submittedOnDailyAt": "2025-05-23T05:10:31.730Z",
      "title": "Modelo de Confianza y el Papel de la Retroalimentación - \"¿Cómo Reconocen los Errores los Modelos de LLMs?\"",
      "submittedOnDailyBy": {
        "_id": "65c0de12efbb14b39c97f78e",
        "avatarUrl": "/avatars/18485f79427a35bd9e19f71b67c88dce.svg",
        "isPro": false,
        "fullname": "Yuqing Yang",
        "user": "ayyyq",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) plantean la pregunta de si pueden reconocer errores cuando están más informados de lo que deberían ser. En este estudio, definimos la acción de reconocer errores en respuestas anteriormente generadas como \"retirarse\" y buscamos comprender en qué situaciones los LLMs hacen esto. Primero, construimos un conjunto de datos propio para evaluar si los modelos retiran errores que contravienen a su conocimiento de parámetros. Los LLMs tienen la capacidad de retirarse, pero esto no sucede con frecuencia. El retirarse muestra que está relacionado con la confianza interna del modelo, y que no se retiran errores que confían en su respuesta. Experimentos dirigidos muestran la relación causal entre la confianza interna y el retirarse, y promueven que los modelos busquen verificar su respuesta cuando no la confían, mostrando cómo esto puede cambiar su comportamiento. Finalmente, una pequeña regulación de supervisador muestra que los modelos pueden aprender un conocimiento interno más preciso, lo que mejora significativamente la capacidad de retirarse. Los códigos y conjuntos de datos están disponibles en https://github.com/ayyyq/llm-retraction.",
      "upvotes": 1,
      "discussionId": "68301341cc0d12cd87334304",
      "githubRepo": "https://github.com/ayyyq/llm-retraction",
      "ai_summary": "LLMs rarely retract incorrect answers they believe to be factually correct, but supervised fine-tuning can improve their retraction performance by refining their internal beliefs.",
      "ai_keywords": [
        "retraction",
        "model-specific datasets",
        "parametric knowledge",
        "internal belief",
        "self-verification",
        "attention behavior",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-05-21T23:16:00.000Z",
    "title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model\n  Belief in Retraction",
    "summary": "Can large language models (LLMs) admit their mistakes when they should know\nbetter? In this work, we define the behavior of acknowledging errors in\npreviously generated answers as \"retraction\" and aim to understand when and why\nLLMs choose to retract. We first construct model-specific datasets to evaluate\nwhether a model will retract an incorrect answer that contradicts its own\nparametric knowledge. While LLMs are capable of retraction, they do so only\ninfrequently. We demonstrate that retraction is closely tied to previously\nidentified indicators of models' internal belief: models fail to retract wrong\nanswers that they \"believe\" to be factually correct. Steering experiments\nfurther demonstrate that internal belief causally influences model retraction.\nIn particular, when the model does not believe its answer, this not only\nencourages the model to attempt to verify the answer, but also alters attention\nbehavior during self-verification. Finally, we demonstrate that simple\nsupervised fine-tuning significantly improves retraction performance by helping\nthe model learn more accurate internal beliefs. Code and datasets are available\non https://github.com/ayyyq/llm-retraction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16170.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c0de12efbb14b39c97f78e",
      "avatarUrl": "/avatars/18485f79427a35bd9e19f71b67c88dce.svg",
      "fullname": "Yuqing Yang",
      "name": "ayyyq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16088",
      "authors": [
        {
          "_id": "68302befc518550cc0c2e505",
          "name": "Gagan Bhatia",
          "hidden": false
        },
        {
          "_id": "68302befc518550cc0c2e506",
          "name": "Maxime Peyrard",
          "hidden": false
        },
        {
          "_id": "68302befc518550cc0c2e507",
          "name": "Wei Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T00:06:29.000Z",
      "submittedOnDailyAt": "2025-05-23T06:35:22.205Z",
      "title": "Data Frame Work: Hidden Tokenizer Backlocking of Temporal Inference",
      "submittedOnDailyBy": {
        "_id": "60394599033b61166496163b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614366097007-noauth.jpeg",
        "isPro": false,
        "fullname": "Gagan Bhatia",
        "user": "gagan3012",
        "type": "user"
      },
      "summary": "El moderno tokenizer BPE divide a fechas a menudo como fragmentos sin sentido. Por ejemplo, 20250312 se divide en 202,503,12, lo que aumenta el número de tokens y oculta una fuerte estructura lógica temporal. En este estudio, se presentan (1) indicadores sencillos y interpretables, llamados porcentaje de fragmentos de fecha, que evalúan cómo se mantienen los componentes de fecha en un multi-digito. (2) Se actualiza DateAugBench, proporcionando un sistema con 6500 casos que abordan tres tareas temporales: resolución de fechas basadas en contexto, puzzles de invariancia de formato, y operaciones de fechas históricas, modernas y futuras. (3) Mediante análisis de pruebas por capa y de atención causal, se descubre la estructura abstracta de datos que modelos de lenguaje grandes usan para combinar los componentes de mes, día y año para realizar lógica temporal. Los resultados de los experimentos muestran que un exceso de fragmentación reduce la precisión en fechas no observadas, como las históricas y las futuras, en un 10%. Además, la estructura abstracta de datos para tratar fragmentos de fecha se desarrolla rápidamente cuando el tamaño del modelo aumenta. Finalmente, se observa el paso de inferencia en que los modelos de LLM combinan fragmentos, lo que generalmente difiere de la forma en que la humanidad interpreta (año → mes → día).",
      "upvotes": 1,
      "discussionId": "68302bf0c518550cc0c2e52c",
      "ai_summary": "New DateAugBench benchmarks reveal how modern tokenizers fragment dates, impacting the accuracy of temporal reasoning in large language models, which compensate for fragmentation more effectively as they grow larger.",
      "ai_keywords": [
        "BPE tokenizers",
        "date fragmentation ratio",
        "DateAugBench",
        "temporal reasoning tasks",
        "context-based date resolution",
        "format-invariance puzzles",
        "date arithmetic",
        "layer-wise probing",
        "causal attention-hop analyses",
        "date-abstraction mechanism",
        "large language models",
        "historical dates",
        "futuristic dates"
      ]
    },
    "publishedAt": "2025-05-21T20:06:29.000Z",
    "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal\n  Reasoning",
    "summary": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 rightarrow 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future\nregimes; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year rightarrow month rightarrow\nday).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60394599033b61166496163b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614366097007-noauth.jpeg",
      "fullname": "Gagan Bhatia",
      "name": "gagan3012",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15865",
      "authors": [
        {
          "_id": "68301a21694f7a58a32919a8",
          "name": "Ingeol Baek",
          "hidden": false
        },
        {
          "_id": "68301a21694f7a58a32919a9",
          "name": "Hwan Chang",
          "hidden": false
        },
        {
          "_id": "68301a21694f7a58a32919aa",
          "name": "Sunghyun Ryu",
          "hidden": false
        },
        {
          "_id": "68301a21694f7a58a32919ab",
          "name": "Hwanhee Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T10:53:41.000Z",
      "submittedOnDailyAt": "2025-05-23T05:19:58.152Z",
      "title": "El modelo de lenguaje de visión explica cómo el modelo reconoce texto en imágenes y destaca claramente el papel característico del capítulo de Reconocimiento Óptico de Caracteres (OCR).",
      "submittedOnDailyBy": {
        "_id": "63f6f245e94ed998c46316df",
        "avatarUrl": "/avatars/9c0ec8682d4a85b96d2180602b1bbe6c.svg",
        "isPro": false,
        "fullname": "ingeolbaek",
        "user": "ingeol",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje y visión largo (LVLMs) han experimentado un progreso claro, pero aún persiste errores en la interpretación y la detección y interpretación de información contextual dentro de las imágenes. En este artículo, se revisa estos LVLMs y identifica cabezas específicas responsables de reconocer caracteres en imágenes. Estas cabezas se llamarán \"cabezas OCR\". Los hallazgos de estas cabezas son los siguientes: 1) No son raras: en contraste con las cabezas de búsqueda anteriores, muchas cabezas se activan para extraer información contextual de las imágenes. 2) Diferentes en cuanto a la calidad: las cabezas OCR muestran una baja similitud con las cabezas de búsqueda generales. 3) Activadas dinámicamente: la frecuencia de activación de estas cabezas se correlaciona estrechamente con el score OCR. Estos hallazgos se aplicaron a la técnica de \"Chain-of-Thought\" (CoT) para OCR y cabezas de búsqueda general, y se validaron mediante la mascarización de estas cabezas en tareas posteriores. Además, se confirmó un mejoramiento de la eficiencia al reorganizar los valores de los tokens sincronizadores dentro de las cabezas OCR. Estas observaciones proporcionan una comprensión más profunda de la estructura interna utilizada por los LVLMs para procesar la información de caracteres incorporados en las imágenes.",
      "upvotes": 1,
      "discussionId": "68301a22694f7a58a32919ef",
      "ai_summary": "The study identifies and analyzes OCR Heads within Large Vision Language Models, revealing their unique activation patterns and roles in interpreting text within images.",
      "ai_keywords": [
        "Large Vision Language Models",
        "LVLMs",
        "Optical Character Recognition Head",
        "OCR Head",
        "retrieval heads",
        "Chain-of-Thought",
        "CoT",
        "sink-token values"
      ]
    },
    "publishedAt": "2025-05-21T06:53:41.000Z",
    "title": "How Do Large Vision-Language Models See Text in Image? Unveiling the\n  Distinctive Role of OCR Heads",
    "summary": "Despite significant advancements in Large Vision Language Models (LVLMs), a\ngap remains, particularly regarding their interpretability and how they locate\nand interpret textual information within images. In this paper, we explore\nvarious LVLMs to identify the specific heads responsible for recognizing text\nfrom images, which we term the Optical Character Recognition Head (OCR Head).\nOur findings regarding these heads are as follows: (1) Less Sparse: Unlike\nprevious retrieval heads, a large number of heads are activated to extract\ntextual information from images. (2) Qualitatively Distinct: OCR heads possess\nproperties that differ significantly from general retrieval heads, exhibiting\nlow similarity in their characteristics. (3) Statically Activated: The\nfrequency of activation for these heads closely aligns with their OCR scores.\nWe validate our findings in downstream tasks by applying Chain-of-Thought (CoT)\nto both OCR and conventional retrieval heads and by masking these heads. We\nalso demonstrate that redistributing sink-token values within the OCR heads\nimproves performance. These insights provide a deeper understanding of the\ninternal mechanisms LVLMs employ in processing embedded textual information in\nimages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15865.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f6f245e94ed998c46316df",
      "avatarUrl": "/avatars/9c0ec8682d4a85b96d2180602b1bbe6c.svg",
      "fullname": "ingeolbaek",
      "name": "ingeol",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14395",
      "authors": [
        {
          "_id": "682db49f167398cff979ec27",
          "user": {
            "_id": "654f3cca8cc59d5b490b805b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654f3cca8cc59d5b490b805b/WAZZjN4q8VGp3VS2lAWyy.png",
            "isPro": false,
            "fullname": "Seyoung Song",
            "user": "seyoungsong",
            "type": "user"
          },
          "name": "Seyoung Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T12:27:01.867Z",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec28",
          "name": "Seogyeong Jeong",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec29",
          "name": "Eunsu Kim",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2a",
          "name": "Jiho Jin",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2b",
          "name": "Dongkwan Kim",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2c",
          "name": "Jay Shin",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2d",
          "user": {
            "_id": "60e0251ea9b5d8282481f2b7",
            "avatarUrl": "/avatars/43441373af054a6184c22097bfeb97e4.svg",
            "isPro": false,
            "fullname": "Alice Oh",
            "user": "aliceoh",
            "type": "user"
          },
          "name": "Alice Oh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T19:06:44.285Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/654f3cca8cc59d5b490b805b/cxsyLLbll1-fv-itFCzp_.png"
      ],
      "publishedAt": "2025-05-20T14:14:00.000Z",
      "submittedOnDailyAt": "2025-05-23T07:02:40.832Z",
      "title": "MUG-Eval: Marco de Evaluación Virtual para la Generación Multilingüe\n  Habilidades Disponibles para Cada Lenguaje",
      "submittedOnDailyBy": {
        "_id": "654f3cca8cc59d5b490b805b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654f3cca8cc59d5b490b805b/WAZZjN4q8VGp3VS2lAWyy.png",
        "isPro": false,
        "fullname": "Seyoung Song",
        "user": "seyoungsong",
        "type": "user"
      },
      "summary": "La evaluación del habilidad de generación de texto en modelos de lenguaje grande (LLMs) es un desafío, especialmente en lenguas de bajo recurso donde los métodos de evaluación directa son limitados. Proponemos un nuevo marco de referencia llamado MUG-Eval. Este marco de referencia transforma los benchmarks existentes en tareas de diálogo para evaluar la capacidad de generación de texto de LLMs por lenguaje. En particular, estas tareas de diálogo están diseñadas para facilitar una comunicación efectiva en la lengua objetivo. Además, el éxito en estas tareas se utiliza como indicador de éxito en la generación de diálogo. Nuestro enfoque proporciona dos principales ventajas. Una es que no depende de herramientas NLP específicas por lenguaje o conjuntos de datos etiquetados. La segunda es que no depende de los jueces basados en modelos de lenguaje grande (LLMs-as-judges). Por lo tanto, la calidad de la evaluación no disminuye fuera del rango de lenguajes de alto recurso. Evaluamos 8 modelos de lenguaje grande en un rango de 30 lenguas, que incluyen categorías de alto, medio y bajo recurso. Demostramos que MUG-Eval tiene una fuerte correlación con los benchmarks existentes y permite la comparación de lenguajes y modelos. Nuestro marco de referencia ofrece una solución eficiente en términos de recursos para la evaluación de generación por lenguaje, y puede ser extendido a miles de lenguajes.",
      "upvotes": 1,
      "discussionId": "682db4a0167398cff979ec67",
      "ai_summary": "MUG-Eval assesses LLMs' multilingual generation by transforming benchmarks into conversational tasks, offering a language-independent and NLP tool-free method that correlates well with established benchmarks.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "multilingual generation",
        "conversational tasks",
        "task success rate",
        "low-resource languages",
        "high-resource languages",
        "NLP tools",
        "annotated datasets",
        "MUG-Eval",
        "standardized comparisons"
      ]
    },
    "publishedAt": "2025-05-20T10:14:00.000Z",
    "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language",
    "summary": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks (r > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/654f3cca8cc59d5b490b805b/cxsyLLbll1-fv-itFCzp_.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14395.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654f3cca8cc59d5b490b805b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654f3cca8cc59d5b490b805b/WAZZjN4q8VGp3VS2lAWyy.png",
      "fullname": "Seyoung Song",
      "name": "seyoungsong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16048",
      "authors": [
        {
          "_id": "68302100d260f25aad14b1c7",
          "user": {
            "_id": "62a1e17591f85abff79c2cdf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
            "isPro": false,
            "fullname": "Philipp Siedler",
            "user": "philippds",
            "type": "user"
          },
          "name": "Philipp D. Siedler",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T07:26:08.014Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T22:00:20.000Z",
      "submittedOnDailyAt": "2025-05-23T05:49:26.287Z",
      "title": "SPhyR: Descripción espacial física de la distribución de materia benchmark",
      "submittedOnDailyBy": {
        "_id": "62a1e17591f85abff79c2cdf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
        "isPro": false,
        "fullname": "Philipp Siedler",
        "user": "philippds",
        "type": "user"
      },
      "summary": "Aquí presentamos un nuevo conjunto de datos basado en optimización topología. La optimización topología es un método que calcula la distribución óptima de materiales dentro de un espacio de diseño, bajo condiciones de cargas y apoyos específicas. En este conjunto de datos, proporcionamos a los modelos de lenguaje de grandes escala (LLM) condiciones de frontera bidimensionales, fuerzas aplicadas y apoyos, para explicar por qué se obtiene una distribución óptima de materiales. Este conjunto de datos incluye diversas tareas, desde el llenado de áreas mascaradas dentro de estructuras hasta la predicción de distribuciones completas de materiales. Para resolver estas tareas, es necesario comprender el flujo de fuerza bajo las restricciones dadas, explicar por qué no se depende de herramientas de simulación o modelos físicos claros. Así, la tarea es explicar la estabilidad de la estructura y la organización del espacio. Nuestro conjunto de datos tiene como objetivo evaluar la capacidad de razonamiento espacial y físico en un entorno bidimensional, ofreciendo una perspectiva complementaria a los marcos de referencia lingüísticos y lógicos tradicionales.",
      "upvotes": 0,
      "discussionId": "68302101d260f25aad14b215",
      "githubRepo": "https://github.com/philippds/SPhyR",
      "ai_summary": "A dataset benchmarks spatial and physical reasoning of LLMs using topology optimization tasks without simulation tools.",
      "ai_keywords": [
        "Large Language Models (LLM)",
        "topology optimization",
        "material distribution",
        "structural stability",
        "spatial reasoning",
        "physical reasoning",
        "boundary conditions",
        "applied forces",
        "supports"
      ]
    },
    "publishedAt": "2025-05-21T18:00:20.000Z",
    "title": "SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution",
    "summary": "We introduce a novel dataset designed to benchmark the physical and spatial\nreasoning capabilities of Large Language Models (LLM) based on topology\noptimization, a method for computing optimal material distributions within a\ndesign space under prescribed loads and supports. In this dataset, LLMs are\nprovided with conditions such as 2D boundary, applied forces and supports, and\nmust reason about the resulting optimal material distribution. The dataset\nincludes a variety of tasks, ranging from filling in masked regions within\npartial structures to predicting complete material distributions. Solving these\ntasks requires understanding the flow of forces and the required material\ndistribution under given constraints, without access to simulation tools or\nexplicit physical models, challenging models to reason about structural\nstability and spatial organization. Our dataset targets the evaluation of\nspatial and physical reasoning abilities in 2D settings, offering a\ncomplementary perspective to traditional language and logic benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16048.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a1e17591f85abff79c2cdf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
      "fullname": "Philipp Siedler",
      "name": "philippds",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]