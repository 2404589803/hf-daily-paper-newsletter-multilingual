[
  {
    "paper": {
      "id": "2504.15120",
      "authors": [
        {
          "_id": "680733cf7722bb6407ca0787",
          "user": {
            "_id": "65276c7911a8a521c91bc10f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
            "isPro": false,
            "fullname": "Khalil Hennara",
            "user": "Hennara",
            "type": "user"
          },
          "name": "Khalil Hennara",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-22T09:37:47.479Z",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca0788",
          "name": "Sara Chrouf",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca0789",
          "user": {
            "_id": "63aa7667769a10efc404fbbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63aa7667769a10efc404fbbc/tn8ZxUmTEMS0Gze7_F7JL.jpeg",
            "isPro": false,
            "fullname": "Mohamed Motasim Hamed",
            "user": "Moatasem444",
            "type": "user"
          },
          "name": "Mohamed Motaism Hamed",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:37:25.702Z",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078a",
          "user": {
            "_id": "65704741e1cfce1764ce652e",
            "avatarUrl": "/avatars/9189aaf417426af4ebe381ed364a6c0e.svg",
            "isPro": false,
            "fullname": "Zeina Aldallal",
            "user": "ZeinaD",
            "type": "user"
          },
          "name": "Zeina Aldallal",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-23T05:30:42.569Z",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078b",
          "name": "Omar Hadid",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078c",
          "name": "Safwan AlModhayan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T14:17:25.000Z",
      "submittedOnDailyAt": "2025-04-23T03:28:02.778Z",
      "title": "クワイン 1.5B: SLM de árabe debido a la injección de lenguaje",
      "submittedOnDailyBy": {
        "_id": "65276c7911a8a521c91bc10f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
        "isPro": false,
        "fullname": "Khalil Hennara",
        "user": "Hennara",
        "type": "user"
      },
      "summary": "Una etapa importante en el desarrollo de la inteligencia artificial es la adición de nuevos conocimientos a modelos existentes. En este artículo, se presenta una nueva metodología para la integración de nuevas lenguas en modelos de lenguaje grande (LLM). Nuestro enfoque consiste en registrar en los modelos existentes lenguajes objetivos que no han sido vistos antes, logrando así la incorporación de nuevo conocimiento sin destruirlo. Se utilizó un pequeño modelo de código abierto entrenado en inglés para inyectar árabe, y se entrenó un pequeño modelo de 1500 millones de parámetros llamado \"Kuwain\". Nuestro método mejoró en promedio el rendimiento en árabe en diferentes marcos de referencia en un 8%, manteniendo el conocimiento previo y entrenando el modelo con un mínimo de datos del modelo original. Esto es una elección eficiente en términos de costo, en lugar de entrenar modelos detallados para cada lenguaje individualmente. Los resultados demuestran la posibilidad de una expansión eficiente de modelos de lenguaje y la capacidad de expandir y evitar el proceso de fuerza de recursos, mostrando así la eficiencia y la viabilidad de este enfoque.",
      "upvotes": 58,
      "discussionId": "680733d07722bb6407ca07da",
      "githubRepo": "https://github.com/misraj-ai/Kuwain-Arabic-cleaner",
      "ai_keywords": [
        "large language model (LLM)",
        "tiny model",
        "Kuwain",
        "language integration",
        "Arabic language",
        "benchmarks",
        "language model expansion"
      ]
    },
    "publishedAt": "2025-04-21T10:17:25.000Z",
    "title": "Kuwain 1.5B: An Arabic SLM via Language Injection",
    "summary": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15120.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65276c7911a8a521c91bc10f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
      "fullname": "Khalil Hennara",
      "name": "Hennara",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16084",
      "authors": [
        {
          "_id": "6808558a07e80b69b2e351b5",
          "name": "Yuxin Zuo",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b6",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:51.438Z",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b7",
          "name": "Shang Qu",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b8",
          "name": "Li Sheng",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b9",
          "name": "Xuekai Zhu",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351ba",
          "name": "Biqing Qi",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bb",
          "name": "Youbang Sun",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bc",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bd",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351be",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-23T01:22:31.055Z",
      "title": "TTRL: Aprendizaje por refuerzo durante el entrenamiento",
      "submittedOnDailyBy": {
        "_id": "60bc94cd85a3ab33829b6211",
        "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
        "isPro": false,
        "fullname": "Kaiyan Zhang",
        "user": "iseesaw",
        "type": "user"
      },
      "summary": "Este artículo investiga el aprendizaje por refuerzo (RL) en modelos de lenguaje grande (LLMs) para tareas de inferencia sobre datos sin etiquetas explícitas. El problema central es que en esta configuración, no se puede acceder a la información de valores que evalúan la recompensa en los estados donde la recompensa es posible. Esta situación parece simple, pero es común en la práctica de escalado de tiempo de prueba (TTS), lo que genera resultados sorprendentes. En este estudio, se presenta un nuevo método de entrenamiento de LLMs utilizando RL sin etiquetas, llamado aprendizaje por refuerzo en tiempo de prueba (TTRL). TTRL permite a los modelos de LLMs auto-evolucionar utilizando resultados previos de modelos pre-entrenados. Los resultados de los experimentos muestran mejoras experienciales en diferentes tareas y modelos. En particular, en AIME 2024, el rendimiento pass@1 de Qwen-2.5-Math-7B con datos de prueba sin etiquetas se mejoró aproximadamente en un 159%. Además, TTRL solo requiere soporte para el métrica de @N, supera los límites de rendimiento del modelo inicial y se acerca a los rendimientos de modelos directamente entrenados con datos de prueba y etiquetas reales. Los resultados de los experimentos demuestran la efectividad general de TTRL en diferentes tareas y sus posibilidades en una amplia gama de tareas y áreas. GitHub: https://github.com/PRIME-RL/TTRL",
      "upvotes": 44,
      "discussionId": "6808558b07e80b69b2e351f3",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "Large Language Models (LLMs)",
        "reward estimation",
        "Test-Time Scaling (TTS)",
        "majority voting",
        "Test-Time Reinforcement Learning (TTRL)",
        "self-evolution",
        "pre-trained models",
        "pass@1",
        "Qwen-2.5-Math-7B",
        "AIME 2024",
        "Maj@N metric"
      ]
    },
    "publishedAt": "2025-04-22T13:59:56.000Z",
    "title": "TTRL: Test-Time Reinforcement Learning",
    "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60bc94cd85a3ab33829b6211",
      "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
      "fullname": "Kaiyan Zhang",
      "name": "iseesaw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15521",
      "authors": [
        {
          "_id": "6808458f07e80b69b2df2440",
          "name": "Minghao Wu",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2441",
          "name": "Weixuan Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2442",
          "name": "Sinuo Liu",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2443",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2444",
          "name": "Xintong Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2445",
          "name": "Yu Zhao",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2446",
          "user": {
            "_id": "6527d8b077bceabaab382a75",
            "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
            "isPro": false,
            "fullname": "Chenyang Lyu",
            "user": "ChenyangLyu",
            "type": "user"
          },
          "name": "Chenyang Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:16.770Z",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2447",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2448",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2449",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T01:47:37.000Z",
      "submittedOnDailyAt": "2025-04-23T00:13:52.385Z",
      "title": "La Lección Amantada Adquirida de 2,000+ Pruebas Multilingües",
      "submittedOnDailyBy": {
        "_id": "62d4bf8c97ab9eb08762a975",
        "avatarUrl": "/avatars/73c6228e317cf37b4e3c3e7a4b3d8ae8.svg",
        "isPro": false,
        "fullname": "Minghao Wu",
        "user": "minghaowu",
        "type": "user"
      },
      "summary": "El desarrollo continuo de las capacidades lingüísticas de los modelos de lenguaje grandes (LLMs) ha llevado a que la evaluación multilingüe se ha convertido en un factor crucial para promover la equidad de la tecnología. En este artículo, se evaluan más de 2,000 marcadores de rendimiento multilingüe (en lenguas distintas a inglés) publicados desde 2021 hasta 2024, y se evalúan la utilidad de estos marcadores pasado, presente y futuro. Nuestra investigación revela que, a pesar de inversiones de cientos de miles de dólares, el inglés se ha mostrado excesivamente representado en estos marcadores. Además, la mayoría de los marcadores no son traducciones, sino contenidos originales en sus respectivas lenguas, y la mayoría de los datos provienen de países con abundantes recursos, como China, India, Alemania, Reino Unido y Estados Unidos. Además, hay diferencias claras entre el rendimiento de los marcadores y la evaluación humana. Las tareas relacionadas con STEM muestran una fuerte correlación con la evaluación humana (0.70 a 0.85), mientras que las tareas tradicionales de NLP, como sistemas de preguntas y respuestas (por ejemplo, XQuAD), muestran una correlación más débil (0.11 a 0.30). Además, traducir los marcadores de inglés a otros idiomas no es suficiente. Los marcadores regionales muestran un alto grado de concordancia con la percepción humana (0.68), lo que es superior a los marcadores traducidos (0.47). Esto subraya la importancia de los marcadores que se alineen con la cultura y la lengua, y que no se basen solo en la traducción. A través de estas análisis detalladas, se identifican seis limitaciones importantes en la práctica actual de la evaluación multilingüe y se proponen guías efectivas para los marcadores multilingües, así como cinco direcciones de investigación importantes. Finalmente, se invita a la cooperación internacional para el desarrollo de marcadores de rendimiento que sean realistas y acordes a la percepción humana.",
      "upvotes": 41,
      "discussionId": "6808459007e80b69b2df249e",
      "ai_keywords": [
        "multilingual large language models (LLMs)",
        "multilingual benchmarks",
        "benchmark performance",
        "human judgments",
        "STEM-related tasks",
        "question answering (e.g., XQuAD)",
        "culturally and linguistically tailored benchmarks",
        "human-aligned benchmarks",
        "real-world applications"
      ]
    },
    "publishedAt": "2025-04-21T21:47:37.000Z",
    "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
    "summary": "As large language models (LLMs) continue to advance in linguistic\ncapabilities, robust multilingual evaluation has become essential for promoting\nequitable technological progress. This position paper examines over 2,000\nmultilingual (non-English) benchmarks from 148 countries, published between\n2021 and 2024, to evaluate past, present, and future practices in multilingual\nbenchmarking. Our findings reveal that, despite significant investments\namounting to tens of millions of dollars, English remains significantly\noverrepresented in these benchmarks. Additionally, most benchmarks rely on\noriginal language content rather than translations, with the majority sourced\nfrom high-resource countries such as China, India, Germany, the UK, and the\nUSA. Furthermore, a comparison of benchmark performance with human judgments\nhighlights notable disparities. STEM-related tasks exhibit strong correlations\nwith human evaluations (0.70 to 0.85), while traditional NLP tasks like\nquestion answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).\nMoreover, translating English benchmarks into other languages proves\ninsufficient, as localized benchmarks demonstrate significantly higher\nalignment with local human judgments (0.68) than their translated counterparts\n(0.47). This underscores the importance of creating culturally and\nlinguistically tailored benchmarks rather than relying solely on translations.\nThrough this comprehensive analysis, we highlight six key limitations in\ncurrent multilingual evaluation practices, propose the guiding principles\naccordingly for effective multilingual benchmarking, and outline five critical\nresearch directions to drive progress in the field. Finally, we call for a\nglobal collaborative effort to develop human-aligned benchmarks that prioritize\nreal-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15521.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d4bf8c97ab9eb08762a975",
      "avatarUrl": "/avatars/73c6228e317cf37b4e3c3e7a4b3d8ae8.svg",
      "fullname": "Minghao Wu",
      "name": "minghaowu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16072",
      "authors": [
        {
          "_id": "6808467a867c3ef14f8326ce",
          "user": {
            "_id": "63797c273f575acc2f6893c0",
            "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
            "isPro": true,
            "fullname": "Long(Tony) Lian",
            "user": "longlian",
            "type": "user"
          },
          "name": "Long Lian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:14.686Z",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326cf",
          "name": "Yifan Ding",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d0",
          "name": "Yunhao Ge",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d1",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d2",
          "name": "Hanzi Mao",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d3",
          "user": {
            "_id": "620dd3888528f797e88cb9b5",
            "avatarUrl": "/avatars/af04728788d78fe7d6375e19e32a535e.svg",
            "isPro": false,
            "fullname": "Boyi Li",
            "user": "Boyiliee",
            "type": "user"
          },
          "name": "Boyi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:09.738Z",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d4",
          "name": "Marco Pavone",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d5",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d6",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d7",
          "user": {
            "_id": "6333a9195a032dcd095dda13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664329996201-noauth.jpeg",
            "isPro": false,
            "fullname": "Adam Yala",
            "user": "yala",
            "type": "user"
          },
          "name": "Adam Yala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:12.415Z",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d8",
          "user": {
            "_id": "649f05367b57fab3a5b27c8b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649f05367b57fab3a5b27c8b/UDJB4yqF2NmaRwCyTOfcl.jpeg",
            "isPro": true,
            "fullname": "Yin Cui",
            "user": "richardaecn",
            "type": "user"
          },
          "name": "Yin Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:06.739Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63797c273f575acc2f6893c0/37vXp0iDKwhBbVyNnCGHy.qt"
      ],
      "publishedAt": "2025-04-22T17:51:41.000Z",
      "submittedOnDailyAt": "2025-04-23T00:22:38.011Z",
      "title": "Detalles de capturas de imágenes y videos por áreas geográficas.",
      "submittedOnDailyBy": {
        "_id": "63797c273f575acc2f6893c0",
        "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
        "isPro": true,
        "fullname": "Long(Tony) Lian",
        "user": "longlian",
        "type": "user"
      },
      "summary": "Generar una descripción detallada y precisa sobre un área específica de una imagen o video es un problema básico de los modelos de lenguaje visuolingüístico. En este trabajo, se presenta un modelo que tiene como objetivo crear una descripción concreta. El Describe Anything Model (DAM) introdujo dos elementos innovadores para mantener tanto la información local como el contexto global. Esto se logra mediante un prompt de foco que asegura la codificación de alta resolución en el área objetivo y un retrotracking visual local que integra la descripción local con el contexto global. Para abordar la escasez de datos de alta calidad de DLC, se propone una pipeline de aprendizaje semi-supervisado (SSL) basada en datos, llamada DLC-SDP. DLC-SDP comienza con conjuntos de datos de segmentación existentes y expande los datos a imágenes web sin etiquetas utilizando SSL. Se presenta el DLC-Bench, un marco de evaluación diseñado para evaluar el DLC sin depender de capturas de referencia. El DAM ha alcanzado el nuevo record en 7 marcos de evaluación, que incluyen niveles de palabras clave, niveles de frases y niveles de descripción específicas para imágenes y videos locales.",
      "upvotes": 28,
      "discussionId": "6808467e867c3ef14f832831",
      "projectPage": "https://describe-anything.github.io",
      "githubRepo": "https://github.com/NVlabs/describe-anything",
      "ai_keywords": [
        "focal prompt",
        "localized vision backbone",
        "Semi-supervised learning (SSL)-based Data Pipeline (DLC-SDP)",
        "segmentation datasets",
        "DLC-Bench",
        "keyword-level",
        "phrase-level",
        "detailed multi-sentence localized image and video captioning"
      ]
    },
    "publishedAt": "2025-04-22T13:51:41.000Z",
    "title": "Describe Anything: Detailed Localized Image and Video Captioning",
    "summary": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63797c273f575acc2f6893c0/37vXp0iDKwhBbVyNnCGHy.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16072.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63797c273f575acc2f6893c0",
      "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
      "fullname": "Long(Tony) Lian",
      "name": "longlian",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15466",
      "authors": [
        {
          "_id": "6808480c49c8f78b6a4e492f",
          "name": "Jiayi Pan",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4930",
          "user": {
            "_id": "644570ba2d91b15b4c7f6311",
            "avatarUrl": "/avatars/d5e66012066d0c330b8f23718b1499d8.svg",
            "isPro": false,
            "fullname": "Xiuyu Li",
            "user": "xiuyul",
            "type": "user"
          },
          "name": "Xiuyu Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:59.248Z",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4931",
          "name": "Long Lian",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4932",
          "name": "Charlie Snell",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4933",
          "name": "Yifei Zhou",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4934",
          "user": {
            "_id": "6333a9195a032dcd095dda13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664329996201-noauth.jpeg",
            "isPro": false,
            "fullname": "Adam Yala",
            "user": "yala",
            "type": "user"
          },
          "name": "Adam Yala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:02.029Z",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4935",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4936",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4937",
          "name": "Alane Suhr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T22:29:02.000Z",
      "submittedOnDailyAt": "2025-04-23T00:30:52.876Z",
      "title": "Aprendizaje de la lógica parelela",
      "submittedOnDailyBy": {
        "_id": "63797c273f575acc2f6893c0",
        "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
        "isPro": true,
        "fullname": "Long(Tony) Lian",
        "user": "longlian",
        "type": "user"
      },
      "summary": "El cálculo de tiempo de inferencia en escalado ha traído un gran aumento en la capacidad lógica de los modelos de lenguaje. Sin embargo, los métodos actuales tienen limitaciones importantes: el enfoque de Consistencia de Cadena genera salidas demasiado largas y los problemas que surgen de la Latencia y el Context Window. Por otro lado, los métodos que dependen de la auto-coherencia como los tipos paralelos suelen dejar de lado problemas como cálculos prolongados y límites de rendimiento debido a una insuficiente colaboración. Para solucionar estos puntos, proponemos un nuevo marco lógico que integra cálculos paralelos y de cadena en un cálculo end-to-end. APR generaliza los modelos lógicos actuales y permite cálculos adaptativos utilizando operaciones como SPaN() y JOIN(). La innovación clave es la optimización del tiempo de inferencia de padres y hijos desde el principio mediante una estrategia de aprendizaje por refuerzo, lo que mejora la tasa de éxito sin necesidad de definir estructuras de inferencia previas. En las experimentaciones con el Tarea de Razonamiento Countdown, se demostró la gran ventaja de APR: (1) alto rendimiento en el mismo Context Window (83.4% vs. 60.0% a 4k context); (2) flexibilidad en escala para mayores cantidades de tokens (80.1% vs. 66.6% a 20k tokens totales); (3) mejora en la precisión en la misma Latencia (75.2% vs. 57.3% a aproximadamente 5,000ms). APR permite a los modelos de lenguaje optimizar los procesos lógicos a través de una división adaptativa automática de los cálculos.",
      "upvotes": 27,
      "discussionId": "6808480c49c8f78b6a4e4968",
      "githubRepo": "https://github.com/Parallel-Reasoning/APR",
      "ai_keywords": [
        "Adaptive Parallel Reasoning (APR)",
        "serialized chain-of-thought approaches",
        "parallel methods",
        "self-consistency",
        "adaptive multi-threaded inference",
        "spawn()",
        "join()",
        "reinforcement learning strategy",
        "parent inference threads",
        "child inference threads",
        "Countdown reasoning task",
        "context window",
        "scalability",
        "total tokens",
        "reasoning processes",
        "adaptive allocation of computation"
      ]
    },
    "publishedAt": "2025-04-21T18:29:02.000Z",
    "title": "Learning Adaptive Parallel Reasoning with Language Models",
    "summary": "Scaling inference-time computation has substantially improved the reasoning\ncapabilities of language models. However, existing methods have significant\nlimitations: serialized chain-of-thought approaches generate overly long\noutputs, leading to increased latency and exhausted context windows, while\nparallel methods such as self-consistency suffer from insufficient\ncoordination, resulting in redundant computations and limited performance\ngains. To address these shortcomings, we propose Adaptive Parallel Reasoning\n(APR), a novel reasoning framework that enables language models to orchestrate\nboth serialized and parallel computations end-to-end. APR generalizes existing\nreasoning methods by enabling adaptive multi-threaded inference using spawn()\nand join() operations. A key innovation is our end-to-end reinforcement\nlearning strategy, optimizing both parent and child inference threads to\nenhance task success rate without requiring predefined reasoning structures.\nExperiments on the Countdown reasoning task demonstrate significant benefits of\nAPR: (1) higher performance within the same context window (83.4% vs. 60.0% at\n4k context); (2) superior scalability with increased computation (80.1% vs.\n66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%\nvs. 57.3% at approximately 5,000ms). APR represents a step towards enabling\nlanguage models to autonomously optimize their reasoning processes through\nadaptive allocation of computation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15466.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63797c273f575acc2f6893c0",
      "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
      "fullname": "Long(Tony) Lian",
      "name": "longlian",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15415",
      "authors": [
        {
          "_id": "68084b04ba1dd0e6a077e09f",
          "user": {
            "_id": "64c910233d5a0dfed5ce5abb",
            "avatarUrl": "/avatars/8c73f380219c05ae7e7c2fad75a570d8.svg",
            "isPro": false,
            "fullname": "dma",
            "user": "mdh98",
            "type": "user"
          },
          "name": "David Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:56.437Z",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a0",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a1",
          "user": {
            "_id": "6704ee27386892c420db1938",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6704ee27386892c420db1938/lb5mtEwYhn47RawkynYPs.jpeg",
            "isPro": false,
            "fullname": "JinCheng Ren",
            "user": "JinChengRen",
            "type": "user"
          },
          "name": "Jincheng Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:36:46.274Z",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a2",
          "name": "Jarvis Guo",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a3",
          "name": "Yifan Yao",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a4",
          "name": "Zhenlin Wei",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a5",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a6",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a7",
          "name": "Boyu Feng",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a8",
          "name": "Jun Ma",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a9",
          "name": "Xiao Gu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0aa",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ab",
          "name": "King Zhu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ac",
          "name": "Yancheng He",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ad",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ae",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0af",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b0",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b1",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b2",
          "name": "Xiaojie Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T19:53:44.000Z",
      "submittedOnDailyAt": "2025-04-23T00:59:32.168Z",
      "title": "IV-Bench: Reconocimiento de vídeo basado en imágenes y benchmark basado en lógica",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "Los actuales marcos de evaluación de los Grandes Modelos de Lenguaje Multimodal (MLLMs) se centran principalmente en la interpretación de imágenes y en tareas generales de comprensión de vídeos, donde el papel de la contexto de las imágenes es crucial en la comprensión de los vídeos. Para abordar esta limitación, se propone IV-Bench, el primer marco de evaluación integral. IV-Bench cuenta con 967 vídeos y 2,585 consultas de imágenes-texto anotadas con precisión, y se compone de 13 tareas (7 de pose y 6 de interpretación) y 5 categorías representativas. En evaluaciones detalladas de los MLLMs más recientes (por ejemplo, InternVL2.5, Qwen2.5-VL, GPT-4o, Gemini2-Flash, Gemini2-Pro), los modelos actuales muestran una alta pérdida de rendimiento en la comprensión de vídeos basado en el contexto de las imágenes, alcanzando un máximo de 28.9% de precisión. Un análisis adicional en IV-Bench revela factores que afectan el rendimiento de los modelos, incluyendo patrones de inferencia, número de frames y resolución, y demuestra cómo estos factores influyen. Además, un enfoque sencillo de síntesis de datos muestra que los problemas de IV-Bench no solo se deben a la correspondencia de formatos de datos durante el proceso de entrenamiento, sino también a una gama más amplia de problemas. Estos hallazgos proporcionan consejos valiosos para futuras investigaciones. El código y los datos están disponibles en https://github.com/multimodal-art-projection/IV-Bench.",
      "upvotes": 15,
      "discussionId": "68084b0bba1dd0e6a077e279",
      "githubRepo": "https://github.com/multimodal-art-projection/IV-Bench",
      "ai_keywords": [
        "Image-Grounded Video Perception and Reasoning",
        "IV-Bench",
        "image-text queries",
        "frame number",
        "resolution"
      ]
    },
    "publishedAt": "2025-04-21T15:53:44.000Z",
    "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning\n  in Multimodal LLMs",
    "summary": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)\nprimarily focus on image reasoning or general video understanding tasks,\nlargely overlooking the significant role of image context in video\ncomprehension. To bridge this gap, we propose IV-Bench, the first comprehensive\nbenchmark for evaluating Image-Grounded Video Perception and Reasoning.\nIV-Bench consists of 967 videos paired with 2,585 meticulously annotated\nimage-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5\nrepresentative categories. Extensive evaluations of state-of-the-art\nopen-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,\nGemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models\nsubstantially underperform in image-grounded video Perception and Reasoning,\nmerely achieving at most 28.9% accuracy. Further analysis reveals key factors\ninfluencing model performance on IV-Bench, including inference pattern, frame\nnumber, and resolution. Additionally, through a simple data synthesis approach,\nwe demonstratethe challenges of IV- Bench extend beyond merely aligning the\ndata format in the training proecss. These findings collectively provide\nvaluable insights for future research. Our codes and data are released in\nhttps://github.com/multimodal-art-projection/IV-Bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14538",
      "authors": [
        {
          "_id": "680863ed3767f6ed7c969fbf",
          "name": "Yiting Ran",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc0",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc1",
          "name": "Tian Qiu",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc2",
          "name": "Jiaqing Liang",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc3",
          "name": "Yanghua Xiao",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc4",
          "name": "Deqing Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-20T08:56:27.000Z",
      "submittedOnDailyAt": "2025-04-23T02:23:09.187Z",
      "title": "Invitamos al Grupo de Efectos Interactivos que se enfoca en la creación de historias creativas en un libro.",
      "submittedOnDailyBy": {
        "_id": "64c7bf2c4524c2aea7eac0b3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7bf2c4524c2aea7eac0b3/5ocZ69MvN4RFv86Aa7ks3.png",
        "isPro": false,
        "fullname": "Xintao Wang",
        "user": "Neph0s",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de grandes modelos de lenguaje (LLMs) ha permitido la simulación con sistemas de múltiples agentes. Los intentos anteriores han creado una socialidad de agentes asignando nuevas funciones, pero la simulación de mundos virtuales y personajes ya establecidos, aunque tiene valor práctico, no ha sido ampliamente investigado. En este artículo, se presenta el sistema \"BookWorld\", que integra la socialidad básica de múltiples agentes y funciones para la simulación. El diseño de \"BookWorld\" incluye la complejidad de la realidad, como caracteres dinámicos, un mundo virtual, restricciones geográficas y cambios. \"BookWorld\" ofrece aplicaciones variadas, como la generación de historias, juegos interactivos y simulaciones sociales, expandiendo el amor por obras virtuales y proporcionando nuevas formas de exploración. A través de experimentos extendidos, \"BookWorld\" ha generado simulaciones de alta calidad que se alinean con los libros originales y superan métodos anteriores (Tasa de éxito: 75.36%). El código del artículo está disponible en la página del proyecto: https://bookworld2025.github.io/",
      "upvotes": 13,
      "discussionId": "680863ef3767f6ed7c96a026",
      "ai_keywords": [
        "large language models (LLMs)",
        "social simulation",
        "multi-agent systems",
        "agent societies",
        "personas",
        "book-based",
        "comprehensive real-world intricacies",
        "diverse and dynamic characters",
        "fictional worldviews",
        "geographical constraints",
        "story generation",
        "interactive games",
        "creative, high-quality stories",
        "fidelity to the source books"
      ]
    },
    "publishedAt": "2025-04-20T04:56:27.000Z",
    "title": "BookWorld: From Novels to Interactive Agent Societies for Creative Story\n  Generation",
    "summary": "Recent advances in large language models (LLMs) have enabled social\nsimulation through multi-agent systems. Prior efforts focus on agent societies\ncreated from scratch, assigning agents with newly defined personas. However,\nsimulating established fictional worlds and characters remain largely\nunderexplored, despite its significant practical value. In this paper, we\nintroduce BookWorld, a comprehensive system for constructing and simulating\nbook-based multi-agent societies. BookWorld's design covers comprehensive\nreal-world intricacies, including diverse and dynamic characters, fictional\nworldviews, geographical constraints and changes, e.t.c. BookWorld enables\ndiverse applications including story generation, interactive games and social\nsimulation, offering novel ways to extend and explore beloved fictional works.\nThrough extensive experiments, we demonstrate that BookWorld generates\ncreative, high-quality stories while maintaining fidelity to the source books,\nsurpassing previous methods with a win rate of 75.36%. The code of this paper\ncan be found at the project page: https://bookworld2025.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c7bf2c4524c2aea7eac0b3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7bf2c4524c2aea7eac0b3/5ocZ69MvN4RFv86Aa7ks3.png",
      "fullname": "Xintao Wang",
      "name": "Neph0s",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14992",
      "authors": [
        {
          "_id": "68074ed102571b837f03463c",
          "user": {
            "_id": "64722a616facfb01d8ae8349",
            "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
            "isPro": false,
            "fullname": "Wu Bohong",
            "user": "bongbohong",
            "type": "user"
          },
          "name": "Bohong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:15.811Z",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463d",
          "name": "Shen Yan",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463e",
          "name": "Sijun Zhang",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463f",
          "name": "Jianqiao Lu",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034640",
          "user": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "isPro": false,
            "fullname": "Yutao Zeng",
            "user": "Taoer",
            "type": "user"
          },
          "name": "Yutao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:18.659Z",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034641",
          "name": "Ya Wang",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034642",
          "name": "Xun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T09:41:26.000Z",
      "submittedOnDailyAt": "2025-04-23T00:38:26.026Z",
      "title": "Efficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length Scaling\n\nEfficient Pretraining Length",
      "submittedOnDailyBy": {
        "_id": "64722a616facfb01d8ae8349",
        "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
        "isPro": false,
        "fullname": "Wu Bohong",
        "user": "bongbohong",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los modelos de lenguaje de gran escala ha demostrado el efecto de la escalabilidad de longitud después del entrenamiento, pero la posibilidad de la escalabilidad de longitud antes del entrenamiento aún ha sido poco investigada. Proponemos un nuevo marco de trabajo que permita escalar eficientemente la longitud antes del entrenamiento. Se conoce como PHD-Transformer, y mantiene la eficiencia de la inferencia mientras permite la escalabilidad de longitud antes del entrenamiento. PHD-Transformer introduce una estrategia innovadora de gestión de caché KV y diferencia entre tokens de entrada originales y tokens de decodificación oculta. Para mantener la dependencia a larga distancia, mantiene el único caché KV de los tokens originales y elimina los tokens de decodificación oculta inmediatamente después de su uso, así como el PHD-Transformer de la misma tamaño que el Transformer de BERNEE FORM. Además, introduce dos versiones optimizadas para mejorar el rendimiento: PHD-SWA utiliza atención en ventanas de ventana para mantener la dependencia local, y PHD-CSWA implementa atención en ventanas de bloque para eliminar el crecimiento lineal en el tiempo de prefiltering. Los experimentos detallados muestran mejoras consistentes en varios benchmarks.",
      "upvotes": 12,
      "discussionId": "68074ed202571b837f03468b",
      "ai_keywords": [
        "KV cache",
        "original tokens",
        "hidden decoding tokens",
        "long-range dependencies",
        "local dependencies",
        "sliding window attention",
        "chunk-wise sliding window attention"
      ]
    },
    "publishedAt": "2025-04-21T05:41:26.000Z",
    "title": "Efficient Pretraining Length Scaling",
    "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(PHD-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\nPHD-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: PHD-SWA employs\nsliding window attention to preserve local dependencies, while\nPHD-CSWA implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14992.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64722a616facfb01d8ae8349",
      "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
      "fullname": "Wu Bohong",
      "name": "bongbohong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13820",
      "authors": [
        {
          "_id": "6805ab2c40034a5a792a26b2",
          "user": {
            "_id": "63f1d16fbe95ed4c9a9418fe",
            "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
            "isPro": false,
            "fullname": "Yang Yue",
            "user": "yueyang2000",
            "type": "user"
          },
          "name": "Yang Yue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:59:59.547Z",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b3",
          "name": "Yulin Wang",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b4",
          "name": "Chenxin Tao",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b5",
          "name": "Pan Liu",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b6",
          "name": "Shiji Song",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b7",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T17:50:43.000Z",
      "submittedOnDailyAt": "2025-04-23T01:09:25.550Z",
      "title": "CheXWorld: Modelación de un mundo de imágenes para la representación de imágenes de radiografías - Estudio de aprendizaje",
      "submittedOnDailyBy": {
        "_id": "63f1d16fbe95ed4c9a9418fe",
        "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
        "isPro": false,
        "fullname": "Yang Yue",
        "user": "yueyang2000",
        "type": "user"
      },
      "summary": "El humano puede registrar conocimientos comunes, comprender cómo funciona el mundo y desarrollar un modelo interno del mundo para predecir los resultados de sus acciones. Este concepto ha aparecido en investigaciones previas como un prometedor camino hacia la construcción de módulos generales. Por ejemplo, también en el aprendizaje de representaciones visuales. En este artículo, se presenta el primer intento de un modelo de mundo ajustado automáticamente llamado CheXWorld. En particular, nuestro estudio ha desarrollado un marco integrado que modela simultáneamente las tres dimensiones de conocimientos médicos necesarios para un radiologo: 1) la estructura anatómica local explica las características microestructurales de los tejidos locales (por ejemplo, estructura, forma y textura). 2) la estructura anatómica local explica la estructura del cuerpo humano (por ejemplo, la disposición de órganos y huesos). 3) el cambio de área es el objetivo de que CheXWorld modele la movida de diferentes áreas externas en las imágenes de radiografías (por ejemplo, cambios en brillo, contraste y exposición). Experimentalmente, se diseñó un análisis cualitativo y cuantitativo adecuado, y se demuestra que CheXWorld captura exitosamente estas tres dimensiones de conocimiento médico. Además, los experimentos de aprendizaje transferido en 8 marcos de referencia de clasificación y segmentación de imágenes médicas muestran que CheXWorld supera significativamente los métodos actuales de aprendizaje no supervisado y modelos de gran escala basados en datos médicos. El código y modelos pre-entrenados están disponibles en https://github.com/LeapLabTHU/CheXWorld.",
      "upvotes": 11,
      "discussionId": "6805ab2f40034a5a792a27c8",
      "githubRepo": "https://github.com/LeapLabTHU/CheXWorld",
      "ai_keywords": [
        "CheXWorld",
        "self-supervised world model",
        "radiographic images",
        "local anatomical structures",
        "global anatomical layouts",
        "organs",
        "skeletons",
        "domain variations",
        "medical image classification",
        "medical image segmentation",
        "SSL methods",
        "medical foundation models"
      ]
    },
    "publishedAt": "2025-04-18T13:50:43.000Z",
    "title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation\n  Learning",
    "summary": "Humans can develop internal world models that encode common sense knowledge,\ntelling them how the world works and predicting the consequences of their\nactions. This concept has emerged as a promising direction for establishing\ngeneral-purpose machine-learning models in recent preliminary works, e.g., for\nvisual representation learning. In this paper, we present CheXWorld, the first\neffort towards a self-supervised world model for radiographic images.\nSpecifically, our work develops a unified framework that simultaneously models\nthree aspects of medical knowledge essential for qualified radiologists,\nincluding 1) local anatomical structures describing the fine-grained\ncharacteristics of local tissues (e.g., architectures, shapes, and textures);\n2) global anatomical layouts describing the global organization of the human\nbody (e.g., layouts of organs and skeletons); and 3) domain variations that\nencourage CheXWorld to model the transitions across different appearance\ndomains of radiographs (e.g., varying clarity, contrast, and exposure caused by\ncollecting radiographs from different hospitals, devices, or patients).\nEmpirically, we design tailored qualitative and quantitative analyses,\nrevealing that CheXWorld successfully captures these three dimensions of\nmedical knowledge. Furthermore, transfer learning experiments across eight\nmedical image classification and segmentation benchmarks showcase that\nCheXWorld significantly outperforms existing SSL methods and large-scale\nmedical foundation models. Code & pre-trained models are available at\nhttps://github.com/LeapLabTHU/CheXWorld.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13820.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f1d16fbe95ed4c9a9418fe",
      "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
      "fullname": "Yang Yue",
      "name": "yueyang2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16030",
      "authors": [
        {
          "_id": "68084e2c59762f55a5a8b5f3",
          "name": "Joya Chen",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f4",
          "name": "Ziyun Zeng",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f5",
          "name": "Yiqi Lin",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f6",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f7",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f8",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642435a1a3adbc7142c3b0a6/8JExSHYg9ME-w-L_VUt4W.mp4"
      ],
      "publishedAt": "2025-04-22T16:52:09.000Z",
      "submittedOnDailyAt": "2025-04-23T00:56:24.970Z",
      "title": "LiveCC: Aprendizaje de video con LLM utilizando transcripciones de estrímas de discurso grandes",
      "submittedOnDailyBy": {
        "_id": "642435a1a3adbc7142c3b0a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642435a1a3adbc7142c3b0a6/EBmQ7LnfdTdyuhSUti0-d.png",
        "isPro": true,
        "fullname": "Joya Chen",
        "user": "chenjoya",
        "type": "user"
      },
      "summary": "Recientemente, los modelos de lenguaje y vídeo (Video LLMs) están limitados en su entrenamiento a escala por la alta costo de anotaciones humanas o por el uso de APIs de modelos de propiedad (como GPT-4o) para generar datos de entrenamiento. En este artículo, se investiga el entrenamiento a gran escala de Video LLMs utilizando la traducción automática de voz a texto (ASR) de bajo costo. En particular, se propone un nuevo enfoque de entrenamiento en flujo que cruza densamente palabras de ASR y frame de vídeo según el tiempo de ASR. Comparado con la investigación previa sobre representaciones visuales de lenguaje, nuestro método adapta naturalmente las características de flujo de ASR y permite que el modelo aprenda a modelar lenguaje visual con precisión en el tiempo. Para apoyar el algoritmo de entrenamiento, se introduce una pipeline de generación de datos que procesa vídeos de YouTube y captiones de comentarios (CC, equivalentes a ASR), y se crea el conjunto de datos Live-CC-5M previamente en directorio y el conjunto de datos Live-WhisperX-526K de alta calidad para entrenamiento de retroalimentación controlada (SFT). En particular, el modelo previamente en directorio sin SFT, LiveCC-7B-Base, que solo tiene ASR, muestra nuevas capacidades en comentarios de vídeo en tiempo real y presenta un rendimiento competitivo en la evaluación general de video QA. Para evaluar esto, se diseñó un nuevo benchmark de LiveSports-3K utilizando LLM-as-a-judge. Los experimentos muestran que nuestro modelo final LiveCC-7B-Instruct supera la calidad de comentarios de modelos avanzados de 72B (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) en modo real y obtiene los mejores resultados en los benchmarks de QA de vídeo como Video MME y OVOBench, demostrando la capacidad generalizada extensa de nuestro enfoque. Todos los recursos del artículo están disponibles en https://showlab.github.io/livecc.",
      "upvotes": 8,
      "discussionId": "68084e2f59762f55a5a8b721",
      "projectPage": "https://showlab.github.io/livecc/",
      "githubRepo": "https://github.com/showlab/livecc",
      "ai_keywords": [
        "Video LLMs",
        "automatic speech recognition (ASR)",
        "streaming training",
        "timestamps",
        "vision-language representation",
        "temporally-aligned",
        "fine-grained vision-language modeling",
        "data production pipeline",
        "YouTube videos",
        "closed captions (CC)",
        "Live-CC-5M",
        "Live-WhisperX-526K",
        "supervised fine-tuning (SFT)",
        "general video QA",
        "real-time video commentary",
        "LiveSports-3K benchmark",
        "LLM-as-a-judge",
        "LiveCC-7B-Base",
        "LiveCC-7B-Instruct",
        "Qwen2.5-VL-72B-Instruct",
        "LLaVA-Video-72B",
        "VideoMME",
        "OVOBench"
      ]
    },
    "publishedAt": "2025-04-22T12:52:09.000Z",
    "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
    "summary": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642435a1a3adbc7142c3b0a6/8JExSHYg9ME-w-L_VUt4W.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16030.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642435a1a3adbc7142c3b0a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642435a1a3adbc7142c3b0a6/EBmQ7LnfdTdyuhSUti0-d.png",
      "fullname": "Joya Chen",
      "name": "chenjoya",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15681",
      "authors": [
        {
          "_id": "680846defa5a6cc6bd9d2cf3",
          "name": "Vidi Team",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf4",
          "name": "Celong Liu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf5",
          "name": "Chia-Wen Kuo",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf6",
          "user": {
            "_id": "6476af4402fc644c810b29a2",
            "avatarUrl": "/avatars/68aefabe6b000443f4601137e6672187.svg",
            "isPro": false,
            "fullname": "Dawei Du",
            "user": "daviddousa",
            "type": "user"
          },
          "name": "Dawei Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:04.152Z",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf7",
          "name": "Fan Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf8",
          "name": "Guang Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf9",
          "name": "Jiamin Yuan",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfa",
          "name": "Lingxi Zhang",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfb",
          "name": "Lu Guo",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfc",
          "name": "Lusha Li",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfd",
          "name": "Longyin Wen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfe",
          "name": "Qingyu Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cff",
          "name": "Rachel Deng",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d00",
          "name": "Sijie Zhu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d01",
          "name": "Stuart Siew",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d02",
          "name": "Tong Jin",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d03",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d04",
          "name": "Wen Zhong",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d05",
          "name": "Xiaohui Shen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d06",
          "name": "Xin Gu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d07",
          "name": "Xing Mei",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d08",
          "name": "Xueqiong Qu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T08:04:45.000Z",
      "submittedOnDailyAt": "2025-04-23T00:19:34.185Z",
      "title": "Vidi: Comprensión y edición de películas en modelos de grandes escalas multimodelo",
      "submittedOnDailyBy": {
        "_id": "65cbdea6d6c974694f09249a",
        "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
        "isPro": false,
        "fullname": "Jay",
        "user": "Zilence006",
        "type": "user"
      },
      "summary": "La humanidad establece relaciones cercanas con la naturaleza con aquellos que la conocen y comparten información e imágenes, que son principales medios de comunicación y expresión en la internet. Para apoyar la producción de contenido de alta calidad en imágenes de gran escala, los sistemas de pipeline actuales requieren una comprensión detallada de datos vivos (por ejemplo, imágenes capturadas por cámaras sin editar) y componentes de edición (por ejemplo, efectos visuales). En el caso de edición de imágenes, el modelo debe tener un conocimiento fuerte, procesar longitudes de entrada flexibles (por ejemplo, imágenes de un hora de vida) y esto es un gran desafío frente a los modelos tradicionales. En este informe, se presenta a la familia de modelos de gran diversidad (LMMs) llamados Vidi, diseñados para apoyar una amplia comprensión e edición de imágenes. La primera versión de Vidi se centra en la búsqueda temporal (temporal retrieval), es decir, en determinar el rango temporal de las imágenes de entrada que corresponden a una consulta de texto, lo que es fundamental para realizar ediciones inteligentes. Este modelo tiene una fuerte capacidad para comprender el tiempo y puede buscar rangos temporales específicos para una consulta. Para apoyar una evaluación detallada en escaneos reales, se presenta el benchmark VUE-TR, que tiene cinco mejoras: 1) longitud de la imagen: un conjunto de datos de búsqueda temporal más largo, 2) soporte de voz: consultas basadas en voz, 3) forma de la consulta: varias longitudes y formatos de consulta, 4) calidad de las anotaciones: las zonas de tiempo se anotan manualmente, 5) métrica de evaluación: mejora del métrico IoU para evaluar múltiples rangos de tiempo. Sorprendentemente, Vidi supera significativamente a los modelos líder en tareas de búsqueda temporal (por ejemplo, GPT-4o y Gemini) y muestra excelentes resultados en el escaneo de edición de imágenes.",
      "upvotes": 7,
      "discussionId": "680846dffa5a6cc6bd9d2d59",
      "ai_keywords": [
        "Vidi",
        "Large Multimodal Models (LMMs)",
        "temporal retrieval",
        "video editing scenarios",
        "temporal understanding",
        "VUE-TR benchmark",
        "IoU metric"
      ]
    },
    "publishedAt": "2025-04-22T04:04:45.000Z",
    "title": "Vidi: Large Multimodal Models for Video Understanding and Editing",
    "summary": "Humans naturally share information with those they are connected to, and\nvideo has become one of the dominant mediums for communication and expression\non the Internet. To support the creation of high-quality large-scale video\ncontent, a modern pipeline requires a comprehensive understanding of both the\nraw input materials (e.g., the unedited footage captured by cameras) and the\nediting components (e.g., visual effects). In video editing scenarios, models\nmust process multiple modalities (e.g., vision, audio, text) with strong\nbackground knowledge and handle flexible input lengths (e.g., hour-long raw\nvideos), which poses significant challenges for traditional models. In this\nreport, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a\nwide range of video understand editing scenarios. The first release focuses on\ntemporal retrieval, i.e., identifying the time ranges within the input videos\ncorresponding to a given text query, which plays a critical role in intelligent\nediting. The model is capable of processing hour-long videos with strong\ntemporal understanding capability, e.g., retrieve time ranges for certain\nqueries. To support a comprehensive evaluation in real-world scenarios, we also\npresent the VUE-TR benchmark, which introduces five key advancements. 1) Video\nduration: significantly longer than existing temporal retrival datasets, 2)\nAudio support: includes audio-based queries, 3) Query format: diverse query\nlengths/formats, 4) Annotation quality: ground-truth time ranges are manually\nannotated. 5) Evaluation metric: a refined IoU metric to support evaluation\nover multiple time ranges. Remarkably, Vidi significantly outperforms leading\nproprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,\nindicating its superiority in video editing scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cbdea6d6c974694f09249a",
      "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
      "fullname": "Jay",
      "name": "Zilence006",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16080",
      "authors": [
        {
          "_id": "680845d997f32b8ffc13569c",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569d",
          "name": "Liangbing Zhao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569e",
          "name": "Sayak Paul",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569f",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a0",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a1",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a2",
          "name": "Peng Gao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a3",
          "name": "Mohamed Elhoseiny",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a4",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5f7fbd813e94f16a85448745/L_S4ww0dm1-ZUiNRLQm_h.png"
      ],
      "publishedAt": "2025-04-22T17:58:07.000Z",
      "submittedOnDailyAt": "2025-04-23T00:16:04.186Z",
      "title": "「Optimización de la expansión bidireccional: Optimización de la propagación de modelos de expansión de texto a imágenes durante la inferencia」",
      "submittedOnDailyBy": {
        "_id": "5f7fbd813e94f16a85448745",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
        "isPro": false,
        "fullname": "Sayak Paul",
        "user": "sayakpaul",
        "type": "user"
      },
      "summary": "Recientemente, modelos de difusión a imágenes han logrado una calidad visual impresionante a través de la expansión de datos de entrenamiento y parámetros del modelo, pero presentan dificultades con escenarios complejos y detalles mínimos. Basándonos en la capacidad de auto-reflexión que se observa en grandes modelos de lenguaje, proponemos la ReflectionFlow. Este es un flujo de trabajo para la inferencia que permite a los modelos de difusión reflexionar y mejorar su salida. La ReflectionFlow introduce tres enfoques de expansión durante la inferencia: (1) la expansión del nivel de ruido optimiza las posibles inicializaciones; (2) la expansión del nivel de prompt proporciona una guía significativamente precisa; (3) particularmente, la expansión del nivel de reflexión claramente define las acciones que permiten reflexionar y modificar la generación previa. Para fomentar la expansión del nivel de reflexión, construimos el GenRef, que consiste en un conjunto de datos de 1 millón de tuplas, incluyendo imágenes de reflexión, imágenes con defectos y imágenes de apertura. Utilizando este conjunto de datos, realizamos la reflexión de inferencia adecuadamente en el modelo de difusión transformador FLUX.1-dev, uno de los líderes en el campo. Los resultados de los experimentos muestran que la ReflectionFlow supera significativamente un método simple de expansión del nivel de ruido, proporcionando una solución eficiente y calculable para la síntesis de imágenes de alta calidad en tareas difíciles.",
      "upvotes": 5,
      "discussionId": "680845de97f32b8ffc1357c7",
      "ai_keywords": [
        "text-to-image diffusion models",
        "visual quality",
        "training data",
        "model parameters",
        "self-reflection capabilities",
        "diffusion models",
        "inference-time framework",
        "noise-level scaling",
        "latent initialization",
        "prompt-level scaling",
        "semantic guidance",
        "reflection-level scaling",
        "GenRef",
        "multimodal inputs",
        "unified framework",
        "image synthesis"
      ]
    },
    "publishedAt": "2025-04-22T13:58:07.000Z",
    "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for\n  Text-to-Image Diffusion Models via Reflection Tuning",
    "summary": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5f7fbd813e94f16a85448745/L_S4ww0dm1-ZUiNRLQm_h.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16080.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f7fbd813e94f16a85448745",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
      "fullname": "Sayak Paul",
      "name": "sayakpaul",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 605
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16078",
      "authors": [
        {
          "_id": "68087a231e425a6eee93570d",
          "name": "Thomas Schmied",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee93570e",
          "name": "Jörg Bornschein",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee93570f",
          "name": "Jordi Grau-Moya",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee935710",
          "name": "Markus Wulfmeier",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee935711",
          "name": "Razvan Pascanu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:57:14.000Z",
      "submittedOnDailyAt": "2025-04-23T03:57:30.931Z",
      "title": "Los LLMs tienen el efecto del extremo de la ayuda: el impacto sobre la decisión mediante la micro-ajuste de la RL.",
      "submittedOnDailyBy": {
        "_id": "64c3849269b1a6796052eac7",
        "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
        "isPro": false,
        "fullname": "Thomas Schmied",
        "user": "thomasschmied",
        "type": "user"
      },
      "summary": "El éxito de los LLM ha aumentado rápidamente el interés por aplicaciones basadas en agentes diversos. La principal asunción es que los LLM puedan explorar y resolver áreas complejas eficazmente utilizando sentimientos comunes y inferencia de la cadena de pensamiento (CoT). Sin embargo, los agentes basados en LLM enfrentan desafíos debido a la diferencia entre la exploración óptima y el conocimiento necesario para acciones reales (gapo conocimiento-haciendo). En este estudio, se investiga sistemáticamente las razones por las que los LLM no muestran un rendimiento óptimo en escenarios de decisión. Se enfoca particularmente en tres modos de fallo comunes: el descuido, la sesgo de frecuencia y el gapo conocimiento-haciendo. Se propone un método de mejora utilizando aprendizaje por refuerzo (RL) basado en razones de CoT autogeneradas. Los experimentos en diferentes bandos (multi-armed bandits, contextual bandits) y Tic-tac-toe muestran que la mejora por RL aumenta la exploración y reduce la distancia entre el conocimiento y las acciones reales, mejorando la capacidad de decisión de los LLM. Finalmente, se combina un episodio vacío y el modo de acceso propio del LLM (auto-mejora, auto-coherencia) para mejorar aún más la mejora de los LLM.",
      "upvotes": 5,
      "discussionId": "68087a241e425a6eee93576b",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Chain-of-Thought (CoT) reasoning",
        "sub-optimal exploration",
        "knowing-doing gap",
        "decision-making scenarios",
        "greediness",
        "frequency bias",
        "fine-tuning",
        "Reinforcement Learning (RL)",
        "self-generated CoT rationales",
        "multi-armed bandits",
        "contextual bandits",
        "Tic-tac-toe",
        "$\\epsilon$-greedy",
        "self-correction",
        "self-consistency"
      ]
    },
    "publishedAt": "2025-04-22T13:57:14.000Z",
    "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making\n  Abilities",
    "summary": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\nepsilon-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16078.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3849269b1a6796052eac7",
      "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
      "fullname": "Thomas Schmied",
      "name": "thomasschmied",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15785",
      "authors": [
        {
          "_id": "6808579f91ba7dbcc19dbd3e",
          "name": "Siyu Zhou",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd3f",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:49.077Z",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd40",
          "name": "Yijun Yang",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd41",
          "name": "Guodong Long",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd42",
          "name": "Deheng Ye",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd43",
          "name": "Jing Jiang",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd44",
          "name": "Chengqi Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ESSjdqSUTzyiFhpuhfUhO.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/-ScBiECcYZw2_83Qu3-29.png"
      ],
      "publishedAt": "2025-04-22T10:58:27.000Z",
      "submittedOnDailyAt": "2025-04-23T01:40:28.955Z",
      "title": "WALL-E 2.0: El aprendizaje por refuerzo neurodinámico realiza la gestión del mundo, mejorando las inteligencias artificiales basadas en modelos de mundo.",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "¿Es posible construir un modelo mundial preciso en los LLMs? ¿Y ¿qué ventajas ofrecería un modelo mundial a un agente de LLM? La diferencia entre el conocimiento previo de los LLMs y la dinámica de un entorno específico a menudo es la causa de un impacto negativo en el rendimiento. Para mitigar este efecto, se propone una metodología de entrenamiento sin límites llamada \"ajuste del mundo\", que permite que los LLMs aprendan indirectamente sobre el entorno. El conocimiento objetivo incluye reglas de acción, grafos y grafos de escala, que los LLMs extraen de los flujos de exploración y codifican en código ejecutable para ajustar la política de los agentes de LLM. Además, se propone un agente de RL sin límites basado en modelos, llamado WALL-E 2.0, a través del marco de MPC para control por predicción del modelo, en contraste con el MPC tradicional que requiere cálculos real-time costosos, los agentes de LLM interactúan con el mundo cognitivo y optimizan eficientemente las acciones futuras. La fuerte heurística de los agentes de LLM actúa como un planificador eficiente en MPC, garantizando la calidad de las acciones planificadas según la predicción precisa del modelo mundial. Esto puede significativamente mejorar la eficiencia de aprendizaje en nuevos entornos. En desafíos de un mundo abierto más amplio que Minecraft, como el de Marte (Minecraft más abierto) y ALFWorld (entornos interiores estructurados), WALL-E 2.0 supera los métodos actuales notablemente. En Marte, logra un porcentaje de éxito del 16.1% a 51.6% más que el estándar y mejora la puntuación en más del 61.7%. En ALFWorld, logra 98% de nuevos enlaces en cuatro iteraciones.",
      "upvotes": 5,
      "discussionId": "680857a191ba7dbcc19dbda6",
      "githubRepo": "https://github.com/elated-sawyer/WALL-E",
      "ai_keywords": [
        "world models",
        "large language models (LLMs)",
        "symbolic knowledge",
        "knowledge graphs",
        "scene graphs",
        "exploration trajectories",
        "executable codes",
        "policies",
        "RL-free",
        "model-based agent",
        "WALL-E 2.0",
        "model-predictive control (MPC)",
        "neurosymbolic world model",
        "look-ahead optimizer",
        "heuristics",
        "planner",
        "predictions",
        "learning efficiency",
        "open-world challenges",
        "Mars (Minecraft like)",
        "ALFWorld (embodied indoor environments)",
        "success rate",
        "score"
      ]
    },
    "publishedAt": "2025-04-22T06:58:27.000Z",
    "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\n  Model-based LLM Agents",
    "summary": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ESSjdqSUTzyiFhpuhfUhO.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/-ScBiECcYZw2_83Qu3-29.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15785.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16082",
      "authors": [
        {
          "_id": "6808486f043aa415b647ca77",
          "name": "Ziqi Pang",
          "hidden": false
        },
        {
          "_id": "6808486f043aa415b647ca78",
          "name": "Yu-Xiong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:59:41.000Z",
      "submittedOnDailyAt": "2025-04-23T00:25:18.841Z",
      "title": "Master Video: \"MapReduce\" es una base de principios básicos para entender un largo video.",
      "submittedOnDailyBy": {
        "_id": "642a33ea5673845d9854f458",
        "avatarUrl": "/avatars/ea8ed29a0b4ad629a2ba6e7d26cbd923.svg",
        "isPro": false,
        "fullname": "Ziqi Pang",
        "user": "ziqipang",
        "type": "user"
      },
      "summary": "El Manejador Video es una arquitectura efectiva para entender largos videos. Esta arquitectura muestra una versión sencilla y efectiva del principio MapReduce en el procesamiento de largos videos. (1) Map: Reconoce de manera densa cortos videos en secuencias independientes. (2) Reduce: Agrupa la información de todos los cortos videos. Comparado con modelos de lenguaje visual (VLMs) que se basan en secuencias, el Manejador Video realiza un reconocimiento detallado de cortos videos independientemente de la longitud del contexto. En contraste con agentes de video existentes, el Manejador Video no depende de la selección secuencial de segmentos clave, sino que permite un reconocimiento de segmentos cortos de videos de manera ordenada y escalable mediante la operación Map. En el paso Reduce, se realiza una agregación más detallada del contexto y se justifican los motivos, superando la búsqueda explícita de segmentos clave. Este principio de MapReduce también es aplicable a VLMs y agentes de video. Se prueba su eficiencia utilizando agentes de lenguaje largo (LLM).\n\nEn realidad, el Manejador Video utiliza dos etapas de MapReduce. (A) Capturación: Genera capturas de cortos videos y (map) normaliza las palabras y objetos repetidos con nombres comunes (reduce); (B) Análisis: Analiza la información relacionada de cortos videos según las preguntas del usuario y los integra en la respuesta final (map), (reduce). Comparado con VLMs y agentes de video, el Manejador Video logra un aumento de precisión del 10% o más en el LVBench, un benchmark difícil.\n\nEl código está disponible en la siguiente URL: https://github.com/ziqipang/MR-Video",
      "upvotes": 3,
      "discussionId": "68084870043aa415b647caaf",
      "ai_keywords": [
        "MapReduce",
        "short video clips",
        "sequence-to-sequence vision-language models (VLMs)",
        "sequence parallel perception",
        "context aggregation",
        "context reasoning",
        "key segment selection",
        "key segment retrieval",
        "Captioning",
        "standardizing",
        "repeated characters",
        "shared names",
        "Analysis",
        "relevant information",
        "final answer",
        "LVBench"
      ]
    },
    "publishedAt": "2025-04-22T13:59:41.000Z",
    "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding",
    "summary": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16082.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642a33ea5673845d9854f458",
      "avatarUrl": "/avatars/ea8ed29a0b4ad629a2ba6e7d26cbd923.svg",
      "fullname": "Ziqi Pang",
      "name": "ziqipang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11703",
      "authors": [
        {
          "_id": "6800a4f9f16f9f820ed748af",
          "user": {
            "_id": "64f27f74f1b6c235aed4b904",
            "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
            "isPro": false,
            "fullname": "stneng",
            "user": "stneng",
            "type": "user"
          },
          "name": "Tianneng Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T10:13:56.123Z",
          "hidden": true
        },
        {
          "_id": "6800a4f9f16f9f820ed748b0",
          "name": "Jingxuan He",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b1",
          "name": "Zhun Wang",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b2",
          "name": "Linyu Wu",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b3",
          "name": "Hongwei Li",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b4",
          "name": "Wenbo Guo",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b5",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T01:58:40.000Z",
      "submittedOnDailyAt": "2025-04-23T00:27:37.099Z",
      "title": "Progent: Limitación de defensa contra restricciones de derechos de programación en LLM Agent",
      "submittedOnDailyBy": {
        "_id": "64f27f74f1b6c235aed4b904",
        "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
        "isPro": false,
        "fullname": "stneng",
        "user": "stneng",
        "type": "user"
      },
      "summary": "El LLM Agent es un nuevo proceso final que completa tareas asignadas a un usuario utilizando diferentes herramientas, con el LLM como componente esencial. Sin embargo, el LLM Agent tiene un alto riesgo de seguridad. Al interactuar con el mundo exterior, puede recibir instrucciones maliciosas que lo induzcan a realizar acciones peligrosas. Para resolver esto, limitar los permisos es una buena estrategia. Permitir solo lo necesario para completar la tarea y bloquear lo innecesario. Sin embargo, al hacerlo, es necesario mantener tanto la seguridad como la utilidad. Esto es difícil.\n\nPresentamos por primera vez a Progent, el primer controlador de permisos. El punto clave es el uso de un lenguaje de dominio específico para expresar políticas de restricción de permisos que se aplican durante la ejecución del agente. Estas políticas imponen restricciones específicas sobre las llamadas a herramientas y deciden si una llamada es permitida o no, designando un backup por defecto en caso de no ser permitida. De esta manera, los desarrolladores de agentes y usuarios pueden escribir y ejecutar políticas que se adapten a casos de uso específicos, asegurando así la seguridad. El diseño modular de Progent permite su integración sin modificar el interior del agente, con solo necesarios cambios en la implementación del agente y mejorando la practicidad y la amplia capacidad. Se puede automatizar la creación de políticas utilizando el LLM para generar políticas basadas en preguntas del usuario y actualizarlas dinámicamente para mejorar la seguridad y la utilidad. Según nuestra evaluación ampliada, AgentDojo, ASB y AgentPoison, en tres escenarios y benchmarks diferentes, se muestran capaces de mantener una seguridad fuerte y una alta utilidad. Además, se analiza en detalle el efecto de los componentes clave y la correspondencia con la generación automática de políticas, demostrando su resistencia frente a ataques.",
      "upvotes": 3,
      "discussionId": "6800a4faf16f9f820ed748ee",
      "ai_keywords": [
        "LLM agents",
        "large language models (LLMs)",
        "principle of least privilege",
        "privilege control mechanism",
        "domain-specific language",
        "privilege control policies",
        "tool calls",
        "agent execution",
        "fine-grained constraints",
        "fallbacks",
        "security",
        "utility",
        "policy writing",
        "automated policy generation",
        "AgentDojo",
        "ASB",
        "AgentPoison"
      ]
    },
    "publishedAt": "2025-04-15T21:58:40.000Z",
    "title": "Progent: Programmable Privilege Control for LLM Agents",
    "summary": "LLM agents are an emerging form of AI systems where large language models\n(LLMs) serve as the central component, utilizing a diverse set of tools to\ncomplete user-assigned tasks. Despite their great potential, LLM agents pose\nsignificant security risks. When interacting with the external world, they may\nencounter malicious commands from attackers, leading to the execution of\ndangerous actions. A promising way to address this is by enforcing the\nprinciple of least privilege: allowing only essential actions for task\ncompletion while blocking unnecessary ones. However, achieving this is\nchallenging, as it requires covering diverse agent scenarios while preserving\nboth security and utility.\n  We introduce Progent, the first privilege control mechanism for LLM agents.\nAt its core is a domain-specific language for flexibly expressing privilege\ncontrol policies applied during agent execution. These policies provide\nfine-grained constraints over tool calls, deciding when tool calls are\npermissible and specifying fallbacks if they are not. This enables agent\ndevelopers and users to craft suitable policies for their specific use cases\nand enforce them deterministically to guarantee security. Thanks to its modular\ndesign, integrating Progent does not alter agent internals and requires only\nminimal changes to agent implementation, enhancing its practicality and\npotential for widespread adoption. To automate policy writing, we leverage LLMs\nto generate policies based on user queries, which are then updated dynamically\nfor improved security and utility. Our extensive evaluation shows that it\nenables strong security while preserving high utility across three distinct\nscenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we\nperform an in-depth analysis, showcasing the effectiveness of its core\ncomponents and the resilience of its automated policy generation against\nadaptive attacks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f27f74f1b6c235aed4b904",
      "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
      "fullname": "stneng",
      "name": "stneng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14977",
      "authors": [
        {
          "_id": "68084dbc2eff5d45775d8f14",
          "name": "Jingkai Zhou",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f15",
          "name": "Yifan Wu",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f16",
          "name": "Shikai Li",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f17",
          "name": "Min Wei",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f18",
          "name": "Chao Fan",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f19",
          "name": "Weihua Chen",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f1a",
          "name": "Wei Jiang",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f1b",
          "name": "Fan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T09:09:21.000Z",
      "submittedOnDailyAt": "2025-04-23T00:49:31.236Z",
      "title": "RealisDance-DiT: Construye un sencillo y potente estándar para animaciones de personajes controlables en la naturaleza",
      "submittedOnDailyBy": {
        "_id": "6434caa64b34368fdb07da48",
        "avatarUrl": "/avatars/8da3d3d1e274ff4ad409234678b1b952.svg",
        "isPro": false,
        "fullname": "Jingkai Zhou",
        "user": "theFoxofSky",
        "type": "user"
      },
      "summary": "La animación de personajes controlables es particularmente rara, especializada en personajes y interacción con objetos, iluminación compleja y escenarios dinámicos, lo que genera problemas difíciles. Para abordar estos, los estudios previos han utilizado principalmente redes bipasaje complejas y introducido guías para la postura y apariencia. Sin embargo, generalmente tienen dificultades para generalizar en escenarios abiertos. En este artículo, proponemos una nueva perspectiva para resolver estos problemas, utilizando un modelo base suficientemente potente y una estrategia flexible de ajuste. Específicamente, presentamos RealisDance-DiT, un modelo construido en base al modelo Wan-2.1 de vídeo. Nuestra análisis exhaustivo muestra que el diseño de la Red de Referencia, ampliamente utilizado, no es óptimo para grandes modelos DiT. En su lugar, realizamos un ajuste mínimo en la arquitectura del modelo base y demostramos un fuerte base de referencia. Además, proponemos un entrenamiento con bajo ruido y la estrategia \"grandes batches y pequeñas iteraciones\" para acelerar la convergencia del modelo en el ajuste y preservar los mejores características del modelo base. Además, presentamos un nuevo conjunto de datos de prueba para abordar diversos desafíos reales, complementando los benchmarks existentes como el de TikTok o el de videos de moda de UBC. A través de una amplia gama de experimentos, se demuestra que RealisDance-DiT supera significativamente los métodos actuales.",
      "upvotes": 2,
      "discussionId": "68084dc02eff5d45775d902c",
      "projectPage": "https://thefoxofsky.github.io/project_pages/RealisDance-DiT/index",
      "githubRepo": "https://github.com/damo-cv/RealisDance",
      "ai_keywords": [
        "RealisDance-DiT",
        "Wan-2.1 video foundation model",
        "Reference Net design",
        "DiT models",
        "low-noise warmup",
        "large batches and small iterations strategies",
        "foundation model architecture",
        "model convergence",
        "priors of the foundation model",
        "TikTok dataset",
        "UBC fashion video dataset"
      ]
    },
    "publishedAt": "2025-04-21T05:09:21.000Z",
    "title": "RealisDance-DiT: Simple yet Strong Baseline towards Controllable\n  Character Animation in the Wild",
    "summary": "Controllable character animation remains a challenging problem, particularly\nin handling rare poses, stylized characters, character-object interactions,\ncomplex illumination, and dynamic scenes. To tackle these issues, prior work\nhas largely focused on injecting pose and appearance guidance via elaborate\nbypass networks, but often struggles to generalize to open-world scenarios. In\nthis paper, we propose a new perspective that, as long as the foundation model\nis powerful enough, straightforward model modifications with flexible\nfine-tuning strategies can largely address the above challenges, taking a step\ntowards controllable character animation in the wild. Specifically, we\nintroduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our\nsufficient analysis reveals that the widely adopted Reference Net design is\nsuboptimal for large-scale DiT models. Instead, we demonstrate that minimal\nmodifications to the foundation model architecture yield a surprisingly strong\nbaseline. We further propose the low-noise warmup and \"large batches and small\niterations\" strategies to accelerate model convergence during fine-tuning while\nmaximally preserving the priors of the foundation model. In addition, we\nintroduce a new test dataset that captures diverse real-world challenges,\ncomplementing existing benchmarks such as TikTok dataset and UBC fashion video\ndataset, to comprehensively evaluate the proposed method. Extensive experiments\nshow that RealisDance-DiT outperforms existing methods by a large margin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6434caa64b34368fdb07da48",
      "avatarUrl": "/avatars/8da3d3d1e274ff4ad409234678b1b952.svg",
      "fullname": "Jingkai Zhou",
      "name": "theFoxofSky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15524",
      "authors": [
        {
          "_id": "68084b46fa5a6cc6bd9e6a83",
          "user": {
            "_id": "64560618bfdf9c63ce2d658a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
            "isPro": false,
            "fullname": "Mathsion Wong",
            "user": "QiYao-Wang",
            "type": "user"
          },
          "name": "Qiyao Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:53.885Z",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a84",
          "name": "Guhong Chen",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a85",
          "name": "Hongbo Wang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a86",
          "name": "Huaren Liu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a87",
          "name": "Minghui Zhu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a88",
          "name": "Zhifei Qin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a89",
          "name": "Linwei Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8a",
          "name": "Yilin Yue",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8b",
          "name": "Shiqiang Wang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8c",
          "name": "Jiayan Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8d",
          "name": "Yihang Wu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8e",
          "name": "Ziqiang Liu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8f",
          "name": "Longze Chen",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a90",
          "name": "Run Luo",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a91",
          "name": "Liyang Fan",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a92",
          "name": "Jiaming Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a93",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a94",
          "name": "Kan Xu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a95",
          "name": "Hongfei Lin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a96",
          "name": "Hamid Alinejad-Rokny",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a97",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a98",
          "name": "Yuan Lin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a99",
          "name": "Min Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T02:00:41.000Z",
      "submittedOnDailyAt": "2025-04-23T05:32:33.756Z",
      "title": "IPBench: Marco de prueba del conocimiento de modelos de lenguaje de gran escala sobre la propiedad intelectual",
      "submittedOnDailyBy": {
        "_id": "64560618bfdf9c63ce2d658a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
        "isPro": false,
        "fullname": "Mathsion Wong",
        "user": "QiYao-Wang",
        "type": "user"
      },
      "summary": "Los derechos de propiedad intelectual (DPI) son un ámbito especial con una complejidad única y una concentración de conocimiento, integrando conocimientos técnicos y legales. A medida que los modelos de lenguaje grandes (LLMs) se desarrollan, tienen un gran potencial en el trabajo de DPI, permitiendo una mejor análisis, comprensión y generación de contenido relacionado con DPI de manera más eficiente. Sin embargo, los conjuntos de datos y marcos de referencia actuales se centran principalmente en patentes amplias o en partes específicas del campo de DPI, no siendo adecuados para escenarios reales. Para llenar esta brecha, se presenta IPBench, el primer marco de referencia multilingüe práctico que aborda 8 instituciones de DPI y 20 trabajos. Este marco de referencia está diseñado para evaluar modelos de infraestructura de conocimiento reales, incluyendo tanto comprensión como generación. Se evaluaron 16 modelos de LLMs, abarcando un rango desde modelos generales hasta modelos especializados en el campo de DPI. El mejor modelo alcanzó una precisión del 75.8%, demostrando una gran mejora. En particular, los modelos abierto-source y especializados en DPI ejecutan mejores resultados que los modelos generales cerrado-source. Se publican todos los datos y códigos de IPBench, y se continuará con la extensión de trabajos relacionados con DPI para reflejar más precisamente los desafíos de una infraestructura de conocimiento real.",
      "upvotes": 1,
      "discussionId": "68084b4cfa5a6cc6bd9e6c75",
      "projectPage": "https://ipbench.github.io/",
      "githubRepo": "https://github.com/IPBench/IPBench"
    },
    "publishedAt": "2025-04-21T22:00:41.000Z",
    "title": "IPBench: Benchmarking the Knowledge of Large Language Models in\n  Intellectual Property",
    "summary": "Intellectual Property (IP) is a unique domain that integrates technical and\nlegal knowledge, making it inherently complex and knowledge-intensive. As large\nlanguage models (LLMs) continue to advance, they show great potential for\nprocessing IP tasks, enabling more efficient analysis, understanding, and\ngeneration of IP-related content. However, existing datasets and benchmarks\neither focus narrowly on patents or cover limited aspects of the IP field,\nlacking alignment with real-world scenarios. To bridge this gap, we introduce\nthe first comprehensive IP task taxonomy and a large, diverse bilingual\nbenchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is\ndesigned to evaluate LLMs in real-world intellectual property applications,\nencompassing both understanding and generation. We benchmark 16 LLMs, ranging\nfrom general-purpose to domain-specific models, and find that even the\nbest-performing model achieves only 75.8% accuracy, revealing substantial room\nfor improvement. Notably, open-source IP and law-oriented models lag behind\nclosed-source general-purpose models. We publicly release all data and code of\nIPBench and will continue to update it with additional IP-related tasks to\nbetter reflect real-world challenges in the intellectual property domain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15524.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64560618bfdf9c63ce2d658a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
      "fullname": "Mathsion Wong",
      "name": "QiYao-Wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14735",
      "authors": [
        {
          "_id": "6807f85efcd784a902c87126",
          "user": {
            "_id": "621d85a10e35b2fbbf3e6196",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
            "isPro": false,
            "fullname": "Chin-Yun Yu",
            "user": "yoyolicoris",
            "type": "user"
          },
          "name": "Chin-Yun Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:18.913Z",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c87127",
          "name": "Marco A. Martínez-Ramírez",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c87128",
          "name": "Junghyun Koo",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c87129",
          "name": "Ben Hayes",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c8712a",
          "name": "Wei-Hsiang Liao",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c8712b",
          "name": "György Fazekas",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c8712c",
          "name": "Yuki Mitsufuji",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/621d85a10e35b2fbbf3e6196/alz9iP58RZiY22SYNBEzr.png"
      ],
      "publishedAt": "2025-04-20T20:52:58.000Z",
      "submittedOnDailyAt": "2025-04-23T08:05:27.097Z",
      "title": "DiffVox: Modelo diferencial para la distribución efectiva de expertos",
      "submittedOnDailyBy": {
        "_id": "621d85a10e35b2fbbf3e6196",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
        "isPro": false,
        "fullname": "Chin-Yun Yu",
        "user": "yoyolicoris",
        "type": "user"
      },
      "summary": "En este estudio, se presenta un nuevo modelo interpretable \"DiffVox\" para la asignación efectiva de sonidos en la creación de música. DiffVox es una abreviatura de \"Differentiable Vocal Fx\", que integra los efectos de retroalimentación de parámetros, control de rango dinámico, desfade y reverberación mediante implementaciones válidas de diferenciación, permitiendo así la optimización basada en gradientes en la evaluación de parámetros. Los presets de sonidos se buscan en MedleyDB con 70 trabajos y en colecciones personales con 365 trabajos. El análisis de correlación de parámetros muestra claramente que el filtro de dolor y el filtro de bajo son necesarios para formar el sonido, y que el tiempo de desfade y la intensidad del desfade se correlacionan. En el análisis de componentes principales, se demuestra la dimensión de color de McAdams, donde el componente más importante controla la espacialidad observada y el segundo componente afecta la brillo del espectro. En la prueba estadística, se confirma la no normalidad de la distribución de parámetros y se subraya la complejidad del espacio de efectos sonoros. Estos hallazgos iniciales constituyen la base para futuras investigaciones en la modelación de efectos sonoros y en la automatización de mezcla. El código fuente y los conjuntos de datos están disponibles en https://github.com/SonyResearch/diffvox.",
      "upvotes": 0,
      "discussionId": "6807f860fcd784a902c87194",
      "githubRepo": "https://github.com/SonyResearch/diffvox",
      "ai_keywords": [
        "parametric equalisation",
        "dynamic range control",
        "delay",
        "reverb",
        "differentiable implementations",
        "gradient-based optimisation",
        "parameter estimation",
        "principal component analysis",
        "McAdams' timbre dimensions",
        "perceived spaciousness",
        "spectral brightness",
        "non-Gaussian nature"
      ]
    },
    "publishedAt": "2025-04-20T16:52:58.000Z",
    "title": "DiffVox: A Differentiable Model for Capturing and Analysing Professional\n  Effects Distributions",
    "summary": "This study introduces a novel and interpretable model, DiffVox, for matching\nvocal effects in music production. DiffVox, short for ``Differentiable Vocal\nFx\", integrates parametric equalisation, dynamic range control, delay, and\nreverb with efficient differentiable implementations to enable gradient-based\noptimisation for parameter estimation. Vocal presets are retrieved from two\ndatasets, comprising 70 tracks from MedleyDB and 365 tracks from a private\ncollection. Analysis of parameter correlations highlights strong relationships\nbetween effects and parameters, such as the high-pass and low-shelf filters\noften behaving together to shape the low end, and the delay time correlates\nwith the intensity of the delayed signals. Principal component analysis reveals\nconnections to McAdams' timbre dimensions, where the most crucial component\nmodulates the perceived spaciousness while the secondary components influence\nspectral brightness. Statistical testing confirms the non-Gaussian nature of\nthe parameter distribution, highlighting the complexity of the vocal effects\nspace. These initial findings on the parameter distributions set the foundation\nfor future research in vocal effects modelling and automatic mixing. Our source\ncode and datasets are accessible at https://github.com/SonyResearch/diffvox.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/621d85a10e35b2fbbf3e6196/alz9iP58RZiY22SYNBEzr.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "621d85a10e35b2fbbf3e6196",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
      "fullname": "Chin-Yun Yu",
      "name": "yoyolicoris",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]