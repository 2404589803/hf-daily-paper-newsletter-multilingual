[
  {
    "paper": {
      "id": "2505.24864",
      "authors": [
        {
          "_id": "683d2d05ae87a04bca311b22",
          "name": "Mingjie Liu",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b23",
          "name": "Shizhe Diao",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b24",
          "name": "Ximing Lu",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b25",
          "name": "Jian Hu",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b26",
          "name": "Xin Dong",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b27",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b28",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b29",
          "name": "Yi Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:59:01.000Z",
      "submittedOnDailyAt": "2025-06-02T03:18:21.654Z",
      "title": "ProRL: Aprendizaje de Refuerzo Prolongado Amplia las Fronteras de la Reacción en Modelos de Lenguaje Grandes",
      "submittedOnDailyBy": {
        "_id": "633bd54b00732349209a18fe",
        "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
        "isPro": false,
        "fullname": "Shizhe Diao",
        "user": "shizhediao",
        "type": "user"
      },
      "summary": "El desarrollo reciente de modelos de lenguaje centrados en lógica ha presentado la aprendizaje por refuerzo (RL) como un método potencial para que los modelos respondan a una recompensa. Sin embargo, se discute si el RL realmente extiende la capacidad lógica del modelo o si simplemente enfatiza la producción de altas recompensas en la distribución base, y si el escalado de cálculos RL a largo plazo conduce a un mejor rendimiento lógico. Este estudio desafía estas suposiciones y muestra que el entrenamiento a largo plazo (ProRL) puede descubrir estrategias lógicas nuevas que los modelos base no pueden alcanzar. Se propone un nuevo método de entrenamiento llamado ProRL, que incluye control de la varianza KL, reset de políticas de referencia y una mayor variedad de tareas. Los análisis experimentales demuestran que modelos entrenados con RL superan a los modelos base en una amplia gama de evaluaciones PAS@k, incluso en casos en los que los modelos base fallan completamente. Además, la mejora de la frontera lógica está fuertemente relacionada con la capacidad de tareas del modelo base y el tiempo de entrenamiento, y muestra que el RL explora nuevos espacios de solución a lo largo del tiempo, los cuales pueden ser útiles para las personas. Estos hallazgos proporcionan nuevas compromisos sobre la extensión lógica de los modelos de lenguaje por medio del RL y establecen una base para futuras investigaciones sobre la lógica del RL. Se publican los pesos del modelo y apoya la investigación en curso: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",
      "upvotes": 44,
      "discussionId": "683d2d08ae87a04bca311bd4",
      "ai_summary": "Prolonged reinforcement learning training (ProRL) uncovers novel reasoning strategies in language models, outperforming base models and suggesting meaningful expansion of reasoning capabilities.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "ProRL",
        "KL divergence control",
        "reference policy resetting",
        "pass@k evaluations",
        "reasoning boundary improvements",
        "task competence",
        "long-horizon RL"
      ]
    },
    "publishedAt": "2025-05-30T13:59:01.000Z",
    "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in\n  Large Language Models",
    "summary": "Recent advances in reasoning-centric language models have highlighted\nreinforcement learning (RL) as a promising method for aligning models with\nverifiable rewards. However, it remains contentious whether RL truly expands a\nmodel's reasoning capabilities or merely amplifies high-reward outputs already\nlatent in the base model's distribution, and whether continually scaling up RL\ncompute reliably leads to improved reasoning performance. In this work, we\nchallenge prevailing assumptions by demonstrating that prolonged RL (ProRL)\ntraining can uncover novel reasoning strategies that are inaccessible to base\nmodels, even under extensive sampling. We introduce ProRL, a novel training\nmethodology that incorporates KL divergence control, reference policy\nresetting, and a diverse suite of tasks. Our empirical analysis reveals that\nRL-trained models consistently outperform base models across a wide range of\npass@k evaluations, including scenarios where base models fail entirely\nregardless of the number of attempts. We further show that reasoning boundary\nimprovements correlates strongly with task competence of base model and\ntraining duration, suggesting that RL can explore and populate new regions of\nsolution space over time. These findings offer new insights into the conditions\nunder which RL meaningfully expands reasoning boundaries in language models and\nestablish a foundation for future work on long-horizon RL for reasoning. We\nrelease model weights to support further research:\nhttps://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633bd54b00732349209a18fe",
      "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
      "fullname": "Shizhe Diao",
      "name": "shizhediao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24863",
      "authors": [
        {
          "_id": "683d0b3de2a7d8d9778bd141",
          "user": {
            "_id": "6719bfd07c6e6c83a388aeae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719bfd07c6e6c83a388aeae/jHxryk04dzHo23TX5F5sz.png",
            "isPro": false,
            "fullname": "Junyu Zhang",
            "user": "jyzhang1208",
            "type": "user"
          },
          "name": "Junyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:59.716Z",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd142",
          "user": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "isPro": false,
            "fullname": "Runpei Dong",
            "user": "RunpeiDong",
            "type": "user"
          },
          "name": "Runpei Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:41:03.079Z",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd143",
          "name": "Han Wang",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd144",
          "name": "Xuying Ning",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd145",
          "name": "Haoran Geng",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd146",
          "name": "Peihao Li",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd147",
          "name": "Xialin He",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd148",
          "name": "Yutong Bai",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd149",
          "name": "Jitendra Malik",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd14a",
          "name": "Saurabh Gupta",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd14b",
          "name": "Huan Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/dBNLCtnWBtBclw0ZZsYBU.png"
      ],
      "publishedAt": "2025-05-30T17:58:36.000Z",
      "submittedOnDailyAt": "2025-06-02T00:55:04.615Z",
      "title": "AlphaOne: Modelo de pensamiento rápido vs. lento dentro del tiempo establecido",
      "submittedOnDailyBy": {
        "_id": "6201fc5d91d53938a6432fbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
        "isPro": false,
        "fullname": "Runpei Dong",
        "user": "RunpeiDong",
        "type": "user"
      },
      "summary": "En este artículo, se presenta un general marco de trabajo para ajustar el proceso de razonamiento de grandes modelos lógicos (LRMs), llamado AlphaOne (alpha1). AlphaOne introduce por primera vez el concepto de \"alpha moment\", que representa las etapas de pensamiento escaladas con un parámetro general alpha. En el alpha moment, se modela el token de transición de proceso de razonamiento como un proceso de probabilidad de Bernoulli y se utiliza para programar dinámicamente la velocidad del proceso de razonamiento. Después del alpha moment, AlphaOne utiliza el token de finalización de razonamiento para detener efectivamente las pensamientos lentos y promover la creación de lógica rápida y eficiente. Este enfoque uniformiza los métodos de escalamiento constantes y permite ajustar de manera flexible y densa el proceso de razonamiento lento o rápido. Los extensos estudios de prueba en diferentes campos como matemáticas, programación y ciencias muestran la excelente capacidad lógica y eficiencia de AlphaOne. Página del proyecto: https://alphaone-project.github.io/",
      "upvotes": 29,
      "discussionId": "683d0b3ee2a7d8d9778bd1ce",
      "projectPage": "https://alphaone-project.github.io/",
      "githubRepo": "https://github.com/ASTRAL-Group/AlphaOne",
      "ai_summary": "AlphaOne dynamically modulates reasoning in large models by introducing $\\alpha$ moment and Bernoulli process for slow thinking, improving efficiency and capability across diverse domains.",
      "ai_keywords": [
        "AlphaOne",
        "$\\alpha$ moment",
        "Bernoulli stochastic process",
        "large reasoning models",
        "reasoning transition tokens",
        "end-of-thinking token",
        "monotonic scaling methods",
        "fast reasoning",
        "efficient answer generation"
      ]
    },
    "publishedAt": "2025-05-30T13:58:36.000Z",
    "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time",
    "summary": "This paper presents AlphaOne (alpha1), a universal framework for\nmodulating reasoning progress in large reasoning models (LRMs) at test time.\nalpha1 first introduces alpha moment, which represents the scaled\nthinking phase with a universal parameter alpha. Within this scaled\npre-alpha moment phase, it dynamically schedules slow thinking transitions\nby modeling the insertion of reasoning transition tokens as a Bernoulli\nstochastic process. After the alpha moment, alpha1 deterministically\nterminates slow thinking with the end-of-thinking token, thereby fostering fast\nreasoning and efficient answer generation. This approach unifies and\ngeneralizes existing monotonic scaling methods by enabling flexible and dense\nslow-to-fast reasoning modulation. Extensive empirical studies on various\nchallenging benchmarks across mathematical, coding, and scientific domains\ndemonstrate alpha1's superior reasoning capability and efficiency. Project\npage: https://alphaone-project.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/dBNLCtnWBtBclw0ZZsYBU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24863.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6201fc5d91d53938a6432fbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
      "fullname": "Runpei Dong",
      "name": "RunpeiDong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24867",
      "authors": [
        {
          "_id": "683d3d6f3f97feb881155aef",
          "user": {
            "_id": "5df7ca7cda6d0311fd3d53f2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5df7ca7cda6d0311fd3d53f2/dtAoDSqgNxeO9AYg9V3na.jpeg",
            "isPro": false,
            "fullname": "Ujjwal Upadhyay",
            "user": "ujjwal9",
            "type": "user"
          },
          "name": "Ujjwal Upadhyay",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-02T05:58:12.617Z",
          "hidden": false
        },
        {
          "_id": "683d3d6f3f97feb881155af0",
          "user": {
            "_id": "65262a396b41932089fd7bae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
            "isPro": true,
            "fullname": "Mukul Ranjan",
            "user": "mukul54",
            "type": "user"
          },
          "name": "Mukul Ranjan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:23.895Z",
          "hidden": false
        },
        {
          "_id": "683d3d6f3f97feb881155af1",
          "name": "Zhiqiang Shen",
          "hidden": false
        },
        {
          "_id": "683d3d6f3f97feb881155af2",
          "name": "Mohamed Elhoseiny",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:59:12.000Z",
      "submittedOnDailyAt": "2025-06-02T04:31:40.253Z",
      "title": "TIMEBRILLING: El motivo por el cual el módulo de lenguaje de video ve como una persona",
      "submittedOnDailyBy": {
        "_id": "65262a396b41932089fd7bae",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
        "isPro": true,
        "fullname": "Mukul Ranjan",
        "user": "mukul54",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los modelos de lenguaje visual y espacial (VLMs) ha demostrado un avance notable en la comprensión de las relaciones espaciales y temporales en películas. Sin embargo, cuando la información espacial está oculta, estos modelos no pueden identificar patrones simplemente temporales. Presentamos un benchmark que simula la naturaleza de la comunicación no real en los sistemas de señalización biológicos, utilizando arreglos de tiempo con ruido para representar información. Interesantemente, los humanos pueden reconocer formas, textos y patrones con una precisión superior a 98% a través de estos arreglos, mientras que los VLMs más recientes no alcanzan ninguna precisión. Esta diferencia de rendimiento demuestra la dependencia excesiva de características espaciales y la imposibilidad de extraer significado a partir del código temporal. Además, cuando se entrenan con conjuntos de datos con bajo SNR espacial, la comprensión temporal del modelo se reduce rápidamente, especialmente en tareas que requieren un entendimiento de pequeños intervalos de tiempo. Para superar esta limitación, es necesario diseñar nuevas arquitecturas o paradigmas de entrenamiento que separen la dependencia espacial del procesamiento temporal. Nuestra análisis sistemático muestra que este problema también depende de la escala y la arquitectura del modelo. Presentamos SpookyBench para fomentar la investigación en la reconocimiento de patrones temporales y para reducir la distancia entre el entendimiento de películas de los humanos y los dispositivos. Los conjuntos de datos y el código están disponibles en el sitio web del proyecto (https://timeblindness.github.io/).",
      "upvotes": 23,
      "discussionId": "683d3d743f97feb881155c56",
      "projectPage": "https://timeblindness.github.io",
      "githubRepo": "https://github.com/TimeBlindness/time-blindness",
      "ai_summary": "SpookyBench is a benchmark for temporal pattern recognition in videos that highlights the limitations of vision-language models in processing noise-like frames without spatial information.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "spatio-temporal relationships",
        "temporal sequences",
        "noise-like frames",
        "biological signaling",
        "covert communication",
        "frame-level spatial features",
        "temporal understanding",
        "data sets",
        "low spatial signal-to-noise ratios",
        "SNR",
        "temporal reasoning",
        "novel architectures",
        "training paradigms",
        "systematic analysis"
      ]
    },
    "publishedAt": "2025-05-30T13:59:12.000Z",
    "title": "Time Blindness: Why Video-Language Models Can't See What Humans Can?",
    "summary": "Recent advances in vision-language models (VLMs) have made impressive strides\nin understanding spatio-temporal relationships in videos. However, when spatial\ninformation is obscured, these models struggle to capture purely temporal\npatterns. We introduce SpookyBench, a benchmark where information is\nencoded solely in temporal sequences of noise-like frames, mirroring natural\nphenomena from biological signaling to covert communication. Interestingly,\nwhile humans can recognize shapes, text, and patterns in these sequences with\nover 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance\ngap highlights a critical limitation: an over-reliance on frame-level spatial\nfeatures and an inability to extract meaning from temporal cues. Furthermore,\nwhen trained in data sets with low spatial signal-to-noise ratios (SNR),\ntemporal understanding of models degrades more rapidly than human perception,\nespecially in tasks requiring fine-grained temporal reasoning. Overcoming this\nlimitation will require novel architectures or training paradigms that decouple\nspatial dependencies from temporal processing. Our systematic analysis shows\nthat this issue persists across model scales and architectures. We release\nSpookyBench to catalyze research in temporal pattern recognition and bridge the\ngap between human and machine video understanding. Dataset and code has been\nmade available on our project website: https://timeblindness.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24867.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65262a396b41932089fd7bae",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
      "fullname": "Mukul Ranjan",
      "name": "mukul54",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18842",
      "authors": [
        {
          "_id": "6839543d6451d371f9e834ec",
          "name": "Jiwan Chung",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834ed",
          "user": {
            "_id": "646aecb04c1cd18b497a50ee",
            "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
            "isPro": false,
            "fullname": "Junhyeok Kim",
            "user": "kjunh",
            "type": "user"
          },
          "name": "Junhyeok Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:46:37.442Z",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834ee",
          "user": {
            "_id": "67021743e4d49b157afd8260",
            "avatarUrl": "/avatars/2a22a18cd45f6d115e8a3a5d1e477dcb.svg",
            "isPro": false,
            "fullname": "Siyeol Kim",
            "user": "siyeolkim",
            "type": "user"
          },
          "name": "Siyeol Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:46:34.334Z",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834ef",
          "name": "Jaeyoung Lee",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834f0",
          "name": "Min Soo Kim",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834f1",
          "name": "Youngjae Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T19:30:47.000Z",
      "submittedOnDailyAt": "2025-06-02T02:58:04.513Z",
      "title": "No es necesariamente lo que no debe ver: para diversos tipos de teorías lógicas interactivas destinadas a la revisión visual selectiva.",
      "submittedOnDailyBy": {
        "_id": "646aecb04c1cd18b497a50ee",
        "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
        "isPro": false,
        "fullname": "Junhyeok Kim",
        "user": "kjunh",
        "type": "user"
      },
      "summary": "Presentamos v1. Esta es una extensión ligera de los Modelos de Lenguaje de Grandes Tamaños Multimodal (MLLMs). v1 permite la revisión visual selectiva durante la inferencia. Los MLLMs actuales consumen una vez solo los datos visuales y basan su inferencia en la memoria interna, pero v1 introduce un mecanismo sencillo y copia para que el modelo pueda cargar de manera dinámica áreas de imágenes relevantes durante el proceso de inferencia. Este mecanismo extiende la arquitectura existente con una pequeña modificación, permitiendo que el modelo acceda al contexto de los tokens visuales basado en su evolución. Para entrenar esta capacidad, construimos el conjunto de datos v1g, que incluye 300K trabajos de inferencia multimodal y notaciones visuales cruzadas. Los experimentos en los benchmarks MathVista, MathVision y MathVerse muestran que v1 mejora significativamente los resultados comparativos, especialmente en tareas que requieren referencias visuales detalladas y inferencia multietapa. Nuestros resultados indican que la aproximación visual dinámica es una prometedora dirección para mejorar el multimodal inferencia. Los códigos, modelos y datos se publicarán para apoyar futuras investigaciones.",
      "upvotes": 20,
      "discussionId": "6839543f6451d371f9e83544",
      "githubRepo": "https://github.com/jun297/v1",
      "ai_summary": "v1 enhances Multimodal Large Language Models by enabling selective and dynamic visual region retrieval during inference, improving performance on multimodal reasoning tasks.",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "point-and-copy mechanism",
        "visual tokens",
        "multimodal reasoning traces",
        "visual grounding annotations",
        "MathVista",
        "MathVision",
        "MathVerse",
        "grounded multimodal reasoning"
      ]
    },
    "publishedAt": "2025-05-24T15:30:47.000Z",
    "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with\n  Selective Visual Revisitation",
    "summary": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18842.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646aecb04c1cd18b497a50ee",
      "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
      "fullname": "Junhyeok Kim",
      "name": "kjunh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14752",
      "authors": [
        {
          "_id": "6832c2c8ba29b909f4013a6d",
          "user": {
            "_id": "67569b1860146dd8c9c8008f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
            "isPro": false,
            "fullname": "Yihong Tang",
            "user": "HYTYH",
            "type": "user"
          },
          "name": "Yihong Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:00.941Z",
          "hidden": false
        },
        {
          "_id": "6832c2c8ba29b909f4013a6e",
          "name": "Menglin Kong",
          "hidden": false
        },
        {
          "_id": "6832c2c8ba29b909f4013a6f",
          "name": "Lijun Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/VaA9NxCa0ncxzh03aZE8c.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/oEgXPOzTQVTDfuTp5Z575.png"
      ],
      "publishedAt": "2025-05-20T13:35:38.000Z",
      "submittedOnDailyAt": "2025-06-02T02:10:16.659Z",
      "title": "Modelo de lenguaje de modelo de datos de gran tamaño para síntesis",
      "submittedOnDailyBy": {
        "_id": "67569b1860146dd8c9c8008f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
        "isPro": false,
        "fullname": "Yihong Tang",
        "user": "HYTYH",
        "type": "user"
      },
      "summary": "La generación de datos sintéticos, capturar exactamente la estructura estadística de la distribución del mundo real, es un desafío fundamental en el modelado de datos. Los métodos tradicionales generalmente basan su trabajo en asunciones fuertes de parámetros o en la diseño directo de la estructura, lo cual a menudo resulta insuficiente en dominios altamente dimensionales o multidimensionales. El avance reciente en los grandes modelos de lenguaje (LLMs) ha demostrado la potencial de una distribución flexible y altamente dimensional en su pre-entrenamiento. Sin embargo, los métodos de muestreo basados en LLMs para la generación de datos sintéticos son no eficientes y están limitados por contextos fijos, lo que impide garantizar la coincidencia estadística. Teniendo en cuenta estos puntos, proponemos el framework general de generación de datos sintéticos LLMSynthor. Este framework convierte a los LLMs en simuladores de reconocimiento de estructuras guiados por retroalimentación de la distribución. LLMSynthor considera a los LLMs como simuladores de Copula no paramétricos para modelar relaciones altamente dimensionales, y introduce la muestreo de propuesta de LLM para generar distribuciones de propuesta basadas en situaciones reales, mejorando la eficiencia de muestreo sin necesidad de rechazo. El ciclo de síntesis iterativo que minimiza las diferencias en el espacio estadístico adapta los datos reales y sintéticos mientras revela y refina la estructura potencial de generación. Hemos evaluado LLMSynthor en entornos controlados y de mundo real con diferentes tipos de datos (estructurados y no estructurados, como: comercio electrónico, población, movilidad). Los datos sintéticos generados por LLMSynthor muestran alta similitud estadística, utilidad en situaciones reales y adaptabilidad multidimensional, lo que los convierte en una herramienta valiosa para aplicaciones en economía, ciencias sociales y investigación urbana.",
      "upvotes": 19,
      "discussionId": "6832c2c9ba29b909f4013aea",
      "projectPage": "https://yihongt.github.io/llmsynthor_web/",
      "githubRepo": "https://github.com/YihongT/LLMSynthor",
      "ai_summary": "LLMSynthor enhances LLMs for efficient and statistically accurate data synthesis through distributional feedback and proposal sampling.",
      "ai_keywords": [
        "Large Language Models",
        "LLMSynthor",
        "nonparametric copula simulator",
        "LLM Proposal Sampling",
        "summary statistics space",
        "synthetic data",
        "statistical fidelity",
        "practical utility",
        "cross-data adaptability"
      ]
    },
    "publishedAt": "2025-05-20T09:35:38.000Z",
    "title": "Large Language Models for Data Synthesis",
    "summary": "Generating synthetic data that faithfully captures the statistical structure\nof real-world distributions is a fundamental challenge in data modeling.\nClassical approaches often depend on strong parametric assumptions or manual\nstructural design and struggle in high-dimensional or heterogeneous domains.\nRecent progress in Large Language Models (LLMs) reveals their potential as\nflexible, high-dimensional priors over real-world distributions. However, when\napplied to data synthesis, standard LLM-based sampling is inefficient,\nconstrained by fixed context limits, and fails to ensure statistical alignment.\nGiven this, we introduce LLMSynthor, a general framework for data synthesis\nthat transforms LLMs into structure-aware simulators guided by distributional\nfeedback. LLMSynthor treats the LLM as a nonparametric copula simulator for\nmodeling high-order dependencies and introduces LLM Proposal Sampling to\ngenerate grounded proposal distributions that improve sampling efficiency\nwithout requiring rejection. By minimizing discrepancies in the summary\nstatistics space, the iterative synthesis loop aligns real and synthetic data\nwhile gradually uncovering and refining the latent generative structure. We\nevaluate LLMSynthor in both controlled and real-world settings using\nheterogeneous datasets in privacy-sensitive domains (e.g., e-commerce,\npopulation, and mobility) that encompass both structured and unstructured\nformats. The synthetic data produced by LLMSynthor shows high statistical\nfidelity, practical utility, and cross-data adaptability, positioning it as a\nvaluable tool across economics, social science, urban studies, and beyond.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/VaA9NxCa0ncxzh03aZE8c.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/oEgXPOzTQVTDfuTp5Z575.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14752.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67569b1860146dd8c9c8008f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
      "fullname": "Yihong Tang",
      "name": "HYTYH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24098",
      "authors": [
        {
          "_id": "683d2cee5bdbb3803e42bc8a",
          "name": "Zhongmou He",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8b",
          "name": "Yee Man Choi",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8c",
          "name": "Kexun Zhang",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8d",
          "name": "Jiabao Ji",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8e",
          "user": {
            "_id": "65a374a59acab1998092a9bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a374a59acab1998092a9bc/M3s_7bSf9G-6b9nLg7N3Z.jpeg",
            "isPro": false,
            "fullname": "Antonio",
            "user": "JuntingZhou",
            "type": "user"
          },
          "name": "Junting Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:34.926Z",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8f",
          "name": "Dejia Xu",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc90",
          "name": "Ivan Bercovich",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc91",
          "name": "Aidan Zhang",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc92",
          "name": "Lei Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62ee423b4bebb4ab55c674b1/yE3pB5JGaOf-sdyDjpCk6.png"
      ],
      "publishedAt": "2025-05-30T01:00:34.000Z",
      "submittedOnDailyAt": "2025-06-02T03:20:27.903Z",
      "title": "HardTests: Generación de código de LLM a través de la síntesis de casos de prueba de alta calidad.",
      "submittedOnDailyBy": {
        "_id": "62ee423b4bebb4ab55c674b1",
        "avatarUrl": "/avatars/ce2797937e8225937fc84d6847d50077.svg",
        "isPro": false,
        "fullname": "Kexun Zhang",
        "user": "k1z",
        "type": "user"
      },
      "summary": "El Validador de Datos es un elemento crucial en la inferencia de modelos de lenguaje grandes (LLM) y desempeña un papel esencial en las tecnologías de aprendizaje por refuerzo y otros procesos post-entrenamiento. Sin embargo, obtener un Validador de Datos confiable es un desafío, ya que los errores ocultos pueden ser detectados por casos excepcionales que son legibles para el humano pero no reflejan la realidad. Para abordar este problema, proponemos HARDTESTGEN y presentamos un pipeline para la síntesis de pruebas de alta calidad utilizando LLM. Con este pipeline, es posible crear un conjunto de datos de programación de competencias incluyendo 47k de problemas y pruebas de alta calidad sintéticas en HARDTESTS. En comparación con las pruebas existentes, las pruebas generadas por HARDTESTGEN mejoran la precisión en un 11.3% puntos y la reproducibilidad en un 17.5% puntos en la evaluación de código generado por LLM. En casos más complejos, el aumento en precisión puede alcanzar un máximo de 40 puntos. HARDTESTS es efectivo en el entrenamiento de modelos y su desempeño se mide a través de la generación de código en futuras generaciones. Publicamos nuestro conjunto de datos y el pipeline de síntesis.",
      "upvotes": 18,
      "discussionId": "683d2cef5bdbb3803e42bccc",
      "projectPage": "https://leililab.github.io/HardTests/",
      "ai_summary": "HARDTESTGEN creates a large, high-quality competitive programming dataset to enhance the precision and recall of verifiers in evaluating LLM-generated code.",
      "ai_keywords": [
        "LLM reasoning",
        "reinforcement learning",
        "verifiers",
        "test synthesis",
        "LLMs",
        "competitive programming",
        "synthetic tests",
        "precision",
        "recall",
        "code generation performance"
      ]
    },
    "publishedAt": "2025-05-29T21:00:34.000Z",
    "title": "HardTests: Synthesizing High-Quality Test Cases for LLM Coding",
    "summary": "Verifiers play a crucial role in large language model (LLM) reasoning, needed\nby post-training techniques such as reinforcement learning. However, reliable\nverifiers are hard to get for difficult coding problems, because a\nwell-disguised wrong solution may only be detected by carefully human-written\nedge cases that are difficult to synthesize. To address this issue, we propose\nHARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this\npipeline, we curate a comprehensive competitive programming dataset HARDTESTS\nwith 47k problems and synthetic high-quality tests. Compared with existing\ntests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points\nhigher and recall that is 17.5 percentage points higher when evaluating\nLLM-generated code. For harder problems, the improvement in precision can be as\nlarge as 40 points. HARDTESTS also proves to be more effective for model\ntraining, measured by downstream code generation performance. We will\nopen-source our dataset and synthesis pipeline at\nhttps://leililab.github.io/HardTests/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62ee423b4bebb4ab55c674b1/yE3pB5JGaOf-sdyDjpCk6.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24098.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ee423b4bebb4ab55c674b1",
      "avatarUrl": "/avatars/ce2797937e8225937fc84d6847d50077.svg",
      "fullname": "Kexun Zhang",
      "name": "k1z",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24878",
      "authors": [
        {
          "_id": "683d160e51706d12b2c6f79f",
          "name": "Yaxin Luo",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a0",
          "name": "Zhaoyi Li",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a1",
          "name": "Jiacheng Liu",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a2",
          "user": {
            "_id": "683d2ac900c71614bab8ea02",
            "avatarUrl": "/avatars/7cb1a5c2c778774262a7d7cb6d309abe.svg",
            "isPro": false,
            "fullname": "Jiacheng Cui",
            "user": "jiachengcui888",
            "type": "user"
          },
          "name": "Jiacheng Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:52.498Z",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a3",
          "name": "Xiaohan Zhao",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a4",
          "name": "Zhiqiang Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-02T01:40:24.093Z",
      "title": "Open CaptchaWorld: Plataforma basada en computadoras para testear y evaluar agentes de LLM Monomodal.",
      "submittedOnDailyBy": {
        "_id": "653cb809b424289c5f384a02",
        "avatarUrl": "/avatars/a1565ab5ae51075c75d6857d64c426a8.svg",
        "isPro": true,
        "fullname": "YaxinLuo",
        "user": "YaxinLuo",
        "type": "user"
      },
      "summary": "CAPTCHA desempeña un papel crucial como obstáculo en las aplicaciones de la realidad mundial al impedir la introducción de agentes web automatizados. Principalmente se utilizan para evitar la completación de tareas de automatización de un extremo a otro. Sin embargo, los agentes de LLM multimodelos modernos muestran un desempeño sorprendente en tareas de reconocimiento estático, pero su capacidad para enfrentar desafíos de interacción y inferencia en pasos no ha sido ampliamente probada. Para resolver estas diferencias, presentamos Open CaptchaWorld. Este es el primer marco de referencia y plataforma web diseñado para evaluar la capacidad de inferencia visual e interactiva de agentes de MLLM a través de diversos problemas dinámicos de CAPTCHA. El marco de referencia comprende 225 problemas de CAPTCHA modernos, incluyendo 20 diferentes tipos, y se evalúa utilizando un nuevo métrico llamado \"Depth of CAPTCHA Reasoning\", que cuantifica la cantidad de etapas cognitivas y funcionales necesarias para resolver cada problema. Los resultados de los experimentos muestran que los humanos alcanzan puntuaciones casi perfectas, mientras que los agentes MLLM más avanzados experimentan significativas dificultades, con un porcentaje de éxito del agente Browser-Use Openai-o3 de máximo 40.0%, que representa solo el 93.3% del rendimiento humano. Esto hace evidente el papel crucial de Open CaptchaWorld como un marco de referencia importante para diagnosticar las limitaciones de los agentes multimodelos actuales y guiar el desarrollo de sistemas de inferencia multimodelo más robustos. Los códigos y datos están disponibles en la siguiente URL.",
      "upvotes": 12,
      "discussionId": "683d160f51706d12b2c6f7f4",
      "githubRepo": "https://github.com/MetaAgentX/OpenCaptchaWorld",
      "ai_summary": "Open CaptchaWorld benchmark evaluates MLLM-powered agents on diverse CAPTCHA puzzles, revealing significant performance gaps compared to humans.",
      "ai_keywords": [
        "multimodal LLM",
        "CAPTCHA",
        "visual reasoning",
        "interaction capabilities",
        "CAPTCHA Reasoning Depth",
        "Browser-Use Openai-o3"
      ]
    },
    "publishedAt": "2025-05-30T13:59:55.000Z",
    "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents",
    "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24878.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653cb809b424289c5f384a02",
      "avatarUrl": "/avatars/a1565ab5ae51075c75d6857d64c426a8.svg",
      "fullname": "YaxinLuo",
      "name": "YaxinLuo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24862",
      "authors": [
        {
          "_id": "683d54f364b44c0ccabb9e65",
          "name": "Cailin Zhuang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e66",
          "name": "Ailin Huang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e67",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:49.393Z",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e68",
          "name": "Jingwei Wu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e69",
          "name": "Yaoqi Hu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6a",
          "name": "Jiaqi Liao",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6b",
          "name": "Zhewei Huang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6c",
          "name": "Hongyuan Wang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6d",
          "name": "Xinyao Liao",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6e",
          "name": "Weiwei Cai",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6f",
          "name": "Hengyuan Xu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e70",
          "name": "Xuanyang Zhang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e71",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e72",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e73",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ZHmu_F8c4mbhiPtomY6H7.mp4"
      ],
      "publishedAt": "2025-05-30T17:58:21.000Z",
      "submittedOnDailyAt": "2025-06-02T06:09:52.296Z",
      "title": "ViStoryBench: Hoja de evaluación integral para la visualización de videos cortos",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "La simulación de storia y realidad virtual tiene como objetivo crear una secuencia de imágenes visualmente coherentes con respecto a las notas y imágenes de referencia dadas. Con el desarrollo reciente de los modelos generativos, ha evolucionado. Para mejorar el rendimiento de los marcos de trabajo de simulación de storia y realidad virtual en escenarios reales, se introduce el complejo evaluación benchmark ViStoryBench. Se colectan diversos conjuntos de datos que incluyen diferentes tipos de storia y estilos de artistas, y la evaluación del modelo se realiza en varias dimensiones, como por ejemplo, comedia, historia, plot (narración) y arte visual (como animación, renderización 3D). ViStoryBench tiene como objetivo equilibrar la estructura de la historia y los elementos visuales, presentando historias en un solo entorno y en varios, para verificar la coherencia de los personajes del modelo. Además, se evalúan modelos que intentan crear imágenes precisas incluyendo proyectos complejos y entornos de mundo. Al ampliar la gama de indicadores de evaluación, se garantiza la realización de evaluaciones detalladas, permitiendo a los investigadores identificar con precisión las fortalezas y debilidades del modelo y promover mejoras dirigidas.",
      "upvotes": 12,
      "discussionId": "683d54f764b44c0ccabb9f60",
      "projectPage": "https://vistorybench.github.io/",
      "githubRepo": "https://github.com/vistorybench/vistorybench"
    },
    "publishedAt": "2025-05-30T13:58:21.000Z",
    "title": "ViStoryBench: Comprehensive Benchmark Suite for Story Visualization",
    "summary": "Story visualization, which aims to generate a sequence of visually coherent\nimages aligning with a given narrative and reference images, has seen\nsignificant progress with recent advancements in generative models. To further\nenhance the performance of story visualization frameworks in real-world\nscenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We\ncollect a diverse dataset encompassing various story types and artistic styles,\nensuring models are evaluated across multiple dimensions such as different\nplots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D\nrenderings). ViStoryBench is carefully curated to balance narrative structures\nand visual elements, featuring stories with single and multiple protagonists to\ntest models' ability to maintain character consistency. Additionally, it\nincludes complex plots and intricate world-building to challenge models in\ngenerating accurate visuals. To ensure comprehensive comparisons, our benchmark\nincorporates a wide range of evaluation metrics assessing critical aspects.\nThis structured and multifaceted framework enables researchers to thoroughly\nidentify both the strengths and weaknesses of different models, fostering\ntargeted improvements.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ZHmu_F8c4mbhiPtomY6H7.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24862.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24196",
      "authors": [
        {
          "_id": "683d29da83edd521f116444c",
          "name": "Longze Chen",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f116444d",
          "name": "Renke Shan",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f116444e",
          "name": "Huiming Wang",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f116444f",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164450",
          "name": "Ziqiang Liu",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164451",
          "name": "Run Luo",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164452",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164453",
          "name": "Hamid Alinejad-Rokny",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164454",
          "name": "Min Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T04:15:06.000Z",
      "submittedOnDailyAt": "2025-06-02T03:23:19.536Z",
      "title": "CLaSp: Procesamiento automático de predicciones utilizando escaping de capas proyectivas",
      "submittedOnDailyBy": {
        "_id": "64c7b4d1c547ed5243c07b6c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7b4d1c547ed5243c07b6c/h96CLBj6dcm01soK2UJzr.jpeg",
        "isPro": false,
        "fullname": "Longze Chen",
        "user": "lzchen2001",
        "type": "user"
      },
      "summary": "El específico decodificación (SD) es un método potencial para acelerar el proceso de decodificación de modelos de lenguaje grandes (LLMs). La eficiencia de SD depende significativamente de la coincidencia entre el modelo de borrado y el modelo de verificación. Sin embargo, las técnicas actuales de borrado generalmente requieren la entrenamiento de módulos adicionales, lo que plantea desafíos en la implementación y la compatibilidad entre diferentes LLMs. En este artículo, se propone CLaSp para proporcionar una estrategia de salto de capas específica para el decodificación de un modelo propio en el contexto. A diferencia de los métodos existentes, CLaSp no requiere módulos de borrado adicionales o entrenamiento adicional. En su lugar, utiliza una estructura de plugin y juego de juegos para construir un modelo de borrado comprimido mediante el salto de capas intermedias del modelo de verificación. En particular, se desarrolló un algoritmo de programación dinámica para optimizar el proceso de salto de capas, con el objetivo de alcanzar un estado completo de estado oculto en el último paso de verificación. Así, CLaSp puede ajustar estrategias de salto de capas dinámicamente después de cada paso de verificación, sin depender de conjuntos de capas de salto previamente optimizados. Los resultados de experimentos en tareas de flujo posterior han demostrado que CLaSp logra un aumento de velocidad del 1.3 a 1.7 veces en las series de modelos LLaMA3, sin modificar la distribución inicial de las frases generadas.",
      "upvotes": 10,
      "discussionId": "683d29db83edd521f1164482",
      "ai_summary": "CLaSp, an in-context layer-skipping strategy for self-speculative decoding, accelerates Large Language Model decoding without additional modules or training, achieving a 1.3x to 1.7x speedup on LLaMA3 models.",
      "ai_keywords": [
        "speculative decoding",
        "large language models",
        "draft model",
        "verify model",
        "in-context layer-skipping",
        "dynamic programming algorithm",
        "hidden states",
        "verification stage"
      ]
    },
    "publishedAt": "2025-05-30T00:15:06.000Z",
    "title": "CLaSp: In-Context Layer Skip for Self-Speculative Decoding",
    "summary": "Speculative decoding (SD) is a promising method for accelerating the decoding\nprocess of Large Language Models (LLMs). The efficiency of SD primarily hinges\non the consistency between the draft model and the verify model. However,\nexisting drafting approaches typically require additional modules to be\ntrained, which can be challenging to implement and ensure compatibility across\nvarious LLMs. In this paper, we propose CLaSp, an in-context layer-skipping\nstrategy for self-speculative decoding. Unlike prior methods, CLaSp does not\nrequire additional drafting modules or extra training. Instead, it employs a\nplug-and-play mechanism by skipping intermediate layers of the verify model to\nconstruct a compressed draft model. Specifically, we develop a dynamic\nprogramming algorithm that optimizes the layer-skipping process by leveraging\nthe complete hidden states from the last verification stage as an objective.\nThis enables CLaSp to dynamically adjust its layer-skipping strategy after each\nverification stage, without relying on pre-optimized sets of skipped layers.\nExperimental results across diverse downstream tasks demonstrate that CLaSp\nachieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the\noriginal distribution of the generated text.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24196.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "64c7b4d1c547ed5243c07b6c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7b4d1c547ed5243c07b6c/h96CLBj6dcm01soK2UJzr.jpeg",
      "fullname": "Longze Chen",
      "name": "lzchen2001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23941",
      "authors": [
        {
          "_id": "683cf4405810d395f0a3788b",
          "name": "An Vo",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788c",
          "name": "Khai-Nguyen Nguyen",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788d",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Reza Taesiri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-02T00:52:37.933Z",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788e",
          "name": "Vy Tuong Dang",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788f",
          "user": {
            "_id": "653194a4c8da3465f4701ad1",
            "avatarUrl": "/avatars/6682164fcaf1d339ce9ac82ba131af5e.svg",
            "isPro": true,
            "fullname": "Khai-Nguyen Nguyen",
            "user": "knguyennguyen",
            "type": "user"
          },
          "name": "Anh Totti Nguyen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-02T00:45:56.803Z",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a37890",
          "name": "Daeyoung Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T18:47:58.000Z",
      "submittedOnDailyAt": "2025-06-02T03:28:19.444Z",
      "title": "Visión de Long Range Group es prejudicada.",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje general (LLMs) recuerdan muchas conocidas en la red, lo que puede ayudar a tareas posteriores pero también puede ser afectado por sesgos y respuestas incorrectas. En este estudio, investigamos cómo el modelo de lenguaje visual (VLMs) tiene un sesgo en tareas visuales estándar y objetivas (detección de objetos e identificación) y cómo afecta la precisión del modelo conocimientos populares en la red. De esta manera, se demostró claramente que los mejores VLMs tienen un sesgo fuerte. Por ejemplo, mostró que el reconocimiento de 'ADVISOR' en el modelo 'ADVISOR' con 4 streams no es posible, y la precisión promedio de detección de objetos en 7 diferentes áreas (animales, logotipos, ajedrez, juegos, trucos ópticos, tableros con patrones) es de 17.05%. Al agregar texto (por ejemplo, 'ADVISOR') a imágenes reales, la precisión de los VLMs se reduce aún más. El sesgo de los VLMs es fuerte, y aunque se indique al modelo para verificar los resultados o solo creer en los detalles de la imagen, no se logra aumentar la precisión promedio. Este estudio muestra un interesante modo de fallo de los VLMs y proporciona un marco de trabajo para automatizar el sesgo de los VLMs. Los códigos y datos están disponibles en vlmsarebiased.github.io.",
      "upvotes": 10,
      "discussionId": "683cf4445810d395f0a37983",
      "projectPage": "https://vlmsarebiased.github.io/",
      "githubRepo": "https://github.com/anvo25/vlms-are-biased",
      "ai_summary": "Vision language models exhibit strong biases in counting and identification tasks, demonstrating a failure mode that persist even with additional instructions or context.",
      "ai_keywords": [
        "large language models",
        "vision language models",
        "downstream tasks",
        "popular subjects",
        "accuracy",
        "visual tasks",
        "counting",
        "identification",
        "biases",
        "counterfactual image",
        "automated framework"
      ]
    },
    "publishedAt": "2025-05-29T14:47:58.000Z",
    "title": "Vision Language Models are Biased",
    "summary": "Large language models (LLMs) memorize a vast amount of prior knowledge from\nthe Internet that help them on downstream tasks but also may notoriously sway\ntheir outputs towards wrong or biased answers. In this work, we test how the\nknowledge about popular subjects hurt the accuracy of vision language models\n(VLMs) on standard, objective visual tasks of counting and identification. We\nfind that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a\nfourth stripe has been added to a 3-stripe Adidas logo) scoring an average of\n17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)\nacross 7 diverse domains from animals, logos, chess, board games, optical\nillusions, to patterned grids. Insert text (e.g., \"Adidas\") describing the\nsubject name into the counterfactual image further decreases VLM accuracy. The\nbiases in VLMs are so strong that instructing them to double-check their\nresults or rely exclusively on image details to answer improves counting\naccuracy by only +2 points, on average. Our work presents an interesting\nfailure mode in VLMs and an automated framework for testing VLM biases. Code\nand data are available at: vlmsarebiased.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 84
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24521",
      "authors": [
        {
          "_id": "683d11d1495f0b58f2fd49a9",
          "name": "Yang-Tian Sun",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49aa",
          "name": "Xin Yu",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ab",
          "name": "Zehuan Huang",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ac",
          "name": "Yi-Hua Huang",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ad",
          "name": "Yuan-Chen Guo",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ae",
          "name": "Ziyi Yang",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49af",
          "name": "Yan-Pei Cao",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49b0",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T12:31:59.000Z",
      "submittedOnDailyAt": "2025-06-02T01:25:45.570Z",
      "title": "UniGeo: Control de la difusión genérica de vídeos unificada",
      "submittedOnDailyBy": {
        "_id": "6375d136dee28348a9c63cbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
        "isPro": false,
        "fullname": "zehuan-huang",
        "user": "huanngzh",
        "type": "user"
      },
      "summary": "Recientemente, los métodos que utilizan modelos de diferenciación para ayudar a medir la estructura de monologías (por ejemplo, profundidad y normales) han ganado atención debido a su poderosa capacidad de generalización. Sin embargo, muchos estudios anteriores centran la medición de características estructurales en cada frame de video dentro de la coordenada de cámara, ignorando la capacidad de los modelos de diferenciación para determinar las relaciones de correspondencia entre los frames dentro de una estructura única. En este artículo, mostramos cómo mediante un diseño adecuado y ajustes, se puede efectivamente utilizar la coherencia propia de modelos de generación de video para medir estructuras. Específicamente, 1) se seleccionan como objetivo de predicción videos de frames que comparten una relación de correspondencia común en la misma coordenada, para seleccionar características estructurales, 2) se introduce una nueva forma de normalización condicional basada en el reuso de datos de ubicación, y 3) se mejora el rendimiento mediante el entrenamiento paralelo de múltiples características estructurales que comparten la misma relación de correspondencia. Nuestros resultados muestran que se logra una excelente predicción de las características estructurales en su conjunto, lo que puede ser aplicado directamente a tareas de reconstrucción. Si se entrena solo con datos de video estáticos, nuestro enfoque también funciona efectivamente en la generalización a secuencias de video dinámicas.",
      "upvotes": 8,
      "discussionId": "683d11d3495f0b58f2fd4a95",
      "projectPage": "https://sunyangtian.github.io/UniGeo-web/",
      "githubRepo": "https://github.com/SunYangtian/UniGeo",
      "ai_summary": "Video generation models leveraging diffusion priors achieve superior global geometric attribute estimation and reconstructions, benefiting from inter-frame consistency and joint training on shared attributes.",
      "ai_keywords": [
        "diffusion models",
        "monocular geometric estimation",
        "depth",
        "normal",
        "camera coordinate system",
        "intrinsic consistency",
        "video generation models",
        "global coordinate system",
        "positional encodings",
        "joint training",
        "static video data",
        "dynamic video scenes"
      ]
    },
    "publishedAt": "2025-05-30T08:31:59.000Z",
    "title": "UniGeo: Taming Video Diffusion for Unified Consistent Geometry\n  Estimation",
    "summary": "Recently, methods leveraging diffusion model priors to assist monocular\ngeometric estimation (e.g., depth and normal) have gained significant attention\ndue to their strong generalization ability. However, most existing works focus\non estimating geometric properties within the camera coordinate system of\nindividual video frames, neglecting the inherent ability of diffusion models to\ndetermine inter-frame correspondence. In this work, we demonstrate that,\nthrough appropriate design and fine-tuning, the intrinsic consistency of video\ngeneration models can be effectively harnessed for consistent geometric\nestimation. Specifically, we 1) select geometric attributes in the global\ncoordinate system that share the same correspondence with video frames as the\nprediction targets, 2) introduce a novel and efficient conditioning method by\nreusing positional encodings, and 3) enhance performance through joint training\non multiple geometric attributes that share the same correspondence. Our\nresults achieve superior performance in predicting global geometric attributes\nin videos and can be directly applied to reconstruction tasks. Even when\ntrained solely on static video data, our approach exhibits the potential to\ngeneralize to dynamic video scenes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24521.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6375d136dee28348a9c63cbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
      "fullname": "zehuan-huang",
      "name": "huanngzh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24858",
      "authors": [
        {
          "_id": "683d2a3651706d12b2cc8ace",
          "name": "Gabrielle Kaili-May Liu",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8acf",
          "name": "Gal Yona",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad0",
          "name": "Avi Caciularu",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad1",
          "name": "Idan Szpektor",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad2",
          "name": "Tim G. J. Rudner",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad3",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:54:08.000Z",
      "submittedOnDailyAt": "2025-06-02T03:13:37.735Z",
      "title": "MetaFaith: La expresión de la verdadera incertidumbre de la naturaleza del lenguaje natural en un LLM",
      "submittedOnDailyBy": {
        "_id": "64f1ca1d5b8a6a5d39d75771",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1ca1d5b8a6a5d39d75771/Caq_Ahp7Qkm1nHyhBcztE.jpeg",
        "isPro": false,
        "fullname": "John Chih Liu",
        "user": "johncliu",
        "type": "user"
      },
      "summary": "Uno de los elementos importantes para la confianza en los LLMs es la transmisión de incertidumbre que puede confiarse, pero los LLMs utilizan expresiones de confianza en la transmisión de declaraciones falsas, lo que puede llevar a confianzas exageradas o a la rotura de la confianza. Proponemos un primer estudio sistemático para evaluar la capacidad de los LLMs de usar lenguaje que refleje incertidumbre propia, en un amplio rango de modelos, datasets y estrategias de prompts. Nuestros resultados muestran que los LLMs fallan significativamente en esta tarea y que las soluciones existentes no son suficientes: el método estándar de prompts solo proporciona algunos beneficios, y los métodos de ajuste basados en hechos también pueden causar daños en la precisión. Para resolver estas importantes limitaciones, proponemos un nuevo enfoque de ajuste basado en el metacognitivo de los humanos, llamado MetaFaith. MetaFaith mejora significativamente la precisión en ajustes en diferentes modelos y áreas de tareas, logrando un aumento máximo del 61% en la precisión y un ganancia de 83% en la probabilidad de ser evaluado como originalmente generado por un humano.",
      "upvotes": 7,
      "discussionId": "683d2a3751706d12b2cc8b0a",
      "ai_summary": "A study reveals that Large Language Models (LLMs) struggle with expressing uncertainty accurately and introduces MetaFaith, a prompt-based method that enhances their calibration significantly.",
      "ai_keywords": [
        "faithful confidence calibration",
        "linguistic expressions of uncertainty",
        "intrinsic uncertainty",
        "prompting strategies",
        "metacognition"
      ]
    },
    "publishedAt": "2025-05-30T13:54:08.000Z",
    "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs",
    "summary": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of faithful confidence calibration of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\nfaithfully reflect their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24858.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f1ca1d5b8a6a5d39d75771",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1ca1d5b8a6a5d39d75771/Caq_Ahp7Qkm1nHyhBcztE.jpeg",
      "fullname": "John Chih Liu",
      "name": "johncliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24417",
      "authors": [
        {
          "_id": "683d0b6c5810d395f0a9a49e",
          "name": "Runnan Lu",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a49f",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a4a0",
          "name": "Jailing Liu",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a4a1",
          "name": "Haifa Wang",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a4a2",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T09:55:39.000Z",
      "submittedOnDailyAt": "2025-06-02T00:55:24.254Z",
      "title": "EasyText: Controllable Diffusion Transformer for Multilingual Text\nRendering",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "Utilizar modelos de difusión para generar texto preciso ha sido una expectativa a largo plazo, pero hasta ahora ha sido considerado difícil. Los métodos recientes han mejorado la visualización de textos en la misma lengua, pero la visualización de textos en idiomas arbitrarios aún no ha sido investigada. En este artículo, se propone un marco de visualización de texto basado en DiT (Diffusion Transformer) llamado EasyText, que combina la eliminación de ruido y el codificado de tokens de caracteres en idiomas múltiples para realizar la visualización de texto de manera precisa y controlable. Además, se propone un enfoque para codificar la información de posición de los caracteres y una interpretación de la información de posición, con el objetivo de mejorar la visualización de texto. Se construyeron conjuntos de datos grandes, incluyendo 1 millón de anotaciones de imágenes-texto en múltiples idiomas y 20,000 de imágenes de alta calidad anotadas, para utilizarlos en entrenamiento previo y fine-tuning. Se realizaron experimentos y evaluaciones detallados para demostrar la visualización de texto en múltiples idiomas, la calidad de la visualización y el efecto de la integración de texto en la ruteo hacia adelante.",
      "upvotes": 7,
      "discussionId": "683d0b6f5810d395f0a9a57b",
      "ai_summary": "The paper presents EasyText, a multilingual text rendering framework using DiT that enhances rendering precision and visual quality with large datasets.",
      "ai_keywords": [
        "DiT (Diffusion Transformer)",
        "denoising latents",
        "multilingual character tokens",
        "character positioning encoding",
        "position encoding interpolation",
        "synthetic text image dataset",
        "pretraining",
        "fine-tuning",
        "multilingual text rendering",
        "visual quality",
        "layout-aware text integration"
      ]
    },
    "publishedAt": "2025-05-30T05:55:39.000Z",
    "title": "EasyText: Controllable Diffusion Transformer for Multilingual Text\n  Rendering",
    "summary": "Generating accurate multilingual text with diffusion models has long been\ndesired but remains challenging. Recent methods have made progress in rendering\ntext in a single language, but rendering arbitrary languages is still an\nunexplored area. This paper introduces EasyText, a text rendering framework\nbased on DiT (Diffusion Transformer), which connects denoising latents with\nmultilingual character tokens encoded as character tokens. We propose character\npositioning encoding and position encoding interpolation techniques to achieve\ncontrollable and precise text rendering. Additionally, we construct a\nlarge-scale synthetic text image dataset with 1 million multilingual image-text\nannotations as well as a high-quality dataset of 20K annotated images, which\nare used for pretraining and fine-tuning respectively. Extensive experiments\nand evaluations demonstrate the effectiveness and advancement of our approach\nin multilingual text rendering, visual quality, and layout-aware text\nintegration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20873",
      "authors": [
        {
          "_id": "68395a548ead63ba096bba45",
          "user": {
            "_id": "6770efb5b673f241332fc4a7",
            "avatarUrl": "/avatars/7aca599492233e6a51c2d6a8f52c644e.svg",
            "isPro": false,
            "fullname": "Chaeyoung Jung",
            "user": "Chae0",
            "type": "user"
          },
          "name": "Chaeyoung Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:46:20.597Z",
          "hidden": false
        },
        {
          "_id": "68395a548ead63ba096bba46",
          "name": "Youngjoon Jang",
          "hidden": false
        },
        {
          "_id": "68395a548ead63ba096bba47",
          "name": "Jongmin Choi",
          "hidden": false
        },
        {
          "_id": "68395a548ead63ba096bba48",
          "name": "Joon Son Chung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T08:22:56.000Z",
      "submittedOnDailyAt": "2025-06-02T06:33:06.810Z",
      "title": "Pork-Maze Decoding: Large-Scale Language Model for Enhancing Understanding of Multimodal Structure in Audio-Visual Data",
      "submittedOnDailyBy": {
        "_id": "6770efb5b673f241332fc4a7",
        "avatarUrl": "/avatars/7aca599492233e6a51c2d6a8f52c644e.svg",
        "isPro": false,
        "fullname": "Chaeyoung Jung",
        "user": "Chae0",
        "type": "user"
      },
      "summary": "El objetivo de esta investigación es ajustar el modelo para evitar que se produzca un \"bounce\" en los modelos de lenguaje de audio y video (AV-LLMs), mejorando así la comprensión multimodal equilibrada. En los actuales AV-LLMs, las características de audio y video generalmente son procesadas juntas en el decodificador. Esta estrategia fomenta una comprensión multimodal unificada, pero también tiene el riesgo de que el modelo dependa excesivamente de una fuente de entrenamiento desbalanceada, lo que puede provocar un \"bounce\" en el modelo. Para mitigar esto, se propone una estrategia sencilla y efectiva llamada Fork-Merge Decoding (FMD). FMD procesa solo el audio o solo el video en las primeras capas del decodificador, ajustando así la producción del \"bounce\" y mejorando la comprensión multimodal equilibrada en AV-LLMs.",
      "upvotes": 6,
      "discussionId": "68395a558ead63ba096bba7b",
      "ai_summary": "The Fork-Merge Decoding strategy improves balanced multimodal understanding in audio-visual large language models by separating and then combining modality-specific reasoning.",
      "ai_keywords": [
        "fork-merge decoding",
        "AU-LLMs",
        "modality bias",
        "audio-visual large language models",
        "VideoLLaMA2",
        "video-SALMONN",
        "benchmark datasets",
        "audio-visual reasoning"
      ]
    },
    "publishedAt": "2025-05-27T04:22:56.000Z",
    "title": "Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual\n  Large Language Models",
    "summary": "The goal of this work is to enhance balanced multimodal understanding in\naudio-visual large language models (AV-LLMs) by addressing modality bias\nwithout requiring additional training. In current AV-LLMs, audio and video\nfeatures are typically processed jointly in the decoder. While this strategy\nfacilitates unified multimodal understanding, it may introduce modality bias,\nwhere the model tends to over-rely on one modality due to imbalanced training\nsignals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet\neffective inference-time strategy that requires no additional training or\narchitectural modifications. FMD first performs modality-specific reasoning by\nprocessing audio-only and video-only inputs through the early decoder layers (a\nfork phase), and then merges the resulting hidden states for joint reasoning in\nthe remaining layers (a merge phase). This approach promotes balanced modality\ncontributions and leverages complementary information across modalities. We\nevaluate our method on two representative AV-LLMs, VideoLLaMA2 and\nvideo-SALMONN, using three benchmark datasets. Experimental results demonstrate\nconsistent performance improvements on tasks focused on audio, video, and\ncombined audio-visual reasoning, demonstrating the effectiveness of\ninference-time interventions for robust multimodal understanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20873.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6770efb5b673f241332fc4a7",
      "avatarUrl": "/avatars/7aca599492233e6a51c2d6a8f52c644e.svg",
      "fullname": "Chaeyoung Jung",
      "name": "Chae0",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24850",
      "authors": [
        {
          "_id": "683d0ffbe41c42faceda19b2",
          "user": {
            "_id": "6587e5a4b2177de3967ff434",
            "avatarUrl": "/avatars/f2dfbc44eb2bff8d8d66d26db8539708.svg",
            "isPro": false,
            "fullname": "Shuyao Xu",
            "user": "Tim-Xu",
            "type": "user"
          },
          "name": "Shuyao Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:56.229Z",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b3",
          "name": "Cheng Peng",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b4",
          "name": "Jiangxuan Long",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b5",
          "name": "Weidi Xu",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b6",
          "name": "Wei Chu",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b7",
          "name": "Yuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:47:17.000Z",
      "submittedOnDailyAt": "2025-06-02T02:07:38.924Z",
      "title": "Introducción de señales negativas: datos sobre la teoría lógica de la fortalecimiento de la inteligencia artificial en modelos LLM a partir de los profesores.",
      "submittedOnDailyBy": {
        "_id": "66e83ec5deb449d8d856e78d",
        "avatarUrl": "/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg",
        "isPro": false,
        "fullname": "Tongyan Hu",
        "user": "entropyhu",
        "type": "user"
      },
      "summary": "Recientemente, los modelos de alto nivel de lógica (por ejemplo, DeepSeek-R1, o1 de OpenAI) han demostrado que los datos obtenidos pueden transmitirse eficazmente a pequeños modelos estudiantes con alta eficiencia. Sin embargo, los entrenamientos estándar utilizan un rechazo de muestras que eliminan ejemplos de lógica negativa, aunque estos tienen valor pero no se utilizan bien. Este artículo busca abordar cuestiones cruciales para maximizar el rendimiento lógico de los modelos de lenguaje grandes (LLM). Para ello, proponemos un marco de dos etapas llamado Reinforcement Distillation (REDI). En la primera etapa, se entrena con Supervised Fine-Tuning (SFT) para aprender de trazas positivas. En la segunda etapa, utilizamos una función de objetivo propuesta por nosotros, REDI, para mejorar el modelo utilizando tanto trazas positivas como negativas. Esta nueva función de objetivo es más sencilla que DPO o SimPO y se adapta directamente a la función de pérdida sin necesidad de referencias. Nuestra evaluación experimental muestra que REDI muestra un excelente rendimiento en pruebas lógicas matemáticas, superando a entrenamientos de SFT con rechazo de muestras o combinaciones de SFT con DPO/SimPO. Específicamente, el modelo Qwen-REDI-1.5B, después de ser mejorado con 131k ejemplos positivos y negativos en el conjunto de datos Open-R1, alcanzó un 83.1% en MATH-500 (pass@1). Este rendimiento es comparable al de DeepSeek-R1-Distill-Qwen-1.5B (modelo mejorado con 800k datos propiétarios) y mejora o alcanza el rendimiento de los modelos mejorados con datos publicos, convirtiéndose en el nuevo líder en modelos de 1.5B con datos publicos.",
      "upvotes": 5,
      "discussionId": "683d0ffce41c42faceda19da",
      "githubRepo": "https://github.com/Tim-Siu/reinforcement-distillation",
      "ai_summary": "Reinforcement Distillation (REDI) leverages both positive and negative traces to enhance large language model reasoning performance offline, outperforming traditional methods and achieving state-of-the-art results with limited open data.",
      "ai_keywords": [
        "model distillation",
        "DeepSeek-R1",
        "OpenAI's o1",
        "Reinforcement Distillation (REDI)",
        "Supervised Fine-Tuning (SFT)",
        "REDI objective",
        "DPO",
        "SimPO",
        "mathematical reasoning tasks",
        "MATH-500",
        "Qwen-REDI-1.5B",
        "DeepSeek-R1-Distill-Qwen-1.5B"
      ]
    },
    "publishedAt": "2025-05-30T13:47:17.000Z",
    "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning",
    "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24850.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66e83ec5deb449d8d856e78d",
      "avatarUrl": "/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg",
      "fullname": "Tongyan Hu",
      "name": "entropyhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24293",
      "authors": [
        {
          "_id": "683d01ec446fd0c8ff323010",
          "user": {
            "_id": "6658f863ce1b283888625af3",
            "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
            "isPro": false,
            "fullname": "James Golden",
            "user": "jamesgolden1",
            "type": "user"
          },
          "name": "James R. Golden",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-02T02:38:10.635Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6658f863ce1b283888625af3/uulwAnV1EYXXSdsC-eDMW.png"
      ],
      "publishedAt": "2025-05-30T07:08:33.000Z",
      "submittedOnDailyAt": "2025-06-02T01:02:58.980Z",
      "title": "Los modelos de lenguaje globales son mapeos lineales localmente.",
      "submittedOnDailyBy": {
        "_id": "6658f863ce1b283888625af3",
        "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
        "isPro": false,
        "fullname": "James Golden",
        "user": "jamesgolden1",
        "type": "user"
      },
      "summary": "Demostramos que podemos mapear la inferencia de operaciones grandes de modelos de lenguaje de alto rendimiento (LLM) con pesos públicos sin modificar las secuencias de entrada ni cambiar los pesos del modelo o las predicciones de salida. Extendimos las técnicas que muestran la linearidad local o parcial de modelos de difusión de imágenes para cambiar estratégicamente el cálculo de la gradiente para la predicción del siguiente token en una secuencia de entrada dada, de manera que el Jacobian del modelo se propague casi exactamente a un sistema lineal. Esta metodología se aplicó a varios modelos (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral, OLMo 2, y el máximo Llama 3.3 70B Q4), y se confirmó mediante la decomposición en valores singulares (SVD) que estos LLMs funcionan en un espacio de muy baja dimensión. En este espacio, muchos de los vectores singulares más grandes codifican conceptos relacionados con los tokens de salida más probables. Esta abordaje permite observar casi exactamente como funciona cada capa continua (y sus componentes de atención y MLP internos) y observar el aparición de conceptos contextuales. Aunque los modernos LLMs tienen una alta expresividad y una global nonlinearidad, pueden interpretarse como aproximaciones casi exactas de un sistema lineal local, revelando estructuras contextuales comprensibles en el proceso de predicción del siguiente token y proporcionando una visión interna de las representaciones del modelo.",
      "upvotes": 3,
      "discussionId": "683d01ee446fd0c8ff323087",
      "githubRepo": "https://github.com/jamesgolden1/llms-are-llms/",
      "ai_summary": "LLMs can be approximated as linear systems for inference, offering insights into their internal representations and semantic structures without altering the models or their predictions.",
      "ai_keywords": [
        "large language models (LLMs)",
        "inference operations",
        "linear system",
        "gradient computation",
        "Jacobian",
        "singular value decomposition",
        "low-dimensional subspaces",
        "semantic concepts",
        "attention components",
        "MLP components",
        "locally linear decompositions"
      ]
    },
    "publishedAt": "2025-05-30T03:08:33.000Z",
    "title": "Large Language Models are Locally Linear Mappings",
    "summary": "We demonstrate that the inference operations of several open-weight large\nlanguage models (LLMs) can be mapped to an exactly equivalent linear system for\nan input sequence without modifying the model weights or altering output\npredictions. Extending techniques from image diffusion models that exhibit\nlocal or piecewise linearity, we strategically alter the gradient computation\nwith respect to a given input sequence for a next-token prediction such that\nthe Jacobian of the model nearly exactly reproduces the forward prediction with\na linear system. We demonstrate this approach across models (Llama 3, Gemma 3,\nQwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show\nthrough the singular value decomposition of the detached Jacobian that these\nLLMs operate in extremely low-dimensional subspaces where many of the largest\nsingular vectors decode to concepts related to the most-likely output token.\nThis approach also allows us to examine the operation of each successive layer\n(and its attention and MLP components) as nearly-exact linear systems and\nobserve the emergence of semantic concepts. Despite their expressive power and\nglobal nonlinearity, modern LLMs can be interpreted through nearly-exact\nlocally linear decompositions that provide insights into their internal\nrepresentations and reveal interpretable semantic structures in the next-token\nprediction process.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6658f863ce1b283888625af3/uulwAnV1EYXXSdsC-eDMW.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24293.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6658f863ce1b283888625af3",
      "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
      "fullname": "James Golden",
      "name": "jamesgolden1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23926",
      "authors": [
        {
          "_id": "683d33be277ad05e5a672f79",
          "name": "Xuweiyi Chen",
          "hidden": false
        },
        {
          "_id": "683d33be277ad05e5a672f7a",
          "name": "Wentao Zhou",
          "hidden": false
        },
        {
          "_id": "683d33be277ad05e5a672f7b",
          "name": "Aruni RoyChowdhury",
          "hidden": false
        },
        {
          "_id": "683d33be277ad05e5a672f7c",
          "name": "Zezhou Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T18:21:47.000Z",
      "submittedOnDailyAt": "2025-06-02T03:49:08.677Z",
      "title": "Point-MoE: Mezcla de Expertos para la Generalización Cruzada de la Segmentación Semántica 3D",
      "submittedOnDailyBy": {
        "_id": "634632aaac1cb29fb2ac9f14",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634632aaac1cb29fb2ac9f14/nGZ2TzKOOcKMAR_NFYKkR.jpeg",
        "isPro": false,
        "fullname": "Xuweiyi Chen",
        "user": "Xuweiyi",
        "type": "user"
      },
      "summary": "El texto explica por qué el entendimiento de los clústeres de puntos no ha alcanzado el nivel de los escaladores y propone soluciones. A continuación, se presenta la traducción al español del texto original:\n\nLos escaladores han sido afectados por el procesamiento del lenguaje natural y la visión computacional, pero el entendimiento de los clústeres de puntos de punto aún no ha alcanzado ese nivel. Esto se debe a la escasez relativa de la escala de los conjuntos de datos 3D y a la diversidad de sus fuentes. Los clústeres de puntos se capturan en diferentes áreas, desde interiores hasta exteriores, utilizando diferentes sensores (por ejemplo: cámaras de profundidad, Lidar). Cada sensor introduce patrones de escaneo únicos, densidades de muestreo y sesgos semánticos. Estas diferencias entre áreas son obstáculos cruciales para la entrenamiento de modelos uniformes en escala. En particular, este problema es especialmente relevante debido a las restricciones reales que impiden la etiquetación de áreas en el proceso de inferencia. En este artículo, se propone una arquitectura Mixture-of-Experts que permite la generalización entre grandes escalas y áreas. Point-MoE, que utiliza datos de clústeres de puntos como base, se ve afectado considerablemente en su rendimiento. Sin embargo, Point-MoE puede especializar automáticamente a los expertos incluso cuando los etiquetados de los datos no están disponibles, utilizando una estrategia simple de top-k routing. Los experimentos muestran que Point-MoE supera a los clústeres de puntos estándar y tiene un buen rendimiento en la generalización hacia áreas no vistas anteriormente. Esta investigación muestra un camino escalable para el entendimiento 3D. El modelo prioriza la descubrimiento automático de la estructura de diferentes datos 3D y reemplaza la edición automática o la sub-provisión de áreas.",
      "upvotes": 3,
      "discussionId": "683d33c4277ad05e5a67310e",
      "ai_summary": "Point-MoE, a Mixture-of-Experts architecture, enables large-scale, cross-domain generalization in 3D perception by automatically specializing experts without domain labels.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "Point-MoE",
        "point cloud backbones",
        "3D perception",
        "domain heterogeneity",
        "domain labels",
        "top-k routing",
        "multi-domain baselines"
      ]
    },
    "publishedAt": "2025-05-29T14:21:47.000Z",
    "title": "Point-MoE: Towards Cross-Domain Generalization in 3D Semantic\n  Segmentation via Mixture-of-Experts",
    "summary": "While scaling laws have transformed natural language processing and computer\nvision, 3D point cloud understanding has yet to reach that stage. This can be\nattributed to both the comparatively smaller scale of 3D datasets, as well as\nthe disparate sources of the data itself. Point clouds are captured by diverse\nsensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor,\noutdoor), each introducing unique scanning patterns, sampling densities, and\nsemantic biases. Such domain heterogeneity poses a major barrier towards\ntraining unified models at scale, especially under the realistic constraint\nthat domain labels are typically inaccessible at inference time. In this work,\nwe propose Point-MoE, a Mixture-of-Experts architecture designed to enable\nlarge-scale, cross-domain generalization in 3D perception. We show that\nstandard point cloud backbones degrade significantly in performance when\ntrained on mixed-domain data, whereas Point-MoE with a simple top-k routing\nstrategy can automatically specialize experts, even without access to domain\nlabels. Our experiments demonstrate that Point-MoE not only outperforms strong\nmulti-domain baselines but also generalizes better to unseen domains. This work\nhighlights a scalable path forward for 3D understanding: letting the model\ndiscover structure in diverse 3D data, rather than imposing it via manual\ncuration or domain supervision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23926.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634632aaac1cb29fb2ac9f14",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634632aaac1cb29fb2ac9f14/nGZ2TzKOOcKMAR_NFYKkR.jpeg",
      "fullname": "Xuweiyi Chen",
      "name": "Xuweiyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24615",
      "authors": [
        {
          "_id": "683d4295c31058e5bf2e2b0b",
          "name": "Yan Liu",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0c",
          "user": {
            "_id": "646a11791556443f24b582e9",
            "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
            "isPro": false,
            "fullname": "Zonglin Yang",
            "user": "ZonglinY",
            "type": "user"
          },
          "name": "Zonglin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:15.714Z",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0d",
          "name": "Soujanya Poria",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0e",
          "name": "Thanh-Son Nguyen",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0f",
          "name": "Erik Cambria",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T14:08:13.000Z",
      "submittedOnDailyAt": "2025-06-02T04:51:23.329Z",
      "title": "Utilizar el lenguaje natural para explorar un nuevo elemento de la ciencia",
      "submittedOnDailyBy": {
        "_id": "646a11791556443f24b582e9",
        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
        "isPro": false,
        "fullname": "Zonglin Yang",
        "user": "ZonglinY",
        "type": "user"
      },
      "summary": "En la era del crecimiento de la ciencia, la especificidad de las nuevas ideas de investigación es importante pero también puede resultar difícil en el entorno académico. A pesar de las posibles posibilidades, la falta de conjuntos de datos de referencia adecuados impide avanzar en la investigación de nuevas funciones. Lo más importante es que, la simple utilización de las tecnologías actuales de Procesamiento del Lenguaje Natural (NLP), como en búsquedas y verificaciones, no pueden aplicarse de manera coherente con la similitud de las frases y la conceptualización de las ideas. En este artículo, se propone la utilización de modelos de lenguaje grandes (LLMs) para la detección de nuevas funciones (ND) en la ciencia, y se preparan dos nuevos conjuntos de datos relacionados con el mercado y el campo de la NLP. Para construir conjuntos de datos adecuados para la ND, se extraen conjuntos cerrados según las referencias del artículo y se propone la resumen de las ideas principales mediante los LLMs. Para conceptualizar las ideas, se propone que los LLMs se utilicen para obtener con precisión el conocimiento a nivel de ideas y que se ajusten a conceptos similares, y se propone entrenar modelos de búsqueda ligeros para una búsqueda eficiente y precisa de ideas. Las experimentaciones muestran que los métodos propuestos presentan un rendimiento coherente en la búsqueda de ideas y en la ND, comparado con otros métodos. Los códigos y datos están disponibles en https://anonymous.4open.science/r/NoveltyDetection-10FB/.",
      "upvotes": 2,
      "discussionId": "683d4296c31058e5bf2e2b63",
      "ai_summary": "A method utilizing large language models to detect scientific novelty by distilling idea-level knowledge and constructing specialized datasets in marketing and NLP domains.",
      "ai_keywords": [
        "large language models",
        "scientific novelty detection",
        "closure sets",
        "idea retrieval",
        "idea conception",
        "lightweight retriever",
        "knowledge distillation"
      ]
    },
    "publishedAt": "2025-05-30T10:08:13.000Z",
    "title": "Harnessing Large Language Models for Scientific Novelty Detection",
    "summary": "In an era of exponential scientific growth, identifying novel research ideas\nis crucial and challenging in academia. Despite potential, the lack of an\nappropriate benchmark dataset hinders the research of novelty detection. More\nimportantly, simply adopting existing NLP technologies, e.g., retrieving and\nthen cross-checking, is not a one-size-fits-all solution due to the gap between\ntextual similarity and idea conception. In this paper, we propose to harness\nlarge language models (LLMs) for scientific novelty detection (ND), associated\nwith two new datasets in marketing and NLP domains. To construct the\nconsiderate datasets for ND, we propose to extract closure sets of papers based\non their relationship, and then summarize their main ideas based on LLMs. To\ncapture idea conception, we propose to train a lightweight retriever by\ndistilling the idea-level knowledge from LLMs to align ideas with similar\nconception, enabling efficient and accurate idea retrieval for LLM novelty\ndetection. Experiments show our method consistently outperforms others on the\nproposed benchmark datasets for idea retrieval and ND tasks. Codes and data are\navailable at https://anonymous.4open.science/r/NoveltyDetection-10FB/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24615.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646a11791556443f24b582e9",
      "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
      "fullname": "Zonglin Yang",
      "name": "ZonglinY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24517",
      "authors": [
        {
          "_id": "683d3f3100c71614babecb8c",
          "user": {
            "_id": "64395702bb7ded0a0fee8889",
            "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
            "isPro": false,
            "fullname": "Yinqi Li",
            "user": "yinqi",
            "type": "user"
          },
          "name": "Yinqi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:19.037Z",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb8d",
          "name": "Jiahe Zhao",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb8e",
          "name": "Hong Chang",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb8f",
          "name": "Ruibing Hou",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb90",
          "name": "Shiguang Shan",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb91",
          "name": "Xilin Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T12:29:38.000Z",
      "submittedOnDailyAt": "2025-06-02T04:55:28.985Z",
      "title": "un^2CLIP: CLIP: Un método de retroceso para comprender mejor los detalles visuales de CLIP",
      "submittedOnDailyBy": {
        "_id": "64395702bb7ded0a0fee8889",
        "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
        "isPro": false,
        "fullname": "Yinqi Li",
        "user": "yinqi",
        "type": "user"
      },
      "summary": "Contrastive Language-Image Pre-training (CLIP) se utiliza como modelo base y se aplican diversas aplicaciones en tareas visuales y variadas. Sin embargo, recientes estudios muestran que CLIP tiene limitaciones en la capacidad de diferenciar diferencias detalladas en las imágenes y no muestra el mejor rendimiento en tareas de predicción densa y visuales. Por lo tanto, este artículo tiene como objetivo mejorar el modelo CLIP y capturar posibles informaciones visuales detalladas en las imágenes. Hemos descubierto que un modelo generativo específico llamado unCLIP proporciona un marco adecuado para alcanzar este objetivo. En particular, unCLIP entrena un generador de imágenes basado en los embajadores de CLIP. Es decir, invertimos el encoder de CLIP. Comparado con CLIP como modelo discriminativo, el modelo generativo muestra un mejor rendimiento en capturar informaciones detalladas de las imágenes. Además, el espacio de entrada condicional de unCLIP coincide con el espacio original de embajadores de CLIP (imágenes-texto). Por lo tanto, proponemos invertir unCLIP (llamado un^2CLIP) para mejorar el modelo CLIP. Esta mejora en el encoder de imágenes permite capturar informaciones visuales detalladas mientras mantiene la coincidencia con el encoder de texto original. Evaluamos el CLIP mejorado en diversas tareas donde se aplica, especialmente en el benchmark difícil MMVP-VLM, la tarea de segmentación de etiquetas de cajas abiertas para predicción densa, y varias tareas de modelo contra lenguaje. Las experimentos muestran que un^2CLIP mejora significativamente a CLIP original y a otros métodos de mejora anteriores. Los códigos y modelos están disponibles en https://github.com/LiYinqi/un2CLIP.",
      "upvotes": 2,
      "discussionId": "683d3f3200c71614babecbe3",
      "githubRepo": "https://github.com/LiYinqi/un2CLIP",
      "ai_summary": "A generative model framework, unCLIP, is inverted to improve CLIP's ability to capture detailed visual information while maintaining text alignment.",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training",
        "CLIP",
        "unCLIP",
        "image generator",
        "image encoding",
        "data distribution",
        "dense-prediction",
        "vision-centric",
        "multimodal",
        "open-vocabulary segmentation",
        "multimodal large language model",
        "MMVP-VLM benchmark"
      ]
    },
    "publishedAt": "2025-05-30T08:29:38.000Z",
    "title": "un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via\n  Inverting unCLIP",
    "summary": "Contrastive Language-Image Pre-training (CLIP) has become a foundation model\nand has been applied to various vision and multimodal tasks. However, recent\nworks indicate that CLIP falls short in distinguishing detailed differences in\nimages and shows suboptimal performance on dense-prediction and vision-centric\nmultimodal tasks. Therefore, this work focuses on improving existing CLIP\nmodels, aiming to capture as many visual details in images as possible. We find\nthat a specific type of generative models, unCLIP, provides a suitable\nframework for achieving our goal. Specifically, unCLIP trains an image\ngenerator conditioned on the CLIP image embedding. In other words, it inverts\nthe CLIP image encoder. Compared to discriminative models like CLIP, generative\nmodels are better at capturing image details because they are trained to learn\nthe data distribution of images. Additionally, the conditional input space of\nunCLIP aligns with CLIP's original image-text embedding space. Therefore, we\npropose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this\nway, the improved image encoder can gain unCLIP's visual detail capturing\nability while preserving its alignment with the original text encoder\nsimultaneously. We evaluate our improved CLIP across various tasks to which\nCLIP has been applied, including the challenging MMVP-VLM benchmark, the\ndense-prediction open-vocabulary segmentation task, and multimodal large\nlanguage model tasks. Experiments show that un^2CLIP significantly improves\nthe original CLIP and previous CLIP improvement methods. Code and models will\nbe available at https://github.com/LiYinqi/un2CLIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24517.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64395702bb7ded0a0fee8889",
      "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
      "fullname": "Yinqi Li",
      "name": "yinqi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23009",
      "authors": [
        {
          "_id": "683916c60df60182c0dee89d",
          "user": {
            "_id": "66958c29d4ca2767b9c41005",
            "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
            "isPro": true,
            "fullname": "Ruskin Raj Manku",
            "user": "ruskinmanku",
            "type": "user"
          },
          "name": "Ruskin Raj Manku",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:47:12.390Z",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee89e",
          "name": "Yuzhi Tang",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee89f",
          "name": "Xingjian Shi",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee8a0",
          "name": "Mu Li",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee8a1",
          "name": "Alex Smola",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T02:36:24.000Z",
      "submittedOnDailyAt": "2025-06-02T01:24:21.995Z",
      "title": "EmergentTTS-Eval: La evaluación de modelos TTS frente a problemas complejos de pronunciación, expresividad y lenguaje se realiza utilizando el Model-as-a-Judge.",
      "submittedOnDailyBy": {
        "_id": "66958c29d4ca2767b9c41005",
        "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
        "isPro": true,
        "fullname": "Ruskin Raj Manku",
        "user": "ruskinmanku",
        "type": "user"
      },
      "summary": "Text-to-Speech (TTS) benchmarks incluyen varios desafíos para modelos que tratarán textos más complejos de manera más efectiva. Se presenta EmergentTTS-Eval, un framework basado en EmergentTTS. Este framework proporciona seis difíciles escenarios de TTS que incluyen emociones, paralelismo, idiomas extranjeros, complejidad sintáctica, pronunciaciones complejas (como URLs y fórmulas) y preguntas. Una de las características importantes es que automatiza la generación de casos de prueba y la evaluación, facilitando la extensión del benchmark. Inicialmente, se utilizan pequeños conjuntos de prompts escritos por humanos para expandir incrementalmente, utilizando modelos de lenguaje grandes (LLMs), casos de prueba que presenten desafíos estructurales, acústicos y pronunciacionales. Esto genera 1,645 casos de prueba diversos. Además, los modelos utilizan la estructura de evaluación para evaluar la emoción, pronunciación, tono y precisión de pronunciación con un modelo de lenguaje grande de audio (LALM). EmergentTTS-Eval evalua sistemas TTS abiertos y proprietarios (como 11Labs, Deepgram y OpenAI's 4o-mini-TTS) y demuestra sus diferencias de rendimiento. Los resultados muestran que el método utilizando la estructura de evaluación proporciona una evaluación fuerte de TTS y muestra una alta correlación con las preferencias humanas. Los códigos de evaluación y los conjuntos de datos de EmergentTTS-Eval están disponibles en https://github.com/boson-ai/EmergentTTS-Eval-public{code} y https://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset}.",
      "upvotes": 2,
      "discussionId": "683916c70df60182c0dee8dc",
      "ai_summary": "A comprehensive TTS benchmark, EmergentTTS-Eval, automates test-case generation and evaluation using LLMs and LALM to assess nuanced and semantically complex text in speech outputs.",
      "ai_keywords": [
        "EmergentTTS-Eval",
        "LLMs",
        "Large Audio Language Model (LALM)",
        "expressed emotion",
        "prosodic",
        "intonational",
        "pronunciation accuracy",
        "TTS systems",
        "model-as-a-judge"
      ]
    },
    "publishedAt": "2025-05-28T22:36:24.000Z",
    "title": "EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,\n  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge",
    "summary": "Text-to-Speech (TTS) benchmarks often fail to capture how well models handle\nnuanced and semantically complex text. Building on EmergentTTS, we\nintroduce EmergentTTS-Eval, a comprehensive benchmark covering six\nchallenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic\ncomplexity, complex pronunciation (e.g. URLs, formulas), and questions.\nCrucially, our framework automates both test-case generation and evaluation,\nmaking the benchmark easily extensible. Starting from a small set of\nhuman-written seed prompts, we iteratively extend them using LLMs to target\nspecific structural, phonetic and prosodic challenges, resulting in 1,645\ndiverse test cases. Moreover, we employ a model-as-a-judge approach, using a\nLarge Audio Language Model (LALM) to assess the speech across multiple\ndimensions such as expressed emotion, prosodic, intonational, and pronunciation\naccuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,\nsuch as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,\ndemonstrating its ability to reveal fine-grained performance differences.\nResults show that the model-as-a-judge approach offers robust TTS assessment\nand a high correlation with human preferences. We open source the evaluation\nhttps://github.com/boson-ai/EmergentTTS-Eval-public{code} and the\nhttps://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66958c29d4ca2767b9c41005",
      "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
      "fullname": "Ruskin Raj Manku",
      "name": "ruskinmanku",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23844",
      "authors": [
        {
          "_id": "683d0ac47852d920b7dc3dc5",
          "name": "Zhenglun Kong",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc6",
          "name": "Zheng Zhan",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc7",
          "name": "Shiyue Hou",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc8",
          "name": "Yifan Gong",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc9",
          "name": "Xin Meng",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dca",
          "name": "Pengwei Sui",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcb",
          "name": "Peiyan Dong",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcc",
          "name": "Xuan Shen",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcd",
          "name": "Zifeng Wang",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dce",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcf",
          "name": "Hao Tang",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dd0",
          "name": "Stratis Ioannidis",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dd1",
          "name": "Yanzhi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T16:24:50.000Z",
      "submittedOnDailyAt": "2025-06-02T00:52:55.266Z",
      "title": "Implementar el marco de flexibilidad para facilitar la concentración de conocimientos escalables.",
      "submittedOnDailyBy": {
        "_id": "5f2c36551ebc8c6ede2f0e53",
        "avatarUrl": "/avatars/e3ddbd15f50b86958377b5fc2460a57e.svg",
        "isPro": false,
        "fullname": "Tony Kong",
        "user": "TonyK",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) muestran una gran potencial pero presentan desafíos para mejorar continuamente, especialmente cuando se trata de integrar funciones de otros LLMs específicos. Los métodos populares como integración en servidor y integración de pesos requieren mucha memoria y tienen dificultades adaptándose a entornos de datos variables. Recientemente, se han realizado esfuerzos para recopilar conocimiento de varios LLMs y moverlo hacia un único modelo de lenguaje. Sin embargo, se ha observado que se experimenta con interferencias de conocimiento y pérdida de funcionalidades, lo que se atribuye principalmente a la selección de candidatos y la flexibilidad limitada del flujo de trabajo de entrenamiento. Para abordar estos problemas, proponemos un marco que selecciona y centra conocimiento de diferentes LLMs para construir un solo modelo fuerte. Este marco evita el alto overhead de memoria de integración en servidor y pesos, y busca mitigar la limitada flexibilidad de la selección. En particular, diseñamos una red de selección adaptativa que identifica los LLMs fuentes más relevantes y reduce las interferencias de conocimiento. Además, proponemos una estrategia dinámica de integración de pesos que considera las fortalezas únicas de los LLMs candidatos y una función de pérdida retroalimentada que evita que el seleccionador se concentre en un solo conjunto de fuentes. Los resultados de los experimentos muestran que nuestro método reduce las interferencias de conocimiento en un 50% y permite un proceso de integración de conocimiento estable y escalable. El código está disponible en: https://github.com/ZLKong/LLM_Integration.",
      "upvotes": 2,
      "discussionId": "683d0ac57852d920b7dc3e20",
      "projectPage": "https://github.com/ZLKong/LLM_Integration/tree/main",
      "githubRepo": "https://github.com/ZLKong/LLM_Integration/tree/main",
      "ai_summary": "A framework for adaptive selection and dynamic weighted fusion of knowledge from multiple LLMs reduces interference and improves scalability in knowledge aggregation.",
      "ai_keywords": [
        "large language models",
        "fine-tuning",
        "ensemble",
        "weight merging",
        "adaptive selection network",
        "dynamic weighted fusion",
        "feedback-driven loss function",
        "knowledge interference"
      ]
    },
    "publishedAt": "2025-05-28T12:24:50.000Z",
    "title": "Enabling Flexible Multi-LLM Integration for Scalable Knowledge\n  Aggregation",
    "summary": "Large language models (LLMs) have shown remarkable promise but remain\nchallenging to continually improve through traditional finetuning, particularly\nwhen integrating capabilities from other specialized LLMs. Popular methods like\nensemble and weight merging require substantial memory and struggle to adapt to\nchanging data environments. Recent efforts have transferred knowledge from\nmultiple LLMs into a single target model; however, they suffer from\ninterference and degraded performance among tasks, largely due to limited\nflexibility in candidate selection and training pipelines. To address these\nissues, we propose a framework that adaptively selects and aggregates knowledge\nfrom diverse LLMs to build a single, stronger model, avoiding the high memory\noverhead of ensemble and inflexible weight merging. Specifically, we design an\nadaptive selection network that identifies the most relevant source LLMs based\non their scores, thereby reducing knowledge interference. We further propose a\ndynamic weighted fusion strategy that accounts for the inherent strengths of\ncandidate LLMs, along with a feedback-driven loss function that prevents the\nselector from converging on a single subset of sources. Experimental results\ndemonstrate that our method can enable a more stable and scalable knowledge\naggregation process while reducing knowledge interference by up to 50% compared\nto existing approaches. Code is avaliable at\nhttps://github.com/ZLKong/LLM_Integration",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23844.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f2c36551ebc8c6ede2f0e53",
      "avatarUrl": "/avatars/e3ddbd15f50b86958377b5fc2460a57e.svg",
      "fullname": "Tony Kong",
      "name": "TonyK",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21864",
      "authors": [
        {
          "_id": "683b8af5091615f46fabadde",
          "user": {
            "_id": "655a50a850b9a14799165d53",
            "avatarUrl": "/avatars/6bb37ac0b6840771ff7a6a6da4192ace.svg",
            "isPro": false,
            "fullname": "Mengda Xu",
            "user": "mengdaxu",
            "type": "user"
          },
          "name": "Mengda Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:42:10.223Z",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabaddf",
          "name": "Han Zhang",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade0",
          "name": "Yifan Hou",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade1",
          "name": "Zhenjia Xu",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade2",
          "name": "Linxi Fan",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade3",
          "name": "Manuela Veloso",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade4",
          "name": "Shuran Song",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655a50a850b9a14799165d53/IaKGx3B7I2pvk3nOzX9s2.mp4"
      ],
      "publishedAt": "2025-05-28T01:25:27.000Z",
      "submittedOnDailyAt": "2025-06-02T06:21:16.447Z",
      "title": "DexUMI: Manipulación de manos humanas para un interfaz de manipulación generalizada",
      "submittedOnDailyBy": {
        "_id": "655a50a850b9a14799165d53",
        "avatarUrl": "/avatars/6bb37ac0b6840771ff7a6a6da4192ace.svg",
        "isPro": false,
        "fullname": "Mengda Xu",
        "user": "mengdaxu",
        "type": "user"
      },
      "summary": "Se presenta DexUMI. Es un marco de trabajo para la recopilación de datos y el aprendizaje de políticas que permite a diferentes manos de robot aprender habilidades de movimiento de manera detallada mediante una interfaz natural de las manos humanas. DexUMI minimiza las diferencias físicas entre las manos humanas y las de diferentes robots a través de ajustes en el hardware y software. Los ajustes en el hardware incluyen el uso de actuadores escalonados para reducir las diferencias de movilidad, lo que permite proporcionar retroalimentación directa durante la recopilación de datos de movimiento, facilitando así la aplicación de movimientos humanos a manos de robot. Los ajustes en el software reducen las diferencias visuales mediante la transformación de datos de imagen para que las manos humanas se sustituyan por manos de robot de alta calidad usando redes neuronales artificiales. Para demostrar sus capacidades, se realizaron experimentos en dos plataformas de hardware de dos tipos de manos de robot de detalle, logrando un promedio de éxito en tareas del 86%.",
      "upvotes": 1,
      "discussionId": "683b8af8091615f46fabaf00",
      "projectPage": "https://dex-umi.github.io/",
      "githubRepo": "https://github.com/real-stanford/DexUMI",
      "ai_summary": "DexUMI framework utilizes a wearable hand exoskeleton and high-fidelity robot hand inpainting to transfer dexterous manipulation skills from human hands to robot hands, achieving high task success rates.",
      "ai_keywords": [
        "wearable hand exoskeleton",
        "haptic feedback",
        "robot hand inpainting",
        "dexterous manipulation",
        "kinematics",
        "visual gap"
      ]
    },
    "publishedAt": "2025-05-27T21:25:27.000Z",
    "title": "DexUMI: Using Human Hand as the Universal Manipulation Interface for\n  Dexterous Manipulation",
    "summary": "We present DexUMI - a data collection and policy learning framework that uses\nthe human hand as the natural interface to transfer dexterous manipulation\nskills to various robot hands. DexUMI includes hardware and software\nadaptations to minimize the embodiment gap between the human hand and various\nrobot hands. The hardware adaptation bridges the kinematics gap using a\nwearable hand exoskeleton. It allows direct haptic feedback in manipulation\ndata collection and adapts human motion to feasible robot hand motion. The\nsoftware adaptation bridges the visual gap by replacing the human hand in video\ndata with high-fidelity robot hand inpainting. We demonstrate DexUMI's\ncapabilities through comprehensive real-world experiments on two different\ndexterous robot hand hardware platforms, achieving an average task success rate\nof 86%.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655a50a850b9a14799165d53/IaKGx3B7I2pvk3nOzX9s2.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655a50a850b9a14799165d53",
      "avatarUrl": "/avatars/6bb37ac0b6840771ff7a6a6da4192ace.svg",
      "fullname": "Mengda Xu",
      "name": "mengdaxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13157",
      "authors": [
        {
          "_id": "683b58eb84fbd4b28d8d891e",
          "user": {
            "_id": "6469408ab2321e47d3294414",
            "avatarUrl": "/avatars/05621d33f9f6337bc66e59d9e81d05ef.svg",
            "isPro": false,
            "fullname": "Yassine El Boudouri",
            "user": "yelboudouri",
            "type": "user"
          },
          "name": "Yassine El Boudouri",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:42:17.658Z",
          "hidden": false
        },
        {
          "_id": "683b58eb84fbd4b28d8d891f",
          "name": "Walter Nuninger",
          "hidden": false
        },
        {
          "_id": "683b58eb84fbd4b28d8d8920",
          "name": "Julian Alvarez",
          "hidden": false
        },
        {
          "_id": "683b58eb84fbd4b28d8d8921",
          "name": "Yvan Peter",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T14:18:16.000Z",
      "submittedOnDailyAt": "2025-06-02T07:13:19.898Z",
      "title": "Evaluación del Modelo de Roling Plating en Grandes Modelos de Lenguaje",
      "submittedOnDailyBy": {
        "_id": "6469408ab2321e47d3294414",
        "avatarUrl": "/avatars/05621d33f9f6337bc66e59d9e81d05ef.svg",
        "isPro": false,
        "fullname": "Yassine El Boudouri",
        "user": "yelboudouri",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje de gran escala (LLMs) demuestran capacidades que pueden aplicarse tanto a jugadores de roles como a jugadores profesionales. Sin embargo, la evaluación de estas capacidades puede ser un gran desafío, ya que la evaluación humana puede ser muy costosa y el método automático puede incluir sesgos. Para enfrentar estas desafíos, presentamos un nuevo índice de evaluación para las habilidades de jugadores de roles en LLMs, llamado \"Role-Playing Eval (RPEval)\". Esta evaluación se ha construido en cuatro dimensiones: comprensión emocional, política de decisión, coherencia ética y coherencia del personaje. En este artículo, detallamos la construcción y los criterios de evaluación de RPEval. Nuestro código y dataset están disponibles para uso en https://github.com/yelboudouri/RPEval.",
      "upvotes": 1,
      "discussionId": "683b58ec84fbd4b28d8d8935",
      "githubRepo": "https://github.com/yelboudouri/RPEval",
      "ai_summary": "A benchmark called Role-Playing Eval assesses Large Language Models in role-playing across emotional understanding, decision-making, moral alignment, and in-character consistency.",
      "ai_keywords": [
        "Large Language Models",
        "Role-Playing Eval",
        "emotional understanding",
        "decision-making",
        "moral alignment",
        "in-character consistency"
      ]
    },
    "publishedAt": "2025-05-19T10:18:16.000Z",
    "title": "Role-Playing Evaluation for Large Language Models",
    "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13157.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6469408ab2321e47d3294414",
      "avatarUrl": "/avatars/05621d33f9f6337bc66e59d9e81d05ef.svg",
      "fullname": "Yassine El Boudouri",
      "name": "yelboudouri",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23832",
      "authors": [
        {
          "_id": "683d67343f97feb881211cf8",
          "name": "Chaeeun Kim",
          "hidden": false
        },
        {
          "_id": "683d67343f97feb881211cf9",
          "name": "Jinu Lee",
          "hidden": false
        },
        {
          "_id": "683d67343f97feb881211cfa",
          "name": "Wonseok Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T09:02:41.000Z",
      "submittedOnDailyAt": "2025-06-02T07:26:28.412Z",
      "title": "Revisión de casos jurídicos: Revisión de elementos jurídicos en la búsqueda de casos jurídicos revisados",
      "submittedOnDailyBy": {
        "_id": "614c9487cbb5e52274a4024d",
        "avatarUrl": "/avatars/a923db5ea27c4184ed2ce84738860203.svg",
        "isPro": false,
        "fullname": "Chaeeun Kim",
        "user": "Chaeeun-Kim",
        "type": "user"
      },
      "summary": "La búsqueda de casos judiciales (LCR), la búsqueda de casos relacionados a partir de palabras clave es una tarea básica para los expertos en derecho. Sin embargo, actualmente hay dos grandes limitaciones en la investigación. Uno es que el corpus de búsqueda utilizado para evaluar es de una escala relativamente pequeña (por ejemplo, 100-55K casos) y el rango de tipos de palabras clave penales es estrecho, lo que no permite reflejar suficientemente la complejidad de los escenarios de búsqueda de derecho en el mundo real. El otro es que dependen de métodos de embedding o de coincidencia de palabras, lo que limita la coincidencia a expresiones legales irrelevantes. Para enfrentar estos problemas, se presentan LEGAR BENCH y LegalSearchLM. LEGAR BENCH es el primer marco de referencia de gran escala de LCR en Corea, con 120 mil casos judiciales y 411 tipos de delitos diferentes. LegalSearchLM realiza el principio judicial sobre los casos de búsqueda y genera directamente el contenido basado en el caso objetivo mediante decodificación restringida. Los resultados de los experimentos muestran que en LEGAR BENCH superan el base de referencia en un 6-20% y alcanzan un rendimiento avanzado, y superan en más de 15% al modelo AUTOMATOPRIME, aprendido a partir de datos.",
      "upvotes": 0,
      "discussionId": "683d67353f97feb881211d6a"
    },
    "publishedAt": "2025-05-28T05:02:41.000Z",
    "title": "LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements\n  Generation",
    "summary": "Legal Case Retrieval (LCR), which retrieves relevant cases from a query case,\nis a fundamental task for legal professionals in research and decision-making.\nHowever, existing studies on LCR face two major limitations. First, they are\nevaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and\nuse a narrow range of criminal query types, which cannot sufficiently reflect\nthe complexity of real-world legal retrieval scenarios. Second, their reliance\non embedding-based or lexical matching methods often results in limited\nrepresentations and legally irrelevant matches. To address these issues, we\npresent: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering\n411 diverse crime types in queries over 1.2M legal cases; and (2)\nLegalSearchLM, a retrieval model that performs legal element reasoning over the\nquery case and directly generates content grounded in the target cases through\nconstrained decoding. Experimental results show that LegalSearchLM outperforms\nbaselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It\nalso demonstrates strong generalization to out-of-domain cases, outperforming\nnaive generative models trained on in-domain data by 15%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23832.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "614c9487cbb5e52274a4024d",
      "avatarUrl": "/avatars/a923db5ea27c4184ed2ce84738860203.svg",
      "fullname": "Chaeeun Kim",
      "name": "Chaeeun-Kim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]