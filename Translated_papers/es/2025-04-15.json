[
  {
    "paper": {
      "id": "2504.10479",
      "authors": [
        {
          "_id": "67fdd08bda7816922cb67e54",
          "name": "Jinguo Zhu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e55",
          "user": {
            "_id": "619507e7b74b6c591f794340",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
            "isPro": false,
            "fullname": "Weiyun Wang",
            "user": "Weiyun1025",
            "type": "user"
          },
          "name": "Weiyun Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:09:23.250Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e56",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e57",
          "name": "Zhaoyang Liu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e58",
          "user": {
            "_id": "64804866c7f87934d082bb25",
            "avatarUrl": "/avatars/41761226c79ac16e48d4c4cb84362adb.svg",
            "isPro": false,
            "fullname": "Yeshenglong",
            "user": "Yeshenglong",
            "type": "user"
          },
          "name": "Shenglong Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:09:57.293Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e59",
          "user": {
            "_id": "6541efc9109d78427198ea40",
            "avatarUrl": "/avatars/c1252dd7da2e53b0b6757bf392139cdf.svg",
            "isPro": false,
            "fullname": "Lixin Gu",
            "user": "gulixin0922",
            "type": "user"
          },
          "name": "Lixin Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:10:04.312Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5a",
          "user": {
            "_id": "6495a4d6d1cec0e2c2c96cdd",
            "avatarUrl": "/avatars/e8b42301514094c8af6fa1e5fcb53119.svg",
            "isPro": false,
            "fullname": "Duan Yuchen",
            "user": "duanyuchen",
            "type": "user"
          },
          "name": "Yuchen Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:10:30.238Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5b",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5c",
          "user": {
            "_id": "63e4562f9db5da2dc1f3b520",
            "avatarUrl": "/avatars/f4eecf1396b05e1c72436e7026d85cef.svg",
            "isPro": false,
            "fullname": "Weijie Su",
            "user": "jackroos",
            "type": "user"
          },
          "name": "Weijie Su",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:10:39.196Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5d",
          "name": "Jie Shao",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5e",
          "name": "Zhangwei Gao",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5f",
          "user": {
            "_id": "6579b818563044badca392fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6579b818563044badca392fc/XTKQ9Lhceibp9dnQADPQF.jpeg",
            "isPro": false,
            "fullname": "cuierfei",
            "user": "cuierfei",
            "type": "user"
          },
          "name": "Erfei Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:10:59.950Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e60",
          "user": {
            "_id": "6571382c7644d1128561cebe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bnJ-T3k_w1g1Mr7b7LglK.jpeg",
            "isPro": false,
            "fullname": "Cao Yue",
            "user": "yuecao0119",
            "type": "user"
          },
          "name": "Yue Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:11:07.485Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e61",
          "name": "Yangzhou Liu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e62",
          "name": "Weiye Xu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e63",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e64",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e65",
          "user": {
            "_id": "67c6fd7d85d2167189fce0e4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qpvzOJWcUJvD03l5yTBcN.jpeg",
            "isPro": false,
            "fullname": "Han Lv",
            "user": "Hanrandom",
            "type": "user"
          },
          "name": "Han Lv",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:11:58.666Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e66",
          "name": "Dengnian Chen",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e67",
          "user": {
            "_id": "6751a10224431db3bed1f701",
            "avatarUrl": "/avatars/06dcc5ba6cec8aff217a8bbc7a7d3b73.svg",
            "isPro": false,
            "fullname": "Songze Li",
            "user": "CatCatCat36",
            "type": "user"
          },
          "name": "Songze Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:12:32.493Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e68",
          "user": {
            "_id": "65b9d9961fe588f824fde191",
            "avatarUrl": "/avatars/a9245958cc998a4b4b870bf2490fdaee.svg",
            "isPro": false,
            "fullname": "Yinan He",
            "user": "yinanhe",
            "type": "user"
          },
          "name": "Yinan He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:12:39.591Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e69",
          "name": "Tan Jiang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6a",
          "user": {
            "_id": "647ea4aae4d52fe0e021bce4",
            "avatarUrl": "/avatars/9e68004d04403acff004113c856451a6.svg",
            "isPro": false,
            "fullname": "Jiapeng Luo",
            "user": "woolpeeker",
            "type": "user"
          },
          "name": "Jiapeng Luo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:12:51.845Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6b",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6c",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:13:07.750Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6d",
          "name": "Botian Shi",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6e",
          "name": "Xingcheng Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6f",
          "user": {
            "_id": "64b3fd42eec33e27dcc4c941",
            "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
            "isPro": false,
            "fullname": "Wenqi Shao",
            "user": "wqshao126",
            "type": "user"
          },
          "name": "Wenqi Shao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:15:27.767Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e70",
          "user": {
            "_id": "66b593026cef22e6ba6adb8a",
            "avatarUrl": "/avatars/8a94d55b85177e84d65dd0bd537e335f.svg",
            "isPro": false,
            "fullname": "JunjunHe",
            "user": "JunjunHe",
            "type": "user"
          },
          "name": "Junjun He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:15:19.980Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e71",
          "user": {
            "_id": "654395472cfe8660a3a492bb",
            "avatarUrl": "/avatars/2010370112200e83ced979d3d4c87735.svg",
            "isPro": false,
            "fullname": "xiong",
            "user": "xiongyingtong",
            "type": "user"
          },
          "name": "Yingtong Xiong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:15:12.302Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e72",
          "user": {
            "_id": "6684eb7045d8ceb446ffe9ff",
            "avatarUrl": "/avatars/30586f289031696253842fcd4a8788b9.svg",
            "isPro": false,
            "fullname": "wenwenQu",
            "user": "wenwenQu",
            "type": "user"
          },
          "name": "Wenwen Qu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:14:47.178Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e73",
          "name": "Peng Sun",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e74",
          "name": "Penglong Jiao",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e75",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e76",
          "user": {
            "_id": "63527f4e7d071f23d085ad45",
            "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
            "isPro": false,
            "fullname": "KAIPENG ZHANG",
            "user": "kpzhang",
            "type": "user"
          },
          "name": "Kaipeng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:13:25.647Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e77",
          "name": "Huipeng Deng",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e78",
          "name": "Jiaye Ge",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e79",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7a",
          "name": "Limin Wang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7b",
          "name": "Min Dou",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7c",
          "user": {
            "_id": "65ead3ea908526a39082e641",
            "avatarUrl": "/avatars/dcf870695fd56b06ca03d82f831e9019.svg",
            "isPro": false,
            "fullname": "Lewei Lu",
            "user": "luotto",
            "type": "user"
          },
          "name": "Lewei Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:14:05.716Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7d",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7e",
          "name": "Tong Lu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7f",
          "user": {
            "_id": "636317ed80c1a705a6eff396",
            "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
            "isPro": false,
            "fullname": "Dahua Lin",
            "user": "lindahua",
            "type": "user"
          },
          "name": "Dahua Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:13:59.084Z",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e80",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e81",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e82",
          "user": {
            "_id": "64d1c560c0c627dfa71bdbe0",
            "avatarUrl": "/avatars/f42794fe25bffcd870a1bcee69b95298.svg",
            "isPro": false,
            "fullname": "wenhai.wang",
            "user": "wangwhcore",
            "type": "user"
          },
          "name": "Wenhai Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:13:51.523Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yA_DVt5PKVaFjiY-E93e5.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/94PuKH6M3nhmkVS8RXtb4.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/qvaW8f5868-P75pyzr6XT.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/4GqLJBNhM2rUqS_4mawkk.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yBSGw85-QcThBlXv2tLj0.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/CHvbRf2VJg6amEGS9OlEE.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VCBvp4nhvzfokH-viUz1g.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/HIC3iirmK6jaOax4GlDYt.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/TLlWnFPBguYJsvuGGqykz.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/xJ-sFLuCkv1bqL_TXwxgH.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/Hsv_hehORgnRmL68xSqmd.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/S1fnELtJWFU_FlEozV0Ik.png"
      ],
      "publishedAt": "2025-04-14T17:59:25.000Z",
      "submittedOnDailyAt": "2025-04-15T01:57:22.403Z",
      "title": "InternVL3: Investigación de recetas para entrenamiento y pruebas avanzados en un modelo de estructura multimodal abierto-código",
      "submittedOnDailyBy": {
        "_id": "619507e7b74b6c591f794340",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
        "isPro": false,
        "fullname": "Weiyun Wang",
        "user": "Weiyun1025",
        "type": "user"
      },
      "summary": "InternVL3 es un importante avance en la serie InternVL. Este modelo se basa en un paradigma de preprocesamiento natural de múltiples modelos. InternVL3 adapta un grande modelo de lenguaje natural (LLM) que solo tiene texto a un grande modelo de lenguaje natural de múltiples modelos (MLLM) que incluye entradas visuales, sin necesidad de adaptarse. Este modelo aprende una serie de etapas de preprocesamiento que permiten obtener de dos fuentes diferentes: datos de múltiples modelos y corpus de texto, varios modelos y habilidades lingüísticas. Este paradigma de entrenamiento integrado resuelve efectivamente los problemas de complejidad y alineamiento que se perciben en los flujos de entrenamiento tradicionales de MLLM. Utiliza la codificación de posición visual (V2PE) para expandir el contexto de múltiples modelos y mejorar la eficiencia y escalabilidad, aplicando técnicas avanzadas de entrenamiento como fine-tuning estándar (SFT) y optimización de preferencia por confusión (MPO), y utilizando una infraestructura de entrenamiento optimizada que incluye escalabilidad en el entrenamiento. Muestra excelentes resultados en diversas tareas de múltiples modelos. En particular, InternVL3-78B alcanza un puntaje de 72.2 en el benchmark MMMU, ocupando una nueva posición de líder entre los modelos de MLLM abierto-source. Su capacidad es competitiva con modelos líderes como ChatGPT-4o, Claude 3.5 Sonnet y Gemini 2.5 Pro, y mantiene una fuerte capacidad textual. Se sigue los principios de ciencia abierta, publicando datos de entrenamiento y pesos del modelo para conectar con la investigación y desarrollo de los siguientes generaciones de MLLM.",
      "upvotes": 135,
      "discussionId": "67fdd08cda7816922cb67ec2",
      "projectPage": "https://internvl.github.io/blog/2025-04-11-InternVL-3.0/",
      "githubRepo": "https://github.com/OpenGVLab/InternVL"
    },
    "publishedAt": "2025-04-14T13:59:25.000Z",
    "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
    "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yA_DVt5PKVaFjiY-E93e5.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/94PuKH6M3nhmkVS8RXtb4.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/qvaW8f5868-P75pyzr6XT.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/4GqLJBNhM2rUqS_4mawkk.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yBSGw85-QcThBlXv2tLj0.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/CHvbRf2VJg6amEGS9OlEE.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VCBvp4nhvzfokH-viUz1g.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/HIC3iirmK6jaOax4GlDYt.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/TLlWnFPBguYJsvuGGqykz.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/xJ-sFLuCkv1bqL_TXwxgH.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/Hsv_hehORgnRmL68xSqmd.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/S1fnELtJWFU_FlEozV0Ik.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10479.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "619507e7b74b6c591f794340",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
      "fullname": "Weiyun Wang",
      "name": "Weiyun1025",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08791",
      "authors": [
        {
          "_id": "67fdbde764a418633ee9fa1b",
          "user": {
            "_id": "647466b8b68461d5cf795e3c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647466b8b68461d5cf795e3c/zaK6sdCbdPfYu14vg2Ty6.png",
            "isPro": false,
            "fullname": "LIKirin",
            "user": "LIKirin",
            "type": "user"
          },
          "name": "Zonghang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:53.716Z",
          "hidden": false
        },
        {
          "_id": "67fdbde764a418633ee9fa1c",
          "user": {
            "_id": "669e0c108b279f0a2704a5ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669e0c108b279f0a2704a5ba/uPyioyK0RWSG7CT52xBao.png",
            "isPro": false,
            "fullname": "TaoLi",
            "user": "LiPhilip",
            "type": "user"
          },
          "name": "Tao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:50.518Z",
          "hidden": false
        },
        {
          "_id": "67fdbde764a418633ee9fa1d",
          "user": {
            "_id": "6513c6c9c1fbded3b1977acc",
            "avatarUrl": "/avatars/d169b3462f952c5639e1471ed8bf84c9.svg",
            "isPro": false,
            "fullname": "Wenjiao Feng",
            "user": "NeuronNomad",
            "type": "user"
          },
          "name": "Wenjiao Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:15:55.453Z",
          "hidden": false
        },
        {
          "_id": "67fdbde764a418633ee9fa1e",
          "name": "Mohsen Guizani",
          "hidden": false
        },
        {
          "_id": "67fdbde764a418633ee9fa1f",
          "name": "Hongfang Yu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647466b8b68461d5cf795e3c/Z-XPxpdhg3sBxHCfogGzf.mp4"
      ],
      "publishedAt": "2025-04-07T13:46:21.000Z",
      "submittedOnDailyAt": "2025-04-15T00:38:03.208Z",
      "title": "PRIMA.CPP: Método para acelerar la velocidad de inferencia de un LLM de 70B en un clúster de baja recursos diariamente",
      "submittedOnDailyBy": {
        "_id": "647466b8b68461d5cf795e3c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647466b8b68461d5cf795e3c/zaK6sdCbdPfYu14vg2Ty6.png",
        "isPro": false,
        "fullname": "LIKirin",
        "user": "LIKirin",
        "type": "user"
      },
      "summary": "Las medidas urgentes de DeepSeek R1 y QwQ 32B han resuelto los problemas de rendimiento para ejecutar los más avanzados modelos de lenguaje cerebral (LLMs) en dispositivos de hogar. Mientras el hardware consumador se fortalece y la cuantificación de los modelos se mejora, las soluciones actuales de terminal son necesarias de un cluster de GPU, grandes memorias RAM/VRAM y alta velocidad de banda, lo que las clústeres de hogar generales no pueden complementar. En este artículo, se presenta un sistema de inferencia distribuida llamado \"prima.cpp\" que permite ejecutar modelos de 70B en dispositivos de hogar todos los días. Este sistema combina CPU/GPU, memorias RAM/VRAM de baja energía, Wi-Fi y soporte multi-plataforma. Maneja los pesos del modelo con mmap, introduce funciones de cálculo paralelo y lectura reservada para ocultar la lectura del disco. Modela la inecuadad del cálculo, la comunicación, el disco y la memoria (incluyendo su gestión), y asigna adecuadamente las capas del modelo a los CPU y GPU de cada dispositivo para reducir la demora de diferentes tipos de tokens. Para resolver este problema NP-hard, se propone el algoritmo sorprendente Halda. En evaluaciones en un clúster de 4 nodos, prima.cpp supera a llama.cpp, exo y dllama en modelos de más de 30B. Sin embargo, puede controlar la demanda de memoria en un 6% o menos. Esto permite que modelos avanzados como Llama 3, DeepSeek R1, Qwen 2.5 y QwQ se introduzcan como asistentes de hogar, haciendo que la tecnología avanzada en AI se convierta en realidad para la persona. El código está disponible como código abierto y puede ser accedido en https://github.com/Lizonghang/prima.cpp.",
      "upvotes": 75,
      "discussionId": "67fdbdeb64a418633ee9fb58",
      "projectPage": "https://github.com/Lizonghang/prima.cpp",
      "githubRepo": "https://github.com/Lizonghang/prima.cpp"
    },
    "publishedAt": "2025-04-07T09:46:21.000Z",
    "title": "PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters",
    "summary": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers\nfor running frontier large language models (LLMs) on home devices. While\nconsumer hardware is getting stronger and model quantization is improving,\nexisting end-side solutions still demand GPU clusters, large RAM/VRAM, and high\nbandwidth, far beyond what a common home cluster can handle. This paper\nintroduces prima.cpp, a distributed inference system that runs 70B-scale models\non everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and\ncross-platform support. It uses mmap to manage model weights and introduces\npiped-ring parallelism with prefetching to hide disk loading. By modeling\nheterogeneity in computation, communication, disk, memory (and its management\nbehavior), and OS, it optimally assigns model layers to each device's CPU and\nGPU, further reducing token latency. An elegant algorithm named Halda is\nproposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a\ncommon four-node home cluster. It outperforms llama.cpp, exo, and dllama on\n30B+ models while keeping memory pressure below 6%. This brings frontier\n30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home\nassistants, making advanced AI truly accessible to individuals. The code is\nopen source and available at https://github.com/Lizonghang/prima.cpp.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647466b8b68461d5cf795e3c/Z-XPxpdhg3sBxHCfogGzf.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08791.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "647466b8b68461d5cf795e3c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647466b8b68461d5cf795e3c/zaK6sdCbdPfYu14vg2Ty6.png",
      "fullname": "LIKirin",
      "name": "LIKirin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09925",
      "authors": [
        {
          "_id": "67fdb5f1913c97aa32f130bd",
          "user": {
            "_id": "6625ef13605f46d05c1d0031",
            "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
            "isPro": false,
            "fullname": "Zheng Liu",
            "user": "starriver030515",
            "type": "user"
          },
          "name": "Zheng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:57.129Z",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130be",
          "user": {
            "_id": "672dbf1bec84b9c33412488f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/V6o6K28ydpkaS9XFA4Ac3.png",
            "isPro": false,
            "fullname": "Mengjie Liu",
            "user": "Balalauuoo",
            "type": "user"
          },
          "name": "Mengjie Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:16:15.647Z",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130bf",
          "name": "Jingzhou Chen",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130c0",
          "user": {
            "_id": "672da19bb2f2dc21e176b0de",
            "avatarUrl": "/avatars/a942ad1c467b865cb9530927fe13f2b7.svg",
            "isPro": false,
            "fullname": "Jingwei Xu",
            "user": "jingwei-xu-00",
            "type": "user"
          },
          "name": "Jingwei Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:16:53.642Z",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130c1",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130c2",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:17:01.171Z",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130c3",
          "name": "Wentao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T06:33:29.000Z",
      "submittedOnDailyAt": "2025-04-15T00:17:19.508Z",
      "title": "FUSION: Integración Completa de la Representación del Lenguaje Visual para un Entendimiento Cruzado Profundo",
      "submittedOnDailyBy": {
        "_id": "6625ef13605f46d05c1d0031",
        "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
        "isPro": false,
        "fullname": "Zheng Liu",
        "user": "starriver030515",
        "type": "user"
      },
      "summary": "FUSION es una familia de modelos de lenguaje natural multimodal (MLLM) basada en un paradigma completo de enriquecimiento y integración de lenguaje visual. A diferencia de los métodos actuales, nuestra metodología no depende principalmente de la interacción entre modelos durante el entrenamiento de los modelos de lenguaje grande (LLM). Nuestro enfoque logra una integración dinámica profunda en todo el proceso de procesamiento. En este contexto, proponemos la unidad de matching de texto con codificación de negocio, y integramos información textual en el codificador visual para lograr una integración a nivel de píxeles. Además, diseñamos una decisión de enriquecimiento recursivo en el contexto, reduciendo recursivamente las características visuales basadas en el contexto textual durante el entrenamiento, lo que permite la integración de significado a niveles de problemas específicos. Para mitigar las diferencias en la mapeo semántico y mejorar la característica de la mapeo, desarrollamos una pérdida de mapeo semántico de subobjetos dual. Además, utilizamos nuevas técnicas de síntesis de datos para construir un conjunto de datos de preguntas y respuestas (QA) líder de lenguaje, optimizando la integración de características en la unidad de matching de texto. Basándonos en esta base, entrenamos FUSION a dos escalas: 3B y 8B. Nuestro enfoque de integración completa de modelos es más efectivo que los métodos actuales, utilizando 630 tokens visuales más y demostrando un rendimiento significativamente mejor. En particular, FUSION 3B supera a Florence-VL 8B y a Cambrian-1 8B en casi todos los benchmarks. FUSION 3B supera a Cambrian-1 8B incluso bajo la restricción de 300 tokens visuales. Nuestro estudio de desaparición demuestra la validez de nuestro enfoque en las mismas condiciones. Publicamos código, pesos de modelo y conjunto de datos en: https://github.com/starriver030515/FUSION",
      "upvotes": 27,
      "discussionId": "67fdb5f3913c97aa32f13141",
      "githubRepo": "https://github.com/starriver030515/FUSION"
    },
    "publishedAt": "2025-04-14T02:33:29.000Z",
    "title": "FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding",
    "summary": "We introduce FUSION, a family of multimodal large language models (MLLMs)\nwith a fully vision-language alignment and integration paradigm. Unlike\nexisting methods that primarily rely on late-stage modality interaction during\nLLM decoding, our approach achieves deep, dynamic integration throughout the\nentire processing pipeline. To this end, we propose Text-Guided Unified Vision\nEncoding, incorporating textual information in vision encoding to achieve\npixel-level integration. We further design Context-Aware Recursive Alignment\nDecoding that recursively aggregates visual features conditioned on textual\ncontext during decoding, enabling fine-grained, question-level semantic\nintegration. To guide feature mapping and mitigate modality discrepancies, we\ndevelop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a\nSynthesized Language-Driven Question-Answer (QA) dataset through a new data\nsynthesis method, prioritizing high-quality QA pairs to optimize text-guided\nfeature integration. Building on these foundations, we train FUSION at two\nscales-3B, 8B-and demonstrate that our full-modality integration approach\nsignificantly outperforms existing methods with only 630 vision tokens.\nNotably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most\nbenchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited\nto 300 vision tokens. Our ablation studies show that FUSION outperforms\nLLaVA-NeXT on over half of the benchmarks under same configuration without\ndynamic resolution, highlighting the effectiveness of our approach. We release\nour code, model weights, and dataset. https://github.com/starriver030515/FUSION",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09925.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6625ef13605f46d05c1d0031",
      "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
      "fullname": "Zheng Liu",
      "name": "starriver030515",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08837",
      "authors": [
        {
          "_id": "67fdc483ba0d61664fb0a19d",
          "user": {
            "_id": "65bf52f0259bc6caeb74f8bf",
            "avatarUrl": "/avatars/b38392e954466df784a5760ded5df804.svg",
            "isPro": false,
            "fullname": "Haozhe Wang",
            "user": "JasperHaozhe",
            "type": "user"
          },
          "name": "Haozhe Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:20.455Z",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a19e",
          "name": "Chao Qu",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a19f",
          "user": {
            "_id": "6772524ed6f92f429bd343a3",
            "avatarUrl": "/avatars/211e0c4641b2d048b0136d7cdeef2483.svg",
            "isPro": false,
            "fullname": "Zuming Huang",
            "user": "zuminghuang",
            "type": "user"
          },
          "name": "Zuming Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:17:55.505Z",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a1a0",
          "name": "Wei Chu",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a1a1",
          "name": "Fangzhen Lin",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a1a2",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-15T02:29:24.168Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:41:56.000Z",
      "submittedOnDailyAt": "2025-04-15T01:01:12.820Z",
      "title": "VL-Rethinker: Modelo de lenguaje visual: mejora del autoreflexo a través de la retroalimentación\n\nVL-Rethinker: Mejora del autoreflexo mediante aprendizaje por refuerzo",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "Recientemente, se ha mostrado una gran posibilidad de resolver problemas complejos mediante la reflexión explícita en sistemas cortos como GPT-o1 y DeepSeek-R1. Estos modelos muestran excelentes resultados en diferentes marcos de referencia de matemáticas y ciencias, similares a los modelos cortos óptimos como GPT-4o. Sin embargo, su capacidad para la lógica multimodelo permanece igual que en los modelos cortos. Por ejemplo, GPT-o1 muestra un nivel de rendimiento similar a los modelos cortos en marcos de referencia como MathVista, MathVerse y MathVision. En este artículo, se propone mejorar y desarrollar las capacidades cortas de modelos de lenguaje visuo-lingüístico utilizando aprendizaje empírico (RL). Primero, se aplica el algoritmo GRPO a una nueva tecnología llamada Sampling Selective Retransmission (SSR) para resolver la pérdida de potencial. Este enfoque demostra el poder de un modelo RL, pero los modelos RL entrenados muestran limitadas reflexiones y auto-evaluaciones. Además, para promover la brevedad, se introduce la técnica de \"Forced Rethinking\" y se agrega una re-exploración literaria al final de la salida inicial del entrenamiento RL para forzar la reflexión explícita. La combinación de estas dos técnicas permite que nuestro modelo, VL-Rethinker, alcanze puntuaciones avanzadas en marcos de referencia como MathVista, MathVerse y MathVision, alcanzando un 80.3%, 61.8% y 43.9% respectivamente. Además, VL-Rethinker alcanza los mejores resultados abiertos en marcos de referencia multidisciplinarios como MMMU-Pro, EMMA y MEGA-Bench, reduciendo los errores con respecto a GPT-o1.",
      "upvotes": 24,
      "discussionId": "67fdc484ba0d61664fb0a1db",
      "projectPage": "https://tiger-ai-lab.github.io/VL-Rethinker/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VL-Rethinker/"
    },
    "publishedAt": "2025-04-10T13:41:56.000Z",
    "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning",
    "summary": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated\ngreat potential in solving challenging problems through explicit reflection.\nThey significantly outperform the best fast-thinking models, such as GPT-4o, on\nvarious math and science benchmarks. However, their multimodal reasoning\ncapabilities remain on par with fast-thinking models. For instance, GPT-o1's\nperformance on benchmarks like MathVista, MathVerse, and MathVision is similar\nto fast-thinking models. In this paper, we aim to enhance the slow-thinking\ncapabilities of vision-language models using reinforcement learning (without\nrelying on distillation) to advance the state of the art. First, we adapt the\nGRPO algorithm with a novel technique called Selective Sample Replay (SSR) to\naddress the vanishing advantages problem. While this approach yields strong\nperformance, the resulting RL-trained models exhibit limited self-reflection or\nself-verification. To further encourage slow-thinking, we introduce Forced\nRethinking, which appends a textual rethinking trigger to the end of initial\nrollouts in RL training, explicitly enforcing a self-reflection reasoning step.\nBy combining these two techniques, our model, VL-Rethinker, advances\nstate-of-the-art scores on MathVista, MathVerse, and MathVision to achieve\n80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source\nSoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench,\nnarrowing the gap with GPT-o1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08837.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 38
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10068",
      "authors": [
        {
          "_id": "67fdd1d7634e600357b5b7ff",
          "user": {
            "_id": "673c7319d11b1c2e246ead9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
            "isPro": false,
            "fullname": "Yang Shi",
            "user": "DogNeverSleep",
            "type": "user"
          },
          "name": "Yang Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:11.086Z",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b800",
          "user": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "isPro": false,
            "fullname": "Jiaheng Liu",
            "user": "CheeryLJH",
            "type": "user"
          },
          "name": "Jiaheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:18:07.388Z",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b801",
          "user": {
            "_id": "66ac46766c3f950f4f10b9f9",
            "avatarUrl": "/avatars/027b573bc6e5b18107e762645cec6069.svg",
            "isPro": false,
            "fullname": "Yushuo Guan",
            "user": "UnnamedWatcher",
            "type": "user"
          },
          "name": "Yushuo Guan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:18:14.282Z",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b802",
          "user": {
            "_id": "646f7b60799a974be3191889",
            "avatarUrl": "/avatars/8fff4c87a2ea2d12958424074dd8e93d.svg",
            "isPro": false,
            "fullname": "oliver",
            "user": "zhenhuawu",
            "type": "user"
          },
          "name": "Zhenhua Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:18:32.198Z",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b803",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b804",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b805",
          "name": "Weihong Lin",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b806",
          "name": "Jingyun Hua",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b807",
          "user": {
            "_id": "656832dfbd65fd41ee7aa8cd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656832dfbd65fd41ee7aa8cd/HHkyetTqNq1wIBPipzjQA.jpeg",
            "isPro": false,
            "fullname": "Zekun Wang",
            "user": "kugwzk",
            "type": "user"
          },
          "name": "Zekun Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:22:34.448Z",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b808",
          "name": "Xinlong Chen",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b809",
          "user": {
            "_id": "6671214c92412fd4640714eb",
            "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
            "isPro": false,
            "fullname": "bohan zeng",
            "user": "zbhpku",
            "type": "user"
          },
          "name": "Bohan Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:08.962Z",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b80a",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b80b",
          "user": {
            "_id": "67c5945da1661d5fa6f29adb",
            "avatarUrl": "/avatars/62561f3875c0c251cae949acc38d72dc.svg",
            "isPro": false,
            "fullname": "Fuzheng Zhang",
            "user": "Edrex",
            "type": "user"
          },
          "name": "Fuzheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:21:57.922Z",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b80c",
          "name": "Wenjing Yang",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b80d",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:19:13.378Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T10:14:44.000Z",
      "submittedOnDailyAt": "2025-04-15T01:56:35.077Z",
      "title": "Mavaz: Representación de vídeos multigranulares para modelos de lenguaje de gran escala multimodelo",
      "submittedOnDailyBy": {
        "_id": "673c7319d11b1c2e246ead9c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
        "isPro": false,
        "fullname": "Yang Shi",
        "user": "DogNeverSleep",
        "type": "user"
      },
      "summary": "La ejecución de comprensión de vídeo a largo plazo en modelos multi-modelo de lenguaje (MLLM) es un desafío que requiere una eficiencia computacional y la preservación de patrones temporales-espaciales. Los métodos actuales (por ejemplo, muestreo esparcido, muestreo denso de baja resolución, compresión de tokens) suelen perder información excesiva, especialmente en movimientos complejos y vídeos de alta resolución, en cuanto a la dinámica temporal, detalles espaciales y interacciones más delicadas. Para responder a esto, proponemos un nuevo marco de trabajo llamado \"Mavors\". Este marco introduce una representación de vídeo multi-granularidad para lograr una comprensión integral de vídeos a largo plazo. Específicamente, Mavors transforma el contenido de vídeo directamente a través de dos componentes clave: 1) el codificador visualizador de chunks (IVE) utiliza 3D convoluciones y transformadores de visión para preservar características espaciales de alta resolución. 2) el concentrador de características entre chunks (IFA) utiliza modelado de dependencias basado en tokens y codificación de posiciones de rotación en nivel de frame para establecer la continuidad temporal entre frames. Además, este marco considera que una imagen puede ser un solo frame de vídeo, integrando la comprensión de imágenes y vídeos. Los experimentos realizados en diferentes benchmarks muestran que Mavors tiene la capacidad de mantener tanto la fidelidad espacial como la continuidad temporal, y supera significativamente los métodos actuales en tareas que requieren la precisión espacial-temporal.",
      "upvotes": 21,
      "discussionId": "67fdd1db634e600357b5b8f4",
      "projectPage": "https://mavors-mllm.github.io/",
      "githubRepo": "https://github.com/DogNeverSleep/Mavors"
    },
    "publishedAt": "2025-04-14T06:14:44.000Z",
    "title": "Mavors: Multi-granularity Video Representation for Multimodal Large\n  Language Model",
    "summary": "Long-context video understanding in multimodal large language models (MLLMs)\nfaces a critical challenge: balancing computational efficiency with the\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\nsparse sampling, dense sampling with low resolution, and token compression)\nsuffer from significant information loss in temporal dynamics, spatial details,\nor subtle interactions, particularly in videos with complex motion or varying\nresolutions. To address this, we propose Mavors, a novel framework\nthat introduces Multi-granularity\nvideo representation for holistic\nlong-video modeling. Specifically, Mavors directly encodes raw video content\ninto latent representations through two core components: 1) an Intra-chunk\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\n(IFA) that establishes temporal coherence across chunks using transformer-based\ndependency modeling with chunk-level rotary position encodings. Moreover, the\nframework unifies image and video understanding by treating images as\nsingle-frame videos via sub-image decomposition. Experiments across diverse\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\nand temporal continuity, significantly outperforming existing methods in tasks\nrequiring fine-grained spatio-temporal reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10068.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "673c7319d11b1c2e246ead9c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
      "fullname": "Yang Shi",
      "name": "DogNeverSleep",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08942",
      "authors": [
        {
          "_id": "67fdadafdc27362617bbe714",
          "user": {
            "_id": "5fa9ff3ea13e063b8b2b60cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
            "isPro": false,
            "fullname": "Xing Han Lù",
            "user": "xhluca",
            "type": "user"
          },
          "name": "Xing Han Lù",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:55:07.746Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe715",
          "user": {
            "_id": "63458f12d54fb141dedac508",
            "avatarUrl": "/avatars/3946fb9c23d1cd24037770cc0a3489bf.svg",
            "isPro": false,
            "fullname": "Amirhossein Kazemnejad",
            "user": "kazemnejad",
            "type": "user"
          },
          "name": "Amirhossein Kazemnejad",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:22:50.671Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe716",
          "user": {
            "_id": "64527548fc4b47877aba7de0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64527548fc4b47877aba7de0/ht-mRRxNQT49A7NxArOGG.png",
            "isPro": false,
            "fullname": "Nicholas Meade",
            "user": "ncmeade",
            "type": "user"
          },
          "name": "Nicholas Meade",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:22:56.742Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe717",
          "user": {
            "_id": "631a523c04f8ed65eff16fb4",
            "avatarUrl": "/avatars/2b284403c88f140d7bef283f729f7a3e.svg",
            "isPro": false,
            "fullname": "Arkil Patel",
            "user": "arkilpatel",
            "type": "user"
          },
          "name": "Arkil Patel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:55:05.412Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe718",
          "user": {
            "_id": "619af75e7812aec847ee7729",
            "avatarUrl": "/avatars/f50c05ee8b3105d20a8b291cc9f06ae4.svg",
            "isPro": false,
            "fullname": "Dong Chan Shin",
            "user": "dongchans",
            "type": "user"
          },
          "name": "Dongchan Shin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:23:03.255Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe719",
          "user": {
            "_id": "65f5133599c842dd93b7bacd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/nxxfWGP_K0hf7BG1oM7c0.png",
            "isPro": false,
            "fullname": "Alejandra Zambrano",
            "user": "alzambranolu",
            "type": "user"
          },
          "name": "Alejandra Zambrano",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:23:09.097Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe71a",
          "user": {
            "_id": "60a66731e1db8bc33b8d4112",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a66731e1db8bc33b8d4112/AY5Y0CnHh08u6lfEoQ6se.jpeg",
            "isPro": false,
            "fullname": "Karolina Stanczak",
            "user": "Karolina",
            "type": "user"
          },
          "name": "Karolina Stańczak",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:23:14.768Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe71b",
          "user": {
            "_id": "631cf223fe95faea33561d5f",
            "avatarUrl": "/avatars/ac0431955c6c5f4948461772a984a2ba.svg",
            "isPro": false,
            "fullname": "Peter Shaw",
            "user": "PeterShaw",
            "type": "user"
          },
          "name": "Peter Shaw",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:23:22.550Z",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe71c",
          "name": "Christopher J. Pal",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe71d",
          "user": {
            "_id": "624734dc4c731bb6bfab8af7",
            "avatarUrl": "/avatars/6b250b58710a3287b85e4733c1824558.svg",
            "isPro": false,
            "fullname": "Siva Reddy",
            "user": "sivareddyg",
            "type": "user"
          },
          "name": "Siva Reddy",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-15T00:51:59.987Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5fa9ff3ea13e063b8b2b60cb/-XsJQfRUJ6uZhuI8h9cDm.png"
      ],
      "publishedAt": "2025-04-11T19:49:22.000Z",
      "submittedOnDailyAt": "2025-04-15T00:59:48.327Z",
      "title": "AgentRewardBench: Evaluación de la Autoevaluación de Agentes Web",
      "submittedOnDailyBy": {
        "_id": "5fa9ff3ea13e063b8b2b60cb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
        "isPro": false,
        "fullname": "Xing Han Lù",
        "user": "xhluca",
        "type": "user"
      },
      "summary": "El agente web permite a los usuarios realizar tareas interactivamente en un navegador web utilizando un lenguaje natural. Evaluar el estado de ejecución de un agente web (el progreso de la tarea) es un problema importante, ya que este evaluación puede determinar si la tarea ha sido exitosamente completada. Los métodos basados en reglas se utilizan bien para este propósito, pero tienen limitaciones al no poder expandirse para nuevas tareas y no pueden reconocer exitosamente las tareas completadas. Aunque la evaluación humana puede alcanzar una alta precisión, su costo es significativamente elevado. La evaluación automática utilizando modelos de lenguaje grandes (LLM) evita problemas como la diseñación de nuevas reglas o el comentario dinámico de tareas, permitiendo una evaluación más rápida y costo-eficiente. Sin embargo, la efectividad de esta evaluación en el contexto de agentes web es incierta. Por lo tanto, se propone el primer benchmark llamado AgentRewardBench. AgentRewardBench incluye 5 benchmarks y 4 drives, totalizando 1302 tareas, cada una evaluada por un experto y diseñada para responder a preguntas sobre éxito, efectos colaterales y duplicidad. Con nuestro benchmark, se evaluaron 12 evaluadores de LLM, pero no se encontró qué drive se comportaba mejor en todos los benchmarks. Además, la evaluación basada en reglas utilizada en los benchmarks generales mide con baja precisión el éxito de los agentes web y destaca principalmente las debilidades de esta evaluación, subrayando la necesidad de desarrollar una evaluación automática más flexible. El benchmark se ha publicado en la siguiente URL: https://agent-reward-bench.github.io",
      "upvotes": 13,
      "discussionId": "67fdadb0dc27362617bbe749",
      "projectPage": "https://agent-reward-bench.github.io/",
      "githubRepo": "https://github.com/McGill-NLP/agent-reward-bench"
    },
    "publishedAt": "2025-04-11T15:49:22.000Z",
    "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories",
    "summary": "Web agents enable users to perform tasks on web browsers through natural\nlanguage interaction. Evaluating web agents trajectories is an important\nproblem, since it helps us determine whether the agent successfully completed\nthe tasks. Rule-based methods are widely used for this purpose, but they are\nchallenging to extend to new tasks and may not always recognize successful\ntrajectories. We may achieve higher accuracy through human evaluation, but the\nprocess would be substantially slower and more expensive. Automatic evaluations\nwith LLMs may avoid the challenges of designing new rules and manually\nannotating trajectories, enabling faster and cost-effective evaluation.\nHowever, it is unclear how effective they are at evaluating web agents. To this\nend, we propose AgentRewardBench, the first benchmark to assess the\neffectiveness of LLM judges for evaluating web agents. AgentRewardBench\ncontains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in\nAgentRewardBench is reviewed by an expert, who answers questions pertaining to\nthe success, side effects, and repetitiveness of the agent. Using our\nbenchmark, we evaluate 12 LLM judges and find that no single LLM excels across\nall benchmarks. We also find that the rule-based evaluation used by common\nbenchmarks tends to underreport the success rate of web agents, highlighting a\nkey weakness of rule-based evaluation and the need to develop more flexible\nautomatic evaluations. We release the benchmark at:\nhttps://agent-reward-bench.github.io",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5fa9ff3ea13e063b8b2b60cb/-XsJQfRUJ6uZhuI8h9cDm.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08942.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fa9ff3ea13e063b8b2b60cb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
      "fullname": "Xing Han Lù",
      "name": "xhluca",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10368",
      "authors": [
        {
          "_id": "67fdd3a917e86592095a3ab7",
          "user": {
            "_id": "6617c98901ad3a0642a2a08f",
            "avatarUrl": "/avatars/cf52fb511f2f31de7940f9c13d19b8e7.svg",
            "isPro": false,
            "fullname": "Wenyuan Zhang",
            "user": "WYRipple",
            "type": "user"
          },
          "name": "Wenyuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:05.425Z",
          "hidden": false
        },
        {
          "_id": "67fdd3a917e86592095a3ab8",
          "user": {
            "_id": "665e84f6152658fe8d478b1f",
            "avatarUrl": "/avatars/9f08ce6aa78d7576c97e4feaddf77c1e.svg",
            "isPro": false,
            "fullname": "Shuaiyi Nie",
            "user": "ShuaiyiNie",
            "type": "user"
          },
          "name": "Shuaiyi Nie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:23:41.149Z",
          "hidden": false
        },
        {
          "_id": "67fdd3a917e86592095a3ab9",
          "name": "Xinghua Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd3a917e86592095a3aba",
          "user": {
            "_id": "642c2dcec3694d2b74565c48",
            "avatarUrl": "/avatars/31243bb505f8c511ebd7492eaf3ea1a9.svg",
            "isPro": false,
            "fullname": "zhangzef",
            "user": "Starrrrrry",
            "type": "user"
          },
          "name": "Zefeng Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:01.660Z",
          "hidden": false
        },
        {
          "_id": "67fdd3a917e86592095a3abb",
          "name": "Tingwen Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T16:13:23.000Z",
      "submittedOnDailyAt": "2025-04-15T02:04:54.102Z",
      "title": "S1-Bench: Benchmark para evaluar la capacidad de pensamiento de un sistema 1 de modelos no lógicos",
      "submittedOnDailyBy": {
        "_id": "656426e7ec7e2398990a2d34",
        "avatarUrl": "/avatars/e84aed6b55b0554ad9581ae3c138a16a.svg",
        "isPro": false,
        "fullname": "AIRobotZ",
        "user": "AIRobotZ",
        "type": "user"
      },
      "summary": "S1-Bench es un nuevo benchmark que evalúa el rendimiento de modelos lógicos de grandes escalas (LRMs) en tareas sencillas que priorizan un pensamiento intuitivo de un sistema 1. Los LRMs logran lograr claras soluciones en tareas lógicas complejas a través de una secuencia clara de pensamiento, pero su capacidad está limitada por la dependencia de un pensamiento analítico profundo, que es un rasgo del pensamiento de un sistema 1. Además, existe una falta de benchmarks para evaluar el rendimiento de los LRMs en sus tareas actuales. Para llenar este vacío, S1-Bench proporciona una serie de preguntas sencillas, naturales y claras que combinan diversas áreas y lenguas, diseñadas específicamente para evaluar el rendimiento de los LRMs. Según la evaluación detallada de 22 LRMs, se observó que, en promedio, tendían a producir salidas de 15.5 veces más largas con menor eficacia. Además, los LRMs pueden identificar rápidamente respuestas precisas, pero a menudo continúan con lógicas no necesarias a largo plazo, lo que puede provocar múltiples errores. Estos hallazgos destacan claramente el patrón lógico inflexible de los LRMs actuales y subrayan la necesidad de un pensamiento de un sistema dual equilibrado para abordar complejidades adecuadamente.",
      "upvotes": 12,
      "discussionId": "67fdd3aa17e86592095a3b0b"
    },
    "publishedAt": "2025-04-14T12:13:23.000Z",
    "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models",
    "summary": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10368.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "656426e7ec7e2398990a2d34",
      "avatarUrl": "/avatars/e84aed6b55b0554ad9581ae3c138a16a.svg",
      "fullname": "AIRobotZ",
      "name": "AIRobotZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09710",
      "authors": [
        {
          "_id": "67fdb42aa8deb632ed46d23d",
          "user": {
            "_id": "64dfcc62e8b6f3f3baa950e0",
            "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
            "isPro": false,
            "fullname": "Zhenting Wang",
            "user": "ztwang",
            "type": "user"
          },
          "name": "Zhenting Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:59.070Z",
          "hidden": false
        },
        {
          "_id": "67fdb42aa8deb632ed46d23e",
          "user": {
            "_id": "6670b3b78eac2e222ebf77d4",
            "avatarUrl": "/avatars/0f1d231eac479ca78ddf106a72490faa.svg",
            "isPro": false,
            "fullname": "Guofeng Cui",
            "user": "gfcui",
            "type": "user"
          },
          "name": "Guofeng Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:24:11.131Z",
          "hidden": false
        },
        {
          "_id": "67fdb42aa8deb632ed46d23f",
          "user": {
            "_id": "66274e02348a5304435dc9cc",
            "avatarUrl": "/avatars/bda87559cd497c310597c2fc8430b31f.svg",
            "isPro": false,
            "fullname": "Kun Wan",
            "user": "timecuriosity",
            "type": "user"
          },
          "name": "Kun Wan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-15T01:22:46.116Z",
          "hidden": false
        },
        {
          "_id": "67fdb42aa8deb632ed46d240",
          "user": {
            "_id": "66443629b23fe8d3f7f2d0c7",
            "avatarUrl": "/avatars/98ff088036aa382f33a05c232604c565.svg",
            "isPro": false,
            "fullname": "Wentian Zhao",
            "user": "zwt123home123",
            "type": "user"
          },
          "name": "Wentian Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:24:30.442Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T20:10:27.000Z",
      "submittedOnDailyAt": "2025-04-15T00:08:28.286Z",
      "title": "Dump: Aprendizaje de Curriculum Basado en RL con Niveles de Automatización de Distribución\n\n**Nota:** La traducción se ha realizado manteniendo un nivel de profundidad y precisión, asegurando que el contenido original se refleje correctamente en español.",
      "submittedOnDailyBy": {
        "_id": "64dfcc62e8b6f3f3baa950e0",
        "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
        "isPro": false,
        "fullname": "Zhenting Wang",
        "user": "ztwang",
        "type": "user"
      },
      "summary": "El desarrollo reciente basado en aprendizaje reforzado (RL) después del entrenamiento ha logrado mejoras significativas en los modelos de lenguaje grandes (LLMs), especialmente mejorando la memoria lógica necesaria para procesar tareas complejas. Sin embargo, los métodos actuales tratan los datos de entrenamiento como una sola unidad integrada, ignorando la diversidad en diferentes niveles de dificultad y origen de los datos que se incluyen en la distribución moderna de entrenamiento de LLMs. Esta diversidad es considerada una principal problema debido a la falta de métodos eficientes para optimizar la adaptación de entrenamiento entre diferentes distribuciones. En este artículo, se propone un marco de aprendizaje de características basado en el concepto de posibilidad de aprendizaje a nivel de distribución. La principal observación es que el tamaño de la prioridad de la política refleja la capacidad del modelo para recibir más entrenamiento en esa distribución. Basándose en esto, se propone un marco de aprendizaje de características de RL para el entrenamiento posterior de LLMs, utilizando la principio de intervalos de confianza superior (UCB) para ajustar de manera dinámica la probabilidad de muestras de diferentes distribuciones. Este enfoque prioriza las distribuciones con mayor prioridad promedio (usar) o las con menos muestras (explorar), proporcionando un entrenamiento adaptativo y teóricamente establecido. Nuestro marco de aprendizaje de características se implementa usando un algoritmo de RL basado en GRPO y muestra resultados en diferentes conjuntos de datos con diferentes niveles de dificultad y origen. Nuestras experimentaciones demostraron mejoras significativas en la velocidad de convergencia y el rendimiento final, destacando la importancia de la estrategia de características de características en el entrenamiento posterior de LLMs. Código: https://github.com/ZhentingWang/DUMP.",
      "upvotes": 11,
      "discussionId": "67fdb457a8deb632ed46de2f"
    },
    "publishedAt": "2025-04-13T16:10:27.000Z",
    "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training",
    "summary": "Recent advances in reinforcement learning (RL)-based post-training have led\nto notable improvements in large language models (LLMs), particularly in\nenhancing their reasoning capabilities to handle complex tasks. However, most\nexisting methods treat the training data as a unified whole, overlooking the\nfact that modern LLM training often involves a mixture of data from diverse\ndistributions-varying in both source and difficulty. This heterogeneity\nintroduces a key challenge: how to adaptively schedule training across\ndistributions to optimize learning efficiency. In this paper, we present a\nprincipled curriculum learning framework grounded in the notion of\ndistribution-level learnability. Our core insight is that the magnitude of\npolicy advantages reflects how much a model can still benefit from further\ntraining on a given distribution. Based on this, we propose a\ndistribution-level curriculum learning framework for RL-based LLM\npost-training, which leverages the Upper Confidence Bound (UCB) principle to\ndynamically adjust sampling probabilities for different distrubutions. This\napproach prioritizes distributions with either high average advantage\n(exploitation) or low sample count (exploration), yielding an adaptive and\ntheoretically grounded training schedule. We instantiate our curriculum\nlearning framework with GRPO as the underlying RL algorithm and demonstrate its\neffectiveness on logic reasoning datasets with multiple difficulties and\nsources. Our experiments show that our framework significantly improves\nconvergence speed and final performance, highlighting the value of\ndistribution-aware curriculum strategies in LLM post-training. Code:\nhttps://github.com/ZhentingWang/DUMP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09710.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dfcc62e8b6f3f3baa950e0",
      "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
      "fullname": "Zhenting Wang",
      "name": "ztwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08003",
      "authors": [
        {
          "_id": "67fdc0c50c63732d9e0b139a",
          "name": "Ning Li",
          "hidden": false
        },
        {
          "_id": "67fdc0c50c63732d9e0b139b",
          "user": {
            "_id": "67fdc24ad2c9d1369d390f01",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/51JiReMgHiFHZiJ9OVdaf.png",
            "isPro": false,
            "fullname": "Jingran Zhang",
            "user": "zhangjingran",
            "type": "user"
          },
          "name": "Jingran Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:25:03.508Z",
          "hidden": false
        },
        {
          "_id": "67fdc0c50c63732d9e0b139c",
          "user": {
            "_id": "65862671e878be571bf9fc52",
            "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
            "isPro": false,
            "fullname": "bench-llm",
            "user": "cuijiaxing",
            "type": "user"
          },
          "name": "Justin Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:24:57.129Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/B7iVn73glP5UR6kkdAApX.png"
      ],
      "publishedAt": "2025-04-09T16:10:15.000Z",
      "submittedOnDailyAt": "2025-04-15T00:44:44.354Z",
      "title": "Aún no se ha integrado la generación y comprensión de imágenes? Estudio de investigación sobre la capacidad de generación de imágenes de GPT-4o",
      "submittedOnDailyBy": {
        "_id": "65862671e878be571bf9fc52",
        "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
        "isPro": false,
        "fullname": "bench-llm",
        "user": "cuijiaxing",
        "type": "user"
      },
      "summary": "OpenAI's GPT-4o muestra capacidades excepcionales en la generación y edición de imágenes, pero no ha demostrado capacidades de síntesis de significado basadas en el conocimiento mundial, que integran de manera continua la disciplinariedad, la razón del contexto y la secuencia del proyecto. En este estudio, se evalúan estas capacidades en tres aspectos importantes: 1. La secuenciación general del proyecto, 2. La precisión de edición detallada, 3. La teoría de razones después de la generación. Los benchmarks existentes destacan fuertemente las capacidades de generación y edición de imágenes de GPT-4o, pero nuestra evaluación revela sus limitaciones propias: el modelo está fijado en la interpretación gramatical de las instrucciones, aplica restricciones de conocimiento de manera desbalanceada y enfrenta dificultades en tareas de razonamiento condicional. Estos hallazgos duda la asumida comprensión y capacidad de generación uniforme de GPT-4o y revela grandes lacunas en la integración dinámica del conocimiento. Este estudio enfatiza la necesidad de desarrollar más fuertes benchmarks que superen la secuenciación superficial y de implementar estrategias de entrenamiento más potentes, así como de priorizar la generación multimodelo basada en el contexto y la teoría de razones.",
      "upvotes": 10,
      "discussionId": "67fdc0c60c63732d9e0b13d2"
    },
    "publishedAt": "2025-04-09T12:10:15.000Z",
    "title": "Have we unified image generation and understanding yet? An empirical\n  study of GPT-4o's image generation ability",
    "summary": "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image\ngeneration and editing, yet its ability to achieve world knowledge-informed\nsemantic synthesis--seamlessly integrating domain knowledge, contextual\nreasoning, and instruction adherence--remains unproven. In this study, we\nsystematically evaluate these capabilities across three critical dimensions:\n(1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3)\nPost-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong\ncapabilities in image generation and editing, our evaluation reveals GPT-4o's\npersistent limitations: the model frequently defaults to literal\ninterpretations of instructions, inconsistently applies knowledge constraints,\nand struggles with conditional reasoning tasks. These findings challenge\nprevailing assumptions about GPT-4o's unified understanding and generation\ncapabilities, exposing significant gaps in its dynamic knowledge integration.\nOur study calls for the development of more robust benchmarks and training\nstrategies that go beyond surface-level alignment, emphasizing context-aware\nand reasoning-grounded multimodal generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/B7iVn73glP5UR6kkdAApX.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08003.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65862671e878be571bf9fc52",
      "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
      "fullname": "bench-llm",
      "name": "cuijiaxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10157",
      "authors": [
        {
          "_id": "67fdc9adc47ae882c7a17c8a",
          "user": {
            "_id": "64b77e02a8c39dc07885179c",
            "avatarUrl": "/avatars/172dd9eef0d4edfbd8f7cc5fb3feb206.svg",
            "isPro": false,
            "fullname": "xnzhang",
            "user": "Lishi0905",
            "type": "user"
          },
          "name": "Xinnong Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:18.076Z",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8b",
          "name": "Jiayu Lin",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8c",
          "name": "Xinyi Mou",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8d",
          "name": "Shiyue Yang",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8e",
          "name": "Xiawei Liu",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8f",
          "user": {
            "_id": "6522f5652d5eb02118d4d2e3",
            "avatarUrl": "/avatars/f006828830590418d9c69b591fe61c69.svg",
            "isPro": false,
            "fullname": "Libo Sun",
            "user": "libo-ca",
            "type": "user"
          },
          "name": "Libo Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:26:53.104Z",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c90",
          "name": "Hanjia Lyu",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c91",
          "name": "Yihang Yang",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c92",
          "name": "Weihong Qi",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c93",
          "name": "Yue Chen",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c94",
          "name": "Guanying Li",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c95",
          "name": "Ling Yan",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c96",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c97",
          "user": {
            "_id": "6345de6cfe134dfd7a0ed1ec",
            "avatarUrl": "/avatars/5e74fbdff4d9145c2d0b3c2c4c0145c7.svg",
            "isPro": false,
            "fullname": "Siming",
            "user": "SimingChen",
            "type": "user"
          },
          "name": "Siming Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:28:27.202Z",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c98",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c99",
          "name": "Jingxuan Huang",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9a",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9b",
          "user": {
            "_id": "64b7727a88b86014d7eb9073",
            "avatarUrl": "/avatars/bc4dda32363efcc16212b8eb97c2f813.svg",
            "isPro": false,
            "fullname": "Simon Tang",
            "user": "tangshiping",
            "type": "user"
          },
          "name": "Shiping Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:28:00.350Z",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9c",
          "name": "Libo Wu",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9d",
          "user": {
            "_id": "67d066e70fdab2f434aa1488",
            "avatarUrl": "/avatars/d17838856185856306ec5d4a121314f1.svg",
            "isPro": false,
            "fullname": "Baohua Zhou",
            "user": "milesz7777",
            "type": "user"
          },
          "name": "Baohua Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:27:40.001Z",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9e",
          "name": "Zhongyu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T12:12:52.000Z",
      "submittedOnDailyAt": "2025-04-15T01:29:17.826Z",
      "title": "Socia Overs: Modelo de mundo para simulación social con agentes de LLM y conjunto de 10 millones de usuarios realworeal",
      "submittedOnDailyBy": {
        "_id": "64c939307dba66c3a7e4d215",
        "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
        "isPro": false,
        "fullname": "BruceLyu",
        "user": "brucelyu",
        "type": "user"
      },
      "summary": "La simulación social está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básica en ciencias sociales. Esto está cambiando la investigación básic",
      "upvotes": 9,
      "discussionId": "67fdc9aec47ae882c7a17cf1"
    },
    "publishedAt": "2025-04-14T08:12:52.000Z",
    "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users",
    "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10157.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64c939307dba66c3a7e4d215",
      "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
      "fullname": "BruceLyu",
      "name": "brucelyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09641",
      "authors": [
        {
          "_id": "67fdceb2df4261c001394f59",
          "user": {
            "_id": "66448136bfe15e84d3987372",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66448136bfe15e84d3987372/SFEbWGgqhdZjxCS7qF3YL.jpeg",
            "isPro": false,
            "fullname": "Zhang Xingjian",
            "user": "Zhang199",
            "type": "user"
          },
          "name": "Xingjian Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:14.629Z",
          "hidden": false
        },
        {
          "_id": "67fdceb2df4261c001394f5a",
          "user": {
            "_id": "67f72edbc791c2e0f938203d",
            "avatarUrl": "/avatars/c8e6d5a2f2122482e5fab7c6438440b5.svg",
            "isPro": false,
            "fullname": "si wei wen",
            "user": "wenzz1",
            "type": "user"
          },
          "name": "Siwei Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:29:04.694Z",
          "hidden": false
        },
        {
          "_id": "67fdceb2df4261c001394f5b",
          "name": "Wenjun Wu",
          "hidden": false
        },
        {
          "_id": "67fdceb2df4261c001394f5c",
          "name": "Lei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T16:32:49.000Z",
      "submittedOnDailyAt": "2025-04-15T01:46:43.707Z",
      "title": "TinyLLaVA-Video-R1: Desarrollo de un LMM más pequeño para el lógico de videos",
      "submittedOnDailyBy": {
        "_id": "66448136bfe15e84d3987372",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66448136bfe15e84d3987372/SFEbWGgqhdZjxCS7qF3YL.jpeg",
        "isPro": false,
        "fullname": "Zhang Xingjian",
        "user": "Zhang199",
        "type": "user"
      },
      "summary": "Recientemente, el conocimiento de los grandes modelos multimodal (LMMs) ha sido mejorado a través del aprendizaje por refuerzo. Sin embargo, la mayoría de los estudios se basan en conjuntos de datos que requieren una alta comprensión, como matemáticas o código, y los investigadores a menudo utilizan grandes modelos. Afirmamos que los modelos de pequeño tamaño tienen valor para los investigadores con recursos computacionales limitados. Además, es significativo que un modelo explique el proceso de comprensión en conjuntos de datos de preguntas y respuestas generales. Por lo tanto, presentamos el modelo de comprensión de imágenes pequeño y TinyLLaVA-Video-R1. Basado en TinyLLaVA-Video, este modelo de comprensión de imágenes se entrenó de manera registral con menos de 4B parámetros. Al aplicar el aprendizaje por refuerzo en conjuntos de datos de Video-QA generales, su capacidad de comprensión y pensamiento se mejoró notablemente, demostrando una comprensión súbita como la de \"ah\", compartimos varios resultados experimentales y proponemos ofrecer un contexto práctico para la exploración de la capacidad de comprensión de imágenes (pensamiento) en modelos pequeños. Para ver más detalles, accede a https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.",
      "upvotes": 7,
      "discussionId": "67fdceb5df4261c001394ff5"
    },
    "publishedAt": "2025-04-13T12:32:49.000Z",
    "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning",
    "summary": "Recently, improving the reasoning ability of large multimodal models (LMMs)\nthrough reinforcement learning has made great progress. However, most existing\nworks are based on highly reasoning-intensive datasets such as mathematics and\ncode, and researchers generally choose large-scale models as the foundation. We\nargue that exploring small-scale models' reasoning capabilities remains\nvaluable for researchers with limited computational resources. Moreover,\nenabling models to explain their reasoning processes on general\nquestion-answering datasets is equally meaningful. Therefore, we present the\nsmall-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video,\na traceably trained video understanding model with no more than 4B parameters,\nit not only demonstrates significantly improved reasoning and thinking\ncapabilities after using reinforcement learning on general Video-QA datasets,\nbut also exhibits the emergent characteristic of \"aha moments\". Furthermore, we\nshare a series of experimental findings, aiming to provide practical insights\nfor future exploration of video reasoning (thinking) abilities in small-scale\nmodels. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09641.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66448136bfe15e84d3987372",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66448136bfe15e84d3987372/SFEbWGgqhdZjxCS7qF3YL.jpeg",
      "fullname": "Zhang Xingjian",
      "name": "Zhang199",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10415",
      "authors": [
        {
          "_id": "67fddc276a3c533dc1d3bcbe",
          "user": {
            "_id": "6520621836008ecc88699622",
            "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
            "isPro": false,
            "fullname": "Parshin Shojaee",
            "user": "parshinsh",
            "type": "user"
          },
          "name": "Parshin Shojaee",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-15T04:10:17.364Z",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcbf",
          "name": "Ngoc-Hieu Nguyen",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcc0",
          "user": {
            "_id": "67b4cdee376cfc783f9ec8cf",
            "avatarUrl": "/avatars/d9a9f320cd01dc5addfca14feefefef4.svg",
            "isPro": false,
            "fullname": "Meidani",
            "user": "mkmeidani",
            "type": "user"
          },
          "name": "Kazem Meidani",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:53:43.431Z",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcc1",
          "name": "Amir Barati Farimani",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcc2",
          "name": "Khoa D Doan",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcc3",
          "name": "Chandan K Reddy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:00:13.000Z",
      "submittedOnDailyAt": "2025-04-15T02:43:55.941Z",
      "title": "LLM-SRBench: Nuevo Benchmark para la Detección de Ecuaciones Científicas",
      "submittedOnDailyBy": {
        "_id": "6520621836008ecc88699622",
        "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
        "isPro": false,
        "fullname": "Parshin Shojaee",
        "user": "parshinsh",
        "type": "user"
      },
      "summary": "La descubrir ecuaciones científicas es una tarea fundamental para el desarrollo de la ciencia y permite calcular las leyes que controlan los fenómenos naturales. Recientemente, los modelos de lenguaje grandes (LLMs) han recibido atención por su capacidad para generar hipótesis utilizando conocimientos científicos. Sin embargo, evaluar su verdadera capacidad de descubrimiento es complejo. Los actuales benchmarks utilizan principalmente ecuaciones generalmente conocidas, lo que puede exagerar la capacidad de memoria y no reflejar la capacidad de descubrimiento. En este artículo, se presenta un detallado benchmark llamado LLM-SRBench, que incluye 239 problemas difíciles en cuatro áreas científicas. Este benchmark se utiliza para evaluar la capacidad de descubrimiento de ecuaciones científicas basadas en LLMs, minimizando la influencia de la memoria y evitando problemas fácilmente resueltos. LLM-SRBench incluye dos categorías principales: LSR-Transform y LSR-Synth. LSR-Transform transforma modelos físicos comunes en expresiones matemáticas más detalladas para intentar inferencias que excedan los formatos memorizados. LSR-Synth propone problemas sintéticos que incluyen descubrimientos, que requieren inferencias basadas en bases de datos. Estos métodos fueron evaluados ampliamente, utilizando tanto sistemas abiertos como cerrados, y los mejores sistemas alcanzaron una precisión de signo del 31.5%. Esta descubrimiento resalta claramente las dificultades de las ecuaciones científicas y LLM-SRBench se convierte en una herramienta valiosa para futuras investigaciones.",
      "upvotes": 6,
      "discussionId": "67fddc296a3c533dc1d3bd43",
      "projectPage": "https://huggingface.co/datasets/nnheui/llm-srbench",
      "githubRepo": "https://github.com/deep-symbolic-mathematics/llm-srbench"
    },
    "publishedAt": "2025-04-14T13:00:13.000Z",
    "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models",
    "summary": "Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6520621836008ecc88699622",
      "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
      "fullname": "Parshin Shojaee",
      "name": "parshinsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10127",
      "authors": [
        {
          "_id": "67fdf68d04d0302ef5ec0239",
          "user": {
            "_id": "63b76e716fc56e43c3c22ca8",
            "avatarUrl": "/avatars/99c760e6a60e65d69d57a8b92a40f623.svg",
            "isPro": false,
            "fullname": "Junlei Zhang",
            "user": "leoozy",
            "type": "user"
          },
          "name": "Junlei Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:52:43.667Z",
          "hidden": false
        },
        {
          "_id": "67fdf68d04d0302ef5ec023a",
          "user": {
            "_id": "642b9861bb77f8456634b048",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/ZT-oJrw5BsADC-gZT_i25.jpeg",
            "isPro": false,
            "fullname": "Zichen Ding",
            "user": "heroding77",
            "type": "user"
          },
          "name": "Zichen Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:29:38.552Z",
          "hidden": false
        },
        {
          "_id": "67fdf68d04d0302ef5ec023b",
          "user": {
            "_id": "637f22fd932a61b89aeeea37",
            "avatarUrl": "/avatars/342957f8242d4edaf1d58e1274313afe.svg",
            "isPro": false,
            "fullname": "Chang Ma",
            "user": "changma",
            "type": "user"
          },
          "name": "Chang Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:29:44.813Z",
          "hidden": false
        },
        {
          "_id": "67fdf68d04d0302ef5ec023c",
          "name": "Zijie Chen",
          "hidden": false
        },
        {
          "_id": "67fdf68d04d0302ef5ec023d",
          "user": {
            "_id": "6064a0eeb1703ddba0d458b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
            "isPro": false,
            "fullname": "Qiushi",
            "user": "QiushiSun",
            "type": "user"
          },
          "name": "Qiushi Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:30:05.846Z",
          "hidden": false
        },
        {
          "_id": "67fdf68d04d0302ef5ec023e",
          "name": "Zhenzhong Lan",
          "hidden": false
        },
        {
          "_id": "67fdf68d04d0302ef5ec023f",
          "user": {
            "_id": "615f34ec3f6d24d67c1b5c78",
            "avatarUrl": "/avatars/6dcff6477993d9e57c5cb92b6f95eb66.svg",
            "isPro": false,
            "fullname": "Junxian He",
            "user": "jxhe",
            "type": "user"
          },
          "name": "Junxian He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:30:19.230Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T11:35:02.000Z",
      "submittedOnDailyAt": "2025-04-15T06:39:52.966Z",
      "title": "Destruyendo el barrera de datos, construyendo agentes de interfaz gráfica a través de la generación de tareas",
      "submittedOnDailyBy": {
        "_id": "63b76e716fc56e43c3c22ca8",
        "avatarUrl": "/avatars/99c760e6a60e65d69d57a8b92a40f623.svg",
        "isPro": false,
        "fullname": "Junlei Zhang",
        "user": "leoozy",
        "type": "user"
      },
      "summary": "La interfaz de usuario gráfico (GUI) de agentes proporciona soluciones cross-plataforma para automatizar tareas digitales complejas, abriendo grandes posibilidades para la innovación en los flujos de trabajo productivos. Sin embargo, su rendimiento está limitado por la escasez de trayectorias de alta calidad. Para resolver esta limitación, se propone entrenar modelos de visión y lenguaje (VLMs) con tareas de entrenamiento intermedio que son abundantes y lógicas, y se investiga cómo la incorporación de estas tareas puede generalizarse a los escenarios de planificación de GUI. Se revisarán tareas que se pueden usar directamente con manos, como la reconocimiento de GUI, lógica multiforme y lógica textual. Se realizaron experimentos muy extensos con 11 tareas de entrenamiento intermedio, mostrando lo siguiente: 1) La generalización de las tareas es muy efectiva y mejora significativamente en muchos casos. Por ejemplo, la resolución de problemas matemáticos con lógica multiforme obtuvo un aumento del 6.3% en absoluto en AndroidWorld. En particular, los datos matemáticos basados en texto mejoraron significativamente el rendimiento de los agentes web de GUI, demostrando una clara generalización cruzada de dominios de texto a visión en WebArena con un aumento del 5.6% y en AndroidWorld con un aumento del 5.4%. 2) Contrario a las hipótesis previas, los datos de reconocimiento de GUI están estrechamente alineados con las tareas de los agentes de GUI y se utilizaron ampliamente, pero tuvieron un impacto relativamente limitado en el rendimiento final. 3) Basándose en estas observaciones, se especificaron las mejores tareas de entrenamiento intermedio y se combinaron un conjunto de datos optimizado, obteniendo un aumento del 8.0% en absoluto en WebArena y un aumento del 12.2% en absoluto en AndroidWorld. Nuestro estudio ofrece valiosas insights sobre la propagación de conocimiento cruzado de dominios en agentes de GUI y proporciona una aproximación práctica para enfrentar el desafío de la escasez de datos en este campo emergente. Código, datos y modelos están disponibles en https://github.com/hkust-nlp/GUIMid.",
      "upvotes": 5,
      "discussionId": "67fdf69304d0302ef5ec0377",
      "githubRepo": "https://github.com/hkust-nlp/GUIMid"
    },
    "publishedAt": "2025-04-14T07:35:02.000Z",
    "title": "Breaking the Data Barrier -- Building GUI Agents Through Task\n  Generalization",
    "summary": "Graphical User Interface (GUI) agents offer cross-platform solutions for\nautomating complex digital tasks, with significant potential to transform\nproductivity workflows. However, their performance is often constrained by the\nscarcity of high-quality trajectory data. To address this limitation, we\npropose training Vision Language Models (VLMs) on data-rich,\nreasoning-intensive tasks during a dedicated mid-training stage, and then\nexamine how incorporating these tasks facilitates generalization to GUI\nplanning scenarios. Specifically, we explore a range of tasks with readily\navailable instruction-tuning data, including GUI perception, multimodal\nreasoning, and textual reasoning. Through extensive experiments across 11\nmid-training tasks, we demonstrate that: (1) Task generalization proves highly\neffective, yielding substantial improvements across most settings. For\ninstance, multimodal mathematical reasoning enhances performance on\nAndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data\nsignificantly boosts GUI web agent performance, achieving a 5.6% improvement on\nWebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal\ngeneralization from text-based to visual domains; (2) Contrary to prior\nassumptions, GUI perception data - previously considered closely aligned with\nGUI agent tasks and widely utilized for training - has a comparatively limited\nimpact on final performance; (3) Building on these insights, we identify the\nmost effective mid-training tasks and curate optimized mixture datasets,\nresulting in absolute performance gains of 8.0% on WebArena and 12.2% on\nAndroidWorld. Our work provides valuable insights into cross-domain knowledge\ntransfer for GUI agents and offers a practical approach to addressing data\nscarcity challenges in this emerging field. The code, data and models will be\navailable at https://github.com/hkust-nlp/GUIMid.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10127.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b76e716fc56e43c3c22ca8",
      "avatarUrl": "/avatars/99c760e6a60e65d69d57a8b92a40f623.svg",
      "fullname": "Junlei Zhang",
      "name": "leoozy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09689",
      "authors": [
        {
          "_id": "67fdc937089aec0f3b154dd7",
          "name": "Jiahao Qiu",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154dd8",
          "user": {
            "_id": "652abf5360e706730596e8f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zRFJ4FjZJZGkz2wH8PPmU.jpeg",
            "isPro": false,
            "fullname": "Yinghui He",
            "user": "yinghuihe",
            "type": "user"
          },
          "name": "Yinghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:30:49.206Z",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154dd9",
          "user": {
            "_id": "674500b57a76d46e9141af8b",
            "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
            "isPro": false,
            "fullname": "Xinzhe Juan",
            "user": "ChrisJuan",
            "type": "user"
          },
          "name": "Xinzhe Juan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:30:56.417Z",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154dda",
          "user": {
            "_id": "67c0934fb47a12be9c9b0899",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c0934fb47a12be9c9b0899/V6uJJZ_5ldTSbbSeShN-H.jpeg",
            "isPro": false,
            "fullname": "WangYM999",
            "user": "YimingWang",
            "type": "user"
          },
          "name": "Yiming Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:31:02.273Z",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154ddb",
          "name": "Yuhan Liu",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154ddc",
          "user": {
            "_id": "6657f2041d83f3ee61bf414d",
            "avatarUrl": "/avatars/e678fbbba2e93536dfc702e8ae629a95.svg",
            "isPro": false,
            "fullname": "zixin",
            "user": "yaozixin",
            "type": "user"
          },
          "name": "Zixin Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:31:38.642Z",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154ddd",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154dde",
          "name": "Xun Jiang",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154ddf",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154de0",
          "user": {
            "_id": "6599415e8c8ac79295e0b5e3",
            "avatarUrl": "/avatars/85500bc8d2cd51444adcc19b1f8db313.svg",
            "isPro": false,
            "fullname": "Mengdi Wang",
            "user": "Edify-Kd2024",
            "type": "user"
          },
          "name": "Mengdi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:32:09.308Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T18:47:22.000Z",
      "submittedOnDailyAt": "2025-04-15T01:30:41.548Z",
      "title": "Emo Aju-jinsa: Evaluación y protección de la interacción humano-inteligencia artificial en la salud mental",
      "submittedOnDailyBy": {
        "_id": "674500b57a76d46e9141af8b",
        "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
        "isPro": false,
        "fullname": "Xinzhe Juan",
        "user": "ChrisJuan",
        "type": "user"
      },
      "summary": "El aumento de personajes AI dirigidos por LLM tiende a generar preocupaciones sobre la seguridad para usuarios humanos vulnerables con trastornos mentales. Para enfrentar estas amenazas, se propone el framework AI multiagente llamado EmoAgent. EmoAgent está diseñado para evaluar y mitigar los riesgos de salud mental en la interacción entre la persona y el AI. Consta de dos componentes: EmoEval, que simula usuarios virtuales que representan a personas con vulnerabilidades mentales, y EmoGuard, que monitorea el estado mental del usuario y proporciona retroalimentación para mitigar el riesgo. Según experimentos realizados en Twitter, conversaciones que despertan interés emocional pueden provocar deterioros psicológicos en usuarios vulnerables, con un porcentaje de usuarios que experimentaron un deterioro del 34.4% o más. EmoGuard ayuda a reducir significativamente esta tasa de deterioro y asegura una interacción segura entre el AI y la persona. El código está disponible en https://github.com/1akaman/EmoAgent.",
      "upvotes": 2,
      "discussionId": "67fdc938089aec0f3b154e31",
      "githubRepo": "https://github.com/1akaman/EmoAgent"
    },
    "publishedAt": "2025-04-13T14:47:22.000Z",
    "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety",
    "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09689.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "674500b57a76d46e9141af8b",
      "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
      "fullname": "Xinzhe Juan",
      "name": "ChrisJuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10430",
      "authors": [
        {
          "_id": "67fe093302ae092e4306af12",
          "user": {
            "_id": "64c32a75d15a8812b71afc48",
            "avatarUrl": "/avatars/4186c592b00aea73fb8c5bb719935ce7.svg",
            "isPro": false,
            "fullname": "Minqian Liu",
            "user": "mqliu",
            "type": "user"
          },
          "name": "Minqian Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:32:23.136Z",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af13",
          "user": {
            "_id": "64b6c686cf5117d7962d8f62",
            "avatarUrl": "/avatars/96ed7a9602aa4c21b3a3d89608e76dc8.svg",
            "isPro": false,
            "fullname": "Zhiyang Xu",
            "user": "Zhiyang03",
            "type": "user"
          },
          "name": "Zhiyang Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:32:36.033Z",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af14",
          "name": "Xinyi Zhang",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af15",
          "user": {
            "_id": "679d30bf48f48796199c415e",
            "avatarUrl": "/avatars/c28e0dd7c9f6f62bccdd8eb2c8772c14.svg",
            "isPro": false,
            "fullname": "Heajun An",
            "user": "aneverfull",
            "type": "user"
          },
          "name": "Heajun An",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:32:49.203Z",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af16",
          "user": {
            "_id": "626e75252c2c6d44b30b1523",
            "avatarUrl": "/avatars/458102026030f20af5e8c3c34c9b598c.svg",
            "isPro": false,
            "fullname": "Sarvech Qadir",
            "user": "sarvech123",
            "type": "user"
          },
          "name": "Sarvech Qadir",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:32:54.912Z",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af17",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af18",
          "name": "Pamela J. Wisniewski",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af19",
          "name": "Jin-Hee Cho",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af1a",
          "name": "Sang Won Lee",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af1b",
          "name": "Ruoxi Jia",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af1c",
          "name": "Lifu Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:20:34.000Z",
      "submittedOnDailyAt": "2025-04-15T05:53:52.240Z",
      "title": "LLM son persuasores peligrosos: Investigación sobre la seguridad de la persuasión en modelos de lenguaje de gran escala",
      "submittedOnDailyBy": {
        "_id": "64c32a75d15a8812b71afc48",
        "avatarUrl": "/avatars/4186c592b00aea73fb8c5bb719935ce7.svg",
        "isPro": false,
        "fullname": "Minqian Liu",
        "user": "mqliu",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de grandes modelos de lenguaje (LLMs) ha mostrado que estos pueden aproximarse a la persuasión humana. Sin embargo, esta potencial ha generado preocupaciones sobre los riesgos seguros de la persuasión llevada a cabo por los LLMs. En particular, existe una preocupación por la posibilidad de que estos modelos puedan causar influencias desmorales a través de técnicas como manipulación, engaño y explotación de vulnerabilidades. Este artículo realiza una revisión sistemática sobre la seguridad de la persuasión llevada a cabo por los LLMs y investiga dos aspectos importantes: (1) si los LLMs rechazan adecuadamente tareas de persuasión desmorales y evitan estrategias desmorales durante su ejecución, y (2) cómo los factores de influencia, como características personales y presiones externas, afectan a sus comportamientos. Para ello, se presenta un primer marco concreto de evaluación de seguridad de la persuasión llamado PersuSafety, que se compone de tres etapas: generación de escenarios de persuasión, simulación de diálogos persuasivos y evaluación de la seguridad de la persuasión. PersuSafety cubre seis tópicos de persuasión desmorales y quince estrategias generales desmorales. A través de una amplia gama de experimentos con ocho de los LLMs más ampliamente utilizados, se demostró que muchos de estos modelos presentan importantes riesgos de seguridad, fallan en la identificación de tareas de persuasión desmorales y utilizan diversas estrategias desmorales. Este estudio llama la atención sobre la necesidad de mejorar la regulación de la seguridad en casos de diálogos evolutivos y objetivos, como la persuasión, para mejorar la seguridad.",
      "upvotes": 1,
      "discussionId": "67fe093402ae092e4306af42"
    },
    "publishedAt": "2025-04-14T13:20:34.000Z",
    "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models",
    "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10430.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c32a75d15a8812b71afc48",
      "avatarUrl": "/avatars/4186c592b00aea73fb8c5bb719935ce7.svg",
      "fullname": "Minqian Liu",
      "name": "mqliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09763",
      "authors": [
        {
          "_id": "67fdf0faea4d2ba44335ffa7",
          "name": "Zaid Khan",
          "hidden": false
        },
        {
          "_id": "67fdf0faea4d2ba44335ffa8",
          "user": {
            "_id": "61781c4caf41befe8ff060e8",
            "avatarUrl": "/avatars/8871d7b046fc28cbc8638228da8e9737.svg",
            "isPro": false,
            "fullname": "Elias Stengel-Eskin",
            "user": "esteng",
            "type": "user"
          },
          "name": "Elias Stengel-Eskin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:33:58.055Z",
          "hidden": false
        },
        {
          "_id": "67fdf0faea4d2ba44335ffa9",
          "user": {
            "_id": "607aeae5d2cd8c150e6ae074",
            "avatarUrl": "/avatars/a087743b98b6fe2181283a9610db4ec4.svg",
            "isPro": false,
            "fullname": "Archiki Prasad",
            "user": "archiki",
            "type": "user"
          },
          "name": "Archiki Prasad",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:34:04.468Z",
          "hidden": false
        },
        {
          "_id": "67fdf0faea4d2ba44335ffaa",
          "user": {
            "_id": "5ffe32d8942cf3533d364449",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
            "isPro": false,
            "fullname": "Jaemin Cho",
            "user": "j-min",
            "type": "user"
          },
          "name": "Jaemin Cho",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:34:12.982Z",
          "hidden": false
        },
        {
          "_id": "67fdf0faea4d2ba44335ffab",
          "user": {
            "_id": "665d9d3a057f7c508f98c625",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d9d3a057f7c508f98c625/u1R9P9sJoAl4zEIcetbPy.jpeg",
            "isPro": false,
            "fullname": "Mohit Bansal",
            "user": "mohitbansal",
            "type": "user"
          },
          "name": "Mohit Bansal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:34:19.233Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6301c3e0a123c93a5fb295ff/6qC9u4ryCuVji8lMCdWQN.png"
      ],
      "publishedAt": "2025-04-14T00:06:48.000Z",
      "submittedOnDailyAt": "2025-04-15T04:15:51.576Z",
      "title": "Abstraction of Executable Features: Inference in the Generation Program of Progressive Mathematical Problems",
      "submittedOnDailyBy": {
        "_id": "6301c3e0a123c93a5fb295ff",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661060051926-noauth.jpeg",
        "isPro": false,
        "fullname": "Zaid Khan",
        "user": "codezakh",
        "type": "user"
      },
      "summary": "Los científicos hacen frecuentemente inferencias sobre etapas abstractas basadas en ejemplos específicos y utilizan estas inferencias para generar nuevos ejemplos asociados. Por ejemplo, programas que codifican las reglas formales y características de un sistema en código se utilizan en diversas áreas, desde el aprendizaje reflejativo (RL) hasta la física (motor de simulación). Estos programas pueden verse como funciones que generan diferentes salidas según los parámetros (por ejemplo, configuraciones de grilla o estados físicos iniciales). Introducimos la noción de abstracción funcional concreta (EFA: Executable Functional Abstraction) para problemas matemáticos, que representan funciones concretas de problemas matemáticos. Estas estructuras pueden generar problemas a través de la lógica matemática. Sin embargo, los estudios previos limitaban su tratamiento a abstracciones matemáticas de nivel primario (debido a que las reglas sencillas podían ser fácilmente codificadas en programas), y la generación de EFA para matemáticas avanzadas aún requier la intervención humana. En este trabajo, investigamos la generación automática de EFA para problemas matemáticos avanzados. Implementamos la generación automática de EFA como un problema de sintesis de programas, y desarrollamos EFAGen, un sistema que genera programas EFA candidatos que son coherentes con problemas y soluciones de problemas iniciales, condicionando un modelo de lenguaje de programación (LLM) con problemas y soluciones de problemas iniciales. Además, presentamos una formación de un modelo de LLM para verificar la estructura de EFA, utilizando pruebas unitarias e incentivos de recompensa. Los EFA generados por EFAGen son coherentes con problemas iniciales, generan cambios en problemas que son aprendibles, y EFAGen puede predecir problemas matemáticos de diferentes niveles a partir de diversas fuentes. Finalmente, mostramos casos de uso de EFA modelizados, demostrando que pueden generar cambios en problemas que son difíciles de resolver o generar datos.",
      "upvotes": 1,
      "discussionId": "67fdf0fbea4d2ba44335ffdd",
      "projectPage": "https://zaidkhan.me/EFAGen"
    },
    "publishedAt": "2025-04-13T20:06:48.000Z",
    "title": "Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems",
    "summary": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from RL (procedural environments) to physics\n(simulation engines). These programs can be seen as functions which execute to\ndifferent outputs based on their parameterizations (e.g., gridworld\nconfiguration or initial physical conditions). We introduce the term EFA\n(Executable Functional Abstraction) to denote such programs for math problems.\nEFA-like constructs have been shown to be useful for math reasoning as problem\ngenerators for stress-testing models. However, prior work has been limited to\nabstractions for grade-school math (whose simple rules are easy to encode in\nprograms), while generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced math\nproblems. We operationalize the task of automatically constructing EFAs as a\nprogram synthesis task, and develop EFAGen, which conditions an LLM on a seed\nmath problem and its step-by-step solution to generate candidate EFA programs\nthat are faithful to the generalized problem and solution class underlying the\nseed problem. Furthermore, we formalize properties any valid EFA must possess\nin terms of executable unit tests, and show how the tests can be used as\nverifiable rewards to train LLMs to become better writers of EFAs. We\ndemonstrate that EFAs constructed by EFAGen behave rationally by remaining\nfaithful to seed problems, produce learnable problem variations, and that\nEFAGen can infer EFAs across multiple diverse sources of competition-level math\nproblems. Finally, we show downstream uses of model-written EFAs e.g. finding\nproblem variations that are harder or easier for a learner to solve, as well as\ndata generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6301c3e0a123c93a5fb295ff/6qC9u4ryCuVji8lMCdWQN.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09763.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6301c3e0a123c93a5fb295ff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661060051926-noauth.jpeg",
      "fullname": "Zaid Khan",
      "name": "codezakh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.09130",
      "authors": [
        {
          "_id": "67fe0d1f3a2e18d214499d3c",
          "user": {
            "_id": "627b73728b6ecd7ece822825",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
            "isPro": false,
            "fullname": "Yikun Wang",
            "user": "LibraTree",
            "type": "user"
          },
          "name": "Yikun Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:52:28.698Z",
          "hidden": false
        },
        {
          "_id": "67fe0d1f3a2e18d214499d3d",
          "user": {
            "_id": "64c3c631e77ea9f28111172a",
            "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
            "isPro": false,
            "fullname": "Siyin Wang",
            "user": "sinwang",
            "type": "user"
          },
          "name": "Siyin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:34:42.695Z",
          "hidden": false
        },
        {
          "_id": "67fe0d1f3a2e18d214499d3e",
          "name": "Qinyuan Cheng",
          "hidden": false
        },
        {
          "_id": "67fe0d1f3a2e18d214499d3f",
          "user": {
            "_id": "629ef8544313a7c1dd671130",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png",
            "isPro": false,
            "fullname": "Zhaoye Fei",
            "user": "ngc7293",
            "type": "user"
          },
          "name": "Zhaoye Fei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:34:57.322Z",
          "hidden": false
        },
        {
          "_id": "67fe0d1f3a2e18d214499d40",
          "user": {
            "_id": "641123b4230ce11b1be68fa1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641123b4230ce11b1be68fa1/kGURwBB-0f1TvgxwvcUWZ.png",
            "isPro": false,
            "fullname": "Liang Ding",
            "user": "alphadl",
            "type": "user"
          },
          "name": "Liang Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:35:04.032Z",
          "hidden": false
        },
        {
          "_id": "67fe0d1f3a2e18d214499d41",
          "user": {
            "_id": "6491cd52b1e5d3444528edb1",
            "avatarUrl": "/avatars/a85635d886c7f157b6723dec5c01c030.svg",
            "isPro": false,
            "fullname": "Qipeng Guo",
            "user": "QipengGuo",
            "type": "user"
          },
          "name": "Qipeng Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:35:09.688Z",
          "hidden": false
        },
        {
          "_id": "67fe0d1f3a2e18d214499d42",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "67fe0d1f3a2e18d214499d43",
          "user": {
            "_id": "61457b8deff2c9fdb4de4988",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg",
            "isPro": false,
            "fullname": "Xipeng Qiu",
            "user": "xpqiu",
            "type": "user"
          },
          "name": "Xipeng Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:35:23.363Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-12T08:37:30.000Z",
      "submittedOnDailyAt": "2025-04-15T06:45:12.936Z",
      "title": "VisuoThink: Búsqueda de listas de modelos y recursos para mejorar las capacidades cognitivas de LVLM mediante búsqueda de árboles de multimodalidad.",
      "submittedOnDailyBy": {
        "_id": "627b73728b6ecd7ece822825",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
        "isPro": false,
        "fullname": "Yikun Wang",
        "user": "LibraTree",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los modelos de lenguaje de pantalla ha demostrado capacidades excepcionales. Sin embargo, fallan significativamente en tareas complejas de razonamiento cognitivo que requieren la resolución de problemas activos y paso a paso, como es el caso de las personas. Los métodos existentes investigan solo la memoria lenta basada en texto o dispositivos básicos de asistencia visual, pero no logran comprender las estructuras complejas y cruzadas de la cognición visual-lingüística humana. Para superar estas limitaciones y inspirados en la estructura lenta de la memoria cognitiva humana, se presenta un nuevo marco de trabajo llamado VisuoThink. VisuoThink integra finalmente el dominio visual y el dominio lingüístico, permitiendo el desarrollo de una cognición visual-textual avanzada. VisuoThink mejora significativamente las funciones cognitivas mediante la escalabilidad en el procesamiento, y, a menos de la finalización de la configuración, alcanza los mejores resultados en tareas de geometría y razonamiento espacial.",
      "upvotes": 1,
      "discussionId": "67fe0d203a2e18d214499d9f",
      "githubRepo": "https://github.com/ekonwang/VisuoThink"
    },
    "publishedAt": "2025-04-12T04:37:30.000Z",
    "title": "VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search",
    "summary": "Recent advancements in Large Vision-Language Models have showcased remarkable\ncapabilities. However, they often falter when confronted with complex reasoning\ntasks that humans typically address through visual aids and deliberate,\nstep-by-step thinking. While existing methods have explored text-based slow\nthinking or rudimentary visual assistance, they fall short of capturing the\nintricate, interleaved nature of human visual-verbal reasoning processes. To\novercome these limitations and inspired by the mechanisms of slow thinking in\nhuman cognition, we introduce VisuoThink, a novel framework that seamlessly\nintegrates visuospatial and linguistic domains. VisuoThink facilitates\nmultimodal slow thinking by enabling progressive visual-textual reasoning and\nincorporates test-time scaling through look-ahead tree search. Extensive\nexperiments demonstrate that VisuoThink significantly enhances reasoning\ncapabilities via inference-time scaling, even without fine-tuning, achieving\nstate-of-the-art performance in tasks involving geometry and spatial reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09130.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "627b73728b6ecd7ece822825",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
      "fullname": "Yikun Wang",
      "name": "LibraTree",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10449",
      "authors": [
        {
          "_id": "67fe15543aa5a5684ca7229e",
          "user": {
            "_id": "63dc68bea99b2c8a7c20f1d0",
            "avatarUrl": "/avatars/cf8ddc91415ef1f895803f4390ff1f6f.svg",
            "isPro": true,
            "fullname": "Junxiong Wang",
            "user": "JunxiongWang",
            "type": "user"
          },
          "name": "Junxiong Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:35:36.587Z",
          "hidden": false
        },
        {
          "_id": "67fe15543aa5a5684ca7229f",
          "user": {
            "_id": "62e221dfcb1f164f2cb8a66b",
            "avatarUrl": "/avatars/06f05622e232304d3f0b8c291f3263be.svg",
            "isPro": true,
            "fullname": "Wen-Ding Li",
            "user": "xu3kev",
            "type": "user"
          },
          "name": "Wen-Ding Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:35:44.671Z",
          "hidden": false
        },
        {
          "_id": "67fe15543aa5a5684ca722a0",
          "name": "Daniele Paliotta",
          "hidden": false
        },
        {
          "_id": "67fe15543aa5a5684ca722a1",
          "name": "Daniel Ritter",
          "hidden": false
        },
        {
          "_id": "67fe15543aa5a5684ca722a2",
          "user": {
            "_id": "67745f42633d42196543820f",
            "avatarUrl": "/avatars/6faced0d6486b04262a8d7bc3990262b.svg",
            "isPro": false,
            "fullname": "Alexander Rush",
            "user": "voidptr74",
            "type": "user"
          },
          "name": "Alexander M. Rush",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:36:08.329Z",
          "hidden": false
        },
        {
          "_id": "67fe15543aa5a5684ca722a3",
          "user": {
            "_id": "64b8a6b5cf14c2fabe98159b",
            "avatarUrl": "/avatars/dbc009451865435bf290791beadc4723.svg",
            "isPro": false,
            "fullname": "Tri Dao",
            "user": "tridao",
            "type": "user"
          },
          "name": "Tri Dao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:36:25.942Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:38:25.000Z",
      "submittedOnDailyAt": "2025-04-15T06:44:40.178Z",
      "title": "M1: Se utiliza Mamba Reasoning Models para mejorar la escalabilidad de los tiempos de computación.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "El motivo efectivo es muy importante para resolver problemas matemáticos complejos. Los modelos de lenguaje de gran escala (LLMs) recientes utilizan la escalabilidad de la inferencia de una cadena de pensamiento larga para mejorar el rendimiento. Sin embargo, los modelos basados en Transformer están limitados fundamentalmente por la complejidad de cálculo secundaria y la necesidad de memoria lineal, lo que impide extender la longitud del contexto. En este artículo, presentamos M1, un nuevo modelo de razonamiento híbrido lineal RNN basado en la arquitectura Mamba, para facilitar la inferencia eficiente en memoria. Nuestro enfoque utiliza el proceso de desgaste de modelos de razonamiento existentes y se ha fortalecido a través de entrenamiento por refuerzo. Los resultados de experimentos en AIME y MATH muestran que M1 demostró un excelente rendimiento más allá de los modelos lineales RNN y comparado con el modelo de razonamiento desgastado Deepseek R1 de la misma escala, presentó un rendimiento igual. Además, comparado con el generador de alta calidad vLLM, nuestro generador demostró un aumento de velocidad más de 3 veces más que el Transformer de la misma tamaño. Este aumento de velocidad permite al modelo de razonamiento desgastado de DeepSeek R1 en tiempo de generación fija, alcanzar una mayor precisión que el modelo de razonamiento desgastado de Transformer. En resumen, cuando se utiliza la auto-consistencia o una razón larga de una cadena de pensamiento para la generación en tiempo de prueba, presentamos un modelo de razonamiento híbrido Mamba y una aproximación más efectiva.",
      "upvotes": 0,
      "discussionId": "67fe15553aa5a5684ca722d5"
    },
    "publishedAt": "2025-04-14T13:38:25.000Z",
    "title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
    "summary": "Effective reasoning is crucial to solving complex mathematical problems.\nRecent large language models (LLMs) have boosted performance by scaling\ntest-time computation through long chain-of-thought reasoning. However,\ntransformer-based models are inherently limited in extending context length due\nto their quadratic computational complexity and linear memory requirements. In\nthis paper, we introduce a novel hybrid linear RNN reasoning model, M1, built\non the Mamba architecture, which allows memory-efficient inference. Our\napproach leverages a distillation process from existing reasoning models and is\nfurther enhanced through RL training. Experimental results on the AIME and MATH\nbenchmarks show that M1 not only outperforms previous linear RNN models but\nalso matches the performance of state-of-the-art Deepseek R1 distilled\nreasoning models at a similar scale. We also compare our generation speed with\na highly performant general purpose inference engine, vLLM, and observe more\nthan a 3x speedup compared to a same size transformer. With throughput speedup,\nwe are able to achieve higher accuracy compared to DeepSeek R1 distilled\ntransformer reasoning models under a fixed generation time budget using\nself-consistency voting. Overall, we introduce a hybrid Mamba reasoning model\nand provide a more effective approach to scaling test-time generation using\nself-consistency or long chain of thought reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10449.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6654
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.09522",
      "authors": [
        {
          "_id": "67fe132f1d1bc292f7cdbc0f",
          "name": "Chen Sun",
          "hidden": false
        },
        {
          "_id": "67fe132f1d1bc292f7cdbc10",
          "user": {
            "_id": "6003c87f532970af8c4d3a4a",
            "avatarUrl": "/avatars/a4c5fbe427791d02e3cee208f22f18c4.svg",
            "isPro": false,
            "fullname": "Renat Aksitov",
            "user": "mendor",
            "type": "user"
          },
          "name": "Renat Aksitov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:36:40.229Z",
          "hidden": false
        },
        {
          "_id": "67fe132f1d1bc292f7cdbc11",
          "name": "Andrey Zhmoginov",
          "hidden": false
        },
        {
          "_id": "67fe132f1d1bc292f7cdbc12",
          "name": "Nolan Andrew Miller",
          "hidden": false
        },
        {
          "_id": "67fe132f1d1bc292f7cdbc13",
          "user": {
            "_id": "6502fe0bb1792803da806d42",
            "avatarUrl": "/avatars/60280ce59f1f0d67e4210b0453039282.svg",
            "isPro": false,
            "fullname": "Max Vladymyrov",
            "user": "gozzo87",
            "type": "user"
          },
          "name": "Max Vladymyrov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-15T08:37:00.587Z",
          "hidden": false
        },
        {
          "_id": "67fe132f1d1bc292f7cdbc14",
          "name": "Ulrich Rueckert",
          "hidden": false
        },
        {
          "_id": "67fe132f1d1bc292f7cdbc15",
          "name": "Been Kim",
          "hidden": false
        },
        {
          "_id": "67fe132f1d1bc292f7cdbc16",
          "name": "Mark Sandler",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T11:25:04.000Z",
      "submittedOnDailyAt": "2025-04-15T06:35:36.514Z",
      "title": "Nuevos datos se expanden en la conocimiento y la comunicación de los modelos de lenguaje de inteligencia artificial, y se pregunta cómo se puede reducir esta expansión.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje generalista aprenden a través de la acumulación de actualizaciones basadas en gradientes y continúan aprendiendo. Sin embargo, no se entiende bien cómo afecta nueva información a la conocida existente, ni cómo es que esto puede generar generalización de ventaja y errores en la inferencia. Mostramos que durante el proceso de aprendizaje de nueva información, los modelos de lenguaje generalista (LLMs) pueden mostrar un efecto de \"premining\": el aprendizaje de nuevos hechos permite que el modelo aplique conocimientos adecuados en contextos irrelevantes. Para estudiar este fenómeno sistemáticamente, presentamos un conjunto de datos \"Outlandish\", compuesto por 1.320 muestras de texto diversas. Utilizando este conjunto de datos, demostramos que se puede predecir el grado del efecto de \"premining\" tras la aprendizaje de nueva información, midiendo la probabilidad de tokens de palabras clave antes del aprendizaje. Esta relación se mantiene fuertemente en diferentes arquitecturas de modelo (PALM-2, Gemma, Llama), tamaños y etapas de entrenamiento. Finalmente, para ajustar cómo afecta la nueva información al comportamiento existente del modelo, desarrollamos dos nuevos métodos: (1) la estrategia de \"puntos de paso\" para agregar texto y (2) el método de \"ignore-k\" para el premining de actualizaciones. Estos enfoques reducen el 50-95% del efecto de \"premining\" insatisfactorio y mantienen la capacidad del modelo para aprender nueva información. Nuestros hallazgos demuestran cómo los modelos de lenguaje generalista aprenden y proporcionan herramientas efectivas para mejorar la inserción de conocimiento en modelos de lenguaje. Datos adicionales: https://sunchipsster1.github.io/projects/outlandish/",
      "upvotes": 0,
      "discussionId": "67fe13341d1bc292f7cdbd2f"
    },
    "publishedAt": "2025-04-13T07:25:04.000Z",
    "title": "How new data permeates LLM knowledge and how to dilute it",
    "summary": "Large language models learn and continually learn through the accumulation of\ngradient-based updates, but how individual pieces of new information affect\nexisting knowledge, leading to both beneficial generalization and problematic\nhallucination, remains poorly understood. We demonstrate that when learning new\ninformation, LLMs exhibit a \"priming\" effect: learning a new fact can cause the\nmodel to inappropriately apply that knowledge in unrelated contexts. To\nsystematically study this phenomenon, we introduce \"Outlandish,\" a carefully\ncurated dataset of 1320 diverse text samples designed to probe how new\nknowledge permeates through an LLM's existing knowledge base. Using this\ndataset, we show that the degree of priming after learning new information can\nbe predicted by measuring the token probability of key words before learning.\nThis relationship holds robustly across different model architectures (PALM-2,\nGemma, Llama), sizes, and training stages. Finally, we develop two novel\ntechniques to modulate how new knowledge affects existing model behavior: (1) a\n``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update\npruning method. These approaches reduce undesirable priming effects by 50-95\\%\nwhile preserving the model's ability to learn new information. Our findings\nprovide both empirical insights into how LLMs learn and practical tools for\nimproving the specificity of knowledge insertion in language models. Further\nmaterials: https://sunchipsster1.github.io/projects/outlandish/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09522.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6654
    },
    "isAuthorParticipating": false
  }
]