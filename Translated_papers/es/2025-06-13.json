[
  {
    "paper": {
      "id": "2506.09513",
      "authors": [
        {
          "_id": "684b8dbd3b733ba33368701b",
          "user": {
            "_id": "6723079ad1306fe9c76a1d29",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
            "isPro": false,
            "fullname": "Yu Sun",
            "user": "YuSun-AI",
            "type": "user"
          },
          "name": "Yu Sun",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:32:30.652Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701c",
          "name": "Xingyu Qian",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701d",
          "name": "Weiwen Xu",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701e",
          "user": {
            "_id": "64b7cd74ff6d81ae297feded",
            "avatarUrl": "/avatars/880fbc96cc093f5e901ce84f32a1d21d.svg",
            "isPro": false,
            "fullname": "ZHANG HAO",
            "user": "26hzhang",
            "type": "user"
          },
          "name": "Hao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:43.056Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701f",
          "name": "Chenghao Xiao",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687020",
          "name": "Long Li",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687021",
          "user": {
            "_id": "642eecbf9b2484d7d8526781",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642eecbf9b2484d7d8526781/4IvGbd66s49Wx5pZyZGHA.png",
            "isPro": false,
            "fullname": "Yu Rong",
            "user": "Swrooy",
            "type": "user"
          },
          "name": "Yu Rong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:40.908Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687022",
          "name": "Wenbing Huang",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687023",
          "name": "Qifeng Bai",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687024",
          "name": "Tingyang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T08:36:55.000Z",
      "submittedOnDailyAt": "2025-06-13T01:06:46.741Z",
      "title": "ReasonMed: 370K Dataset de Datos de Creación de Agentes para la Investigación sobre el Desarrollo de la Lógica Médica",
      "submittedOnDailyBy": {
        "_id": "6723079ad1306fe9c76a1d29",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
        "isPro": false,
        "fullname": "Yu Sun",
        "user": "YuSun-AI",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grandes basados en lógica (LLMs) muestran excelentes resultados en matemáticas y programación, pero tienen una capacidad insuficiente para responder a consultas médicas de tipo conocimiento-denso. Para abordar este desafío, presentamos la ReasonMed, el mayor conjunto de datos de razones médicas. La ReasonMed consiste en 1.7 millones de pasos de razones iniciales, configurados en 370.000 ejemplos de alta calidad. La ReasonMed identifica y fortalece los pasos de razones con riesgo de error marcados por evaluadores, diseñando un proceso de error-Finn para corregir estos pasos y construyendo un proceso de investigación y mejora efectivo. Utilizando la ReasonMed, investigamos sistemáticamente el mejor proceso de entrenamiento de modelos de razones médicas y descubrimos un proceso de fine-tuning óptimo que combina razones detalladas de tipo Chain-of-Thought (CoT) y resumenes concisos. Con este proceso, entrenamos la ReasonMed-7B y establecemos un nuevo estándar de prueba para modelos de 10B, superando el mejor modelo del pasado semana en un 4.17% y la LLaMA3.1-70B en la PubMedQA en un 4.60%.",
      "upvotes": 46,
      "discussionId": "684b8dbe3b733ba333687025",
      "githubRepo": "https://github.com/YuSun-Work/ReasonMed",
      "ai_summary": "ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.",
      "ai_keywords": [
        "reasoning-based large language models",
        "LLMs",
        "medical question answering",
        "ReasonMed",
        "multi-agent verification",
        "Error Refiner",
        "Chain-of-Thought",
        "CoT reasoning",
        "ReasonMed-7B",
        "PubMedQA"
      ]
    },
    "publishedAt": "2025-06-11T04:36:55.000Z",
    "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning",
    "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a multi-agent\nverification and refinement process, where we design an Error Refiner\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09513.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6723079ad1306fe9c76a1d29",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
      "fullname": "Yu Sun",
      "name": "YuSun-AI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10954",
      "authors": [
        {
          "_id": "684b7ea83b733ba333686f8a",
          "name": "Lianghong Guo",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8b",
          "name": "Yanlin Wang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8c",
          "name": "Caihua Li",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8d",
          "name": "Pengyu Yang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8e",
          "name": "Jiachi Chen",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8f",
          "user": {
            "_id": "6355473d525beaee688b7ba1",
            "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
            "isPro": false,
            "fullname": "Wei Tao",
            "user": "itaowe",
            "type": "user"
          },
          "name": "Wei Tao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:58.091Z",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f90",
          "name": "Yingtian Zou",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f91",
          "name": "Duyu Tang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f92",
          "name": "Zibin Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:54:17.000Z",
      "submittedOnDailyAt": "2025-06-13T00:07:20.052Z",
      "title": "SWE-Factory: Fábrica Automática de Datos de Solución de Problemas y Benchmark de Evaluación",
      "submittedOnDailyBy": {
        "_id": "6355473d525beaee688b7ba1",
        "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
        "isPro": false,
        "fullname": "Wei Tao",
        "user": "itaowe",
        "type": "user"
      },
      "summary": "La construcción de grandes conjuntos de datos para tareas de solución de problemas en GitHub es crucial para el entrenamiento y evaluación de las capacidades de desarrollo de software de modelos de lenguaje grandes (LLMs). Sin embargo, los procedimientos tradicionales de generación de estos benchmarks son especialmente difíciles en etapas como la configuración del entorno de evaluación, la evaluación de resultados de prueba y la verificación de instancias de tareas, lo que requiere mucho esfuerzo. En este artículo, se propone SWE-Factory, una pipeline de automatización para abordar estas desafíos. Este pipeline combina tres componentes de automatización esenciales diseñados para resolver estos problemas. Primero, SWE-Builder, un sistema de agentes multi-agente, automatiza el entorno de evaluación y utiliza memorias de entorno especializadas para mejorar la eficiencia con la colaboración de cuatro agentes especializados. Luego, se introduce un método de evaluación basado en código extendido y estándarizado, evitando la necesidad de escribir manualmente los parámetros de configuración. Finalmente, se automatiza el proceso de verificación de instancias de tareas utilizando señales de código extendido de alta confianza. Los resultados de experimentos con 671 problemas en cuatro lenguajes de programación demuestran que nuestro pipeline es efectivo en la construcción de instancias válidas de tareas. Por ejemplo, utilizando GPT-4.1-mini, se construyeron 269 instancias válidas en un costo de 0.045 instancias por unidad, y utilizando Gemini-2.5-flash, se alcanzó un rendimiento similar con un costo de 0.024 instancias por unidad, lo más bajo. Además, la evaluación basada en código extendido alcanzó una precisión del 100% en comparación con la verificación manual, y la automatización del proceso de verificación de dos pasos (fail2pass) alcanzó una precisión del 0.92 y una reproducibilidad del 1.00. Esperamos que este pipeline de automatización acelere la recopilación de grandes conjuntos de datos de solución de problemas en GitHub y les ayude en su entrenamiento y evaluación. Nuestro código y conjunto de datos están disponibles en https://github.com/DeepSoftwareAnalytics/swe-factory.",
      "upvotes": 28,
      "discussionId": "684b7ea83b733ba333686f93",
      "githubRepo": "https://github.com/DeepSoftwareAnalytics/swe-factory",
      "ai_summary": "A pipeline named SWE-Factory automates the creation and validation of GitHub issue resolution datasets for training and evaluating Large Language Models, using SWE-Builder for environment setup, exit-code-based grading, and automated fail2pass validation.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "SWE-Factory",
        "SWE-Builder",
        "multi-agent system",
        "environment memory pool",
        "exit-code-based grading",
        "automated fail2pass validation",
        "GPT-4.1-mini",
        "Gemini-2.5-flash",
        "precision",
        "recall"
      ]
    },
    "publishedAt": "2025-06-12T13:54:17.000Z",
    "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks",
    "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of 0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10954.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6355473d525beaee688b7ba1",
      "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
      "fullname": "Wei Tao",
      "name": "itaowe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09993",
      "authors": [
        {
          "_id": "684ae204dbd21a9cc27b0fba",
          "user": {
            "_id": "66012e9c9e1cf5eb41ee0c4c",
            "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
            "isPro": false,
            "fullname": "Jaewon Min",
            "user": "Min-Jaewon",
            "type": "user"
          },
          "name": "Jaewon Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:25.024Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbb",
          "user": {
            "_id": "65ec3449a69aaabb431db0da",
            "avatarUrl": "/avatars/d7b507be0175a61a8fc21176eea45001.svg",
            "isPro": false,
            "fullname": "Jin Hyeon Kim",
            "user": "jinlovespho",
            "type": "user"
          },
          "name": "Jin Hyeon Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:22.776Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbc",
          "user": {
            "_id": "6752b6315281c3cae4b0783f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xmcyVEl2xBhk3G5_7dmpz.png",
            "isPro": false,
            "fullname": "Paul Hyunbin Cho",
            "user": "paulcho98",
            "type": "user"
          },
          "name": "Paul Hyunbin Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:20.327Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbd",
          "name": "Jaeeun Lee",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbe",
          "name": "Jihye Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbf",
          "name": "Minkyu Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc0",
          "name": "Sangpil Kim",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc1",
          "name": "Hyunhee Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc2",
          "name": "Seungryong Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:59:46.000Z",
      "submittedOnDailyAt": "2025-06-13T00:32:01.285Z",
      "title": "Listado de Imágenes Interesantes Utilizando el Modelo de Difusión",
      "submittedOnDailyBy": {
        "_id": "66012e9c9e1cf5eb41ee0c4c",
        "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
        "isPro": false,
        "fullname": "Jaewon Min",
        "user": "Min-Jaewon",
        "type": "user"
      },
      "summary": "El objetivo de la refinación de imágenes es recuperar imágenes deterioradas. Sin embargo, los métodos de refinación basados en difusión actuales han tenido éxito significativo en la refinación de imágenes naturales, pero han encontrado dificultades para reconstruir de manera precisa las áreas de texto en imágenes deterioradas. Estos métodos generan patrones similares a los de texto, lo cual se conoce como \"cuesque de imágenes de texto\" y es considerado incorrecto. En este artículo, se presenta la Imagen de Texto Interesante (TAIR) para la refinación de imágenes. Esta es una nueva tarea de refinación que recupera tanto el contenido visual como la precisión del texto. Para resolver esto, se propone SA-Text, que es un marco de referencia de alta calidad con 100K imágenes de escena, donde se han anotado de manera densa y compleja diferentes e instancias de texto. Además, se propone TeReDiff, un marco de difusión multitarea que integra las características internas del modelo de difusión en un módulo de filtrado de texto, lo que permite que ambos componentes obtengan beneficios durante el entrenamiento conjunto. De esta manera, se extraen representaciones ricas de texto que se utilizan como prompts en las etapas posteriores de desdenoise. Las experimentaciones extendidas muestran que nuestro enfoque supera los métodos de refinación más avanzados de manera consistente y demostró un significativo impacto en la precisión de la reconocción de texto. Para más información, consulte el sitio web del proyecto: https://cvlab-kaist.github.io/TAIR/",
      "upvotes": 28,
      "discussionId": "684ae204dbd21a9cc27b0fc5",
      "projectPage": "https://cvlab-kaist.github.io/TAIR/",
      "githubRepo": "https://github.com/cvlab-kaist/TAIR",
      "ai_summary": "The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.",
      "ai_keywords": [
        "diffusion-based restoration",
        "text-image hallucination",
        "Text-Aware Image Restoration (TAIR)",
        "SA-Text",
        "multi-task diffusion framework",
        "TeReDiff",
        "text-spotting module",
        "text recognition accuracy"
      ]
    },
    "publishedAt": "2025-06-11T13:59:46.000Z",
    "title": "Text-Aware Image Restoration with Diffusion Models",
    "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09993.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66012e9c9e1cf5eb41ee0c4c",
      "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
      "fullname": "Jaewon Min",
      "name": "Min-Jaewon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10857",
      "authors": [
        {
          "_id": "684b817e3b733ba333686f95",
          "user": {
            "_id": "64b89a14cf14c2fabe96664c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
            "isPro": false,
            "fullname": "Jiashuo Yu",
            "user": "awojustin",
            "type": "user"
          },
          "name": "Jiashuo Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:55.618Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f96",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f97",
          "name": "Meng Chu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f98",
          "name": "Zhifei Ren",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f99",
          "name": "Zizheng Huang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9a",
          "user": {
            "_id": "64c9beb2904317f42de06dd8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c9beb2904317f42de06dd8/he3rxfyzfwEd1vLuK6_o2.jpeg",
            "isPro": false,
            "fullname": "Pei Chu",
            "user": "chupei",
            "type": "user"
          },
          "name": "Pei Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:51.884Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9b",
          "name": "Ruijie Zhang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9c",
          "name": "Yinan He",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9d",
          "name": "Qirui Li",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9e",
          "user": {
            "_id": "64acbbd51aee69ece03c6c0c",
            "avatarUrl": "/avatars/604df1cabc5faeda55022ae4c1997e56.svg",
            "isPro": false,
            "fullname": "Songze Li",
            "user": "LarryLee",
            "type": "user"
          },
          "name": "Songze Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:53.776Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9f",
          "name": "Zhenxiang Li",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa0",
          "name": "Zhongying Tu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa1",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa2",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa3",
          "name": "Yali Wang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa4",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa5",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T16:17:17.000Z",
      "submittedOnDailyAt": "2025-06-13T00:10:47.082Z",
      "title": "VRBench: Marcador de rendimiento para videos largos de varios estadios adecuados",
      "submittedOnDailyBy": {
        "_id": "64b89a14cf14c2fabe96664c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
        "isPro": false,
        "fullname": "Jiashuo Yu",
        "user": "awojustin",
        "type": "user"
      },
      "summary": "VRBench es el primer largo benchmark de neural rendering para evaluar la capacidad de inferencia multi-nivel de grandes modelos. Actualmente, resuelve las deficiencias en la inferencia temporal y la justificación de los procedimientos en las evaluaciones. Incluye 1,010 largos videos, 9,468 pares de preguntas y respuestas multi-nivel con etiquetas de etiquetado humano y 30,292 trazas temporales de justificación. Estos videos fueron filtrados a través de un proceso de filtrado multi-nivel para priorizar la coherencia del plot. Desarrolla un marco de colaboración entre humanos y IA, y genera un curso de justificación único que requiere múltiples etapas temporalmente basadas y incluye 7 tipos de justificaciones (por ejemplo, explicación de eventos, historia oculta). VRBench diseña un proceso de evaluación multi-nivel para evaluar tanto los resultados como los procesos. Propone un métrico de puntuación de guías de LLM multi-nivel para evaluar de manera estructurada la calidad de los cursos de justificación, no solo la respuesta final MCQ. Se validan 12 modelos de LLM y 16 de VLM en VRBench, y se realizan análisis detallados para proporcionar consejos valiosos en la área de inferencia multi-nivel.",
      "upvotes": 23,
      "discussionId": "684b817e3b733ba333686fa6",
      "projectPage": "https://vrbench.github.io/",
      "githubRepo": "https://github.com/OpenGVLab/VRBench",
      "ai_summary": "VRBench is a long narrative video benchmark designed to evaluate models' multi-step reasoning and procedural validity through human-labeled question-answering pairs and a human-AI collaborative framework with a multi-phase evaluation pipeline.",
      "ai_keywords": [
        "VRBench",
        "multi-step reasoning",
        "temporal reasoning",
        "procedural validity",
        "long videos",
        "human-labeled",
        "multi-step question-answering",
        "expert inter-rater reviewing",
        "coherent reasoning chains",
        "event attribution",
        "implicit inference",
        "multi-phase evaluation",
        "progress-level LLM-guided scoring metric",
        "LLMs",
        "VLMs"
      ]
    },
    "publishedAt": "2025-06-12T12:17:17.000Z",
    "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
    "summary": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10857.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b89a14cf14c2fabe96664c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
      "fullname": "Jiashuo Yu",
      "name": "awojustin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10540",
      "authors": [
        {
          "_id": "684bad683b733ba3336870b6",
          "user": {
            "_id": "652fb8bcc9dd2692a25ef2e3",
            "avatarUrl": "/avatars/461e6cc1c3441cde18192b080b0b8576.svg",
            "isPro": false,
            "fullname": "Haoyuan Shi",
            "user": "MrSunshy",
            "type": "user"
          },
          "name": "Haoyuan Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:52.713Z",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b7",
          "user": {
            "_id": "62fdb01bc1588e1d4c6c1a7c",
            "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
            "isPro": false,
            "fullname": "Yunxin Li",
            "user": "YunxinLi",
            "type": "user"
          },
          "name": "Yunxin Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T04:47:39.539Z",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b8",
          "name": "Xinyu Chen",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b9",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870ba",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870bb",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T10:06:21.000Z",
      "submittedOnDailyAt": "2025-06-13T03:26:17.710Z",
      "title": "AniMaker: MCTS Droid's automatized multi-agent animation storytelling and clip creation",
      "submittedOnDailyBy": {
        "_id": "62fdb01bc1588e1d4c6c1a7c",
        "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
        "isPro": false,
        "fullname": "Yunxin Li",
        "user": "YunxinLi",
        "type": "user"
      },
      "summary": "Aunque el desarrollo rápido de modelos de TV no ha dejado de surpreender, combinar muchos escenarios y personajes para crear historias continuas es un desafío. Los métodos actuales transforman los prefabricados keyframes en clips de longitud fija, generando problemas de continuidad y paginación en los neulats continuos. Además, la incertidumbre interna de los modelos de TV puede significativamente afectar la coherencia lógica y la continuidad visual de la animación completa. Para superar estos obstáculos, presentamos AniMaker. AniMaker es un marco de trabajo multi-agente que permite la eficiente generación de múltiples clips y la selección de clips relacionados con la historia. Este marco de trabajo está compuesto por un agente director para la generación de guiones, un agente fotográfico para la creación de clips, un agente de evaluación y un agente de postproducción para edición y overs de voz. El enfoque clave de AniMaker son los dos componentes tecnológicos: MCTS-Gen, una estrategia de exploración de espacios candidatos basada en Monte Carlo Tree Search (MCTS) para generar clips de alta calidad, y AniEval, un marco de trabajo especializado en la evaluación de animaciones continuas, que considera el contexto de los clips previos y siguientes para evaluar aspectos como la coherencia narrativa, la completitud de las acciones y las características únicas de la animación. Los experimentos, realizados con VBench y el marco de trabajo AniEval propuesto, muestran un mejoramiento significativo en la eficiencia de la generación de múltiples clips y demostraron que la producción de animación de historia generada por AI se acerca a los estándares de producción.",
      "upvotes": 23,
      "discussionId": "684bad683b733ba3336870bc",
      "ai_summary": "AniMaker, a multi-agent framework using MCTS-Gen and AniEval, generates coherent storytelling videos from text input, outperforming existing models with better quality and efficiency.",
      "ai_keywords": [
        "multi-agent framework",
        "Director Agent",
        "Photography Agent",
        "Reviewer Agent",
        "Post-Production Agent",
        "Monte Carlo Tree Search (MCTS)",
        "AniEval",
        "VBench",
        "action completion",
        "story-level consistency",
        "animation-specific features"
      ]
    },
    "publishedAt": "2025-06-12T06:06:21.000Z",
    "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation",
    "summary": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity. To overcome these\nobstacles, we introduce AniMaker, a multi-agent framework enabling efficient\nmulti-candidate clip generation and storytelling-aware clip selection, thus\ncreating globally consistent and story-coherent animation solely from text\ninput. The framework is structured around specialized agents, including the\nDirector Agent for storyboard generation, the Photography Agent for video clip\ngeneration, the Reviewer Agent for evaluation, and the Post-Production Agent\nfor editing and voiceover. Central to AniMaker's approach are two key technical\ncomponents: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search\n(MCTS)-inspired strategy that intelligently navigates the candidate space to\ngenerate high-potential clips while optimizing resource usage; and AniEval in\nReviewer Agent, the first framework specifically designed for multi-shot\nanimation evaluation, which assesses critical aspects such as story-level\nconsistency, action completion, and animation-specific features by considering\neach clip in the context of its preceding and succeeding clips. Experiments\ndemonstrate that AniMaker achieves superior quality as measured by popular\nmetrics including VBench and our proposed AniEval framework, while\nsignificantly improving the efficiency of multi-candidate generation, pushing\nAI-generated storytelling animation closer to production standards.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10540.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62fdb01bc1588e1d4c6c1a7c",
      "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
      "fullname": "Yunxin Li",
      "name": "YunxinLi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10952",
      "authors": [
        {
          "_id": "684b96403b733ba33368703a",
          "user": {
            "_id": "65e808ed7c10574cc3f8e363",
            "avatarUrl": "/avatars/ed10759d354e271bfc15afd946b66b4a.svg",
            "isPro": false,
            "fullname": "zhangmozhi",
            "user": "mzzhang",
            "type": "user"
          },
          "name": "Mozhi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:33.791Z",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703b",
          "user": {
            "_id": "6718fc605e14ff6b94a7109f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
            "isPro": false,
            "fullname": "Howe Tissue",
            "user": "Howe77",
            "type": "user"
          },
          "name": "Howe Tissue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:31.965Z",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703c",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703d",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:53:51.000Z",
      "submittedOnDailyAt": "2025-06-13T01:43:47.223Z",
      "title": "Vectores de dominio 2: Vectorización de conjuntos de datos que buscan una mezcla óptima de datos sin entrenamiento",
      "submittedOnDailyBy": {
        "_id": "6718fc605e14ff6b94a7109f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
        "isPro": false,
        "fullname": "Howe Tissue",
        "user": "Howe77",
        "type": "user"
      },
      "summary": "Domain2Vec es un nuevo enfoque que utiliza la combinación lineal de meta-dominios (meta-domains), un concepto nuevo, para decomposir conjuntos de datos. Este enfoque está diseñado para explorar las características ocultas de los conjuntos de datos. Domain2Vec utiliza un clasificador de clases que decompone un conjunto de datos dado en vectores de dominios correspondientes a un arreglo de meta-dominios, mientras se mantiene el arreglo de meta-dominios. Estos vectores de dominios pueden utilizarse eficazmente para identificar la mejor mezcla de datos y mejorar el rendimiento de modelos pre-entrenados de modelos de lenguaje (LM), sin necesidad de entrenamiento adicional, basándose en la asumida distribución de datos (DA^2). Esta asumida distribución muestra que una mayor concordancia en la distribución de datos entre el conjunto de entrenamiento y el conjunto de validación conduce a una pérdida de validación más baja. Además, Domain2Vec puede modelar de manera indiscriminada la relación entre vectores de dominios y el rendimiento de LM, mejorando significativamente la eficiencia y la escalabilidad de los métodos anteriores. Los experimentos ampliados muestran que Domain2Vec puede encontrar mezclas de datos que mejoran el rendimiento de tareas de bajo nivel con un mínimo de sobrecarga computacional. En particular, con una mezcla de Pile-CC, el porcentaje de cálculo necesario para entrenar es reducido a un 51.5%, logrando la misma pérdida de validación y mejorando en promedio el rendimiento de tareas de bajo nivel en un 2.83%.",
      "upvotes": 14,
      "discussionId": "684b96413b733ba33368703e",
      "ai_summary": "Domain2Vec decomposes datasets into meta-domains to optimize language model pretraining and downstream performance with reduced computational cost.",
      "ai_keywords": [
        "Domain2Vec",
        "meta-domains",
        "domain vector",
        "distribution alignment assumption",
        "DA²",
        "language model",
        "pretraining",
        "downstream task performance",
        "Pile-CC",
        "The Pile dataset"
      ]
    },
    "publishedAt": "2025-06-12T13:53:51.000Z",
    "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training",
    "summary": "We introduce~Domain2Vec, a novel approach that decomposes any\ndataset into a linear combination of several meta-domains, a new concept\ndesigned to capture the key underlying features of datasets.\nDomain2Vec maintains a vocabulary of meta-domains and uses a\nclassifier to decompose any given dataset into a domain vector that corresponds\nto a distribution over this vocabulary. These domain vectors enable the\nidentification of the optimal data mixture for language model (LM) pretraining\nin a training-free manner under the \\textbf{Distribution\nAlignment Assumption} (DA^{2}), which suggests that when\nthe data distributions of the training set and the validation set are better\naligned, a lower validation loss is achieved. Moreover, Domain2vec can\nbe seamlessly integrated into previous works to model the relationship between\ndomain vectors and LM performance, greatly enhancing the efficiency and\nscalability of previous methods. Extensive experiments demonstrate that\nDomain2Vec helps find the data mixture that enhances downstream task\nperformance with minimal computational overhead. Specifically,\nDomain2Vec achieves the same validation loss on Pile-CC using only\n51.5% of the computation required when training on the original mixture of\nThe Pile dataset. Under equivalent compute budget, Domain2Vec improves\ndownstream performance by an average of 2.83%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6718fc605e14ff6b94a7109f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
      "fullname": "Howe Tissue",
      "name": "Howe77",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10357",
      "authors": [
        {
          "_id": "684b86bf3b733ba333686fbe",
          "name": "Zaijing Li",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fbf",
          "name": "Yuquan Xie",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc0",
          "name": "Rui Shao",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc1",
          "name": "Gongwei Chen",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc2",
          "name": "Weili Guan",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc3",
          "name": "Dongmei Jiang",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc4",
          "name": "Liqiang Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T05:29:40.000Z",
      "submittedOnDailyAt": "2025-06-13T00:37:48.793Z",
      "title": "Optimus-3: Empleado un explorador de tareas escalables para un agente microsoft asembler multi-tipo manual generalmente",
      "submittedOnDailyBy": {
        "_id": "66b45fe75d0ac130d7d82764",
        "avatarUrl": "/avatars/09253f41f82c533b36199f82620cd075.svg",
        "isPro": false,
        "fullname": "Zaijing Li",
        "user": "dawn0815",
        "type": "user"
      },
      "summary": "Recientemente, los modelos de lenguaje multimodal (MLLM) basados en agentes han logrado avances sorprendentes en diversas áreas. Sin embargo, la construcción de agentes generales que posean funciones comunes como reconocimiento visual, planificación, acción, fundamentación y reflexión en entornos abiertos como juegos de estado o Minecraft es un problema complejo: se tiene una escasez de datos específicos, hay interferencias entre tareas diferentes y existe una diversidad visual en los entornos abiertos. En este artículo, se proponen tres contribuciones principales para resolver estos problemas. 1) Se propone una pipeline de generación de datos para proporcionar datos de alta calidad expandibles para el desarrollo de los agentes. 2) Para mitigar las interferencias entre tareas, se introduce una arquitectura Mixture-of-Experts (MoE) utilizando ruteo a nivel de tarea. 3) Se desarrolla una aproximación de Reinforcement Learning Aumentada con Reasoning Multimodal para elevar la capacidad de inferencia visual del agente en Minecraft. Basándose en estas innovaciones, se presenta Optimus-3, un agente general para Minecraft. A través de resultados experimentales extendidos, se muestra que Optimus-3 supera a los agentes generales de MLLM y a los mejores agentes actuales en diferentes tareas en el entorno de Minecraft. Página del proyecto: https://cybertronagent.github.io/Optimus-3.github.io/",
      "upvotes": 12,
      "discussionId": "684b86bf3b733ba333686fc5",
      "projectPage": "https://cybertronagent.github.io/Optimus-3.github.io/",
      "githubRepo": "https://github.com/JiuTian-VL/Optimus-3",
      "ai_summary": "Optimus-3, a multimodal large language model agent, uses knowledge-enhanced data generation, a Mixture-of-Experts architecture, and multimodal reasoning-augmented reinforcement learning to achieve superior performance across various tasks in Minecraft.",
      "ai_keywords": [
        "multimodal large language models",
        "knowledge-enhanced data generation",
        "Mixture-of-Experts",
        "task-level routing",
        "multimodal reasoning-augmented reinforcement learning"
      ]
    },
    "publishedAt": "2025-06-12T01:29:40.000Z",
    "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts",
    "summary": "Recently, agents based on multimodal large language models (MLLMs) have\nachieved remarkable progress across various domains. However, building a\ngeneralist agent with capabilities such as perception, planning, action,\ngrounding, and reflection in open-world environments like Minecraft remains\nchallenges: insufficient domain-specific data, interference among heterogeneous\ntasks, and visual diversity in open-world settings. In this paper, we address\nthese challenges through three key contributions. 1) We propose a\nknowledge-enhanced data generation pipeline to provide scalable and\nhigh-quality training data for agent development. 2) To mitigate interference\namong heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture\nwith task-level routing. 3) We develop a Multimodal Reasoning-Augmented\nReinforcement Learning approach to enhance the agent's reasoning ability for\nvisual diversity in Minecraft. Built upon these innovations, we present\nOptimus-3, a general-purpose agent for Minecraft. Extensive experimental\nresults demonstrate that Optimus-3 surpasses both generalist multimodal large\nlanguage models and existing state-of-the-art agents across a wide range of\ntasks in the Minecraft environment. Project page:\nhttps://cybertronagent.github.io/Optimus-3.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10357.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b45fe75d0ac130d7d82764",
      "avatarUrl": "/avatars/09253f41f82c533b36199f82620cd075.svg",
      "fullname": "Zaijing Li",
      "name": "dawn0815",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10974",
      "authors": [
        {
          "_id": "684b8e193b733ba333687028",
          "user": {
            "_id": "6241749cf80bd930bd99f3dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669210243382-6241749cf80bd930bd99f3dd.jpeg",
            "isPro": false,
            "fullname": "Ou Yixin",
            "user": "OE-Heart",
            "type": "user"
          },
          "name": "Yixin Ou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:36.148Z",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba333687029",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702a",
          "name": "Jingsheng Zheng",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702b",
          "name": "Lanning Wei",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702c",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702d",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702e",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702f",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba333687030",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:37.984Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:59:32.000Z",
      "submittedOnDailyAt": "2025-06-13T03:44:21.173Z",
      "title": "AutoMind: Agente adaptativo de conocimiento para el ciencia de datos automática",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLM) demostrados potencial para resolver problemas de ciencia de datos reales. Los agentes de ciencia de datos dirigidos por LLM prometen automatizar todas las pilas de aprendizaje automático, pero su efectividad es limitada. Los actuales marcos de trabajo dependen de flujos de trabajo estrictos y estrategias de codificación inmutables, demostrando excelente rendimiento en problemas relativamente sencillos pero no logrando captar el conocimiento humano empírico para tareas innovadoras complejas. En este estudio, se presenta \"AutoMind\", un marco de trabajo de agente conocimiento automatizado que supera estas limitaciones a través de tres innovaciones clave: (1) basado en el conocimiento de expertos, (2) algoritmos de búsqueda de árbol conocimiento del agente, y (3) estrategias de codificación automáticas. Evaluados en dos marcos de prueba de ciencia de datos automática, AutoMind demostró un desempeño superior en comparación con los límites avanzados. Los análisis adicionales confirmaron la excelencia en validez, eficiencia y calidad de soluciones cualitativas, estableciendo claramente que AutoMind desempeña un papel adecuado y potente como etapa para la ciencia de datos completamente automática.",
      "upvotes": 10,
      "discussionId": "684b8e193b733ba333687031",
      "ai_summary": "AutoMind, a flexible and knowledgeable LLM-agent framework, improves automated data science through expert knowledge integration, strategic solution exploration, and adaptive coding, outperforming existing systems.",
      "ai_keywords": [
        "LLM",
        "data science agents",
        "machine learning pipeline",
        "expert knowledge base",
        "agentic knowledgeable tree search",
        "self-adaptive coding strategy"
      ]
    },
    "publishedAt": "2025-06-12T13:59:32.000Z",
    "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
    "summary": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10974.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10960",
      "authors": [
        {
          "_id": "684bb33a3b733ba3336870c5",
          "name": "Kangwei Liu",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c6",
          "name": "Siyuan Cheng",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c7",
          "name": "Bozhong Tian",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c8",
          "name": "Xiaozhuan Liang",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c9",
          "name": "Yuyang Yin",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870ca",
          "name": "Meng Han",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cb",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:50.549Z",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cc",
          "name": "Bryan Hooi",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cd",
          "user": {
            "_id": "635113fdcba4ff2e81cb236e",
            "avatarUrl": "/avatars/f80df906b722b4901debce9baa867073.svg",
            "isPro": false,
            "fullname": "chen",
            "user": "Jasonchen123",
            "type": "user"
          },
          "name": "Xi Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T05:12:27.526Z",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870ce",
          "name": "Shumin Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:57:05.000Z",
      "submittedOnDailyAt": "2025-06-13T03:43:12.055Z",
      "title": "China Benchmark for Detecting Harmful Content",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) se han automatizado para la detección de contenidos perjudiciales. Ellos ayudan a los modeleros a reconocer violaciones de políticas y a mejorar la eficiencia y precisión general de la revisión de contenido, aunque los recursos actuales de detección de contenidos perjudiciales se centran principalmente en el inglés, y los conjuntos de datos en chino son raros y limitados. Presentamos un benchmark profesionalmente explicado que incluye seis categorías representativas, construido con datos reales. Este proceso de explicación permite que LLMs obtengan conocimientos especializados claros que les ayuden a la detección de contenidos perjudiciales en chino. Además, proponemos una línea de conocimiento aditivo basada en reglas humanas y conocimiento potencial obtenido de LLMs, lo que permitirá a pequeños modelos alcanzar un rendimiento comparable a los más avanzados. Los códigos y datos están disponibles en https://github.com/zjunlp/ChineseHarm-bench.",
      "upvotes": 9,
      "discussionId": "684bb33a3b733ba3336870cf",
      "ai_summary": "A benchmark for Chinese harmful content detection is introduced, along with a knowledge-augmented model that enhances efficiency and accuracy using human-annotated rules and LLMs.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "harmful content detection",
        "knowledge-augmented baseline",
        "annotation process",
        "knowledge rule base",
        "Chinese datasets"
      ]
    },
    "publishedAt": "2025-06-12T13:57:05.000Z",
    "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark",
    "summary": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which covers six representative categories and\nis constructed entirely from real-world data. Our annotation process further\nyields a knowledge rule base that provides explicit expert knowledge to assist\nLLMs in Chinese harmful content detection. In addition, we propose a\nknowledge-augmented baseline that integrates both human-annotated knowledge\nrules and implicit knowledge from large language models, enabling smaller\nmodels to achieve performance comparable to state-of-the-art LLMs. Code and\ndata are available at https://github.com/zjunlp/ChineseHarm-bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10960.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10821",
      "authors": [
        {
          "_id": "684b91c73b733ba333687033",
          "name": "Huaying Yuan",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687034",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687035",
          "name": "Junjie Zhou",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687036",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687037",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T15:39:10.000Z",
      "submittedOnDailyAt": "2025-06-13T01:20:51.837Z",
      "title": "VideoDeepResearch: Uso de un herramienta de fuera de la ruta para entender largos videos",
      "submittedOnDailyBy": {
        "_id": "66d916a7b86f0d569aa19b60",
        "avatarUrl": "/avatars/2537cee66afecc2d999e05b01c78d319.svg",
        "isPro": false,
        "fullname": "huaying Yuan",
        "user": "avery00",
        "type": "user"
      },
      "summary": "La Comprensión de Video a Largo Plazo (LVU) es un problema importante para los modelos de lenguaje multimodal (MLLMs) actuales. Para resolver este problema, se necesita generalmente un contexto de extensión, una fuerte capacidad de reconocimiento visual y un modelo MLLM con conocimientos especializados. En este artículo, desafíamos esta creencia común y presentamos un nuevo marco de agente llamado VideoDeepResearch. Nuestro enfoque combina un modelo de lenguaje grande basado en texto único (LRM) y un conjunto de módulos de herramientas multimodal. Este conjunto incluye un riflector multimodal y un reconocido visual, lo que permite su uso práctico. Para cada tarea de LVU, el sistema construye estrategias de solución de problemas a través de lógica y accede selectivamente al contenido visual necesario para utilizar las herramientas. Se han realizado amplios experimentos en los populares marcos de evaluación LVU, como MLVU (Test), LVBench y LongVideoBench. VideoDeepResearch ha logrado notables mejoras frente a los modelos MLLM actuales, con tasas de mejora del 9.6%, 6.6% y 3.9% en cada uno de los marcos de evaluación. Estos resultados revelan la posibilidad de que el sistema de agentes supere los problemas cruciales de LVU.",
      "upvotes": 9,
      "discussionId": "684b91c73b733ba333687038",
      "ai_summary": "VideoDeepResearch, a text-only reasoning model with modular tools, surpasses existing baselines in long video understanding tasks without extending context windows or enhancing visual perception capabilities.",
      "ai_keywords": [
        "long video understanding",
        "multi-modal large language models",
        "VideoDeepResearch",
        "text-only large reasoning model",
        "multimodal retrievers",
        "visual perceivers",
        "MLVU",
        "Video-MME",
        "LVBench",
        "LongVideoBench",
        "agentic systems"
      ]
    },
    "publishedAt": "2025-06-12T11:39:10.000Z",
    "title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using",
    "summary": "Long video understanding (LVU) presents a significant challenge for current\nmulti-modal large language models (MLLMs) due to the task's inherent complexity\nand context window constraint. It is widely assumed that addressing LVU tasks\nrequires foundation MLLMs with extended context windows, strong visual\nperception capabilities, and proficient domain expertise. In this work, we\nchallenge this common belief by introducing VideoDeepResearch, a novel agentic\nframework for long video understanding. Our approach relies solely on a\ntext-only large reasoning model (LRM) combined with a modular multi-modal\ntoolkit, including multimodal retrievers and visual perceivers, all of which\nare readily available in practice. For each LVU task, the system formulates a\nproblem-solving strategy through reasoning, while selectively accessing and\nutilizing essential video content via tool using. We conduct extensive\nexperiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.\nOur results demonstrate that VideoDeepResearch achieves substantial\nimprovements over existing MLLM baselines, surpassing the previous\nstate-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and\nLongVideoBench, respectively. These findings highlight the promise of agentic\nsystems in overcoming key challenges in LVU problems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d916a7b86f0d569aa19b60",
      "avatarUrl": "/avatars/2537cee66afecc2d999e05b01c78d319.svg",
      "fullname": "huaying Yuan",
      "name": "avery00",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10741",
      "authors": [
        {
          "_id": "684b881f3b733ba333686fd4",
          "user": {
            "_id": "64966691990b342dcc9fccb5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64966691990b342dcc9fccb5/tQSrE3MkBeakk5QYfgHSo.jpeg",
            "isPro": false,
            "fullname": "sixiang chen",
            "user": "Ephemeral182",
            "type": "user"
          },
          "name": "SiXiang Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:47.414Z",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd5",
          "name": "Jianyu Lai",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd6",
          "name": "Jialin Gao",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd7",
          "name": "Tian Ye",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd8",
          "name": "Haoyu Chen",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd9",
          "name": "Hengyu Shi",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fda",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdb",
          "name": "Yunlong Lin",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdc",
          "name": "Song Fei",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdd",
          "name": "Zhaohu Xing",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fde",
          "name": "Yeying Jin",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdf",
          "name": "Junfeng Luo",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fe0",
          "name": "Xiaoming Wei",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fe1",
          "name": "Lei Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T14:28:12.000Z",
      "submittedOnDailyAt": "2025-06-13T04:30:38.214Z",
      "title": "PosterCraft: Revisión sobre la generación de pósteres de arte de alta calidad en un marco de trabajo unificado",
      "submittedOnDailyBy": {
        "_id": "66015e8aa4d296af07de538e",
        "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
        "isPro": false,
        "fullname": "Ye",
        "user": "Owen777",
        "type": "user"
      },
      "summary": "La generación de pósteres no es solo un desafío de diseño gráfico, sino una tarea más compleja: requiere un diseño de imágenes precisas de texto, un ajuste infinito de contenido artístico abstracto, una estética de diseño atractivo y una armonía visual. Para abordar estos desafíos, proponemos PosterCraft, un marco integrado que permite a los modelos explorar y generar diseños visualmente atractivos y coherentes sin necesidad de formar parte de un sistema modular o con layouts predefinidos. PosterCraft optimiza la creación de pósteres de alta calidad mediante un flujo de trabajo estricto: (i) optimización de imágenes de texto con el nuevo conjunto de datos Text-Render-2M; (ii) fine-tuning para áreas específicas con HQ-Poster100K; (iii) optimización del estilo artístico de texto; y (iv) refinamiento de la comunicación visual. Cada etapa se soporta a través de formularios de diseño de datos computacionales adaptados a sus necesidades específicas, permitiendo una fortaleza de entrenamiento fuerte, excepto por cambios en la arquitectura compleja. PosterCraft, evaluado a través de múltiples experimentos, supera significativamente a la línea abierta y se acerca a la calidad de los sistemas comerciales más avanzados en términos de precisión de renderización, coherencia del diseño y atractividad visual. Nuestro código, modelos y conjuntos de datos están disponibles en nuestra página del proyecto: https://ephemeral182.github.io/PosterCraft",
      "upvotes": 9,
      "discussionId": "684b881f3b733ba333686fe2",
      "projectPage": "https://ephemeral182.github.io/PosterCraft/",
      "githubRepo": "https://github.com/Ephemeral182/PosterCraft",
      "ai_summary": "PosterCraft improves aesthetic poster generation through a unified, modular pipeline with enhanced text rendering, region-aware fine-tuning, aesthetic reinforcement learning, and joint vision-language refinement.",
      "ai_keywords": [
        "text-rendering optimization",
        "Text-Render-2M",
        "region-aware supervised fine-tuning",
        "HQ-Poster100K",
        "aesthetic-text-reinforcement learning",
        "best-of-n preference optimization",
        "joint vision-language feedback refinement"
      ]
    },
    "publishedAt": "2025-06-12T10:28:12.000Z",
    "title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a\n  Unified Framework",
    "summary": "Generating aesthetic posters is more challenging than simple design images:\nit requires not only precise text rendering but also the seamless integration\nof abstract artistic content, striking layouts, and overall stylistic harmony.\nTo address this, we propose PosterCraft, a unified framework that abandons\nprior modular pipelines and rigid, predefined layouts, allowing the model to\nfreely explore coherent, visually compelling compositions. PosterCraft employs\na carefully designed, cascaded workflow to optimize the generation of\nhigh-aesthetic posters: (i) large-scale text-rendering optimization on our\nnewly introduced Text-Render-2M dataset; (ii) region-aware supervised\nfine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via\nbest-of-n preference optimization; and (iv) joint vision-language feedback\nrefinement. Each stage is supported by a fully automated data-construction\npipeline tailored to its specific needs, enabling robust training without\ncomplex architectural modifications. Evaluated on multiple experiments,\nPosterCraft significantly outperforms open-source baselines in rendering\naccuracy, layout coherence, and overall visual appeal-approaching the quality\nof SOTA commercial systems. Our code, models, and datasets can be found in the\nProject page: https://ephemeral182.github.io/PosterCraft",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10741.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66015e8aa4d296af07de538e",
      "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
      "fullname": "Ye",
      "name": "Owen777",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10890",
      "authors": [
        {
          "_id": "684b8b533b733ba333686fe4",
          "user": {
            "_id": "62bc1adacaf01b9bec398547",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
            "isPro": false,
            "fullname": "Zhao Zhang",
            "user": "zbrl",
            "type": "user"
          },
          "name": "Zhao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:45.153Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe5",
          "name": "Yutao Cheng",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe6",
          "user": {
            "_id": "6669a0cc9f28880b31d7c4ef",
            "avatarUrl": "/avatars/bd66a6f68a9af2bf7ee40510579e57fe.svg",
            "isPro": false,
            "fullname": "dexiang hong",
            "user": "hxxxl",
            "type": "user"
          },
          "name": "Dexiang Hong",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:22:14.202Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe7",
          "user": {
            "_id": "63fd7279ed9eead590fd02ed",
            "avatarUrl": "/avatars/4cf6f005069412ee87ed07cd81500f1e.svg",
            "isPro": false,
            "fullname": "YangMaoke",
            "user": "YangMaoke",
            "type": "user"
          },
          "name": "Maoke Yang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:22:14.202Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe8",
          "user": {
            "_id": "6436619ead9b9147de287a24",
            "avatarUrl": "/avatars/180c43c79e552dd345636a47db80e3e9.svg",
            "isPro": false,
            "fullname": "ShiLayne",
            "user": "ShiLayne",
            "type": "user"
          },
          "name": "Gonglei Shi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-13T02:23:52.345Z",
          "hidden": true
        },
        {
          "_id": "684b8b533b733ba333686fe9",
          "name": "Lei Ma",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fea",
          "name": "Hui Zhang",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686feb",
          "name": "Jie Shao",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fec",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T16:54:39.000Z",
      "submittedOnDailyAt": "2025-06-13T00:55:02.473Z",
      "title": "CreatiPoster: Herramienta para la edición y control de la generación de diseños gráficos multi-capa.",
      "submittedOnDailyBy": {
        "_id": "62bc1adacaf01b9bec398547",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
        "isPro": false,
        "fullname": "Zhao Zhang",
        "user": "zbrl",
        "type": "user"
      },
      "summary": "El diseño gráfico desempeña un papel importante tanto en los aspectos comerciales como personales, pero la creación de composiciones gráficas de alta calidad, editables y hermosas es especialmente desafiante para principiantes, ya que requiere tiempo y alta técnica. Las herramientas de IA actuales automatizan parte del flujo de trabajo, pero es difícil que los usuarios incluyan precisamente sus recursos y mantengan la posibilidad de edición, al mismo tiempo que logren una belleza visual profesional. En sistemas comerciales, herramientas como Canva Magic Design basadas en grandes bibliotecas de templates son útiles, pero no son eficientes para reutilizar. En este artículo se presenta el framework CreatiPoster. Este framework genera composiciones multi-capa editables a partir de indicaciones naturales o recursos seleccionados. Los modelos protocolo y de escala RGBA generan archivos JSON específicos que detallan con precisión la posición, estructura, contenido y estilo de cada capa de texto o recurso. Además, incluye un simple fondo de fondo. Luego, un modelo de fondo condicional se basa en esta capa de fondo para sintetizar un fondo coherente. CreatiPoster establece un marco de referencia en el mercado de la automatización gráfica, demostrando una aproximación avanzada y patentable, así como una accesibilidad abierta. Publica un corpus de 100,000 diseños multi-capa sin derechos de autor para promover el feedback. CreatiPoster apoya aplicaciones variadas, como edición de fondo, sobrescritura de texto, reescalado responsivo, multilingüismo y animación de póster, y fomenta la democratización de diseño gráfico con la ayuda de la IA. Página del proyecto: https://github.com/graphic-design-ai/creatiposter",
      "upvotes": 7,
      "discussionId": "684b8b533b733ba333686fed",
      "githubRepo": "https://github.com/graphic-design-ai/creatiposter",
      "ai_summary": "CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.",
      "ai_keywords": [
        "RGBA large multimodal model",
        "JSON specification",
        "conditional background model",
        "automated metrics",
        "graphic-design generation",
        "multi-layer designs",
        "AI-assisted graphic design"
      ]
    },
    "publishedAt": "2025-06-12T12:54:39.000Z",
    "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation",
    "summary": "Graphic design plays a crucial role in both commercial and personal contexts,\nyet creating high-quality, editable, and aesthetically pleasing graphic\ncompositions remains a time-consuming and skill-intensive task, especially for\nbeginners. Current AI tools automate parts of the workflow, but struggle to\naccurately incorporate user-supplied assets, maintain editability, and achieve\nprofessional visual appeal. Commercial systems, like Canva Magic Design, rely\non vast template libraries, which are impractical for replicate. In this paper,\nwe introduce CreatiPoster, a framework that generates editable, multi-layer\ncompositions from optional natural-language instructions or assets. A protocol\nmodel, an RGBA large multimodal model, first produces a JSON specification\ndetailing every layer (text or asset) with precise layout, hierarchy, content\nand style, plus a concise background prompt. A conditional background model\nthen synthesizes a coherent background conditioned on this rendered foreground\nlayers. We construct a benchmark with automated metrics for graphic-design\ngeneration and show that CreatiPoster surpasses leading open-source approaches\nand proprietary commercial systems. To catalyze further research, we release a\ncopyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports\ndiverse applications such as canvas editing, text overlay, responsive resizing,\nmultilingual adaptation, and animated posters, advancing the democratization of\nAI-assisted graphic design. Project homepage:\nhttps://github.com/graphic-design-ai/creatiposter",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10890.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bc1adacaf01b9bec398547",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
      "fullname": "Zhao Zhang",
      "name": "zbrl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09967",
      "authors": [
        {
          "_id": "684ae1eedbd21a9cc27b0f10",
          "user": {
            "_id": "67469d6a8407f929491dce06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
            "isPro": true,
            "fullname": "Shangshang Wang",
            "user": "upup-ashton-wang",
            "type": "user"
          },
          "name": "Shangshang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:31.479Z",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f11",
          "name": "Julian Asilis",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f12",
          "name": "Ömer Faruk Akgül",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f13",
          "name": "Enes Burak Bilgin",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f14",
          "name": "Ollie Liu",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f15",
          "user": {
            "_id": "63c8454e46421a2efe82709d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
            "isPro": true,
            "fullname": "Deqing Fu",
            "user": "deqing",
            "type": "user"
          },
          "name": "Deqing Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:29.284Z",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f16",
          "user": {
            "_id": "644bf65522d211df6444a7f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf65522d211df6444a7f4/k_ddZdQDg2fzhwjI1EXyx.jpeg",
            "isPro": false,
            "fullname": "Willie Neiswanger",
            "user": "willieneis",
            "type": "user"
          },
          "name": "Willie Neiswanger",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:27.301Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:44:01.000Z",
      "submittedOnDailyAt": "2025-06-13T02:39:37.215Z",
      "title": "Resa: Se genera una teoría de la transparencia con modelos de MoMo a través de SAEs.",
      "submittedOnDailyBy": {
        "_id": "67469d6a8407f929491dce06",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
        "isPro": true,
        "fullname": "Shangshang Wang",
        "user": "upup-ashton-wang",
        "type": "user"
      },
      "summary": "Investigamos si se puede extraer una fuerte teoría lógica eficientemente del modelo de lenguaje utilizando las potenciales representaciones del modelo. La respuesta a esta pregunta es Resa. Resa es una familia de modelos lógicos de 150M unidades que utilizan un nuevo procedimiento eficiente de ajuste de codificadores autómatos esparsos (SAE-Tuning). Este método se realiza de la siguiente manera: primero, se entrena un SAE para identificar las capacidades lógicas del modelo fuente, y luego se utiliza el SAE entrenado para guiar el proceso de entrenamiento estándar de sobrepajo para extraer las capacidades lógicas al modelo objetivo. Esta técnica se realiza con datos de respuesta a preguntas validadas sin trazas lógicas. En particular, al aplicar esta técnica a un modelo basado en una base específica antes de entrenar el modelo de RL, SAE-Tuning mantiene el rendimiento lógico del modelo de entrenamiento de RL en más del 97%, reduciendo el costo de entrenamiento en más de 2000 veces y disminuyendo el tiempo de entrenamiento a aproximadamente 1 dólar y 20 minutos. Además, este método puede aplicarse a modelos de entrenamiento de RL ligeros en 1 hora con 2 GPUs, logrando un Rendimiento Pass@1 de 43.33% en AIME24 y un 90% en AMC23, con un rendimiento lógico casi igual sin aumentar significativamente los costos. Una notable observación es que las capacidades lógicas extraídas de un SAE son potencialmente modulares y generalizables. La generalización significa que las capacidades extraídas mejoran el rendimiento en mayores corpus repetidos. La modularidad significa que las capacidades extraídas en Qwen o Qwen-Math pueden ser agregadas a un modelo R1-Distill sin necesidad de retrenar, obteniendo un beneficio relativamente grande. Los experimentos extendidos demuestran estas observaciones, y todos los artefactos están completamente disponibles bajo un licencia de código abierto.",
      "upvotes": 6,
      "discussionId": "684ae1eedbd21a9cc27b0f17",
      "projectPage": "https://shangshangwang.notion.site/resa",
      "githubRepo": "https://github.com/shangshang-wang/Resa",
      "ai_summary": "SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.",
      "ai_keywords": [
        "sparse autoencoder tuning",
        "SAE-Tuning",
        "reasoning models",
        "verification",
        "sparse autoencoders",
        "supervised fine-tuning",
        "RL post-training",
        "Pass@1",
        "AIME24",
        "AMC23",
        "generality",
        "modularity",
        "R1-Distill",
        "Qwen",
        "Qwen-Math"
      ]
    },
    "publishedAt": "2025-06-11T13:44:01.000Z",
    "title": "Resa: Transparent Reasoning Models via SAEs",
    "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround 1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09967.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67469d6a8407f929491dce06",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
      "fullname": "Shangshang Wang",
      "name": "upup-ashton-wang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10910",
      "authors": [
        {
          "_id": "684bbe273b733ba3336870ed",
          "name": "Mistral-AI",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870ef",
          "name": "Abhinav Rastogi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f0",
          "name": "Albert Q. Jiang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f1",
          "name": "Andy Lo",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f2",
          "name": "Gabrielle Berrada",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f3",
          "name": "Guillaume Lample",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f4",
          "name": "Jason Rute",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f5",
          "name": "Joep Barmentlo",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f6",
          "name": "Karmesh Yadav",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f7",
          "name": "Kartik Khandelwal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f8",
          "name": "Khyathi Raghavi Chandu",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f9",
          "name": "Léonard Blier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fa",
          "name": "Lucile Saulnier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fb",
          "name": "Matthieu Dinot",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fc",
          "name": "Maxime Darrin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fd",
          "name": "Neha Gupta",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fe",
          "name": "Roman Soletskyi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870ff",
          "name": "Sagar Vaze",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687100",
          "name": "Teven Le Scao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687101",
          "name": "Yihan Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687102",
          "name": "Adam Yang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687103",
          "name": "Alexander H. Liu",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687104",
          "name": "Alexandre Sablayrolles",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687105",
          "name": "Amélie Héliou",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687106",
          "name": "Amélie Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687107",
          "name": "Andy Ehrenberg",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687108",
          "name": "Anmol Agarwal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687109",
          "name": "Antoine Roux",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710a",
          "name": "Arthur Darcet",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710b",
          "name": "Arthur Mensch",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710c",
          "name": "Baptiste Bout",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710d",
          "name": "Baptiste Rozière",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710e",
          "name": "Baudouin De Monicault",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710f",
          "name": "Chris Bamford",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687110",
          "name": "Christian Wallenwein",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687111",
          "name": "Christophe Renaudin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687112",
          "name": "Clémence Lanfranchi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687113",
          "name": "Darius Dabert",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687114",
          "name": "Devon Mizelle",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687115",
          "name": "Diego de las Casas",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687116",
          "name": "Elliot Chane-Sane",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687117",
          "name": "Emilien Fugier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687118",
          "name": "Emma Bou Hanna",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687119",
          "name": "Gauthier Delerce",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711a",
          "name": "Gauthier Guinet",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711b",
          "name": "Georgii Novikov",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711c",
          "name": "Guillaume Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711d",
          "name": "Himanshu Jaju",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711e",
          "name": "Jan Ludziejewski",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711f",
          "name": "Jean-Hadrien Chabran",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687120",
          "name": "Jean-Malo Delignon",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687121",
          "name": "Joachim Studnia",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687122",
          "name": "Jonas Amar",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687123",
          "name": "Josselin Somerville Roberts",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687124",
          "name": "Julien Denize",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687125",
          "name": "Karan Saxena",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687126",
          "name": "Kush Jain",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687127",
          "name": "Lingxiao Zhao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687128",
          "name": "Louis Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687129",
          "name": "Luyu Gao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712a",
          "name": "Lélio Renard Lavaud",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712b",
          "name": "Marie Pellat",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712c",
          "name": "Mathilde Guillaumin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712d",
          "name": "Mathis Felardos",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712e",
          "name": "Maximilian Augustin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712f",
          "name": "Mickaël Seznec",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687130",
          "name": "Nikhil Raghuraman",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687131",
          "name": "Olivier Duchenne",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687132",
          "name": "Patricia Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687133",
          "name": "Patrick von Platen",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687134",
          "name": "Patryk Saffer",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687135",
          "name": "Paul Jacob",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687136",
          "name": "Paul Wambergue",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687137",
          "name": "Paula Kurylowicz",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687138",
          "name": "Pavankumar Reddy Muddireddy",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687139",
          "name": "Philomène Chagniot",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713a",
          "name": "Pierre Stock",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713b",
          "name": "Pravesh Agrawal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713c",
          "name": "Romain Sauvestre",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713d",
          "name": "Rémi Delacourt",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713e",
          "name": "Sanchit Gandhi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713f",
          "name": "Sandeep Subramanian",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687140",
          "name": "Shashwat Dalal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687141",
          "name": "Siddharth Gandhi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687142",
          "name": "Soham Ghosh",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687143",
          "name": "Srijan Mishra",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687144",
          "name": "Sumukh Aithal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687145",
          "name": "Szymon Antoniak",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687146",
          "name": "Thibault Schueller",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687147",
          "name": "Thibaut Lavril",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687148",
          "name": "Thomas Robert",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687149",
          "name": "Thomas Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714a",
          "name": "Timothée Lacroix",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714b",
          "name": "Valeriia Nemychnikova",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714c",
          "name": "Victor Paltz",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714d",
          "name": "Virgile Richard",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714e",
          "name": "Wen-Ding Li",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714f",
          "name": "William Marshall",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687150",
          "name": "Xuanyu Zhang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687151",
          "name": "Yunhao Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:22:37.000Z",
      "submittedOnDailyAt": "2025-06-13T04:29:39.974Z",
      "title": "Majestà",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "Introducing the initial inference model of Mistral and our scalable reinforcement learning (RL) pipeline. Unlike existing implementations and previous models that rely on a chain of RL tools derived from them, we adopt a simple approach that only depends on our models and infrastructure. Specifically, we show a stack for exploring the limitations of simple RL training for LLMs, demonstrate a forced simple approach for model reasoning, and find that RL based solely on document data maintains the initial checkpoint's capabilities. We discover that RL on document data can maintain or enhance understanding, following instructions, function calls, and more. We introduce models trained on the reasoning of Magistral Medium and Mistral Medium 3, and we release Magistral Small (Apache 2.0). Magistral Small is a stronger model that starts coldly from data included in Magistral Medium.",
      "upvotes": 5,
      "discussionId": "684bbe283b733ba333687152",
      "ai_summary": "Magistral, a scalable reinforcement learning pipeline, demonstrates that RL can enhance multimodal understanding and instruction following in large language models without requiring existing RL traces.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "LLMs",
        "multimodal understanding",
        "instruction following",
        "function calling",
        "cold-start data"
      ]
    },
    "publishedAt": "2025-06-12T13:22:37.000Z",
    "title": "Magistral",
    "summary": "We introduce Magistral, Mistral's first reasoning model and our own scalable\nreinforcement learning (RL) pipeline. Instead of relying on existing\nimplementations and RL traces distilled from prior models, we follow a ground\nup approach, relying solely on our own models and infrastructure. Notably, we\ndemonstrate a stack that enabled us to explore the limits of pure RL training\nof LLMs, present a simple method to force the reasoning language of the model,\nand show that RL on text data alone maintains most of the initial checkpoint's\ncapabilities. We find that RL on text maintains or improves multimodal\nunderstanding, instruction following and function calling. We present Magistral\nMedium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we\nopen-source Magistral Small (Apache 2.0) which further includes cold-start data\nfrom Magistral Medium.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10910.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2746
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09344",
      "authors": [
        {
          "_id": "684ae277dbd21a9cc27b118d",
          "name": "Inclusion AI",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b118e",
          "name": "Biao Gong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b118f",
          "name": "Cheng Zou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1190",
          "name": "Chuanyang Zheng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1191",
          "name": "Chunluan Zhou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1192",
          "name": "Canxiang Yan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1193",
          "name": "Chunxiang Jin",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1194",
          "name": "Chunjie Shen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1195",
          "name": "Dandan Zheng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1196",
          "name": "Fudong Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1197",
          "name": "Furong Xu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1198",
          "name": "GuangMing Yao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1199",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119a",
          "name": "Jingdong Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119b",
          "name": "Jianxin Sun",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119c",
          "name": "Jiajia Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119d",
          "name": "Jianjiang Zhu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119e",
          "name": "Jun Peng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119f",
          "name": "Kaixiang Ji",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a0",
          "name": "Kaiyou Song",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a1",
          "name": "Kaimeng Ren",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a2",
          "name": "Libin Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a3",
          "name": "Lixiang Ru",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a4",
          "name": "Lele Xie",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a5",
          "name": "Longhua Tan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a6",
          "name": "Lyuxin Xue",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a7",
          "name": "Lan Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a8",
          "name": "Mochen Bai",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a9",
          "name": "Ning Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11aa",
          "name": "Pei Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ab",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ac",
          "name": "Qinglong Zhang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ad",
          "name": "Qiang Xu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ae",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11af",
          "name": "Ruijie Xiong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b0",
          "name": "Sirui Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b1",
          "name": "Tinghao Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b2",
          "name": "Taisong Li",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b3",
          "name": "Weilong Chai",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b4",
          "name": "Xinyu Xiao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b5",
          "name": "Xiaomei Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b6",
          "name": "Xiaoxue Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b7",
          "name": "Xiao Lu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b8",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b9",
          "name": "Xingning Dong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ba",
          "name": "Xuzheng Yu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bb",
          "name": "Yi Yuan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bc",
          "name": "Yuting Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bd",
          "name": "Yunxiao Sun",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11be",
          "name": "Yipeng Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bf",
          "name": "Yifei Wu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c0",
          "name": "Yongjie Lyu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c1",
          "name": "Ziping Ma",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c2",
          "name": "Zipeng Feng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c3",
          "name": "Zhijiang Fang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c4",
          "name": "Zhihao Qiu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c5",
          "name": "Ziyuan Huang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c6",
          "name": "Zhengyu He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T02:50:49.000Z",
      "submittedOnDailyAt": "2025-06-13T01:53:15.172Z",
      "title": "Nom-Omni: Modelo de integración de la sensación y la generación",
      "submittedOnDailyBy": {
        "_id": "644fcbea4f7316588267dc80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
        "isPro": false,
        "fullname": "Biao Gong",
        "user": "BiaoGong",
        "type": "user"
      },
      "summary": "Men-Omni es un modelo de modalidad unificada que puede procesar imágenes, texto, voz y video, y muestra una fuerza potente en la generación de discursos e imágenes. Men-Omni utiliza un encoder especializado para extraer tokens de otras modalidades, que se conecta a un nuevo router específico para procesar en la arquitectura MoE. Esta diseño opera dentro de un marco unificado para procesar y fusionar múltiples entradas de modalidad de manera eficiente, sin necesidad de modelos o entrenamientos finales específicos para tareas diferentes, ni redesignes estructurales. Es crucial que Men-Omni aumente su valor al soportar la generación de voz y imágenes. Esto se logra al combinar un decodificador de voz avanzado con la generación de imágenes de alta calidad de Men-Lite-Uni, permitiendo la creación de vozes naturales, conversaciones contextualizadas, la transformación de texto en discursos y diversas ediciones de imágenes. Los resultados de los experimentos muestran que Men-Omni proporciona soluciones sólidas para la generación y integración de todas las modalidades. En particular, Men-Omni es el primer modelo abierto de código que implementa soporte para múltiples modalidades, y se propone a publicar todos los códigos y pesos del modelo para fomentar el progreso de la comunidad.",
      "upvotes": 4,
      "discussionId": "684ae277dbd21a9cc27b11c7",
      "ai_summary": "Ming-Omni is a unified multimodal model with dedicated encoders and modality-specific routers that can process images, text, audio, and video, and performs tasks like speech and image generation, context-aware chatting, and versatile image editing.",
      "ai_keywords": [
        "multimodal model",
        "encoders",
        "tokens",
        "MoE architecture",
        "modality-specific routers",
        "audio decoder",
        "Ming-Lite-Uni",
        "context-aware chatting",
        "text-to-speech conversion",
        "image editing",
        "unified perception",
        "generation",
        "open-source"
      ]
    },
    "publishedAt": "2025-06-10T22:50:49.000Z",
    "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
    "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644fcbea4f7316588267dc80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
      "fullname": "Biao Gong",
      "name": "BiaoGong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08060",
      "authors": [
        {
          "_id": "6848e0b042e4f9106973f280",
          "user": {
            "_id": "62f32eab52ad88c930bb3f3b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
            "isPro": true,
            "fullname": "Asankhaya Sharma",
            "user": "codelion",
            "type": "user"
          },
          "name": "Asankhaya Sharma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:08.045Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T08:37:19.000Z",
      "submittedOnDailyAt": "2025-06-13T00:31:17.814Z",
      "title": "Capacidades de un Transformer Fine-Tuned por Inferencia",
      "submittedOnDailyBy": {
        "_id": "62f32eab52ad88c930bb3f3b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
        "isPro": true,
        "fullname": "Asankhaya Sharma",
        "user": "codelion",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje natural han cambiado el procesamiento de la naturaleza del lenguaje, pero la entrenamiento de ajuste de detalles (SFT) es muy computacionalmente costoso. En este artículo, se demuestra formalmente que, en casos ideales, un método de inferencia que incluye el apoyo de cálculos infinitos y el acceso a conjuntos de datos de entrenamiento de ajuste de detalles (SFT) puede aproximar las habilidades obtenidas en un modelo básico, utilizando en particular el aprendizaje en contexto (ICL). Estos resultados se expanden a escenarios reales. Para la tarea de generación de texto con longitud de salida fija l, un conjunto de datos suficiente para aproximar las acciones de entrenamiento de ajuste de detalles en m contextos con un error de ε es O(mVε² log m/δ) o, en caso de contextos finitos, O(l log Vε² log 1/δ). En clasificación lineal, un conjunto de datos suficiente es O(dε) o, en caso de contextos fijos, O(1/ε² log 1/δ). Estos resultados proporcionan una base teórica para modelos basados en la completitud de Turing, y conectan la eficiencia de recursos de los modelos de lenguaje natural.",
      "upvotes": 4,
      "discussionId": "6848e0b042e4f9106973f281",
      "githubRepo": "https://github.com/codelion/optillm",
      "ai_summary": "Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.",
      "ai_keywords": [
        "supervised fine-tuning",
        "in-context learning",
        "base transformer model",
        "Turing completeness",
        "retrieval-augmented generation",
        "text generation",
        "linear classification"
      ]
    },
    "publishedAt": "2025-06-09T04:37:19.000Z",
    "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time\n  Techniques",
    "summary": "Large language models have transformed natural language processing, yet\nsupervised fine-tuning (SFT) remains computationally intensive. This paper\nformally proves that capabilities acquired through SFT can be approximated by a\nbase transformer model using inference-time techniques, specifically in-context\nlearning (ICL), without altering model parameters, under idealized assumptions\nincluding unbounded computational resources and access to the fine-tuning\ndataset. We extend these results to practical scenarios with finite context\nlengths and partial dataset access. For text generation tasks with fixed output\nlength l, datasets of size Oleft( m V{varepsilon^2} log\nm{delta} right) or, with bounded context, Oleft( l\nlog V{varepsilon^2} log 1{delta} right) suffice to approximate\nfine-tuned behavior across m contexts within error varepsilon, where V\nis the vocabulary size and delta is the failure probability. For linear\nclassification, datasets of size Oleft( d{varepsilon}\nright) or, with fixed context, Oleft( 1{varepsilon^2} log\n1{delta} right) are sufficient, where d is the input dimension.\nGrounded in the Turing completeness of transformers, these results provide a\ntheoretical foundation for resource-efficient deployment of large language\nmodels, with practical techniques like retrieval-augmented generation bridging\ntheory to real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f32eab52ad88c930bb3f3b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
      "fullname": "Asankhaya Sharma",
      "name": "codelion",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 91
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09952",
      "authors": [
        {
          "_id": "684ae226dbd21a9cc27b107a",
          "user": {
            "_id": "63579b21a8e247a69d4e13de",
            "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
            "isPro": false,
            "fullname": "Ziyi Wang",
            "user": "LavenderLA",
            "type": "user"
          },
          "name": "Ziyi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:40:18.063Z",
          "hidden": false
        },
        {
          "_id": "684ae226dbd21a9cc27b107b",
          "user": {
            "_id": "661cfae9a853782abad2a495",
            "avatarUrl": "/avatars/39723a07bf9efed8278e009fe966d044.svg",
            "isPro": false,
            "fullname": "Yanran Zhang",
            "user": "Yanran21",
            "type": "user"
          },
          "name": "Yanran Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:09.847Z",
          "hidden": false
        },
        {
          "_id": "684ae226dbd21a9cc27b107c",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "684ae226dbd21a9cc27b107d",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:23:21.000Z",
      "submittedOnDailyAt": "2025-06-13T06:56:09.722Z",
      "title": "UniPre3D: Aprendizaje de predicción integrado para nubes de puntos 3D y suavizado gaussiano cruzado de modo",
      "submittedOnDailyBy": {
        "_id": "63579b21a8e247a69d4e13de",
        "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
        "isPro": false,
        "fullname": "Ziyi Wang",
        "user": "LavenderLA",
        "type": "user"
      },
      "summary": "La variedad de escala en los datos de punto cloud representa un gran desafío para el desarrollo de métodos de aprendizaje de representación unificada en la visión 3D. Actualmente, existen pocos modelos 3D unificados y no hay métodos de pre-entrenamiento efectivos para ambos tipos de modelos: objetos y espacios. En este artículo, presentamos UniPre3D, el primer método de pre-entrenamiento unificado que puede aplicarse sin restricciones a cualquier escala de datos de punto cloud o cualquier arquitectura de modelo 3D. Nuestro enfoque consiste en predecir primitivas gaussianas como tarea de pre-entrenamiento, utilizar un splitting gaussiano diferenciable para renderizar imágenes y optimizar a nivel de píxeles para subobjetos y objetos finales. Además, controlamos la complejidad de la tarea de pre-entrenamiento y ajustamos el enfoque del modelo hacia la estructura geométrica, integrando características 2D de modelos de imágenes pre-entrenados y técnicas existentes. Demostramos la generalidad de nuestro método mediante una amplia gama de experimentos en diferentes modelos de punto cloud y varios tareas para modelos de objetos y espacios. El código está disponible en https://github.com/wangzy22/UniPre3D.",
      "upvotes": 3,
      "discussionId": "684ae226dbd21a9cc27b107e",
      "ai_summary": "UniPre3D is a unified pre-training method for 3D point clouds and models of any scale, using Gaussian primitives and 2D feature integration for effective performance across object and scene tasks.",
      "ai_keywords": [
        "point cloud",
        "3D vision",
        "representation learning",
        "UniPre3D",
        "Gaussian primitives",
        "differentiable Gaussian splatting",
        "pixel-level supervision",
        "end-to-end optimization",
        "2D features",
        "pre-trained image models",
        "geometric structures"
      ]
    },
    "publishedAt": "2025-06-11T13:23:21.000Z",
    "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal\n  Gaussian Splatting",
    "summary": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63579b21a8e247a69d4e13de",
      "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
      "fullname": "Ziyi Wang",
      "name": "LavenderLA",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09942",
      "authors": [
        {
          "_id": "684ae26adbd21a9cc27b1177",
          "user": {
            "_id": "625a5446f1063e7085d5178a",
            "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
            "isPro": false,
            "fullname": "Hao Peng",
            "user": "Wesleythu",
            "type": "user"
          },
          "name": "Hao Peng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:40:06.456Z",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b1178",
          "name": "Yunjia Qi",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b1179",
          "name": "Xiaozhi Wang",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117a",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117b",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117c",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:10:36.000Z",
      "submittedOnDailyAt": "2025-06-13T00:15:19.828Z",
      "title": "VerIF: Mejora de la Aprendizaje por Refuerzo en la Ingeniería de Verificación para la Instrumentación Pura",
      "submittedOnDailyBy": {
        "_id": "625a5446f1063e7085d5178a",
        "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
        "isPro": false,
        "fullname": "Hao Peng",
        "user": "Wesleythu",
        "type": "user"
      },
      "summary": "En el aprendizaje por refuerzo, la recompensa confiable (RLVR) se ha establecido como una tecnología importante para mejorar las funciones de los modelos de lenguaje grandes (LLMs), desempeñando un papel central en la prueba de teoría. Sin embargo, la optimización en aprendizaje por refuerzo basada en instrucciones aún está en un estado investigado poco profundo. En este estudio, se investiga el problema de la prueba de aprendizaje por refuerzo basado en instrucciones y se propone un método de prueba que combina la prueba de código basada en reglas y la prueba con modelos de lenguaje grandes (LLMs) (por ejemplo, QwQ-32B), llamado VerIF. Para ello, se construyó un dataset de alta calidad denominado VerInstruct, que incluye aproximadamente 22,000 instancias. La aplicación del aprendizaje por refuerzo con VerIF se ha realizado en dos modelos, y los modelos entrenados en un conjunto de pruebas de instrucciones representativas han mejorado significativamente su rendimiento en varias áreas, alcanzando un rendimiento avanzado en modelos relativamente pequeños y adaptándose ampliamente a nuevos restringimientos. Además, su capacidad general no está limitada por la RLVR y demuestra que puede mejorar el rendimiento de los modelos actuales. En este estudio, se publican el dataset, el código y los modelos, y se invita a acceder a https://github.com/THU-KEG/VerIF para promover futuras investigaciones.",
      "upvotes": 3,
      "discussionId": "684ae26adbd21a9cc27b117d",
      "githubRepo": "https://github.com/THU-KEG/VerIF",
      "ai_summary": "VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "RLVR",
        "large language models",
        "LLMs",
        "rule-based code verification",
        "QwQ-32B",
        "instruction-following",
        "VerInstruct",
        "RL training",
        "instruction-following benchmarks",
        "state-of-the-art performance",
        "existing RL recipes"
      ]
    },
    "publishedAt": "2025-06-11T13:10:36.000Z",
    "title": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09942.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625a5446f1063e7085d5178a",
      "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
      "fullname": "Hao Peng",
      "name": "Wesleythu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10953",
      "authors": [
        {
          "_id": "684b9fa13b733ba333687066",
          "user": {
            "_id": "5fa9ff3ea13e063b8b2b60cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
            "isPro": false,
            "fullname": "Xing Han Lù",
            "user": "xhluca",
            "type": "user"
          },
          "name": "Xing Han Lù",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:26.665Z",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687067",
          "name": "Gaurav Kamath",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687068",
          "name": "Marius Mosbach",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687069",
          "name": "Siva Reddy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:53:58.000Z",
      "submittedOnDailyAt": "2025-06-13T02:22:59.640Z",
      "title": "Preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la web para el outbound y el outbound para la web, preparar la",
      "submittedOnDailyBy": {
        "_id": "5fa9ff3ea13e063b8b2b60cb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
        "isPro": false,
        "fullname": "Xing Han Lù",
        "user": "xhluca",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de modelos de lenguaje grande (LLMs) y de diversificación ha generado un gran interés en el desarrollo de agentes web (sistemas AI). Este tipo de sistemas pueden realizar tareas automáticamente en entornos web. Debido a las diferencias fundamentales entre las interfaces web y las capacidades de los LLMs, los métodos actuales de acceso presentan problemas significativos en la automatización de interacciones web complejas. Actualmente, estos métodos consisten en procesar árboles DOM grandes, agregar información adicional a pantallas de escena o interactuar con APIs que evitan completamente la interfaz de usuario. Este artículo propone una transición en el paradigma de la investigación en agentes web: es posible que los agentes web se adapten a interfaces humanas, pero no necesariamente. En su lugar, es necesario desarrollar un nuevo paradigma de interfaz optimizado para los capacidades de los agentes. En este trabajo, se presenta el concepto de una interfaz web centrada en el agente (AWI) y se destacan principios de diseño centrados en seguridad, eficiencia y estándarización, así como se proponen seis principios de orientación que consideran los intereses de las principales partes interesadas. Esta referencia supera los límites básicos de las interfaces actuales y permite diseñar agentes web más eficientes y confiables con mayor transparencia. Esto requiere una mayor colaboración de la comunidad más amplia de ML.",
      "upvotes": 2,
      "discussionId": "684b9fa13b733ba33368706a",
      "ai_summary": "A new Agentic Web Interface (AWI) design paradigm is proposed to optimize web agents for navigating websites, focusing on safety, efficiency, and standardization to address fundamental interface mismatches.",
      "ai_keywords": [
        "Large Language Models",
        "multimodal",
        "web agents",
        "Agentic Web Interface",
        "AWI",
        "DOM trees",
        "screenshots",
        "API interactions"
      ]
    },
    "publishedAt": "2025-06-12T13:53:58.000Z",
    "title": "Build the web for agents, not agents for the web",
    "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10953.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fa9ff3ea13e063b8b2b60cb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
      "fullname": "Xing Han Lù",
      "name": "xhluca",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07795",
      "authors": [
        {
          "_id": "6848dca942e4f9106973f25c",
          "user": {
            "_id": "6659b410a69183808d04b22f",
            "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
            "isPro": false,
            "fullname": "Xiaotian Ye",
            "user": "Acruxos",
            "type": "user"
          },
          "name": "Xiaotian Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:10.086Z",
          "hidden": false
        },
        {
          "_id": "6848dca942e4f9106973f25d",
          "name": "Mengqi Zhang",
          "hidden": false
        },
        {
          "_id": "6848dca942e4f9106973f25e",
          "name": "Shu Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T14:21:25.000Z",
      "submittedOnDailyAt": "2025-06-13T05:51:30.395Z",
      "title": "LLM Unlearning no debe depender de la forma.",
      "submittedOnDailyBy": {
        "_id": "6659b410a69183808d04b22f",
        "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
        "isPro": false,
        "fullname": "Xiaotian Ye",
        "user": "Acruxos",
        "type": "user"
      },
      "summary": "Los grandes modelos de lenguaje (LLM) tienen la capacidad de olvidar, que permite eliminar o suprimir conocimientos inadecuados, controlar información dañina o oculta, y prevenir usos erróneos. Sin embargo, recientes estudios indican que este efecto es limitado en escalas reales y puede impidir la introducción práctica. En este trabajo, se identificaron generalmente los problemas de fallo en varios sub-estados: el efecto de los métodos de olvidar se basa en una fuerte dependencia de la forma de los ejemplos de entrenamiento, y la generalización hacia otras representaciones del mismo conocimiento es deficiente. Estos problemas se caracterizaron formalmente como \"Bias de Dependencia de la Forma\" y se investigaron patrones de representación específicos en diferentes sub-tareas. Para mitigar la propagación de estos problemas, se introdujo un nuevo benchmark llamado \"ORT\" para evaluar la robustez de los métodos de olvidar frente a cambios en la representación de conocimiento. Los resultados muestran que en la tecnología actual, el \"Bias de Dependencia de la Forma\" es ampliamente y gravemente presente.\n\nPara evitar que la función de olvidar dependa de la forma de las sub-tareas que aparecen en el mundo real, es necesario que estos métodos no dependan de la forma. Para lograr esto, se presenta un nuevo método basado en el aprendizaje no supervisado llamado \"Redirection de Conceptos de Rank Uno (ROCR)\", que introduce las raíces de soluciones adecuadas. El ROCR se centra en la invariancia de las sub-tareas y olvida los conceptos peligrosos activados, cambiando los parámetros del modelo en tiempo real para redirigir la percepción del modelo a otros conceptos no dañinos. Los experimentos extendidos muestran que comparados con los métodos tradicionales, el ROCR mejora significativamente el efecto de olvidar y genera salidas naturales de alta calidad.",
      "upvotes": 2,
      "discussionId": "6848dca942e4f9106973f25f",
      "ai_summary": "Form-Dependent Bias limits the effectiveness of LLM unlearning across different knowledge expressions, and Rank-one Concept Redirection (ROCR) is proposed as a form-independent solution that enhances unlearning efficacy.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "unlearning",
        "Form-Dependent Bias",
        "ORT",
        "Rank-one Concept Redirection (ROCR)",
        "downstream tasks",
        "unlearning methods",
        "concept redirection",
        "model parameters",
        "activated dangerous concepts"
      ]
    },
    "publishedAt": "2025-06-09T10:21:25.000Z",
    "title": "LLM Unlearning Should Be Form-Independent",
    "summary": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to generalize to alternate expressions of\nthe same knowledge. We formally characterize this problem as Form-Dependent\nBias and systematically investigate its specific manifestation patterns across\nvarious downstream tasks. To quantify its prevalence and support future\nresearch, we introduce ORT, a novel benchmark designed to evaluate the\nrobustness of unlearning methods against variations in knowledge expression.\nResults reveal that Form-Dependent Bias is both widespread and severe among\ncurrent techniques.\n  We argue that LLM unlearning should be form-independent to address the\nendless forms of downstream tasks encountered in real-world security-critical\nscenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),\na novel training-free method, as a promising solution path. ROCR performs\nunlearning by targeting the invariants in downstream tasks, specifically the\nactivated dangerous concepts. It is capable of modifying model parameters\nwithin seconds to redirect the model's perception of a specific unlearning\ntarget concept to another harmless concept. Extensive experiments demonstrate\nthat ROCR significantly improves unlearning effectiveness compared to\ntraditional methods while generating highly natural outputs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07795.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6659b410a69183808d04b22f",
      "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
      "fullname": "Xiaotian Ye",
      "name": "Acruxos",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06694",
      "authors": [
        {
          "_id": "684ba6bc3b733ba333687093",
          "name": "Yuan Yuan",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687094",
          "name": "Yukun Liu",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687095",
          "name": "Chonghua Han",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687096",
          "user": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "isPro": false,
            "fullname": "Jie Feng",
            "user": "JJ-TMT",
            "type": "user"
          },
          "name": "Jie Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:13.023Z",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687097",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/LfNEmKBa7cYZhNNRnZRIx.png"
      ],
      "publishedAt": "2025-06-07T07:19:11.000Z",
      "submittedOnDailyAt": "2025-06-13T02:53:49.430Z",
      "title": "Destruir el Data Scroll: Construir un modelo a través de aprendizaje generativo con un enfoque abierto y escalable basado en movimiento.",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "El modelo básico ha cambiado de manera innovadora las áreas de procesamiento del lenguaje natural y visión por computadora. Estos modelos han permitido un aprendizaje generalizado para varias tareas y conjuntos de datos. Sin embargo, la construcción de modelos básicos para el movimiento ha sido difícil debido a la escondida naturaleza de los datos de movimiento y la existencia de diferentes escalas de datos entre instituciones. Para resolver esto, proponemos el \"MoveGCL\", un marco de trabajo escalable y preservador de la privacidad que permite el entrenamiento de modelos básicos de movimiento a través de aprendizaje continuo generativo. MoveGCL recrea el tráfico sintético generado por modelos de enseñanza libre, permitiendo la evolución distribuida y progresiva del modelo, y evitando la comparación de datos mediante una estrategia de desaceleración de catástrofes para fortalecer la memoria. Para abordar la diversidad de patrones de movimiento, MoveGCL utiliza un Transformer Mixture-of-Experts con expertos en el conocimiento del movimiento, adaptándose de manera jerárquica y estable a través de actualizaciones continuas. Los experimentos con 6 conjuntos de datos reales de ciudades muestran que MoveGCL logra resultados comparables a los de entrenamiento conjunto, supera significativamente el aprendizaje federado y ofrece una fuerte protección de la privacidad. MoveGCL marca un importante paso en el desarrollo de modelos básicos para el movimiento y proporciona una planificación práctica para el desarrollo de modelos abiertos y escalables que preservan la privacidad.",
      "upvotes": 2,
      "discussionId": "684ba6bc3b733ba333687098",
      "githubRepo": "https://github.com/ScottLiu2003/MoveGCL",
      "ai_summary": "MoveGCL is a privacy-preserving framework using generative continual learning and a Mixture-of-Experts Transformer for training mobility foundation models without sharing raw data.",
      "ai_keywords": [
        "generative continual learning",
        "privacy-preserving",
        "Mixture-of-Experts Transformer",
        "mobility-aware expert routing mechanism",
        "layer-wise progressive adaptation",
        "catastrophic forgetting",
        "federated learning"
      ]
    },
    "publishedAt": "2025-06-07T03:19:11.000Z",
    "title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation\n  Models via Generative Continual Learning",
    "summary": "Foundation models have revolutionized fields such as natural language\nprocessing and computer vision by enabling general-purpose learning across\ndiverse tasks and datasets. However, building analogous models for human\nmobility remains challenging due to the privacy-sensitive nature of mobility\ndata and the resulting data silos across institutions. To bridge this gap, we\npropose MoveGCL, a scalable and privacy-preserving framework for training\nmobility foundation models via generative continual learning. Without sharing\nraw data, MoveGCL enables decentralized and progressive model evolution by\nreplaying synthetic trajectories generated from a frozen teacher model, and\nreinforces knowledge retention through a tailored distillation strategy that\nmitigates catastrophic forgetting. To address the heterogeneity of mobility\npatterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a\nmobility-aware expert routing mechanism, and employs a layer-wise progressive\nadaptation strategy to stabilize continual updates. Experiments on six\nreal-world urban datasets demonstrate that MoveGCL achieves performance\ncomparable to joint training and significantly outperforms federated learning\nbaselines, while offering strong privacy protection. MoveGCL marks a crucial\nstep toward unlocking foundation models for mobility, offering a practical\nblueprint for open, scalable, and privacy-preserving model development in the\nera of foundation models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/LfNEmKBa7cYZhNNRnZRIx.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10036",
      "authors": [
        {
          "_id": "684bc8db3b733ba33368718c",
          "name": "Javad Rajabi",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718d",
          "name": "Soroush Mehraban",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718e",
          "user": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "isPro": false,
            "fullname": "Seyedmorteza Sadat",
            "user": "msadat97",
            "type": "user"
          },
          "name": "Seyedmorteza Sadat",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:32.584Z",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718f",
          "name": "Babak Taati",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T21:25:46.000Z",
      "submittedOnDailyAt": "2025-06-13T05:21:38.595Z",
      "title": "Token PaBERBER GUIDFLIVE Model",
      "submittedOnDailyBy": {
        "_id": "63b4b02a103617b0a5b0ee2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
        "isPro": false,
        "fullname": "Seyedmorteza Sadat",
        "user": "msadat97",
        "type": "user"
      },
      "summary": "La Clasificación de Preferencias (CFG) es uno de los componentes importantes de los modelos de diferenciación modernos, y es esencial para mejorar la calidad de generación y la coincidencia con las condiciones de entrada. Sin embargo, la CFG requiere un proceso de entrenamiento específico y está limitada en la generación condicional. Para resolver estos limitaciones, proponemos un nuevo método llamado Token Perturbation Guidance (TPG). El TPG aplica una matriz de perturbación directamente a las representaciones de tokens intermedios dentro de la red de diferenciación. Utilizando operaciones de mezcla para mantener la normalidad, el TPG proporciona un señal de guía efectivo y estable que mejora la calidad de generación a pesar de cambios estructurales. De esta manera, el TPG puede aplicarse tanto a la generación condicional como a la no condicional sin necesidad de entrenamiento y sin depender de las condiciones de entrada. Además, el análisis adicional de la información de guía proporcionada por el TPG muestra métodos de guía no necesitando entrenamiento, como el TPG, y compara su desempeño con la CFG. Los experimentos expandidos en SDXL y Stable Diffusion 2.1 muestran que el TPG logra una mejora aproximadamente en la calidad de generación no condicional de 2 veces en comparación con la línea SDXL, y también muestra una coincidencia similar con la CFG en la calidad de los prompts. Estos resultados demuestran que el TPG es un método de guía que no depende de condiciones generales, y establece que ofrece las mismas ventajas que la CFG en una amplia gama de modelos de diferenciación. El código está disponible en la siguiente URL.\nhttps://github.com/TaatiTeam/Token-Perturbation-Guidance",
      "upvotes": 1,
      "discussionId": "684bc8db3b733ba333687190",
      "githubRepo": "https://github.com/TaatiTeam/Token-Perturbation-Guidance",
      "ai_summary": "Token Perturbation Guidance (TPG) enhances diffusion models with condition-agnostic, training-free guidance, similar to classifier-free guidance (CFG), without requiring architectural changes.",
      "ai_keywords": [
        "classifier-free guidance (CFG)",
        "Token Perturbation Guidance (TPG)",
        "perturbation matrices",
        "intermediate token representations",
        "norm-preserving shuffling",
        "FID",
        "prompt alignment",
        "SDXL",
        "Stable Diffusion 2.1"
      ]
    },
    "publishedAt": "2025-06-10T17:25:46.000Z",
    "title": "Token Perturbation Guidance for Diffusion Models",
    "summary": "Classifier-free guidance (CFG) has become an essential component of modern\ndiffusion models to enhance both generation quality and alignment with input\nconditions. However, CFG requires specific training procedures and is limited\nto conditional generation. To address these limitations, we propose Token\nPerturbation Guidance (TPG), a novel method that applies perturbation matrices\ndirectly to intermediate token representations within the diffusion network.\nTPG employs a norm-preserving shuffling operation to provide effective and\nstable guidance signals that improve generation quality without architectural\nchanges. As a result, TPG is training-free and agnostic to input conditions,\nmaking it readily applicable to both conditional and unconditional generation.\nWe further analyze the guidance term provided by TPG and show that its effect\non sampling more closely resembles CFG compared to existing training-free\nguidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1\nshow that TPG achieves nearly a 2times improvement in FID for unconditional\ngeneration over the SDXL baseline, while closely matching CFG in prompt\nalignment. These results establish TPG as a general, condition-agnostic\nguidance method that brings CFG-like benefits to a broader class of diffusion\nmodels. The code is available at\nhttps://github.com/TaatiTeam/Token-Perturbation-Guidance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10036.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b4b02a103617b0a5b0ee2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
      "fullname": "Seyedmorteza Sadat",
      "name": "msadat97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08373",
      "authors": [
        {
          "_id": "684ae1f3dbd21a9cc27b0f32",
          "name": "Kevin Galim",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f33",
          "name": "Ethan Ewer",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f34",
          "name": "Wonjun Kang",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f35",
          "name": "Minjae Lee",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f36",
          "name": "Hyung Il Koo",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f37",
          "name": "Kangwook Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T02:37:46.000Z",
      "submittedOnDailyAt": "2025-06-13T00:35:36.456Z",
      "title": "Inferencia aproximada basada en el dramaturgo para los modelos de lenguaje grandes",
      "submittedOnDailyBy": {
        "_id": "630c90123dc31beba6e8f406",
        "avatarUrl": "/avatars/2188b41fff122d4f5683b46c529ed79d.svg",
        "isPro": false,
        "fullname": "Kevin Galim",
        "user": "kev95",
        "type": "user"
      },
      "summary": "Optimizar la inferencia de modelos de lenguaje de gran escala en contextos de largo texto es ahora crucial debido a la importancia de la complejidad de cálculo en dos dimensiones y la complejidad de memoria lineal en Transformers. Los métodos aproximados actuales predominantemente utilizan la eliminación de la caché de KV, la sparseificación de los parámetros de función y la compresión de los bloques para predecir aproximadamente la importancia de los tokens o pares de KV. Proponemos un nuevo marco que utiliza pequeños modelos de \"draft\" para predecir con mayor precisión la importancia de los tokens y pares de KV. En particular, proponemos dos implementaciones: (i) SpecKV, que utiliza los salidas del modelo de \"draft\" para evaluar precisamente la importancia de cada pare de KV, lo que permite una eliminación más efectiva de la caché de KV. (ii) SpecPC, que utiliza la activación de la atención del modelo de \"draft\" para identificar y eliminar los tokens de bloque más importantes. Según nuestra experiencia, esto es el primer estudio que aplica aproximaciones a la velocidad de inferencia de LLMs utilizando modelos de \"draft\". Esto amplia el papel tradicional de la inferencia sin pérdida de calidad. Utilizamos análisis teórico y experimental para explicar nuestro método y mostrar la relación entre los patrones de atención de los modelos de \"draft\" y los modelos objetivos. A través de experimentos en un marco de referencia de largo texto, nuestro método demostró ser más preciso y mejorar la memoria utilizada, el número de turnos y el número de ciclos en comparación con los estándares existentes. Nuestro código está disponible en https://github.com/furiosa-ai/draft-based-approx-llm.",
      "upvotes": 1,
      "discussionId": "684ae1f3dbd21a9cc27b0f38",
      "ai_summary": "A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.",
      "ai_keywords": [
        "Large Language Models",
        "Transformers",
        "key-value cache dropping",
        "sparse attention",
        "prompt compression",
        "draft models",
        "SpecKV",
        "SpecPC",
        "attention activations",
        "long-context benchmarks"
      ]
    },
    "publishedAt": "2025-06-09T22:37:46.000Z",
    "title": "Draft-based Approximate Inference for LLMs",
    "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08373.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630c90123dc31beba6e8f406",
      "avatarUrl": "/avatars/2188b41fff122d4f5683b46c529ed79d.svg",
      "fullname": "Kevin Galim",
      "name": "kev95",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08234",
      "authors": [
        {
          "_id": "684ae1dddbd21a9cc27b0edc",
          "name": "Yu-Ang Lee",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0edd",
          "name": "Guan-Ting Yi",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0ede",
          "name": "Mei-Yi Liu",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0edf",
          "name": "Jui-Chao Lu",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0ee0",
          "name": "Guan-Bo Yang",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0ee1",
          "name": "Yun-Nung Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T21:04:14.000Z",
      "submittedOnDailyAt": "2025-06-13T07:22:29.081Z",
      "title": "Optimización de sistemas AI de síntesis: resumen de métodos, problemas y direcciones futuras",
      "submittedOnDailyBy": {
        "_id": "6615752da15c52fa7ab3e2f7",
        "avatarUrl": "/avatars/37e72cfb829a42630d229080ad8d60f3.svg",
        "isPro": false,
        "fullname": "Lee",
        "user": "Speeeed",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de grandes modelos de lenguaje (LLMs) y sistemas de IA ha introducido un nuevo paradigma en la diseño y optimización de procesos complejos de trabajo de IA. Los sistemas de IA complejos han integrado diversos componentes para realizar tareas complejas. Sin embargo, con la complejidad de estos sistemas, se han surgido nuevos problemas no solo en la optimización de cada componente individual, sino también en la optimización de su interacción mutua. A pesar de que métodos tradicionales como aprendizaje supervisado basado en texto (SFT) y aprendizaje por refuerzo (RL) siguen siendo fundamentales, el aumento de retroalimentación natural ha llevado a la identificación de nuevas aproximaciones para la optimización de sistemas no diferenciables. Este artículo investiga avances recientes en la optimización de sistemas de IA complejos de manera sistemática y proporciona técnicas numéricas y basadas en lenguaje. Se formaliza el concepto de optimización de sistemas de IA complejos y clasifica métodos existentes para iluminar las investigaciones abiertas y futuras en esta área que está cambiando rápidamente. La lista de artículos investigados está disponible en https://github.com/MiuLab/AISysOpt-Survey.",
      "upvotes": 1,
      "discussionId": "684ae1dedbd21a9cc27b0ee2",
      "ai_summary": "Recent advancements in optimizing compound AI systems highlight challenges in integrating various components, with an emphasis on natural language feedback methods for non-differentiable systems.",
      "ai_keywords": [
        "large language models",
        "AI systems",
        "compound AI systems",
        "supervised fine-tuning",
        "reinforcement learning",
        "natural language feedback",
        "non-differentiable systems"
      ]
    },
    "publishedAt": "2025-06-09T17:04:14.000Z",
    "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and\n  Future Directions",
    "summary": "Recent advancements in large language models (LLMs) and AI systems have led\nto a paradigm shift in the design and optimization of complex AI workflows. By\nintegrating multiple components, compound AI systems have become increasingly\nadept at performing sophisticated tasks. However, as these systems grow in\ncomplexity, new challenges arise in optimizing not only individual components\nbut also their interactions. While traditional optimization methods such as\nsupervised fine-tuning (SFT) and reinforcement learning (RL) remain\nfoundational, the rise of natural language feedback introduces promising new\napproaches, especially for optimizing non-differentiable systems. This paper\nprovides a systematic review of recent progress in optimizing compound AI\nsystems, encompassing both numerical and language-based techniques. We\nformalize the notion of compound AI system optimization, classify existing\nmethods along several key dimensions, and highlight open research challenges\nand future directions in this rapidly evolving field. A list of surveyed papers\nis publicly available at https://github.com/MiuLab/AISysOpt-Survey.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08234.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6615752da15c52fa7ab3e2f7",
      "avatarUrl": "/avatars/37e72cfb829a42630d229080ad8d60f3.svg",
      "fullname": "Lee",
      "name": "Speeeed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06950",
      "authors": [
        {
          "_id": "684ae1fbdbd21a9cc27b0f51",
          "name": "Do Xuan Long",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f52",
          "name": "Duy Dinh",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f53",
          "name": "Ngoc-Hai Nguyen",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f54",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f55",
          "name": "Nancy F. Chen",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f56",
          "name": "Shafiq Joty",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f57",
          "name": "Min-Yen Kan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-07T23:19:27.000Z",
      "submittedOnDailyAt": "2025-06-13T03:14:57.042Z",
      "title": "¿Qué elementos necesitan para crear un buen prompt de naturaleza?",
      "submittedOnDailyBy": {
        "_id": "63a9a0d13453852ef53c0b37",
        "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
        "isPro": false,
        "fullname": "Do Xuan Long",
        "user": "dxlong2000",
        "type": "user"
      },
      "summary": "LLM se desarrolla de manera casi idéntica al de los humanos en el desarrollo de la comunicación. El prompt se convirtió en un elemento determinante. Sin embargo, se encuentra limitada la conciencia común conceptual para cuantificar los prompts de lenguaje natural. Se investigaron más de 150 artículos y blogs sobre prompts en conferencias de NLP y AI avanzadas de 2022 a 2025 para abordar este problema. Se proponen propiedades y un marco centrado en el ser humano, clasificadas en 6 dimensiones a partir de 21 propiedades. Se examina también la influencia de la investigación actual en los LLMs, revelando que el equilibrio entre modelos y tareas puede ser desbalanceado y que los defectos de la investigación pueden ser significativos. Además, se analizan las correlaciones entre las propiedades de los prompts de alta calidad de lenguaje natural y se obtienen recomendaciones para los prompts. Se investiga experimentalmente la mejora de los prompts con múltiples propiedades y se encuentra que la mejora de una sola propiedad tiene el mayor impacto. Finalmente, se observa que los modelos lógicos se mejoran durante la tunelamiento de entrenamiento con prompts que incluyen propiedades expandidas. Nuestros hallazgos preparan la base para la evaluación y optimización centrada en propiedades de prompts, aumentan la diferencia entre el ser humano y el AI, y abren nuevas direcciones para la investigación en prompts.",
      "upvotes": 1,
      "discussionId": "684ae1fbdbd21a9cc27b0f58",
      "ai_summary": "A framework for evaluating and optimizing natural language prompts in large language models is proposed, revealing correlations between prompt properties and their impact on reasoning tasks.",
      "ai_keywords": [
        "large language models",
        "prompting",
        "meta-analysis",
        "property-centric framework",
        "instruction-tuning",
        "reasoning tasks"
      ]
    },
    "publishedAt": "2025-06-07T19:19:27.000Z",
    "title": "What Makes a Good Natural Language Prompt?",
    "summary": "As large language models (LLMs) have progressed towards more human-like and\nhuman--AI communications have become prevalent, prompting has emerged as a\ndecisive component. However, there is limited conceptual consensus on what\nexactly quantifies natural language prompts. We attempt to address this\nquestion by conducting a meta-analysis surveying more than 150\nprompting-related papers from leading NLP and AI conferences from 2022 to 2025\nand blogs. We propose a property- and human-centric framework for evaluating\nprompt quality, encompassing 21 properties categorized into six dimensions. We\nthen examine how existing studies assess their impact on LLMs, revealing their\nimbalanced support across models and tasks, and substantial research gaps.\nFurther, we analyze correlations among properties in high-quality natural\nlanguage prompts, deriving prompting recommendations. We then empirically\nexplore multi-property prompt enhancements in reasoning tasks, observing that\nsingle-property enhancements often have the greatest impact. Finally, we\ndiscover that instruction-tuning on property-enhanced prompts can result in\nbetter reasoning models. Our findings establish a foundation for\nproperty-centric prompt evaluation and optimization, bridging the gaps between\nhuman--AI communication and opening new prompting research directions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06950.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a9a0d13453852ef53c0b37",
      "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
      "fullname": "Do Xuan Long",
      "name": "dxlong2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06561",
      "authors": [
        {
          "_id": "684b88113b733ba333686fc7",
          "name": "Ho Yin 'Sam' Ng",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fc8",
          "name": "Ting-Yao Hsu",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fc9",
          "name": "Aashish Anantha Ramakrishnan",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fca",
          "name": "Branislav Kveton",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcb",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcc",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:49.692Z",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcd",
          "name": "Dongwon Lee",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fce",
          "name": "Tong Yu",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcf",
          "name": "Sungchul Kim",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fd0",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fd1",
          "name": "Ting-Hao 'Kenneth' Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T22:16:16.000Z",
      "submittedOnDailyAt": "2025-06-13T00:38:33.715Z",
      "title": "LaMP-Cap: Generación de capturas de fijación personalizadas mediante perfiles de fijación multimodal",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "El título de un gráfico es crucial para que los lectores puedan comprender y recordar la principal mensaje del gráfico. Muchos modelos han sido desarrollados para generar estos títulos. Estos modelos ayudan a los autores a crear títulos de mejor calidad de manera sencilla. Sin embargo, los autores necesitan editar títulos de AI estándar para que se ajusten a estilos de escritura y campos específicos. Esto hace evidente la necesidad de personalización. A pesar del desarrollo de LaMP (Language Model Personalization), estas tecnologías se centran principalmente en la configuración del lenguaje y poco se investigan casos de entradas y perfiles diversos. En este artículo, se presenta LaMP-Cap. LaMP-Cap proporciona un conjunto de datos para la generación de títulos personalizados de gráficos utilizando archivos de perfiles de diferentes gráficos. LaMP-Cap proporciona la información necesaria para el gráfico en cuestión, así como un perfil que incluye tres imágenes de gráficos, títulos y frases que mencionan el gráfico, caracterizando el contexto. Los experimentos con cuatro modelos de lenguaje (LLM) muestran que los títulos se generan utilizando la información del perfil, y que estos son similares a los que el autor original escribió. Los test de exclusión demuestran que las imágenes del perfil son más útiles que las frases que mencionan el gráfico, y que utilizar diversos perfiles es más beneficioso que solo usar el lenguaje.",
      "upvotes": 1,
      "discussionId": "684b88123b733ba333686fd2",
      "ai_summary": "LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.",
      "ai_keywords": [
        "LaMP-Cap",
        "personalized figure caption generation",
        "multimodal figures",
        "figure profiles",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-06-06T18:16:16.000Z",
    "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles",
    "summary": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06561.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05982",
      "authors": [
        {
          "_id": "684b86913b733ba333686fb8",
          "name": "Zonglin Wu",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fb9",
          "name": "Yule Xue",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fba",
          "name": "Xin Wei",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fbb",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T11:02:01.000Z",
      "submittedOnDailyAt": "2025-06-13T00:33:34.648Z",
      "title": "MCA-Bench: Criterio de Evaluación de Diversidad para la Evaluación de la Resistencia de CAPTCHA frente a Ataques basados en VLM",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "Los métodos de ataques de automatización están desarrollandose rápidamente, lo que hace que CAPTCHA sea una estructura de defensa importante para bloquear bots maliciosos. Sin embargo, aunque los escaneadores de CAPTCHA actuales han ampliado su rango a incluir diversas modalidades, como letras de giro estáticas, imágenes contaminadas con ruido, clics cruzados, puzzles de deslizamiento y preguntas basadas en lógica, la comunidad aún no tiene un marco de referencia unificado y de gran escala que incluya diversas modalidades. Para resolver esta limitación, se presenta MCA-Bench, un sistema de referencia de prueba integrado que incluye diversos tipos de CAPTCHA. Utilizando el retrotrack de modelos de lenguaje de visión compartidos, se ajustan cráneos especializados para cada categoría de CAPTCHA y se permite una evaluación consistente y cruzada de modalidades. A través de una amplia gama de experimentos, MCA-Bench ha convertido eficazmente el espectro de vulnerabilidades de los diseños modernos de CAPTCHA en un banco de palabras, y, de manera importante, proporciona por primera vez un análisis cuantitativo de la complejidad de los desafíos, la profundidad de la interacción y la relación entre la resolución del modelo. Basándose en estas descubrimientos, se proponen tres principios de diseño operativos, se identifican problemas abiertos principales, y se establece una base para la fortalecimiento sistemático de los CAPTCHA, la creación de un marco de referencia justo y la construcción de una cooperación más amplia de la comunidad. El conjunto de datos y el código están disponibles en línea.",
      "upvotes": 1,
      "discussionId": "684b86923b733ba333686fbc",
      "ai_summary": "MCA-Bench provides a unified benchmark for evaluating CAPTCHA security using a shared vision-language model and attackers specialized for each type of CAPTCHA.",
      "ai_keywords": [
        "vision-language model",
        "CAPTCHA",
        "benchmark",
        "evaluation protocol",
        "cracking agents",
        "vulnerability spectrum",
        "challenge complexity",
        "interaction depth",
        "model solvability"
      ]
    },
    "publishedAt": "2025-06-06T07:02:01.000Z",
    "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks",
    "summary": "As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking\nsuite that integrates heterogeneous CAPTCHA types into a single evaluation\nprotocol. Leveraging a shared vision-language model backbone, we fine-tune\nspecialized cracking agents for each CAPTCHA category, enabling consistent,\ncross-modal assessments. Extensive experiments reveal that MCA-Bench\neffectively maps the vulnerability spectrum of modern CAPTCHA designs under\nvaried attack settings, and crucially offers the first quantitative analysis of\nhow challenge complexity, interaction depth, and model solvability interrelate.\nBased on these findings, we propose three actionable design principles and\nidentify key open challenges, laying the groundwork for systematic CAPTCHA\nhardening, fair benchmarking, and broader community collaboration. Datasets and\ncode are available online.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05982.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10378",
      "authors": [
        {
          "_id": "684bb08e3b733ba3336870bf",
          "name": "Jikai Jin",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c0",
          "name": "Vasilis Syrgkanis",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c1",
          "name": "Sham Kakade",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c2",
          "name": "Hanlin Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T06:07:42.000Z",
      "submittedOnDailyAt": "2025-06-13T03:36:14.455Z",
      "title": "Disbarreling Whirlixical Retent Capability Format Module by Causally Regulated Learning",
      "submittedOnDailyBy": {
        "_id": "624054bcc2c17da6a63eb539",
        "avatarUrl": "/avatars/bf52dc0683b4100733f8696a97696d0e.svg",
        "isPro": false,
        "fullname": "hlzhang109",
        "user": "hlzhang109",
        "type": "user"
      },
      "summary": "La evaluación de capacidades de modelos de lenguaje confiable es un elemento importante en el proceso de desarrollo. Sin embargo, en esta área existen grandes problemas metodológicos, como los efectos complejos de la confusión y los altos costos computacionales debido a la expansión de entrenamiento de múltiples modelos. Para resolver estos problemas, proponemos un marco de aprendizaje causal que modela el rendimiento de benchmark observados a través de una transformación lineal de unos pocos coeficientes potenciales de capacidad. Es crucial que estos coeficientes potenciales puedan controlar adecuadamente la confusión común del modelo y reconocer las relaciones causales entre ellos. Aplicando este enfoque a un conjunto detallado de 1500 modelos más que incluye 6 benchmarks seleccionados en el Open LLM Leaderboard, hemos demostrado que los cambios en el rendimiento observado pueden explicarse mediante una estructura causal lineal de confianza en tres nodos. La interpretación adicional de esta estructura causal proporciona una comprensión científica más profunda que simples rankings numéricos, desde la capacidad para resolver problemas generales hasta la demostración de una dirección causal clara en la capacidad de razonamiento matemático. Nuestros resultados subrayan la importancia de controlar cuidadosamente los cambios en el modelo básico durante la evaluación y explican que eso es un paso esencial para clarificar las posibles relaciones causales potenciales entre las capacidades del modelo.",
      "upvotes": 0,
      "discussionId": "684bb08e3b733ba3336870c3",
      "ai_summary": "A causal representation learning framework identifies a concise causal structure to explain performance variations in language models across benchmarks by controlling for base model variations.",
      "ai_keywords": [
        "causal representation learning",
        "latent capability factors",
        "causal interrelated",
        "base model confounder",
        "Open LLM Leaderboard",
        "linear causal structure",
        "general problem-solving capabilities",
        "instruction-following proficiency",
        "mathematical reasoning ability"
      ]
    },
    "publishedAt": "2025-06-12T02:07:42.000Z",
    "title": "Discovering Hierarchical Latent Capabilities of Language Models via\n  Causal Representation Learning",
    "summary": "Faithful evaluation of language model capabilities is crucial for deriving\nactionable insights that can inform model development. However, rigorous causal\nevaluations in this domain face significant methodological challenges,\nincluding complex confounding effects and prohibitive computational costs\nassociated with extensive retraining. To tackle these challenges, we propose a\ncausal representation learning framework wherein observed benchmark performance\nis modeled as a linear transformation of a few latent capability factors.\nCrucially, these latent factors are identified as causally interrelated after\nappropriately controlling for the base model as a common confounder. Applying\nthis approach to a comprehensive dataset encompassing over 1500 models\nevaluated across six benchmarks from the Open LLM Leaderboard, we identify a\nconcise three-node linear causal structure that reliably explains the observed\nperformance variations. Further interpretation of this causal structure\nprovides substantial scientific insights beyond simple numerical rankings:\nspecifically, we reveal a clear causal direction starting from general\nproblem-solving capabilities, advancing through instruction-following\nproficiency, and culminating in mathematical reasoning ability. Our results\nunderscore the essential role of carefully controlling base model variations\nduring evaluation, a step critical to accurately uncovering the underlying\ncausal relationships among latent model capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "624054bcc2c17da6a63eb539",
      "avatarUrl": "/avatars/bf52dc0683b4100733f8696a97696d0e.svg",
      "fullname": "hlzhang109",
      "name": "hlzhang109",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08862",
      "authors": [
        {
          "_id": "684ae26ddbd21a9cc27b117f",
          "name": "Zike Wu",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1180",
          "name": "Qi Yan",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1181",
          "name": "Xuanyu Yi",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1182",
          "name": "Lele Wang",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1183",
          "name": "Renjie Liao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T14:52:36.000Z",
      "submittedOnDailyAt": "2025-06-13T07:33:07.278Z",
      "title": "StreamSplat: Hacia lo que, reconstrucción dinámica 3D en línea desde un vídeo stream sin corrección",
      "submittedOnDailyBy": {
        "_id": "648058ff8c6a3b8f11f77893",
        "avatarUrl": "/avatars/043c832314a8d6713af90d7c255fc2f2.svg",
        "isPro": false,
        "fullname": "Wu Zike",
        "user": "Nickwzk",
        "type": "user"
      },
      "summary": "En un espacio 3D dinámico, la streaming de vídeos de texturas sin desafío y su reconstrucción en unidades de tiempo son importantes para múltiples aplicaciones en la realidad. Sin embargo, los métodos existentes tienen dificultades para resolver simultáneamente tres problemas principales: 1) procesar la entrada de texturas sin desafío en unidades de tiempo, 2) modelar precisamente la evolución del espacio dinámico, y 3) mantener la estabilidad a largo plazo y la eficiencia computacional. En este sentido, presentamos el primer marco de propagación completo para convertir el streaming de texturas sin desafío de longitudes arbitrarias en una representación de Gauss Dinámica 3D (3DGS). Este marco permite reconstruir el movimiento del espacio a partir de observaciones temporalmente cercanas. Propomos dos innovaciones técnicas fundamentales: una estructura de muestreo probabilístico para la predicción de posiciones en el 3DGS desde un encoder estático, y un campo de deformación de Bilinear en el decoder dinámico, que son adecuados para modelar dinámicamente con robustez y eficiencia. A través de experimentos extendidos en marcos estáticos y dinámicos, StreamSplat muestra una excelente calidad de reconstrucción y un excelente modelado espacial dinámico en comparación con anteriores trabajos, especialmente apoyando la reconstrucción en línea de videos de longitudes arbitrarias. El código y los modelos están disponibles en https://github.com/nickwzk/StreamSplat.",
      "upvotes": 0,
      "discussionId": "684ae26ddbd21a9cc27b1184",
      "ai_summary": "StreamSplat, a fully feed-forward framework, addresses real-time 3D scene reconstruction from uncalibrated video with accurate dynamics and long-term stability.",
      "ai_keywords": [
        "3D Gaussian Splatting",
        "3DGS",
        "probabilistic sampling mechanism",
        "bidirectional deformation field",
        "online reconstruction"
      ]
    },
    "publishedAt": "2025-06-10T10:52:36.000Z",
    "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated\n  Video Streams",
    "summary": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams\nis crucial for numerous real-world applications. However, existing methods\nstruggle to jointly address three key challenges: 1) processing uncalibrated\ninputs in real time, 2) accurately modeling dynamic scene evolution, and 3)\nmaintaining long-term stability and computational efficiency. To this end, we\nintroduce StreamSplat, the first fully feed-forward framework that transforms\nuncalibrated video streams of arbitrary length into dynamic 3D Gaussian\nSplatting (3DGS) representations in an online manner, capable of recovering\nscene dynamics from temporally local observations. We propose two key technical\ninnovations: a probabilistic sampling mechanism in the static encoder for 3DGS\nposition prediction, and a bidirectional deformation field in the dynamic\ndecoder that enables robust and efficient dynamic modeling. Extensive\nexperiments on static and dynamic benchmarks demonstrate that StreamSplat\nconsistently outperforms prior works in both reconstruction quality and dynamic\nscene modeling, while uniquely supporting online reconstruction of arbitrarily\nlong video streams. Code and models are available at\nhttps://github.com/nickwzk/StreamSplat.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08862.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648058ff8c6a3b8f11f77893",
      "avatarUrl": "/avatars/043c832314a8d6713af90d7c255fc2f2.svg",
      "fullname": "Wu Zike",
      "name": "Nickwzk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]