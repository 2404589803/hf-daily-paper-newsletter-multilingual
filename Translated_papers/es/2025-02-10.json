[
  {
    "paper": {
      "id": "2502.05173",
      "authors": [
        {
          "_id": "67a97a47174028234b74f687",
          "name": "Xilin Wei",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f688",
          "user": {
            "_id": "64f033ef82c6eea604c4da8b",
            "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
            "isPro": false,
            "fullname": "Liu Xiaoran",
            "user": "LiuXR",
            "type": "user"
          },
          "name": "Xiaoran Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:59.999Z",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f689",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:50:02.011Z",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68a",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68b",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68c",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68d",
          "name": "Jian Tong",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68e",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68f",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f690",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f691",
          "name": "Xipeng Qiu",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f692",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:56:04.000Z",
      "title": "VideoRoPE: ¿Qué condiciones son adecuadas para generar un vídeo rotación posición inyección?",
      "summary": "Las posiciones de rotación (RoPE) y sus variantes se utilizan ampliamente debido a su capacidad para entender largos contextos, pero extenderlas a imágenes para comprender estructuras complejas de orden espacial-temporal que las poseen sigue siendo un desafío abierto. En este estudio, identificamos cuatro características esenciales necesarias para una aplicación efectiva de RoPE y examinamos aspectos que no han sido completamente considerados en investigaciones anteriores. En parte de nuestro análisis, proponemos el desafío de V-NIAH-D (Visuo-NIAH-D), que añade un distorsor periódico a V-NIAH. El objetivo de V-NIAH-D es demostrar que las variantes anteriores de RoPE fallan en mantener el orden temporal adecuado, lo que puede provocar errores en el distorsor. Basándonos en este análisis, proponemos VideoRoPE. VideoRoPE diseña una estructura 3D para mantener las relaciones de orden espacial-temporal. VideoRoPE utiliza una distribución de tiempo de baja frecuencia para suprimir los movimientos periódicos, una disposición de gradientes para mantener la simetría espacial, y separa el espacio de tiempo y el índice espacial utilizando un espacio de tiempo variable. VideoRoPE coincide en tareas posteriores como búsqueda de videos de largo contexto, comprensión de videos y generación de videos con respecto a las variantes anteriores de RoPE. El código está disponible en https://github.com/Wiselnn570/VideoRoPE.",
      "upvotes": 32,
      "discussionId": "67a97a4a174028234b74f707"
    },
    "publishedAt": "2025-02-09T23:03:21.947Z",
    "title": "VideoRoPE: What Makes for Good Video Rotary Position Embedding?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05173.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04507",
      "authors": [
        {
          "_id": "67a98cd1b8b21202c9004628",
          "name": "Peiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c9004629",
          "user": {
            "_id": "65416817271d3bc4d70f6745",
            "avatarUrl": "/avatars/55cc24918c62ab39540c4df813b026ef.svg",
            "isPro": false,
            "fullname": "Yongqi Chen",
            "user": "BrianChen1129",
            "type": "user"
          },
          "name": "Yongqi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:48.410Z",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462a",
          "name": "Runlong Su",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462b",
          "name": "Hangliang Ding",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462c",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462d",
          "name": "Zhenghong Liu",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462e",
          "name": "Hao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T21:17:09.000Z",
      "title": "Attención con estilo de Sliding para la generación de vídeos rápidas",
      "summary": "Los Transformers de Difusión (DiTs) utilizan la teoría de difusión 3D para lograr la generación de vídeos más avanzados, aunque con un costo computacional elevado. Durante la generación de un vídeo de 5 segundos a 720p, el tiempo computacional de difusión solo representa 945 segundos, pero ocupa 800 segundos. En este artículo, se presenta la Sliding Style Attention (STA) como solución a este problema. La STA se basa en que los scores de atención en modelos de difusión previamente entrenados se concentran principalmente dentro de ventanas 3D locales. Aplicando la atención en dominios espectrales y temporales locales, la STA reduce la ineficiencia en difusión. A diferencia de la Sliding Window Attention (SWA) basada en tokens, la STA utiliza un diseño de ventanas de deslizamiento para reconocimiento de nuevas herramientas de hardware, permitiendo una mejor eficiencia y manteniendo la representación. Mediante ajustes a nivel de canvas de reconocimiento, la STA proporciona implementaciones eficientes de atención 2D/3D, alcanzando un uso máximo de 58.79% de la capacidad de procesamiento (MFU). Además, la STA acelera FlashAttention-2 (FA2) en un rango de 2.8 a 17 veces y FlashAttention-3 (FA3) en un rango de 1.6 a 10 veces. En los DiTs de vídeo avanzados, la STA finalmente reduce el tiempo computacional de difusión de 945 segundos a 685 segundos, ejecutándose sin cambios en la calidad y sin necesidad de entrenamiento. Tras una tunelación final, el tiempo computacional de difusión es reducido a 268 segundos, con un error de difusión del modelo VBench permitido en un 0.09%.",
      "upvotes": 30,
      "discussionId": "67a98cd7b8b21202c90047c5"
    },
    "publishedAt": "2025-02-10T00:22:26.568Z",
    "title": "Fast Video Generation with Sliding Tile Attention",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63565cc56d7fcf1bedb7d347",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
      "fullname": "Zhang Peiyuan",
      "name": "PY007",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 80
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05176",
      "authors": [
        {
          "_id": "67a9889dc1fbde5146aba8b1",
          "name": "Chung-Ho Wu",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b2",
          "name": "Yang-Jung Chen",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b3",
          "name": "Ying-Huan Chen",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b4",
          "name": "Jie-Ying Lee",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b5",
          "name": "Bo-Hsu Ke",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b6",
          "name": "Chun-Wei Tuan Mu",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b7",
          "name": "Yi-Chuan Huang",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b8",
          "name": "Chin-Yang Lin",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b9",
          "user": {
            "_id": "64ae22dd1aee69ece065cdcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
            "isPro": false,
            "fullname": "Min-Hung Chen",
            "user": "cmhungsteve",
            "type": "user"
          },
          "name": "Min-Hung Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:50.370Z",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8ba",
          "name": "Yen-Yu Lin",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8bb",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:59:55.000Z",
      "title": "Aurora Fusion 360: Implementación de 360 grados sin límites basada en referencias, con adorno en áreas no visibles",
      "summary": "3D escena ingenio es crucial en diversas aplicaciones desde la realidad virtual hasta la visualización arquitectónica, pero los métodos actuales enfrentan desafíos en la consistencia de la perspectiva y la precisión geométrica en escenas sin límites de 360 grados. Presentamos un nuevo método basado en referencias que permite la eliminación de objetos de alta calidad y el relleno de defectos en escenas 3D, llamado AuraFusion360. Nuestro enfoque consiste en: (1) la generación de partes ocultas medidas, para un buen reconocimiento de la máscara, (2) un difusor de guias adaptativos, que permite un punto inicial preciso sin necesidad de entrenamiento adicional, (3) la expansión basada en SDEdit, que garantiza la consistencia de puntos de vuelta. Además, presentamos el primer dataset detallado que incluye datos reales de escenas sin límites de 360 grados, 360-USID. A través de amplios experimentos, AuraFusion360 supera significativamente los métodos actuales, manteniendo la precisión de ubicación a través de cambios de perspectiva y alcanzando una calidad visual alta. Puede ver los resultados en video y el dataset en la página del proyecto.",
      "upvotes": 18,
      "discussionId": "67a988a4c1fbde5146abaa3b"
    },
    "publishedAt": "2025-02-10T00:05:28.205Z",
    "title": "AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/KMKt5j_3UB0zDhxjSiyxI.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05176.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6459d5da3b6fafd9664807ab",
      "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
      "fullname": "Yu-Lun Liu",
      "name": "yulunliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04896",
      "authors": [
        {
          "_id": "67a983ea9b72585dd12587fb",
          "user": {
            "_id": "6412a33900634c4fe9873652",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
            "isPro": false,
            "fullname": "Shoufa Chen",
            "user": "ShoufaChen",
            "type": "user"
          },
          "name": "Shoufa Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:52.136Z",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd12587fc",
          "name": "Chongjian Ge",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd12587fd",
          "name": "Yuqi Zhang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd12587fe",
          "name": "Yida Zhang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd12587ff",
          "name": "Fengda Zhu",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258800",
          "name": "Hao Yang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258801",
          "name": "Hongxiang Hao",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258802",
          "name": "Hui Wu",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258803",
          "name": "Zhichao Lai",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258804",
          "name": "Yifei Hu",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258805",
          "name": "Ting-Che Lin",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258806",
          "name": "Shilong Zhang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258807",
          "name": "Fu Li",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258808",
          "name": "Chuan Li",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258809",
          "name": "Xing Wang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880a",
          "name": "Yanghua Peng",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880b",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880c",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880d",
          "name": "Yi Jiang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880e",
          "name": "Zehuan Yuan",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880f",
          "name": "Bingyue Peng",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258810",
          "name": "Xiaobing Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T13:03:55.000Z",
      "title": "悟空: Modelo de Base de Piso para la Generación de Videos Basado en la Fundación",
      "summary": "En este artículo se presenta la introducción a la última arquitectura de procesamiento de imágenes y videos, Goku. Goku es una serie de modelos que utilizan transformadores de forma normalizada para generar imágenes y videos de manera simultánea, y es el líder en la industria por su rendimiento superior. Se detallan los elementos fundamentales que permiten la generación de visualizaciones de alta calidad, incluyendo el sistema de carga de datos, la diseño de la arquitectura del modelo, la formalización de las formas y la infraestructura eficiente y potente para entrenamiento a gran escala. Los modelos de Goku muestran el mejor rendimiento tanto cualitativo como cuantitativo, estableciendo nuevos estándares de prueba en todas las principales tareas. En particular, se alcanzó un 0.76 en GenEval, 83.65 en DPG-Bench y 84.85 en VBench. Confiamos en que esta investigación proporcione consejos valiosos y desarrollos prácticos para el desarrollo de modelos de generación de imágenes y videos en el ámbito de los puntos de venta.",
      "upvotes": 14,
      "discussionId": "67a983ee9b72585dd125890f"
    },
    "publishedAt": "2025-02-09T23:43:39.239Z",
    "title": "Goku: Flow Based Video Generative Foundation Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04896.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05003",
      "authors": [
        {
          "_id": "67a9b1a69a99341e859c488d",
          "user": {
            "_id": "623753b5eddd7763adc9346a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
            "isPro": false,
            "fullname": "Andrei Panferov",
            "user": "BlackSamorez",
            "type": "user"
          },
          "name": "Andrei Panferov",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-10T08:09:18.686Z",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c488e",
          "name": "Jiale Chen",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c488f",
          "user": {
            "_id": "632a2e325f2ff1958c0103be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632a2e325f2ff1958c0103be/Tb0ql9e4LcaFktTK1hzqe.jpeg",
            "isPro": false,
            "fullname": "Soroush Tabesh",
            "user": "soroushtabesh",
            "type": "user"
          },
          "name": "Soroush Tabesh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:37.573Z",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c4890",
          "name": "Roberto L. Castro",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c4891",
          "user": {
            "_id": "6526b8ebba9a8279c139616b",
            "avatarUrl": "/avatars/09f6b677603a03be128996a0765233e6.svg",
            "isPro": false,
            "fullname": "Mahdi Nikdan",
            "user": "mnikdan97",
            "type": "user"
          },
          "name": "Mahdi Nikdan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:50:25.944Z",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c4892",
          "user": {
            "_id": "64ef52c2718f94ae8e78a5e7",
            "avatarUrl": "/avatars/d169f4ee62786a3eb4a3fa9d1fec52e9.svg",
            "isPro": false,
            "fullname": "Alistarh",
            "user": "d-alistarh",
            "type": "user"
          },
          "name": "Dan Alistarh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:35.449Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T15:23:34.000Z",
      "title": "¿Qué es la pesa de un bit y cómo se utiliza para regular la entrenamiento de un modelo de lenguaje de red neuronal (LLM) para mejorar su estabilidad?",
      "summary": "Una de las grandes reducciones de costo en un grande modelo de lenguaje (LLMs) es el uso de cuantización o representación esparsa durante el entrenamiento o implementación. Los métodos de compresión después del entrenamiento son muy populares. Sin embargo, el problema de entrenar directamente estas representaciones para obtener modelos de compresión más precisos sigue siendo abierto. Por ejemplo, recientes investigaciones (arXiv:2411.04330v2) muestran que el \"óptimo\" ancho de bits para entrenar un modelo con QAT (Entrenamiento Conozciente de la Cuantización) es el uso de pesos y activaciones de 8 bits, comparado con la precisión estándar de FP16/BF16.\n\nPara alcanzar esta mejora, utilizamos un nuevo método llamado QuEST. Este método proporciona una precisión mejorada en comparación con FP16, permite reducir el tamaño del modelo y puede usar pesos y activaciones de 4 bits o menos. Además, QuEST permite entrenar un modelo estable usando pesos y activaciones de 1 bit. QuEST mejora dos aspectos esenciales de los métodos de QAT: (1) la cuantización de distribuciones precisas y rápidas mediante la normalización de Hadamard y la optimización de MSE, y (2) la evaluación de gradientes sin ruido durante la cuantización y la minimización explícita del error de la \"precisión total\" (aunque desconocida). Los experimentos con la arquitectura de tipo Llama demuestran que QuEST puede ejecutar modelos de manera eficiente, lo que también puede ser extendido a representaciones esparsas. El apoyo de teclados de GPU muestra que los modelos generados con QuEST pueden ser ejecutados de manera eficiente, y el código está disponible en https://github.com/IST-DASLab/QuEST.",
      "upvotes": 11,
      "discussionId": "67a9b1a79a99341e859c48c7"
    },
    "publishedAt": "2025-02-10T03:00:12.065Z",
    "title": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05003.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ef52c2718f94ae8e78a5e7",
      "avatarUrl": "/avatars/d169f4ee62786a3eb4a3fa9d1fec52e9.svg",
      "fullname": "Alistarh",
      "name": "d-alistarh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05163",
      "authors": [
        {
          "_id": "67a9604851169a582d14c113",
          "user": {
            "_id": "642f4c789b2484d7d8551a93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
            "isPro": true,
            "fullname": "Yihe Deng",
            "user": "ydeng9",
            "type": "user"
          },
          "name": "Yihe Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:50:06.136Z",
          "hidden": false
        },
        {
          "_id": "67a9604851169a582d14c114",
          "name": "Yu Yang",
          "hidden": false
        },
        {
          "_id": "67a9604851169a582d14c115",
          "name": "Junkai Zhang",
          "hidden": false
        },
        {
          "_id": "67a9604851169a582d14c116",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67a9604851169a582d14c117",
          "name": "Bo Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:45:03.000Z",
      "title": "Dualogue: Framework de conducción de vehículos RL para protección de IA multilingüe de dos usuarios",
      "summary": "El rápido desarrollo de los modelos de lenguaje grande (LLMs) ha aumentado la necesidad de modelos de responsabilidad en su uso. En particular, la importancia de esta responsabilidad se ha incrementado en el campo de la detección de contenidos inseguros y ilegales. Se sabe que hay una abundancia de datos seguros en inglés. Sin embargo, la escasez de datos de seguridad abiertos en otros idiomas ha impedido que la investigación en modelos de responsabilidad multilingüe no haya sido realizada. Para llenar esta brecha, proponemos un nuevo marco de aprendizaje por refuerzo (RL) con dos jugadores. En este marco, el generador y el modelo de responsabilidad se evolucionan de manera competitiva, con el objetivo de generar datos sintéticos de alta calidad para el entrenamiento de responsabilidad multilingüe. Esta interacción se regulariza teóricamente y se ha demostrado que alcanza un equilibrio nash. En la evaluación experimental, nuestro modelo \\ours supera los modelos más avanzados y logra un mejoramiento del 10% en comparación con LlamaGuard3 (8B), además de ser 4.5 veces más rápido y tener un tamaño de 0.5B. En tareas de seguridad multilingüe, especialmente en datos reales recopilados, nuestro modelo resuelve la desigualdad en lenguajes ricos en recursos. Los estudios de impacto destacan la importancia de la generación de datos sintéticos para especificar la desigualdad en los datos abiertos. Estos hallazgos sugieren una aproximación escalable y eficiente para la generación de datos sintéticos, asociándose con la mejora de la seguridad de los LLMs a través de modelos de responsabilidad multilingüe. El código, el modelo y los datos están disponibles en open source en https://github.com/yihedeng9/DuoGuard.",
      "upvotes": 11,
      "discussionId": "67a9604951169a582d14c14d"
    },
    "publishedAt": "2025-02-10T00:43:32.191Z",
    "title": "DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05163.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642f4c789b2484d7d8551a93",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
      "fullname": "Yihe Deng",
      "name": "ydeng9",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05171",
      "authors": [
        {
          "_id": "67a97e27495b23306cd5ea56",
          "name": "Jonas Geiping",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea57",
          "name": "Sean McLeish",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea58",
          "name": "Neel Jain",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea59",
          "name": "John Kirchenbauer",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5a",
          "name": "Siddharth Singh",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5b",
          "name": "Brian R. Bartoldson",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5c",
          "name": "Bhavya Kailkhura",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5d",
          "name": "Abhinav Bhatele",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5e",
          "name": "Tom Goldstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:55:02.000Z",
      "title": "La metodología de escalado en tiempos de prueba de computación, realizada mediante la teoría de la red de redes, utiliza el enfoque de acceso a redes convolucionales.",
      "summary": "Estamos investigando una nueva arquitectura de modelos de lenguaje que permite ocultar la computación durante el proceso de evaluación. Nuestro modelo funciona iterando bloques recursivos y puede expandirse a una profundidad arbitraria durante el proceso de evaluación. Esto se opone al método de expansión de la computación a través de la generación de tokens, que es utilizado por los modelos de razonamiento predominantes. Nuestra metodología es diferente a la basada en cadenas de Scope, ya que no requiere datos de entrenamiento especializados, funciona con pequeños ventanas de contexto y permite identificar diferentes tipos de razones que son difíciles de expresar en lenguaje. Hemos expandido nuestros modelos de prueba a 350 millones de parámetros y 800 billones de tokens. Hemos comprobado que estos modelos pueden mejorar el rendimiento en los benchmarks de razonamiento, a veces de manera sorprendente, demostrando una carga de cálculo equivalente a 500 millones de parámetros.",
      "upvotes": 7,
      "discussionId": "67a97e29495b23306cd5eae5"
    },
    "publishedAt": "2025-02-09T23:19:16.714Z",
    "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05171.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04403",
      "authors": [
        {
          "_id": "67a97c7542d4d2f92ee57d20",
          "name": "David Abel",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d21",
          "name": "André Barreto",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d22",
          "name": "Michael Bowling",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d23",
          "name": "Will Dabney",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d24",
          "name": "Shi Dong",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d25",
          "name": "Steven Hansen",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d26",
          "name": "Anna Harutyunyan",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d27",
          "name": "Khimya Khetarpal",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d28",
          "name": "Clare Lyle",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d29",
          "name": "Razvan Pascanu",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2a",
          "name": "Georgios Piliouras",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2b",
          "name": "Doina Precup",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2c",
          "name": "Jonathan Richens",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2d",
          "name": "Mark Rowland",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2e",
          "name": "Tom Schaul",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2f",
          "name": "Satinder Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T08:34:57.000Z",
      "title": "Agency Is Frame-Dependent\n\nLa Agencia es dependiente del marco.",
      "summary": "¡Hola! A continuación, se presenta la traducción del texto proporcionado del inglés al español, manteniendo la profundidad y precisión:\n\n\"¡Hola! Agencia (Agency) es un sistema que posee la capacidad de controlar los resultados relacionados con el propósito, lo cual lo convierte en uno de los temas centrales de investigación en las áreas de la biología, filosofía, ciencias cognitivas y inteligencia artificial. Determinar si una entidad tiene Agencia es una cuestión muy compleja, y Dennett (1989) plantea el problema de descubrir si piedras, cibernéticos y robots tienen Agencia. Para abordar este problema, nos enfocamos desde la perspectiva de aprendizaje por refuerzo (Reinforcement Learning). Agencia es fundamentalmente dependiente de un marco, por lo que para medir la Agencia de un sistema, es necesario medir su dependencia del marco. Para apoyar esta posición, Banderara (2009) y Moreno (2018) proporcionan una demostración lógica de que las características básicas de la Agencia tienen una dependencia del marco. La teoría básica de la Agencia concluye que depende de un marco, y se discuten los efectos de este enfoque en el aprendizaje por refuerzo.\"",
      "upvotes": 7,
      "discussionId": "67a97c7642d4d2f92ee57d77"
    },
    "publishedAt": "2025-02-09T23:11:57.959Z",
    "title": "Agency Is Frame-Dependent",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04403.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04520",
      "authors": [
        {
          "_id": "67a97eea96d822bc6e13a1bb",
          "name": "Letian Peng",
          "hidden": false
        },
        {
          "_id": "67a97eea96d822bc6e13a1bc",
          "name": "Chenyang An",
          "hidden": false
        },
        {
          "_id": "67a97eea96d822bc6e13a1bd",
          "name": "Shibo Hao",
          "hidden": false
        },
        {
          "_id": "67a97eea96d822bc6e13a1be",
          "name": "Chengyu Dong",
          "hidden": false
        },
        {
          "_id": "67a97eea96d822bc6e13a1bf",
          "user": {
            "_id": "660655119e3555d648f6c6b5",
            "avatarUrl": "/avatars/ae1e2c97a08be39b77a9f1a5c2a718ef.svg",
            "isPro": false,
            "fullname": "Jingbo Shang",
            "user": "shangjingbo",
            "type": "user"
          },
          "name": "Jingbo Shang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:54.200Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T21:44:30.000Z",
      "title": "La amplitud de la composición de LM y la relación rectilínea con la fantasía",
      "summary": "El modelo de lenguaje (LM) de generalización está en discusión activa y compara la posibilidad de inteligencia generalizada con el desgaste de la estructura básica de conocimiento (por ejemplo, el enigma de la inversión/conversión). Este artículo revela el fenómeno de la correlación lineal en los constituyentes de la estructura de conocimiento de un LM. En particular, la transformación lineal entre conocimientos asociados mapea la lógica de predicción de próximo token en los prompts. Por ejemplo, \"X vive en la ciudad de\" → \"X vive en el país de\". Esto imita la linearidad de la constitución del conocimiento humano. Lo que hemos descubierto es que la relación positiva y realista con el ajuste micrográfico en el caso de un LM generalizado se alinea con la actualización de conocimiento, mientras que el caso que se desvía llama la atención hacia errores. Los resultados experimentales muestran que la correlación lineal puede desempeñar un papel potencial en la identificación de la generalización de un LM. Finalmente, esta correlación lineal puede ser aprendida en una red retropropagatoria unidireccional y con representaciones de palabras preentrenadas, lo que demuestra su gran dependencia en la generalización de un LM.",
      "upvotes": 6,
      "discussionId": "67a97eea96d822bc6e13a1e7"
    },
    "publishedAt": "2025-02-09T23:22:06.784Z",
    "title": "Linear Correlation in LM's Compositional Generalization and Hallucination",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04520.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05179",
      "authors": [
        {
          "_id": "67a9901cc0310368e2488929",
          "name": "Shilong Zhang",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892a",
          "name": "Wenbo Li",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892b",
          "user": {
            "_id": "6412a33900634c4fe9873652",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
            "isPro": false,
            "fullname": "Shoufa Chen",
            "user": "ShoufaChen",
            "type": "user"
          },
          "name": "Shoufa Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:46.264Z",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892c",
          "name": "Chongjian Ge",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892d",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892e",
          "name": "Yida Zhang",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892f",
          "name": "Yi Jiang",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e2488930",
          "name": "Zehuan Yuan",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e2488931",
          "name": "Binyue Peng",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e2488932",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:59:59.000Z",
      "title": "FlashVideo: Fusion de la Fidelidad de Pixeles y la Fidelidad de Detalle para la Generación de Vídeos de Alta Resolución Eficiente",
      "summary": "DiT difusión modelos han logrado un gran éxito en la generación de animaciones desde el texto, utilizando la escalabilidad de la capacidad del modelo y el tamaño de los datos. Sin embargo, para implementar contenido de alta calidad y precisión de acciones correspondientes a los textos, se requieren grandes parámetros del modelo y una gran cantidad de evaluaciones funcionales (NFEs). Los detalles realistas y visualmente apreciables generalmente se reflejan en un alto rendimiento de salida, lo que particularmente incrementa la carga computacional en un modelo de DiT en un solo paso. Para resolver estos problemas, proponemos un nuevo marco de trabajo de dos etapas llamado FlashVideo. FlashVideo se centra en la distribución estratégica de la capacidad del modelo y los NFEs dentro de cada frame para equilibrar la precisión y calidad de la generación. En la primera etapa, utilizamos un proceso de generación de baja resolución para mejorar la eficiencia computacional utilizando grandes parámetros y suficientes NFEs, priorizando la precisión del pronóstico. En la segunda etapa, establecemos un ajuste de flujo entre la resolución baja y alta para generar detalles con el mínimo número de NFEs. Con estos resultados positivos, FlashVideo muestra que puede lograr la generación de animaciones de alta resolución más avanzada y mostrar una alta eficiencia computacional. Además, el diseño de dos etapas permite a los usuarios ver previamente una salida inicial antes de confirmar la generación en toda la resolución, reduciendo significativamente los costos de cálculo y el tiempo de espera, así como aumentando la posibilidad comercial.",
      "upvotes": 5,
      "discussionId": "67a9901ec0310368e24889c2"
    },
    "publishedAt": "2025-02-10T00:35:37.019Z",
    "title": "FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05179.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04363",
      "authors": [
        {
          "_id": "67a98180d0dc1ed664297368",
          "name": "Bosung Kim",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed664297369",
          "name": "Kyuhwan Lee",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed66429736a",
          "name": "Isu Jeong",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed66429736b",
          "name": "Jungmin Cheon",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed66429736c",
          "name": "Yeojin Lee",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed66429736d",
          "name": "Seulki Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T05:42:29.000Z",
      "title": "Sora es una tecnología que permite generar vídeos a partir de texto basada en distribución en dispositivos móviles.",
      "summary": "Se presenta Sora en el dispositivo. Es una solución líder para la generación de videos a partir de textos basada en la difusión, que funciona eficientemente en dispositivos de nivel de teléfono inteligente. Basado en Open-Sora, Sora en el dispositivo resuelve los desafíos de la generación de videos a partir de textos basada en la difusión en dispositivos móviles con limitaciones en cálculo y memoria mediante tres nuevas tecnologías. Primero, el Salto Lineal Proporcional (SLP) utiliza un enfoque eficiente de saltos para reducir los pasos de desnoise excesivos necesarios para la difusión del video. Luego, el Token Merging en Dimensiones de Tiempo (TDTM) minimiza los cálculos de procesamiento de tokens continuos en las capas de atención, mergiendo los tokens continuos según la dimensión del tiempo. Además, la Inferencia Paralela y Carga Dinámica (CI-DL) divide grandes modelos en pequeños bloques que se cargan dinamicamente en la memoria y se inferen en paralelo, resolviendo efectivamente las limitaciones de la memoria del dispositivo. Se implementó Sora en el dispositivo en el iPhone 15 Pro, y según la evaluación experimental, puede generar videos de alta calidad y produce resultados similares a los de Open-Sora ejecutado en un GPU de alto rendimiento. Estos resultados muestran que Sora en el dispositivo permite la generación de videos de alta calidad y baja calidad de recursos en dispositivos móviles limitados, expandiendo el acceso, protegiendo los derechos de los usuarios, reduciendo la dependencia de la infraestructura de nube y disminuyendo los costos asociados. La propuesta de Sora en el dispositivo es considerada una importante primera etapa para la democratización de las tecnologías de creación líder, permitiendo a dispositivos móviles y incorporados la funcionalidad de generación de videos. La implementación de código está publicada en el repositorio GitHub: https://github.com/eai-lab/On-device-Sora.",
      "upvotes": 3,
      "discussionId": "67a98185d0dc1ed664297491"
    },
    "publishedAt": "2025-02-09T23:33:13.185Z",
    "title": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04363.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04728",
      "authors": [
        {
          "_id": "67a97d1c02da0cdf059cb0d8",
          "name": "Zhouliang Yu",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0d9",
          "name": "Yuhuan Yuan",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0da",
          "name": "Tim Z. Xiao",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0db",
          "name": "Fuxiang Frank Xia",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0dc",
          "name": "Jie Fu",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0dd",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:56.250Z",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0de",
          "name": "Ge Lin",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0df",
          "name": "Weiyang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T07:52:25.000Z",
      "title": "Programación de tiempo de pruebas para la generación de un modelo mundial de símbolos a través de grandes modelos de lenguaje",
      "summary": "Para resolver problemas complejos de planificación, los modelos de lenguaje grande (LLMs) necesitan modelar claramente las transformaciones de estado, evitar violaciones de reglas, seguir restricciones y garantizar la óptimalidad. Estos desafíos son complicados por la naturaleza inherentemente incierta del lenguaje natural. Para superar esta incertidumbre, el lenguaje de lenguaje de dominio de planificación (PDDL) se utiliza para abstractar la planificación y proporciona una descripción formal precisa del estado. Con PDDL, podemos crear modelos de mundo simbólico y aplicar algoritmos de búsqueda clásicos como A* para encontrar planes óptimos. Sin embargo, los actuales LLMs no pueden generar directamente áreas de PDDL debido a la escasez de datos de entrenamiento de PDDL para problemas abiertos. Para resolver este problema, expandimos los cálculos de los LLMs durante el test para mejorar su capacidad de razonamiento en PDDL y facilitar la generación de áreas de PDDL de alta calidad. En particular, introducimos algoritmos sencillos y efectivos que utilizan un enfoque de selección de la mejor de N para mejorar la calidad de las soluciones iniciales y refinar las soluciones con aprendizaje automático. Nuestro método supera significativamente a o1-mini en ambas tareas: generación de áreas de PDDL a partir de descripciones naturales y generación de áreas de PDDL a partir de problemas de PDDL, alcanzando un rendimiento de más del 50% sin necesidad de entrenamiento adicional. PDDL se utiliza en nuestro método para la abstracción de estado. Nuestro método puede superar la mejor técnica en casi todos los niveles de competencia de tareas de planificación actuales.",
      "upvotes": 3,
      "discussionId": "67a97d1d02da0cdf059cb11a"
    },
    "publishedAt": "2025-02-09T23:17:42.258Z",
    "title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04728.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04404",
      "authors": [
        {
          "_id": "67a97bc5500b3bcf5babc5e8",
          "user": {
            "_id": "64bb3d1eb1a618880956da76",
            "avatarUrl": "/avatars/ec393b5eee8a3ccec61107b4aa63c4d9.svg",
            "isPro": false,
            "fullname": "Xiao-Wen Yang",
            "user": "yangxw",
            "type": "user"
          },
          "name": "Xiao-Wen Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:57.842Z",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5e9",
          "name": "Xuan-Yi Zhu",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ea",
          "name": "Wen-Da Wei",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5eb",
          "name": "Ding-Chu Zhang",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ec",
          "name": "Jie-Jing Shao",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ed",
          "name": "Zhi Zhou",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ee",
          "name": "Lan-Zhe Guo",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ef",
          "name": "Yu-Feng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T08:52:43.000Z",
      "title": "StepBack Forward: Mejora de la Inferencia de Modelos de Lenguaje por Tracking Automático de Reverso",
      "summary": "La integración de la estructura de Swapping en LLMs puede parecer un sistema similar a OpenAI's o1, ofreciendo una ruta deseable para alcanzar los Reasoners de nivel 2 AGI. Sin embargo, siguen existiendo varios problemas importantes. Uno de ellos es el sobre-oversampling y la dependencia excesiva con modelos de control asistido. Estamos convencidos de que estas limitaciones son causadas porque los LLMs no pueden internalizar el proceso de exploración. Una etapa crucial para resolver estos problemas es permitir a los LLMs decidir automáticamente el tiempo y lugar para realizar retrocesos. En este sentido, proponemos una estructura de retrocesos automáticos que permita a los LLMs retroceder tanto en el aprendizaje como en la inferencia. Esta estructura transforma el proceso de Swapping en un Swapping rápido mediante mejoras automáticas, mejorando tanto la capacidad lógica como la eficiencia. Según evaluaciones experimentales, nuestra propuesta logra un aumento del rendimiento del 40% más comparado con métodos de ajuste de subconjuntos de rutas óptimas. Creemos que esta investigación introduce nuevas metodologías para el desarrollo de potentes Reasoners.",
      "upvotes": 2,
      "discussionId": "67a97bc7500b3bcf5babc64e"
    },
    "publishedAt": "2025-02-09T23:09:01.160Z",
    "title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04404.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04350",
      "authors": [
        {
          "_id": "67a97a77d163c9e6ea2bdb85",
          "name": "Yongchao Chen",
          "hidden": false
        },
        {
          "_id": "67a97a77d163c9e6ea2bdb86",
          "name": "Yilun Hao",
          "hidden": false
        },
        {
          "_id": "67a97a77d163c9e6ea2bdb87",
          "name": "Yueying Liu",
          "hidden": false
        },
        {
          "_id": "67a97a77d163c9e6ea2bdb88",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "67a97a77d163c9e6ea2bdb89",
          "name": "Chuchu Fan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T15:53:59.000Z",
      "title": "CodeSteer: Modelo de lenguaje de símbolos mediante lineamientos de código/texto",
      "summary": "Los métodos actuales no pueden controlar eficazmente la lógica de lenguaje y la generación de código en modelos de lenguaje grandes (LLMs), lo que ha llevado a un uso excesivo de la capacidad de cálculo de símbolos. Presentamos CodeSteer, una metodología efectiva para guiar la generación de código y texto por parte de un LLM. Hemos construido un marco de referencia detallado llamado SymBench, que incluye 37 tareas símbolicas, y hemos sintetizado 12k trazas de guiado/generación multinivel y 5.5k pasos de comparación de guiados. Usamos un entrenamiento de supervisión de subproyectos multinivel (SFT) y optimización directa de interés (DPO) para finejar el modelo Llama-3-8B. El modelo resultante, CodeSteerLLM, puede guiar eficazmente la generación de código/texto de grandes modelos añadiendo nuevos chequeadores de símbolos y respuestas automáticas. La adición de CodeSteer a GPT-4o ha aumentado su puntuación promedio de 53.3 a 86.4, superando a los mejores LLMs actuales como OpenAI o1 (82.7), o1-preview (74.8) y DeepSeek R1 (76.8) en todos los 37 tareas (28 confirmadas, 9 no confirmadas). El entrenamiento de CodeSteer ha demostrado un aumento promedio de 41.8 en el rendimiento de GPT-4o frente a Claude, Mistral y GPT-3.5. Los modelos guiados por CodeSteer mantienen una alta eficiencia en tareas complejas y utilizan completamente la capacidad de cálculo de símbolos. Los modelos, conjuntos de datos y código están disponibles en la siguiente URL: https://github.com/yongchao98/CodeSteer-v1.0.",
      "upvotes": 2,
      "discussionId": "67a97a79d163c9e6ea2bdc0c"
    },
    "publishedAt": "2025-02-09T23:03:14.294Z",
    "title": "CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04350.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04689",
      "authors": [
        {
          "_id": "67a9b911b1f5eece682d7961",
          "user": {
            "_id": "64510a21f800611f94f0d9f8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lOeHK9Bvt3IXcB7Urx6jZ.jpeg",
            "isPro": false,
            "fullname": "Yuwei Yin",
            "user": "yuweiyin",
            "type": "user"
          },
          "name": "Yuwei Yin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:32.672Z",
          "hidden": false
        },
        {
          "_id": "67a9b911b1f5eece682d7962",
          "name": "Giuseppe Carenini",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T06:30:33.000Z",
      "title": "ARR: Análisis de Respuestas a Preguntas con Modelos de Lenguaje de Gran Escala, Búsqueda y Lógica de Argumentos",
      "summary": "Los modelos de lenguaje grande (LLMs) alcanzan resultados impresionantes en marcadores estructurados difíciles como tareas de respuesta a preguntas (QA). El Zero-shot Chain-of-Thought (CoT) pruning fortalece la lógica de los LLMs, pero está limitado a \"pensar uno a uno\" y a guías generales. En este artículo, se presenta una metodología zero-shot eficaz y intuitiva llamada ARR para resolver tareas de QA. Esta metodología se basa en tres etapas clave: análisis del objetivo de la pregunta, búsqueda de información relevante y presentación de razones en etapas. Extensos experimentos en tareas de QA difíciles muestran que ARR mejora consistentemente un Baseline (sin incluir el ARR) y supera a CoT. Experimentos controlados y estudios de casos demuestran una vez más la contribución positiva de cada componente: análisis, búsqueda y razones. En particular, el análisis del objetivo es crucial en ARR. Además, evaluaciones en una amplia gama de tamaños de modelo, series de LLMs y configuraciones de generación demuestran la efectividad, robustez y capacidad de generalización de ARR.",
      "upvotes": 1,
      "discussionId": "67a9b911b1f5eece682d798c"
    },
    "publishedAt": "2025-02-10T03:30:51.974Z",
    "title": "ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04689.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64510a21f800611f94f0d9f8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lOeHK9Bvt3IXcB7Urx6jZ.jpeg",
      "fullname": "Yuwei Yin",
      "name": "yuweiyin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.03512",
      "authors": [
        {
          "_id": "67a9a7cb6be3ca4a7ede471e",
          "name": "Amitava Das",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede471f",
          "name": "Yaswanth Narsupalli",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4720",
          "name": "Gurpreet Singh",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4721",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4722",
          "name": "Vasu Sharma",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4723",
          "name": "Suranjana Trivedy",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4724",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:39.550Z",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4725",
          "name": "Amit Sheth",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T18:46:20.000Z",
      "title": "InversoAlign: Marco de Desafío y Optimización Multiobjetivo Basada DPO",
      "summary": "En un sistema T2I, es crucial que las imágenes generadas comprendan exactamente el propósito del usuario y se ajusten a estrictas normas éticas y estéticas. Por ejemplo, como ocurrió con Google's DeepMind, cuando se produjeron resultados inapropiados que provocaron una fuerte reacción pública, se ha destacado la necesidad de un estructura de respuesta robusta. En contraste, los modelos de lenguaje de gran escala (LLMs) han tenido éxito significativo en estas responsabilidades. Basándose en estas avances, los investigadores buscan aplicar técnicas como la Direct Preference Optimization (DPO) en sistemas T2I para mejorar la precisión y confiabilidad de la generación de imágenes.\n\nProponemos YinYangAlign, un avanzado marco de referencia de evaluación. Este marco tiene como objetivo diseñar un sistema que cuantifique de manera sistemática la precisión de la respuesta de un T2I, abordando seis problemas básicos y únicos. Cada par representa una base fundamental de la generación de imágenes y, por ejemplo, equilibrar el comportamiento según el prompt del usuario con la creatividad, o mantener diversidad y coherencia visual. YinYangAlign incluye un conjunto de datos detallados que contienen el prompt humano, la salida de la respuesta seleccionada (elección), la salida de la generación de la IA inapropiada (rechazo), y una explicación posterior de las contradicciones.",
      "upvotes": 1,
      "discussionId": "67a9a7cf6be3ca4a7ede47d5"
    },
    "publishedAt": "2025-02-10T02:21:52.370Z",
    "title": "YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03512.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05178",
      "authors": [
        {
          "_id": "67a99dfe98423dca45d8f659",
          "user": {
            "_id": "638fe91639f7e2a7f9d2a8c6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
            "isPro": false,
            "fullname": "Yue Zhao",
            "user": "zhaoyue-zephyrus",
            "type": "user"
          },
          "name": "Yue Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:43.493Z",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65a",
          "name": "Fuzhao Xue",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65b",
          "name": "Scott Reed",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65c",
          "name": "Linxi Fan",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65d",
          "name": "Yuke Zhu",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65e",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65f",
          "name": "Zhiding Yu",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f660",
          "name": "Philipp Krähenbühl",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f661",
          "name": "De-An Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:59:57.000Z",
      "title": "QLIP: La tokenización visual de correspondencias de texto integra la comprensión multimodal deductivo y la generación automática.",
      "summary": "QLIP (Preentrenamiento de Lenguaje y Imágenes Cuantizado) se presenta. QLIP es un método de tokenización visual que integra la mejor calidad de reconstrucción y la mejor comprensión de imágenes en 0-shot. QLIP entrena un autoencoder basado en la cuantificación binaria de dos capas de supervisado para el objetivo de reconstrucción y el arreglo de imágenes y lenguaje. Primero, QLIP demostró que estos dos objetivos no se conflictian. QLIP implementó un flujo de entrenamiento en dos etapas para equilibrar dinámicamente estos dos términos de pérdida, permitiendo una mezcla más efectiva de los grandes lotes de entrenamiento de un diccionario de imágenes y lenguaje con los puntos de corte de memoria debidos a los objetivos de reconstrucción, así como mejorar la eficiencia. La eficiencia de QLIP se verificó en la comprensión multimodel y la generación de documentos con imágenes. En particular, QLIP puede reemplazar el encoder visual de LLaVA y el tokenizador de imágenes de LlamaGen, mostrando un rendimiento relativamente óptimo o mejor. Finalmente, QLIP demuestra la implementación de un modelo mixto integrado para comprensión y generación.",
      "upvotes": 1,
      "discussionId": "67a99dfe98423dca45d8f691"
    },
    "publishedAt": "2025-02-10T01:35:35.818Z",
    "title": "QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05178.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638fe91639f7e2a7f9d2a8c6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
      "fullname": "Yue Zhao",
      "name": "zhaoyue-zephyrus",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.04376",
      "authors": [
        {
          "_id": "67a998fe495b23306cdbf51d",
          "name": "Lingxiang Hu",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf51e",
          "name": "Shurun Yuan",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf51f",
          "name": "Xiaoting Qin",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf520",
          "name": "Jue Zhang",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf521",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf522",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf523",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf524",
          "name": "Qi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T16:25:43.000Z",
      "title": "Representante de la reunión: Represento la normalización de los estándares de la LLM para la realización de la reunión.",
      "summary": "En la actualidad de la vida laboral, las reuniones son esenciales para el intercambio de ideas y la coordinación del equipo, pero presentan problemas como el consumo de tiempo, conflictos en la programación y la disminución de eficiencia debido a la participación inadecuada. El desarrollo reciente de los modelos de lenguaje grandes (LLMs) ha demostrado capacidades potentes en generación y inferencia de lenguaje, lo que ha llevado a la pregunta: \"¿Pueden los LLMs delegar efectivamente a los participantes de las reuniones?\" Para investigar esta cuestión, hemos desarrollado un sistema de delegación de reuniones utilizando modelos de LLM y hemos creado un marco de referencia detallado utilizando transcripciones de reuniones reales. Los resultados de la evaluación muestran que GPT-4/4o mantiene un equilibrio entre una participación activa y cuidadosa, mientras que Gemini 1.5 Pro muestra un comportamiento más cuidadoso, y Gemini 1.5 Flash y Llama3-8B/70B muestran un comportamiento más activo. En general, aproximadamente el 60% de las respuestas tratan de al menos un punto clave de la verdad. Sin embargo, es necesario reducir contenidos irrelevantes y repeticiones, así como mejorar la robustez frente a errores de copia en entornos reales. Además, hemos implementado el sistema en entornos prácticos y recopilado feedback realistas de demostraciones. Nuestros hallazgos revelan la posibilidad y los desafíos de utilizar LLMs para la delegación de reuniones, con el objetivo de proporcionar contenidos valiosos para aplicaciones prácticas que resuelvan los desafíos de las reuniones.",
      "upvotes": 1,
      "discussionId": "67a99900495b23306cdbf57e"
    },
    "publishedAt": "2025-02-10T01:15:52.070Z",
    "title": "MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662b0bc9c709a61df8291c0f",
      "avatarUrl": "/avatars/16dd4d945e9fbef5ac889a8087101ded.svg",
      "fullname": "Xiaoting Qin",
      "name": "XiaotingQin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03738",
      "authors": [
        {
          "_id": "67a8d049406cb5a65f847eb1",
          "name": "Feng Wang",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb2",
          "name": "Yaodong Yu",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb3",
          "name": "Guoyizhe Wei",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb4",
          "name": "Wei Shao",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb5",
          "name": "Yuyin Zhou",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb6",
          "name": "Alan Yuille",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb7",
          "name": "Cihang Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T03:01:38.000Z",
      "title": "En la escalación de Raz, el factor de patches: las imágenes tienen más valor que 50,176 tokens.",
      "summary": "Después de la introducción del Vision Transformer (ViT), el Patch Factory ha sido reconocido como una práctica efectiva para la tokenización de imágenes en arquitecturas visuales sencillas a largo plazo. Este método permite reducir el tamaño espacial de las imágenes, lo que reduce eficientemente los costos de cálculo en arquitecturas simples como el ViT. En este artículo, se investiga en detalle los patrones de compresión que generan pérdida de información en el Patch Factory y se analiza cuál es su impacto en la comprensión visual. A través de una amplia gama de experimentos con escalas de tamaño de patch, se descubrió las leyes de escalado de los puntos de interés en el Patch Factory. El modelo obtiene beneficios consistentes al reducir el tamaño del patch, aumentando la precisión de predicción, pero al alcanzar el tamaño mínimo de 1x1, se produce la tokenización de píxeles. Esta conclusión puede ser ampliamente aplicada en diversas tareas visuales, escalas de entrada, arquitecturas como el ViT y Mamba. Además, cuando el tamaño del patch disminuye, la importancia de las cabezas de decodificador para ciertas tareas se reduce en relación con la predicción densa. En los experimentos, se expandió la extensión de secuencias de imágenes hasta 50,176 tokens, y el modelo de tamaño base alcanzó una precisión de prueba competitiva de 84.6% en el benchmark de ImageNet-1k. Este estudio ofrece feedback y una base teórica para la construcción de modelos visuales no compresivos en el futuro. El código está disponible en https://github.com/wangf3014/Patch_Scaling.",
      "upvotes": 0,
      "discussionId": "67a8d04a406cb5a65f847ed3"
    },
    "publishedAt": "2025-02-10T02:34:31.480Z",
    "title": "Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03738.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 754
    },
    "isAuthorParticipating": false
  }
]