[
  {
    "paper": {
      "id": "2506.18095",
      "authors": [
        {
          "_id": "685a0ac20e4ad7e2197584ea",
          "name": "Junying Chen",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584eb",
          "name": "Zhenyang Cai",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ec",
          "user": {
            "_id": "675130185d873b8ed24d964a",
            "avatarUrl": "/avatars/30ee8ce21f95423db8ced7db4df3112b.svg",
            "isPro": false,
            "fullname": "Pengcheng Chen",
            "user": "cppppppc",
            "type": "user"
          },
          "name": "Pengcheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:33:07.093Z",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ed",
          "name": "Shunian Chen",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ee",
          "name": "Ke Ji",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ef",
          "name": "Xidong Wang",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584f0",
          "name": "Yunjin Yang",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584f1",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-22T16:51:09.000Z",
      "submittedOnDailyAt": "2025-06-26T02:58:42.859Z",
      "title": "ShareGPT-4o-Image: Generación de imágenes de nivel GPT-4o para la alineación de múltiples modelos",
      "submittedOnDailyBy": {
        "_id": "64097dd1b6a334f53e2b3e4c",
        "avatarUrl": "/avatars/18d036aab5e096054a8706bc78027126.svg",
        "isPro": false,
        "fullname": "Junying Chen",
        "user": "jymcc",
        "type": "user"
      },
      "summary": "El desarrollo reciente de modelos generativos multimodal ha permitido la creación de imágenes que responden a comandos, pero sistemas como GPT-4o-Image tienen limitaciones en propiedades y acceso. Para democratizar estas capacidades, se presenta ShareGPT-4o-Image. Este es el primer conjunto de datos de 45K frases sintéticas a imágenes y 46K frases y imágenes a imágenes, utilizando la capacidad de generación de imágenes de GPT-4o. Este conjunto de datos ha sido utilizado para desarrollar Janus-4o, un modelo multimodal que permite la generación de imágenes a partir de frases y de imágenes a partir de frases y imágenes. Janus-4o mejora significativamente la generación de imágenes a partir de frases en comparación con Janus-Pro, y también introduce la generación de imágenes a partir de frases y imágenes. En particular, ha logrado excelentes resultados en la generación de imágenes a partir de frases y imágenes, demostrando su efectividad en escenarios de escritura y visión. Se ha desarrollado utilizando 91K muestras sintéticas y 6 horas de entrenamiento en una máquina con GPU 8A800. La publicación de ShareGPT-4o-Image y Janus-4o busca fomentar la investigación abierta en la generación de imágenes realistas y respondientes a comandos.",
      "upvotes": 43,
      "discussionId": "685a0ac30e4ad7e2197584f2",
      "githubRepo": "https://github.com/FreedomIntelligence/ShareGPT-4o-Image",
      "ai_summary": "ShareGPT-4o-Image and Janus-4o enable open research in photorealistic, instruction-aligned image generation through a large dataset and multimodal model.",
      "ai_keywords": [
        "multimodal generative models",
        "text-to-image",
        "text-and-image-to-image",
        "photorealistic",
        "instruction-aligned",
        "dataset",
        "large language model",
        "synthetic samples"
      ],
      "githubStars": 63
    },
    "publishedAt": "2025-06-22T12:51:09.000Z",
    "title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image\n  Generation",
    "summary": "Recent advances in multimodal generative models have unlocked photorealistic,\ninstruction-aligned image generation, yet leading systems like GPT-4o-Image\nremain proprietary and inaccessible. To democratize these capabilities, we\npresent ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and\n46K text-and-image-to-image data, all synthesized using GPT-4o's image\ngeneration capabilities for distilling its advanced image generation abilities.\nLeveraging this dataset, we develop Janus-4o, a multimodal large language model\ncapable of both text-to-image and text-and-image-to-image generation. Janus-4o\nnot only significantly improves text-to-image generation over its predecessor,\nJanus-Pro, but also newly supports text-and-image-to-image generation. Notably,\nit achieves impressive performance in text-and-image-to-image generation from\nscratch, using only 91K synthetic samples and 6 hours of training on an 8\nA800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will\nfoster open research in photorealistic, instruction-aligned image generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18095.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64097dd1b6a334f53e2b3e4c",
      "avatarUrl": "/avatars/18d036aab5e096054a8706bc78027126.svg",
      "fullname": "Junying Chen",
      "name": "jymcc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19697",
      "authors": [
        {
          "_id": "685c1546df8a0d6c70bbf94e",
          "user": {
            "_id": "60f8435644e75317cc02ed51",
            "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
            "isPro": false,
            "fullname": "Jungwoo Park",
            "user": "affjljoo3581",
            "type": "user"
          },
          "name": "Jungwoo Park",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:32:38.469Z",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf94f",
          "name": "Taewhoo Lee",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf950",
          "name": "Chanwoong Yoon",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf951",
          "name": "Hyeon Hwang",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf952",
          "name": "Jaewoo Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T15:03:57.000Z",
      "submittedOnDailyAt": "2025-06-26T02:17:13.263Z",
      "title": "Prueba de Entrevista Externa para un Modelo de Lenguaje de Grandes Escalas con una Expresión Fija de 4 Bits para Decimales\n\n(Note: The original text \"アウトライアーサフェースの予ち練習を用いた大規模な言語モデルの強固な4ビット定数化\" was translated to Korean as \"외부면접 연습을 활용한 강력한 4비트 고정소수점 표현의 대규모 언어 모델\". This translation maintains the professional and accurate nature of the original text.)",
      "submittedOnDailyBy": {
        "_id": "60f8435644e75317cc02ed51",
        "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
        "isPro": false,
        "fullname": "Jungwoo Park",
        "user": "affjljoo3581",
        "type": "user"
      },
      "summary": "Los outliers de activación extremos significativamente degradan el rendimiento de calificación de los grandes modelos de lenguaje (LLMs) y obstaculizan la funcionalidad eficiente de los sistemas. Se han reconocido como causas la manipulación de canales y el ajuste de gradientes adaptativos, pero encontrar soluciones prácticas es difícil. Presentamos un guía práctico para prevenir la activación extremo mediante el entrenamiento previo seguro de los outliers (OSP). La OSP combina tres innovaciones clave: 1. El optimizador Muon optimizer elimina las bases privilegiadas mientras mantiene la eficiencia del entrenamiento. 2. Single-Scale RMSNorm evita el acceso por canales. 3. La proyección de embeddings aprendibles redistribuye los magnitud de activación en la matriz de embeddings. Demostramos el efecto de la OSP entrenando un modelo de 14 mil millones de parámetros con 100 mil millones de tokens. El modelo OSP obtuvo un promedio de 35.7 puntos en cuanto a cuantización de 4 bits, superando al modelo Adam con un puntaje de 26.5. Además, el sobrecargo de entrenamiento es de solo 2%. En particular, el modelo OSP muestra valores extremos (1818.56) comparados con cercanos a cero (0.04). Los outliers de activación extremo no son exclusivos de los LLMs, sino resultados de estrategias de entrenamiento. Hemos abierto una ruta para la funcionalidad eficiente de los LLMs. Los códigos fuentes y los puntos de chequeo de entrenamiento completos están disponibles en https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
      "upvotes": 29,
      "discussionId": "685c1546df8a0d6c70bbf953",
      "githubRepo": "https://github.com/dmis-lab/Outlier-Safe-Pre-Training",
      "ai_summary": "Outlier-Safe Pre-Training improves large language model quantization performance by preventing extreme activation outliers through innovative training techniques.",
      "ai_keywords": [
        "Muon optimizer",
        "Single-Scale RMSNorm",
        "learnable embedding projection",
        "outlier formation",
        "quantization performance",
        "LLM deployment",
        "excess kurtosis"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-06-24T11:03:57.000Z",
    "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models",
    "summary": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19697.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f8435644e75317cc02ed51",
      "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
      "fullname": "Jungwoo Park",
      "name": "affjljoo3581",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16012",
      "authors": [
        {
          "_id": "685cf7c0696820ba1f28f2ea",
          "name": "Boyu Li",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2eb",
          "name": "Siyuan He",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ec",
          "name": "Hang Xu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ed",
          "name": "Haoqi Yuan",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ee",
          "name": "Yu Zang",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ef",
          "name": "Liwei Hu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f0",
          "name": "Junpeng Yue",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f1",
          "name": "Zhenxiong Jiang",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f2",
          "name": "Pengbo Hu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f3",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:24.327Z",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f4",
          "user": {
            "_id": "63e5e3807f9730f523655c5d",
            "avatarUrl": "/avatars/3ded710049790d025e862861039d9df2.svg",
            "isPro": false,
            "fullname": "YehuiTang",
            "user": "WizardTY",
            "type": "user"
          },
          "name": "Yehui Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:21.125Z",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f5",
          "name": "Zongqing Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T04:13:36.000Z",
      "submittedOnDailyAt": "2025-06-26T06:05:04.111Z",
      "title": "DualTHOR: Plataforma de Simulación de Huérfanos de Dos Brazos para Planificaciones Interesadas en el Contexto",
      "submittedOnDailyBy": {
        "_id": "61e52be53d6dbb1da842316a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
        "isPro": false,
        "fullname": "Börje Karlsson",
        "user": "tellarin",
        "type": "user"
      },
      "summary": "El desarrollo de la capacidad de modelos AI asociados para realizar tareas de interacción complejas en escenarios de la realidad es uno de los problemas básicos de la inteligencia artificial asociada. El desarrollo reciente de plataformas de simulación ha aumentado significativamente la diversidad de tareas durante el entrenamiento de modelos de lenguaje de visión y lenguaje (VLM), pero la mayoría de estas plataformas evita la simplificación de robots y las características probabilísticas de bajo nivel, limitando así la transferencia a robots de la realidad. Para abordar estos problemas, se presenta DualTHOR, una plataforma de simulación basada en robots de doble brazo asociados y física. Esta simulación introduce contenidos que incluyen los activos de robots de la realidad, tareas de colaboración de brazos, un solutor invernadero para robots asociados y potenciales fallos debido a la ejecución física de bajo nivel. Esta simulación permite evaluar con mayor precisión la robustez y la capacidad de generalización de los VLM. Según evaluaciones distribuidas, actualmente los VLM enfrentan dificultades en la colaboración de brazos y su robustez en entornos realistas con contenido es limitada. A través de nuestra simulación, se enfatiza la importancia del desarrollo de VLM con alta capacidad para tareas asociadas. El código está disponible en https://github.com/ds199895/DualTHOR.git.",
      "upvotes": 16,
      "discussionId": "685cf7c1696820ba1f28f2f6",
      "ai_summary": "A simulator named DualTHOR for training dual-arm humanoid robots integrates real-world assets and physics to enhance the robustness and generalization of Vision Language Models.",
      "ai_keywords": [
        "embodied AI",
        "Vision Language Models",
        "VLMs",
        "physics-based simulation",
        "DualTHOR",
        "AI2-THOR",
        "dual-arm robots",
        "real-world robot assets",
        "task suite",
        "inverse kinematics solvers",
        "contingency mechanism",
        "physics-based low-level execution"
      ]
    },
    "publishedAt": "2025-06-19T00:13:36.000Z",
    "title": "DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware\n  Planning",
    "summary": "Developing embodied agents capable of performing complex interactive tasks in\nreal-world scenarios remains a fundamental challenge in embodied AI. Although\nrecent advances in simulation platforms have greatly enhanced task diversity to\ntrain embodied Vision Language Models (VLMs), most platforms rely on simplified\nrobot morphologies and bypass the stochastic nature of low-level execution,\nwhich limits their transferability to real-world robots. To address these\nissues, we present a physics-based simulation platform DualTHOR for complex\ndual-arm humanoid robots, built upon an extended version of AI2-THOR. Our\nsimulator includes real-world robot assets, a task suite for dual-arm\ncollaboration, and inverse kinematics solvers for humanoid robots. We also\nintroduce a contingency mechanism that incorporates potential failures through\nphysics-based low-level execution, bridging the gap to real-world scenarios.\nOur simulator enables a more comprehensive evaluation of the robustness and\ngeneralization of VLMs in household environments. Extensive evaluations reveal\nthat current VLMs struggle with dual-arm coordination and exhibit limited\nrobustness in realistic environments with contingencies, highlighting the\nimportance of using our simulator to develop more capable VLMs for embodied\ntasks. The code is available at https://github.com/ds199895/DualTHOR.git.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e52be53d6dbb1da842316a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
      "fullname": "Börje Karlsson",
      "name": "tellarin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18315",
      "authors": [
        {
          "_id": "685cb786696820ba1f28f286",
          "name": "Lehan He",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f287",
          "user": {
            "_id": "66c44c6826efa38bc783b07a",
            "avatarUrl": "/avatars/7a09179a2c91adc97f8db851fca37eea.svg",
            "isPro": false,
            "fullname": "Zeren Chen",
            "user": "zx55",
            "type": "user"
          },
          "name": "Zeren Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:31.152Z",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f288",
          "name": "Zhe Zhang",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f289",
          "name": "Jing Shao",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f28a",
          "name": "Xiang Gao",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f28b",
          "user": {
            "_id": "65b722dbe02a17f0f8d1cc6b",
            "avatarUrl": "/avatars/65f20601ef9b8ebfdddadd737f9153d6.svg",
            "isPro": false,
            "fullname": "Lu Sheng",
            "user": "lsheng2024",
            "type": "user"
          },
          "name": "Lu Sheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:29.105Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T06:01:12.000Z",
      "submittedOnDailyAt": "2025-06-26T06:27:20.139Z",
      "title": "Propiedades basadas en pruebas para combinar la generación y verificación de código de modelos de lenguaje de inteligencia artificial.",
      "submittedOnDailyBy": {
        "_id": "64a96a375a69e2ca889abdff",
        "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
        "isPro": false,
        "fullname": "fanhongxing",
        "user": "fanhongxing",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) se destacan en la generación de código, pero, especialmente en tareas de programación complejas, verificar que los resultados funcionalmente son correctos es un problema a largo plazo. El desarrollo dirigido por pruebas (TDD) proporciona un camino para la mejora del código, sin embargo, el efecto de este método se reduce debido a la falta de casos de prueba de alta calidad y problemas de generación automática de pruebas en los LLMs. La sesgación de las pruebas y la predicción de salidas imprecisas pueden fallar en el proceso de modificación. En este artículo, se presenta un nuevo marco de trabajo llamado Property-Generated Solver, que utiliza la Prueba de Características (PBT). Este marco no depende de pruebas basadas en ejemplos concretos de entrada y salida, por lo tanto, confirma la exactitud del código a través de características o eventos del programa. Estas características no son capaces de predecir un oráculo perfecto de pruebas, sino que son fáciles de definir y verificar, y rompen el círculo de dependencia entre pruebas y código, conocido como \"ciclo de la mariposa\". El Property-Generated Solver utiliza dos agentes basados en LLMs: un Generador especializado en la generación de código y la iteración de mejora, y un Tester que gestiona el ciclo de vida de la PBT y proporciona feedback significativo desde las violaciones de características. Estos agentes logran mejoras significativas en el paso @1, alcanzando un efecto relativo del 23.1% a 37.3% más allá de los métodos TDD existentes. Este marco ofrece una estructura fuerte para que los LLMs se orienten hacia códigos que pueden ser generalizados correctamente, utilizando la Prueba de Características como motor principal de verificación. Los resultados de experimentos distribuidos en varios marcos de referencia de generación de código muestran que el Property-Generated Solver proporciona feedback efectivo y afecta positivamente en la mejora del Generador.",
      "upvotes": 8,
      "discussionId": "685cb786696820ba1f28f28c",
      "githubRepo": "https://github.com/HeLeHanPrivate/PBTwithCodeGen",
      "ai_summary": "A novel framework using Property-Based Testing and collaborative LLM-based agents improves code generation correctness and generalization.",
      "ai_keywords": [
        "Large Language Models",
        "code generation",
        "Test-Driven Development",
        "Property-Based Testing",
        "PBT",
        "iterative refinement",
        "property violations",
        "pass@1 improvements"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-06-23T02:01:12.000Z",
    "title": "Use Property-Based Testing to Bridge LLM Code Generation and Validation",
    "summary": "Large Language Models (LLMs) excel at code generation, but ensuring their\noutputs to be functionally correct, especially in complex programming tasks, is\na persistent challenge. While traditional Test-Driven Development (TDD) offers\na path for code refinement, its efficacy with LLMs is often undermined by the\nscarcity of high-quality test cases or the pitfalls of automated test\ngeneration, including biased tests or inaccurate output predictions that can\nmisdirect the correction process. This paper introduces Property-Generated\nSolver, a novel framework that leverages Property-Based Testing (PBT) to\nvalidate high-level program properties or invariants, instead of relying on\nspecific input-output examples. These properties are often simpler to define\nand verify than directly predicting exhaustive test oracles, breaking the\n\"cycle of self-deception\" where tests might share flaws with the code they are\nmeant to validate. Property-Generated Solver employs two collaborative\nLLM-based agents: a Generator dedicated to code generation and iterative\nrefinement, and a Tester that manages the PBT life-cycle and formulate\nsemantically rich feedback from property violations. The resulting\ncomprehensive and actionable feedback then guides the Generator in its\nrefinement efforts. By establishing PBT as the core validation engine within\nthis iterative, closed-loop paradigm, Property-Generated Solver provides a\nrobust mechanism for steering LLMs towards more correct and generalizable code.\nExtensive experimental results on multiple code generation benchmarks\ndemonstrate that Property-Generated Solver achieves substantial pass@1\nimprovements, ranging from 23.1% to 37.3% relative gains over established TDD\nmethods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a96a375a69e2ca889abdff",
      "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
      "fullname": "fanhongxing",
      "name": "fanhongxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18088",
      "authors": [
        {
          "_id": "685ae8e8d2ee4fac76521d03",
          "user": {
            "_id": "65b37a9b06d8b55123ef8921",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
            "isPro": false,
            "fullname": "Tianxing Chen",
            "user": "TianxingChen",
            "type": "user"
          },
          "name": "Tianxing Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T18:13:30.491Z",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d04",
          "name": "Zanxin Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d05",
          "name": "Baijun Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d06",
          "name": "Zijian Cai",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d07",
          "name": "Yibin Liu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d08",
          "name": "Qiwei Liang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d09",
          "name": "Zixuan Li",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0a",
          "name": "Xianliang Lin",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0b",
          "name": "Yiheng Ge",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0c",
          "name": "Zhenyu Gu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0d",
          "name": "Weiliang Deng",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0e",
          "name": "Yubin Guo",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0f",
          "name": "Tian Nian",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d10",
          "name": "Xuanbing Xie",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d11",
          "name": "Qiangyu Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d12",
          "name": "Kailun Su",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d13",
          "name": "Tianling Xu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d14",
          "name": "Guodong Liu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d15",
          "name": "Mengkang Hu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d16",
          "name": "Huan-ang Gao",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d17",
          "name": "Kaixuan Wang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d18",
          "name": "Zhixuan Liang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d19",
          "name": "Yusen Qin",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1a",
          "name": "Xiaokang Yang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1b",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1c",
          "name": "Yao Mu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-22T16:26:53.000Z",
      "submittedOnDailyAt": "2025-06-26T06:13:35.593Z",
      "title": "RoboTwin 2.0: Diseño de Manipulación Robusto de Robots de Dos Manos con Generador de Datos Escalable y Randomización de Dominio Fortalecida, con Marcos de Referencia",
      "submittedOnDailyBy": {
        "_id": "65b37a9b06d8b55123ef8921",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
        "isPro": false,
        "fullname": "Tianxing Chen",
        "user": "TianxingChen",
        "type": "user"
      },
      "summary": "La síntesis de datos basada en simulación ha aparecido como un poderoso paradigma para fortalecer la manipulación de robots en la realidad. Sin embargo, los conjuntos de datos actuales de síntesis no son suficientes para manejos robustos de manos izquierda y derecha, presentando dos desafíos: (1) la falta de métodos eficientes para la generación de datos escalables para nuevas tareas y (2) la incapacidad de detectar la complejidad de la realidad compleja en entornos de simulación simplificados. Presentamos RoboTwin 2.0, un marco de trabajo de simulación escalable. Este marco de trabajo permite la generación automática y de gran escala de datos diversos y realistas, así como un protocolo de evaluación unificado para manejos de manos izquierda y derecha. Primero, construimos una biblioteca de objetos de 147 categorías y 731 instancias, cada una con etiquetas significativas y relacionadas con el comportamiento. Basándonos en esto, desarrollamos un pipeline de síntesis de datos especializado que combina modelos multimodales de lenguaje y la precisión de simulación in-line. Para mejorar la transición de simulación a la realidad, RoboTwin 2.0 aplica una domínio randomización estructurada en cinco ejes: craje, iluminación, fondo, altura de la mesa y instrucciones lingüísticas, lo que aumenta la diversidad de los datos y la robustez de las políticas. Este marco de trabajo es capaz de implementar tareas de manos izquierda y derecha en un rango de 50 tareas, superando las imágenes corporales de 5 robots, y se ha colectado previamente un conjunto de 100,000 trayectorias de ingeniería domínio randomizadas. Los resultados de los experimentos muestran que la tasa de éxito en la generación de código aumentó en un 10.9%, y la generalización a nuevos escenarios de la realidad se mejoró. El ajuste de modelos VLA del conjunto de datos resultó en un mejoramiento relativo del 367% en nuevas tareas de la realidad (de 9.0% a 42.0%), y entrenando solo en datos sin supervisar, se obtuvo un beneficio relativo del 228% (de 9.0% a 42.0%), demostrando una fortaleza en la generalización. Se liberarán los generadores de datos, el benchmark, el conjunto de datos y el código, apoyando la investigación escalable de manejos de manos izquierda y derecha.",
      "upvotes": 7,
      "discussionId": "685ae8e8d2ee4fac76521d1d",
      "projectPage": "https://robotwin-platform.github.io/",
      "githubRepo": "https://github.com/robotwin-Platform/RoboTwin",
      "ai_summary": "RoboTwin 2.0 is a scalable simulation framework for bimanual robotic manipulation that uses expert data synthesis and structured domain randomization to generate diverse and realistic synthetic data, improving sim-to-real transfer and generalization.",
      "ai_keywords": [
        "multimodal large language models",
        "simulation-in-the-loop",
        "structured domain randomization",
        "bimanual manipulation",
        "task-level execution code",
        "sim-to-real transfer",
        "zero-shot learning"
      ],
      "githubStars": 1129
    },
    "publishedAt": "2025-06-22T12:26:53.000Z",
    "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain\n  Randomization for Robust Bimanual Robotic Manipulation",
    "summary": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b37a9b06d8b55123ef8921",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
      "fullname": "Tianxing Chen",
      "name": "TianxingChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18674",
      "authors": [
        {
          "_id": "685cd8fc696820ba1f28f2aa",
          "name": "Raquel Ferrando",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ab",
          "name": "Javier Conde",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ac",
          "name": "Gonzalo Martínez",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ad",
          "name": "Pedro Reviriego",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T14:18:46.000Z",
      "submittedOnDailyAt": "2025-06-26T03:52:31.234Z",
      "title": "El modelo de texto de robotes se utiliza para el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el modelo de texto de robotes, el",
      "submittedOnDailyBy": {
        "_id": "64f31365ed48e3bb9c487d5d",
        "avatarUrl": "/avatars/979c1979eadbd4529c95b925bbb58d78.svg",
        "isPro": false,
        "fullname": "Gonzalo",
        "user": "gonzmart",
        "type": "user"
      },
      "summary": "El costo de cálculo y energía de los LLM aumenta exponencialmente al aumentar el tamaño del modelo y la introducción a millones de usuarios. El costo unitario de los LLM es el cálculo de tokens. Por lo tanto, la tokenización juega un papel importante en la eficiencia del modelo y se ajusta para minimizar el número de tokens del texto del corpus de entrenamiento. Una de las aplicaciones más populares de los LLM es el chatbot, que interactúa con usuarios. La observación clave es que en un chatbot, el rendimiento de la tokenización de los textos de entrada del usuario y de las respuestas del chatbot es crucial. Esto difiere de la tokenización del texto del corpus de entrenamiento. Por lo tanto, se surge inmediatamente el interés en los posibles beneficios de optimizar la tokenización para el chatbot. En este artículo, se investiga la posibilidad de optimizar la tokenización para el chatbot utilizando un corpus de diálogos de chatbots públicos disponibles, redesignando la vectorización y evaluando su rendimiento en este campo. Los resultados muestran que la tokenización optimizada para el diálogo reduce de manera consistente el número de tokens en los diálogos del chatbot, lo que puede lograr reducciones significativas de energía del 5% al 10%, y tiene un impacto positivo significativo o incluso mayor que la tokenización original del corpus de entrenamiento.",
      "upvotes": 5,
      "discussionId": "685cd8fc696820ba1f28f2ae",
      "ai_summary": "Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.",
      "ai_keywords": [
        "Large Language Models",
        "token",
        "tokenizer",
        "chatbots",
        "conversation-optimized tokenizers"
      ]
    },
    "publishedAt": "2025-06-23T10:18:46.000Z",
    "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?",
    "summary": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f31365ed48e3bb9c487d5d",
      "avatarUrl": "/avatars/979c1979eadbd4529c95b925bbb58d78.svg",
      "fullname": "Gonzalo",
      "name": "gonzmart",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20544",
      "authors": [
        {
          "_id": "685cdd71696820ba1f28f2b8",
          "user": {
            "_id": "677cfa6cac2db4c2265edb26",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Kbi96ndfY-CIuJNd2TRZt.jpeg",
            "isPro": false,
            "fullname": "Ammar Khairi",
            "user": "ammar-cohere",
            "type": "user"
          },
          "name": "Ammar Khairi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:26.999Z",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2b9",
          "name": "Daniel D'souza",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2ba",
          "name": "Ye Shen",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2bb",
          "name": "Julia Kreutzer",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2bc",
          "name": "Sara Hooker",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T15:37:53.000Z",
      "submittedOnDailyAt": "2025-06-26T04:17:49.221Z",
      "title": "Si tuvieras una vida como un ejemplo: los puntos de ventaja para mejorar la inferencia de un Multilingual LLM al expandir el cálculo.",
      "submittedOnDailyBy": {
        "_id": "6544e43b12da508864c38f96",
        "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
        "isPro": false,
        "fullname": "Julia Kreutzer",
        "user": "JuliaKreutzerCohere",
        "type": "user"
      },
      "summary": "El reciente desarrollo de los grandes modelos de lenguaje (LLMs) se centra en ampliar la cantidad de cálculos en la inferencia y mejorar el rendimiento evitando la reentrenamiento. Los métodos comunes incluyen la muestreo en paralelo de varios resultados para seleccionar el final. Sin embargo, la investigación actual se centra en áreas como el inglés, matemáticas o código. En contraste, nosotros estamos interesados en tareas abiertas, tareas que pueden ser verificadas de manera formal y tecnologías que pueden generalizarse entre lenguajes. En esta investigación, estudiamos las tareas generativas abiertas que amplían significativamente la cantidad de cálculos en la inferencia en varios lenguajes y configuraciones de tareas.\n\nNuestro hallazgo es que la estrategia de muestreo basada en la varianza y la estrategia de selección deben responder a diferentes campos y configuraciones de lenguaje. Evaluamos los métodos de selección existentes y demostramos que las estrategias efectivas en inglés no pueden generalizarse a otros lenguajes. Proponemos nuevas estrategias de muestreo y selección adaptadas a las escenarios de inferencia en varios lenguajes y tareas, que presentan efectos claros en ambos. En particular, nuestra combinación de estrategias de muestreo y selección aumentó en un promedio de +6.8 la probabilidad de victoria de nuestro modelo de 8B en el m-ArenaHard-v2.0, y también demostró eficacia frente a modelos propietarios como ゲミニ. A mayor escala, aplicando nuestro método al modelo Command-A (111B) nosotros obtuvimos una mejora en la probabilidad de victoria de +9.0 en el mismo benchmark, con un gran aumento en la validación de un solo muestra utilizando solo 5 muestras y con un aumento efectivo a bajo costo. Nuestros resultados subrayan la necesidad de intereses en la cantidad de cálculos en la inferencia relacionadas con lenguajes y tareas, y nos dirigemos a la democratización como objetivo para mejorar el rendimiento en lenguajes representativos.",
      "upvotes": 4,
      "discussionId": "685cdd71696820ba1f28f2bd",
      "ai_summary": "The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.",
      "ai_keywords": [
        "sampling strategy",
        "selection strategy",
        "temperature variation",
        "open-ended generative tasks",
        "multilingual",
        "multi-task",
        "m-ArenaHard-v2.0",
        "win-rates",
        "Command-A",
        "single-sample decoding",
        "language-aware",
        "task-aware"
      ]
    },
    "publishedAt": "2025-06-25T11:37:53.000Z",
    "title": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs",
    "summary": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20544.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6544e43b12da508864c38f96",
      "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
      "fullname": "Julia Kreutzer",
      "name": "JuliaKreutzerCohere",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20495",
      "authors": [
        {
          "_id": "685d0223696820ba1f28f322",
          "name": "Haoze Wu",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f323",
          "name": "Yunzhi Yao",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f324",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f325",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f326",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T14:41:13.000Z",
      "submittedOnDailyAt": "2025-06-26T06:48:06.720Z",
      "title": "ReCode: Aprendizaje por Refuerzo para la Actualización de Conocimientos de API de Código",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grandes (LLMs) muestran capacidad para generar código, pero fallan en adaptarse a las frecuentes actualizaciones de las API externas. Esta limitación importante se basa en conocimientos antiguos de API en los datos de entrenamiento y en la falta de documentación actual, lo que impide la generación de código confiable en entornos dinámicos. Para resolver este problema, proponemos un nuevo marco de trabajo basado en aprendizaje por refuerzo (ReCode), que utiliza aprendizaje por refuerzo basado en reglas para adaptar a los cambios en las API. En particular, entrenamos los LLMs con información de actualización basada en la versión de los códigos utilizando un conjunto de datos que incluye aproximadamente 2,000 entradas de datos. Además, utilizamos una mejora de la métrica de similitud de cadenas para evaluar el código como recompensa en el aprendizaje por refuerzo. Los resultados de las pruebas muestran que ReCode mejora significativamente la capacidad de generación de código de los LLMs en entornos dinámicos de API, con un efecto claro en la tarea de CodeUpdateArena. Además, observamos que, en comparación con el ajuste de observación controlada, ReCode no afecta la capacidad general de generación de código de los LLMs. ReCode se ha aplicado a diversos LLMs y algoritmos de aprendizaje por refuerzo (GRPO y DAPO), obteniendo mejoras consistentes en todos los casos. En particular, después del entrenamiento, Qwen2.5-Coder-7B superó a modelos de ajuste de código y modelos de lógica de la misma arquitectura. El código está disponible en https://github.com/zjunlp/ReCode.",
      "upvotes": 4,
      "discussionId": "685d0224696820ba1f28f327",
      "githubRepo": "https://github.com/zjunlp/ReCode",
      "ai_summary": "ReCode, a rule-based reinforcement learning framework, enhances large language models' adaptation to API updates without compromising their general code generation capabilities.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "code generation",
        "API updates",
        "dataset",
        "version migration",
        "string similarity metric",
        "reinforcement learning",
        "rule-based",
        "Qwen2.5-Coder-7B",
        "CodeUpdateArena",
        "GRPO",
        "DAPO"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-06-25T10:41:13.000Z",
    "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
    "summary": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20495.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 25
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20452",
      "authors": [
        {
          "_id": "685cfd21696820ba1f28f30a",
          "name": "Tobias Vontobel",
          "hidden": false
        },
        {
          "_id": "685cfd21696820ba1f28f30b",
          "user": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "isPro": false,
            "fullname": "Seyedmorteza Sadat",
            "user": "msadat97",
            "type": "user"
          },
          "name": "Seyedmorteza Sadat",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:18.462Z",
          "hidden": false
        },
        {
          "_id": "685cfd21696820ba1f28f30c",
          "name": "Farnood Salehi",
          "hidden": false
        },
        {
          "_id": "685cfd21696820ba1f28f30d",
          "user": {
            "_id": "630f7646197cd3f24e7f8e9f",
            "avatarUrl": "/avatars/59bbd4ed38277b313051aac78f6808ac.svg",
            "isPro": false,
            "fullname": "Romann Weber",
            "user": "RMW",
            "type": "user"
          },
          "name": "Romann M. Weber",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:16.404Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T13:58:37.000Z",
      "submittedOnDailyAt": "2025-06-26T06:29:51.626Z",
      "title": "Hiwab: Generación de imágenes de alta resolución sin entrenamiento mediante muestreo basado en dispersión de muestras de Wavelet",
      "submittedOnDailyBy": {
        "_id": "63b4b02a103617b0a5b0ee2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
        "isPro": false,
        "fullname": "Seyedmorteza Sadat",
        "user": "msadat97",
        "type": "user"
      },
      "summary": "El módulo de difusión ha aparecido como una aproximación avanzada en la síntesis de imágenes, demostrando una excelente realismo y diversidad. Sin embargo, el entrenamiento de módulos de difusión de alta resolución es computacionalmente complejo, y las técnicas actuales de generación de pruebas tienden a incluir repeticiones de objetos y artefactos espaciales no continuos en la síntesis de imágenes de resolución de entrenamiento más alta. En este artículo, se presenta un método para mejorar significativamente la confiabilidad visual y la coherencia estructural de la síntesis de imágenes de alta resolución sin entrenamiento, utilizando un módulo de difusión previamente entrenado con HiWave. Nuestro método utiliza una pipeline de dos etapas: se genera la imagen base en el módulo de entrenamiento previo, y se conecta el paso inverso DDIM con un nuevo módulo de amplificación detallada basado en wavelets. En particular, utilizamos el método inverso para obtener un vector de ruido inicial que mantiene la coherencia global en la imagen base. Luego, durante la muestración, nuestro módulo de amplificación detallada en el dominio de wavelets mantiene los componentes bajas de la imagen base, asegurando la coherencia estructural y, opcionalmente, guiando los componentes altas para mejorar los detalles y texturas. La evaluación con la expansión de Stable Diffusion XL muestra que HiWave efectivamente reduce los artefactos generales de los métodos existentes y alcanza una calidad visual superior. En la etapa de usuario, se confirma el rendimiento de HiWave, comparándolo con el estado de la arte, y se valora con un rendimiento superior de al menos el 80%, destacando la efectividad de la síntesis de imágenes de alta resolución de alta calidad.",
      "upvotes": 4,
      "discussionId": "685cfd22696820ba1f28f30e",
      "ai_summary": "HiWave enhances ultra-high-resolution image synthesis using pretrained diffusion models through a two-stage pipeline involving DDIM inversion and wavelet-based detail enhancement, improving visual fidelity and reducing artifacts.",
      "ai_keywords": [
        "diffusion models",
        "image synthesis",
        "photorealism",
        "high resolutions",
        "zero-shot generation",
        "artifacts",
        "object duplication",
        "spatial incoherence",
        "pretrained diffusion models",
        "two-stage pipeline",
        "DDIM inversion",
        "wavelet-based detail enhancer",
        "structural consistency",
        "fine details",
        "textures",
        "perceptual quality",
        "user study"
      ]
    },
    "publishedAt": "2025-06-25T09:58:37.000Z",
    "title": "HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based\n  Diffusion Sampling",
    "summary": "Diffusion models have emerged as the leading approach for image synthesis,\ndemonstrating exceptional photorealism and diversity. However, training\ndiffusion models at high resolutions remains computationally prohibitive, and\nexisting zero-shot generation techniques for synthesizing images beyond\ntraining resolutions often produce artifacts, including object duplication and\nspatial incoherence. In this paper, we introduce HiWave, a training-free,\nzero-shot approach that substantially enhances visual fidelity and structural\ncoherence in ultra-high-resolution image synthesis using pretrained diffusion\nmodels. Our method employs a two-stage pipeline: generating a base image from\nthe pretrained model followed by a patch-wise DDIM inversion step and a novel\nwavelet-based detail enhancer module. Specifically, we first utilize inversion\nmethods to derive initial noise vectors that preserve global coherence from the\nbase image. Subsequently, during sampling, our wavelet-domain detail enhancer\nretains low-frequency components from the base image to ensure structural\nconsistency, while selectively guiding high-frequency components to enrich fine\ndetails and textures. Extensive evaluations using Stable Diffusion XL\ndemonstrate that HiWave effectively mitigates common visual artifacts seen in\nprior methods, achieving superior perceptual quality. A user study confirmed\nHiWave's performance, where it was preferred over the state-of-the-art\nalternative in more than 80% of comparisons, highlighting its effectiveness for\nhigh-quality, ultra-high-resolution image synthesis without requiring\nretraining or architectural modifications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20452.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b4b02a103617b0a5b0ee2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
      "fullname": "Seyedmorteza Sadat",
      "name": "msadat97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18403",
      "authors": [
        {
          "_id": "685a555f0e4ad7e2197586b1",
          "user": {
            "_id": "65eef9ce7443c09267513796",
            "avatarUrl": "/avatars/62547f99130557f54093b2ff4d6c9c24.svg",
            "isPro": false,
            "fullname": "Muntasir Adnan",
            "user": "adnaan525",
            "type": "user"
          },
          "name": "Muntasir Adnan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:33:02.852Z",
          "hidden": false
        },
        {
          "_id": "685a555f0e4ad7e2197586b2",
          "name": "Carlos C. N. Kuhn",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T08:40:45.000Z",
      "submittedOnDailyAt": "2025-06-26T01:42:45.705Z",
      "title": "Índice de desagregación de depuración: revisión de la estrategia de depuración de código de un LLM",
      "submittedOnDailyBy": {
        "_id": "65eef9ce7443c09267513796",
        "avatarUrl": "/avatars/62547f99130557f54093b2ff4d6c9c24.svg",
        "isPro": false,
        "fullname": "Muntasir Adnan",
        "user": "adnaan525",
        "type": "user"
      },
      "summary": "El efecto de la depuración de IA se reconoce como un patrón de enfriamiento de una función exponencial predictable, y la mayoría de los modelos pierden el 60-80% de su capacidad depuradora en 2-3 intentos, lo que reconoce la importancia de repetir intentos para un sistema efectivo de generación de código. Estamos cuantificando los casos en los que la depuración no es efectiva y presentamos un marco matemático llamado Índice de Depuración de Caida (DDI) para predecir los puntos de enfoque que pueden mejorar la eficiencia de la depuración. Nuestro nuevo inicio estratégico explora los aspectos estratégicos de la introducción de depuración y muestra cómo el enfoque adecuado en ese momento puede recuperar la eficiencia de la depuración. El DDI claramente identifica las limitaciones fundamentales de la depuración de IA y proporciona por primera vez un marco cuantitativo para optimizar la estrategia de generación de código que repite intentos múltiples.",
      "upvotes": 2,
      "discussionId": "685a555f0e4ad7e2197586b3",
      "ai_summary": "The Debugging Decay Index (DDI) quantifies and optimizes the effectiveness of iterative AI debugging by predicting intervention points to revive and enhance debugging capability.",
      "ai_keywords": [
        "AI debugging",
        "Debugging Decay Index (DDI)",
        "iterative debugging",
        "code generation",
        "effectiveness",
        "intervention points"
      ]
    },
    "publishedAt": "2025-06-23T04:40:45.000Z",
    "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs",
    "summary": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18403.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65eef9ce7443c09267513796",
      "avatarUrl": "/avatars/62547f99130557f54093b2ff4d6c9c24.svg",
      "fullname": "Muntasir Adnan",
      "name": "adnaan525",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.20512",
      "authors": [
        {
          "_id": "685cb8d7696820ba1f28f296",
          "name": "Zengzhi Wang",
          "hidden": false
        },
        {
          "_id": "685cb8d7696820ba1f28f297",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "685cb8d7696820ba1f28f298",
          "name": "Xuefeng Li",
          "hidden": false
        },
        {
          "_id": "685cb8d7696820ba1f28f299",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62cbeb2d72dfd24b86bdf977/cZoks9vqpXBnkpA9PgVRV.png"
      ],
      "publishedAt": "2025-06-25T14:58:13.000Z",
      "submittedOnDailyAt": "2025-06-26T07:21:37.577Z",
      "title": "Octotihneer: Promueve la escalabilidad de la aprendizaje por reforzamiento en el aprendizaje intermedio.",
      "submittedOnDailyBy": {
        "_id": "62cbeb2d72dfd24b86bdf977",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62cbeb2d72dfd24b86bdf977/UcGYYSBNrCvPM5K9v-sro.png",
        "isPro": false,
        "fullname": "Zengzhi Wang",
        "user": "SinclairWang",
        "type": "user"
      },
      "summary": "Diferentes familias de modelos de lenguaje básico, como por ejemplo Llama y Qwen, muestran diferentes acciones en el paso de modificación después del aprendizaje por refuerzo (RL). ¿Por qué son los modelos de lenguaje básico adecuados para el aprendizaje por refuerzo? La profunda comprensión de estos problemas es crucial para el desarrollo de los próximos generadores de RL escalables de modelos básicos. En este estudio, se investiga cómo la estrategia de aprendizaje intermedio afecta a RL de manera dinámica, centrandose en las familias de dos modelos representativos, Qwen y Llama. El estudio presenta los siguientes resultados: (1) los corpus de alta calidad como MegaMath-Web-Pro mejoran significativamente el rendimiento de los modelos básicos y RL, demostrando que los corpus actuales (por ejemplo, FineMath-4plus) no logran estas mejoras; (2) la adición de datos de tipo QA, especialmente datos que incluyen ejemplos de inferencia larga de cadena de pensamiento (CoT), mejoran los resultados del RL y muestran que los datos directivos tienen estos efectos; (3) la CoT aumenta la profundidad de la inferencia pero también lleva a un aumento en la longitud de la respuesta del modelo y en la instabilidad del entrenamiento RL, destacando la importancia del formato de los datos; (4) la escalabilidad de la estrategia de aprendizaje intermedio muestra un rendimiento fuerte en RL. Basándose en estas observaciones, se introduce la estrategia de aprendizaje intermedio en dos etapas, Stable-then-Decay, donde los modelos básicos se entrenan con un ritmo de aprendizaje fijo a 200B tokens, y luego se entrenan en 3 ramas de foco en CoT con 20B tokens, reduciendo el ritmo de aprendizaje. De esta manera, la familia de OctoThinker muestra una fuerte compatibilidad con el RL y reduce la diferencia de rendimiento con los familias de modelos más adecuados para el RL. Este estudio espera formar las estrategias de entrenamiento previa de modelos básicos en la era del RL. Para fomentar más investigación, este estudio publica modelos de código abierto y un corpus de más de 700B tokens con una fuerte capacidad teórica en matemáticas (MegaMath-Web-Pro-Max).",
      "upvotes": 1,
      "discussionId": "685cb8d7696820ba1f28f29a",
      "githubRepo": "https://github.com/GAIR-NLP/OctoThinker",
      "ai_summary": "Investigating mid-training strategies reveals that high-quality mathematical corpora and well-formatted chain-of-thought reasoning examples enhance reinforcement learning performance in language models, leading to the development of OctoThinker.",
      "ai_keywords": [
        "reinforcement learning",
        "base language model",
        "mid-training strategy",
        "MegaMath-Web-Pro",
        "QA-style data",
        "chain-of-thought (CoT) reasoning",
        "data formatting",
        "learning rate decay",
        "OctoThinker",
        "MegaMath-Web-Pro-Max"
      ],
      "githubStars": 66
    },
    "publishedAt": "2025-06-25T10:58:13.000Z",
    "title": "OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling",
    "summary": "Different base language model families, such as Llama and Qwen, exhibit\ndivergent behaviors during post-training with reinforcement learning (RL),\nespecially on reasoning-intensive tasks. What makes a base language model\nsuitable for reinforcement learning? Gaining deeper insight into this question\nis essential for developing RL-scalable foundation models of the next\ngeneration. In this work, we investigate how mid-training strategies shape RL\ndynamics, focusing on two representative model families: Qwen and Llama. Our\nstudy reveals that (1) high-quality mathematical corpora, such as\nMegaMath-Web-Pro, significantly improve both base model and RL performance,\nwhile existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further\nadding QA-style data, particularly long chain-of-thought (CoT) reasoning\nexamples, enhances RL outcomes, and instruction data further unlocks this\neffect; (3) while long-CoT improves reasoning depth, it can also induce\nverbosity of model responses and unstability of RL training, underscoring the\nimportance of data formatting; (4) scaling mid-training consistently leads to\nstronger downstream RL performance. Building on these insights, we introduce a\ntwo-stage mid-training strategy, Stable-then-Decay, in which base models are\nfirst trained on 200B tokens with a constant learning rate, followed by 20B\ntokens across three CoT-focused branches with learning rate decay. This yields\nOctoThinker, a family of models demonstrating strong RL compatibility and\nclosing the performance gap with more RL-friendly model families, i.e., Qwen.\nWe hope our work will help shape pre-training strategies for foundation models\nin the RL era. To support further research, we release our open-source models\nalong with a curated math reasoning-intensive corpus of over 70 billion tokens\n(i.e., MegaMath-Web-Pro-Max).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62cbeb2d72dfd24b86bdf977/cZoks9vqpXBnkpA9PgVRV.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20512.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62cbeb2d72dfd24b86bdf977",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62cbeb2d72dfd24b86bdf977/UcGYYSBNrCvPM5K9v-sro.png",
      "fullname": "Zengzhi Wang",
      "name": "SinclairWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19502",
      "authors": [
        {
          "_id": "685c02aadf8a0d6c70bbf918",
          "user": {
            "_id": "674ed490fc7f50ef61c3a7bd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
            "isPro": false,
            "fullname": "Aleksandr Algazinov",
            "user": "AleksandrAlgazinov",
            "type": "user"
          },
          "name": "Aleksandr Algazinov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T20:59:09.301Z",
          "hidden": false
        },
        {
          "_id": "685c02aadf8a0d6c70bbf919",
          "name": "Matt Laing",
          "hidden": false
        },
        {
          "_id": "685c02aadf8a0d6c70bbf91a",
          "name": "Paul Laban",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T10:40:23.000Z",
      "submittedOnDailyAt": "2025-06-26T01:26:07.991Z",
      "title": "MATE: Ingeniería de robots móviles Terminal Programación Emprendimientos FOE Accesibilidad Aplicaciones Premium",
      "submittedOnDailyBy": {
        "_id": "674ed490fc7f50ef61c3a7bd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
        "isPro": false,
        "fullname": "Aleksandr Algazinov",
        "user": "AleksandrAlgazinov",
        "type": "user"
      },
      "summary": "La accesibilidad es un problema importante en la sociedad moderna, ya que muchas tecnologías no satisfacen las necesidades completas de los usuarios. Los sistemas multi-agente (SMA) existentes, debido a su diseño cerrado, no ofrecen suficientes opciones de personalización, lo que hace que muchos usuarios no reciban la ayuda necesaria. Esto genera un gran desafío para personas con discapacidades, especialmente al interactuar con entornos digitales. Presentamos un enfoque multi-modelo para un SMA que permite la conversión de modelos basada en las necesidades del usuario. Este sistema transforma los datos en formatos comprensibles y ofrece apoyo a personas con discapacidades. Por ejemplo, si un usuario tiene una pérdida visual, el sistema convierte las imágenes en descripciones de voz. El sistema Multi-Agent MATE puede aplicarse en una amplia gama de áreas, como la atención médica, y puede ser una herramienta útil para muchos grupos de usuarios. El sistema soporta una variedad de modelos, desde la llamada a la API de modelos grandes de lenguaje (LLM) hasta el uso de ML personalizado mediante chatbots. Esta flexibilidad permite que el sistema sea adaptable a diferentes necesidades y mantenga compatibilidad con diversos dispositivos. Se supone que el sistema se ejecuta localmente, garantizando la privacidad y la seguridad de la información importante. Además, el marco de trabajo puede integrarse efectivamente con la tecnología de las instituciones (por ejemplo, servicios digitales de atención médica) y ofrecer asistencia en tiempo real. También presentamos ModCon-Task-Identifier, un modelo que extrae tareas de conversión de modelos precisas a partir de las entradas del usuario. Varios experimentos han demostrado que ModCon-Task-Identifier muestra un rendimiento consistente en nuestros datos de usuario, mejorando sobre otros modelos LLM o modelos estadísticos. Nuestro código y datos están disponibles en https://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
      "upvotes": 1,
      "discussionId": "685c02abdf8a0d6c70bbf91b",
      "ai_summary": "MATE, a multimodal accessibility multi-agent system, converts data into understandable formats based on user needs, supporting various disabilities and integrating with institutional technologies.",
      "ai_keywords": [
        "MULTIAGENT SYSTEMS",
        "MAS",
        "MODALITY CONVERSIONS",
        "LLM API",
        "CUSTOM MACHINE LEARNING CLASSIFIERS",
        "MODCON-TASK-IDENTIFIER",
        "LLM",
        "STATISTICAL MODELS"
      ]
    },
    "publishedAt": "2025-06-24T06:40:23.000Z",
    "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications",
    "summary": "Accessibility remains a critical concern in today's society, as many\ntechnologies are not developed to support the full range of user needs.\nExisting multi-agent systems (MAS) often cannot provide comprehensive\nassistance for users in need due to the lack of customization stemming from\nclosed-source designs. Consequently, individuals with disabilities frequently\nencounter significant barriers when attempting to interact with digital\nenvironments. We introduce MATE, a multimodal accessibility MAS, which performs\nthe modality conversions based on the user's needs. The system is useful for\nassisting people with disabilities by ensuring that data will be converted to\nan understandable format. For instance, if the user cannot see well and\nreceives an image, the system converts this image to its audio description.\nMATE can be applied to a wide range of domains, industries, and areas, such as\nhealthcare, and can become a useful assistant for various groups of users. The\nsystem supports multiple types of models, ranging from LLM API calling to using\ncustom machine learning (ML) classifiers. This flexibility ensures that the\nsystem can be adapted to various needs and is compatible with a wide variety of\nhardware. Since the system is expected to run locally, it ensures the privacy\nand security of sensitive information. In addition, the framework can be\neffectively integrated with institutional technologies (e.g., digital\nhealthcare service) for real-time user assistance. Furthermore, we introduce\nModCon-Task-Identifier, a model that is capable of extracting the precise\nmodality conversion task from the user input. Numerous experiments show that\nModCon-Task-Identifier consistently outperforms other LLMs and statistical\nmodels on our custom data. Our code and data are publicly available at\nhttps://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674ed490fc7f50ef61c3a7bd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
      "fullname": "Aleksandr Algazinov",
      "name": "AleksandrAlgazinov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.20331",
      "authors": [
        {
          "_id": "685d0b5d696820ba1f28f349",
          "user": {
            "_id": "62a9b0acf6708cb85014f9dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9b0acf6708cb85014f9dc/Sem1qcBt1lJjFEPK-xz4_.jpeg",
            "isPro": false,
            "fullname": "Rian Touchent",
            "user": "rntc",
            "type": "user"
          },
          "name": "Rian Touchent",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:08.325Z",
          "hidden": false
        },
        {
          "_id": "685d0b5d696820ba1f28f34a",
          "name": "Nathan Godey",
          "hidden": false
        },
        {
          "_id": "685d0b5d696820ba1f28f34b",
          "name": "Eric de la Clergerie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T11:30:25.000Z",
      "submittedOnDailyAt": "2025-06-26T07:44:08.382Z",
      "title": "Biomedi Enriched: Extension of the Biomedi Dataset for Pre-training and Extracting Rare or Hidden Content Using LLMs",
      "submittedOnDailyBy": {
        "_id": "62a9b0acf6708cb85014f9dc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9b0acf6708cb85014f9dc/Sem1qcBt1lJjFEPK-xz4_.jpeg",
        "isPro": false,
        "fullname": "Rian Touchent",
        "user": "rntc",
        "type": "user"
      },
      "summary": "Biomedic Enity, un conjunto de datos biomédicos construido a partir de PubMed, se presenta. Este conjunto de datos se ha construido a través de un proceso de anotación en dos etapas. En la primera etapa, un modelo de lenguaje grande anotó 400K páginas de artículos científicos de PubMed, asignando a cada página un score de calidad educativa (evaluado de 1 a 5), así como categorías como revisión, investigación, caso clínico y otros, y áreas como clínica, biomédica y otros. Estos scores de calidad educativa se utilizan para calibrar un modelo de lenguaje de menor escala, lo que permite propagar etiquetas a todo el corpus PMC-OA. Con esto, se puede extraer un subconjunto de casos clínicos y construir un subconjunto de más de 450K artículos de alta calidad con permisos de uso comercial. Además, mediante filtrado de calidad y muestreo por área, se construyen varias versiones del conjunto de datos.",
      "upvotes": 0,
      "discussionId": "685d0b5d696820ba1f28f34c",
      "ai_summary": "A biomedical text dataset, constructed from PubMed, uses a two-stage annotation process involving large and small language models to fine-tune and extract subsets for clinical NLP, improving pretraining efficiency and performance.",
      "ai_keywords": [
        "Biomed-Enriched",
        "PubMed",
        "large language model",
        "small language model",
        "fine-tuning",
        "PMC-OA corpus",
        "educational quality",
        "clinical cases",
        "biomedical NLP",
        "continual-pretraining",
        "OLMo2",
        "MMLU ProfMed",
        "MedQA",
        "MedMCQA",
        "training tokens"
      ]
    },
    "publishedAt": "2025-06-25T07:30:25.000Z",
    "title": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining\n  and Extracting Rare and Hidden Content",
    "summary": "We introduce Biomed-Enriched, a biomedical text dataset constructed from\nPubMed via a two-stage annotation process. In the first stage, a large language\nmodel annotates 400K paragraphs from PubMed scientific articles, assigning\nscores for their type (review, study, clinical case, other), domain (clinical,\nbiomedical, other), and educational quality. The educational quality score\n(rated 1 to 5) estimates how useful a paragraph is for college-level learning.\nThese annotations are then used to fine-tune a small language model, which\npropagates the labels across the full PMC-OA corpus. The resulting metadata\nallows us to extract refined subsets, including 2M clinical case paragraphs\nwith over 450K high-quality ones from articles with commercial-use licenses,\nand to construct several variants via quality filtering and domain upsampling.\nClinical text is typically difficult to access due to privacy constraints, as\nhospital records cannot be publicly shared. Hence, our dataset provides an\nalternative large-scale, openly available collection of clinical cases from\nPubMed, making it a valuable resource for biomedical and clinical NLP.\nPreliminary continual-pretraining experiments with OLMo2 suggest these curated\nsubsets enable targeted improvements, with clinical upsampling boosting\nperformance by ~5% on MMLU ProfMed and educational quality filtering improving\nMedQA and MedMCQA by ~1%. Combinations of these techniques led to faster\nconvergence, reaching same performance with a third of training tokens,\nindicating potential for more efficient and effective biomedical pretraining\nstrategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20331.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a9b0acf6708cb85014f9dc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9b0acf6708cb85014f9dc/Sem1qcBt1lJjFEPK-xz4_.jpeg",
      "fullname": "Rian Touchent",
      "name": "rntc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": true
  }
]