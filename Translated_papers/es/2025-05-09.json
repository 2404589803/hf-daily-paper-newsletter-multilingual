[
  {
    "paper": {
      "id": "2505.04620",
      "authors": [
        {
          "_id": "681c6c1817fc8222eff39a1a",
          "user": {
            "_id": "647773a1168cb428e00e9a8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
            "isPro": false,
            "fullname": "Hao Fei",
            "user": "scofield7419",
            "type": "user"
          },
          "name": "Hao Fei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:07.591Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1b",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1c",
          "user": {
            "_id": "67bc247b593452cc18965cb1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/EA3kTYaaff0Hr7-dGiOOj.png",
            "isPro": false,
            "fullname": "JUNCHENG LI",
            "user": "JunchengLi",
            "type": "user"
          },
          "name": "Juncheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:36:52.461Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1d",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:36:59.117Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1e",
          "name": "Qingshan Xu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1f",
          "name": "Bobo Li",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a20",
          "user": {
            "_id": "64c139d867eff857ea51caa8",
            "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
            "isPro": false,
            "fullname": "Shengqiong Wu",
            "user": "ChocoWu",
            "type": "user"
          },
          "name": "Shengqiong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:39.333Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a21",
          "user": {
            "_id": "64ff369d9abcc85a5519b33e",
            "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
            "isPro": false,
            "fullname": "Yaoting Wang",
            "user": "Gh0stAR",
            "type": "user"
          },
          "name": "Yaoting Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:37:27.790Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a22",
          "user": {
            "_id": "67e906836c7216f5bf91f70c",
            "avatarUrl": "/avatars/9c7f34d5b1d41ad7231d2733a399abb3.svg",
            "isPro": false,
            "fullname": "junbao.zhou",
            "user": "junbaozhou",
            "type": "user"
          },
          "name": "Junbao Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:37:34.046Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a23",
          "user": {
            "_id": "65a28e129acab19980226731",
            "avatarUrl": "/avatars/abc3828f807efc4e03837b0eae063f98.svg",
            "isPro": false,
            "fullname": "Jiahao Meng",
            "user": "marinero4972",
            "type": "user"
          },
          "name": "Jiahao Meng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:37:40.200Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a24",
          "user": {
            "_id": "656724074f6ec72017754d33",
            "avatarUrl": "/avatars/e61de248f6f53719b2375077340dd033.svg",
            "isPro": false,
            "fullname": "QingyuShi",
            "user": "QingyuShi",
            "type": "user"
          },
          "name": "Qingyu Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:34.088Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a25",
          "name": "Zhiyuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a26",
          "name": "Liangtao Shi",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a27",
          "user": {
            "_id": "648ef24dc92367eecac0f4bd",
            "avatarUrl": "/avatars/38f1afd6b52efeee3aa41cc80225d788.svg",
            "isPro": false,
            "fullname": "Minghe Gao",
            "user": "gmh5811",
            "type": "user"
          },
          "name": "Minghe Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:38:16.554Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a28",
          "user": {
            "_id": "6241b95cfee9374a2598ecfe",
            "avatarUrl": "/avatars/196669df1689a5872fc18b271e80fdc1.svg",
            "isPro": false,
            "fullname": "Zhang Daoan",
            "user": "hazard",
            "type": "user"
          },
          "name": "Daoan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:38:28.567Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a29",
          "name": "Zhiqi Ge",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2a",
          "name": "Weiming Wu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2b",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2c",
          "name": "Kaihang Pan",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2d",
          "user": {
            "_id": "662917afda1cae6cbb50cd00",
            "avatarUrl": "/avatars/aa66de6cef6665c5d67071d82bac35c4.svg",
            "isPro": false,
            "fullname": "Yaobo Ye",
            "user": "superyyb",
            "type": "user"
          },
          "name": "Yaobo Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:55:06.463Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2e",
          "user": {
            "_id": "6391e41f2e73987364e6bcb2",
            "avatarUrl": "/avatars/d09a9ee329bb8c3a9e2929d67d24e97d.svg",
            "isPro": false,
            "fullname": "Haobo Yuan",
            "user": "HarborYuan",
            "type": "user"
          },
          "name": "Haobo Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:39:11.094Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2f",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a30",
          "user": {
            "_id": "6816d98fc075e49c1b15928e",
            "avatarUrl": "/avatars/6b24d047fc25075bedb3e74f78981bc0.svg",
            "isPro": false,
            "fullname": "Tianjie Ju",
            "user": "jometeorieNUS",
            "type": "user"
          },
          "name": "Tianjie Ju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:53:06.930Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a31",
          "name": "Zixiang Meng",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a32",
          "name": "Shilin Xu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a33",
          "name": "Liyu Jia",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a34",
          "name": "Wentao Hu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a35",
          "user": {
            "_id": "64ad1c0bad6218d51a07b54e",
            "avatarUrl": "/avatars/0f84d9a51c6ca9bcef44de2d7c707d9b.svg",
            "isPro": false,
            "fullname": "LUO MENG",
            "user": "Eureka-Leo",
            "type": "user"
          },
          "name": "Meng Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:37.235Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a36",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a37",
          "name": "Tat-Seng Chua",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a38",
          "user": {
            "_id": "67eaa070b9fa8908e151fd7d",
            "avatarUrl": "/avatars/1fe2fd678d2e71099a83a9bcb9ab517e.svg",
            "isPro": false,
            "fullname": "shuicheng yan",
            "user": "shuicheng",
            "type": "user"
          },
          "name": "Shuicheng Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:52:30.205Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a39",
          "name": "Hanwang Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/TqzNcdmo0rwc-0JkEc0-i.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HBVHtWBiagedRLXD-lWZc.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/X02B6xk6CZDywtG8tGliK.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Xnc89DOmtr5j2hST5Um17.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Awl6jj9MX38cMRkgXpxHp.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HT_9Y1ponvvqUREjMTMBB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/sG3FBiQCx3EONF25cEbED.png"
      ],
      "publishedAt": "2025-05-07T17:59:32.000Z",
      "submittedOnDailyAt": "2025-05-09T01:19:17.510Z",
      "title": "Multimodal Acceleration Plan: General Level and General Benchmark",
      "submittedOnDailyBy": {
        "_id": "647773a1168cb428e00e9a8f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
        "isPro": false,
        "fullname": "Hao Fei",
        "user": "scofield7419",
        "type": "user"
      },
      "summary": "El modelo de lenguaje multimodal de difusión (MLLM) está creciendo rápidamente gracias a las capacidades avanzadas de los modelos de lenguaje de difusión (LLM). Contrariamente a lo que dicen los expertos, el actual MLLM está demostrando un desarrollo significativo en el campo de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la difusión de la dif",
      "upvotes": 42,
      "discussionId": "681c6c1d17fc8222eff39b45",
      "projectPage": "https://generalist.top/",
      "githubRepo": "https://github.com/path2generalist/General-Level",
      "ai_keywords": [
        "Multimodal Large Language Model (MLLM)",
        "Multimodal Generalist",
        "multimodal understanding",
        "comprehension",
        "generation",
        "General-Level",
        "Synergy",
        "General-Bench",
        "AGI (Artificial General Intelligence)"
      ]
    },
    "publishedAt": "2025-05-07T13:59:32.000Z",
    "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
    "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/TqzNcdmo0rwc-0JkEc0-i.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HBVHtWBiagedRLXD-lWZc.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/X02B6xk6CZDywtG8tGliK.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Xnc89DOmtr5j2hST5Um17.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Awl6jj9MX38cMRkgXpxHp.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HT_9Y1ponvvqUREjMTMBB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/sG3FBiQCx3EONF25cEbED.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04620.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "647773a1168cb428e00e9a8f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
      "fullname": "Hao Fei",
      "name": "scofield7419",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05470",
      "authors": [
        {
          "_id": "681d9829edf34a77aab565eb",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ec",
          "user": {
            "_id": "6553316bf151de82f6a23e1d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6553316bf151de82f6a23e1d/GTBkSj4Fa3OoyM6Muz_Sc.jpeg",
            "isPro": false,
            "fullname": "Gongye Liu",
            "user": "liuhuohuo",
            "type": "user"
          },
          "name": "Gongye Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:41:47.403Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ed",
          "name": "Jiajun Liang",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ee",
          "user": {
            "_id": "64d71083a787c9bc7b9f1238",
            "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
            "isPro": false,
            "fullname": "Yangguang Li",
            "user": "Lp256",
            "type": "user"
          },
          "name": "Yangguang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:43:44.697Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ef",
          "user": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "isPro": false,
            "fullname": "Jiaheng Liu",
            "user": "CheeryLJH",
            "type": "user"
          },
          "name": "Jiaheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:45:07.297Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f0",
          "user": {
            "_id": "60e272ca6c78a8c122b12127",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
            "isPro": false,
            "fullname": "Xintao Wang",
            "user": "Xintao",
            "type": "user"
          },
          "name": "Xintao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:42:27.977Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f1",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f2",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:43:01.366Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f3",
          "name": "Wanli Ouyang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T17:58:45.000Z",
      "submittedOnDailyAt": "2025-05-09T05:45:53.355Z",
      "title": "Flow-GRPO: Entrenamiento de un modelo de matching de flujos utilizando RL en línea",
      "submittedOnDailyBy": {
        "_id": "64d71083a787c9bc7b9f1238",
        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
        "isPro": false,
        "fullname": "Yangguang Li",
        "user": "Lp256",
        "type": "user"
      },
      "summary": "Propongo Flow-GRPO. Esta es el primer método útil. Nuestro enfoque utiliza dos estrategias clave.\n\n(1) Transformación ODE-to-SDE: Convertimos una ecuación diferencial ordinaria (ODE) determinista en una ecuación diferencial estocástica equivalente (SDE) que se ajusta al distribución de frontera del modelo original en todos los pasos de tiempo. De esta manera, se puede realizar muestreo estadístico en la búsqueda de RL.\n\n(2) Estrategia de Reducción de Denoising: Reduciendo la cantidad de denoising mientras mantenemos el número de pasos de tiempo de inferencia del modelo, se puede significativamente mejorar la eficiencia de muestreo.\n\nExperimentalmente, Flow-GRPO es efectivo en organizaciones complejas y mejora significativamente la precisión de GenEval, aumentando la precisión del 63% a 95%. También mejora la precisión en la renderización de texto visual, aumentando el 59% a 92%. Flow-GRPO también muestra un gran efecto en la concordancia con las preferencias humanas. En particular, no se observó el problema de acumulación de recompensas. Esto significa que la calidad y diversidad de las imágenes no se redujeron y la recompensa no aumentó, lo cual fue estable en los experimentos.",
      "upvotes": 26,
      "discussionId": "681d982aedf34a77aab56635",
      "ai_keywords": [
        "Flow-GRPO",
        "reinforcement learning (RL)",
        "flow matching models",
        "ODE-to-SDE conversion",
        "Ordinary Differential Equation (ODE)",
        "Stochastic Differential Equation (SDE)",
        "Denoising Reduction strategy",
        "GenEval accuracy",
        "text-to-image tasks",
        "SD3.5",
        "visual text rendering",
        "human preference alignment",
        "reward hacking"
      ]
    },
    "publishedAt": "2025-05-08T13:58:45.000Z",
    "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
    "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05470.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64d71083a787c9bc7b9f1238",
      "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
      "fullname": "Yangguang Li",
      "name": "Lp256",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04921",
      "authors": [
        {
          "_id": "681dbb9988ca86d430f1d0d2",
          "user": {
            "_id": "62fdb01bc1588e1d4c6c1a7c",
            "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
            "isPro": false,
            "fullname": "Yunxin Li",
            "user": "YunxinLi",
            "type": "user"
          },
          "name": "Yunxin Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:05:42.761Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d3",
          "user": {
            "_id": "64380ae1819f3ab20d17431b",
            "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
            "isPro": false,
            "fullname": "ZhenyuLiu",
            "user": "foggyforest",
            "type": "user"
          },
          "name": "Zhenyu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T10:08:42.493Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d4",
          "user": {
            "_id": "67ecc6a08647cfa1775a9fda",
            "avatarUrl": "/avatars/bb15abd7a3d2c51380b0b1f819ef76e2.svg",
            "isPro": false,
            "fullname": "Zitao Li",
            "user": "TerenceL-TL",
            "type": "user"
          },
          "name": "Zitao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T08:33:26.702Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d5",
          "name": "Xuanyu Zhang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d6",
          "user": {
            "_id": "639c379cdb7c5f35004066cb",
            "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
            "isPro": false,
            "fullname": "Zhenran Xu",
            "user": "imryanxu",
            "type": "user"
          },
          "name": "Zhenran Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T10:08:40.339Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d7",
          "name": "Xinyu Chen",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d8",
          "name": "Haoyuan Shi",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d9",
          "name": "Shenyuan Jiang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0da",
          "name": "Xintong Wang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0db",
          "name": "Jifang Wang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0dc",
          "name": "Shouzheng Huang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0dd",
          "name": "Xinping Zhao",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0de",
          "name": "Borui Jiang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0df",
          "name": "Lanqing Hong",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e0",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e1",
          "name": "Zhuotao Tian",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e2",
          "name": "Baoxing Huai",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e3",
          "name": "Wenhan Luo",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e4",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e5",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e6",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e7",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T03:35:23.000Z",
      "submittedOnDailyAt": "2025-05-09T06:54:36.013Z",
      "title": "Percepción, razonamiento, pensamiento, planificación: Investigación sobre el enfoque de razonamiento en modelos multimodal a gran escala",
      "submittedOnDailyBy": {
        "_id": "64380ae1819f3ab20d17431b",
        "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
        "isPro": false,
        "fullname": "ZhenyuLiu",
        "user": "foggyforest",
        "type": "user"
      },
      "summary": "La teoría se centra en el centro de la inteligencia, formando capacidades como la decisión, la extracción de conclusiones y la generalización que sobrepasan dominios. En el campo de la inteligencia artificial, es crucial que los sistemas se desarrollen para funcionar en entornos abiertos y inciertos, diversos. La teoría debe permitir acciones fuertes y adaptativas. Los modelos de teoría multimodal grandes (LMRMs) integran modelos de texto, imágenes, voz y vídeo, apoyando capacidades complejas de reconocimiento, comprensión precisa y teoría profunda. Con el progreso de la investigación, la teoría multimodal ha evolucionado rápidamente desde un pipeline modular de reconocimiento hasta un marco continuo centrado en el lenguaje. En este proceso, la ajuste de comandos y el aprendizaje por refuerzo han mejorado la teoría del modelo, pero han dejado grandes problemas en la generalización, la profundidad de la teoría y los comportamientos de salida. Para resolver estos problemas, basamos nuestro trabajo en una nueva filosofía de diseño y nuevas capacidades, proporcionando un estudio detallado de la investigación de teoría multimodal en cuatro etapas. Primero, investigamos los primeros esfuerzos modulares basados en tareas específicas, donde la teoría se veía oculta en etapas de representación, arranque y fusión. Luego, examinamos los últimos enfoques, integrando la teoría en modelos multimodal de LLMs y desarrollando cadenas de pensamiento multimodal (MCoT) y aprendizaje por refuerzo multimodal, facilitando cadenas teóricas más complejas y estructuradas. Finalmente, basándonos en los difíciles benchmarks de OpenAI O3 y O4-mini y en casos de experimentación, discutimos la dirección conceptual de los modelos de teoría multimodal grandes (N-LMRMs) y el objetivo de apoyar teorías adaptativas y escalables en entornos complejos y reales.",
      "upvotes": 21,
      "discussionId": "681dbb9b88ca86d430f1d183",
      "projectPage": "https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models",
      "githubRepo": "https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models",
      "ai_keywords": [
        "Large Multimodal Reasoning Models (LMRMs)",
        "multimodal reasoning",
        "Cross-modal understanding",
        "task-specific modules",
        "representation",
        "alignment",
        "fusion",
        "Multimodal Chain-of-Thought (MCoT)",
        "multimodal reinforcement learning",
        "native large multimodal reasoning models (N-LMRMs)",
        "scalable",
        "agentic",
        "adaptive reasoning",
        "planning"
      ]
    },
    "publishedAt": "2025-05-07T23:35:23.000Z",
    "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models",
    "summary": "Reasoning lies at the heart of intelligence, shaping the ability to make\ndecisions, draw conclusions, and generalize across domains. In artificial\nintelligence, as systems increasingly operate in open, uncertain, and\nmultimodal environments, reasoning becomes essential for enabling robust and\nadaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a\npromising paradigm, integrating modalities such as text, images, audio, and\nvideo to support complex reasoning capabilities and aiming to achieve\ncomprehensive perception, precise understanding, and deep reasoning. As\nresearch advances, multimodal reasoning has rapidly evolved from modular,\nperception-driven pipelines to unified, language-centric frameworks that offer\nmore coherent cross-modal understanding. While instruction tuning and\nreinforcement learning have improved model reasoning, significant challenges\nremain in omni-modal generalization, reasoning depth, and agentic behavior. To\naddress these issues, we present a comprehensive and structured survey of\nmultimodal reasoning research, organized around a four-stage developmental\nroadmap that reflects the field's shifting design philosophies and emerging\ncapabilities. First, we review early efforts based on task-specific modules,\nwhere reasoning was implicitly embedded across stages of representation,\nalignment, and fusion. Next, we examine recent approaches that unify reasoning\ninto multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)\nand multimodal reinforcement learning enabling richer and more structured\nreasoning chains. Finally, drawing on empirical insights from challenging\nbenchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the\nconceptual direction of native large multimodal reasoning models (N-LMRMs),\nwhich aim to support scalable, agentic, and adaptive reasoning and planning in\ncomplex, real-world environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64380ae1819f3ab20d17431b",
      "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
      "fullname": "ZhenyuLiu",
      "name": "foggyforest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02847",
      "authors": [
        {
          "_id": "681d7031e9969eecfcb4eb81",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb82",
          "user": {
            "_id": "648294b2eb4befee378951c1",
            "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
            "isPro": false,
            "fullname": "Ruotian Ma",
            "user": "vvibt",
            "type": "user"
          },
          "name": "Ruotian Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:30.886Z",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb83",
          "name": "Qingxuan Jiang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb84",
          "name": "Peisong Wang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb85",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb86",
          "name": "Zheng Xie",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb87",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb88",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb89",
          "name": "Fanghua Ye",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8a",
          "name": "Jian Li",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8b",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8c",
          "user": {
            "_id": "67485743561b1e6f9579389f",
            "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
            "isPro": false,
            "fullname": "Zhaopeng Tu",
            "user": "zptu",
            "type": "user"
          },
          "name": "Zhaopeng Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:28.468Z",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8d",
          "name": "Xiaolong Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T19:06:10.000Z",
      "submittedOnDailyAt": "2025-05-09T01:37:10.548Z",
      "title": "Se presenta el uso de agentes de lenguaje de gran escala como jueces para evaluar la cognición social avanzada de modelos de lenguaje.",
      "submittedOnDailyBy": {
        "_id": "648294b2eb4befee378951c1",
        "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
        "isPro": false,
        "fullname": "Ruotian Ma",
        "user": "vvibt",
        "type": "user"
      },
      "summary": "La evaluación de cuánto los modelos de lenguaje de alto nivel (LLM) comprenden a la humanidad, no solo en el sentido de entender frases, es un desafío abierto. Para abordar este desafío, presentamos el marco de evaluación automático \"Sentient Agent as a Judge\" (SAGE). Este marco mide el alto nivel de cognición social de los LLM. SAGE se esfuerza en que el agente se comporte como un ser humano, simulando cambios emocionales y pensamientos internos, y evalúa el modelo de manera más realista a través de múltiples conversaciones. En cada turno, el agente proporciona una trayectoria emocional numérica y unas pensamientos internos interpretables basados en cómo cambia la emoción, cómo se siente y cómo responde. Los experimentos en escenarios de 100 conversaciones mostraron que el puntaje final de emoción sentiente tiene una fuerte correlación con los puntajes del Inventario de Relaciones de Barrett-Lennard (BLRI) y con el nivel de emociones de un ser humano, demostrando la confianza psicológica. Además, se ha construido un laboratorio abierto que cubre 18 modelos comerciales y de código abierto, revelando una gran diferencia entre los modelos más recientes (como GPT-4o-Latest y Gemini2.5-Pro) y los modelos iniciales (máximo 4 veces). Esta diferencia no se refleja en los experimentos comunes (por ejemplo, Arena). SAGE proporciona una herramienta principiante, expandible y interpretable necesaria para seguir el desarrollo de agentes de lenguaje emocional y socialmente excelentes.",
      "upvotes": 15,
      "discussionId": "681d7033e9969eecfcb4ec2d",
      "githubRepo": "https://github.com/Tencent/digitalhuman/tree/main/SAGE",
      "ai_keywords": [
        "Sentient Agent as a Judge (SAGE)",
        "higher-order social cognition",
        "emotional changes",
        "inner thoughts",
        "multi-turn conversations",
        "numerical emotion trajectory",
        "Barrett-Lennard Relationship Inventory (BLRI)",
        "utterance-level empathy metrics",
        "psychological fidelity",
        "Sentient Leaderboard",
        "empathetic",
        "socially adept language agents"
      ]
    },
    "publishedAt": "2025-05-01T15:06:10.000Z",
    "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models",
    "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02847.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "648294b2eb4befee378951c1",
      "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
      "fullname": "Ruotian Ma",
      "name": "vvibt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05315",
      "authors": [
        {
          "_id": "681d7ccb572e742b3f42d1f3",
          "user": {
            "_id": "6602869253a0518b2a98cafd",
            "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
            "isPro": false,
            "fullname": "Yuhui Xu",
            "user": "yuhuixu",
            "type": "user"
          },
          "name": "Yuhui Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:04.644Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f4",
          "user": {
            "_id": "63a3ff69f91ad3ea5703841d",
            "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
            "isPro": false,
            "fullname": "Hanze Dong",
            "user": "hendrydong",
            "type": "user"
          },
          "name": "Hanze Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:10.829Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f5",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f6",
          "user": {
            "_id": "65f84fd980481173afd91233",
            "avatarUrl": "/avatars/6ac7bd6beba24d1476c5179b88c9e3fa.svg",
            "isPro": false,
            "fullname": "Doyen",
            "user": "doyensahoo",
            "type": "user"
          },
          "name": "Doyen Sahoo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:18.676Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f7",
          "user": {
            "_id": "61f9d3b54ac99e8a1bae85f4",
            "avatarUrl": "/avatars/ac47d13204dd22452e4bc46e280842d5.svg",
            "isPro": false,
            "fullname": "JunnanLi",
            "user": "JunnanLi",
            "type": "user"
          },
          "name": "Junnan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:32.272Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f8",
          "user": {
            "_id": "649dbcc4e0fff1ed099dc80a",
            "avatarUrl": "/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg",
            "isPro": false,
            "fullname": "Caiming Xiong",
            "user": "cxiong",
            "type": "user"
          },
          "name": "Caiming Xiong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:38.430Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T15:01:06.000Z",
      "submittedOnDailyAt": "2025-05-09T02:31:21.542Z",
      "title": "Scalable Continous Scope by Elesford Resizing",
      "submittedOnDailyBy": {
        "_id": "6602869253a0518b2a98cafd",
        "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
        "isPro": false,
        "fullname": "Yuhui Xu",
        "user": "yuhuixu",
        "type": "user"
      },
      "summary": "Los modelos de lógica general (LRMs) han logrado impresionantes avances al generar largas cadenas de razonamiento (CoT) en tareas complejas. Sin embargo, la longitud ilimitada de los outputs puede resultar en problemas significativos en implementaciones reales donde los tokens, la lengua latina o los métodos de razonamiento computacional estén estrictamente limitados. Proponemos un nuevo marco de trabajo para razonamientos largos escalables. Este marco utiliza un enfoque de distribución independiente y distingue claramente entre dos etapas: \"pensar\" y \"resolver\". En los tests, Elastic Reasoning prioriza la completitud de la sección de soluciones, mejorando significativamente la confianza bajo estrictas restricciones de recursos. Para entrenar modelos robustos en procesos de pensamiento duraderos, introducimos una estructura ligera y robusta basada en GRPO. Esta estructura se ajusta a la brevedad del proceso de pensamiento, enseñando al modelo a presentar las razones más adecuadas y generalizando efectivamente a nuevas restricciones de formación, sin necesidad de entrenamiento adicional. Los resultados en los benchmarks de matemáticas (AIME, MATH500) y programación (LiveCodeBench, Codeforces) muestran que Elastic Reasoning funciona robustamente bajo estrictas restricciones de formación y requiere menos costo de entrenamiento que el método base. En particular, nuestro enfoque genera razones más claras y eficientes en entornos sin restricciones. Elastic Reasoning proporciona una solución lógica y práctica para los problemas urgentes de razonamiento controlable y escalable.",
      "upvotes": 13,
      "discussionId": "681d7ccc572e742b3f42d21a",
      "ai_keywords": [
        "Large reasoning models (LRMs)",
        "chain of thought (CoT)",
        "inference-time budgets",
        "tokens",
        "latency",
        "compute",
        "Elastic Reasoning",
        "scalable chain of thoughts",
        "thinking phase",
        "solution phase",
        "independently allocated budgets",
        "completeness of solution segments",
        "reliability",
        "resource constraints",
        "lightweight budget-constrained rollout strategy",
        "GRPO",
        "adaptive reasoning",
        "unseen budget constraints",
        "mathematical benchmarks (AIME, MATH500)",
        "programming benchmarks (LiveCodeBench, Codeforces)",
        "unconstrained settings",
        "principled solution"
      ]
    },
    "publishedAt": "2025-05-08T11:01:06.000Z",
    "title": "Scalable Chain of Thoughts via Elastic Reasoning",
    "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602869253a0518b2a98cafd",
      "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
      "fullname": "Yuhui Xu",
      "name": "yuhuixu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05071",
      "authors": [
        {
          "_id": "681da6375f701833274a0d21",
          "user": {
            "_id": "6621e591c50869c1e91a1639",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621e591c50869c1e91a1639/L_PoEn2BRAJcnWZX-JebR.jpeg",
            "isPro": false,
            "fullname": "Chunyu Xie",
            "user": "xiechunyu",
            "type": "user"
          },
          "name": "Chunyu Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:03.296Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d22",
          "user": {
            "_id": "5e49e8cf37cb5b49818287ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e49e8cf37cb5b49818287ae/IV9b5Z70NhgmBNfAlc_co.jpeg",
            "isPro": false,
            "fullname": "Bin Wang",
            "user": "binwang",
            "type": "user"
          },
          "name": "Bin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:33.968Z",
          "hidden": true
        },
        {
          "_id": "681da6375f701833274a0d23",
          "user": {
            "_id": "632c098b456c31252774e7c5",
            "avatarUrl": "/avatars/e3720d2fcb69d93c8d5aa5f50aab5f0e.svg",
            "isPro": false,
            "fullname": "kong",
            "user": "fanjing",
            "type": "user"
          },
          "name": "Fanjing Kong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:42.881Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d24",
          "user": {
            "_id": "65b793b374698ba5a815bf4f",
            "avatarUrl": "/avatars/44a7e694a5089dbc773018111270ac26.svg",
            "isPro": false,
            "fullname": "Jincheng Li",
            "user": "jinchenglijc",
            "type": "user"
          },
          "name": "Jincheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:50.538Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d25",
          "user": {
            "_id": "659b8576999b82db2ad8a398",
            "avatarUrl": "/avatars/2ec7663e25e4a0238819818e69d9a5bd.svg",
            "isPro": false,
            "fullname": "Liang",
            "user": "DaweiLiang",
            "type": "user"
          },
          "name": "Dawei Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:56.445Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d26",
          "name": "Gengshen Zhang",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d27",
          "user": {
            "_id": "649935abbe8fd92c27ab1ed8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
            "isPro": false,
            "fullname": "David Leon",
            "user": "DavidLeon",
            "type": "user"
          },
          "name": "Dawei Leng",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-09T07:00:10.249Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d28",
          "name": "Yuhui Yin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T09:06:53.000Z",
      "submittedOnDailyAt": "2025-05-09T05:27:38.509Z",
      "title": "FG-CLIP: Detalleados arreglos visuales y textuales",
      "submittedOnDailyBy": {
        "_id": "649935abbe8fd92c27ab1ed8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
        "isPro": false,
        "fullname": "David Leon",
        "user": "DavidLeon",
        "type": "user"
      },
      "summary": "Contrastive Language-Image Pre-training (CLIP) muestra excelente rendimiento en tareas como búsqueda de imágenes-texto y clasificación sin ejemplos, pero opera principalmente en base a breves captiones de oración clave, lo que no es adecuado para un entendimiento detallado. Para resolver estos problemas, proponemos Fine-Grained CLIP (FG-CLIP). FG-CLIP fortalece el entendimiento detallado a través de tres innovaciones clave. Primero, utiliza modelos de diversificación a gran escala para generar 16 mil millones de pares de captiones largas e imágenes, permitiendo la comprensión de detalles significativos a nivel global. Luego, construye un alto rendimiento dataset que incluye 12 millones de imágenes y 40 millones de parches con bounding boxes, asegurando expresiones precisas con detalles y contexto. Finalmente, agrega 10 millones de pares de muestras negativas difíciles para mejorar la capacidad del modelo de distinguir diferencias significativas. Además, se ha diseñado un método de entrenamiento para adaptarse a estos datos. Experimentaciones ampliadas muestran que FG-CLIP supera a CLIP original y a otros métodos de vanguardia en tareas de entendimiento detallado, detección de objetos abiertos, búsqueda de imágenes-texto y benchmark de diversificación general. Estos resultados demuestran claramente que FG-CLIP mejora la comprensión de detalles en imágenes y el rendimiento general del modelo. Los datos, código y modelos están disponibles en https://github.com/360CVGroup/FG-CLIP.",
      "upvotes": 8,
      "discussionId": "681da6385f701833274a0d8a",
      "githubRepo": "https://github.com/360CVGroup/FG-CLIP",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training (CLIP)",
        "image-text retrieval",
        "zero-shot classification",
        "fine-grained understanding",
        "coarse-grained short captions",
        "multimodal models",
        "1.6 billion long caption-image pairs",
        "high-quality dataset",
        "12 million images",
        "40 million region-specific bounding boxes",
        "detailed captions",
        "10 million hard fine-grained negative samples",
        "fine-grained understanding",
        "open-vocabulary object detection",
        "general multimodal benchmarks",
        "FG-CLIP"
      ]
    },
    "publishedAt": "2025-05-08T05:06:53.000Z",
    "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment",
    "summary": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05071.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649935abbe8fd92c27ab1ed8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
      "fullname": "David Leon",
      "name": "DavidLeon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05474",
      "authors": [
        {
          "_id": "681d7b5ae27a030c96a28bde",
          "user": {
            "_id": "672392c4a4c4381cefc06416",
            "avatarUrl": "/avatars/8ee84a7e3e91e5d13074bc3c407ff75d.svg",
            "isPro": false,
            "fullname": "Wen Beichen",
            "user": "wenbc21",
            "type": "user"
          },
          "name": "Beichen Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:55.900Z",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28bdf",
          "user": {
            "_id": "63f47b5321eb234ab739e91a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
            "isPro": false,
            "fullname": "Haozhe Xie",
            "user": "hzxie",
            "type": "user"
          },
          "name": "Haozhe Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:02.341Z",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be0",
          "user": {
            "_id": "62fc8cf7ee999004b5a8b982",
            "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
            "isPro": false,
            "fullname": "Zhaoxi Chen",
            "user": "FrozenBurning",
            "type": "user"
          },
          "name": "Zhaoxi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:08.543Z",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be1",
          "name": "Fangzhou Hong",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be2",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:27.473Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/PqIosa0nVWamlNxCmd6w1.webp"
      ],
      "publishedAt": "2025-05-08T17:59:54.000Z",
      "submittedOnDailyAt": "2025-05-09T02:21:54.023Z",
      "title": "3D Mecánica de Dispositivos: Resumen",
      "submittedOnDailyBy": {
        "_id": "63f47b5321eb234ab739e91a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
        "isPro": false,
        "fullname": "Haozhe Xie",
        "user": "hzxie",
        "type": "user"
      },
      "summary": "La generación de escenas 3D es un objetivo en aplicaciones como medios de consumo completo, robotería, conducción automática y inteligencia concreta, que busca la estructuración espacial, el significado y la síntesis de ambientes realistas. Los métodos basados en la secuencia de procesamiento temprana eran escalables pero limitaban la diversidad. Los avances recientes en modelos de generación profunda (como GAN y modelos diferenciales) y en representaciones 3D (como NeRF y 3D gaussianas) han permitido aprender la distribución de escenas reales, mejorando la fidelidad, la diversidad y la consistencia de las perspectivas. Recientemente, se han reconstruido los problemas de síntesis de imágenes o vídeo para asociar la generación con la realismo, a través de la funcionalidad de modelos diferenciales. Esta investigación proporciona una resumen sistemático de los métodos más avanzados, organizándolos en cuatro paradigmas: generación secuencial, generación basada en redes neuronales 3D, generación basada en imágenes y generación basada en vídeo. Se analizan las bases técnicas, los trade-offs, los resultados representativos y se presentan conjuntos de datos comunes, protocolos de evaluación y aplicaciones en línea. Finalmente, se discuten los principales desafíos relacionados con la capacidad de generación, la representación 3D, los datos y los comentarios, y se destacan direcciones prometedoras como modelos de generación con alta fidelidad, conocimiento físico y interacción, y modelos de reconocimiento de unidades. Esta revisión colecciona los avances recientes en el cruce de la inteligencia generativa, la visión 3D y la inteligencia concreta, y destaca las direcciones prometedoras en este cruce. Para seguir los avances, se mantiene el repositorio de proyectos más recientes: https://github.com/hzxie/Awesome-3D-Scene-Generation.",
      "upvotes": 7,
      "discussionId": "681d7b5be27a030c96a28c29",
      "githubRepo": "https://github.com/hzxie/Awesome-3D-Scene-Generation",
      "ai_keywords": [
        "deep generative models",
        "GANs",
        "diffusion models",
        "NeRF",
        "3D Gaussians",
        "procedural generation",
        "neural 3D-based generation",
        "image-based generation",
        "video-based generation"
      ]
    },
    "publishedAt": "2025-05-08T13:59:54.000Z",
    "title": "3D Scene Generation: A Survey",
    "summary": "3D scene generation seeks to synthesize spatially structured, semantically\nmeaningful, and photorealistic environments for applications such as immersive\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\nprocedural rules offered scalability but limited diversity. Recent advances in\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\ndistributions, improving fidelity, diversity, and view consistency. Recent\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\nreframing generation as image or video synthesis problems. This survey provides\na systematic overview of state-of-the-art approaches, organizing them into four\nparadigms: procedural generation, neural 3D-based generation, image-based\ngeneration, and video-based generation. We analyze their technical foundations,\ntrade-offs, and representative results, and review commonly used datasets,\nevaluation protocols, and downstream applications. We conclude by discussing\nkey challenges in generation capacity, 3D representation, data and annotations,\nand evaluation, and outline promising directions including higher fidelity,\nphysics-aware and interactive generation, and unified perception-generation\nmodels. This review organizes recent advances in 3D scene generation and\nhighlights promising directions at the intersection of generative AI, 3D\nvision, and embodied intelligence. To track ongoing developments, we maintain\nan up-to-date project page:\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/PqIosa0nVWamlNxCmd6w1.webp"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05474.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f47b5321eb234ab739e91a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
      "fullname": "Haozhe Xie",
      "name": "hzxie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05327",
      "authors": [
        {
          "_id": "681d95bc11abe59dc97e4c5a",
          "user": {
            "_id": "647e99d9becb41a272970ca4",
            "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
            "isPro": false,
            "fullname": "Ann",
            "user": "yyxsghx",
            "type": "user"
          },
          "name": "Yixin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:18.753Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5b",
          "user": {
            "_id": "670740744341dcee459fb990",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/66UkZvrAk7fQr5YCylEFk.png",
            "isPro": false,
            "fullname": "Qingxiu Dong",
            "user": "Rsy24",
            "type": "user"
          },
          "name": "Qingxiu Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:38.844Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5c",
          "user": {
            "_id": "655ca347f426a304c6b393a1",
            "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
            "isPro": false,
            "fullname": "Linli Yao",
            "user": "yaolily",
            "type": "user"
          },
          "name": "Linli Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:47.561Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5d",
          "user": {
            "_id": "654cca3fe1b4cd6d40d5a7ae",
            "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
            "isPro": false,
            "fullname": "Fangwei Zhu",
            "user": "soliz1998",
            "type": "user"
          },
          "name": "Fangwei Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:53.796Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5e",
          "name": "Zhifang Sui",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T15:17:37.000Z",
      "submittedOnDailyAt": "2025-05-09T07:13:58.961Z",
      "title": "ICon: Contribución de la selección automática de datos en el contexto",
      "submittedOnDailyBy": {
        "_id": "647e99d9becb41a272970ca4",
        "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
        "isPro": false,
        "fullname": "Ann",
        "user": "yyxsghx",
        "type": "user"
      },
      "summary": "La selección de datos es crucial para mejorar el rendimiento y reducir los costos de entrenamiento de modelos de lenguaje grandes (LLMs). Sin embargo, los métodos automatizados actuales dependen de métricas de gradiente basadas en cálculos costosos o de heurísticas manualmente diseñadas, lo que no permiten utilizar completamente las características únicas del datoset. En este artículo, se propone una nueva metodología sin gradientes llamada \"In-context Learning for Contribution Measurement\" (ICon). Esta metodología explota las potenciales ajustes de aprendizaje en contexto (ICL) para evaluar la contribución de muestras de manera que no necesite calcular gradientes ni diseñar manualmente manuales manuales. ICon es más eficiente computacionalmente que métodos basados en gradientes y reduce la sesgo de inferencia humano comparado con enfoques basados en heurísticas. ICon está compuesto de tres componentes: evaluación del cambio en el rendimiento mediante aprendizaje en contexto para identificar datos de alta contribución. Se demostró la eficacia de ICon en experimentos extendidos con 3 modelos LLMs, 12 benchmarks y 5 par de conjuntos, donde el modelo entrenado con 15% de los datos seleccionados por ICon en LLaMA3.1-8B superó el rendimiento del modelo entrenado con el conjunto completo en un 5.42% puntos y al mejor rendimiento obtenido con métodos comunes en un 2.06% puntos. Además, se realizó un análisis de los muestras de alta contribución seleccionadas por ICon, que muestran una variedad de tareas y niveles de dificultad, y son los únicos que pueden realizar tareas que son imposibles para los demás.",
      "upvotes": 7,
      "discussionId": "681d95bd11abe59dc97e4c87",
      "ai_keywords": [
        "in-context learning (ICL)",
        "implicit fine-tuning",
        "In-context Learning for Contribution Measurement (ICon)"
      ]
    },
    "publishedAt": "2025-05-08T11:17:37.000Z",
    "title": "ICon: In-Context Contribution for Automatic Data Selection",
    "summary": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05327.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647e99d9becb41a272970ca4",
      "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
      "fullname": "Ann",
      "name": "yyxsghx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03981",
      "authors": [
        {
          "_id": "681d7ee755699177c7fb636a",
          "user": {
            "_id": "617e7729129c9e67703ffe61",
            "avatarUrl": "/avatars/f47ee9f2f0e2b1075bebf3682ee2f817.svg",
            "isPro": false,
            "fullname": "qianchu liu",
            "user": "qianchu",
            "type": "user"
          },
          "name": "Qianchu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:18.501Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636b",
          "user": {
            "_id": "6234c11b7d5de9839bc44163",
            "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
            "isPro": false,
            "fullname": "Sheng Zhang",
            "user": "shengz",
            "type": "user"
          },
          "name": "Sheng Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:22.974Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636c",
          "user": {
            "_id": "64b8e41d52b7353d8c6dd38f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IAItP4FvD6JX9s1jwnQwF.png",
            "isPro": false,
            "fullname": "Guanghui Qin",
            "user": "hiaoxui",
            "type": "user"
          },
          "name": "Guanghui Qin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:24.320Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636d",
          "name": "Timothy Ossowski",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636e",
          "name": "Yu Gu",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636f",
          "name": "Ying Jin",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6370",
          "user": {
            "_id": "627bd86f7e62b4bf5c367108",
            "avatarUrl": "/avatars/4e87eea02d51680ebac7992dfe527e07.svg",
            "isPro": false,
            "fullname": "Sid Kiblawi",
            "user": "sidkiblawi",
            "type": "user"
          },
          "name": "Sid Kiblawi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:37.798Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6371",
          "user": {
            "_id": "65a13da85dce70a3025b7534",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zSPDBEGIULEYN4P7JCdyC.png",
            "isPro": false,
            "fullname": "Sam Preston",
            "user": "RustyArchimedes",
            "type": "user"
          },
          "name": "Sam Preston",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:43.654Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6372",
          "name": "Mu Wei",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6373",
          "user": {
            "_id": "6797f24ded1557b14d708541",
            "avatarUrl": "/avatars/d69ac80a9a500764766ce9ac7d549cc2.svg",
            "isPro": false,
            "fullname": "Paul Vozila",
            "user": "Paulvozila",
            "type": "user"
          },
          "name": "Paul Vozila",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:01.673Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6374",
          "user": {
            "_id": "5e5870466bc35159a08ca572",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e5870466bc35159a08ca572/pT6gEEs8RLRJGeM-faNWj.jpeg",
            "isPro": false,
            "fullname": "Tristan Naumann",
            "user": "tnaumann",
            "type": "user"
          },
          "name": "Tristan Naumann",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:15.476Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6375",
          "user": {
            "_id": "664d07456083f276c4feb1a4",
            "avatarUrl": "/avatars/1bfa6d8f82e9223b47630cefd79d7d0e.svg",
            "isPro": false,
            "fullname": "Hoifung Poon",
            "user": "hoifung",
            "type": "user"
          },
          "name": "Hoifung Poon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:22.468Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T21:08:27.000Z",
      "submittedOnDailyAt": "2025-05-09T02:41:23.534Z",
      "title": "X-Reasoner: Dirección general para la lógica general en diversos modelos y campos.",
      "submittedOnDailyBy": {
        "_id": "6234c11b7d5de9839bc44163",
        "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
        "isPro": false,
        "fullname": "Sheng Zhang",
        "user": "shengz",
        "type": "user"
      },
      "summary": "Los modelos de propiedad recientes (por ejemplo, o3) están desarrollando una capacidad lógica multimodal muy potente. Sin embargo, los estudios abiertos actuales se centran principalmente en entrenar modelos lógicos con textos de párrafos y evaluar estos modelos principalmente en tareas matemáticas y generales. Por lo tanto, no se ha claramente identificado un método efectivo para ampliar las capacidades lógicas generales que superen los textos de párrafos y las áreas generales. En este artículo, investigamos los problemas básicos fundamentales: ¿es la lógica generalizable tanto en el modelo como en el dominio? Nuestro hallazgo apoya una respuesta positiva: el programación basada en textos de párrafos en dominios generales permite esta lógica generalizable fuerte. Utilizando esta descubrimiento, presentamos X-Reasoner. X-Reasoner es un modelo de lenguaje visual programado únicamente para textos de párrafos en dominios generales, que realiza una lógica generalizable mediante dos etapas: primero, el aprendizaje de experiencias en el dominio final y, luego, el entrenamiento final con aprendizaje por refuerzo que evalúa la recompensa. Los experimentos muestran que X-Reasoner supera a los modelos actuales en lógica visual y en dominios generales, y excede a los mejores modelos en los benchmarks generales y médicos (figura 1). Además, el rendimiento de X-Reasoner en dominios específicos se mejora a través de entrenamiento continuo con datos de textos de párrafos propioses. Por lo tanto, presentamos X-Reasoner-Med. X-Reasoner-Med supera a los modelos generales y visuales entrenados, al alcanzar un nuevo modelo óptimo con datos de entrenamiento.",
      "upvotes": 5,
      "discussionId": "681d7ee855699177c7fb63b7",
      "projectPage": "https://github.com/microsoft/x-reasoner",
      "ai_keywords": [
        "multimodal reasoning",
        "vision-language model",
        "post-training",
        "long chain-of-thoughts",
        "reinforcement learning",
        "verifiable rewards",
        "X-Reasoner",
        "out-of-domain settings",
        "X-Reasoner-Med"
      ]
    },
    "publishedAt": "2025-05-06T17:08:27.000Z",
    "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains",
    "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03981.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6234c11b7d5de9839bc44163",
      "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
      "fullname": "Sheng Zhang",
      "name": "shengz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05467",
      "authors": [
        {
          "_id": "681d95b5c7ae5f65b0e55ff9",
          "user": {
            "_id": "63fee47352441fe3e87b5088",
            "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
            "isPro": false,
            "fullname": "WANG HAIBO",
            "user": "WHB139426",
            "type": "user"
          },
          "name": "Haibo Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:20.829Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffa",
          "name": "Bo Feng",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffb",
          "user": {
            "_id": "66b5295f83425904fa7a1a6a",
            "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
            "isPro": false,
            "fullname": "Zhengfeng Lai",
            "user": "jefflai",
            "type": "user"
          },
          "name": "Zhengfeng Lai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:45.264Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffc",
          "name": "Mingze Xu",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffd",
          "user": {
            "_id": "67fa856547b40f55b7ff3ce5",
            "avatarUrl": "/avatars/745937497772e9b533ba7940d758d30d.svg",
            "isPro": false,
            "fullname": "Shiyu Li",
            "user": "ShiyuLi",
            "type": "user"
          },
          "name": "Shiyu Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:01:08.366Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffe",
          "name": "Weifeng Ge",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55fff",
          "user": {
            "_id": "66fc2377516eaf950d4b8209",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mcUxUxXy18Gv9KvCW23s0.png",
            "isPro": false,
            "fullname": "Afshin Dehghan",
            "user": "afshindn",
            "type": "user"
          },
          "name": "Afshin Dehghan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:01:21.716Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e56000",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e56001",
          "name": "Ping Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T17:57:40.000Z",
      "submittedOnDailyAt": "2025-05-09T04:13:07.003Z",
      "title": "Streaming Bridge: Converting Regional Video to Language Models into Active Streaming Assistants",
      "submittedOnDailyBy": {
        "_id": "63fee47352441fe3e87b5088",
        "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
        "isPro": false,
        "fullname": "WANG HAIBO",
        "user": "WHB139426",
        "type": "user"
      },
      "summary": "StreamBridge es un marco sencillo y efectivo. Este marco permite transformar fácilmente los Video-LLMs en líneas en modelos dinámicos. StreamBridge resuelve dos principales problemas que surgen al aplicar modelos existentes en escenarios en línea: (1) la limitación de la capacidad para entender en unidades de tiempo múltiples y (2) la falta de estructuras de respuesta dinámicas. Especialmente, StreamBridge combina (1) un buffer de memoria y una estrategia de síntesis de disipación armónica para apoyar la interacción de múltiples turnos en secuencias largas y (2) utiliza modelos activos ligeros separados, diseñados para integrarse fácilmente con los Video-LLMs existentes, permitiendo respuestas dinámicas continuas. Además, StreamBridge ha construido Stream-IT, un conjunto de datos más grande y adecuado para la comprensión dinámica de videos, que caracteriza a través de secuencias de video-texto aleatorias y diferentes formatos de instrucción. A través de experimentos expandidos, StreamBridge ha notablemente mejorado la capacidad de comprensión dinámica de los Video-LLMs en línea, demostrando desempeños superiores a modelos propietarios como GPT-4o y Gemini 1.5 Pro. Al mismo tiempo, ha mostrado un desempeño relativamente alto en marcos de referencia estándares de comprensión de videos.",
      "upvotes": 4,
      "discussionId": "681d95b6c7ae5f65b0e5606c",
      "ai_keywords": [
        "Video-LLMs",
        "streaming-capable models",
        "multi-turn real-time understanding",
        "proactive response mechanisms",
        "memory buffer",
        "round-decayed compression strategy",
        "long-context multi-turn interactions",
        "decoupled activation model",
        "Stream-IT",
        "interleaved video-text sequences",
        "standard video understanding benchmarks",
        "GPT-4o",
        "Gemini 1.5 Pro"
      ]
    },
    "publishedAt": "2025-05-08T13:57:40.000Z",
    "title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant",
    "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05467.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fee47352441fe3e87b5088",
      "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
      "fullname": "WANG HAIBO",
      "name": "WHB139426",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05469",
      "authors": [
        {
          "_id": "681db3a8a9286b53a51dc77b",
          "user": {
            "_id": "672403d5f328a3e6638331ee",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/TXr0SKWI-z-6FvUXTNWXT.jpeg",
            "isPro": false,
            "fullname": "Ava Pun",
            "user": "AvaLovelace",
            "type": "user"
          },
          "name": "Ava Pun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:33.422Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77c",
          "user": {
            "_id": "645d34ecce72244df7b29317",
            "avatarUrl": "/avatars/1248933d9f89a15e67086325a8322d5e.svg",
            "isPro": false,
            "fullname": "Kangle Deng",
            "user": "kangled",
            "type": "user"
          },
          "name": "Kangle Deng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:39.736Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77d",
          "user": {
            "_id": "658b307b75ddc76f9dc747ca",
            "avatarUrl": "/avatars/fc5393dc0bb33a8c0fea3a6f79640386.svg",
            "isPro": false,
            "fullname": "Ruixuan Liu",
            "user": "RLCMU",
            "type": "user"
          },
          "name": "Ruixuan Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:45.442Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77e",
          "user": {
            "_id": "6337151b0267ebcf02640eb6",
            "avatarUrl": "/avatars/14a723cafc5587043bdfb19304fc202d.svg",
            "isPro": false,
            "fullname": "Deva Ramanan",
            "user": "devakramanan",
            "type": "user"
          },
          "name": "Deva Ramanan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:52.637Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77f",
          "name": "Changliu Liu",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc780",
          "user": {
            "_id": "63a0acc32fabbbb899952a2b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671474335794-noauth.jpeg",
            "isPro": false,
            "fullname": "Jun-Yan Zhu",
            "user": "junyanz",
            "type": "user"
          },
          "name": "Jun-Yan Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:07:07.095Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T17:58:18.000Z",
      "submittedOnDailyAt": "2025-05-09T06:30:13.597Z",
      "title": "Diseños físicamente estables y construibles generados a partir del texto",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "REGOGPT presenta el primer enfoque para generar un modelo de negro de LEGO fisicamente estable a partir de una frase. Para lograrlo, construye un grande conjunto de datos fisicamente estable de diseños de LEGO, y entrena un gran modelo de lenguaje de auto-recuperación mediante la predicción de los siguientes tokens junto con capturas relacionadas. Para mejorar la estabilidad de diseños, utiliza una eficiente verificación de validez y un retroceso basado en conocimientos físicos durante la inferencia de auto-recuperación, y elimina predicciones de tokens imprevisibles utilizando las leyes físicas y restricciones de asamblaje. Los experimentos muestran que REGOGPT genera diseños estables, diversos y hermosos de LEGO cercanos al texto de entrada. Además, desarrolla un método para generar texto que permita la auto-asamble de diseños de LEGO por personas o manos de robot. También se lanza un nuevo conjunto de datos \"StableText2Lego\", que incluye más de 47,000 estructuras de LEGO y más de 28,000 objetos 3D, junto con capturas detalladas y códigos disponibles en el sitio web de proyecto.",
      "upvotes": 3,
      "discussionId": "681db3aca9286b53a51dc875",
      "ai_keywords": [
        "autoregressive large language model",
        "next-token prediction",
        "validity check",
        "physics-aware rollback",
        "autoregressive inference",
        "physics laws",
        "assembly constraints",
        "text-based LEGO texturing method",
        "automatic assembly",
        "robotic arms"
      ]
    },
    "publishedAt": "2025-05-08T13:58:18.000Z",
    "title": "Generating Physically Stable and Buildable LEGO Designs from Text",
    "summary": "We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05469.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6796
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.05408",
      "authors": [
        {
          "_id": "681daba2e3775056736651ce",
          "user": {
            "_id": "61424bf4f0d914a5f606a823",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
            "isPro": false,
            "fullname": "Yong Zheng-Xin",
            "user": "yongzx",
            "type": "user"
          },
          "name": "Zheng-Xin Yong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:20:45.630Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651cf",
          "name": "M. Farid Adilazuarda",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d0",
          "user": {
            "_id": "6509feb92257a3afbaeecfea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6509feb92257a3afbaeecfea/a_UbA-2WtZeLTf0ugVzSh.jpeg",
            "isPro": false,
            "fullname": "Jonibek Mansurov",
            "user": "MJonibek",
            "type": "user"
          },
          "name": "Jonibek Mansurov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:03:39.843Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d1",
          "name": "Ruochen Zhang",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d2",
          "user": {
            "_id": "5f1eb362eec0ad2a071ad6e2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
            "isPro": false,
            "fullname": "Niklas Muennighoff",
            "user": "Muennighoff",
            "type": "user"
          },
          "name": "Niklas Muennighoff",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:00.900Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d3",
          "name": "Carsten Eickhoff",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d4",
          "user": {
            "_id": "5f5c4b20e56d546cd6233098",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
            "isPro": false,
            "fullname": "Genta Indra Winata",
            "user": "gentaiscool",
            "type": "user"
          },
          "name": "Genta Indra Winata",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:13.878Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d5",
          "user": {
            "_id": "6544e43b12da508864c38f96",
            "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
            "isPro": false,
            "fullname": "Julia Kreutzer",
            "user": "JuliaKreutzerCohere",
            "type": "user"
          },
          "name": "Julia Kreutzer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:29.257Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d6",
          "name": "Stephen H. Bach",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d7",
          "name": "Alham Fikri Aji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T16:50:06.000Z",
      "submittedOnDailyAt": "2025-05-09T05:46:57.523Z",
      "title": "La lógica de lenguajes crónica se logra a través del escalado en el proceso de verificación.",
      "submittedOnDailyBy": {
        "_id": "61424bf4f0d914a5f606a823",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
        "isPro": false,
        "fullname": "Yong Zheng-Xin",
        "user": "yongzx",
        "type": "user"
      },
      "summary": "El poder lógico de los modelos de lenguaje general es principalmente estudiado en inglés y se ha expandido en función de la diversidad lingüística y la utilización de modelos pretrenados. En este estudio, se investiga en qué medida la regulación lógica de largo CoT en inglés se generaliza en otros idiomas. Primero, se expandió el cálculo de inferencia de los modelos de lenguaje lógico centrados en inglés (RLM) para mostrar excelentes resultados en lógica matemática y diversas lenguas, especialmente en lenguas de bajo recurso. Además, los CoTs centrados en inglés naturalmente ocupan un papel principal, pero cuando se procesa lógicamente el inglés en entradas no inglesas, se observan patrones continuos de pensamiento que se asocian con referencias. Además, se descubrió una estrategia efectiva para controlar los largos CoT, lo que permite que los modelos procesen lógicamente de manera más eficiente en lenguas ricas en recursos. Finalmente, en particular, la generalización lógica hacia el conocimiento cultural común en las áreas de ciencia, tecnología, matemáticas (STEM) es deficiente, y este fenómeno se observa también en inglés. En resumen, se presentan la posibilidad, la estructura y los límites de la generalización lógica entre idiomas en pruebas de inglés, se recomienda el uso de RLM centrados en inglés para el procesamiento lógico en lenguas ricas en recursos, y se concluye que los RLM deben evolucionar para mejorar el procesamiento lógico en lenguas de bajo recurso y en situaciones extranjeras.",
      "upvotes": 3,
      "discussionId": "681daba2e3775056736651f9",
      "ai_keywords": [
        "reasoning language models (RLMs)",
        "long chain-of-thoughts (CoTs)",
        "multilingual mathematical reasoning",
        "low-resource languages",
        "quote-and-think pattern",
        "scaling up inference compute",
        "high-resource languages",
        "out-of-domain reasoning generalization",
        "STEM",
        "cultural commonsense knowledge",
        "crosslingual generalization",
        "test-time scaling"
      ]
    },
    "publishedAt": "2025-05-08T12:50:06.000Z",
    "title": "Crosslingual Reasoning through Test-Time Scaling",
    "summary": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05408.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61424bf4f0d914a5f606a823",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
      "fullname": "Yong Zheng-Xin",
      "name": "yongzx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.19314",
      "authors": [
        {
          "_id": "681d89e6d025518b321f67ce",
          "user": {
            "_id": "6673cf668d570d59b83511cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
            "isPro": false,
            "fullname": "Peilin Zhou",
            "user": "PALIN2018",
            "type": "user"
          },
          "name": "Peilin Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:01:42.666Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67cf",
          "name": "Bruce Leon",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d0",
          "user": {
            "_id": "64489ca21d52a633c8f55aba",
            "avatarUrl": "/avatars/5199a5e93161c61d14ec13f79dd8c2c5.svg",
            "isPro": false,
            "fullname": "Xiang Ying",
            "user": "MindYing",
            "type": "user"
          },
          "name": "Xiang Ying",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-09T04:51:51.507Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d1",
          "name": "Can Zhang",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d2",
          "name": "Yifan Shao",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d3",
          "user": {
            "_id": "636dfa6193d9a0c987d41b73",
            "avatarUrl": "/avatars/14396c8beb376b0d3c27a23fadaeb15e.svg",
            "isPro": false,
            "fullname": "Qichen YE",
            "user": "yeeeqichen99",
            "type": "user"
          },
          "name": "Qichen Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:02:33.873Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d4",
          "name": "Dading Chong",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d5",
          "user": {
            "_id": "66bc683f432c73a183ef787c",
            "avatarUrl": "/avatars/9505a1e6131093a91d0454e50bcbba00.svg",
            "isPro": false,
            "fullname": "Zhiling Jin",
            "user": "HawkFaust",
            "type": "user"
          },
          "name": "Zhiling Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:02:48.180Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d6",
          "name": "Chenxuan Xie",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d7",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d8",
          "name": "Yuxin Gu",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d9",
          "name": "Sixin Hong",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67da",
          "name": "Jing Ren",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67db",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67dc",
          "name": "Chao Liu",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67dd",
          "name": "Yining Hua",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/NJsFys1PlInj9W3E7pD8t.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/gJJ8b6rc6njgCz7Fepbbd.png"
      ],
      "publishedAt": "2025-04-27T17:32:43.000Z",
      "submittedOnDailyAt": "2025-05-09T03:24:22.739Z",
      "title": "Broadway Competition-ZH: Benchmark de Broadway del Modelo de Lenguaje Grande Chino",
      "submittedOnDailyBy": {
        "_id": "6673cf668d570d59b83511cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
        "isPro": false,
        "fullname": "Peilin Zhou",
        "user": "PALIN2018",
        "type": "user"
      },
      "summary": "El programa devuelve respuestas traducidas al idioma especificado por el usuario. En este caso, la traducción del inglés al japonés sería:\n\n「Cuando las herramientas de uso de la IA avanzan hacia la evolución de agentes, la capacidad de explorar el web por intervalos de tiempo como un navegador es un indicador crucial para evaluar sus habilidades lógicas y de búsqueda. Un ejemplo actual de un marco de referencia es BrowseComp, que se centra principalmente en el inglés y ignora la complejidad lingüística, infraestructural y relacional de otras ecosistemas de información principales. Para remediar esto, se presenta BrowseComp-ZH. BrowseComp-ZH es un alto nivel marco de referencia desarrollado para evaluar los agentes de IA en la web china. BrowseComp-ZH consiste en 289 preguntas de múltiples etapas en 11 áreas, donde cada pregunta es diseñada para obtener respuestas breves, objetivas y fácilmente verificables (por ejemplo, fechas, números, nombres propios). Se aplican protocolos de gestión de calidad en dos etapas, buscando respuestas de alta calidad y diversidad. En el marco de referencia de BrowseComp-ZH, se evaluaron más de 20 modelos de lenguaje y sistemas de búsqueda de agentes. La mayoría de los modelos tienen fuertes habilidades de conversación y búsqueda, pero requieren un entrenamiento estricto: la precisión de muchos modelos es menor de 10% y algunos superan el 20%. El mejor sistema, DeepResearch de OpenAI, alcanza exactamente 42.9%. Estos resultados demuestran la alta dificultad de BrowseComp-ZH y muestran que para el éxito se necesita no solo una estrategia de búsqueda efectiva, sino también la capacidad de integrar lógica compleja e información. Este dataset, guías de diseño y resultados del marco de referencia se publican en https://github.com/PALIN2018/BrowseComp-ZH.」",
      "upvotes": 3,
      "discussionId": "681d89e7d025518b321f6807",
      "githubRepo": "https://github.com/PALIN2018/BrowseComp-ZH",
      "ai_keywords": [
        "tool-using agents",
        "multihop questions",
        "information ecosystems",
        "quality control protocol",
        "reasoning and information reconciliation"
      ]
    },
    "publishedAt": "2025-04-27T13:32:43.000Z",
    "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language\n  Models in Chinese",
    "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/NJsFys1PlInj9W3E7pD8t.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/gJJ8b6rc6njgCz7Fepbbd.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19314.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6673cf668d570d59b83511cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
      "fullname": "Peilin Zhou",
      "name": "PALIN2018",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05288",
      "authors": [
        {
          "_id": "681d9dd229119d666079b275",
          "user": {
            "_id": "63a3170f8c0c89dcae316858",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
            "isPro": false,
            "fullname": "Ahmed Abdelreheem",
            "user": "Samir55",
            "type": "user"
          },
          "name": "Ahmed Abdelreheem",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:06.667Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b276",
          "user": {
            "_id": "66eae6491cd3794eb4cd1992",
            "avatarUrl": "/avatars/5802de373ccc815f68b98b320aa787bf.svg",
            "isPro": false,
            "fullname": "Filippo Aleotti",
            "user": "Filippo8",
            "type": "user"
          },
          "name": "Filippo Aleotti",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:46.129Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b277",
          "user": {
            "_id": "63166685f5e32157c51fe616",
            "avatarUrl": "/avatars/5b7d8b0e54339d2dc982676af9e4f4fe.svg",
            "isPro": false,
            "fullname": "Jamie Watson",
            "user": "Aileron",
            "type": "user"
          },
          "name": "Jamie Watson",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:52.606Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b278",
          "user": {
            "_id": "6703aece547acbd64c531b72",
            "avatarUrl": "/avatars/8042f99c6eb2c9b61be8c9b950818b2f.svg",
            "isPro": false,
            "fullname": "Zawar Qureshi",
            "user": "zuluquebec",
            "type": "user"
          },
          "name": "Zawar Qureshi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:58.663Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b279",
          "user": {
            "_id": "6577f3bacfb2207f11e847bb",
            "avatarUrl": "/avatars/825998cfebc47d8106f633be5ad10964.svg",
            "isPro": false,
            "fullname": "Abdelrahman Eldesokey",
            "user": "abdo-eldesokey",
            "type": "user"
          },
          "name": "Abdelrahman Eldesokey",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:05:05.828Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27a",
          "name": "Peter Wonka",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27b",
          "name": "Gabriel Brostow",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27c",
          "user": {
            "_id": "67f517dcbd75b1099bba2857",
            "avatarUrl": "/avatars/a1a25d7972b1857f8bb49bc9efc02ded.svg",
            "isPro": false,
            "fullname": "Sara Vicente",
            "user": "svicente",
            "type": "user"
          },
          "name": "Sara Vicente",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:05:21.990Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27d",
          "name": "Guillermo Garcia-Hernando",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T14:29:11.000Z",
      "submittedOnDailyAt": "2025-05-09T04:50:18.526Z",
      "title": "PlaceIt3D: Colocación de Objetos 3D con Orientación Guiada por un Esquema 3D Realista",
      "submittedOnDailyBy": {
        "_id": "63a3170f8c0c89dcae316858",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
        "isPro": false,
        "fullname": "Ahmed Abdelreheem",
        "user": "Samir55",
        "type": "user"
      },
      "summary": "Nosotros presentamos un nuevo desafío: la disposición de objetos en un escenario 3D guiados por un lenguaje. Nuestro modelo recibe como entrada un punto de punto de la pantalla 3D, un contenido 3D y un texto de prompt que describe ampliamente donde deben colocarse los contenidos 3D. El desafío es encontrar una disposición válida de contenidos 3D que respete el prompt. Este trabajo presenta un desafío comparado con otros trabajos de posicionamiento de contenidos en diferentes lenguajes: tiene múltiples soluciones válidas, por lo tanto, es ambiguo y requiere considerar relaciones geométricas 3D y espacios libres. Proponemos inicializar este desafío con un nuevo benchmark y protocolo de evaluación. Además, proporcionamos un nuevo conjunto de datos para la entrenamiento de modelos 3D LLM y presentamos un método para proporcionar un primer criterio no triviario. Creemos que este desafío y el nuevo benchmark serán parte de la serie de benchmarks que evalúan y comparan generalmente modelos 3D LLM.",
      "upvotes": 2,
      "discussionId": "681d9dd529119d666079b348",
      "projectPage": "https://nianticlabs.github.io/placeit3d/",
      "githubRepo": "https://github.com/nianticlabs/placeit3d",
      "ai_keywords": [
        "point cloud",
        "3D asset",
        "textual prompt",
        "3D LLMs",
        "bounding",
        "grounding",
        "3D geometric relationships",
        "free space",
        "evaluation protocol",
        "benchmark"
      ]
    },
    "publishedAt": "2025-05-08T10:29:11.000Z",
    "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
    "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05288.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a3170f8c0c89dcae316858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
      "fullname": "Ahmed Abdelreheem",
      "name": "Samir55",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03422",
      "authors": [
        {
          "_id": "681db58b1f1c39ba8fbe0162",
          "user": {
            "_id": "681db120007a2d4056d25c70",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
            "isPro": false,
            "fullname": "yepeng liu",
            "user": "pengliu123",
            "type": "user"
          },
          "name": "Yepeng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T08:33:21.362Z",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0163",
          "name": "Wenpeng Lai",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0164",
          "name": "Zhou Zhao",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0165",
          "name": "Yuxuan Xiong",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0166",
          "name": "Jinchi Zhu",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0167",
          "name": "Jun Cheng",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0168",
          "name": "Yongchao Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T10:59:23.000Z",
      "submittedOnDailyAt": "2025-05-09T08:38:34.519Z",
      "title": "LiftFeat: Características de Coincidencia Local Relacionadas con la Geometría 3D de Gi\n\n(Nota: En el texto original, \"GI\" es una abreviatura que se mantiene para preservar la precisión y la profesionalidad.)",
      "submittedOnDailyBy": {
        "_id": "681db120007a2d4056d25c70",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
        "isPro": false,
        "fullname": "yepeng liu",
        "user": "pengliu123",
        "type": "user"
      },
      "summary": "Una robusta y eficiente asignación de características locales desempeña un papel importante en aplicaciones como el SLAM de robots y la posición visual. Aunque ha evolucionado, la extracción de características visuales robustas y diferenciables es muy difícil en lugares con iluminación rápidamente cambiante, bajas bordes o patrones repetidos. En este artículo, se propone una nueva red ligera llamada LiftFeat, que enfatiza la característica de 3D geométrica y aumenta su robustez mediante la utilización de la dicotómica de los círculos en el espacio de 2D de Descartes. Específicamente, se emplea un modelo de estimación de profundidad de una cámara única previamente entrenado para generar las rectas de normal de superficie predichas, y se extraen características geométricas de 3D basadas en estas rectas. A continuación, se diseña un módulo de características de 3D geométrica y se fusionan las características de las rectas de normal de superficie con las características de los círculos en el espacio de 2D de Descartes. Esta integración de características geométricas 3D maximiza la identificabilidad de las características 2D. Los resultados de la estimación de posición relativa, de homografía y de posición visual muestran que nuestro LiftFeat supera a los métodos ligeros líderes en comparación con el código disponible en la siguiente URL: https://github.com/lyp-deeplearning/LiftFeat.",
      "upvotes": 2,
      "discussionId": "681db58d1f1c39ba8fbe01fb",
      "githubRepo": "https://github.com/lyp-deeplearning/LiftFeat",
      "ai_keywords": [
        "monocular depth estimation model",
        "pseudo surface normal label",
        "3D geometric feature",
        "surface normal feature",
        "2D descriptor feature",
        "3D geometry-aware feature lifting module",
        "relative pose estimation",
        "homography estimation"
      ]
    },
    "publishedAt": "2025-05-06T06:59:23.000Z",
    "title": "LiftFeat: 3D Geometry-Aware Local Feature Matching",
    "summary": "Robust and efficient local feature matching plays a crucial role in\napplications such as SLAM and visual localization for robotics. Despite great\nprogress, it is still very challenging to extract robust and discriminative\nvisual features in scenarios with drastic lighting changes, low texture areas,\nor repetitive patterns. In this paper, we propose a new lightweight network\ncalled LiftFeat, which lifts the robustness of raw descriptor by\naggregating 3D geometric feature. Specifically, we first adopt a pre-trained\nmonocular depth estimation model to generate pseudo surface normal label,\nsupervising the extraction of 3D geometric feature in terms of predicted\nsurface normal. We then design a 3D geometry-aware feature lifting module to\nfuse surface normal feature with raw 2D descriptor feature. Integrating such 3D\ngeometric feature enhances the discriminative ability of 2D feature description\nin extreme conditions. Extensive experimental results on relative pose\nestimation, homography estimation, and visual localization tasks, demonstrate\nthat our LiftFeat outperforms some lightweight state-of-the-art methods. Code\nwill be released at : https://github.com/lyp-deeplearning/LiftFeat.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03422.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "681db120007a2d4056d25c70",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
      "fullname": "yepeng liu",
      "name": "pengliu123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.04955",
      "authors": [
        {
          "_id": "681dc1b7965798cccfeab83c",
          "user": {
            "_id": "654cca3fe1b4cd6d40d5a7ae",
            "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
            "isPro": false,
            "fullname": "Fangwei Zhu",
            "user": "soliz1998",
            "type": "user"
          },
          "name": "Fangwei Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:07:17.372Z",
          "hidden": false
        },
        {
          "_id": "681dc1b7965798cccfeab83d",
          "user": {
            "_id": "656873f33fd0bf1f82558695",
            "avatarUrl": "/avatars/7a085da2e2a91d7f41988501a573ebf9.svg",
            "isPro": false,
            "fullname": "PEIYI, WANG",
            "user": "peiyiwang89",
            "type": "user"
          },
          "name": "Peiyi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:07:23.247Z",
          "hidden": false
        },
        {
          "_id": "681dc1b7965798cccfeab83e",
          "name": "Zhifang Sui",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/654cca3fe1b4cd6d40d5a7ae/omgv1F-oog_5ohzEFDbns.png"
      ],
      "publishedAt": "2025-05-08T05:32:36.000Z",
      "submittedOnDailyAt": "2025-05-09T07:21:04.391Z",
      "title": "Chain-of-Thought Tokens son tokens de variables en un programa computacional.",
      "submittedOnDailyBy": {
        "_id": "654cca3fe1b4cd6d40d5a7ae",
        "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
        "isPro": false,
        "fullname": "Fangwei Zhu",
        "user": "soliz1998",
        "type": "user"
      },
      "summary": "Chain-of-thoughts (CoT) requiere un gran modelo de lenguaje (LLMs) para generar etapas intermedias antes de obtener la respuesta final, y ha demostrado ser efectivo en la resolución de problemas con estructuras lógicas complejas. Sin embargo, la estructura interna de CoT es casi incierta. En este artículo, se investiga experimentalmente el papel de los tokens de CoT en modelos de LLMs: se abordan dos tareas combinadas, el cálculo de multiplicación de múltiples dígitos y el programación dinámica, y se descubre que CoT es esencial para resolver estos problemas, pero solo alcanza un rendimiento aceptable al almacenar resultados intermedios. Además, almacenar resultados intermedios en formas potenciales generales no afecta el rendimiento del modelo. Además, al interrumpir valores de manera aleatoria dentro de CoT, se observa que los tokens de CoT y la respuesta final posteriormente cambian. Estos hallazgos sugieren que los tokens de CoT podrían desempeñar un papel similar a las variables en programación computacional. Además, incluyen posibles limitaciones potenciales como soluciones sencillas de manera no consciente y limitaciones en la cantidad de cálculos entre tokens. El código y los datos están disponibles en https://github.com/solitaryzero/CoTs_are_Variables.",
      "upvotes": 0,
      "discussionId": "681dc1b8965798cccfeab86d",
      "ai_keywords": [
        "chain-of-thought (CoT)",
        "large language models (LLMs)",
        "intermediate steps",
        "reasoning tasks",
        "inner mechanism",
        "CoT tokens",
        "compositional tasks",
        "multi-digit multiplication",
        "dynamic programming",
        "intermediate results",
        "latent form",
        "variables",
        "unintended shortcuts",
        "computational complexity limits"
      ]
    },
    "publishedAt": "2025-05-08T01:32:36.000Z",
    "title": "Chain-of-Thought Tokens are Computer Program Variables",
    "summary": "Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/654cca3fe1b4cd6d40d5a7ae/omgv1F-oog_5ohzEFDbns.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654cca3fe1b4cd6d40d5a7ae",
      "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
      "fullname": "Fangwei Zhu",
      "name": "soliz1998",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]