[
  {
    "paper": {
      "id": "2503.01785",
      "authors": [
        {
          "_id": "67c6816614a1bf9855188b8b",
          "user": {
            "_id": "66fe1334ff3ee1f7569fab6d",
            "avatarUrl": "/avatars/6868b1a545028a9b8bbded52490dc093.svg",
            "isPro": false,
            "fullname": "ziyuliu",
            "user": "ziyuliu",
            "type": "user"
          },
          "name": "Ziyu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:57.481Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b8c",
          "user": {
            "_id": "63fda3fced9eead590ff6918",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
            "isPro": false,
            "fullname": "Zeyi Sun",
            "user": "Zery",
            "type": "user"
          },
          "name": "Zeyi Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:35:03.275Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b8d",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:32.723Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b8e",
          "user": {
            "_id": "67c0849ee08c178ef8d4e05c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mQ6VdnjZnRhb0H_waPclo.png",
            "isPro": false,
            "fullname": "Xiaoyi Dong",
            "user": "sweetFruit",
            "type": "user"
          },
          "name": "Xiaoyi Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:25.627Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b8f",
          "user": {
            "_id": "65000bef18830fabea469fdd",
            "avatarUrl": "/avatars/b320c77dfad039d9f9c54127f610d44f.svg",
            "isPro": false,
            "fullname": "Cao Yuhang",
            "user": "yhcao",
            "type": "user"
          },
          "name": "Yuhang Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:19.177Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b90",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:12:05.281Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b91",
          "user": {
            "_id": "636317ed80c1a705a6eff396",
            "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
            "isPro": false,
            "fullname": "Dahua Lin",
            "user": "lindahua",
            "type": "user"
          },
          "name": "Dahua Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:11:57.087Z",
          "hidden": false
        },
        {
          "_id": "67c6816614a1bf9855188b92",
          "user": {
            "_id": "64638c4d51fa6e63060521b5",
            "avatarUrl": "/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg",
            "isPro": false,
            "fullname": "JIaqi",
            "user": "Jiaqiwang",
            "type": "user"
          },
          "name": "Jiaqi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:11:48.889Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T18:16:32.000Z",
      "title": "Visual-RFT: Regulación Fuerte de Visión Micro",
      "summary": "Reinforcement Fine-Tuning (RFT) es una técnica que se aplica a grandes modelos de lógica, como OpenAI o1, para que aprenda a proporcionar retroalimentación sobre las respuestas, lo que es especialmente útil cuando los datos de fine-tuning son limitados. En trabajos abiertos recientes, modelos como DeepSeek-R1 han demostrado que el aprendizaje por refuerzo con recompensas comprovables puede ser una dirección importante para replicar modelos como o1. Modelos de tipo R1 han tenido éxito en modelos de lenguaje, pero su aplicación en diversas áreas aún requiere más investigación. En este trabajo, se presenta la Introducción de la Visual Reinforcement Fine-Tuning (Visual-RFT), lo que amplía el rango de aplicaciones de RFT a tareas visuales. Específicamente, Visual-RFT utiliza Grandes Modelos de Visión y Lenguaje (LVLMs) para generar varias respuestas, cada una con un token de razonamiento y una respuesta final, y actualiza el modelo utilizando algoritmos de optimización de políticas como Group Relative Policy Optimization (GRPO) y una función de recompensa comprovable de reconocimiento visual propuesta. Se diseñan diferentes funciones de recompensa comprovable, como la recompensa de Intersection over Union (IoU), para distintas tareas de reconocimiento. A través de experimentos en benchmarks de clasificación de imágenes detalladas, detección de objetos poco supervisada, percepción de razonamiento y detección de objetos en cajas abiertas, se muestra que Visual-RFT presenta un rendimiento competitivo y una alta capacidad de generalización en comparación con el Fine-Tuning Supervisado (SFT). Por ejemplo, en la clasificación de imágenes detalladas con un solo ejemplo, Visual-RFT aumenta la precisión en un 24.3% usando 100 muestras de referencia. En la detección de objetos poco supervisada con dos ejemplos en COCO, se supera el rendimiento de referencia con un 21.9 puntos, y en LVIS con un 15.4 puntos. Visual-RFT marca un cambio en el paradigma de fine-tuning de LVLMs, proporcionando una aproximación más eficiente en datos y con recompensas, lo que mejora la razonabilidad y la aplicabilidad en tareas específicas.",
      "upvotes": 31,
      "discussionId": "67c6816c14a1bf9855188d8c",
      "projectPage": "https://github.com/Liuziyu77/Visual-RFT",
      "githubRepo": "https://github.com/Liuziyu77/Visual-RFT"
    },
    "publishedAt": "2025-03-03T23:29:27.952Z",
    "title": "Visual-RFT: Visual Reinforcement Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01785.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fda3fced9eead590ff6918",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
      "fullname": "Zeyi Sun",
      "name": "Zery",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01774",
      "authors": [
        {
          "_id": "67c694febdab31ec59fea175",
          "user": {
            "_id": "633aaf695df91da9cea92960",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633aaf695df91da9cea92960/9T4y1ru5wt5iKUUqf9_Tt.png",
            "isPro": false,
            "fullname": "Jay Wu",
            "user": "jayw",
            "type": "user"
          },
          "name": "Jay Zhangjie Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:53.874Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea176",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea177",
          "user": {
            "_id": "656e000253703dd78fd072a9",
            "avatarUrl": "/avatars/6702ba8fabe3d08884aa757f90cea333.svg",
            "isPro": false,
            "fullname": "Haithem Turki",
            "user": "hturki",
            "type": "user"
          },
          "name": "Haithem Turki",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:13:26.878Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea178",
          "user": {
            "_id": "658529d61c461dfe88afe8e8",
            "avatarUrl": "/avatars/a22c1b07d28c2662833c462c6537d835.svg",
            "isPro": false,
            "fullname": "Xuanchi Ren",
            "user": "xrenaa",
            "type": "user"
          },
          "name": "Xuanchi Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:13:33.467Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea179",
          "name": "Jun Gao",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea17a",
          "user": {
            "_id": "661ab3da2b14565c7acccf5c",
            "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
            "isPro": false,
            "fullname": "Mike Zheng Shou",
            "user": "AnalMom",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:27:21.825Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea17b",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea17c",
          "user": {
            "_id": "6366cda3361a96184dc22139",
            "avatarUrl": "/avatars/d8a88c84cb5f69e69dd038674a29be89.svg",
            "isPro": false,
            "fullname": "Zan Gojcic",
            "user": "zgojcic",
            "type": "user"
          },
          "name": "Zan Gojcic",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:27:34.034Z",
          "hidden": false
        },
        {
          "_id": "67c694febdab31ec59fea17d",
          "name": "Huan Ling",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T17:58:33.000Z",
      "title": "Difix3D+: Mejora de la reconstrucción 3D utilizando un modelo de difusor de primer nivel",
      "summary": "El nuevo pipeline Difix3D+ presenta un modelo de difusión único para mejorar la reconstrucción 3D y la síntesis visual. Nuestro enfoque central es eliminar y fortalecer los artifacts de renderización de visión nueva en áreas inciertas de la representación 3D, utilizando un modelo de difusión en un solo paso llamado Difix. Difix desempeña dos funciones cruciales en el pipeline: primero, en el paso de reconstrucción, limpia las imágenes de visión falsa renderizadas y mejora significativamente las áreas inciertas mediante 3D redesign, aumentando la calidad de la representación 3D completa. Más importante aún, Difix funciona como un enhacedor neural durante la inferencia, eliminando eficazmente los artifacts restantes debidos a las limitaciones y deficiencias de los modelos de reconstrucción actuales y las sub-posicionamientos 3D insuficientes. Difix3D+ es una solución general, se ajusta a las representaciones de NeRF y 3DGS, y mejora, en comparación con los estándares, el promedio de 2 veces el score de FID mientras mantiene la consistencia 3D.",
      "upvotes": 26,
      "discussionId": "67c69500bdab31ec59fea24d",
      "projectPage": "https://research.nvidia.com/labs/toronto-ai/difix3d"
    },
    "publishedAt": "2025-03-04T00:52:22.204Z",
    "title": "Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01774.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633aaf695df91da9cea92960",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633aaf695df91da9cea92960/9T4y1ru5wt5iKUUqf9_Tt.png",
      "fullname": "Jay Wu",
      "name": "jayw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01743",
      "authors": [
        {
          "_id": "67c67d0dfe135a5f482599bb",
          "name": "Abdelrahman Abouelenin",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599bc",
          "user": {
            "_id": "669ed17498ba26df962584f5",
            "avatarUrl": "/avatars/996c9cf05a4f8e5447552220085157c7.svg",
            "isPro": false,
            "fullname": "Atabak Ashfaq",
            "user": "atabakashfaqMSFT",
            "type": "user"
          },
          "name": "Atabak Ashfaq",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:45:15.511Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599bd",
          "name": "Adam Atkinson",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599be",
          "name": "Hany Awadalla",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599bf",
          "name": "Nguyen Bach",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c0",
          "user": {
            "_id": "6481e690f9ed842838a2b106",
            "avatarUrl": "/avatars/e89a3c8366df504a95dc08a1a412bf3d.svg",
            "isPro": false,
            "fullname": "Jianmin Bao",
            "user": "jianmin-ustc",
            "type": "user"
          },
          "name": "Jianmin Bao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:46:34.578Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c1",
          "user": {
            "_id": "65b9b627e7c838136275a681",
            "avatarUrl": "/avatars/22423f3d9a6c4ee34cad3b0894d27d23.svg",
            "isPro": false,
            "fullname": "Alon Benhaim",
            "user": "alonbenhaim",
            "type": "user"
          },
          "name": "Alon Benhaim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:46:41.117Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c2",
          "user": {
            "_id": "66f81b5b3c7ffa7931b4829a",
            "avatarUrl": "/avatars/a7f34e8e3fd92fdb96affc367b522fbe.svg",
            "isPro": false,
            "fullname": "cai",
            "user": "martincai",
            "type": "user"
          },
          "name": "Martin Cai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:46:47.556Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c3",
          "user": {
            "_id": "659c7ac977ac6f1bf5e63d7e",
            "avatarUrl": "/avatars/86a6efde0d483564a67ed5f344d479a0.svg",
            "isPro": false,
            "fullname": "Vishrav Chaudhary",
            "user": "vishravmsft",
            "type": "user"
          },
          "name": "Vishrav Chaudhary",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:46:56.428Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c4",
          "user": {
            "_id": "66c7a93b92e9f5b19f7533ab",
            "avatarUrl": "/avatars/e26ebf5cf083a3ec09fce24026ecc76e.svg",
            "isPro": false,
            "fullname": "Chen",
            "user": "congcongchen",
            "type": "user"
          },
          "name": "Congcong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:04.205Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c5",
          "user": {
            "_id": "666470a28f5513b0cf11e850",
            "avatarUrl": "/avatars/7beea758882677ad32a12ce56d4d084a.svg",
            "isPro": false,
            "fullname": "Dong Chen",
            "user": "DongChen06",
            "type": "user"
          },
          "name": "Dong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:11.865Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c6",
          "user": {
            "_id": "6567651c6fcc82e5e8c36d4d",
            "avatarUrl": "/avatars/ba3cc037a7688c4f8d967fc6043e540d.svg",
            "isPro": false,
            "fullname": "Dongdong Chen",
            "user": "dongdongchen",
            "type": "user"
          },
          "name": "Dongdong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:18.197Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c7",
          "user": {
            "_id": "669db44d61278f96d8c608a4",
            "avatarUrl": "/avatars/92a493da10c086af5f2af680f4e2c6c6.svg",
            "isPro": false,
            "fullname": "Junkun Chen",
            "user": "shtpgshus",
            "type": "user"
          },
          "name": "Junkun Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:43.236Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c8",
          "user": {
            "_id": "64da876370446182be5b608d",
            "avatarUrl": "/avatars/e412fdc71404ecdf638e416846e3ebfb.svg",
            "isPro": false,
            "fullname": "Weizhu Chen",
            "user": "chenweizhu",
            "type": "user"
          },
          "name": "Weizhu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:51.832Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599c9",
          "user": {
            "_id": "662d6b09a47b4da4b23c8b2a",
            "avatarUrl": "/avatars/6770b1d7e25b2cdce04f9904b543d122.svg",
            "isPro": false,
            "fullname": "Yen-Chun Chen",
            "user": "Yen-ChunChen",
            "type": "user"
          },
          "name": "Yen-Chun Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:47:58.051Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ca",
          "name": "Yi-ling Chen",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599cb",
          "name": "Qi Dai",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599cc",
          "name": "Xiyang Dai",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599cd",
          "user": {
            "_id": "64a8b800b35f48e37dfd20fe",
            "avatarUrl": "/avatars/1e66be9a5238ce86df8b54150520bcc8.svg",
            "isPro": false,
            "fullname": "Ruchao Fan",
            "user": "fanruchao",
            "type": "user"
          },
          "name": "Ruchao Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:40:17.936Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ce",
          "name": "Mei Gao",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599cf",
          "name": "Min Gao",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d0",
          "name": "Amit Garg",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d1",
          "user": {
            "_id": "62cdae333529c21a2283a0a1",
            "avatarUrl": "/avatars/cafc2821e522bbd06d49830e36a073e3.svg",
            "isPro": false,
            "fullname": "Abhishek GOSWAMI",
            "user": "abgoswam",
            "type": "user"
          },
          "name": "Abhishek Goswami",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:49:02.466Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d2",
          "user": {
            "_id": "5f04c4394ec31d33a72116d6",
            "avatarUrl": "/avatars/75d4b9020070e73604b12e5adc1c8201.svg",
            "isPro": false,
            "fullname": "Junheng Hao",
            "user": "jeffhao",
            "type": "user"
          },
          "name": "Junheng Hao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:53:16.356Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d3",
          "user": {
            "_id": "660480db07619487a3718a16",
            "avatarUrl": "/avatars/9c08d541913e57fd79988ef93d5095d4.svg",
            "isPro": false,
            "fullname": "Amr Hendy",
            "user": "amrhendy",
            "type": "user"
          },
          "name": "Amr Hendy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:53:24.716Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d4",
          "name": "Yuxuan Hu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d5",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d6",
          "user": {
            "_id": "6440905e27dc46cca590994c",
            "avatarUrl": "/avatars/0346f8ad17038fba87649a0fc59d64ab.svg",
            "isPro": false,
            "fullname": "Mahmoud Khademi",
            "user": "mkhademi",
            "type": "user"
          },
          "name": "Mahmoud Khademi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:53:53.225Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d7",
          "user": {
            "_id": "662476aec8920ec351b8d3d8",
            "avatarUrl": "/avatars/791e40f53073563680ef18f75b3ea95e.svg",
            "isPro": false,
            "fullname": "Dongwoo Kim",
            "user": "dongwookim-ms",
            "type": "user"
          },
          "name": "Dongwoo Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:54:04.257Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d8",
          "user": {
            "_id": "63f5173bb51da4d61da6c038",
            "avatarUrl": "/avatars/0ee530cf80476aa3985c4d591cd384a1.svg",
            "isPro": false,
            "fullname": "Young Jin Kim",
            "user": "ykim362",
            "type": "user"
          },
          "name": "Young Jin Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:40:19.902Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599d9",
          "name": "Gina Lee",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599da",
          "user": {
            "_id": "64004b72330a45b03604303b",
            "avatarUrl": "/avatars/a1fa3fc700173238d0336258b000d934.svg",
            "isPro": false,
            "fullname": "Jinyu Li",
            "user": "FallTraveler",
            "type": "user"
          },
          "name": "Jinyu Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:54:17.115Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599db",
          "name": "Yunsheng Li",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599dc",
          "name": "Chen Liang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599dd",
          "user": {
            "_id": "6464f05e5cdb9ab50f846c98",
            "avatarUrl": "/avatars/3cb2f60a909b59289209ecc7ba75a338.svg",
            "isPro": false,
            "fullname": "Xihui Lin",
            "user": "linxihui",
            "type": "user"
          },
          "name": "Xihui Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:56:29.024Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599de",
          "user": {
            "_id": "62c3a0caf5e2eb44f51de87d",
            "avatarUrl": "/avatars/3c535c5488476b75443666176fcb4c9b.svg",
            "isPro": false,
            "fullname": "Zeqi Lin",
            "user": "linzeqi",
            "type": "user"
          },
          "name": "Zeqi Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:56:38.534Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599df",
          "name": "Mengchen Liu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e0",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e1",
          "user": {
            "_id": "60c790f1accf7da31ed8240d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60c790f1accf7da31ed8240d/YDohCmgf9OUeWqZIs3Thh.jpeg",
            "isPro": false,
            "fullname": "Gilsinia Lopez",
            "user": "lgg",
            "type": "user"
          },
          "name": "Gilsinia Lopez",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:55.169Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e2",
          "name": "Chong Luo",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e3",
          "user": {
            "_id": "66269a329014ef4d10f55d9d",
            "avatarUrl": "/avatars/d4866c32419a7dd07e9aa0660f4bafa9.svg",
            "isPro": false,
            "fullname": "Piyush Madan",
            "user": "PiyushMadan",
            "type": "user"
          },
          "name": "Piyush Madan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:02:38.019Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e4",
          "user": {
            "_id": "65301591944086d1d5fcf656",
            "avatarUrl": "/avatars/250a2e898a4fcbe78feaf6e812851bd6.svg",
            "isPro": false,
            "fullname": "Vadim Mazalovskii",
            "user": "JakeRiley",
            "type": "user"
          },
          "name": "Vadim Mazalov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:02:47.430Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e5",
          "name": "Ali Mousavi",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e6",
          "user": {
            "_id": "649bc84833486cdd77c01c66",
            "avatarUrl": "/avatars/36f4e4bb15c337c4391bfbd234051f4c.svg",
            "isPro": false,
            "fullname": "Nguyen Anh",
            "user": "Anhnguyen",
            "type": "user"
          },
          "name": "Anh Nguyen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:57:52.311Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e7",
          "name": "Jing Pan",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e8",
          "user": {
            "_id": "673b7f70cdc852f69bebfed1",
            "avatarUrl": "/avatars/1efad61a42b948c750c96472a6192de5.svg",
            "isPro": false,
            "fullname": "Daniel Perez-Becker",
            "user": "perezbecker",
            "type": "user"
          },
          "name": "Daniel Perez-Becker",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:09.929Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599e9",
          "name": "Jacob Platin",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ea",
          "user": {
            "_id": "65c52dad286bf45e79491697",
            "avatarUrl": "/avatars/01ebc7979273df6e53971ae9835b503f.svg",
            "isPro": false,
            "fullname": "Thomas Portet",
            "user": "thopo",
            "type": "user"
          },
          "name": "Thomas Portet",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:39.865Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599eb",
          "name": "Kai Qiu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ec",
          "user": {
            "_id": "668dcf92835bf7e64bbca904",
            "avatarUrl": "/avatars/416eb3a3c5318a6a45aad87012296470.svg",
            "isPro": false,
            "fullname": "Bo Ren",
            "user": "rosrad",
            "type": "user"
          },
          "name": "Bo Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:40:15.919Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ed",
          "user": {
            "_id": "63815eff4761ddfa00903762",
            "avatarUrl": "/avatars/3419b239d42e091586f1c51b526d88e5.svg",
            "isPro": false,
            "fullname": "Liliang Ren",
            "user": "renll",
            "type": "user"
          },
          "name": "Liliang Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:57:37.996Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ee",
          "name": "Sambuddha Roy",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ef",
          "name": "Ning Shang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f0",
          "user": {
            "_id": "6454c337a13edf669cd5d8ea",
            "avatarUrl": "/avatars/a383a0dda7c2ef6a0d6c3c64651f42ff.svg",
            "isPro": false,
            "fullname": "Yelong Shen",
            "user": "uuu6",
            "type": "user"
          },
          "name": "Yelong Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:00:05.457Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f1",
          "user": {
            "_id": "62743aec8cb70eed79073bc0",
            "avatarUrl": "/avatars/3c8b9a91d898f616265f823ab7d432df.svg",
            "isPro": false,
            "fullname": "Saksham Singhal",
            "user": "sakshamsinghal",
            "type": "user"
          },
          "name": "Saksham Singhal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:03.188Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f2",
          "user": {
            "_id": "678bc6b432ee4968eca9bb6a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/wT-Xa3TYem_EzkZZMyDG0.png",
            "isPro": false,
            "fullname": "Subhojit Som",
            "user": "susom",
            "type": "user"
          },
          "name": "Subhojit Som",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:59:47.241Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f3",
          "name": "Xia Song",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f4",
          "user": {
            "_id": "64692ad25d701566394fd8da",
            "avatarUrl": "/avatars/d6811ccceb14788bfa0aa10fe4ee1054.svg",
            "isPro": false,
            "fullname": "Tetyana Sych",
            "user": "tesych",
            "type": "user"
          },
          "name": "Tetyana Sych",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:58:27.814Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f5",
          "name": "Praneetha Vaddamanu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f6",
          "name": "Shuohang Wang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f7",
          "name": "Yiming Wang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f8",
          "name": "Zhenghao Wang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599f9",
          "name": "Haibin Wu",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fa",
          "user": {
            "_id": "61384b860317b0a5c10877d3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1631080954171-61384b860317b0a5c10877d3.jpeg",
            "isPro": false,
            "fullname": "Haoran Xu",
            "user": "haoranxu",
            "type": "user"
          },
          "name": "Haoran Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:56:04.939Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fb",
          "user": {
            "_id": "6398f4b32c20654083f36cde",
            "avatarUrl": "/avatars/4591f514483890997c55e9e6d60bbb0f.svg",
            "isPro": false,
            "fullname": "Weijian Xu",
            "user": "xwjabc",
            "type": "user"
          },
          "name": "Weijian Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:58:36.082Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fc",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fd",
          "name": "Ziyi Yang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599fe",
          "user": {
            "_id": "65b01b8a29ae836e9ed5af24",
            "avatarUrl": "/avatars/a8b78a4b54d3f10858c5925521357001.svg",
            "isPro": false,
            "fullname": "Donghan Yu",
            "user": "donghanyu",
            "type": "user"
          },
          "name": "Donghan Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:55:41.798Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f482599ff",
          "name": "Ishmam Zabir",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f48259a00",
          "user": {
            "_id": "63601ee38fb9c2420ffbe45d",
            "avatarUrl": "/avatars/56af091aaff1b42dcfbae84a6ee1e7f7.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Jianwen",
            "type": "user"
          },
          "name": "Jianwen Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:55:12.465Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f48259a01",
          "user": {
            "_id": "62b0009c72043b05d29492b2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
            "isPro": false,
            "fullname": "Li Lyna Zhang",
            "user": "lynazhang",
            "type": "user"
          },
          "name": "Li Lyna Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:55:01.540Z",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f48259a02",
          "name": "Yunan Zhang",
          "hidden": false
        },
        {
          "_id": "67c67d0dfe135a5f48259a03",
          "user": {
            "_id": "66ce4c9f864befb39cfc74e9",
            "avatarUrl": "/avatars/ef66398466c470fc1d384c6817d9e461.svg",
            "isPro": false,
            "fullname": "Xiren Zhou",
            "user": "XirenZhou",
            "type": "user"
          },
          "name": "Xiren Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T09:54:26.629Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T17:05:52.000Z",
      "title": "Phi-4-Mini Reporte Técnico: Modelo de Lenguaje Multimodal Leve y Fuerte Basado en Procesos Mixtos",
      "summary": "Introduzco a Fiesta 4 Mini y a la biblioteca de modelos Fiesta 4. Ellos son modelos de lenguaje y de multimodal con altos rendimientos a pequeñas escalas. Fiesta 4 Mini se entrenó con 38 billones de parámetros usando datos de web de alta calidad y datos sintéticos, superando significativamente a los modelos más recientes de código abierto de la misma dimensión, y muestra un rendimiento similar al de tareas de matemáticas o codificación que requieren complejas inferencias. Este éxito se debe al ajuste de datos sintéticos que priorizan a los conjuntos de datos de matemáticas y codificación. Comparado con modelos anteriores, Fiesta 4 Mini amplió su vocabulario a 200K tokens, mejoró la soporte para aplicaciones multilingües y implementó la técnica de grupo de peticiones para la generación eficiente de secuencias largas. La biblioteca de modelos Fiesta 4 es un modelo multimodal que integra modelos de texto, visión y voz/audio. Utiliza un nuevo enfoque de expansión de modelos, con adaptadores LoRA y rotores por modelo, permitiendo la combinación de diferentes modelos para implementar varios modos de inferencia y evitar interferencias entre modelos. Por ejemplo, actualmente ocupa el primer lugar en el ranking de OpenASR, con un componente LoRA de 460 billones de parámetros. La biblioteca de modelos Fiesta 4 muestra un rendimiento superior a modelos de lenguaje de visión o de lenguaje de voz en tareas complejas, incluyendo escenarios que procesan entradas de (visión + lenguaje), (visión + voz) y (voz/audio). Además, se ha entrenado adicionalmente para mejorar la capacidad lógica de Fiesta 4 Mini, demostrando un rendimiento lógico superior a modelos más grandes como DeepSeek-R1-Distill-Qwen-7B y DeepSeek-R1-Distill-Llama-8B.",
      "upvotes": 22,
      "discussionId": "67c67d0efe135a5f48259a38",
      "projectPage": "https://huggingface.co/microsoft/Phi-4-multimodal-instruct"
    },
    "publishedAt": "2025-03-03T23:15:05.187Z",
    "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01743.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f5173bb51da4d61da6c038",
      "avatarUrl": "/avatars/0ee530cf80476aa3985c4d591cd384a1.svg",
      "fullname": "Young Jin Kim",
      "name": "ykim362",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01496",
      "authors": [
        {
          "_id": "67c6b05f35198d0f397adc98",
          "user": {
            "_id": "66ea643899af9ac3463639b1",
            "avatarUrl": "/avatars/252d470e761a57834dee3dbc60dfefed.svg",
            "isPro": false,
            "fullname": "Disen Lan",
            "user": "landisen",
            "type": "user"
          },
          "name": "Disen Lan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:46.117Z",
          "hidden": false
        },
        {
          "_id": "67c6b05f35198d0f397adc99",
          "user": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "isPro": false,
            "fullname": "Weigao Sun",
            "user": "weigao266",
            "type": "user"
          },
          "name": "Weigao Sun",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-04T08:10:52.130Z",
          "hidden": false
        },
        {
          "_id": "67c6b05f35198d0f397adc9a",
          "user": {
            "_id": "665dc35752ff9daa9ba5a4ed",
            "avatarUrl": "/avatars/df8b01879d97e599b610fa51414d3a18.svg",
            "isPro": false,
            "fullname": "Hu Jiaxi",
            "user": "Jiaxihu2",
            "type": "user"
          },
          "name": "Jiaxi Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:04:18.982Z",
          "hidden": false
        },
        {
          "_id": "67c6b05f35198d0f397adc9b",
          "user": {
            "_id": "65003e857804f04a163328d9",
            "avatarUrl": "/avatars/fe32150aabfde8d283b38ccebcf6982e.svg",
            "isPro": false,
            "fullname": "Jusen Du",
            "user": "JusenK",
            "type": "user"
          },
          "name": "Jusen Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:04:26.432Z",
          "hidden": false
        },
        {
          "_id": "67c6b05f35198d0f397adc9c",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T13:08:00.000Z",
      "title": "Laser: Lineariza los grandes modelos de lenguaje en la estructura de gated linear units.",
      "summary": "Transformadores con modelado recurrente lineal ofrecen entrenamiento en tiempo lineal y inferencia en memoria constante. A pesar de su demostrada eficiencia y rendimiento, entrenar de cero tales arquitecturas no estándar es costoso y peligroso. La linealización de grandes modelos de lenguaje (LLMs) transforma modelos pretrenados estándares en estructuras recurrentes lineales, permitiendo una deployment más eficiente. Sin embargo, los métodos actuales de linealización a menudo introducen módulos adicionales de mapa de características que requieren una entrenamiento extenso y ignoran las mecanismos de gating utilizados en los modelos líderes de modelos recurrentes lineales. Para abordar estos problemas, este artículo presenta Liger, corto por Linearizing LLMs to gated recurrent structures. Liger es un enfoque novedoso para convertir LLMs pretrenados en modelos recurrentes lineales con gating, sin añadir parámetros adicionales. Repropone las pesas de la matriz clave pretrenada para construir diversas mecanismos de gating, facilitando la formación de diferentes estructuras recurrentes mientras evita el necesidad de entrenar componentes adicionales desde cero. Usando una entrenamiento ligero con Adaptación de Rango Bajo (LoRA), Liger restaura el rendimiento de los modelos recurrentes lineales con gating al igual que el de los LLMs originales. Además, presentamos Liger Attention, un mecanismo de atención híbrido intra-capa, que recupera significativamente el 93% del LLM basado en Transformer en 0.02% de tokens de entrenamiento durante el proceso de linealización, logrando resultados competitivos en múltiples evaluaciones, como validado en modelos que van desde 1B hasta 8B parámetros. El código está disponible en https://github.com/OpenSparseLLMs/Linearization.",
      "upvotes": 12,
      "discussionId": "67c6b06035198d0f397adcc4"
    },
    "publishedAt": "2025-03-04T02:48:58.261Z",
    "title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01496.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01307",
      "authors": [
        {
          "_id": "67c68adc0457c9f809c22df8",
          "user": {
            "_id": "63e6a880f2e9a8f22c5a1630",
            "avatarUrl": "/avatars/53b57690fe052ce6882bbfc87b11567c.svg",
            "isPro": false,
            "fullname": "Kanishk Gandhi",
            "user": "obiwan96",
            "type": "user"
          },
          "name": "Kanishk Gandhi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:35:01.161Z",
          "hidden": false
        },
        {
          "_id": "67c68adc0457c9f809c22df9",
          "user": {
            "_id": "624f9e3d07bd004fb855f5e9",
            "avatarUrl": "/avatars/86a349cd4053bc0317e27e75a51c69fa.svg",
            "isPro": false,
            "fullname": "Ayush Chakravarthy",
            "user": "ayushchakravarthy",
            "type": "user"
          },
          "name": "Ayush Chakravarthy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:04:44.344Z",
          "hidden": false
        },
        {
          "_id": "67c68adc0457c9f809c22dfa",
          "user": {
            "_id": "6511ee845b7e52b0251fdee9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6511ee845b7e52b0251fdee9/hTIwiIYBGOVnIrxtpri83.png",
            "isPro": false,
            "fullname": "Anikait Singh",
            "user": "Asap7772",
            "type": "user"
          },
          "name": "Anikait Singh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:05:05.759Z",
          "hidden": false
        },
        {
          "_id": "67c68adc0457c9f809c22dfb",
          "user": {
            "_id": "61aa15fd8a9625ebfe284286",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61aa15fd8a9625ebfe284286/KaGzIeijcgcN15JErCqft.jpeg",
            "isPro": false,
            "fullname": "nathan lile",
            "user": "nlile",
            "type": "user"
          },
          "name": "Nathan Lile",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:58.582Z",
          "hidden": false
        },
        {
          "_id": "67c68adc0457c9f809c22dfc",
          "user": {
            "_id": "67321274c1f20c742bcf7a8d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ltcQhre6eDRVzn6Vbbyhu.png",
            "isPro": false,
            "fullname": "Noah D. Goodman",
            "user": "ngoodman",
            "type": "user"
          },
          "name": "Noah D. Goodman",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-04T10:05:12.186Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T08:46:22.000Z",
      "title": "Cómo Kogaku Mite y Bahabi Alze presentan teóricos sobre el crecimiento personal, y las 4 hábitos efectivos de STaR de alto nivel.",
      "summary": "Durante el entrenamiento, la inferencia se convierte en un fenómeno potente que muestra cómo un modelo de lenguaje puede pensar de maneras más efectivas en problemas complejos. El aprendizaje por refuerzo (RL) puede impulsar la mejora automática de los modelos de lenguaje en tareas provablemente; sin embargo, algunos modelos obtienen grandes mejoras mientras que otros se detengen rápidamente. Por ejemplo, al realizar el mismo entrenamiento por refuerzo en un juego de contador, Qwen-2.5-3B puede superar significativamente a Llama-3.2-3B. Esta diferencia plantea una pregunta importante: ¿qué características internas son útiles en la mejora automática? Proponemos un marco que analiza 4 acciones cognitivas (verificación, retroceso, establecimiento de subobjetivos, retroceso en cadena) para investigar estas preguntas. Estas acciones son utilizadas por resolvedores de problemas expertos o modelos de lenguaje exitosos. Nuestro estudio muestra que Qwen expresa estas acciones cognitivas de manera natural, mientras que Llama comenzó a hacerlo. A través de experimentos sistemáticos, cuando se incluye estas acciones cognitivas en Llama, se observa un gran mejoramiento en el entrenamiento por refuerzo y se puede superar a Qwen. Un punto clave es que la precisión de la respuesta es un elemento crucial para la capacidad de mejora. Por lo tanto, el rendimiento de modelos entrenados sobre soluciones correctas es relativamente similar a los entrenados sobre soluciones no normales que incluyen soluciones correctas. Finalmente, utilizando datos de OpenWebMath para entrenamiento continuo, Llama puede superar el patrón de mejora automática de Qwen al fortalecer patrones de razonamiento. Estos hallazgos fundamentalmente revelan la relación entre las acciones cognitivas iniciales y la capacidad de mejora, explicando por qué algunos modelos de lenguaje pueden utilizar cálculos adicionales efectivamente mientras que otros se detengen.",
      "upvotes": 9,
      "discussionId": "67c68add0457c9f809c22e31"
    },
    "publishedAt": "2025-03-04T00:09:04.418Z",
    "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01307.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e6a880f2e9a8f22c5a1630",
      "avatarUrl": "/avatars/53b57690fe052ce6882bbfc87b11567c.svg",
      "fullname": "Kanishk Gandhi",
      "name": "obiwan96",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.00714",
      "authors": [
        {
          "_id": "67c6a803025b72f14ccb0939",
          "user": {
            "_id": "6577437552f02732a463d97d",
            "avatarUrl": "/avatars/8eb271ec249fa9b0d97dfe0eace6da88.svg",
            "isPro": false,
            "fullname": "Haoyu Li",
            "user": "Haoyu0529",
            "type": "user"
          },
          "name": "Haoyu Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-04T07:13:08.306Z",
          "hidden": false
        },
        {
          "_id": "67c6a803025b72f14ccb093a",
          "name": "Srikanth Kandula",
          "hidden": false
        },
        {
          "_id": "67c6a803025b72f14ccb093b",
          "name": "Maria Angels de Luis Balaguer",
          "hidden": false
        },
        {
          "_id": "67c6a803025b72f14ccb093c",
          "name": "Aditya Akella",
          "hidden": false
        },
        {
          "_id": "67c6a803025b72f14ccb093d",
          "name": "Venkat Arun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T03:44:31.000Z",
      "title": "Spectralなエードハッククエリング",
      "summary": "En el análisis de grandes conjuntos de datos, es necesario una ejecución eficiente de consultas, pero la ejecución de consultas SQL en grandes conjuntos de datos puede ser lenta. En este artículo, investigamos si las consultas se ejecutan antes de que el usuario complete la entrada de claves y probamos métodos para aproximar los resultados. Proponemos el sistema SpeQL. Este sistema utiliza grandes modelos de lenguaje (LLMs) para predecir consultas predictables basándose en el esquema de la base de datos, las consultas pasadas del usuario y consultas incompletas. La predicción de consultas precisas es difícil, por lo que SpeQL predecie las consultas en dos maneras: 1) predecir la estructura de la consulta y editarla y planearla antes. 2) calcular tablas temporales más pequeñas que la base de datos original para predecir información necesaria para la consulta final del usuario. Además, SpeQL muestra continuamente los resultados de las consultas predecidas y parciales por tiempos, lo que puede ayudar en el análisis exploratorio. Según la evaluación basada en la utilidad/estudio de usuario, SpeQL mejora el tiempo de completación de las tareas y ayuda a los participantes a descubrir patrones rápidamente a través de los resultados predecidos. En esta evaluación, SpeQL mejora la demora de consultas en un máximo de 289 veces, controla los costos razonables y puede ejecutarse a un costo de 4$ por hora.",
      "upvotes": 8,
      "discussionId": "67c6a804025b72f14ccb0994",
      "projectPage": "https://github.com/lihy0529/SpeQL",
      "githubRepo": "https://github.com/lihy0529/SpeQL"
    },
    "publishedAt": "2025-03-04T02:21:00.460Z",
    "title": "Speculative Ad-hoc Querying",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6577437552f02732a463d97d/fEkQ4BZ8Yx_CzsjvHBWFq.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00714.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6577437552f02732a463d97d",
      "avatarUrl": "/avatars/8eb271ec249fa9b0d97dfe0eace6da88.svg",
      "fullname": "Haoyu Li",
      "name": "Haoyu0529",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.00784",
      "authors": [
        {
          "_id": "67c673bcf47209364f0cec96",
          "name": "Kai Lv",
          "hidden": false
        },
        {
          "_id": "67c673bcf47209364f0cec97",
          "name": "Honglin Guo",
          "hidden": false
        },
        {
          "_id": "67c673bcf47209364f0cec98",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "67c673bcf47209364f0cec99",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T08:27:48.000Z",
      "title": "Dow Decoding: Inicio de re-sequencing por espectro decoding dinámico para diversos sistemas de interés en hardware",
      "summary": "Los modelos de lenguaje grande (LLMs) muestran excelente rendimiento en una amplia gama de tareas, pero el proceso de generación automática de tokens en tiempo real afecta significativamente la velocidad de inferencia. La decodificación especulativa (Speculative decoding) propone un marco que permite generar y confirmar predicciones de la distribución de salida, con el objetivo de reducir la pérdida de generación. Sin embargo, los modelos propuestos añaden sobrecarga computacional adicional, lo que puede llevar a un desvío en el rendimiento y a un tiempo de primer token (TTFT) más largo. Las soluciones previas para reducir este sobrecarga han sido principalmente basadas en heurísticas, lo cual ha fallado en mantener la calidad del modelo. Para enfrentar estas desafíos, proponemos el DuoDecoding, que distribuye el modelo propuesto y el objetivo de manera estratégica en CPU y GPU, permitiendo una interpretación paralela y manteniendo la calidad del modelo. Nuestro método utiliza un bucket óptimo para la arquitectura de la computadora, minimiza el tiempo de conversación y mejora la calidad del modelo propuesto mediante la generación dinámica de múltiples secuencias. Experimentos amplios que incluyen 7 tareas muestran que el DuoDecoding logra un aumento de velocidad en la generación de texto latín de 2.61 veces y reduce el TTFT del decodificación especulativa tradicional en un 83%. El código está disponible en https://github.com/KaiLv69/DuoDecoding.",
      "upvotes": 7,
      "discussionId": "67c673bdf47209364f0cecb7",
      "githubRepo": "https://github.com/KaiLv69/DuoDecoding"
    },
    "publishedAt": "2025-03-03T22:35:45.299Z",
    "title": "DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00784.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6485d5b300c9cfe5c2470c81",
      "avatarUrl": "/avatars/c29aa81d2add795e8448b99274a04b83.svg",
      "fullname": "Kai",
      "name": "KaiLv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18965",
      "authors": [
        {
          "_id": "67c6bfdf96b9f5fa18c517db",
          "name": "Jiaxin Deng",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517dc",
          "name": "Shiyao Wang",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517dd",
          "name": "Kuo Cai",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517de",
          "name": "Lejian Ren",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517df",
          "name": "Qigen Hu",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517e0",
          "name": "Weifeng Ding",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517e1",
          "name": "Qiang Luo",
          "hidden": false
        },
        {
          "_id": "67c6bfdf96b9f5fa18c517e2",
          "name": "Guorui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T09:25:10.000Z",
      "title": "OneRec: La búsqueda y la clasificación se integran mediante un recomendador generativo y una repetitiva ajuste de preferencias.",
      "summary": "Recientemente, sistemas de recomendación basados en búsqueda generativa han surgido como un paradigma excelente, pero la mayoría de los sistemas modernos de recomendación se basan en búsqueda y ranking, y los modelos generativos solo funcionan como seleccionadores en la etapa de búsqueda. En este artículo, proponemos OneRec, un modelo generativo que reemplaza el marco de aprendizaje continuo actual. A pesar de nuestras limitaciones, OneRec es el primer modelo generativo que supera significativamente los sistemas complejos y bien diseñados actuales, demostrando un gran avance hasta el momento. En particular, OneRec incluye tres aspectos: 1) Estructura encoder-decoder, que codifica las secuencias de acciones históricas del usuario y decodifica gradualmente las películas que interesan al usuario. Utiliza una Mixture-of-Experts (MoE) rara para expandir la capacidad del modelo sin proporcionar un aumento proporcional en los costos computacionales (FLOPs). 2) Método de generación por sesiones, que propone un método de generación más sofisticado y contextual, sin depender de reglas de ajedrez, para predecir el próximo ítem en el proyecto. 3) Combinación del módulo Iterative Preference Alignment y la Direct Preference Optimization (DPO) para mejorar los resultados generados. A diferencia del DPO en el NLP, en sistemas de recomendación, los resultados se muestran una sola vez al usuario y no es posible obtener muestras de positivo y negativo. Para resolver esto, diseñamos un modelo de recompensa para simular la generación del usuario y personalizamos la estrategia de muestreo. La validación extensa muestra que un número limitado de muestras de DPO puede ajustar las preferencias del usuario y mejorar significativamente los resultados generados. OneRec ha sido introducido en los principales escenarios de Kuaishou, aumentando el tiempo de visión en un 1.6% y demostrando un gran avance.",
      "upvotes": 5,
      "discussionId": "67c6bfe396b9f5fa18c518e5"
    },
    "publishedAt": "2025-03-04T03:56:04.503Z",
    "title": "OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18965.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668f5875b5b3081d776e4094",
      "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
      "fullname": "Xiaohuan Zhou",
      "name": "XiaohuanZhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.00501",
      "authors": [
        {
          "_id": "67c6a343ad6b7c2fa29d5e7e",
          "name": "Jia Chen",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e7f",
          "user": {
            "_id": "60c0ed29d8bc072769d78f48",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60c0ed29d8bc072769d78f48/V6q6Tn4kzB46NIbTYw9pQ.jpeg",
            "isPro": false,
            "fullname": "Qian Dong",
            "user": "qian",
            "type": "user"
          },
          "name": "Qian Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:51.762Z",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e80",
          "name": "Haitao Li",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e81",
          "name": "Xiaohui He",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e82",
          "name": "Yan Gao",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e83",
          "name": "Shaosheng Cao",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e84",
          "name": "Yi Wu",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e85",
          "name": "Ping Yang",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e86",
          "name": "Chen Xu",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e87",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e88",
          "name": "Qingyao Ai",
          "hidden": false
        },
        {
          "_id": "67c6a343ad6b7c2fa29d5e89",
          "name": "Yiqun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-01T14:15:00.000Z",
      "title": "\"APP nivel de conversación con usuarios incluyendo diversos conjuntos de datos de búsqueda de información\"",
      "summary": "En comunidades de contenido creado por usuarios (UCC), especialmente en contenidos multimodales, se realiza la integración de información visual y textual sobre los resultados (o ítems) para mejorar la experiencia del usuario. Recientemente, el problema de mejorar la experiencia del usuario en servicios de búsqueda y recomendación (S\\&R) en sistemas complejos ha recibido atención en la academia e industria. Sin embargo, la escasez de conjuntos de datos de alta calidad limita el desarrollo de investigaciones en S\\&R multimodales. Dado que la demanda para mejorar la experiencia del usuario está aumentando, este artículo presenta un nuevo conjunto de datos de búsqueda multimodal llamado \"Qilin\". Este conjunto de datos se ha recopilado en la plataforma social \"小红书\" con más de 300 millones de usuarios activos mensualmente y un porcentaje de coincidencia de búsqueda promedio superior al 70%. Comparado con conjuntos de datos existentes, Qilin proporciona una recopilación coherente de sesiones de usuario que incluyen diversos resultados, como notas de imagen y texto, notas de video, notas de negocio y respuestas directas, fomentando el desarrollo de modelos neurales de búsqueda multimodal que han evolucionado a diferentes configuraciones de tareas. Además, se colecta y analizan señales de contexto ampliadas a nivel de aplicación y el feedback real del usuario para modelar la satisfacción del usuario y analizar otras acciones del usuario. En particular, Qilin incluye respuestas que el usuario prefiere y resultados referenciados en la respuesta del módulo de Respuesta Profunda (DQA), lo que permite investigar cómo estas respuestas afectan a las acciones de búsqueda del usuario. A través de análisis detallados y experimentos, se proporcionan interesantes descubrimientos y pistas que pueden ayudar a mejorar los sistemas de S\\&R. En el futuro, se espera que Qilin contribuya significativamente al desarrollo de plataformas de contenido multimodal que rodean los servicios de S\\&R.",
      "upvotes": 5,
      "discussionId": "67c6a346ad6b7c2fa29d5f88",
      "projectPage": "https://huggingface.co/datasets/THUIR/Qilin",
      "githubRepo": "https://github.com/RED-Search/Qilin/"
    },
    "publishedAt": "2025-03-04T01:56:03.632Z",
    "title": "Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00501.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60c0ed29d8bc072769d78f48",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60c0ed29d8bc072769d78f48/V6q6Tn4kzB46NIbTYw9pQ.jpeg",
      "fullname": "Qian Dong",
      "name": "qian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01370",
      "authors": [
        {
          "_id": "67c691673ff65c55829685a0",
          "name": "Jiantao Lin",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a1",
          "name": "Xin Yang",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a2",
          "name": "Meixi Chen",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a3",
          "name": "Yingjie Xu",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a4",
          "user": {
            "_id": "64049ae20ab5e22719f35103",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg",
            "isPro": false,
            "fullname": "Dongyu Yan",
            "user": "StarYDY",
            "type": "user"
          },
          "name": "Dongyu Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:56.252Z",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a5",
          "name": "Leyi Wu",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a6",
          "name": "Xinli Xu",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a7",
          "name": "Lie XU",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a8",
          "name": "Shunsi Zhang",
          "hidden": false
        },
        {
          "_id": "67c691673ff65c55829685a9",
          "name": "Ying-Cong Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:07:19.000Z",
      "title": "Kiss3DGen: Reproducir modelos de difuminación en la generación de 3D escenas",
      "summary": "DIFFUSION MODEL ha sido un gran éxito en la generación de imágenes 2D. Sin embargo, la calidad de generación y la capacidad de generalización de contenido 3D están limitadas. Los métodos más recientes requieren el aprendizaje con grandes escalas de contenido 3D, lo cual es difícil de recopilar. En este estudio, presentamos un eficiente marco para la reutilización de un modelo de DIFFUSION MODEL en la generación 3D, llamado Kiss3DGen (\"3D Generación un poco fría\"). Específicamente, fine-tunemos el modelo de DIFFUSION para generar \"3D BANDER imagenes\", que son representaciones en tiles constituidas por imágenes de varios puntos y sus mapas normales relativos. Los mapas normales permiten reestructurar la mecánica 3D, mientras que las imágenes de varios puntos proporcionan la textura mapeo para generar modelos 3D completos. Este método sencillo transforma el problema de generación 3D en una tarea de generación de imágenes 2D, maximizando el conocimiento de un modelo de DIFFUSION preentrenado. Además, el modelo Kiss3DGen permite funciones como edición 3D, mejora de la mecánica y textura, entre otras. Los experimentos detallados demostraron la efectividad de nuestro enfoque y la capacidad de generar modelos 3D de alta calidad de manera eficiente.",
      "upvotes": 4,
      "discussionId": "67c6916b3ff65c5582968702",
      "projectPage": "https://ltt-o.github.io/Kiss3dgen.github.io/",
      "githubRepo": "https://github.com/EnVision-Research/Kiss3DGen"
    },
    "publishedAt": "2025-03-04T01:19:45.715Z",
    "title": "Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6332e2689bf698ce68a22e8c",
      "avatarUrl": "/avatars/c1922acfda2e6d2fe7b03194a404eb10.svg",
      "fullname": "JIANTAO LIN",
      "name": "LTT",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01183",
      "authors": [
        {
          "_id": "67c6a15e21d722b4248bd9c2",
          "name": "Ziqian Ning",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c3",
          "name": "Huakang Chen",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c4",
          "name": "Yuepeng Jiang",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c5",
          "name": "Chunbo Hao",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c6",
          "name": "Guobin Ma",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c7",
          "name": "Shuai Wang",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c8",
          "name": "Jixun Yao",
          "hidden": false
        },
        {
          "_id": "67c6a15e21d722b4248bd9c9",
          "name": "Lei Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T05:15:34.000Z",
      "title": "DiffRhythm: Sorprendentemente rápido y gracias desde el principio hasta el final\nLa canción completa se genera mediante la suma de un difusor de potencia",
      "summary": "El reciente avance en la generación de música ha recibido gran atención, aunque los métodos actuales presentan importantes limitaciones. Algunos modelos de generación actuales pueden sintetizar solo el título de artista o solo el acompañamiento. Por otro lado, otros modelos combinan el artista con su interpretación, pero generalmente utilizan arquitecturas complejas de múltiples etapas y flujos de datos intrincados, lo que limita su escalabilidad. Además, muchos sistemas están restringidos a la generación de pequeñas secciones musicales, lo que impide la creación de largas canciones. Además, los métodos basados en modelos de lenguaje de uso amplio presentan velocidades de inferencia lentas. Para abordar estos problemas, se propone DiffRhythm. DiffRhythm es un modelo inicial basado en diferenciales para la generación de canciones, que puede sintetizar 4 minutos 45 segundos de artista y acompañamiento en 10 segundos, manteniendo alta calidad musical y comprensión. Además, DiffRhythm está diseñado para ser simple y atractivo, eliminando la necesidad de preparación de datos complejos, utilizando una estructura de modelo sencilla y solicitando solo el artista y un estilo de proyecto durante la inferencia. Además, la estructura de algoritmo de retroalimentación no automático mantiene la velocidad lenta de inferencia. Esta simplicidad garantiza la escalabilidad de DiffRhythm. Además, se publican códigos de entrenamiento con datos de gran escala y modelos preentrenados para fomentar la investigación en reproducibilidad y desarrollo.",
      "upvotes": 3,
      "discussionId": "67c6a16021d722b4248bda37",
      "projectPage": "https://aslp-lab.github.io/DiffRhythm.github.io/",
      "githubRepo": "https://github.com/ASLP-lab/DiffRhythm"
    },
    "publishedAt": "2025-03-04T04:54:04.054Z",
    "title": "DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01183.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "624bebf604abc7ebb01789af",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649143001781-624bebf604abc7ebb01789af.jpeg",
      "fullname": "Apolinário from multimodal AI art",
      "name": "multimodalart",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isMod": false,
      "followerCount": 3862
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18890",
      "authors": [
        {
          "_id": "67c6cbd6e52534aa6ada2e26",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "67c6cbd6e52534aa6ada2e27",
          "name": "Junzhe Shen",
          "hidden": false
        },
        {
          "_id": "67c6cbd6e52534aa6ada2e28",
          "name": "Zixia Jia",
          "hidden": false
        },
        {
          "_id": "67c6cbd6e52534aa6ada2e29",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "67c6cbd6e52534aa6ada2e2a",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-04T09:45:59.571Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T07:10:08.000Z",
      "title": "Desde horas hasta minutos: aceleración sin pérdida de calidad para secuencias super largas de hasta 100K Tokens",
      "summary": "El uso de modelos de lenguaje grandes (LLMs) es crucial en la generación de secuencias largas, especialmente en tareas que requieren tiempo para procesar secuencias de aproximadamente 100K tokens. Aunque existen métodos predictivos tradicionales, estos pueden no proporcionar una mejora de velocidad al simplemente extenderse, o incluso causar pérdidas. Mediante un análisis detallado, se identificaron tres problemas principales y se introdujo un nuevo marco de trabajo llamado TOKENSWIFT para resolverlos. Este marco de trabajo tiene como objetivo acelerar significativamente la generación de secuencias largas mientras mantiene la calidad propia del modelo. Los resultados de los experimentos muestran que TOKENSWIFT puede acelerar la velocidad del modelo en más de 3 veces, independientemente de su tamaño (1.5B, 7B, 8B, 14B), y reducir significativamente el tiempo de generación de secuencias largas. El código está disponible en https://github.com/bigai-nlco/TokenSwift.",
      "upvotes": 2,
      "discussionId": "67c6cbd7e52534aa6ada2e79",
      "githubRepo": "https://github.com/bigai-nlco/TokenSwift"
    },
    "publishedAt": "2025-03-04T04:56:33.061Z",
    "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63a95a6a7930fa8c7dd63d4e/3WZ10b-Ku3GcY1fc1MWx8.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18890.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a95a6a7930fa8c7dd63d4e",
      "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
      "fullname": "Zilong Zheng",
      "name": "zlzheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16779",
      "authors": [
        {
          "_id": "67c65c06e116e361574405e9",
          "user": {
            "_id": "642bdfc65edcc5760cb1ea12",
            "avatarUrl": "/avatars/599b0bbb379b43cd39097c204c946075.svg",
            "isPro": false,
            "fullname": "huang",
            "user": "yxuan",
            "type": "user"
          },
          "name": "Yaxuan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:51:27.582Z",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405ea",
          "name": "Xili Dai",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405eb",
          "name": "Jianan Wang",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405ec",
          "name": "Xianbiao Qi",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405ed",
          "name": "Yixing Yuan",
          "hidden": false
        },
        {
          "_id": "67c65c06e116e361574405ee",
          "name": "Xiangyu Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T02:14:19.000Z",
      "title": "Modelo de reestructuración de habitaciones para el tiempo de matrícula sin límites utilizando vistas esparsas",
      "summary": "Estimar la disposición de habitaciones en imágenes panorámicas es un desafío complejo que se investiga difícilmente debido a la fotogrametría multi-punto. Esto requiere soluciones en varios pasos, como la estimación de configuraciones internas y externas de la cámara, el alineamiento de imágenes y la triangulación. Sin embargo, en la reconstrucción 3D, el desarrollo reciente de modelos basados en 3D (por ejemplo, DUSt3R) ha introducido un enfoque de un paso menos, cambiando el paradigma de los métodos tradicionales multi-paso. En consecuencia, presentamos un nuevo método llamado Plane-DUSt3R, que utiliza el modelo basado en 3D de DUSt3R. Plane-DUSt3R utiliza el marco de trabajo de DUSt3R y ajusta un objetivo modificado para estimar planos estructurales, aplicado a un conjunto de datos de disposición de habitaciones (Structure3D). Con este resultado, Plane-DUSt3R puede estimar la disposición de habitaciones utilizando los resultados de detección 2D y posteriores pasos de procesamiento. A diferencia de los métodos anteriores, que dependen de solo una imagen visual o de un esquema, Plane-DUSt3R extiende el tratamiento de múltiples perspectivas de la imagen visual. Además, proporciona un enfoque de un paso menos y de flujo, simplificando el proceso y reduciendo la acumulación de errores. Los resultados experimentales superan los métodos más avanzados en conjuntos de datos sintéticos y muestran resultados robustos y efectivos en imágenes naturales de diferentes estilos, como cartuchos y otros tipos de imágenes. El código está disponible en la siguiente URL:\nhttps://github.com/justacar/Plane-DUSt3R",
      "upvotes": 2,
      "discussionId": "67c65c0be116e36157440751",
      "githubRepo": "https://github.com/justacar/Plane-DUSt3R"
    },
    "publishedAt": "2025-03-04T04:17:23.806Z",
    "title": "Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16779.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642bdfc65edcc5760cb1ea12",
      "avatarUrl": "/avatars/599b0bbb379b43cd39097c204c946075.svg",
      "fullname": "huang",
      "name": "yxuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01295",
      "authors": [
        {
          "_id": "67c6a8b534aeb86063e94010",
          "user": {
            "_id": "61711f02e0b1ddb56eb9b526",
            "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
            "isPro": false,
            "fullname": "Mingzhe Du",
            "user": "Elfsong",
            "type": "user"
          },
          "name": "Mingzhe Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:49.954Z",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94011",
          "name": "Anh Tuan Luu",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94012",
          "name": "Bin Ji",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94013",
          "name": "Xiaobao Wu",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94014",
          "name": "Dong Huang",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94015",
          "name": "Terry Yue Zhuo",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94016",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "67c6a8b534aeb86063e94017",
          "name": "See-Kiong Ng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T08:31:16.000Z",
      "title": "CodeArena: Plataforma de Evaluación Concentrada de Creación de Código con LLM",
      "summary": "Los modelos de lenguaje grande (LLMs) han mejorado la eficiencia en la generación de código y aumentado la productividad de los desarrolladores al combinar el entendimiento de la naturaleza del lenguaje y el lenguaje de programación. Estos avances han impulsado los esfuerzos de evaluación diversos en cuanto a la capacidad de generación de código. Sin embargo, obstáculos como la pérdida de puntajes en los benchmarks, la pérdida de datos y las limitaciones de accesibilidad del sistema continúan obstaculizando evaluaciones precisas a lo largo del tiempo. Para resolver estas limitaciones, se presenta CodeArena, un marco de evaluación en línea para la generación de código de LLMs. La innovación principal consiste en un sistema de evaluación centrado en la base general de rendimiento para ajustar de manera dinámica los puntajes de los modelos individuales, lo que mitiga la sesgo de puntajes causado por la pérdida de benchmarks. Además, CodeArena garantiza la publicación de todas las soluciones presentadas y los casos de prueba, y proporciona APIs adecuados para la automatización para streamline el proceso de evaluación de código. Nuestras principales contribuciones son: (1) un sistema de evaluación centrado para la evaluación sin sesgo, (2) un repositorio público de soluciones y casos de prueba, y (3) APIs preparados para la integración automática de manera continua.",
      "upvotes": 2,
      "discussionId": "67c6a8b634aeb86063e9406a"
    },
    "publishedAt": "2025-03-04T02:16:25.633Z",
    "title": "CodeArena: A Collective Evaluation Platform for LLM Code Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61711f02e0b1ddb56eb9b526",
      "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
      "fullname": "Mingzhe Du",
      "name": "Elfsong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01739",
      "authors": [
        {
          "_id": "67c68f7828a037872c5ce5bb",
          "name": "Wenhao Wang",
          "hidden": false
        },
        {
          "_id": "67c68f7828a037872c5ce5bc",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T17:00:36.000Z",
      "title": "VideoUFO: Dataset de 100 mil usuarios centrado en el usuario que genera videos desde texto",
      "summary": "El modelo de generación de videos a partir de texto puede convertir textos en contenido visual dinámico, lo que permitirá su aplicación en diversas áreas como la producción de películas, juegos y educación. Sin embargo, su rendimiento real a menudo es inferior a las expectativas de los usuarios. Una de las principales causas es que estos modelos no se han entrenadido con contenido de videos relacionados con temas que los usuarios deseen crear. En este artículo, se propone el primer conjunto de datos de videos (VideoUFO) que se alinea con las preocupaciones reales de los usuarios. VideoUFO presenta dos características notables: (1) un mínimo de repetición con otros conjuntos de datos de video (0.29%), y (2) se ha buscado y seleccionado bajo la licencia Creative Commons utilizando la API oficial de YouTube. Estas características ofrecen a los investigadores una gran libertad para expandir sus fuentes de entrenamiento. VideoUFO incluye más de 1,090,000 clips de video, cada uno con una breve y una descripción detallada. Específicamente, mediante clustering se identificaron 1,291 temas de interés de usuarios en el conjunto de datos de videos de publicidad basado en texto (VidProM). Luego, se utilizaron estos temas para buscar videos en YouTube, separar los clips y crear breves y detalladas descripciones para cada clip. Después de verificar los clips de los temas específicos, se quedaron aproximadamente 1,090,000 clips de video. Los experimentos mostraron que: (1) un modelo de video generado a partir de 16 textos no logró lograr un rendimiento consistente para todos los temas de interés de los usuarios, y (2) un modelo simple entrenado con VideoUFO demostró ser más efectivo que otros modelos. El conjunto de datos está disponible bajo la licencia CC BY 4.0 (https://huggingface.co/datasets/WenhaoWang/VideoUFO).",
      "upvotes": 2,
      "discussionId": "67c68f7a28a037872c5ce60d"
    },
    "publishedAt": "2025-03-04T00:29:56.570Z",
    "title": "VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01739.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b32a4429a410b7f6b06710",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b32a4429a410b7f6b06710/VzgvmnlYZWuifZTkIkCxy.jpeg",
      "fullname": "Wenhao Wang",
      "name": "WenhaoWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01807",
      "authors": [
        {
          "_id": "67c67ff6dec55d10cb10fc9e",
          "user": {
            "_id": "62608fc2ffe8827cb1d89f9f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png",
            "isPro": false,
            "fullname": "Hamish Ivison",
            "user": "hamishivi",
            "type": "user"
          },
          "name": "Hamish Ivison",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:40:13.649Z",
          "hidden": false
        },
        {
          "_id": "67c67ff6dec55d10cb10fc9f",
          "name": "Muru Zhang",
          "hidden": false
        },
        {
          "_id": "67c67ff6dec55d10cb10fca0",
          "name": "Faeze Brahman",
          "hidden": false
        },
        {
          "_id": "67c67ff6dec55d10cb10fca1",
          "name": "Pang Wei Koh",
          "hidden": false
        },
        {
          "_id": "67c67ff6dec55d10cb10fca2",
          "name": "Pradeep Dasigi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T18:37:26.000Z",
      "title": "Selección de datos de gran escala y ajuste de instrucciones (o ajuste de instrucciones y entrenamiento)",
      "summary": "La elección de datos de ranking es un paso importante en el entrenamiento de modelos de lenguaje para comandos. Un conjunto de datos bien seleccionado puede permitir que un modelo entrenado supere a uno que utilice un conjunto de datos de mayor escala y mayor ruido. El enfoque de selección de datos automática para entrenamiento de comandos generalmente consiste en seleccionar un pequeño conjunto de datos (aproximadamente 10k muestras) desde un pequeño pool (100-200k muestras) para probar. Sin embargo, modelos populares de entrenamiento de comandos utilizan millones de muestras en un pool más grande durante el entrenamiento. Estudiamos sistemáticamente cómo escala el método de selección de datos en estas configuraciones. Seleccionamos hasta 2.5 millones de muestras del pool y evaluamos en 7 tareas diferentes. Mostramos que muchos métodos recientemente propuestos son peores que la selección aleatoria y que su rendimiento disminuye con el crecimiento del pool de datos. Sin embargo, demostramos que la representación de la selección de datos basada en la representación de estados ocultos de modelos de predicción (RDS+) supera a los métodos complejos en todos los escenarios de evaluación, siempre y cuando se priorice la eficiencia computacional. Este método se basa en la representación de estados ocultos de modelos de predicción. Nuestros hallazgos subrayan la necesidad de revisar con mayor precisión las características de escalabilidad de los métodos de selección de datos automática propuestos. Nuestro código, datos y modelos están disponibles en la URL siguiente: https://github.com/hamishivi/automated-instruction-selection",
      "upvotes": 2,
      "discussionId": "67c67ff9dec55d10cb10fcef"
    },
    "publishedAt": "2025-03-03T23:44:06.105Z",
    "title": "Large-Scale Data Selection for Instruction Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62608fc2ffe8827cb1d89f9f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png",
      "fullname": "Hamish Ivison",
      "name": "hamishivi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01063",
      "authors": [
        {
          "_id": "67c6b72b7aad9a016ae60797",
          "name": "David Noever",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T23:59:52.000Z",
      "title": "La lenguaje de voz desarrollado por la IA: métodos para evitar que la lenguaje de la máquina sea incomprensible para los humanos",
      "summary": "Este artículo investiga la posibilidad de desarrollar lenguajes personales mediante el uso de sistemas de comunicación entre máquinas (M2M) a través de modelos de lenguaje grandes (LLMs). Se ha implementado un sistema que transforma un conjunto preciso de caracteres, similar a los de la música, en frecuencias de sonido. Cada carácter se asigna una frecuencia única, comienza con un espacio lógico (220 Hz) y termina en un valor de 126. Este sistema supera aproximadamente 7.9 órcaras y mapea las caracteres más altos en frecuencias de ultrasonido (>20 kHz), que superan el rango de percepción humana. El prototipo de software visualiza, reproduce auditivamente y muestra la notación musical ABC, permitiendo analizar la densidad de información y la velocidad de transmisión. Los resultados de los tests muestran que la codificación de caracteres puede superar la tasa de información de los idiomas humanos y que algunas características pueden funcionar fuera del rango de percepción humano. Este estudio proporciona una respuesta directa a las preocupaciones sobre la posibilidad de desarrollar lenguajes personales a gran escala en los próximos cinco años, y ofrece la base técnica necesaria para la comunicación, su detección, y su manejo.",
      "upvotes": 0,
      "discussionId": "67c6b72c7aad9a016ae607bb"
    },
    "publishedAt": "2025-03-04T03:20:03.380Z",
    "title": "AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond Human Understanding",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63136a82e29fb2e86d5e5bdd/mgIPjnhtUaGLR2Iv4ViL6.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01063.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63136a82e29fb2e86d5e5bdd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63136a82e29fb2e86d5e5bdd/pFZDuQtzfUStovbwwZGvn.png",
      "fullname": "David Noever",
      "name": "dnoever",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.00729",
      "authors": [
        {
          "_id": "67c6ab3ec0b62d612c54ddf5",
          "user": {
            "_id": "6628c6107751d297d7025a71",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628c6107751d297d7025a71/S1rm5VIwV2Uxfv8GetKMU.jpeg",
            "isPro": false,
            "fullname": "Lei Mingcong",
            "user": "SP4595",
            "type": "user"
          },
          "name": "Mingcong Lei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:48.061Z",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddf6",
          "name": "Ge Wang",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddf7",
          "name": "Yiming Zhao",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddf8",
          "name": "Zhixin Mai",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddf9",
          "name": "Qing Zhao",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfa",
          "name": "Yao Guo",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfb",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfc",
          "name": "Shuguang Cui",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfd",
          "name": "Yatong Han",
          "hidden": false
        },
        {
          "_id": "67c6ab3ec0b62d612c54ddfe",
          "name": "Jinke Ren",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T04:50:59.000Z",
      "title": "CLEA: Agente ligero de ciclos cerrados para mejorar la ejecución en entornos dinámicos",
      "summary": "Los modelos de lenguaje grande (LLMs) desarrollan capacidades impresionantes a través de la descomposición estratificada de tareas y la inferencia significativa. Sin embargo, la aplicación de estos sistemas ligeros en tareas a largo plazo es complicada, ya que la secuencia de sub-tareas puede no ser confiable para su ejecución. Para resolver estas limitaciones en entornos dinámicos, proponemos una nueva arquitectura llamada CLEA (Closed-Loop Robot Agent). Esta arquitectura se basa en cuatro modelos de lenguaje grande abiertos y especializados para la gestión de tareas cerradas. El marco tiene dos innovaciones clave: 1) la planificación interactiva de tareas genera sub-tareas ejecutables basándose en la memoria del entorno. 2) la ejecución multimodal curriculum evalúa la probabilidad de éxito de las acciones y, si las variaciones del entorno superan los límites establecidos, activa una estructura de re-planeación jerárquica. Para evaluar los efectos de CLEA, realizamos experimentos en entornos reales utilizando objetos que pueden ser manejados. Con dos robots diferentes, realizamos tareas de búsqueda y manipulación, así como de búsqueda y manipulación combinadas. A través de 12 intentos de tareas, CLEA supera los modelos de referencia, aumentando la tasa de éxito en un 67.3% y la tasa de completación de tareas en un 52.8%. Estos resultados muestran que CLEA mejora significativamente la robustez de la planificación y ejecución de tareas en entornos dinámicos.",
      "upvotes": 0,
      "discussionId": "67c6ab42c0b62d612c54df71",
      "projectPage": "https://sp4595.github.io/CLEA/",
      "githubRepo": "https://github.com/SP4595/CLEA-Closed-Loop-Embodied-Agent"
    },
    "publishedAt": "2025-03-04T02:27:17.351Z",
    "title": "CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic Environments",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00729.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6628c6107751d297d7025a71",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628c6107751d297d7025a71/S1rm5VIwV2Uxfv8GetKMU.jpeg",
      "fullname": "Lei Mingcong",
      "name": "SP4595",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]