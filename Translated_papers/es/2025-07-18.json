[
  {
    "paper": {
      "id": "2507.13334",
      "authors": [
        {
          "_id": "6879aad021b37e676c8e406b",
          "user": {
            "_id": "63120517ae8896941da4c5da",
            "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
            "isPro": false,
            "fullname": "Lingrui Mei",
            "user": "Chevalier",
            "type": "user"
          },
          "name": "Lingrui Mei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:58.423Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406c",
          "user": {
            "_id": "671f9cd9ff056a1b49444f37",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/B3Z9oiFb79Gi-_YXKP13u.png",
            "isPro": false,
            "fullname": "duoduo yao",
            "user": "Theodyy",
            "type": "user"
          },
          "name": "Jiayu Yao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:48.326Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406d",
          "user": {
            "_id": "656ad93853703dd78f3de7b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/r6VB6ICND1td3wFOy8pnz.jpeg",
            "isPro": false,
            "fullname": "YuyaoGe",
            "user": "YuyaoGe",
            "type": "user"
          },
          "name": "Yuyao Ge",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:54.689Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406e",
          "name": "Yiwei Wang",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406f",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4070",
          "name": "Yujun Cai",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4071",
          "name": "Jiazhi Liu",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4072",
          "user": {
            "_id": "6720cf97a0396f933ec93ab8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NAdPIISe9-TYGGM0nAgsb.png",
            "isPro": false,
            "fullname": "Li Max",
            "user": "LImax72",
            "type": "user"
          },
          "name": "Mingyu Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:56.619Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4073",
          "name": "Zhong-Zhi Li",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4074",
          "user": {
            "_id": "662383a20edeabfe3b64a6a5",
            "avatarUrl": "/avatars/a76da726002d853dd08a51a6af6311d9.svg",
            "isPro": false,
            "fullname": "Duzhen Zhang",
            "user": "ShowerMaker",
            "type": "user"
          },
          "name": "Duzhen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:50.488Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4075",
          "user": {
            "_id": "669e27902dbf53ccd23ae47f",
            "avatarUrl": "/avatars/5c193b542dcfe2e64467fa5c686f3e20.svg",
            "isPro": false,
            "fullname": "chenlin",
            "user": "tvstfe",
            "type": "user"
          },
          "name": "Chenlin Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:52.410Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4076",
          "name": "Jiayi Mao",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4077",
          "name": "Tianze Xia",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4078",
          "name": "Jiafeng Guo",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4079",
          "name": "Shenghua Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:50:36.000Z",
      "submittedOnDailyAt": "2025-07-18T00:52:14.092Z",
      "title": "Investigación del aprendizaje de contexto en modelos de lenguaje de Larzaret",
      "submittedOnDailyBy": {
        "_id": "63120517ae8896941da4c5da",
        "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
        "isPro": false,
        "fullname": "Lingrui Mei",
        "user": "Chevalier",
        "type": "user"
      },
      "summary": "El desempeño de un LLM se determina fundamentalmente en función de la información contextual proporcionada durante la inferencia. En este estudio, se presenta una introducción a la \"Ingeniería del Contexto\", una disciplina formal que supera la diseño simple de prompts y integra el sistemaático optimización de la carga de información de un LLM. Se propone una clasificación detallada de la Ingeniería del Contexto, que se divide en sus elementos básicos y su implementación compleja. Primero, se revisan los elementos básicos: búsqueda y generación del contexto, procesamiento y gestión del contexto. A continuación, se investiga cómo estos elementos se integran estructuralmente para crear sistemas complejos: RAG (Recopilación de Archivos y Generación de Texto), sistemas de memoria y lógica de herramientas, y sistemas de múltiples asambleas. Después de analizar sistemáticamente más de 1300 artículos de investigación, este estudio construye un programa técnico que demuestra que los modelos evolucionados mediante la Ingeniería del Contexto muestran una excelente eficiencia para comprender contextos complejos, mientras que también revela limitaciones claras en la generación de largas oraciones complejas. La solución de estas limitaciones es una prioridad clara para futuras investigaciones. Finalmente, este estudio proporciona un marco integral para investigadores y ingenieros que deseen apoyar el desarrollo de AI que puede leer contextos.",
      "upvotes": 65,
      "discussionId": "6879aad021b37e676c8e407a",
      "githubRepo": "https://github.com/Meirtz/Awesome-Context-Engineering",
      "ai_summary": "Context Engineering systematically optimizes information payloads for Large Language Models, addressing gaps in generating sophisticated, long-form outputs.",
      "ai_keywords": [
        "Context Engineering",
        "context retrieval",
        "context generation",
        "context processing",
        "context management",
        "retrieval-augmented generation",
        "memory systems",
        "tool-integrated reasoning",
        "multi-agent systems"
      ],
      "githubStars": 164
    },
    "publishedAt": "2025-07-17T13:50:36.000Z",
    "title": "A Survey of Context Engineering for Large Language Models",
    "summary": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1300 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13334.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63120517ae8896941da4c5da",
      "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
      "fullname": "Lingrui Mei",
      "name": "Chevalier",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.13348",
      "authors": [
        {
          "_id": "6879ba2021b37e676c8e40c9",
          "name": "Senqiao Yang",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40ca",
          "name": "Junyi Li",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40cb",
          "name": "Xin Lai",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40cc",
          "name": "Bei Yu",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40cd",
          "name": "Hengshuang Zhao",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40ce",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:59:55.000Z",
      "submittedOnDailyAt": "2025-07-18T01:47:35.200Z",
      "title": "VisionThink: Modelo de lenguaje visual inteligente y eficiente basado en la teoría de Renors",
      "submittedOnDailyBy": {
        "_id": "6527b7280ae663e384eb8499",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b7280ae663e384eb8499/73yF3eu2cUx7jVZrhXnXx.jpeg",
        "isPro": false,
        "fullname": "Senqiao Yang",
        "user": "Senqiao",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los modelos de lenguaje visual (VLMs) ha aumentado la cantidad de tokens visuales y ha hacido que estos tokens sean más largos que los tokens contextuales, contribuyendo a mejorar el rendimiento. Sin embargo, en la mayoría de los escenarios reales, no es necesario que estos largos tokens visuales. Aunque el rendimiento se ve afectado significativamente en algunas tareas relacionadas con OCR, funcionan adecuadamente en tareas VQA generales a una resolución de cuatro partes. Por lo tanto, proponemos un nuevo paradigma de procesamiento dinámico para diferentes resoluciones y nuevas técnicas de compresión de tokens visuales. Este método comienza con imágenes sub-sampleadas y evalúa inteligentemente si son suficientes para resolver el problema. Si no, el modelo emite un token especial que solicita imágenes de alta resolución. Comparado con los métodos eficientes existentes, VisionThink automáticamente decide si se comprimen individualmente los tokens. Esta capacidad es particularmente fuerte en tareas relacionadas con OCR y permite reducir significativamente los tokens visuales en tareas simples. Proponemos un estrategia de aprendizaje por refuerzo utilizando el enfoque LLM-as-Judge y lo aplicamos en tareas VQA generales. Además, diseñamos rigurosamente funciones de recompensa y estructuras de recompensa para alcanzar una proporción estable de llamadas de reescalado de resolución. Las pruebas de extensión muestran la eficacia, eficiencia y eficacia de nuestro método. El código está disponible en https://github.com/dvlab-research/VisionThink.",
      "upvotes": 38,
      "discussionId": "6879ba2121b37e676c8e40cf",
      "githubRepo": "https://github.com/dvlab-research/VisionThink",
      "ai_summary": "VisionThink dynamically adjusts image resolution and visual token processing for efficient and effective vision-language tasks, improving performance on OCR tasks while reducing token usage in simpler tasks.",
      "ai_keywords": [
        "vision-language models",
        "visual tokens",
        "text tokens",
        "downsampled image",
        "smart decision-making",
        "special token",
        "Efficient VLM",
        "token compression",
        "reinforcement learning",
        "LLM-as-Judge",
        "reward function",
        "penalty mechanism",
        "image resize call ratio"
      ],
      "githubStars": 25
    },
    "publishedAt": "2025-07-17T13:59:55.000Z",
    "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning",
    "summary": "Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13348.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6527b7280ae663e384eb8499",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b7280ae663e384eb8499/73yF3eu2cUx7jVZrhXnXx.jpeg",
      "fullname": "Senqiao Yang",
      "name": "Senqiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.13347",
      "authors": [
        {
          "_id": "6879b78a21b37e676c8e40b1",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b2",
          "name": "Jianjun Zhou",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b3",
          "name": "Haoyi Zhu",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b4",
          "name": "Wenzheng Chang",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b5",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b6",
          "name": "Zizun Li",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b7",
          "name": "Junyi Chen",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b8",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b9",
          "name": "Chunhua Shen",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40ba",
          "name": "Tong He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:59:53.000Z",
      "submittedOnDailyAt": "2025-07-18T01:26:57.410Z",
      "title": "π^3: Verificación de la Igualdad de Orden y Valor en el Aprendizaje Geométrico Visual",
      "submittedOnDailyBy": {
        "_id": "6747ede3a9c72aebe1322382",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/inILqQ05sESbYLdsEldJ_.png",
        "isPro": false,
        "fullname": "Tong He",
        "user": "tonghe90",
        "type": "user"
      },
      "summary": "pi^3 es una red neuronal de proa que rompe la dependencia de un punto fijo decisivo y proporciona una nueva aproximación para la reconstrucción geométrica visual. Los métodos existentes tienen que quedar fijos en una reconstrucción específica de una vista, lo que puede llevar a instabilidad o fracaso cuando el punto de referencia no es óptimo, introduciendo asimetrías. En cambio, pi^3 no utiliza un marco de referencia y utiliza una arquitectura con simetría intercambiable completa para predecir la posición de la cámara con afín invariable y los puntos de referencia local con escala invariable. Esta diseño tiene una fuerte robustez frente a la secuencia de entrada y alta eficiencia. Debido a estos excelentes puntos, nuestro método sencillo y asimétrico desarrolla los mejores resultados en una amplia gama de tareas, como la estimación de la orientación de la cámara, la medición de la profundidad monocular/video y la reconstrucción de mapas de puntos densos. El código y el modelo están disponibles públicamente.",
      "upvotes": 30,
      "discussionId": "6879b78b21b37e676c8e40bb",
      "projectPage": "https://yyfz.github.io/pi3/",
      "githubRepo": "https://github.com/yyfz/Pi3",
      "ai_summary": "A permutation-equivariant neural network, $\\pi^3$, reconstructs visual geometry without a fixed reference view, achieving state-of-the-art performance in camera pose estimation, depth estimation, and point map reconstruction.",
      "ai_keywords": [
        "feed-forward neural network",
        "permutation-equivariant architecture",
        "affine-invariant",
        "scale-invariant",
        "camera pose estimation",
        "monocular depth estimation",
        "video depth estimation",
        "dense point map reconstruction"
      ],
      "githubStars": 104
    },
    "publishedAt": "2025-07-17T13:59:53.000Z",
    "title": "π^3: Scalable Permutation-Equivariant Visual Geometry Learning",
    "summary": "We introduce pi^3, a feed-forward neural network that offers a novel\napproach to visual geometry reconstruction, breaking the reliance on a\nconventional fixed reference view. Previous methods often anchor their\nreconstructions to a designated viewpoint, an inductive bias that can lead to\ninstability and failures if the reference is suboptimal. In contrast, pi^3\nemploys a fully permutation-equivariant architecture to predict\naffine-invariant camera poses and scale-invariant local point maps without any\nreference frames. This design makes our model inherently robust to input\nordering and highly scalable. These advantages enable our simple and bias-free\napproach to achieve state-of-the-art performance on a wide range of tasks,\nincluding camera pose estimation, monocular/video depth estimation, and dense\npoint map reconstruction. Code and models are publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13347.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6747ede3a9c72aebe1322382",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/inILqQ05sESbYLdsEldJ_.png",
      "fullname": "Tong He",
      "name": "tonghe90",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.13332",
      "authors": [
        {
          "_id": "6879b01621b37e676c8e40a8",
          "user": {
            "_id": "66214b4e4991d64ad0e28675",
            "avatarUrl": "/avatars/2574928aab7e45bc581c567d556a4cfd.svg",
            "isPro": false,
            "fullname": "Zhouqi Hua",
            "user": "ZhouqiHUA",
            "type": "user"
          },
          "name": "Zhouqi Hua",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:38.507Z",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40a9",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40aa",
          "name": "Chengqi Lyu",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ab",
          "user": {
            "_id": "6601196cc91ba4c08ad6e270",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
            "isPro": false,
            "fullname": "yuzhe gu",
            "user": "vanilla1116",
            "type": "user"
          },
          "name": "Yuzhe Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:36.203Z",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ac",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ad",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ae",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:50:07.000Z",
      "submittedOnDailyAt": "2025-07-18T00:59:46.053Z",
      "title": "「El fabricante de juegos: El fabricante de máquinas de torreo es generalizable en longitudes」",
      "submittedOnDailyBy": {
        "_id": "6601196cc91ba4c08ad6e270",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
        "isPro": false,
        "fullname": "yuzhe gu",
        "user": "vanilla1116",
        "type": "user"
      },
      "summary": "La capacidad de generalización de longitud, es decir, el poder de resolver problemas de secuencias largas no encontradas durante el período de entrenamiento, es un problema fundamental en los grandes modelos de lenguaje basados en Transformer. Los estudios previos se han centrado principalmente en un enfoque de datos dirigido hacia cálculos numéricos o operaciones de símbolos, lo que ha limitado el rendimiento generalizado en tareas específicas. Para encontrar soluciones más generales, este estudio centra su investigación en problemas que se pueden resolver por cálculos computables. Desde esta perspectiva, se propone la integración de datos de cómputos computacionales que modelan el proceso de ejecución de un Torraín, llamándolo Torraín Machine Embedded Learning (TAIL), con el objetivo de mejorar la capacidad de generalización de longitud en modelos de lenguaje grandes (LLM). TAIL expande los cómputos computacionales de manera lineal a estados atomicos, reduce el entrenamiento de corto plazo y mitiga las dificultades de acceso a datos de larga distancia dinámicos, introduciendo un mecanismo de pistas de memoria explícita. Para probar la confiabilidad y la generalización de TAIL, se construyó un conjunto de datos complejos que incluye 8 algoritmos y 18 tareas. Utilizando solo estes datos, TAIL mejoró significativamente la capacidad de generalización de longitud y el rendimiento en diversas tareas, superando los métodos existentes y DeepSeek-R1. Los resultados de los experimentos muestran claramente que el concepto básico de Torraín Machine es esencial para la generalización de longitud, más que el estilo de pensamiento. Este modelo muestra un comportamiento de lectura y escritura que coincide con las características de Torraín Machine en la capa de atención. Este estudio proporciona una dirección adecuada para futuras investigaciones sobre el aprendizaje de razonamiento en modelos de lenguaje grandes.",
      "upvotes": 30,
      "discussionId": "6879b01721b37e676c8e40af",
      "ai_summary": "TAIL, a method that imitates Turing Machine execution processes, enhances the length generalization and performance of LLMs by synthesizing chain-of-thought data and reducing shortcut learning.",
      "ai_keywords": [
        "Transformer-based large language models",
        "length generalization",
        "Turing MAchine Imitation Learning",
        "TAIL",
        "chain-of-thoughts",
        "Turing Machine",
        "synthetic dataset",
        "Qwen2.5-7B",
        "read-and-write behaviors",
        "attention layers"
      ]
    },
    "publishedAt": "2025-07-17T13:50:07.000Z",
    "title": "The Imitation Game: Turing Machine Imitator is Length Generalizable\n  Reasoner",
    "summary": "Length generalization, the ability to solve problems of longer sequences than\nthose observed during training, poses a core challenge of Transformer-based\nlarge language models (LLM). Although existing studies have predominantly\nfocused on data-driven approaches for arithmetic operations and symbolic\nmanipulation tasks, these approaches tend to be task-specific with limited\noverall performance. To pursue a more general solution, this paper focuses on a\nbroader case of reasoning problems that are computable, i.e., problems that\nalgorithms can solve, thus can be solved by the Turing Machine. From this\nperspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to\nimprove the length generalization ability of LLMs. TAIL synthesizes\nchain-of-thoughts (CoT) data that imitate the execution process of a Turing\nMachine by computer programs, which linearly expands the reasoning steps into\natomic states to alleviate shortcut learning and explicit memory fetch\nmechanism to reduce the difficulties of dynamic and long-range data access in\nelementary operations. To validate the reliability and universality of TAIL, we\nconstruct a challenging synthetic dataset covering 8 classes of algorithms and\n18 tasks. Without bells and whistles, TAIL significantly improves the length\ngeneralization ability as well as the performance of Qwen2.5-7B on various\ntasks using only synthetic data, surpassing previous methods and DeepSeek-R1.\nThe experimental results reveal that the key concepts in the Turing Machine,\ninstead of the thinking styles, are indispensable for TAIL for length\ngeneralization, through which the model exhibits read-and-write behaviors\nconsistent with the properties of the Turing Machine in their attention layers.\nThis work provides a promising direction for future research in the learning of\nLLM reasoning from synthetic data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13332.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12841",
      "authors": [
        {
          "_id": "6879bba621b37e676c8e4195",
          "name": "Yiming Ren",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4196",
          "name": "Zhiqiang Lin",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4197",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4198",
          "name": "Gao Meng",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4199",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419a",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419b",
          "name": "Zicheng Lin",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419c",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419d",
          "name": "Yujiu Yang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419e",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419f",
          "user": {
            "_id": "642e3bcb958faf258a40e89c",
            "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
            "isPro": false,
            "fullname": "Ruihang Chu",
            "user": "Ruihang",
            "type": "user"
          },
          "name": "Ruihang Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:30.538Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T07:04:05.000Z",
      "submittedOnDailyAt": "2025-07-18T02:04:48.913Z",
      "title": "Project AnyCap: Un solo marco de trabajo, conjunto de datos y benchmark, pero capturación de caras de todas las direcciones controlable",
      "submittedOnDailyBy": {
        "_id": "642e3bcb958faf258a40e89c",
        "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
        "isPro": false,
        "fullname": "Ruihang Chu",
        "user": "Ruihang",
        "type": "user"
      },
      "summary": "El controlable captioning es necesario una disposición precisa de modelos y instrucciones, pero actualmente los modelos presentan deficiencias en el control fino y en los protocolos de evaluación confiables. Para remediar esto, presentamos el proyecto AnyCap, que es una solución integrada que combina modelos, datasets y evaluaciones. Se presenta el AnyCapModel (ACM), un framework ligero que permite mejorar la posibilidad de control del modelo básico sin necesidad de reentrenarlo. ACM reutiliza la captioning original de los modelos básicos y combina esto con instrucciones del usuario y características del modelo para generar captiones mejoradas. Para complementar la escasez de datos de captioning controlable, se ha construido el AnyCapDataset (ACD), que registra 3 modelos, 28 tipos de instrucciones del usuario y 300k entradas de alta calidad. Además, se propone el AnyCapEval, que separa la precisión del contenido del respeto al estilo, proporcionando criterios de evaluación confiables para captioning controlable. ACM mejora significativamente la calidad de captioning en los modelos básicos en el AnyCapEval. En particular, ACM-8B aumenta en más de 45% el score de contenido y en más de 12% el score de estilo de GPT-4o, y también obtiene buenos resultados en marcos comunes como MIA-Bench y VidCapBench.",
      "upvotes": 27,
      "discussionId": "6879bba721b37e676c8e41a0",
      "githubRepo": "https://github.com/qishisuren123/AnyCap",
      "ai_summary": "The AnyCap Project introduces a framework, dataset, and evaluation protocol to enhance controllability and reliability in multimodal captioning.",
      "ai_keywords": [
        "AnyCapModel",
        "ACM",
        "omni-modal captioning",
        "AnyCapDataset",
        "ACD",
        "AnyCapEval",
        "content accuracy",
        "stylistic fidelity",
        "MIA-Bench",
        "VidCapBench"
      ],
      "githubStars": 27
    },
    "publishedAt": "2025-07-17T03:04:05.000Z",
    "title": "AnyCap Project: A Unified Framework, Dataset, and Benchmark for\n  Controllable Omni-modal Captioning",
    "summary": "Controllable captioning is essential for precise multimodal alignment and\ninstruction following, yet existing models often lack fine-grained control and\nreliable evaluation protocols. To address this gap, we present the AnyCap\nProject, an integrated solution spanning model, dataset, and evaluation. We\nintroduce AnyCapModel (ACM), a lightweight plug-and-play framework that\nenhances the controllability of existing foundation models for omni-modal\ncaptioning without retraining the base model. ACM reuses the original captions\nfrom base models while incorporating user instructions and modality features to\ngenerate improved captions. To remedy the data scarcity in controllable\nmultimodal captioning, we build AnyCapDataset (ACD), covering three modalities,\n28 user-instruction types, and 300\\,k high-quality data entries. We further\npropose AnyCapEval, a new benchmark that provides more reliable evaluation\nmetrics for controllable captioning by decoupling content accuracy and\nstylistic fidelity. ACM markedly improves caption quality across a diverse set\nof base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores\nby 45\\% and style scores by 12\\%, and it also achieves substantial gains on\nwidely used benchmarks such as MIA-Bench and VidCapBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12841.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e3bcb958faf258a40e89c",
      "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
      "fullname": "Ruihang Chu",
      "name": "Ruihang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.13344",
      "authors": [
        {
          "_id": "6879f3aa21b37e676c8e4202",
          "user": {
            "_id": "649958942ca6f96c8b8c1076",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649958942ca6f96c8b8c1076/olfLIqNryaog1nAnQPwkN.jpeg",
            "isPro": false,
            "fullname": "Yudong Jin",
            "user": "krahets",
            "type": "user"
          },
          "name": "Yudong Jin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:22.520Z",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4203",
          "name": "Sida Peng",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4204",
          "name": "Xuan Wang",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4205",
          "name": "Tao Xie",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4206",
          "name": "Zhen Xu",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4207",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4208",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4209",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e420a",
          "name": "Xiaowei Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649958942ca6f96c8b8c1076/G5xpU1DSr8smrbqbom9hU.mp4"
      ],
      "publishedAt": "2025-07-17T17:59:17.000Z",
      "submittedOnDailyAt": "2025-07-18T06:34:43.522Z",
      "title": "Diffuman4D: Síntesis de visión humana 4D en consonancia con un video de visión 2D esparcido utilizando un modelo de dispersión temporal espectral.",
      "submittedOnDailyBy": {
        "_id": "649958942ca6f96c8b8c1076",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649958942ca6f96c8b8c1076/olfLIqNryaog1nAnQPwkN.jpeg",
        "isPro": false,
        "fullname": "Yudong Jin",
        "user": "krahets",
        "type": "user"
      },
      "summary": "Este artículo aborda el problema de la síntesis visual de alta calidad a partir de vídeos raros. Los métodos existentes utilizan modelos de Difusión 4D para generar nuevos vídeos desde nuevas vistas, lo que resuelve el problema de la falta de observación, pero los vídeos generados por estos modelos suelen perder la coherencia espectral en el tiempo, lo que afecta la calidad de la síntesis visual. En este artículo, se propone un nuevo proceso de desnivelación iterativo con un enfoque de ventana deslizante para elevar la coherencia espectral en el tiempo del modelo de Difusión 4D. Específicamente, se define una grilla potencial que incluye vídeos, estados de cámara y estados humanos, y se utilizan ventanas deslizantes en ambos extremos del tiempo espectral para intercambiar el desnivelado, finalmente decodificando un vídeo en la visión objetivo. En el enfoque iterativo de deslizamiento, la información fluye suficientemente a través de la anchura de la grilla potencial, permitiendo al modelo de Difusión 4D una gran recepción de campo y, por lo tanto, una mayor coherencia en el tiempo 4D de los resultados. Además, este enfoque permite controlar la consumo de memoria del GPU. Los experimentos con los conjuntos de datos DNA-Rendering y ActorsHQ demuestran que nuestro enfoque es capaz de generar vídeos de nuevas visiones de alta calidad y coherencia, superando significativamente los métodos actuales. Puede ver un demo interactivo y resultados de video en la página del proyecto: https://diffuman4d.github.io/.",
      "upvotes": 18,
      "discussionId": "6879f3ab21b37e676c8e420b",
      "projectPage": "https://diffuman4d.github.io/",
      "githubRepo": "https://github.com/zju3dv/Diffuman4D",
      "ai_summary": "A sliding iterative denoising process is proposed to enhance spatio-temporal consistency in 4D diffusion models for high-fidelity view synthesis from sparse-view videos.",
      "ai_keywords": [
        "4D diffusion models",
        "sliding iterative denoising",
        "latent grid",
        "image",
        "camera pose",
        "human pose",
        "spatio-temporal consistency",
        "GPU memory consumption",
        "DNA-Rendering",
        "ActorsHQ"
      ],
      "githubStars": 48
    },
    "publishedAt": "2025-07-17T13:59:17.000Z",
    "title": "Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos\n  with Spatio-Temporal Diffusion Models",
    "summary": "This paper addresses the challenge of high-fidelity view synthesis of humans\nwith sparse-view videos as input. Previous methods solve the issue of\ninsufficient observation by leveraging 4D diffusion models to generate videos\nat novel viewpoints. However, the generated videos from these models often lack\nspatio-temporal consistency, thus degrading view synthesis quality. In this\npaper, we propose a novel sliding iterative denoising process to enhance the\nspatio-temporal consistency of the 4D diffusion model. Specifically, we define\na latent grid in which each latent encodes the image, camera pose, and human\npose for a certain viewpoint and timestamp, then alternately denoising the\nlatent grid along spatial and temporal dimensions with a sliding window, and\nfinally decode the videos at target viewpoints from the corresponding denoised\nlatents. Through the iterative sliding, information flows sufficiently across\nthe latent grid, allowing the diffusion model to obtain a large receptive field\nand thus enhance the 4D consistency of the output, while making the GPU memory\nconsumption affordable. The experiments on the DNA-Rendering and ActorsHQ\ndatasets demonstrate that our method is able to synthesize high-quality and\nconsistent novel-view videos and significantly outperforms the existing\napproaches. See our project page for interactive demos and video results:\nhttps://diffuman4d.github.io/ .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649958942ca6f96c8b8c1076/G5xpU1DSr8smrbqbom9hU.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649958942ca6f96c8b8c1076",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649958942ca6f96c8b8c1076/olfLIqNryaog1nAnQPwkN.jpeg",
      "fullname": "Yudong Jin",
      "name": "krahets",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12508",
      "authors": [
        {
          "_id": "6879af4f21b37e676c8e409b",
          "user": {
            "_id": "65e919332fd9300c7eb96556",
            "avatarUrl": "/avatars/a826f7e14acf34603aa68e4fc27f12af.svg",
            "isPro": false,
            "fullname": "Yuncong Yang",
            "user": "yyuncong",
            "type": "user"
          },
          "name": "Yuncong Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:42.036Z",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409c",
          "name": "Jiageng Liu",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409d",
          "name": "Zheyuan Zhang",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409e",
          "name": "Siyuan Zhou",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409f",
          "name": "Reuben Tan",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e40a0",
          "name": "Jianwei Yang",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e40a1",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e40a2",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65e919332fd9300c7eb96556/sRj5beNxspccadVwKC_vj.mp4"
      ],
      "publishedAt": "2025-07-16T17:59:36.000Z",
      "submittedOnDailyAt": "2025-07-18T00:56:28.122Z",
      "title": "El Deseo del Cerebro: Lógica Espacial por Escalado en la Verificación con Modelos de Inteligencia Artificial",
      "submittedOnDailyBy": {
        "_id": "65e919332fd9300c7eb96556",
        "avatarUrl": "/avatars/a826f7e14acf34603aa68e4fc27f12af.svg",
        "isPro": false,
        "fullname": "Yuncong Yang",
        "user": "yyuncong",
        "type": "user"
      },
      "summary": "El espacio espacial de la inferencia en el espacio 3D es un aspecto central de la cognición humana y es esencial para tareas como la mapeo y el manejo. Sin embargo, los modelos de lenguaje visual (VLMs) de vanguardia tienen dificultades incluso con tareas simples: no poseen un modelo interno de la dinámica 3D solo a partir de imágenes 2D. Por lo tanto, proponemos \"MindJourney\", un marco de escalado de tiempo de prueba basado en difusión de video que combina con modelos de mundo controlables para proporcionar a los VLMs habilidades que faltan. Los VLMs pintan repetidamente la visión relativa sintetizada por el modelo de mundo en cada etapa, creando razones a partir de múltiples evidencias recopiladas por exploración interactiva. Sin entrenamiento final, nuestro MindJourney logra un aumento del rendimiento del 8% en promedio en los benchmarks espaciales de inferencia SAT, demostrando que la combinación de VLMs y modelos de mundo permite un escalado de tiempo de prueba de manera sencilla y potente para la inferencia 3D. Además, nuestro método mejora los VLMs de inferencia de tiempo de prueba entrenados con aprendizaje por refuerzo y muestra la posibilidad de utilizar modelos de mundo en el escalado de tiempo de prueba.",
      "upvotes": 10,
      "discussionId": "6879af5021b37e676c8e40a3",
      "projectPage": "https://umass-embodied-agi.github.io/MindJourney/",
      "githubRepo": "https://github.com/UMass-Embodied-AGI/MindJourney",
      "ai_summary": "MindJourney enhances vision-language models with 3D reasoning by coupling them with a video diffusion-based world model, achieving improved performance on spatial reasoning tasks without fine-tuning.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "world model",
        "video diffusion",
        "camera trajectory",
        "multi-view evidence",
        "spatial reasoning",
        "SAT benchmark",
        "reinforcement learning"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-07-16T13:59:36.000Z",
    "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning",
    "summary": "Spatial reasoning in 3D space is central to human cognition and indispensable\nfor embodied tasks such as navigation and manipulation. However,\nstate-of-the-art vision-language models (VLMs) struggle frequently with tasks\nas simple as anticipating how a scene will look after an egocentric motion:\nthey perceive 2D images but lack an internal model of 3D dynamics. We therefore\npropose MindJourney, a test-time scaling framework that grants a VLM with this\nmissing capability by coupling it to a controllable world model based on video\ndiffusion. The VLM iteratively sketches a concise camera trajectory, while the\nworld model synthesizes the corresponding view at each step. The VLM then\nreasons over this multi-view evidence gathered during the interactive\nexploration. Without any fine-tuning, our MindJourney achieves over an average\n8% performance boost on the representative spatial reasoning benchmark SAT,\nshowing that pairing VLMs with world models for test-time scaling offers a\nsimple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also\nimproves upon the test-time inference VLMs trained through reinforcement\nlearning, which demonstrates the potential of our method that utilizes world\nmodels for test-time scaling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e919332fd9300c7eb96556/sRj5beNxspccadVwKC_vj.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e919332fd9300c7eb96556",
      "avatarUrl": "/avatars/a826f7e14acf34603aa68e4fc27f12af.svg",
      "fullname": "Yuncong Yang",
      "name": "yyuncong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.13300",
      "authors": [
        {
          "_id": "6879c27e21b37e676c8e41a9",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41aa",
          "name": "Weiyuan Chen",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ab",
          "name": "Zhijian Xu",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ac",
          "name": "Manasi Patwardhan",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ad",
          "name": "Yixin Liu",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ae",
          "name": "Chengye Wang",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41af",
          "name": "Lovekesh Vig",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41b0",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:09:22.000Z",
      "submittedOnDailyAt": "2025-07-18T02:12:09.802Z",
      "title": "AbGen: Evaluación de modelos de lenguaje grandes en el diseño y evaluación de pruebas de eliminación de investigación científica",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "Aquí se presenta el primer benchmark \"AbGen\" para evaluar la capacidad de diseño de investigaciones sobre la extinción en la investigación científica. AbGen está compuesto por 1,500 ejemplos evaluados por expertos en 807 artículos de NLP. En este benchmark, se presenta como tarea a un LLM la generación de diseños de investigaciones sobre la extinción de módulos o procesos específicos en el contexto de una investigación dada. Según la evaluación de modelos avanzados como DeepSeek-R1-0528 y o4-mini, se confirmó que existen claras diferencias de rendimiento entre modelos y expertos humanos en la importancia, fidelidad y pertinencia del diseño de investigaciones sobre la extinción. Además, se observó que los métodos de evaluación automática actuales tienen una confianza evidentemente insuficiente en comparación con la evaluación humana. Para investigar este punto más detalladamente, se desarrolló el meta-benchmark \"AbGen-Eval\" para evaluar la confianza de los sistemas de evaluación automática. En AbGen-Eval, se investigan diversos sistemas de jurados basados en LLM para medir la confianza de los sistemas automáticos generales de evaluación de la performance de un LLM en tareas científicas complejas, y se proporcionan guías para la futura investigación que pueden ayudar a desarrollar sistemas de evaluación basados en LLM eficientes y confiables.",
      "upvotes": 9,
      "discussionId": "6879c27f21b37e676c8e41b1",
      "githubRepo": "https://github.com/yale-nlp/AbGen",
      "ai_summary": "AbGen evaluates LLMs in designing ablation studies for scientific research, revealing performance gaps compared to human experts and highlighting the unreliability of current automated evaluation methods.",
      "ai_keywords": [
        "LLMs",
        "ablation studies",
        "NLP papers",
        "DeepSeek-R1-0528",
        "o4-mini",
        "AbGen-Eval",
        "LLM-as-Judge"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-07-17T13:09:22.000Z",
    "title": "AbGen: Evaluating Large Language Models in Ablation Study Design and\n  Evaluation for Scientific Research",
    "summary": "We introduce AbGen, the first benchmark designed to evaluate the capabilities\nof LLMs in designing ablation studies for scientific research. AbGen consists\nof 1,500 expert-annotated examples derived from 807 NLP papers. In this\nbenchmark, LLMs are tasked with generating detailed ablation study designs for\na specified module or process based on the given research context. Our\nevaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a\nsignificant performance gap between these models and human experts in terms of\nthe importance, faithfulness, and soundness of the ablation study designs.\nMoreover, we demonstrate that current automated evaluation methods are not\nreliable for our task, as they show a significant discrepancy when compared to\nhuman assessment. To better investigate this, we develop AbGen-Eval, a\nmeta-evaluation benchmark designed to assess the reliability of commonly used\nautomated evaluation systems in measuring LLM performance on our task. We\ninvestigate various LLM-as-Judge systems on AbGen-Eval, providing insights for\nfuture research on developing more effective and reliable LLM-based evaluation\nsystems for complex scientific tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13300.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.12956",
      "authors": [
        {
          "_id": "6879df6521b37e676c8e41cd",
          "user": {
            "_id": "653b195c5f1703225b2fd571",
            "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
            "isPro": false,
            "fullname": "wangqiang",
            "user": "wangqiang9",
            "type": "user"
          },
          "name": "Qiang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:24.746Z",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41ce",
          "name": "Mengchao Wang",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41cf",
          "name": "Fan Jiang",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41d0",
          "name": "Yaqi Fan",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41d1",
          "name": "Yonggang Qi",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41d2",
          "name": "Mu Xu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/653b195c5f1703225b2fd571/vkwYxJilAke5PetjLV8lm.mp4"
      ],
      "publishedAt": "2025-07-17T09:50:43.000Z",
      "submittedOnDailyAt": "2025-07-18T04:28:50.046Z",
      "title": "\"Qué estoy buscando en una traducción precisa y profesional del texto inglés proporcionado. Aquí está la traducción al español:\"\n\n\"Questar Pop: Mejora de animación de múltiples personajes utilizando un Transformer difuso que mejora la expresión\"",
      "submittedOnDailyBy": {
        "_id": "653b195c5f1703225b2fd571",
        "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
        "isPro": false,
        "fullname": "wangqiang",
        "user": "wangqiang9",
        "type": "user"
      },
      "summary": "La generación de animación de caras expresivas durante la detención es un desafío complejo. Los métodos existentes se basan en conceptos geométricos explícitos y presentan artefactos al recrear y, además, no capturan con precisión emociones subtil. Además, actualmente no soportan la animación de múltiples personajes. La interferencia entre características de diferentes individuos complica los problemas. Para resolver estos desafíos, proponemos FantasyPortrait, un marco de trabajo basado en canales de difusión que genera animación emocional de alta calidad. Nuestro enfoque introduce una estrategia de aprendizaje que expande las expresiones y utiliza representaciones ocultas para capturar los movimientos faciales, mejorando la capacidad de expresión emocional del modelo. En el control de múltiples personajes, diseñamos una estructura de aténción con máscaras para generar expresiones de manera dinámica y independiente, evitando especialmente la interferencia entre características. Contribuimos a este campo con el Multi-Expr dataset y ExprBench, un conjunto de datos y un marco de referencia especializados para la entrenamiento y evaluación de animación de múltiples personajes. Los experimentos extendidos muestran que FantasyPortrait supera los métodos más recientes, destacando especialmente en evaluaciones cualitativas y cuantitativas, así como en la recreación y el contexto complejo de múltiples personajes. Nuestra página del proyecto está disponible en https://fantasy-amap.github.io/fantasy-portrait/.",
      "upvotes": 9,
      "discussionId": "6879df6521b37e676c8e41d3",
      "projectPage": "https://fantasy-amap.github.io/fantasy-portrait/",
      "githubRepo": "https://github.com/Fantasy-AMAP/fantasy-portrait",
      "ai_summary": "FantasyPortrait, a diffusion transformer framework, generates high-fidelity and emotion-rich facial animations for single and multi-character scenarios using implicit representations and a masked cross-attention mechanism.",
      "ai_keywords": [
        "diffusion transformer",
        "expression-augmented learning",
        "implicit representations",
        "masked cross-attention mechanism",
        "Multi-Expr dataset",
        "ExprBench"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-07-17T05:50:43.000Z",
    "title": "FantasyPortrait: Enhancing Multi-Character Portrait Animation with\n  Expression-Augmented Diffusion Transformers",
    "summary": "Producing expressive facial animations from static images is a challenging\ntask. Prior methods relying on explicit geometric priors (e.g., facial\nlandmarks or 3DMM) often suffer from artifacts in cross reenactment and\nstruggle to capture subtle emotions. Furthermore, existing approaches lack\nsupport for multi-character animation, as driving features from different\nindividuals frequently interfere with one another, complicating the task. To\naddress these challenges, we propose FantasyPortrait, a diffusion transformer\nbased framework capable of generating high-fidelity and emotion-rich animations\nfor both single- and multi-character scenarios. Our method introduces an\nexpression-augmented learning strategy that utilizes implicit representations\nto capture identity-agnostic facial dynamics, enhancing the model's ability to\nrender fine-grained emotions. For multi-character control, we design a masked\ncross-attention mechanism that ensures independent yet coordinated expression\ngeneration, effectively preventing feature interference. To advance research in\nthis area, we propose the Multi-Expr dataset and ExprBench, which are\nspecifically designed datasets and benchmarks for training and evaluating\nmulti-character portrait animations. Extensive experiments demonstrate that\nFantasyPortrait significantly outperforms state-of-the-art methods in both\nquantitative metrics and qualitative evaluations, excelling particularly in\nchallenging cross reenactment and multi-character contexts. Our project page is\nhttps://fantasy-amap.github.io/fantasy-portrait/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/653b195c5f1703225b2fd571/vkwYxJilAke5PetjLV8lm.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12956.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b195c5f1703225b2fd571",
      "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
      "fullname": "wangqiang",
      "name": "wangqiang9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12990",
      "authors": [
        {
          "_id": "6879fdc821b37e676c8e422b",
          "name": "Nikita Koriagin",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422c",
          "name": "Yaroslav Aksenov",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422d",
          "user": {
            "_id": "634c5f8cfb80cc6bcaf42c03",
            "avatarUrl": "/avatars/1f37db0e70cbaf9707f4c8cbcee37ca0.svg",
            "isPro": false,
            "fullname": "Daniil Laptev",
            "user": "dlaptev",
            "type": "user"
          },
          "name": "Daniil Laptev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T08:32:16.310Z",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422e",
          "name": "Gleb Gerasimov",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422f",
          "user": {
            "_id": "60b364e7f88532cd79eaff7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
            "isPro": false,
            "fullname": "Nikita Balagansky",
            "user": "elephantmipt",
            "type": "user"
          },
          "name": "Nikita Balagansky",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T08:32:14.730Z",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e4230",
          "user": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "isPro": false,
            "fullname": "Daniil Gavrilov",
            "user": "kefirski",
            "type": "user"
          },
          "name": "Daniil Gavrilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T08:32:17.917Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60b364e7f88532cd79eaff7b/puU_3A7T0uk5E0fGkoc1k.png"
      ],
      "publishedAt": "2025-07-17T10:57:49.000Z",
      "submittedOnDailyAt": "2025-07-18T06:31:25.084Z",
      "title": "Enseña a los SAEs antiguos nuevas trucas de dominio con Boosting\n\nEste artículo explica cómo utilizar \"Boosting\" para enseñar a los \"SAEs antiguos\" nuevas trucas en un dominio. \"SAEs\" significa \"Expertos de la Técnica del Estado del Arte\", mientras que \"Boosting\" es una tecnología que mejora el rendimiento en el aprendizaje automático. Este artículo describe cómo los \"SAEs antiguos\" pueden mejorar su rendimiento en un nuevo dominio utilizando \"Boosting\".",
      "submittedOnDailyBy": {
        "_id": "60b364e7f88532cd79eaff7b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
        "isPro": false,
        "fullname": "Nikita Balagansky",
        "user": "elephantmipt",
        "type": "user"
      },
      "summary": "Los autoencoderes esparsos han aparecido como una herramienta poderosa para interpretar las representaciones internas de los grandes modelos de lenguaje, pero tienden a fallar al no extraer características específicas. En este artículo, se propone una aproximación de aprendizaje residual para complementar estas deficiencias y se propone un método que evite la necesidad de reentrenamiento completo. En particular, se propone entrenar un SAE (Autoencoder Esparso) y modelar los errores de reconstrucción de las características específicas de un SAE preentrenado, permitiendo que el modelo principal capture eficazmente las características no detectadas. Al combinar los resultados de ambos modelos en la inferencia, se observó una mejora significativa en la entropía cruzada y en el métrica de variación explicable de los modelos de lenguaje grandes. Los resultados de los experimentos demuestran que este método integra eficientemente nuevos miembros en la membresía y mantiene el rendimiento general de los modelos. Este enfoque permite seleccionar de manera selectiva la mejora de la interpretación mecánica en ciertos miembros, abriendo nuevas posibilidades para la interpretación mecánica objetiva de los modelos de lenguaje grandes.",
      "upvotes": 4,
      "discussionId": "6879fdc821b37e676c8e4231",
      "ai_summary": "A residual learning approach enhances Sparse Autoencoders to capture domain-specific features without retraining, improving interpretability and performance on specialized domains.",
      "ai_keywords": [
        "Sparse Autoencoders",
        "residual learning",
        "reconstruction error",
        "cross-entropy",
        "explained variance",
        "targeted mechanistic interpretability",
        "Large Language Models"
      ]
    },
    "publishedAt": "2025-07-17T06:57:49.000Z",
    "title": "Teach Old SAEs New Domain Tricks with Boosting",
    "summary": "Sparse Autoencoders have emerged as powerful tools for interpreting the\ninternal representations of Large Language Models, yet they often fail to\ncapture domain-specific features not prevalent in their training corpora. This\npaper introduces a residual learning approach that addresses this feature\nblindness without requiring complete retraining. We propose training a\nsecondary SAE specifically to model the reconstruction error of a pretrained\nSAE on domain-specific texts, effectively capturing features missed by the\nprimary model. By summing the outputs of both models during inference, we\ndemonstrate significant improvements in both LLM cross-entropy and explained\nvariance metrics across multiple specialized domains. Our experiments show that\nthis method efficiently incorporates new domain knowledge into existing SAEs\nwhile maintaining their performance on general tasks. This approach enables\nresearchers to selectively enhance SAE interpretability for specific domains of\ninterest, opening new possibilities for targeted mechanistic interpretability\nof LLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60b364e7f88532cd79eaff7b/puU_3A7T0uk5E0fGkoc1k.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60b364e7f88532cd79eaff7b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
      "fullname": "Nikita Balagansky",
      "name": "elephantmipt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12720",
      "authors": [
        {
          "_id": "68799d0521b37e676c8e4060",
          "name": "Abraham Toluase Owodunni",
          "hidden": false
        },
        {
          "_id": "68799d0521b37e676c8e4061",
          "name": "Orevaoghene Ahia",
          "hidden": false
        },
        {
          "_id": "68799d0521b37e676c8e4062",
          "name": "Sachin Kumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T01:55:41.000Z",
      "submittedOnDailyAt": "2025-07-18T00:36:57.429Z",
      "title": "FLEXITOKENS: Modelos de lenguaje evolucionando para un tokenización flexible",
      "submittedOnDailyBy": {
        "_id": "626d1e1e72169e781945bf44",
        "avatarUrl": "/avatars/6bf9f35042b6d939f2ab525816ad0423.svg",
        "isPro": false,
        "fullname": "Abraham  Owodunni",
        "user": "Owos",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje (LMs) tienen dificultades para adaptarse a nuevas distribuciones de datos con un ajuste débil. Esto se debe a la rigidez de los sub-tokenizadores. Esta inmutabilidad generalmente no cambia durante el período de adaptación, lo que hace que muchas veces se ataquen procesos de tokenización inútiles debido a nuevas distribuciones de datos, idiomas no vistos o excesivos fragmentos de scripts. En este estudio, se desarrollan LMs a nivel de bytes con un tokenizador de entrada adaptable, creando un proceso de tokenización que se adapta. El modelo incluye un submódulo que predice los límites de las secuencias de bytes de entrada, lo cual se convierte en segmentos de transformación. Los métodos actuales sin tokenizadores entrenan la predicción de los límites de los tokenizadores con pérdidas de auxilio y imponen una tasa de compresión fija en todo el corpus de entrenamiento, llevando a una nueva rigidez. Se propone FLEXITOKENS. FLEXITOKENS abordan la inflexibilidad que se incrementa significativamente durante el período de adaptación. Se verifican en múltiples evaluaciones multilingües y tareas estructuralmente diversas, demostrando que FLEXITOKENS reducen la excesiva fragmentación de tokens en comparación con sub-tokenizadores o otros tokenizadores basados en gradientes, mejorando en un 10% en la eficiencia de los tareas posteriores. Los códigos de los experimentos y los datos están disponibles en https://github.com/owos/flexitokens.",
      "upvotes": 4,
      "discussionId": "68799d0521b37e676c8e4063",
      "ai_summary": "FLEXITOKENS, a byte-level language model with a learnable tokenizer, reduces token over-fragmentation and improves performance across multilingual and morphologically diverse tasks.",
      "ai_keywords": [
        "byte-level LMs",
        "learnable tokenizers",
        "boundary predictor",
        "FLEXITOKENS",
        "token over-fragmentation",
        "subword tokenizers",
        "gradient-based tokenizers"
      ]
    },
    "publishedAt": "2025-07-16T21:55:41.000Z",
    "title": "FLEXITOKENS: Flexible Tokenization for Evolving Language Models",
    "summary": "Language models (LMs) are challenging to adapt to new data distributions by\nsimple finetuning. This is due to the rigidity of their subword tokenizers,\nwhich typically remain unchanged during adaptation. This inflexibility often\nleads to inefficient tokenization, causing overfragmentation of\nout-of-distribution domains, unseen languages, or scripts. In this work, we\ndevelop byte-level LMs with learnable tokenizers to make tokenization adaptive.\nOur models include a submodule that learns to predict boundaries between the\ninput byte sequence, encoding it into variable-length segments. Existing\ntokenizer-free methods train this boundary predictor using an auxiliary loss\nthat enforces a fixed compression rate across the training corpus, introducing\na new kind of rigidity. We propose FLEXITOKENS, a simplified training objective\nthat enables significantly greater flexibility during adaptation. Evaluating\nacross multiple multilingual benchmarks, morphologically diverse tasks, and\ndomains, we demonstrate that FLEXITOKENS consistently reduces token\nover-fragmentation and achieves up to 10\\% improvements on downstream task\nperformance compared to subword and other gradient-based tokenizers. Code and\ndata for our experiments will be released at\nhttps://github.com/owos/flexitokens",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626d1e1e72169e781945bf44",
      "avatarUrl": "/avatars/6bf9f35042b6d939f2ab525816ad0423.svg",
      "fullname": "Abraham  Owodunni",
      "name": "Owos",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04984",
      "authors": [
        {
          "_id": "68783633001546c83aa4f928",
          "user": {
            "_id": "67abb26debe64eaa3a624bd7",
            "avatarUrl": "/avatars/39259308f5aeda7f80a5489335554913.svg",
            "isPro": false,
            "fullname": "Zonglin Lyu",
            "user": "ucfzl",
            "type": "user"
          },
          "name": "Zonglin Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:35.041Z",
          "hidden": false
        },
        {
          "_id": "68783633001546c83aa4f929",
          "name": "Chen Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T13:25:32.000Z",
      "submittedOnDailyAt": "2025-07-18T01:45:16.974Z",
      "title": "TLB-VFI: Reconocimiento de Tiempo Potencial de Brouwer Bridge Difusión en Vídeo Frame Inter Procesing",
      "submittedOnDailyBy": {
        "_id": "67abb26debe64eaa3a624bd7",
        "avatarUrl": "/avatars/39259308f5aeda7f80a5489335554913.svg",
        "isPro": false,
        "fullname": "Zonglin Lyu",
        "user": "ucfzl",
        "type": "user"
      },
      "summary": "La Interpolación de Frámenes de Video (VFI) tiene como objetivo predecir un nuevo frámen intermedio \\( I_n \\) entre dos frámenes consecutivos \\( I_0 \\) y \\( I_1 \\), evitando la carga temporal y signo. Los últimos métodos han aplicado modelos de difusión basados en imágenes y videos para lograr un rendimiento fuerte. Sin embargo, los modelos basados en imágenes no pueden extraer información temporal, lo que los hace más antiguos en comparación con los métodos no difusivos. Por otro lado, los modelos basados en video pueden extraer información temporal, pero esto afecta negativamente a la escala de entrenamiento, el tamaño del modelo y el tiempo de inferencia. Para mitigar estos problemas, proponemos el Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI), que utiliza un gating 3D-wavelet y un autoencoder que conoce la información temporal. Este método extrae una gran cantidad de información temporal de los videos y mejora en 20% el rendimiento en términos de FID en comparación con los modelos de difusión basados en imágenes recientes. Además, con la información temporal rica, este método puede lograr un rendimiento fuerte con un número de parámetros 3 veces menor. Esta reducción de parámetros puede llevar a una mejora en la velocidad de 2.3 veces. Usando una línea de gradiente de colores abiertos, este método requiere aproximadamente 9.000 veces menos datos de entrenamiento y alcanza un número de parámetros 20 veces menor en comparación con los modelos de difusión basados en video. Los códigos y resultados se pueden encontrar en la página del proyecto: https://zonglinl.github.io/tlbvfi_page.",
      "upvotes": 4,
      "discussionId": "68783634001546c83aa4f92a",
      "projectPage": "https://zonglinl.github.io/tlbvfi_page/",
      "githubRepo": "https://github.com/ZonglinL/TLBVFI",
      "ai_summary": "Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) improves video frame interpolation by efficiently extracting temporal information, reducing parameters, and requiring less training data compared to existing methods.",
      "ai_keywords": [
        "diffusion models",
        "video frame interpolation",
        "temporal information",
        "3D-wavelet gating",
        "temporal-aware autoencoder",
        "FID",
        "optical flow guidance"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-07-07T09:25:32.000Z",
    "title": "TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame\n  Interpolation",
    "summary": "Video Frame Interpolation (VFI) aims to predict the intermediate frame I_n\n(we use n to denote time in videos to avoid notation overload with the timestep\nt in diffusion models) based on two consecutive neighboring frames I_0 and\nI_1. Recent approaches apply diffusion models (both image-based and\nvideo-based) in this task and achieve strong performance. However, image-based\ndiffusion models are unable to extract temporal information and are relatively\ninefficient compared to non-diffusion methods. Video-based diffusion models can\nextract temporal information, but they are too large in terms of training\nscale, model size, and inference time. To mitigate the above issues, we propose\nTemporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation\n(TLB-VFI), an efficient video-based diffusion model. By extracting rich\ntemporal information from video inputs through our proposed 3D-wavelet gating\nand temporal-aware autoencoder, our method achieves 20% improvement in FID on\nthe most challenging datasets over recent SOTA of image-based diffusion models.\nMeanwhile, due to the existence of rich temporal information, our method\nachieves strong performance while having 3times fewer parameters. Such a\nparameter reduction results in 2.3x speed up. By incorporating optical flow\nguidance, our method requires 9000x less training data and achieves over 20x\nfewer parameters than video-based diffusion models. Codes and results are\navailable at our project page: https://zonglinl.github.io/tlbvfi_page.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67abb26debe64eaa3a624bd7",
      "avatarUrl": "/avatars/39259308f5aeda7f80a5489335554913.svg",
      "fullname": "Zonglin Lyu",
      "name": "ucfzl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.11589",
      "authors": [
        {
          "_id": "687902bccc15e42a72b01ad6",
          "name": "Sandeep Suresh Cranganore",
          "hidden": false
        },
        {
          "_id": "687902bccc15e42a72b01ad7",
          "user": {
            "_id": "66bdb0025bdd611f9a008bec",
            "avatarUrl": "/avatars/468ec3aebcf798bff601583adbc3bf88.svg",
            "isPro": false,
            "fullname": "Bodnar",
            "user": "AndreiB137",
            "type": "user"
          },
          "name": "Andrei Bodnar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T14:59:15.181Z",
          "hidden": false
        },
        {
          "_id": "687902bccc15e42a72b01ad8",
          "name": "Arturs Berzins",
          "hidden": false
        },
        {
          "_id": "687902bccc15e42a72b01ad9",
          "name": "Johannes Brandstetter",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-15T14:55:39.000Z",
      "submittedOnDailyAt": "2025-07-18T08:27:07.754Z",
      "title": "El campo de Einšhtain: Neural Portfolio de la Relatividad Computacional Universal",
      "submittedOnDailyBy": {
        "_id": "66bdb0025bdd611f9a008bec",
        "avatarUrl": "/avatars/468ec3aebcf798bff601583adbc3bf88.svg",
        "isPro": false,
        "fullname": "Bodnar",
        "user": "AndreiB137",
        "type": "user"
      },
      "summary": "EinFields introduce neural representations for compressing organized hidden neural network weights necessary for efficient 4D numerical relativity simulations. By modeling the core tensor field of general relativity, the physical quantities can be automatically differentiated. However, unlike general neural fields (e.g., signed distance, occupancy, radiance fields), EinFields naturally emerge as neural tensor fields that produce dynamics when transforming the spacetime geometry of general relativity into a neural field representation. EinFields showcase remarkable potential in 4D continuous modeling of spacetime, message independence, storage efficiency, differentiation accuracy, and ease of use. They confront these challenges from the standard test bed of general relativity, releasing an open-source library based on JAX to explore a more scalable and expressive approach to numerical relativity. The code is available at https://github.com/AndreiB137/EinFields.",
      "upvotes": 0,
      "discussionId": "687902bdcc15e42a72b01ada",
      "githubRepo": "https://github.com/AndreiB137/EinFields",
      "ai_summary": "Einstein Fields, a neural tensor field representation, compresses four-dimensional numerical relativity simulations into neural network weights, enabling automatic differentiation and natural emergence of dynamics.",
      "ai_keywords": [
        "Einstein Fields",
        "neural representation",
        "implicit neural network",
        "metric",
        "general relativity",
        "neural tensor fields",
        "spacetime geometry",
        "automatic differentiation",
        "numerical relativity",
        "JAX-based library"
      ],
      "githubStars": 23
    },
    "publishedAt": "2025-07-15T10:55:39.000Z",
    "title": "Einstein Fields: A Neural Perspective To Computational General\n  Relativity",
    "summary": "We introduce Einstein Fields, a neural representation that is designed to\ncompress computationally intensive four-dimensional numerical relativity\nsimulations into compact implicit neural network weights. By modeling the\nmetric, which is the core tensor field of general relativity, Einstein\nFields enable the derivation of physical quantities via automatic\ndifferentiation. However, unlike conventional neural fields (e.g., signed\ndistance, occupancy, or radiance fields), Einstein Fields are Neural\nTensor Fields with the key difference that when encoding the spacetime\ngeometry of general relativity into neural field representations, dynamics\nemerge naturally as a byproduct. Einstein Fields show remarkable potential,\nincluding continuum modeling of 4D spacetime, mesh-agnosticity, storage\nefficiency, derivative accuracy, and ease of use. We address these challenges\nacross several canonical test beds of general relativity and release an open\nsource JAX-based library, paving the way for more scalable and expressive\napproaches to numerical relativity. Code is made available at\nhttps://github.com/AndreiB137/EinFields",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66bdb0025bdd611f9a008bec",
      "avatarUrl": "/avatars/468ec3aebcf798bff601583adbc3bf88.svg",
      "fullname": "Bodnar",
      "name": "AndreiB137",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]