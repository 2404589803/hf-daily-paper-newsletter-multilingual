[
  {
    "paper": {
      "id": "2505.02550",
      "authors": [
        {
          "_id": "6819ef0b2ff435c58da4d860",
          "user": {
            "_id": "63ecbccac8827dd0f0f59579",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ecbccac8827dd0f0f59579/kz-2F9Z0QKllifgZmr8tH.jpeg",
            "isPro": false,
            "fullname": "Chris Ociepa",
            "user": "chrisociepa",
            "type": "user"
          },
          "name": "Krzysztof Ociepa",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:56.213Z",
          "hidden": false
        },
        {
          "_id": "6819ef0b2ff435c58da4d861",
          "name": "Łukasz Flis",
          "hidden": false
        },
        {
          "_id": "6819ef0b2ff435c58da4d862",
          "user": {
            "_id": "61786d0b038518aa2827c6b7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61786d0b038518aa2827c6b7/d1UnfivoVreYebS5JM3P9.jpeg",
            "isPro": false,
            "fullname": "Remek Kinas",
            "user": "Remek",
            "type": "user"
          },
          "name": "Remigiusz Kinas",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T06:51:15.217Z",
          "hidden": false
        },
        {
          "_id": "6819ef0b2ff435c58da4d863",
          "user": {
            "_id": "5e47d3eb178ca95365287400",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
            "isPro": true,
            "fullname": "Krzysztof Wróbel",
            "user": "djstrong",
            "type": "user"
          },
          "name": "Krzysztof Wróbel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:54.135Z",
          "hidden": false
        },
        {
          "_id": "6819ef0b2ff435c58da4d864",
          "name": "Adrian Gwoździej",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T10:39:51.000Z",
      "submittedOnDailyAt": "2025-05-12T07:26:20.895Z",
      "title": "Bielik v3 Small: Informe Técnico",
      "submittedOnDailyBy": {
        "_id": "5e47d3eb178ca95365287400",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
        "isPro": true,
        "fullname": "Krzysztof Wróbel",
        "user": "djstrong",
        "type": "user"
      },
      "summary": "Introducing BiELIC v3. This model is part of the parameter-efficient generative text model series (150M and 450M) and is optimized for Polish language processing. These models show the same performance as models that require large computer systems, even on small fields. Our approach includes several key innovations: the Polish User-Custom Tokenizer (APT4) significantly improves token efficiency, weight instance cross-entropy loss for balanced training, and adaptive learning rates that dynamically adjust during training. Based on a meticulously curated corpus of 29.2 billion tokens, the model, consisting of 30.3 million documents, excels in various benchmarks, including the Open PL LLM leaderboard, complex Polish text understanding benchmarks, Polish EQ-Bench, and Polish medical benchmarks. The 450M parameter model achieves competitive results compared to models twice or three times its size, while the 150M parameter model delivers strong performance even under very tight profiles. This development sets a new benchmark for parameter-efficient language modeling and makes it easier to utilize AI for Polish in resource-limited applications.",
      "upvotes": 16,
      "discussionId": "6819ef0c2ff435c58da4d892",
      "projectPage": "https://bielik.ai/",
      "githubRepo": "https://github.com/speakleash",
      "ai_keywords": [
        "parameter-efficient",
        "generative text models",
        "token efficiency",
        "custom Polish tokenizer",
        "Weighted Instruction Cross-Entropy Loss",
        "Adaptive Learning Rate"
      ]
    },
    "publishedAt": "2025-05-05T06:39:51.000Z",
    "title": "Bielik v3 Small: Technical Report",
    "summary": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02550.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e47d3eb178ca95365287400",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
      "fullname": "Krzysztof Wróbel",
      "name": "djstrong",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02410",
      "authors": [
        {
          "_id": "6819f19e5c7ea9f74284d3a3",
          "user": {
            "_id": "63ecbccac8827dd0f0f59579",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ecbccac8827dd0f0f59579/kz-2F9Z0QKllifgZmr8tH.jpeg",
            "isPro": false,
            "fullname": "Chris Ociepa",
            "user": "chrisociepa",
            "type": "user"
          },
          "name": "Krzysztof Ociepa",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:52.265Z",
          "hidden": false
        },
        {
          "_id": "6819f19e5c7ea9f74284d3a4",
          "name": "Łukasz Flis",
          "hidden": false
        },
        {
          "_id": "6819f19e5c7ea9f74284d3a5",
          "user": {
            "_id": "5e47d3eb178ca95365287400",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
            "isPro": true,
            "fullname": "Krzysztof Wróbel",
            "user": "djstrong",
            "type": "user"
          },
          "name": "Krzysztof Wróbel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:50.340Z",
          "hidden": false
        },
        {
          "_id": "6819f19e5c7ea9f74284d3a6",
          "name": "Adrian Gwoździej",
          "hidden": false
        },
        {
          "_id": "6819f19e5c7ea9f74284d3a7",
          "user": {
            "_id": "61786d0b038518aa2827c6b7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61786d0b038518aa2827c6b7/d1UnfivoVreYebS5JM3P9.jpeg",
            "isPro": false,
            "fullname": "Remek Kinas",
            "user": "Remek",
            "type": "user"
          },
          "name": "Remigiusz Kinas",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T06:51:13.426Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T07:03:41.000Z",
      "submittedOnDailyAt": "2025-05-12T07:25:02.402Z",
      "title": "InternLM (书生·浦语) 翻译如下：\n\nInforme Técnico de Tecnología Biólica 11B v2",
      "submittedOnDailyBy": {
        "_id": "5e47d3eb178ca95365287400",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
        "isPro": true,
        "fullname": "Krzysztof Wróbel",
        "user": "djstrong",
        "type": "user"
      },
      "summary": "BiElRick 11B v2, uno de los modelos de lenguaje más avanzados, presenta un modelo optimizado para el procesamiento del portugués. Basado en la arquitectura Mistral 7B v0.2, se ha ampliado el número de parámetros a 11B a través de escalado de profundidad, demostrando excelentes resultados en las marcas de prueba de portugués y una fortaleza en la capacidad de cross-lingüismo. Se presentan dos innovaciones tecnológicas principales: la pérdida de entropía cruzada de instancias de peso e un aprendizaje adaptativo de tasa. La pérdida de entropía cruzada de instancias de peso asigna pesos basados en la calidad para optimizar el aprendizaje de diferentes tipos de instancias, mientras que el aprendizaje adaptativo de tasa ajusta dinamicamente según la longitud de la oración. Las pruebas en varios marcos de referencia han demostrado que BiElRick 11B v2 supera a modelos con un número de parámetros 2 a 6 veces mayores, siendo el mejor modelo en diversas tareas de comprensión lingüística hasta inferencias complejas dentro de los modelos de portugués. La eficiencia de parámetros y la funcionalidad de diversificación del modelo permiten su implementación en diferentes configuraciones de hardware, estableciendo nuevos estándares de rendimiento para la mejora de la funcionalidad del AI del portugués y el modelado de lenguaje de poca representación con una mayor eficiencia de recursos.",
      "upvotes": 16,
      "discussionId": "6819f19e5c7ea9f74284d3cc",
      "projectPage": "https://bielik.ai/",
      "githubRepo": "https://github.com/speakleash",
      "ai_keywords": [
        "Weighted Instruction Cross-Entropy Loss",
        "Adaptive Learning Rate",
        "depth up-scaling",
        "parameter efficiency",
        "quantization"
      ]
    },
    "publishedAt": "2025-05-05T03:03:41.000Z",
    "title": "Bielik 11B v2 Technical Report",
    "summary": "We present Bielik 11B v2, a state-of-the-art language model optimized for\nPolish text processing. Built on the Mistral 7B v0.2 architecture and scaled to\n11B parameters using depth up-scaling, this model demonstrates exceptional\nperformance across Polish language benchmarks while maintaining strong\ncross-lingual capabilities. We introduce two key technical innovations:\nWeighted Instruction Cross-Entropy Loss, which optimizes learning across\ndiverse instruction types by assigning quality-based weights to training\nexamples, and Adaptive Learning Rate, which dynamically adjusts based on\ncontext length. Comprehensive evaluation across multiple benchmarks\ndemonstrates that Bielik 11B v2 outperforms many larger models, including those\nwith 2-6 times more parameters, and significantly surpasses other specialized\nPolish language models on tasks ranging from linguistic understanding to\ncomplex reasoning. The model's parameter efficiency and extensive quantization\noptions enable deployment across various hardware configurations, advancing\nPolish language AI capabilities and establishing new benchmarks for\nresource-efficient language modeling in less-represented languages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e47d3eb178ca95365287400",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
      "fullname": "Krzysztof Wróbel",
      "name": "djstrong",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.06046",
      "authors": [
        {
          "_id": "6821af48696b63e207ae8474",
          "user": {
            "_id": "64cb98c6f103036e23c69b1d",
            "avatarUrl": "/avatars/7ee33880ad39f5335b618dc53554124a.svg",
            "isPro": false,
            "fullname": "Harris",
            "user": "Joshua-Harris",
            "type": "user"
          },
          "name": "Joshua Harris",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T09:03:48.631Z",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8475",
          "name": "Fan Grayson",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8476",
          "name": "Felix Feldman",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8477",
          "name": "Timothy Laurence",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8478",
          "name": "Toby Nonnenmacher",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae8479",
          "name": "Oliver Higgins",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847a",
          "name": "Leo Loman",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847b",
          "name": "Selina Patel",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847c",
          "name": "Thomas Finnie",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847d",
          "name": "Samuel Collins",
          "hidden": false
        },
        {
          "_id": "6821af48696b63e207ae847e",
          "name": "Michael Borowitz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-09T13:42:59.000Z",
      "submittedOnDailyAt": "2025-05-12T07:35:55.202Z",
      "title": "Salud de los LLMs? Marco de referencia de conocimiento de LLMs sobre la información de salud pública de la Gobierno de Reino Unido",
      "submittedOnDailyBy": {
        "_id": "64cb98c6f103036e23c69b1d",
        "avatarUrl": "/avatars/7ee33880ad39f5335b618dc53554124a.svg",
        "isPro": false,
        "fullname": "Harris",
        "user": "Joshua-Harris",
        "type": "user"
      },
      "summary": "Cuando LLMs se vuelven accesibles en una amplia escala, una comprensión detallada de conocimientos en ciertos campos es necesaria para que estos modelos sean exitosamente aplicados en el mundo real. En particular, en la área de salud pública, la falta de acceso a información relevante, precisa y actual puede tener un impacto significativo sobre los residentes de Reino Unido, lo que hace especialmente importante este aspecto. Sin embargo, actualmente, se sabe que los LLMs no tienen un conocimiento completo sobre las informaciones sanitarias de los gobiernos de Reino Unido. Para abordar este problema, este artículo presenta un nuevo benchmark llamado PubHealthBench, que evalúa la respuesta a preguntas de múltiples respuestas posibles (MCQA) y respuestas de forma libre de los LLMs en salud pública. Este benchmark incluye más de 8,000 preguntas generadas automáticamente. Además, se lanza un nuevo conjunto de datos de documentos gubernamentales de salud pública de Reino Unido utilizados como texto de referencia en PubHealthBench. Los resultados de evaluar 24 LLMs en PubHealthBench muestran que los más recientes, especialmente los no públicos (GPT-4.5, GPT-4.1, o1), tienen un alto nivel de conocimiento, alcanzando más del 90% en MCQA y demostrando un desempeño superior a los motores de búsqueda. Sin embargo, ningún modelo superó el 75% en respuestas de forma libre. Por lo tanto, aunque los modelos más recientes (SOTA) pueden mostrar señales precisas de información sanitaria, pueden necesitar medidas adicionales de seguridad y herramientas para proporcionar respuestas de forma libre.",
      "upvotes": 6,
      "discussionId": "6821af49696b63e207ae84c6",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Multiple Choice Question Answering (MCQA)",
        "PubHealthBench",
        "UK Government public health information",
        "automated pipeline",
        "extracted UK Government public health guidance documents",
        "SOTA (state of the art) LLMs",
        "GPT-4.5",
        "GPT-4.1",
        "o1"
      ]
    },
    "publishedAt": "2025-05-09T09:42:59.000Z",
    "title": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information",
    "summary": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries, created via an automated pipeline. We also\nrelease a new dataset of the extracted UK Government public health guidance\ndocuments used as source text for PubHealthBench. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% in the MCQA setup, and outperform\nhumans with cursory search engine use. However, in the free form setup we see\nlower performance with no model scoring >75%. Therefore, whilst there are\npromising signs that state of the art (SOTA) LLMs are an increasingly accurate\nsource of public health information, additional safeguards or tools may still\nbe needed when providing free form responses on public health topics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06046.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64cb98c6f103036e23c69b1d",
      "avatarUrl": "/avatars/7ee33880ad39f5335b618dc53554124a.svg",
      "fullname": "Harris",
      "name": "Joshua-Harris",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.06111",
      "authors": [
        {
          "_id": "68218b847202d193249511b6",
          "name": "Qingwen Bu",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511b7",
          "name": "Yanting Yang",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511b8",
          "name": "Jisong Cai",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511b9",
          "name": "Shenyuan Gao",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511ba",
          "user": {
            "_id": "646ec9b135f55eb49e405faa",
            "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
            "isPro": false,
            "fullname": "Guanghui Ren",
            "user": "sundrops",
            "type": "user"
          },
          "name": "Guanghui Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T06:50:15.305Z",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511bb",
          "name": "Maoqing Yao",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511bc",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "68218b847202d193249511bd",
          "name": "Hongyang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-09T15:11:13.000Z",
      "submittedOnDailyAt": "2025-05-12T04:30:20.087Z",
      "title": "UniVLA: Centra la tarea de aprendizaje de acciones potenciales para realizar el aprendizaje de acciones en cualquier lugar.",
      "submittedOnDailyBy": {
        "_id": "64ac1f169dcc5787461468a4",
        "avatarUrl": "/avatars/c031a75989147009b7850df4eddfcb27.svg",
        "isPro": false,
        "fullname": "Qingwen Bu",
        "user": "qwbu",
        "type": "user"
      },
      "summary": "General robots need to operate effectively in various environments. However, many current approaches prioritize scaling the data that represents the robot's actions, enhancing their capabilities by using more data. This results in robots with a single physical characteristic and difficulty in acquiring learnable knowledge from different bodies and environments. To address these limitations, we propose a new framework called \"UniVLA.\" This framework aims to learn visual, language, and action (VLA) policies across different bodies. Our main innovation is the use of potential action models to obtain task-centric action representations from videos, enabling the use of a wide range of bodies and perspectives for test data. To mitigate the influence of task-irrelevant actions, we employ language instructions and build potential action models within the DINO feature space. General policies trained on internet-scale videos can efficiently interpret potential actions and function across various robots. State-of-the-art results have been achieved in diverse action and manipulation benchmarks, and real robot deployment has also been realized. UniVLA achieves higher performance than OpenVLA, with 1/20 of the computational resources and 1/10 of the deployment data. Continuous performance improvements can be observed by incorporating additional data, especially human videos, into the testing process. These results demonstrate that UniVLA promotes scalable and efficient robot policy learning.",
      "upvotes": 5,
      "discussionId": "68218b857202d19324951214",
      "githubRepo": "https://github.com/OpenDriveLab/UniVLA",
      "ai_keywords": [
        "UniVLA",
        "vision-language-action (VLA) policies",
        "latent action model",
        "DINO feature space",
        "latent action decoding",
        "manipulation benchmarks",
        "navigation benchmarks",
        "real-robot deployments",
        "OpenVLA"
      ]
    },
    "publishedAt": "2025-05-09T11:11:13.000Z",
    "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
    "summary": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06111.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ac1f169dcc5787461468a4",
      "avatarUrl": "/avatars/c031a75989147009b7850df4eddfcb27.svg",
      "fullname": "Qingwen Bu",
      "name": "qwbu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.05026",
      "authors": [
        {
          "_id": "6821771ddf190eabf5f666d8",
          "user": {
            "_id": "655c44752205aab35222aca3",
            "avatarUrl": "/avatars/57900539952382de0ce6892faf50b401.svg",
            "isPro": false,
            "fullname": "Jaehyun Jeon",
            "user": "jeochris",
            "type": "user"
          },
          "name": "Jaehyun Jeon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-12T06:50:17.832Z",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666d9",
          "name": "Jang Han Yoon",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666da",
          "name": "Min Soo Kim",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666db",
          "name": "Sumin Shim",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666dc",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666dd",
          "name": "Hanbin Kim",
          "hidden": false
        },
        {
          "_id": "6821771ddf190eabf5f666de",
          "name": "Youngjae Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T08:00:32.000Z",
      "submittedOnDailyAt": "2025-05-12T05:33:20.932Z",
      "title": "G-FOCUS: Dirección para un potente métodología para evaluar la persuasividad en el diseño de interfaces de usuario\n\n(注意：虽然您要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，我在翻译中保持了原文的格式和结构。如果您需要进一步的解释或调整，请告知。)",
      "submittedOnDailyBy": {
        "_id": "655c44752205aab35222aca3",
        "avatarUrl": "/avatars/57900539952382de0ce6892faf50b401.svg",
        "isPro": false,
        "fullname": "Jaehyun Jeon",
        "user": "jeochris",
        "type": "user"
      },
      "summary": "La evaluación efectiva del diseño de interfaz (UI) se centra en la influencia que tiene sobre las acciones del usuario, en lugar de su artesanía. Esto es un principio fundamental de la persuasividad del diseño (Design Persuasiveness). Las pruebas A/B son uno de los principales métodos para seleccionar una UI que aumente la participación del usuario. Sin embargo, este método requiere costos y tiempo. Los modelos de visión y lenguaje (Vision-Language Models, VLMs) recientes pueden procesar la análisis automático de la UI. Sin embargo, el enfoque actual se centra en atributos de diseño independientes y, de manera relativa, en un enfoque centrado en la persuasividad. Para resolver esto, se presenta WiserUI-Bench, un marco de referencia diseñado para la evaluación de la persuasividad de diseño de interfaz en pares. Se han etiquetado con resultados de pruebas A/B y razones de expertos 300 pares de imágenes de UI de la realidad. Además, se propone G-FOCUS, una estrategia de razonamiento que fortalece la evaluación de persuasividad basada en VLMs, reduce la sesgo de ubicación y mejora la precisión de la evaluación. Los resultados de los experimentos muestran que G-FOCUS explica la coincidencia y precisión de la evaluación de pares de UI que superan las estrategias de inferencia actuales. La promoción de la evaluación de persuasividad de UI mediante VLMs, nuestro estudio proporciona una aproximación que complementa las pruebas A/B, fomenta el modelado de preferencias de UI intercambiables y el desarrollo de la optimización de diseño. Los códigos y datos se han publicado.",
      "upvotes": 5,
      "discussionId": "68217722df190eabf5f66814",
      "ai_keywords": [
        "Vision-Language Models",
        "WiserUI-Bench",
        "Pairwise UI Design Persuasiveness Assessment",
        "G-FOCUS",
        "inference-time reasoning strategy",
        "position bias",
        "VLM-driven evaluation"
      ]
    },
    "publishedAt": "2025-05-08T04:00:32.000Z",
    "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness",
    "summary": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05026.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655c44752205aab35222aca3",
      "avatarUrl": "/avatars/57900539952382de0ce6892faf50b401.svg",
      "fullname": "Jaehyun Jeon",
      "name": "jeochris",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02686",
      "authors": [
        {
          "_id": "6821acfb2808328b91c0e365",
          "name": "Xiaobao Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T14:33:49.000Z",
      "submittedOnDailyAt": "2025-05-12T06:41:36.276Z",
      "title": "El AI que se embarca en las estrellas: investigación sobre la compensación de aprendizaje en la escalación de entrenamiento y prueba de modelos de lenguaje grandes",
      "submittedOnDailyBy": {
        "_id": "64cb02869e30a46f7b80b355",
        "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
        "isPro": false,
        "fullname": "Xiaobao Wu",
        "user": "bobxwu",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los modelos de lenguaje grande (LLMs) ha transitado desde la escalación de entrenamiento previa a la escalación de entrenamiento y prueba posterior. En este proceso, se ha identificado un paradigma central y unificador: que el aprendizaje se guie de la recompensa, lo que hace que la estrella de la recompensa guíe las acciones del LLM. Este paradigma se basa en diversas tecnologías, como el aprendizaje por refuerzo con supervisión de humanos (RLHF), decodificación basada en recompensa, y corrección posterior. Un punto clave es que este paradigma permite la transición desde un aprendizaje pasivo a partir de datos estáticos a un aprendizaje activo a partir de retroalimentación dinámica. De esta manera, los LLMs adquieren una coherencia de orientación y funciones lógicas profundas. En esta investigación, se proporciona una resumen detallado del paradigma de aprendizaje de recompensa. Se clasifican y analizan las estrategias de aprendizaje, inferencia y etapas posteriores a la inferencia en este paradigma. Además, se discuten los marcos de referencia de modelos de recompensa y las principales aplicaciones. Finalmente, se destacan los problemas y las direcciones futuras. Se tiene disponible una colección de artículos: https://github.com/bobxwu/learning-from-rewards-llm-papers.",
      "upvotes": 3,
      "discussionId": "6821acfd2808328b91c0e3e3",
      "githubRepo": "https://github.com/bobxwu/learning-from-rewards-llm-papers",
      "ai_keywords": [
        "reinforcement learning",
        "RLHF",
        "DPO",
        "GRPO",
        "reward-guided decoding",
        "post-hoc correction",
        "active learning",
        "reward models"
      ]
    },
    "publishedAt": "2025-05-05T10:33:49.000Z",
    "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in\n  Post-Training and Test-Time Scaling of Large Language Models",
    "summary": "Recent developments in Large Language Models (LLMs) have shifted from\npre-training scaling to post-training and test-time scaling. Across these\ndevelopments, a key unified paradigm has arisen: Learning from Rewards, where\nreward signals act as the guiding stars to steer LLM behavior. It has\nunderpinned a wide range of prevalent techniques, such as reinforcement\nlearning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc\ncorrection. Crucially, this paradigm enables the transition from passive\nlearning from static data to active learning from dynamic feedback. This endows\nLLMs with aligned preferences and deep reasoning capabilities. In this survey,\nwe present a comprehensive overview of the paradigm of learning from rewards.\nWe categorize and analyze the strategies under this paradigm across training,\ninference, and post-inference stages. We further discuss the benchmarks for\nreward models and the primary applications. Finally we highlight the challenges\nand future directions. We maintain a paper collection at\nhttps://github.com/bobxwu/learning-from-rewards-llm-papers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02686.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb02869e30a46f7b80b355",
      "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
      "fullname": "Xiaobao Wu",
      "name": "bobxwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]