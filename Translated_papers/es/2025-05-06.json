[
  {
    "paper": {
      "id": "2505.02707",
      "authors": [
        {
          "_id": "6819982f17007d963b9d4166",
          "name": "Yemin Shi",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4167",
          "name": "Yu Shu",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4168",
          "name": "Siwei Dong",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4169",
          "user": {
            "_id": "6108ae87823007eaf0c7bd1e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6108ae87823007eaf0c7bd1e/dKjdx9I5waJs6oUQ0_mmT.png",
            "isPro": false,
            "fullname": "Guangyi Liu",
            "user": "guangyil",
            "type": "user"
          },
          "name": "Guangyi Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:46:52.667Z",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416a",
          "user": {
            "_id": "6438a9027de34e8ea7e4b257",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438a9027de34e8ea7e4b257/vib8QSd1AWMr_bR9ig_xJ.jpeg",
            "isPro": false,
            "fullname": "Jaward Sesay",
            "user": "Jaward",
            "type": "user"
          },
          "name": "Jaward Sesay",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:48.746Z",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416b",
          "name": "Jingwen Li",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416c",
          "user": {
            "_id": "665bfa1b0d71762b8613282d",
            "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
            "isPro": false,
            "fullname": "Zhiting Hu",
            "user": "zhitinghu",
            "type": "user"
          },
          "name": "Zhiting Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:46:15.191Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/665bfa1b0d71762b8613282d/zbWarqt8nFt0AwhF0gElE.mp4"
      ],
      "publishedAt": "2025-05-05T15:05:01.000Z",
      "submittedOnDailyAt": "2025-05-06T03:36:16.945Z",
      "title": "Voila: Interacción de conducción autónoma en unidades de tiempo mediante modelos de lenguaje de voz y juego de rol profesional de lenguaje de voz",
      "submittedOnDailyBy": {
        "_id": "665bfa1b0d71762b8613282d",
        "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
        "isPro": false,
        "fullname": "Zhiting Hu",
        "user": "zhitinghu",
        "type": "user"
      },
      "summary": "Voila es un asistente de inteligencia artificial de voz que integra y transforma la vida diaria y la vida de seminarios, interactuando en tiempo real con una expresión emocional y humana. Excede las simples respuestas a comandos, continua escuchando, analizando razones y reaccionando de manera proactiva, promoviendo conversaciones naturales y dinámicas que resonen emocionalmente. Voila ha avanzado hacia esta visión. Voila presenta un modelo de lenguaje de voz de gran escala para familias. Ha introducido una arquitectura que supera los sistemas de pipeline existentes, permitiendo diálogos ricos en tono, ritmo y emoción desde el punto de entrada hasta el punto de salida, manteniendo la armonía del sonido. Alcanza una respuesta de respuesta de 195 milisegundos, respondiendo más rápido que la media de respuesta humana. Utiliza un Transformer multiescala para integrar la capacidad de razonamiento de modelos de lenguaje grandes (LLMs) y un potente modelado de sonido, facilitando la generación de sonidos naturales y profesionales. Los usuarios pueden definir el reconocimiento de voz, tono, y otras características con simples instrucciones de texto. Además, Voila recomienda adaptar nuevos sonidos apropiadamente a partir de muestras de sonido de aproximadamente 10 segundos, y soporta un diccionario pre-construido de más de un millón de sonidos. Además de la conversación, maneja la reconocimiento automático del sonido (ASR), la conversión de texto a voz (TTS), y adapta mínimamente para traducciones de gramática en múltiples idiomas. Voila es completamente abierto-source y apoya la investigación pública, con el objetivo de fomentar el desarrollo de la conversación humano-máquina de las próximas generaciones.",
      "upvotes": 49,
      "discussionId": "6819983117007d963b9d4247",
      "projectPage": "https://voila.maitrix.org",
      "githubRepo": "https://github.com/maitrix-org/Voila",
      "ai_keywords": [
        "full-duplex",
        "low-latency conversations",
        "hierarchical multi-scale Transformer",
        "reasoning capabilities",
        "large language models (LLMs)",
        "acoustic modeling",
        "persona-aware voice generation",
        "automatic speech recognition (ASR)",
        "Text-to-Speech (TTS)",
        "multilingual speech translation",
        "pre-built voices",
        "efficient customization"
      ]
    },
    "publishedAt": "2025-05-05T11:05:01.000Z",
    "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play",
    "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/665bfa1b0d71762b8613282d/zbWarqt8nFt0AwhF0gElE.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02707.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "665bfa1b0d71762b8613282d",
      "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
      "fullname": "Zhiting Hu",
      "name": "zhitinghu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02387",
      "authors": [
        {
          "_id": "681988d6d6a5fee26b52ac28",
          "user": {
            "_id": "6270ff726417aed8a7340c8b",
            "avatarUrl": "/avatars/3f14913c55cc4fc78678ac43fb603e80.svg",
            "isPro": false,
            "fullname": "Xiusi Chen",
            "user": "XtremSup",
            "type": "user"
          },
          "name": "Xiusi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:47:11.654Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac29",
          "user": {
            "_id": "654d784d71a30c4bca09a319",
            "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
            "isPro": false,
            "fullname": "Gaotang Li",
            "user": "gaotang",
            "type": "user"
          },
          "name": "Gaotang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:13.258Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2a",
          "name": "Ziqi Wang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2b",
          "name": "Bowen Jin",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2c",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2d",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2e",
          "user": {
            "_id": "65f906e5c3dbdcae83ff7aac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f906e5c3dbdcae83ff7aac/mdjiVkLDJgJcGLwv0rMe4.jpeg",
            "isPro": false,
            "fullname": "Hongru Wang",
            "user": "Merlin-Hongru",
            "type": "user"
          },
          "name": "Hongru Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:11.136Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2f",
          "name": "Yu Zhang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac30",
          "user": {
            "_id": "66285acb73af5913c6bbf1ec",
            "avatarUrl": "/avatars/8969e3a6ae2dcc0b1c49768fd044b9e0.svg",
            "isPro": false,
            "fullname": "Denghui Zhang",
            "user": "zhangdenghui123",
            "type": "user"
          },
          "name": "Denghui Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:48:00.793Z",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac31",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac32",
          "name": "Hanghang Tong",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac33",
          "name": "Heng Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:11:12.000Z",
      "submittedOnDailyAt": "2025-05-06T02:32:05.558Z",
      "title": "RM-R1: Modelar la compensación desde la lógica de la teoría",
      "submittedOnDailyBy": {
        "_id": "654d784d71a30c4bca09a319",
        "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
        "isPro": false,
        "fullname": "Gaotang Li",
        "user": "gaotang",
        "type": "user"
      },
      "summary": "El modelado de recompensas es crucial para alinear grandes modelos de lenguaje (LLMs) con las preferencias humanas, especialmente a través del aprendizaje por refuerzo con fines de humanos (RLHF). Para proporcionar una señal de recompensa precisa, el modelo de recompensa (RM) debe desencadenar una profunda reflexión y presentar razones interpretables. Sin embargo, los RM actuales generan escalares oscuros o predecir respuestas buenas directamente, lo que dificulta la integración de evaluaciones en naturaleza de lenguaje y reduce su interpretabilidad.\n\nCon el desarrollo reciente de la inferencia continua larga (CoT), hemos asumido y demostrado que podemos mejorar significativamente la interpretabilidad y el rendimiento del RM al incorporar habilidades de razonamiento. En este artículo, presentamos una nueva clase de modelos de recompensa, los modelos de recompensa racionales (ReasRMs), y proponemos una estrategia para configurar el modelado de recompensa como una tarea de razonamiento. Proponemos un proceso de entrenamiento para razones y entrenamos un RM-R1, un tipo de ReasRM. El entrenamiento se divide en dos etapas principales: (1) la refinamiento continuo de razones de alta calidad, y (2) el aprendizaje por refuerzo con recompensas verificables. El RM-R1 genera automáticamente trazas de razones o guías de revisión únicas de diálogos, compara respuestas candidatas y mejora el rendimiento de la red de la LLM. Experimentalmente, nuestro modelo alcanza los mejores o los mejores cercanos en varios benchmarks detallados de modelos de recompensa generativos, y supera modelos grandes abiertos de peso (como Llama3.1-405B) o modelos propietarios (como GPT-4o) en un margen de al menos 13.8%. Además de superar el rendimiento final, realizamos un análisis experimental detallado para comprender los componentes clave del entrenamiento exitoso de un ReasRM. Para futuras investigaciones, publicamos seis modelos de ReasRM, código y datos (https://github.com/RM-R1-UIUC/RM-R1).",
      "upvotes": 28,
      "discussionId": "681988d7d6a5fee26b52ac7e",
      "githubRepo": "https://github.com/RM-R1-UIUC/RM-R1",
      "ai_keywords": [
        "reward modeling",
        "reinforcement learning from human feedback (RLHF)",
        "reward model (RM)",
        "scalar scores",
        "preferred answer",
        "natural language critiques",
        "long chain-of-thought (CoT)",
        "reasoning capabilities",
        "Reasoning Reward Models (ReasRMs)",
        "reasoning-oriented training pipeline",
        "distillation",
        "high-quality reasoning chains",
        "reinforcement learning",
        "verifiable rewards",
        "LLM rollouts",
        "self-generating reasoning traces",
        "chat-specific rubrics",
        "candidate responses",
        "generative reward models",
        "state-of-the-art",
        "near state-of-the-art",
        "reward model benchmarks",
        "open-weight models",
        "proprietary models",
        "empirical analysis",
        "ReasRM models"
      ]
    },
    "publishedAt": "2025-05-05T02:11:12.000Z",
    "title": "RM-R1: Reward Modeling as Reasoning",
    "summary": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654d784d71a30c4bca09a319",
      "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
      "fullname": "Gaotang Li",
      "name": "gaotang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20752",
      "authors": [
        {
          "_id": "6818c145daa8955b2085667d",
          "name": "Roman Abramov",
          "hidden": false
        },
        {
          "_id": "6818c145daa8955b2085667e",
          "user": {
            "_id": "6679882913c63ebaa8ff62fe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
            "isPro": false,
            "fullname": "Felix Steinbauer",
            "user": "fsteinbauer",
            "type": "user"
          },
          "name": "Felix Steinbauer",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-05T13:46:46.742Z",
          "hidden": false
        },
        {
          "_id": "6818c145daa8955b2085667f",
          "name": "Gjergji Kasneci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T13:33:29.000Z",
      "submittedOnDailyAt": "2025-05-06T03:38:21.809Z",
      "title": "El Campo de la Juguetería en el Campo de Saltos: Extensión de Datos para Transformers que Ejecutan Inferencia Multinivel en el Mundo Real",
      "submittedOnDailyBy": {
        "_id": "6679882913c63ebaa8ff62fe",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
        "isPro": false,
        "fullname": "Felix Steinbauer",
        "user": "fsteinbauer",
        "type": "user"
      },
      "summary": "Transformers han sido exitosas en múltiples tareas de NLP, pero presentan claras limitaciones en múltiples etapas de inferencia real. En particular, estas limitaciones se manifiestan claramente cuando la conocida real es insuficiente. Recientemente, los avances en grokking han demostrado que redes neuronales pueden detectar patrones lógicos potenciales y generalizarlos completamente de la memoria. Sin embargo, estas investigaciones han sido principalmente basadas en pequeñas tareas sintéticas. En este artículo, primero, extendemos el grokking a datos de hechos reales, y resolvemos la raridad de los datos mediante la adición cuidadosamente diseñada de datos sintéticos a un grafo de conocimiento existente, logrando que el porcentaje de hechos inferidos, phi_r, supere un valor crítico con respecto a los hechos reales. Sorprendentemente, encontramos que datos sintéticos factualmente incorrectos fortalecen el ciclo de inferencia y obligan a creer en estructuras relacionales más que en estructuras basadas en la memoria. En evaluaciones en marcos de referencia de múltiples etapas de inferencia, nuestro enfoque logró alcanzar una precisión del 95-100% en 2WikiMultiHopQA, mejorando significativamente un fuerte baseline y superando los resultados actuales. Además, analizamos en detalle la formación del ciclo de generalización interno de los Transformers con respecto al aumento de phi_r. Nuestros hallazgos indican que la augmentación de datos basada en grokking libera capacidades de inferencia multi-etapa ocultas y permite a grandes modelos de lenguaje realizar inferencias factuales más robustas y interpretables.",
      "upvotes": 19,
      "discussionId": "6818c146daa8955b208566f1",
      "ai_keywords": [
        "Transformers",
        "multi-step factual reasoning",
        "grokking",
        "neural networks",
        "perfect generalization",
        "logical patterns",
        "real-world factual data",
        "dataset sparsity",
        "knowledge graphs",
        "synthetic data",
        "inferred facts",
        "atomic facts",
        "factually incorrect synthetic data",
        "relational structure",
        "memorization",
        "multi-hop reasoning",
        "benchmarks",
        "2WikiMultiHopQA",
        "baselines",
        "state-of-the-art results",
        "generalizing circuits",
        "grokking-based data augmentation",
        "implicit multi-hop reasoning capabilities",
        "robust",
        "interpretable factual reasoning"
      ]
    },
    "publishedAt": "2025-04-29T09:33:29.000Z",
    "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers",
    "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio phi_r of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing phi_r drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20752.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6679882913c63ebaa8ff62fe",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
      "fullname": "Felix Steinbauer",
      "name": "fsteinbauer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02819",
      "authors": [
        {
          "_id": "6819b5da3d9c61444380f4c5",
          "user": {
            "_id": "66465dfa508db0bde50d95f2",
            "avatarUrl": "/avatars/8b4a583dc0f3cab0f1cd9a1be3daa01b.svg",
            "isPro": false,
            "fullname": "Dmitry Shophoev",
            "user": "dimitriish",
            "type": "user"
          },
          "name": "Dmitriy Shopkhoev",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-06T07:10:19.519Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c6",
          "user": {
            "_id": "6166db59f78a267701a78c2a",
            "avatarUrl": "/avatars/8784efc36f67719e9455b1f081340ed9.svg",
            "isPro": false,
            "fullname": "Ammar Ali",
            "user": "ammarali32",
            "type": "user"
          },
          "name": "Ammar Ali",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:17.870Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c7",
          "name": "Magauiya Zhussip",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c8",
          "user": {
            "_id": "66b1ce4ca14db5aac3e5e755",
            "avatarUrl": "/avatars/ab55ef112fba091813e1cc1f43857cf9.svg",
            "isPro": false,
            "fullname": "Valentin Malykh",
            "user": "madrugado",
            "type": "user"
          },
          "name": "Valentin Malykh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:42.358Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4c9",
          "user": {
            "_id": "6683cc62b466c0d8e60e1bbc",
            "avatarUrl": "/avatars/d781cfb113263f88eaa3250bef521c53.svg",
            "isPro": false,
            "fullname": "Stamatis Lefkimmiatis",
            "user": "stamatisl",
            "type": "user"
          },
          "name": "Stamatios Lefkimmiatis",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:13.923Z",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4ca",
          "name": "Nikos Komodakis",
          "hidden": false
        },
        {
          "_id": "6819b5da3d9c61444380f4cb",
          "user": {
            "_id": "667e7f968c6d7aede7ecb94b",
            "avatarUrl": "/avatars/d6dabd9b909b1f20f661dc4bc07af23f.svg",
            "isPro": false,
            "fullname": "Sergey Zagoruyko",
            "user": "szagoruyko121",
            "type": "user"
          },
          "name": "Sergey Zagoruyko",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:52.244Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T17:47:42.000Z",
      "submittedOnDailyAt": "2025-05-06T07:03:26.032Z",
      "title": "Reducción de la red mediante reemplazo de capas y transformaciones lineales",
      "submittedOnDailyBy": {
        "_id": "610e8c12119bebecb4d807b6",
        "avatarUrl": "/avatars/7230b1584ec45585c12eb5703fd80ff3.svg",
        "isPro": false,
        "fullname": "Ivan Sedykh",
        "user": "idsedykh",
        "type": "user"
      },
      "summary": "ReplaceMe es un método generalizado de reducción de profundidad sin límites de entrenamiento. Este método permite mantener altos rendimientos a bajos ratios de compresión, reemplazando eficazmente los bloques transformer por operaciones lineales. A diferencia de otros métodos de reducción, no requiere adicionales entrenamientos o ajustes. Nuestro método solo necesita un pequeño conjunto de datos de corrección para estimar transformaciones lineales que aproximen los bloques reducidos. Estas transformaciones lineales pueden integrarse sin restricciones con los demás bloques transformer, sin necesidad de agregar parámetros adicionales. Según los experimentos, ReplaceMe muestra un rendimiento comparable a otros métodos sin límites de entrenamiento y compite con los mejores métodos de reducción en la arquitectura más compleja, incluyendo retrenamiento, ajuste final y cambios en la arquitectura. ReplaceMe puede mantener aproximadamente el 90% del rendimiento del modelo original en benchmarks abiertos, reduciendo el modelo en un 25%, con un mínimo de sobrecarga computacional (referencia a Fig.1). Ofrecemos la implementación de ReplaceMe junto con una biblioteca abierta que incluye algunos de los mejores métodos de reducción de profundidad.",
      "upvotes": 16,
      "discussionId": "6819b5db3d9c61444380f518",
      "githubRepo": "https://github.com/mts-ai/ReplaceMe",
      "ai_keywords": [
        "training-free depth pruning",
        "transformer blocks",
        "linear operation",
        "calibration dataset",
        "linear transformation",
        "computational overhead",
        "large language models (LLMs)",
        "open benchmarks",
        "open-source library"
      ]
    },
    "publishedAt": "2025-05-05T13:47:42.000Z",
    "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations",
    "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02819.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "610e8c12119bebecb4d807b6",
      "avatarUrl": "/avatars/7230b1584ec45585c12eb5703fd80ff3.svg",
      "fullname": "Ivan Sedykh",
      "name": "idsedykh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02735",
      "authors": [
        {
          "_id": "6819742e0d1c56fe9124fe3a",
          "user": {
            "_id": "62a80fe3ac97233f1625235a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
            "isPro": false,
            "fullname": "Zhouliang Yu",
            "user": "zhouliang",
            "type": "user"
          },
          "name": "Zhouliang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:34:10.190Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3b",
          "user": {
            "_id": "662f2c8435ab6df959b005de",
            "avatarUrl": "/avatars/3e30053ecbe9cc14b5e1eb2b014755de.svg",
            "isPro": false,
            "fullname": "ruotian peng",
            "user": "prt66",
            "type": "user"
          },
          "name": "Ruotian Peng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:48:20.491Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3c",
          "name": "Keyi Ding",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3d",
          "name": "Yizhe Li",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3e",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3f",
          "user": {
            "_id": "6417d9ea8f689506e7148417",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
            "isPro": false,
            "fullname": "minghao",
            "user": "Liam-Liu",
            "type": "user"
          },
          "name": "Minghao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:31.975Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe40",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yifan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:14.785Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe41",
          "user": {
            "_id": "649da6b4599302cdb9bc232b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DxQT6LCDTZvyGUUe2t19c.jpeg",
            "isPro": false,
            "fullname": "Zheng Yuan",
            "user": "ZhengYuan",
            "type": "user"
          },
          "name": "Zheng Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:20.735Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe42",
          "user": {
            "_id": "6532a060a78e70d19c669103",
            "avatarUrl": "/avatars/3cc9309b0e31da0fb83f1c3ef87dbe9f.svg",
            "isPro": false,
            "fullname": "HuajianXin",
            "user": "HuajianXin",
            "type": "user"
          },
          "name": "Huajian Xin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:28.104Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe43",
          "user": {
            "_id": "641e5bf65f274a0a92c2f6a2",
            "avatarUrl": "/avatars/c15a54c51998c0e6367685e8e1737ec9.svg",
            "isPro": false,
            "fullname": "Wenhao Huang",
            "user": "EZ-hwh",
            "type": "user"
          },
          "name": "Wenhao Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:44.482Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe44",
          "user": {
            "_id": "643c21735fcffe09fb68a46f",
            "avatarUrl": "/avatars/76aabacd318aa954d4c53094ad456056.svg",
            "isPro": false,
            "fullname": "Yandong Wen",
            "user": "ydwen",
            "type": "user"
          },
          "name": "Yandong Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:51.642Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe45",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:49:59.764Z",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe46",
          "user": {
            "_id": "648905d1a15c43c791d4381f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
            "isPro": false,
            "fullname": "Weiyang Liu",
            "user": "wy1iu",
            "type": "user"
          },
          "name": "Weiyang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:50:07.063Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T15:37:00.000Z",
      "submittedOnDailyAt": "2025-05-06T01:00:48.636Z",
      "title": "Prueba de criterios matemáticos de inferencia formal para modelos de lenguaje de gran escala",
      "submittedOnDailyBy": {
        "_id": "62a80fe3ac97233f1625235a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
        "isPro": false,
        "fullname": "Zhouliang Yu",
        "user": "zhouliang",
        "type": "user"
      },
      "summary": "La formación de matemáticas es un asunto importante en la inteligencia artificial, y actualmente está limitado por el rango y escala de los marcos de referencia. En respuesta a esto, se presenta FormalMATH. FormalMATH es un marco de referencia de gran escala en Lean4 que incluye 5,560 problemas verificados formalmente. Estos problemas abarcan desde desafíos de olimpiadas de secundaria hasta teoremas de nivel universitario, y se extienden a diversas áreas como álgebra, matemáticas aplicadas, cálculo diferencial, teoría de números, matemáticas discretas, etc. Para reducir la inadecuación de la validación manual, hemos combinado tres elementos para introducir una nueva línea de profilado automático de lógica: 1) un modelo de lenguaje grande (LLMs) especializado en la automatización de lógica, 2) la verificación lingüística de modelos de dos extremos, y 3) una estrategia de filtración de demostraciones basada en la prueba de líneas de prueba de LLMs. Este enfoque deja solo 72.09% de las secuencias lógicas para la validación manual, asegurando la fidelidad a los problemas naturales. Según la evaluación de los mejores modelos basados en LLMs, se han identificado limitaciones claras. Incluso los modelos más fuertes debajo del límite práctico alcanzaron un 16.46% de éxito, observándose también un sesgo por área (por ejemplo, excelencia en álgebra mientras falla en cálculo diferencial) y una dependencia excesiva de la automatización simplificada. Específicamente, se ha descubierto una relación inversa entre el guión de soluciones en lenguaje natural y el éxito de la demostración, demostrando que las lógicas no formales escritas por humanos pueden actuar como ruido en la configuración de lógicas formales, lo que no es facil de entender. Creemos que FormalMATH puede ser un fuerte marco de referencia para la evaluación de lógicas matemáticas formales.",
      "upvotes": 16,
      "discussionId": "6819742f0d1c56fe9124fe8a",
      "projectPage": "https://spherelab.ai/FormalMATH/",
      "githubRepo": "https://github.com/Sphere-AI-Lab/FormalMATH-Bench"
    },
    "publishedAt": "2025-05-05T11:37:00.000Z",
    "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models",
    "summary": "Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a80fe3ac97233f1625235a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
      "fullname": "Zhouliang Yu",
      "name": "zhouliang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02835",
      "authors": [
        {
          "_id": "6819762e64ae18f1b6fde347",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yi-Fan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:15.220Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde348",
          "user": {
            "_id": "664ba004bfd9b93ba4bfb353",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/UHaEcXmMSKvvFDsY3hCnb.jpeg",
            "isPro": false,
            "fullname": "LuXingyu",
            "user": "XingyuLu",
            "type": "user"
          },
          "name": "Xingyu Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:24.963Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde349",
          "name": "Xiao Hu",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34a",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34b",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34c",
          "name": "Tianke Zhang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34d",
          "user": {
            "_id": "673421bf18caf8e877861cc6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8UfIZTUTFaCnWmJ_Bztr.png",
            "isPro": false,
            "fullname": "Changyi Liu",
            "user": "bhsc24",
            "type": "user"
          },
          "name": "Changyi Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:58:28.151Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34e",
          "user": {
            "_id": "63774c47455f6ad89ac41be1",
            "avatarUrl": "/avatars/e7d6048155cdf4497d58aa18523e745e.svg",
            "isPro": false,
            "fullname": "Kaiyu Jiang",
            "user": "KaiyuValley",
            "type": "user"
          },
          "name": "Kaiyu Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:58:21.743Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34f",
          "name": "Kaibing Chen",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde350",
          "user": {
            "_id": "66c605e808fee728d0dd94f5",
            "avatarUrl": "/avatars/d2ff37fedc5ac1b5b817543b80bf5256.svg",
            "isPro": false,
            "fullname": "Kaiyu Tang",
            "user": "KevinTowne",
            "type": "user"
          },
          "name": "Kaiyu Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:58:06.587Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde351",
          "user": {
            "_id": "6610f64ee94d9046b71e19c8",
            "avatarUrl": "/avatars/11cc11199669129a740956d12c7214e8.svg",
            "isPro": false,
            "fullname": "Haojie Ding",
            "user": "haojieding",
            "type": "user"
          },
          "name": "Haojie Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:59.222Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde352",
          "user": {
            "_id": "6433abff546e16f17a0f1cd8",
            "avatarUrl": "/avatars/7c9bbcba69b823834eb0232da12cc7a9.svg",
            "isPro": false,
            "fullname": "chen",
            "user": "jiankang",
            "type": "user"
          },
          "name": "Jiankang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:51.408Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde353",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde354",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde355",
          "user": {
            "_id": "656453832bdaccfcd5379431",
            "avatarUrl": "/avatars/a0d764ce6b3fd05532c7a9cb2f263e33.svg",
            "isPro": false,
            "fullname": "Gao Ting",
            "user": "TingTingGao",
            "type": "user"
          },
          "name": "Tingting Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:57:35.059Z",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde356",
          "name": "Liang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T17:59:50.000Z",
      "submittedOnDailyAt": "2025-05-06T01:09:45.446Z",
      "title": "R1-Reward: Entrenamiento de un modelo de recompensa de Damo por aprendizaje por refuerzo estable",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "Los modelos de recompensa del modelo de Damo (MRMs) desempeñan un papel crucial en la mejora del rendimiento de los modelos de Damo de lenguaje (MLLMs). El desarrollo reciente ha centrado principalmente en la mejora de la estructura del modelo y los datos de entrenamiento, pero la investigación sobre la eficiencia a largo plazo y los métodos para activar estas capacidades a través de los MRMs ha sido limitada. En este artículo, se revisa la utilización de modelado de recompensa con aprendizaje por refuerzo (RL). En particular, se reescribe el problema de modelado de recompensa como una tarea de RL basada en reglas. Sin embargo, aplicar directamente algoritmos de RL actuales (por ejemplo, Reinforce++) a este problema puede llevar a instabilidades o colapsos en el entrenamiento debido a sus limitaciones intrínsecas. Para resolver estos problemas, se propone el algoritmo StableReinforce. Este algoritmo mejora la pérdida de entrenamiento, la estrategia de estimación de ventaja y la diseño de recompensas en los métodos de RL actuales, lo que permite una mejora en la estabilidad del entrenamiento y un rendimiento alto. Para apoyar el entrenamiento de los MRMs, se recopilan 200K datos de preferencia en diferentes conjuntos de datos. Usando este conjunto de datos con el algoritmo StableReinforce, se entrena un modelo de recompensa, R1-Reward, que puede obtener significativas mejoras en el benchmark de MRMs de Damo. Comparado con los modelos de estado de la arte anteriores, R1-Reward puede obtener un aumento del 8.4% en el benchmark VL Reward-Bench y un aumento del 14.3% en el Multimodal Reward Bench. Además, al establecer un mayor número de cálculos de inferencia, el rendimiento de R1-Reward puede mejorar y se puede descubrir la posibilidad de que el algoritmo de RL optimice los MRMs.",
      "upvotes": 15,
      "discussionId": "6819762f64ae18f1b6fde387",
      "projectPage": "https://github.com/yfzhang114/r1_reward",
      "githubRepo": "https://github.com/yfzhang114/r1_reward",
      "ai_keywords": [
        "Multimodal Reward Models (MRMs)",
        "Multimodal Large Language Models (MLLMs)",
        "Reinforcement Learning (RL)",
        "rule-based RL task",
        "Reinforce++",
        "StableReinforce",
        "training loss",
        "advantage estimation strategy",
        "reward design",
        "preference data",
        "VL Reward-Bench",
        "Multimodal Reward Bench"
      ]
    },
    "publishedAt": "2025-05-05T13:59:50.000Z",
    "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning",
    "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3%\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02835.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02391",
      "authors": [
        {
          "_id": "6819a63c64ae18f1b60a5c43",
          "user": {
            "_id": "66f8689725464a7989b75845",
            "avatarUrl": "/avatars/43a61a528c5779103eaf5687ba44ee14.svg",
            "isPro": false,
            "fullname": "Jiarui Yao",
            "user": "FlippyDora",
            "type": "user"
          },
          "name": "Jiarui Yao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:24.344Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c44",
          "name": "Yifan Hao",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c45",
          "user": {
            "_id": "6470e0f1cfd57849519033a5",
            "avatarUrl": "/avatars/7ffefee3e36a4e37b9f4510bc6b689d1.svg",
            "isPro": false,
            "fullname": "Hanning Zhang",
            "user": "HanningZhang",
            "type": "user"
          },
          "name": "Hanning Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:20.459Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c46",
          "user": {
            "_id": "63a3ff69f91ad3ea5703841d",
            "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
            "isPro": false,
            "fullname": "Hanze Dong",
            "user": "hendrydong",
            "type": "user"
          },
          "name": "Hanze Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:27.200Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c47",
          "user": {
            "_id": "6319b29809baf858241f05de",
            "avatarUrl": "/avatars/29eef2c52814abea82e2aa9bf37a7f9c.svg",
            "isPro": false,
            "fullname": "Xiong",
            "user": "WeiXiong",
            "type": "user"
          },
          "name": "Wei Xiong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:34.381Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c48",
          "user": {
            "_id": "64b8922ca1827cc8d04ae919",
            "avatarUrl": "/avatars/0aaa83e3d09a82434e1d6af724aaa485.svg",
            "isPro": false,
            "fullname": "Nan Jiang",
            "user": "nanjiang",
            "type": "user"
          },
          "name": "Nan Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T08:59:47.400Z",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c49",
          "name": "Tong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:26:00.000Z",
      "submittedOnDailyAt": "2025-05-06T04:34:14.120Z",
      "title": "Optimización de la Estrategia de Inferencia de Cadena de Descartes para Minimizar la Variación de la Gradiente en Rejection Sampling y RL",
      "submittedOnDailyBy": {
        "_id": "64d45451c34a346181b130dd",
        "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
        "isPro": false,
        "fullname": "Rui Yang",
        "user": "Ray2333",
        "type": "user"
      },
      "summary": "La inferencia de cadena de pensamiento (CoT) puede ser formalizada como un problema de variables latentes en modelos de lenguaje grandes (LLMs), y el modelo necesita generar pasos intermedios de razonamiento. En contraste con los métodos anteriores, que dependían de ajustes micro de evaluación de recompensas iterativas (RAFT) y otros similares, este artículo identifica una limitación principal de la entrenamiento de CoT: la eficiencia de la estimación de gradientes aleatorios mediante estrategias de muestreo estático. Se propone el GVM-RAFT, que diseña una estrategia de muestreo dinámico que se adapta a los Prompts y minimiza la varianza estándar bajo restricciones de vectores de cálculo. Mediante la medida de la tasa de aceptación de Prompts y la norma de las descensos aleatorios, se distribuye eficientemente los recursos de cálculo, minimizando así la varianza estándar de los descensos. Un análisis teórico muestra que la estrategia dinámica de muestreo proporciona una garantía de convergencia acelerada bajo ciertas condiciones. En experimentos matemáticos, el GVM-RAFT demostró un aumento de velocidad de 2-4 veces y una mejora en precisión similar a la RAFT original. La estrategia dinámica de muestreo es general y puede ser integrada en otros algoritmos de aprendizaje por refuerzo, como GRPO, obteniendo así la misma mejora en convergencia y precisión de prueba. El código está disponible en https://github.com/RLHFlow/GVM.",
      "upvotes": 15,
      "discussionId": "6819a63d64ae18f1b60a5c75",
      "ai_keywords": [
        "Chain-of-thought (CoT)",
        "latent variable problem",
        "iterative reward-ranked fine-tuning (RAFT)",
        "inference budget",
        "static sampling strategies",
        "GVM-RAFT",
        "Dynamic Sample Allocation Strategy",
        "prompt-specific",
        "computational budget constraint",
        "prompt acceptance rates",
        "stochastic gradient norms",
        "stochastic gradient variance",
        "accelerated convergence guarantees",
        "GRPO",
        "convergence",
        "test accuracy"
      ]
    },
    "publishedAt": "2025-05-05T02:26:00.000Z",
    "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL",
    "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d45451c34a346181b130dd",
      "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
      "fullname": "Rui Yang",
      "name": "Ray2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02222",
      "authors": [
        {
          "_id": "6819780dc3d212ad5b48cc07",
          "name": "Essential AI",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc09",
          "user": {
            "_id": "65ef97d0e5fc4abe66c05ed0",
            "avatarUrl": "/avatars/1d601a22639b3136bfb3519826451ddb.svg",
            "isPro": false,
            "fullname": "Ishaan Shah",
            "user": "ishaan-essential",
            "type": "user"
          },
          "name": "Ishaan Shah",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:24.884Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0a",
          "user": {
            "_id": "6675e3ed66c4fa6d0c10e229",
            "avatarUrl": "/avatars/f73c347d824a56079729c82d60d3edc3.svg",
            "isPro": false,
            "fullname": "Anthony Polloreno",
            "user": "ampolloreno",
            "type": "user"
          },
          "name": "Anthony M. Polloreno",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:47.445Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0b",
          "user": {
            "_id": "64d9ac38badf1110f7fcf030",
            "avatarUrl": "/avatars/c55c61af8dd52e6b4856684638b850a6.svg",
            "isPro": false,
            "fullname": "Karl Stratos",
            "user": "karlstratos",
            "type": "user"
          },
          "name": "Karl Stratos",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:53.826Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0c",
          "user": {
            "_id": "66622dacec18341b268f97a6",
            "avatarUrl": "/avatars/8bc7d6a7c28c83aacdbeeb770716b1c0.svg",
            "isPro": false,
            "fullname": "Philip Monk",
            "user": "monk-essential",
            "type": "user"
          },
          "name": "Philip Monk",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:59.992Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0d",
          "user": {
            "_id": "67bfd6daca6e3c22b6de31ee",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/TTt6_o9tYEgeWK26DgI_7.png",
            "isPro": false,
            "fullname": "Adarsh Chaluvaraju",
            "user": "cadarsh-essential",
            "type": "user"
          },
          "name": "Adarsh Chaluvaraju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:02:16.867Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0e",
          "user": {
            "_id": "6408e4f93461c51cf7345060",
            "avatarUrl": "/avatars/328b508e2de9e50dca2412adeb3542f5.svg",
            "isPro": false,
            "fullname": "Andrew Hojel",
            "user": "andrewhojel",
            "type": "user"
          },
          "name": "Andrew Hojel",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:02:24.655Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc0f",
          "user": {
            "_id": "62e24efc3a616d16e2f426ea",
            "avatarUrl": "/avatars/a2433c971f80e6cf738c03e843666cff.svg",
            "isPro": false,
            "fullname": "Andrew Ma",
            "user": "AndrewMa",
            "type": "user"
          },
          "name": "Andrew Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:02:30.884Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc10",
          "name": "Anil Thomas",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc11",
          "name": "Ashish Tanwer",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc12",
          "name": "Darsh J Shah",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc13",
          "user": {
            "_id": "67ed7aa9290a7f9d33113fb5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jMMTObGZktuXWJ3wVVSxj.png",
            "isPro": false,
            "fullname": "Khoi Nguyen",
            "user": "KTLK",
            "type": "user"
          },
          "name": "Khoi Nguyen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:20.971Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc14",
          "name": "Kurt Smith",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc15",
          "name": "Michael Callahan",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc16",
          "user": {
            "_id": "66cd078ea796074d428fde0f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qThw3H6ukqIAuRy7aJTN1.jpeg",
            "isPro": false,
            "fullname": "Michael Pust",
            "user": "essentialpust",
            "type": "user"
          },
          "name": "Michael Pust",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:14.427Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc17",
          "user": {
            "_id": "674c2737d369a6de1f8f58e1",
            "avatarUrl": "/avatars/40af3aa9b9d574cc63dc328c3a465fff.svg",
            "isPro": false,
            "fullname": "Parmar Mohit",
            "user": "mohitparmar",
            "type": "user"
          },
          "name": "Mohit Parmar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:21.257Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc18",
          "name": "Peter Rushton",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc19",
          "user": {
            "_id": "67f4ced58c4cbc2f5d95cd17",
            "avatarUrl": "/avatars/3f440a59c38f5c0c7a77746ef54ed0a5.svg",
            "isPro": false,
            "fullname": "Platon Mazarakis",
            "user": "Platona",
            "type": "user"
          },
          "name": "Platon Mazarakis",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:32.983Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1a",
          "user": {
            "_id": "654bdcf2e06d25def57cc54b",
            "avatarUrl": "/avatars/2d2612bd7072edd60876b504345fbf25.svg",
            "isPro": false,
            "fullname": "Ritvik Kapila",
            "user": "rkapila",
            "type": "user"
          },
          "name": "Ritvik Kapila",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:03:51.154Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1b",
          "name": "Saurabh Srivastava",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1c",
          "user": {
            "_id": "679bc0b23e12a166672e5275",
            "avatarUrl": "/avatars/fe9d0e79c21c9d3594420e69e3809f0f.svg",
            "isPro": false,
            "fullname": "Somanshu Singla",
            "user": "somanshu-essential",
            "type": "user"
          },
          "name": "Somanshu Singla",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:27.349Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1d",
          "user": {
            "_id": "67101a7165442ddc48cb4b07",
            "avatarUrl": "/avatars/551777fcd1638998ad9fd16804b313ec.svg",
            "isPro": false,
            "fullname": "Tim Romanski",
            "user": "tim-essential",
            "type": "user"
          },
          "name": "Tim Romanski",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:09.387Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1e",
          "user": {
            "_id": "66f5f3f99ef08fe3c1f4c35a",
            "avatarUrl": "/avatars/41b8b6f90eb87e685b74587317296a1b.svg",
            "isPro": false,
            "fullname": "Yash Vanjani",
            "user": "yash-essential",
            "type": "user"
          },
          "name": "Yash Vanjani",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:04:16.859Z",
          "hidden": false
        },
        {
          "_id": "6819780dc3d212ad5b48cc1f",
          "name": "Ashish Vaswani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T19:14:43.000Z",
      "submittedOnDailyAt": "2025-05-06T07:30:25.714Z",
      "title": "El eficiencia práctica del entrenamiento previo de los miones",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "MOON (Muon) es el módulo final más sencillo de 2do nivel y el mejor optimizador de eficiencia. Extiende más claramente la carga de tiempo de cálculo en la frontera de la rueda de la ganancia frente a AdamW, y es más efectivo y eficiente, permitiendo un entrenamiento más eficiente con grandes tamaños de conjunto de datos, manteniendo la eficiencia de datos. De esta manera, MOON permite entrenamientos de recursos eficientes. Además, se investigó la transferencia de hiperparámetros eficiente de la combinación de MOON y la máxima actualización de parámetros (muP), y se propuso un algoritmo eficiente que minimiza el sobre cargo manual de recursos considerando todos los errores de muP. Estos hallazgos se verificaron mediante experimentos con modelos de tamaño de parámetros superiores a 40 mil millones y pruebas de distribución de datos y arquitectura.",
      "upvotes": 15,
      "discussionId": "6819780fc3d212ad5b48cc89",
      "ai_keywords": [
        "second-order optimizer",
        "Pareto frontier",
        "AdamW",
        "data efficiency",
        "critical batch size",
        "computationally efficient",
        "maximal update parameterization",
        "telescoping algorithm",
        "hyperparameter transfer",
        "error sources",
        "model sizes",
        "data distribution",
        "architecture"
      ]
    },
    "publishedAt": "2025-05-04T15:14:43.000Z",
    "title": "Practical Efficiency of Muon for Pretraining",
    "summary": "We demonstrate that Muon, the simplest instantiation of a second-order\noptimizer, explicitly expands the Pareto frontier over AdamW on the\ncompute-time tradeoff. We find that Muon is more effective than AdamW in\nretaining data efficiency at large batch sizes, far beyond the so-called\ncritical batch size, while remaining computationally efficient, thus enabling\nmore economical training. We study the combination of Muon and the maximal\nupdate parameterization (muP) for efficient hyperparameter transfer and present\na simple telescoping algorithm that accounts for all sources of error in muP\nwhile introducing only a modest overhead in resources. We validate our findings\nthrough extensive experiments with model sizes up to four billion parameters\nand ablations on the data distribution and architecture.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02222.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6784
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02094",
      "authors": [
        {
          "_id": "681992911e0fae3880173d43",
          "user": {
            "_id": "66d59dc9b005ad82ca6fc61d",
            "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
            "isPro": false,
            "fullname": "Runyi YU",
            "user": "IngridYU",
            "type": "user"
          },
          "name": "Runyi Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:00:09.333Z",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d44",
          "name": "Yinhuai Wang",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d45",
          "user": {
            "_id": "64341911546e16f17a129733",
            "avatarUrl": "/avatars/ae12aafc8932a7537838e6d3964858cb.svg",
            "isPro": false,
            "fullname": "QiHan Zhao",
            "user": "Crimnos",
            "type": "user"
          },
          "name": "Qihan Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:00:32.182Z",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d46",
          "name": "Hok Wai Tsui",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d47",
          "name": "Jingbo Wang",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d48",
          "name": "Ping Tan",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d49",
          "user": {
            "_id": "6467b121e7a6a374fd19b44b",
            "avatarUrl": "/avatars/3f2874d58986d651aef55e3408b05700.svg",
            "isPro": false,
            "fullname": "Qifeng Chen",
            "user": "cqf",
            "type": "user"
          },
          "name": "Qifeng Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:01:21.751Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T13:00:29.000Z",
      "submittedOnDailyAt": "2025-05-06T03:11:28.738Z",
      "title": "スキルミモク-V2: Aprende habilidades de interacción robustas y generalizables desde demostraciones raras y con mucho ruido.",
      "submittedOnDailyBy": {
        "_id": "66d59dc9b005ad82ca6fc61d",
        "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
        "isPro": false,
        "fullname": "Runyi YU",
        "user": "IngridYU",
        "type": "user"
      },
      "summary": "Nosotros abordamos los desafíos fundamentales en la aprendizaje por refuerzo (RLID) a través de simulaciones de interacción: el ruido de la simulación y los límites de cobertura. Los métodos existentes para la recolección de datos proporcionan simulaciones de interacción útiles, pero generalmente generan rutas raras, desunidas y con mucho ruido, lo que no captura completamente el rango de posibles cambios tecnológicos y transiciones. Nuestra conclusión es central es que, a pesar de la existencia de ruido y raridad en las simulaciones, existen infinitas rutas físicamente posibles que se conectan naturalmente entre las técnicas de simulación y forman una continuidad espacial de posibles cambios y transiciones tecnológicas. Basándonos en esta conclusión, presentamos dos tecnologías de aumento de datos: el Grafo de Trayectorias Estrechadas (STG) encuentra las potenciales transiciones entre técnicas de simulación y el Campo de Transiciones de Estado (STF) establece conexiones únicas para cualquier estado en el entorno de la simulación. Para facilitar un aprendizaje por refuerzo efectivo, desarrollamos una estrategia de muestreo de rutas adaptativo para la generación de currículo dinámico y un mecanismo de codificación histórico para el aprendizaje de técnicas en memoria. Nuestro enfoque permite la obtención de fuertes tecnologías y mejora significativamente la generalización más allá de las simulaciones. A través de una amplia gama de experimentos en tareas de interacción, demostramos significativas mejoras en estabilidad de convergencia, capacidad de generalización y robustez de recuperación, comparados con los métodos más recientes.",
      "upvotes": 12,
      "discussionId": "681992931e0fae3880173dcf",
      "ai_keywords": [
        "Reinforcement Learning from Interaction Demonstration (RLID)",
        "demonstration noise",
        "coverage limitations",
        "interaction demonstrations",
        "sparse trajectories",
        "disconnected trajectories",
        "noise",
        "skill variations",
        "transitions",
        "physically feasible trajectories",
        "Stitched Trajectory Graph (STG)",
        "State Transition Field (STF)",
        "Adaptive Trajectory Sampling (ATS)",
        "dynamic curriculum generation",
        "historical encoding mechanism",
        "skill acquisition",
        "convergence stability",
        "generalization capability",
        "recovery robustness"
      ]
    },
    "publishedAt": "2025-05-04T09:00:29.000Z",
    "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from\n  Sparse and Noisy Demonstrations",
    "summary": "We address a fundamental challenge in Reinforcement Learning from Interaction\nDemonstration (RLID): demonstration noise and coverage limitations. While\nexisting data collection approaches provide valuable interaction\ndemonstrations, they often yield sparse, disconnected, and noisy trajectories\nthat fail to capture the full spectrum of possible skill variations and\ntransitions. Our key insight is that despite noisy and sparse demonstrations,\nthere exist infinite physically feasible trajectories that naturally bridge\nbetween demonstrated skills or emerge from their neighboring states, forming a\ncontinuous space of possible skill variations and transitions. Building upon\nthis insight, we present two data augmentation techniques: a Stitched\nTrajectory Graph (STG) that discovers potential transitions between\ndemonstration skills, and a State Transition Field (STF) that establishes\nunique connections for arbitrary states within the demonstration neighborhood.\nTo enable effective RLID with augmented data, we develop an Adaptive Trajectory\nSampling (ATS) strategy for dynamic curriculum generation and a historical\nencoding mechanism for memory-dependent skill learning. Our approach enables\nrobust skill acquisition that significantly generalizes beyond the reference\ndemonstrations. Extensive experiments across diverse interaction tasks\ndemonstrate substantial improvements over state-of-the-art methods in terms of\nconvergence stability, generalization capability, and recovery robustness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02094.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d59dc9b005ad82ca6fc61d",
      "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
      "fullname": "Runyi YU",
      "name": "IngridYU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02156",
      "authors": [
        {
          "_id": "681975a9fdcf582e6d0effdb",
          "user": {
            "_id": "64bcc373ef8c0e42bf16acc5",
            "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
            "isPro": false,
            "fullname": "mz.w",
            "user": "iiiiwis",
            "type": "user"
          },
          "name": "Minzheng Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:29.678Z",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdc",
          "user": {
            "_id": "66641b2fd8e1e34bc621e688",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66641b2fd8e1e34bc621e688/csPETwnx2zCIHSWi9uAi-.png",
            "isPro": false,
            "fullname": "Yongbin Li",
            "user": "Yongbin-Li",
            "type": "user"
          },
          "name": "Yongbin Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:05:06.051Z",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdd",
          "name": "Haobo Wang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effde",
          "name": "Xinghua Zhang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdf",
          "name": "Nan Xu",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe0",
          "user": {
            "_id": "668bd45044ab5de7e4c5b1e7",
            "avatarUrl": "/avatars/9b087cfcac65a649a12568b601d5ca53.svg",
            "isPro": false,
            "fullname": "bingli wu",
            "user": "bingliwu",
            "type": "user"
          },
          "name": "Bingli Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:05:37.861Z",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe1",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe2",
          "name": "Haiyang Yu",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe3",
          "name": "Wenji Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T15:39:58.000Z",
      "submittedOnDailyAt": "2025-05-06T01:07:27.275Z",
      "title": "Pensar en Tus Pies: Pensamiento Adaptativo por Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Carga de Car",
      "submittedOnDailyBy": {
        "_id": "64bcc373ef8c0e42bf16acc5",
        "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
        "isPro": false,
        "fullname": "mz.w",
        "user": "iiiiwis",
        "type": "user"
      },
      "summary": "En la simulación efectiva de inteligencia social, las entidades de lenguaje necesitan la capacidad de ajustar dinámicamente la profundidad de la razón. En los métodos actuales, esta capacidad de razón es particularmente insuficiente. Los métodos existentes obligan a conectar largos y continuos sentimientos en todo el escenario, lo que lleva a un uso excesivo de tokens y a una simulación social inadecuada. En este artículo, se propone un enfoque basado en el contexto temporal para seleccionar estratégicamente entre cuatro modos de pensamiento (reacción intuitiva → profunda reflexión) mediante el Aprendizaje de Modo Adaptativo (AML). La innovación central de este marco es el algoritmo de Optimización de Políticas de Modo Adaptativo (AMPO), que introduce tres mejoras sobre los métodos existentes: (1) diseño de modos de pensamiento multigranulares, (2) cambio de modo en función del contexto de interacción social, y (3) procesamiento adaptativo de profundidad basado en la eficiencia de tokens. Se realizaron experimentos amplios en tareas de inteligencia social y el AML logró un rendimiento de tarea 15.6% más alto que los métodos más avanzados. En particular, nuestro método supera GRPO en 7.0% y reduce la longitud de la coma de razón en 32.8%. Estos resultados demuestran que la selección de modos de pensamiento contextual implementada por AMPO permite un enfoque de razón adaptativo más humano que el enfoque de profundidad fija de GRPO.",
      "upvotes": 11,
      "discussionId": "681975a9fdcf582e6d0f0014",
      "githubRepo": "https://github.com/MozerWang/AMPO",
      "ai_keywords": [
        "Adaptive Mode Learning (AML)",
        "Adaptive Mode Policy Optimization (AMPO)",
        "multi-granular thinking mode design",
        "context-aware mode switching",
        "token-efficient reasoning",
        "depth-adaptive processing",
        "intuitive reaction",
        "deep contemplation",
        "social interaction",
        "reasoning chains",
        "fixed-depth approach"
      ]
    },
    "publishedAt": "2025-05-04T11:39:58.000Z",
    "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents",
    "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\napproaches. While existing methods either lack this kind of reasoning\ncapability or enforce uniform long chain-of-thought reasoning across all\nscenarios, resulting in excessive token usage and inappropriate social\nsimulation. In this paper, we propose Adaptive Mode\nLearning (AML) that strategically selects from four\nthinking modes (intuitive reaction rightarrow deep contemplation) based on\nreal-time context. Our framework's core innovation, the Adaptive\nMode Policy Optimization (AMPO)\nalgorithm, introduces three key advancements over existing methods: (1)\nMulti-granular thinking mode design, (2) Context-aware mode switching across\nsocial interaction, and (3) Token-efficient reasoning via depth-adaptive\nprocessing. Extensive experiments on social intelligence tasks confirm that AML\nachieves 15.6% higher task performance than state-of-the-art methods. Notably,\nour method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These\nresults demonstrate that context-sensitive thinking mode selection, as\nimplemented in AMPO, enables more human-like adaptive reasoning than GRPO's\nfixed-depth approach",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02156.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bcc373ef8c0e42bf16acc5",
      "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
      "fullname": "mz.w",
      "name": "iiiiwis",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01658",
      "authors": [
        {
          "_id": "6819950bd55db085708dd2e5",
          "user": {
            "_id": "670cb786e73576f33a339144",
            "avatarUrl": "/avatars/c172887c32878aebafd786061680ea1e.svg",
            "isPro": false,
            "fullname": "Sihyeong Park",
            "user": "inputsh",
            "type": "user"
          },
          "name": "Sihyeong Park",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:09:00.902Z",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e6",
          "name": "Sungryeol Jeon",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e7",
          "user": {
            "_id": "64aaa12a04e7b379fed24327",
            "avatarUrl": "/avatars/327482e569c24ee4c97064f07ddd6de7.svg",
            "isPro": false,
            "fullname": "Chaelyn Lee",
            "user": "oos2",
            "type": "user"
          },
          "name": "Chaelyn Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:09:13.239Z",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e8",
          "user": {
            "_id": "6719f17ac5837d514cfff13b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/GnL4RCj7xncVhCIFN5y35.png",
            "isPro": false,
            "fullname": "Seokhun Jeon",
            "user": "Devcow",
            "type": "user"
          },
          "name": "Seokhun Jeon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:09:18.870Z",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e9",
          "name": "Byung-Soo Kim",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2ea",
          "user": {
            "_id": "65b9dee19c4955ae7aee4954",
            "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
            "isPro": false,
            "fullname": "Jemin Lee",
            "user": "leejaymin",
            "type": "user"
          },
          "name": "Jemin Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:32:52.950Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-03T02:47:43.000Z",
      "submittedOnDailyAt": "2025-05-06T03:21:53.083Z",
      "title": "Investigación sobre el motor de inferencia de modelos de lenguaje grandes: desde la perspectiva de optimización y eficiencia",
      "submittedOnDailyBy": {
        "_id": "65b9dee19c4955ae7aee4954",
        "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
        "isPro": false,
        "fullname": "Jemin Lee",
        "user": "leejaymin",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) están aplicados ampliamente en diversas áreas como bots de chat, servicios de generación de código y motores de búsqueda. Tareas complejas, como Chain Short, lógica compleja y servicios de agente, incrementan significativamente los costos de inferencia al requerir la rediseña de los modelos. Se han introducido métodos de optimización como paralelización, compresión y caching, pero la elección de estos métodos puede ser complicada según las necesidades de los servicios. Recientemente, los motores de inferencia especializados han aparecido como componentes cruciales para integrar métodos de optimización específicos a los servicios. Sin embargo, la investigación sistemática sobre estos motores de inferencia es aún insuficiente. En este artículo, se presenta un evaluación detallada de 25 motores de inferencia abierto y comercial. Se investigan los métodos de uso, la distribución, el apoyo a lenguajes comunes, la escalabilidad, la transportabilidad y las características adecuadas para cálculos en el abecedario latino de cada motor de inferencia. Además, se examina la tecnología de optimización que cada motor de inferencia soporta para definir claramente sus objetivos de diseño. También se evalua la madurez de la ecosistema de motores de inferencia abierto y se comparan el rendimiento y las políticas de costo de las soluciones comerciales. Se definen direcciones de investigación futuras y proporcionan guías prácticas para investigadores y desarrolladores que incluyen el apoyo a servicios complejos basados en LLMs, hardware variado y mejoras en seguridad. Además, se ofrecen repositorios públicos para seguir de manera continua el desarrollo de esta área rápidamente evolucionando: https://github.com/sihyeong/Awesome-LLM-Inference-Engine",
      "upvotes": 9,
      "discussionId": "6819950cd55db085708dd32a",
      "ai_keywords": [
        "chain-of-thought",
        "complex reasoning",
        "agent services",
        "inference cost",
        "parallelism",
        "compression",
        "caching",
        "LLM inference engines",
        "ease-of-use",
        "ease-of-deployment",
        "general-purpose support",
        "scalability",
        "throughput-aware computation",
        "latency-aware computation",
        "optimization techniques",
        "ecosystem maturity",
        "performance",
        "cost policy",
        "LLM-based services",
        "enhanced security"
      ]
    },
    "publishedAt": "2025-05-02T22:47:43.000Z",
    "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
    "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01658.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b9dee19c4955ae7aee4954",
      "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
      "fullname": "Jemin Lee",
      "name": "leejaymin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01441",
      "authors": [
        {
          "_id": "68198aea57d4de18fb3e69d6",
          "user": {
            "_id": "61ffaa2943eb0913fa2df74a",
            "avatarUrl": "/avatars/a19971f830abb8a8ae95e5800beb9fcd.svg",
            "isPro": false,
            "fullname": "Singh",
            "user": "joykirat",
            "type": "user"
          },
          "name": "Joykirat Singh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:08.255Z",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d7",
          "user": {
            "_id": "622ca32345261ac5cc0bdade",
            "avatarUrl": "/avatars/7e1d633be69cf86a3affb9168b1cc27b.svg",
            "isPro": false,
            "fullname": "Raghav Magazine",
            "user": "Raghav2002",
            "type": "user"
          },
          "name": "Raghav Magazine",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:06:32.498Z",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d8",
          "user": {
            "_id": "64aba383fddf117e6e5ba818",
            "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
            "isPro": false,
            "fullname": "Akshay  Nambi",
            "user": "akshaynambi",
            "type": "user"
          },
          "name": "Yash Pandya",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-06T04:08:10.843Z",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d9",
          "user": {
            "_id": "64aba383fddf117e6e5ba818",
            "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
            "isPro": false,
            "fullname": "Akshay  Nambi",
            "user": "akshaynambi",
            "type": "user"
          },
          "name": "Akshay Nambi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:06:46.024Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T10:42:49.000Z",
      "submittedOnDailyAt": "2025-05-06T02:43:42.049Z",
      "title": "El método de aplicar la Reasoning Agentic y la Integración de Herramientas en Modelos de Lenguaje de Entrenamiento por Reinforcement Learning (RL) para su realización.",
      "submittedOnDailyBy": {
        "_id": "64aba383fddf117e6e5ba818",
        "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
        "isPro": false,
        "fullname": "Akshay  Nambi",
        "user": "akshaynambi",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) causan asombro en problemas lógicos complejos, pero tienen límites fundamentales debido a que operan basándose en conocimientos internos fijos y contextos. La resolución de problemas mundiales reales exige la capacidad de tomar decisiones adaptativas, interactuar con herramientas y entornos externos, y aplicar lógica multinivel. En este artículo, se presenta un marco integrado llamado Agentic Reasoning and Tool Integration in Self-improving Transformers (ARTIST). Este marco estrictamente combina la lógica de agentes, el aprendizaje por refuerzo y la integración de herramientas en los modelos de LLMs. ARTIST puede decidir automáticamente en qué momento y cómo aplicar una herramienta en una cadena de razonamiento multinivel, y aprende estrategias potentes de interacción con las herramientas y el entorno basándose en aprendizaje por refuerzo. Los experimentos en lógica matemática y en marcos de prueba de llamadas de funciones multinivel muestran que ARTIST coincide con los estándares más avanzados y mejora significativamente (hasta un 22% absoluto) sobre modelos básicos. Los análisis detallados y métricas demuestran que el aprendizaje por refuerzo basado en agentes permite a los modelos aprender lógicas profundas, utilizar herramientas de manera más efectiva y generar soluciones de alta calidad. Nuestros resultados establecen una nueva dirección hacia la resolución de problemas sólidas, interpretables y generalizables en los modelos de LLMs, y han establecido una nueva perspectiva con la combinación de la integración de herramientas y el aprendizaje por refuerzo.",
      "upvotes": 9,
      "discussionId": "68198aec57d4de18fb3e6a30",
      "projectPage": "https://www.microsoft.com/en-us/research/people/akshayn/unlocking-agentic-reasoning-in-llms/",
      "ai_keywords": [
        "agentic reasoning",
        "reinforcement learning",
        "tool integration",
        "ARTIST",
        "multi-turn reasoning chains",
        "outcome-based RL",
        "mathematical reasoning",
        "function calling",
        "agentic RL",
        "tool use",
        "environment interaction"
      ]
    },
    "publishedAt": "2025-04-28T06:42:49.000Z",
    "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement\n  Learning",
    "summary": "Large language models (LLMs) have achieved remarkable progress in complex\nreasoning tasks, yet they remain fundamentally limited by their reliance on\nstatic internal knowledge and text-only reasoning. Real-world problem solving\noften demands dynamic, multi-step reasoning, adaptive decision making, and the\nability to interact with external tools and environments. In this work, we\nintroduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving\nTransformers), a unified framework that tightly couples agentic reasoning,\nreinforcement learning, and tool integration for LLMs. ARTIST enables models to\nautonomously decide when, how, and which tools to invoke within multi-turn\nreasoning chains, leveraging outcome-based RL to learn robust strategies for\ntool use and environment interaction without requiring step-level supervision.\nExtensive experiments on mathematical reasoning and multi-turn function calling\nbenchmarks show that ARTIST consistently outperforms state-of-the-art\nbaselines, with up to 22% absolute improvement over base models and strong\ngains on the most challenging tasks. Detailed studies and metric analyses\nreveal that agentic RL training leads to deeper reasoning, more effective tool\nuse, and higher-quality solutions. Our results establish agentic RL with tool\nintegration as a powerful new frontier for robust, interpretable, and\ngeneralizable problem-solving in LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01441.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64aba383fddf117e6e5ba818",
      "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
      "fullname": "Akshay  Nambi",
      "name": "akshaynambi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02370",
      "authors": [
        {
          "_id": "68197c200e4203d6bc84cdfb",
          "user": {
            "_id": "637f0eb22438d7485b8ef5d7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
            "isPro": false,
            "fullname": "Ming Li",
            "user": "limingcv",
            "type": "user"
          },
          "name": "Ming Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:18.243Z",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfc",
          "name": "Xin Gu",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfd",
          "name": "Fan Chen",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfe",
          "user": {
            "_id": "64ca92f738837b12d5f63729",
            "avatarUrl": "/avatars/a361be3a5ccf9368717980d1faf69df0.svg",
            "isPro": false,
            "fullname": "Xiaoying Xing",
            "user": "xiaoying0505",
            "type": "user"
          },
          "name": "Xiaoying Xing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:06:58.770Z",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdff",
          "user": {
            "_id": "644df7eacfb40c94eae71186",
            "avatarUrl": "/avatars/1daa4967efd34d54c59aa95970093dbd.svg",
            "isPro": false,
            "fullname": "Longyin Wen",
            "user": "lionwen",
            "type": "user"
          },
          "name": "Longyin Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:07:05.196Z",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84ce00",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84ce01",
          "user": {
            "_id": "65cbdea6d6c974694f09249a",
            "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
            "isPro": false,
            "fullname": "Sijie Zhu",
            "user": "Zilence006",
            "type": "user"
          },
          "name": "Sijie Zhu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-06T03:04:04.536Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T05:19:40.000Z",
      "submittedOnDailyAt": "2025-05-06T01:34:41.608Z",
      "title": "Super Edit: Ajuste y optimización de la regulación de edición de imágenes basada en instrucciones",
      "submittedOnDailyBy": {
        "_id": "637f0eb22438d7485b8ef5d7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
        "isPro": false,
        "fullname": "Ming Li",
        "user": "limingcv",
        "type": "user"
      },
      "summary": "Durante el proceso de construcción del conjunto de datos, la recolección directa de datos de edición precisa es difícil, por lo que actualmente el conjunto de datos se construye generalmente utilizando diversos métodos de automatización. Esto conduce a la aparición de señales de sub-sección impurezas debido a ruidos causados por la correspondencia entre instrucciones de edición, la imagen original y la imagen editada. Recientemente, se han intentado mejorar modelos de edición generando imágenes de edición de alta calidad, realizando aprendizajes previos para tareas de reconocimiento visual y introduciendo modelos de lenguaje visual y de visión (VLMs), aunque no se han logrado resolver los problemas fundamentales. En este artículo, se propone una nueva solución para construir instrucciones de edición más efectivas en par de imágenes dadas. Esta solución incluye la precisión de las instrucciones de edición, asegurar una mejor correspondencia entre la imagen original y la imagen editada, y utilizar instrucciones de edición relativas para mejorar su eficacia. Específicamente, el modelo de edición no depende del contexto, muestra propiedades generativas específicas en cada etapa de inferencia. Basándose en estas propiedades previas, se define un guía unificado para refinar las instrucciones de edición para VLMs. Sin embargo, algunos escenarios de edición difíciles no pueden ser resueltos solo con estas instrucciones precisadas. Por lo tanto, se utilizan instrucciones positivas y negativas para construir señales de sub-sección relativas y se insertan en el entrenamiento de un modelo de redes neuronales utilizando una pérdida de tupla para mejorar la eficacia de la sub-sección. Nuestro método no requiere necesariamente módulos de VLM o tareas de aprendizaje previo utilizadas en investigaciones anteriores. Es más directo y eficiente, proporciona una mejor señal de sub-sección y ofrece una nueva solución sencilla y efectiva para el edición de imágenes basada en comandos. Los resultados en múltiples benchmarks demuestran que nuestro método supera significativamente la aproximación actual. Comparado con el SOTA SmartEdit, nuestro método logró un aumento del 9.19% en el benchmark Real-Edit, utilizando solo 30% de los datos de entrenamiento y un tamaño de modelo 13 veces más pequeño.",
      "upvotes": 8,
      "discussionId": "68197c240e4203d6bc84cee9",
      "projectPage": "https://liming-ai.github.io/SuperEdit/",
      "githubRepo": "https://github.com/bytedance/SuperEdit",
      "ai_keywords": [
        "contrastive editing instructions",
        "triplet loss",
        "instruction-based image editing",
        "contrastive supervision signals",
        "generation attributes",
        "unified guide",
        "vision-language models (VLMs)",
        "real-edit benchmark",
        "smartedit"
      ]
    },
    "publishedAt": "2025-05-05T01:19:40.000Z",
    "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing",
    "summary": "Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f0eb22438d7485b8ef5d7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
      "fullname": "Ming Li",
      "name": "limingcv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01043",
      "authors": [
        {
          "_id": "68196e23d9cad0bb5c90dd9b",
          "user": {
            "_id": "64a62e3302e46deb19a7937e",
            "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
            "isPro": false,
            "fullname": "Zhiwei Hao",
            "user": "Zhiwei840",
            "type": "user"
          },
          "name": "Zhiwei Hao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:34:14.150Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9c",
          "user": {
            "_id": "65c4a574d2db41f74ab2a808",
            "avatarUrl": "/avatars/997a8a51996e909eeb318dc592b6c67a.svg",
            "isPro": false,
            "fullname": "Jianyuan Guo",
            "user": "GGJY",
            "type": "user"
          },
          "name": "Jianyuan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:08:12.386Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9d",
          "name": "Li Shen",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9e",
          "user": {
            "_id": "6306dc1fd37ce67e0e53c202",
            "avatarUrl": "/avatars/d53a29925511a516495b1597fd5dc764.svg",
            "isPro": false,
            "fullname": "Yong Luo",
            "user": "csdvT",
            "type": "user"
          },
          "name": "Yong Luo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:08:19.043Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9f",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda0",
          "user": {
            "_id": "662520a75480987954af60b5",
            "avatarUrl": "/avatars/75d2509d21901c4bb187e93b23540e19.svg",
            "isPro": false,
            "fullname": "Guoxia Wang",
            "user": "Guoxia",
            "type": "user"
          },
          "name": "Guoxia Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:08:27.139Z",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda1",
          "name": "Dianhai Yu",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda2",
          "name": "Yonggang Wen",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda3",
          "name": "Dacheng Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-02T06:33:25.000Z",
      "submittedOnDailyAt": "2025-05-06T00:36:54.063Z",
      "title": "Entrenamiento de modelos de lenguaje de gran escala con baja precisión: métodos, problemas y oportunidades",
      "submittedOnDailyBy": {
        "_id": "64a62e3302e46deb19a7937e",
        "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
        "isPro": false,
        "fullname": "Zhiwei Hao",
        "user": "Zhiwei840",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) han logrado desempeños impresionantes en diversas áreas. Sin embargo, los recursos de hardware de gran escala necesarios para su entrenamiento representan un gran obstáculo para la eficiencia y la escalabilidad. Para mitigar este desafío, se ha introducido ampliamente la tecnología de entrenamiento de baja precisión, que ha dado un claro avance en la eficiencia del entrenamiento. Además, el entrenamiento de baja precisión incluye múltiples componentes, como pesos, activaciones y gradientes, que pueden ser representados con diferentes formas numéricas. Esta diversidad ha creado una variedad de perspectivas en la investigación sobre entrenamiento de baja precisión, permitiendo a los investigadores obtener una visión coherente de la materia. En esta investigación, se ofrece una revisión detallada de los métodos actuales de entrenamiento de baja precisión. Para organizar estas metodologías de manera sistemática, se clasifican en tres grupos principales basados en formas numéricas compatibles con el hardware, eficiencia de cálculo y facilidad de lectura, considerando formas numéricas básicas: (1) métodos basados en punto fijo y enteros, (2) métodos basados en punto flotante, y (3) métodos basados en formas numéricas personalizadas. Además, se discuten enfoques de entrenamiento de normalización, que demuestran la relación entre el enfoque de entrenamiento y el entrenamiento de baja precisión. Finalmente, se presentan muchas direcciones de investigación para el desarrollo de esta área. La colección de artículos discutidos en esta investigación está disponible en https://github.com/Hao840/Awesome-Low-Precision-Training.",
      "upvotes": 8,
      "discussionId": "68196e24d9cad0bb5c90de08",
      "githubRepo": "https://github.com/Hao840/Awesome-Low-Precision-Training",
      "ai_keywords": [
        "low-precision training",
        "weights",
        "activations",
        "gradients",
        "fixed-point",
        "integer-based methods",
        "floating-point-based methods",
        "customized format-based methods",
        "quantization-aware training"
      ]
    },
    "publishedAt": "2025-05-02T02:33:25.000Z",
    "title": "Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities",
    "summary": "Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several componentsx2013such\nas weights, activations, and gradientsx2013each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01043.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a62e3302e46deb19a7937e",
      "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
      "fullname": "Zhiwei Hao",
      "name": "Zhiwei840",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02471",
      "authors": [
        {
          "_id": "681973cfa70a4728958323aa",
          "user": {
            "_id": "644fcbea4f7316588267dc80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
            "isPro": false,
            "fullname": "Biao Gong",
            "user": "BiaoGong",
            "type": "user"
          },
          "name": "Biao Gong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:35.774Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ab",
          "name": "Cheng Zou",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ac",
          "user": {
            "_id": "65dd699a89a2a760d15f7d35",
            "avatarUrl": "/avatars/e098b56c413d147d1f38cf33a4b0ecde.svg",
            "isPro": false,
            "fullname": "Dandan Zheng",
            "user": "zhengdd0422",
            "type": "user"
          },
          "name": "Dandan Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:11.829Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ad",
          "name": "Hu Yu",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ae",
          "user": {
            "_id": "64575ac8cd935d48a47774ec",
            "avatarUrl": "/avatars/5d211e2c13d6c4e011e5e58b738413f7.svg",
            "isPro": false,
            "fullname": "chenjingdong ",
            "user": "chenjingdong",
            "type": "user"
          },
          "name": "Jingdong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:51.440Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323af",
          "user": {
            "_id": "6417cd278f689506e71439ac",
            "avatarUrl": "/avatars/0993d834c6c3bbc53081aa139ee14a12.svg",
            "isPro": false,
            "fullname": "jianxinsun",
            "user": "jianxinsun",
            "type": "user"
          },
          "name": "Jianxin Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:10:02.728Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b0",
          "name": "Junbo Zhao",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b1",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b2",
          "name": "Kaixiang Ji",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b3",
          "name": "Lixiang Ru",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b4",
          "name": "Libin Wang",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b5",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b6",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b7",
          "name": "Weilong Chai",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b8",
          "user": {
            "_id": "67cc852d2cfa481bce2dd07e",
            "avatarUrl": "/avatars/0c1c32ec066a8de9148b083b39d1fab8.svg",
            "isPro": false,
            "fullname": "xinyu xiao",
            "user": "bear-xxy",
            "type": "user"
          },
          "name": "Xinyu Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:11:49.316Z",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b9",
          "name": "Ziyuan Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T08:56:12.000Z",
      "submittedOnDailyAt": "2025-05-06T01:00:49.692Z",
      "title": "Nombre del Unidad: Arquitectura Integrada de Interacciones de la Diversidad Natural\n\n(Note: The translation provided is a direct translation of the given text. The term \"자연 다양성 상호작용 통합 아키텍처\" is translated as \"Arquitectura Integrada de Interacciones de la Diversidad Natural\" to maintain the professional and accurate tone requested.)",
      "submittedOnDailyBy": {
        "_id": "644fcbea4f7316588267dc80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
        "isPro": false,
        "fullname": "Biao Gong",
        "user": "BiaoGong",
        "type": "user"
      },
      "summary": "Men-Lite-Uni es un nuevo marco de trabajo abierto multimodelo que presenta un diseño fundamentalmente multimodelo diseñado para integrar visión y lenguaje. Este proyecto integra MetaQueries y el marco de trabajo M2-omni, introduciendo nuevas estructuras de token escalables y arreglos de representación de imágenes con modelos de difusión. Utilizando modelos fijos y modelos de difusión aprendibles, Men-Lite-Uni puede generar imágenes a partir de texto y editar imágenes basadas en comandos, permitiendo también la expansión de funciones más allá del entendimiento visual. Los resultados experimentales demuestran la potente capacidad de Men-Lite-Uni, y se proporcionan procesos de interacción para evaluar su flujo. Todo el código y los pesos de los modelos están disponibles bajo licencia abierta, fomentando la mejora en la comunidad. Este proyecto se destaca como un hito monumental de la integración de modelos en la actualización del ChatGPT-4o en marzo de 2025, que ha actualizado la generación de imágenes. Men-Lite-Uni se encuentra en la etapa de alfa y se espera que continúe desarrollando en el futuro.",
      "upvotes": 6,
      "discussionId": "681973d2a70a47289583249d",
      "projectPage": "https://github.com/inclusionAI/Ming/tree/main/Ming-unify",
      "githubRepo": "https://github.com/inclusionAI/Ming/tree/main/Ming-unify",
      "ai_keywords": [
        "unified visual generator",
        "multimodal autoregressive model",
        "MetaQueries",
        "M2-omni framework",
        "multi-scale learnable tokens",
        "multi-scale representation alignment strategy",
        "MLLM",
        "learnable diffusion model",
        "text-to-image generation",
        "instruction based image editing"
      ]
    },
    "publishedAt": "2025-05-05T04:56:12.000Z",
    "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction",
    "summary": "We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02471.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644fcbea4f7316588267dc80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
      "fullname": "Biao Gong",
      "name": "BiaoGong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01583",
      "authors": [
        {
          "_id": "6819814653612b577df718e7",
          "user": {
            "_id": "65cd4d6256671dee8ee46392",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cd4d6256671dee8ee46392/SH30XVQnGiYqYQIeDk3na.jpeg",
            "isPro": false,
            "fullname": "Jen-Hao (Andy) Cheng",
            "user": "andaba",
            "type": "user"
          },
          "name": "Jen-Hao Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T08:33:15.728Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718e8",
          "name": "Vivian Wang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718e9",
          "name": "Huayu Wang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ea",
          "name": "Huapeng Zhou",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718eb",
          "name": "Yi-Hao Peng",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ec",
          "name": "Hou-I Liu",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ed",
          "user": {
            "_id": "647e4e8da49bffab5d72fbe0",
            "avatarUrl": "/avatars/c5fb00019c7cea23fe3351ecb1e43195.svg",
            "isPro": false,
            "fullname": "Hsiang-Wei Huang",
            "user": "hsiangwei0903",
            "type": "user"
          },
          "name": "Hsiang-Wei Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:19.727Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ee",
          "name": "Kuang-Ming Chen",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ef",
          "name": "Cheng-Yen Yang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f0",
          "user": {
            "_id": "637c7503fe115289cfecbe6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
            "isPro": false,
            "fullname": "Wenhao Chai",
            "user": "wchai",
            "type": "user"
          },
          "name": "Wenhao Chai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:37.240Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f1",
          "user": {
            "_id": "65f8cb651e0c65c13a2b906a",
            "avatarUrl": "/avatars/ffc8ac8f29ab1a3142fe5fab1b2302ca.svg",
            "isPro": false,
            "fullname": "Yi-Ling Chen",
            "user": "yilche",
            "type": "user"
          },
          "name": "Yi-Ling Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:43.313Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f2",
          "user": {
            "_id": "63c8527becdb7c9fdd9cacc6",
            "avatarUrl": "/avatars/c8a3f5e1e5159ae5ead41bd9fc2b9b34.svg",
            "isPro": false,
            "fullname": "Vibhav Vineet",
            "user": "vibhav-vineet",
            "type": "user"
          },
          "name": "Vibhav Vineet",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:13:49.482Z",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f3",
          "name": "Qin Cai",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f4",
          "name": "Jenq-Neng Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-02T21:00:17.000Z",
      "submittedOnDailyAt": "2025-05-06T01:56:09.960Z",
      "title": "TEMPURA: Predicción de eventos temporales con máscaras de eventos y teoría de razones para las acciones basadas en la comprensión",
      "submittedOnDailyBy": {
        "_id": "637c7503fe115289cfecbe6b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
        "isPro": false,
        "fullname": "Wenhao Chai",
        "user": "wchai",
        "type": "user"
      },
      "summary": "Entendiendo las relaciones de eventos causales y lograr la localización temporal a gran detalle en videos sigue siendo un desafío para los modelos de lenguaje visual. Los métodos existentes, o compresan los tokens de video para reducir la resolución temporal, o tratan los videos como flujos no segmentados, lo que dificulta las fronteras de eventos a gran detalle y limita la modelación de dependencias causales. Proponemos TEMPURA (Predicción y Comprensión de Eventos Causales para la Razonamiento de Acciones), un marco de entrenamiento en dos etapas para mejorar la comprensión temporal de videos. TEMPURA primero aplica la razonamiento de predicción de eventos ocultos para reconstruir eventos faltantes y genera explicaciones causales paso a paso a partir de anotaciones de eventos densas, apoyando técnicas de llenado efectivas. TEMPURA entonces aprende a realizar la segmentación de videos y la captionización densa, descomponiendo los videos en eventos no superpuestos con descripciones detalladas y alineaciones de fechas. Entrenamos TEMPURA en VER, un conjunto de datos a gran escala cuidadosamente curado que contiene 1M instancias de entrenamiento y 500K videos con descripciones de eventos alineadas en tiempo y pasos de razonamiento estructurados. Los experimentos en los benchmarks de localización temporal y detección de destaques demuestran que TEMPURA supera a modelos de referencia fuertes, confirmando el beneficio de combinar la razonamiento causal con la segmentación temporal a gran detalle para mejorar la comprensión de videos.",
      "upvotes": 5,
      "discussionId": "6819814853612b577df71943",
      "ai_keywords": [
        "TEMPURA",
        "masked event prediction",
        "causal explanations",
        "dense event annotations",
        "infilling techniques",
        "video segmentation",
        "dense captioning",
        "non-overlapping events",
        "timestamp-aligned descriptions",
        "VER",
        "temporal grounding",
        "highlight detection",
        "baseline models",
        "causal reasoning",
        "fine-grained temporal segmentation"
      ]
    },
    "publishedAt": "2025-05-02T17:00:17.000Z",
    "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action",
    "summary": "Understanding causal event relationships and achieving fine-grained temporal\ngrounding in videos remain challenging for vision-language models. Existing\nmethods either compress video tokens to reduce temporal resolution, or treat\nvideos as unsegmented streams, which obscures fine-grained event boundaries and\nlimits the modeling of causal dependencies. We propose TEMPURA (Temporal Event\nMasked Prediction and Understanding for Reasoning in Action), a two-stage\ntraining framework that enhances video temporal understanding. TEMPURA first\napplies masked event prediction reasoning to reconstruct missing events and\ngenerate step-by-step causal explanations from dense event annotations, drawing\ninspiration from effective infilling techniques. TEMPURA then learns to perform\nvideo segmentation and dense captioning to decompose videos into\nnon-overlapping events with detailed, timestamp-aligned descriptions. We train\nTEMPURA on VER, a large-scale dataset curated by us that comprises 1M training\ninstances and 500K videos with temporally aligned event descriptions and\nstructured reasoning steps. Experiments on temporal grounding and highlight\ndetection benchmarks demonstrate that TEMPURA outperforms strong baseline\nmodels, confirming that integrating causal reasoning with fine-grained temporal\nsegmentation leads to improved video understanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01583.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c7503fe115289cfecbe6b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
      "fullname": "Wenhao Chai",
      "name": "wchai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02823",
      "authors": [
        {
          "_id": "6819893117007d963b997a0b",
          "user": {
            "_id": "66b2e5f5523bf90aa7057467",
            "avatarUrl": "/avatars/ccdb58c2e56cf861e9dcec50c85d7778.svg",
            "isPro": false,
            "fullname": "Guo",
            "user": "Zinan123212",
            "type": "user"
          },
          "name": "Zinan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:14:34.748Z",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0c",
          "name": "Pengze Zhang",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0d",
          "user": {
            "_id": "639709c2be8a14bb9eeea8f6",
            "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
            "isPro": false,
            "fullname": "Yanze Wu",
            "user": "yanze",
            "type": "user"
          },
          "name": "Yanze Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:02.034Z",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0e",
          "name": "Chong Mou",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0f",
          "name": "Songtao Zhao",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a10",
          "user": {
            "_id": "645dcad7a19f3e64bbf35e6c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/rV1uHDSnZv7jAvFq4ftj4.jpeg",
            "isPro": false,
            "fullname": "Qian He",
            "user": "heqian",
            "type": "user"
          },
          "name": "Qian He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:28.996Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T17:50:24.000Z",
      "submittedOnDailyAt": "2025-05-06T02:30:32.888Z",
      "title": "MUSAR: Utilizamos un enfoque de revisión adaptativo desde un conjunto de datos de un solo tema, empleando rutinas de atención.",
      "submittedOnDailyBy": {
        "_id": "639709c2be8a14bb9eeea8f6",
        "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
        "isPro": false,
        "fullname": "Yanze Wu",
        "user": "yanze",
        "type": "user"
      },
      "summary": "Actualmente, en el enfoque de CUSTOMIZE de Danari existen dos problemas importantes: la dificultad de obtener datos de entrenamiento de diferentes Danari y la combinación de características entre diferentes lógicas. Para resolver estos problemas, proponemos MUSAR (enfoque de CUSTOMIZE de Danari). Este es un marco sencillo y efectivo para lograr un CUSTOMIZE multilógico fuerte, incluso con datos de entrenamiento de una sola lógica. Primero, para abordar la limitación de los datos, introducimos el aprendizaje de dibujos de píxeles con ajustes de bias de dispositivos. Este método construye pares de entrenamiento de dibujos de píxeles a partir de imágenes de una sola lógica y ajusta los sesgos de distribución mediante atención dinámica y LoRA de campos dobles, lo que se realiza de manera dinámica. Además, para eliminar la combinación de características entre diferentes lógicas, introducimos la estructura de ruta de atención dinámica. Esta estructura establece una correspondencia 1 a 1 entre las imágenes generadas y las lógicas condicionales de manera adaptativa. Este diseño permite la decodificación de las representaciones multilógicas y mantiene la generalización de características intercambiables mientras se incrementan las referencias lógicas. Los experimentos detallados muestran que MUSAR supera los métodos existentes en cuanto a calidad, consistencia lógica y naturalidad de comunicación, incluso con datos de una sola lógica.",
      "upvotes": 2,
      "discussionId": "6819893317007d963b997ab1",
      "githubRepo": "https://github.com/guozinan126/MUSAR",
      "ai_keywords": [
        "debiased diptych learning",
        "diptych training pairs",
        "static attention routing",
        "dual-branch LoRA",
        "dynamic attention routing mechanism",
        "bijective mappings",
        "multi-subject representations",
        "scalable generalization performance"
      ]
    },
    "publishedAt": "2025-05-05T13:50:24.000Z",
    "title": "MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset\n  via Attention Routing",
    "summary": "Current multi-subject customization approaches encounter two critical\nchallenges: the difficulty in acquiring diverse multi-subject training data,\nand attribute entanglement across different subjects. To bridge these gaps, we\npropose MUSAR - a simple yet effective framework to achieve robust\nmulti-subject customization while requiring only single-subject training data.\nFirstly, to break the data limitation, we introduce debiased diptych learning.\nIt constructs diptych training pairs from single-subject images to facilitate\nmulti-subject learning, while actively correcting the distribution bias\nintroduced by diptych construction via static attention routing and dual-branch\nLoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic\nattention routing mechanism, which adaptively establishes bijective mappings\nbetween generated images and conditional subjects. This design not only\nachieves decoupling of multi-subject representations but also maintains\nscalable generalization performance with increasing reference subjects.\nComprehensive experiments demonstrate that our MUSAR outperforms existing\nmethods - even those trained on multi-subject dataset - in image quality,\nsubject consistency, and interaction naturalness, despite requiring only\nsingle-subject dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02823.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639709c2be8a14bb9eeea8f6",
      "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
      "fullname": "Yanze Wu",
      "name": "yanze",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 140
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02625",
      "authors": [
        {
          "_id": "681975abba26bf20601bb7ca",
          "user": {
            "_id": "65b7573482d384513443875e",
            "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
            "isPro": false,
            "fullname": "Qingkai Fang",
            "user": "poeroz",
            "type": "user"
          },
          "name": "Qingkai Fang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:43.683Z",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cb",
          "name": "Yan Zhou",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cc",
          "user": {
            "_id": "66680c0505c407bfea87667c",
            "avatarUrl": "/avatars/e3c26d2eb13fe8ad2b3fd16897e61e6d.svg",
            "isPro": false,
            "fullname": "Shoutao Guo",
            "user": "guoshoutao",
            "type": "user"
          },
          "name": "Shoutao Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:15:53.154Z",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cd",
          "user": {
            "_id": "64803e5dc57f629056c601f1",
            "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
            "isPro": false,
            "fullname": "Shaolei Zhang",
            "user": "zhangshaolei",
            "type": "user"
          },
          "name": "Shaolei Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:16:00.139Z",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7ce",
          "name": "Yang Feng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T12:53:09.000Z",
      "submittedOnDailyAt": "2025-05-06T01:07:20.259Z",
      "title": "LLaMA-Omni2: Modelo de lenguaje de secuencias de tiempo basado en conversación en tiempo real y síntesis automática de retroceso",
      "submittedOnDailyBy": {
        "_id": "65b7573482d384513443875e",
        "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
        "isPro": false,
        "fullname": "Qingkai Fang",
        "user": "poeroz",
        "type": "user"
      },
      "summary": "La interacción en tiempo real, inteligente y natural de voz es una parte crucial de la interacción humano-computadora de las próximas generaciones. Los últimos avances han demostrado la posibilidad de construir conversadores de lenguaje inteligente basados en modelos de lenguaje de gran escala (LLMs). En este artículo, se presenta una serie de modelos de lenguaje de voz (SpeechLMs) llamada LLaMA-Omni 2, que tienen entre 0.5B y 14B parámetros. Estos modelos poseen la capacidad de realizar interacciones en tiempo real de alta calidad de voz. LLaMA-Omni 2 se basa en la serie de modelos Qwen2.5 y se ha construido integrando un encoder de voz y un decoder de streaming de voz de autocorrección. Aunque se ha entrenado solo con muchos de 200K muestras de diálogo de voz, LLaMA-Omni 2 muestra un gran rendimiento en pruebas de preguntas y respuestas de lenguaje y de instrucciones de voz, superando a los avanzados SpeechLMs como GLM-4-Voice, que se ha entrenado con millones de horas de datos de voz.",
      "upvotes": 2,
      "discussionId": "681975abba26bf20601bb7f2",
      "ai_keywords": [
        "speech language models (SpeechLMs)",
        "Qwen2.5",
        "speech encoder",
        "autoregressive streaming speech decoder",
        "spoken question answering",
        "speech instruction following",
        "GLM-4-Voice"
      ]
    },
    "publishedAt": "2025-05-05T08:53:09.000Z",
    "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis",
    "summary": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b7573482d384513443875e",
      "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
      "fullname": "Qingkai Fang",
      "name": "poeroz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01456",
      "authors": [
        {
          "_id": "6819c7577c36c576e9cb6bfa",
          "user": {
            "_id": "64f64da90efa33bfe0a3d9ba",
            "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
            "isPro": false,
            "fullname": "Vaidehi Patil",
            "user": "vaidehi99",
            "type": "user"
          },
          "name": "Vaidehi Patil",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:16:35.372Z",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfb",
          "user": {
            "_id": "654ffe334d9e71e17becc660",
            "avatarUrl": "/avatars/022b7a77051d26c4e5cbf254b7352eb9.svg",
            "isPro": false,
            "fullname": "Yi-Lin Sung",
            "user": "a2889184",
            "type": "user"
          },
          "name": "Yi-Lin Sung",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:16:41.799Z",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfc",
          "name": "Peter Hase",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfd",
          "name": "Jie Peng",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bfe",
          "name": "Tianlong Chen",
          "hidden": false
        },
        {
          "_id": "6819c7577c36c576e9cb6bff",
          "user": {
            "_id": "665d9d3a057f7c508f98c625",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d9d3a057f7c508f98c625/u1R9P9sJoAl4zEIcetbPy.jpeg",
            "isPro": false,
            "fullname": "Mohit Bansal",
            "user": "mohitbansal",
            "type": "user"
          },
          "name": "Mohit Bansal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-06T09:17:14.823Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T01:54:00.000Z",
      "submittedOnDailyAt": "2025-05-06T06:55:27.299Z",
      "title": "Eliminación de datos de entrenamiento de información sensible en múltiples LLM: evaluación de benchmark y defensa contra ataques",
      "submittedOnDailyBy": {
        "_id": "64f64da90efa33bfe0a3d9ba",
        "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
        "isPro": false,
        "fullname": "Vaidehi Patil",
        "user": "vaidehi99",
        "type": "user"
      },
      "summary": "Los Modelos de Lenguaje de Grandes Tamaños (LLMs) se entrenan con grandes conjuntos de datos, lo que implica un riesgo潜意识mente de obtener información sensitiva (datos personales o contenido potencialmente dañino). Este riesgo aumenta aún más debido a la integración de información de imágenes y texto en modelos diversificados (que combinan múltiples LLMs). Los anuncios pueden extraer detalles sensitivos utilizando estos conocimientos a través de los modelos diversificados. Para evaluar el efecto de eliminar cierta información en los Modelos de Lenguaje y Imágenes (MLLMs), es necesario crear pares de imágenes-texto que se describen de alta calidad. La investigación previa ha centrado principalmente en el texto, pero la forma de diversificación ha recibido poca atención. Para complementar este vacío, presentamos el marco de referencia y el marco de defensa para el benchmark de olvido de la forma de diversificación (UnLOK-VQA), y hemos desarrollado métodos para evaluar la eliminación de conocimientos específicos de la forma de diversificación en MLLMs. Hemos expandido los datos de respuesta a las preguntas visuales mediante un proceso automático, generado nuevos ejemplos para verificar la generalización y la particularidad, y filtrado manualmente para mantener la calidad. Luego, evaluamos 6 objetivos de defensa frente a 7 ataques (4 blanco y 3 negro) utilizando métodos que incluyen la interpretabilidad con un nuevo método blanco. Los resultados muestran que los ataques de la forma de diversificación son más efectivos que los ataques a texto o imágenes, y que la mejor defensa es eliminar la información de respuesta mediante el estado interno del modelo. Además, los grandes modelos tienen una mayor eficiencia posterior y muestran que la escala puede mejorar la seguridad. UnLOK-VQA se convertirá en un estricto benchmark para promover el olvido en los MLLMs.",
      "upvotes": 0,
      "discussionId": "6819c7597c36c576e9cb6c6b",
      "githubRepo": "https://github.com/Vaidehi99/UnLOK-VQA",
      "ai_keywords": [
        "multimodal LLMs",
        "multimodal prompts",
        "targeted unlearning",
        "high-quality, well-annotated image-text pairs",
        "multimodal unlearning",
        "UnLOK-VQA (Unlearning Outside Knowledge VQA)",
        "visual question-answering dataset",
        "varying-proximity samples",
        "whitebox attacks",
        "blackbox attacks",
        "interpretability of hidden states",
        "multimodal attacks",
        "post-editing robustness"
      ]
    },
    "publishedAt": "2025-04-30T21:54:00.000Z",
    "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and\n  Attack-Defense Evaluation",
    "summary": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01456.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f64da90efa33bfe0a3d9ba",
      "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
      "fullname": "Vaidehi Patil",
      "name": "vaidehi99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]