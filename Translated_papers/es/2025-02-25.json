[
  {
    "paper": {
      "id": "2502.17157",
      "authors": [
        {
          "_id": "67bd3285ac4a596a43b53205",
          "user": {
            "_id": "646efd223dd912a539e0bd46",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
            "isPro": true,
            "fullname": "Canyu Zhao",
            "user": "Canyu",
            "type": "user"
          },
          "name": "Canyu Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:20.829Z",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53206",
          "name": "Mingyu Liu",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53207",
          "user": {
            "_id": "64d60375d7e30889c65e8cf4",
            "avatarUrl": "/avatars/640f7c570fc45194557ce7931bdfe87f.svg",
            "isPro": false,
            "fullname": "Huanyi Zheng",
            "user": "zhyya",
            "type": "user"
          },
          "name": "Huanyi Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:18.731Z",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53208",
          "user": {
            "_id": "632179745fc60c44fd91fc33",
            "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
            "isPro": false,
            "fullname": "zhumuzhi",
            "user": "Z-MU-Z",
            "type": "user"
          },
          "name": "Muzhi Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:11.968Z",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53209",
          "name": "Zhiyue Zhao",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320a",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320b",
          "name": "Tong He",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320c",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T13:51:06.000Z",
      "title": "Diseño: Modelo general para la reconocimiento visual de tareas",
      "summary": "El principal objetivo de este artículo es crear un buen modelo general de reconocimiento que pueda manejar varias tareas, considerando las limitaciones de recursos computacionales y datos de entrenamiento. Para lograrlo, se utiliza un modelo extendido de imágenes a partir de texto proporcionado en cientos de millones de imágenes. En nuestros criterios de evaluación, DICEPTION demostró ser efectiva en varias tareas de reconocimiento y alcanzar el rendimiento de los modelos más recientes. Al utilizar solo aproximadamente 0.06% de los datos de SAM-vit-h (por ejemplo, 600K imágenes con etiquetas de nivel de píxeles en 1B imágenes) obtuvo los mismos resultados. Inspirado en Wang et al., DICEPTION configura los resultados de diversas tareas de reconocimiento con codificación de colores y asigna colores aleatorios a diferentes instancias, lo que es muy efectivo para segmentación de objetos y segmentación de significado. Al unificar las tareas de reconocimiento mediante la generación de imágenes condicionales, se puede maximizar el uso de modelos preentrenados desde texto a imágenes. De esta manera, DICEPTION puede entrenarse de manera eficiente con menos datos o parámetros comparados con modelos existentes, especialmente cuando se aplica a diversas tareas. Al aplicar el modelo a otras tareas, solo se necesitan 50 imágenes. DICEPTION ofrece una contribución valiosa a los modelos generales visuales y un solución más sostenible.",
      "upvotes": 35,
      "discussionId": "67bd328aac4a596a43b532ae"
    },
    "publishedAt": "2025-02-24T22:39:29.837Z",
    "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17157.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "646efd223dd912a539e0bd46",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
      "fullname": "Canyu Zhao",
      "name": "Canyu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17129",
      "authors": [
        {
          "_id": "67bd37cb0d41e01cca99aa8b",
          "user": {
            "_id": "64f033ef82c6eea604c4da8b",
            "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
            "isPro": false,
            "fullname": "Liu Xiaoran",
            "user": "LiuXR",
            "type": "user"
          },
          "name": "Xiaoran Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:07.298Z",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8c",
          "name": "Ruixiao Li",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8d",
          "name": "Mianqiu Huang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8e",
          "name": "Zhigeng Liu",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8f",
          "name": "Yuerong Song",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa90",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa91",
          "name": "Siyang He",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa92",
          "name": "Qiqi Wang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa93",
          "name": "Linlin Li",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa94",
          "name": "Qun Liu",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa95",
          "name": "Yaqian Zhou",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa96",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa97",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T13:19:33.000Z",
      "title": "**Título:** Frases Largas y Palabras de Grandes Modelos de Lenguaje\n\n**Texto:**\n\nFrases largas y grandes modelos de lenguaje están diseñados para leer y comprender frases largas. Este modelo puede proporcionar respuestas más precisas considerando el contexto largo. Esta capacidad es importante en muchas aplicaciones y se utiliza en diversas áreas como análisis de texto, traducción y diseño de sistemas.",
      "summary": "La extensión de contexto es un problema importante en el procesamiento del lenguaje natural (NLP), desarrollado a través de arquitecturas de NLP, y ofrece una gran oportunidad para el aprendizaje de vida humano en los modelos de lenguaje de gran escala (LLMs). Sin embargo, el enfoque en la extensión de contexto se enfrenta a diversos obstáculos. Sin embargo, la extensión de contexto mantiene la competencia central de los LLMs. En los últimos dos años, la longitud de contexto de los LLMs ha aumentado hasta millones de tokens, demostrando un gran progreso. Además, la investigación sobre la extensión de contexto de los LLMs no debe centrarse solo en la longitud, sino que también debe enfocarse en la arquitectura, infraestructura, entrenamiento y evaluación.\n\nBasándonos en la cita de la oradora de Smart Field \"Así dijo él\", encontramos una similitud entre el deseo de extender el contexto de los LLMs y el esfuerzo de la humanidad para superar el dolor de su muerte. En este estudio, explicamos la necesidad de los LLMs para una extensión de contexto grande y la necesidad de aceptar que esto finalmente sea limitado. Para lograrlo, presentamos la vida ciclo de los LLMs de extensión de contexto desde cuatro perspectivas: arquitectura, infraestructura, entrenamiento y evaluación, mostrando todo el panorama de la tecnología. Finalmente, este estudio presenta 10 preguntas sin respuesta en los LLMs de extensión de contexto actuales. Esperamos que este estudio sirva como una introducción sistemática a la investigación en extensión de contexto de los LLMs.",
      "upvotes": 25,
      "discussionId": "67bd37cc0d41e01cca99ab1e"
    },
    "publishedAt": "2025-02-24T22:27:11.566Z",
    "title": "Thus Spake Long-Context Large Language Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f033ef82c6eea604c4da8b",
      "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
      "fullname": "Liu Xiaoran",
      "name": "LiuXR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15814",
      "authors": [
        {
          "_id": "67bd3972f077ddf1f98bacda",
          "user": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "isPro": false,
            "fullname": "Gallil Maimon",
            "user": "gallilmaimon",
            "type": "user"
          },
          "name": "Gallil Maimon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:36.258Z",
          "hidden": false
        },
        {
          "_id": "67bd3972f077ddf1f98bacdb",
          "user": {
            "_id": "644662145004f2cb3af08b27",
            "avatarUrl": "/avatars/5f2af24c7410a5db46374d0b84fb479d.svg",
            "isPro": false,
            "fullname": "Avishai Elmakies",
            "user": "avishai-elmakies",
            "type": "user"
          },
          "name": "Avishai Elmakies",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:33.712Z",
          "hidden": false
        },
        {
          "_id": "67bd3972f077ddf1f98bacdc",
          "name": "Yossi Adi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T17:21:15.000Z",
      "title": "Slam: Se entrena un modelo de lenguaje a través de un corte de un día.",
      "summary": "Slam es un receta para entrenar modelos de lenguaje de alta calidad (SLMs) en un solo GPU académico en 24 horas. Para ello, se realizan experimentos que incluyen la inicialización del modelo, la arquitectura, la generación de datos sintéticos, la optimización de preferencias basada en datos sintéticos, y la ajuste de todos los demás componentes. Aunque demostró experimentalmente, esta receta de entrenamiento puede escalarse para usar más cálculo, y requiere menos costo computacional para obtener resultados avanzados como los SLMs. Estas observaciones se proporcionan para facilitar el entrenamiento y la investigación de SLMs. A partir de la escalabilidad de los SLMs, nuestros resultados superan significativamente la performance computacional predicha y ofrecen una visión más equilibrada de las posibilidades de los SLMs. Los códigos, datos, modelos y muestras se pueden encontrar en la siguiente URL: https://pages.cs.huji.ac.il/adiyoss-lab/slamming.",
      "upvotes": 19,
      "discussionId": "67bd3973f077ddf1f98bacf9"
    },
    "publishedAt": "2025-02-24T23:14:12.363Z",
    "title": "Slamming: Training a Speech Language Model on One GPU in a Day",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/t93GkoiYRplnXH1Go0MmY.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15814.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b9bc2dacdbc1d0b39c3b50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
      "fullname": "Gallil Maimon",
      "name": "gallilmaimon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16584",
      "authors": [
        {
          "_id": "67bd42386959e61abd265a9b",
          "name": "Liumeng Xue",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9c",
          "name": "Ziya Zhou",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9d",
          "name": "Jiahao Pan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9e",
          "name": "Zixuan Li",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9f",
          "name": "Shuai Fan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa0",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa1",
          "name": "Sitong Cheng",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa2",
          "name": "Dongchao Yang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa3",
          "name": "Haohan Guo",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa4",
          "name": "Yujia Xiao",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa5",
          "name": "Xinsheng Wang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa6",
          "name": "Zixuan Shen",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa7",
          "name": "Chuanbo Zhu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa8",
          "name": "Xinshen Zhang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa9",
          "name": "Tianchi Liu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aaa",
          "name": "Ruibin Yuan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aab",
          "name": "Zeyue Tian",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aac",
          "name": "Haohe Liu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aad",
          "name": "Emmanouil Benetos",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aae",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aaf",
          "name": "Yike Guo",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265ab0",
          "name": "Wei Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T14:24:15.000Z",
      "title": "Audio-FLAN: Preliminary Liris",
      "summary": "El desarrollo reciente de los modelos de tokenización de voz en las grandes redes de lenguaje (LLMs) ha significativamente mejorado la integración de funciones de voz. Sin embargo, la comprensión y la generación de voz se tratan generalmente como tareas diferentes, lo que hace que el desarrollo de modelos de lenguaje de voz integrados sea más difícil. La fine-tuning de instancias ha demostrado un éxito notable en la generalización y el aprendizaje sin ejemplos en el campo de la imagen y el texto, pero su aplicación en el campo del voz aún es poco explorada. Un de los principales obstáculos es la escasez de conjuntos de datos específicos para integrar la comprensión y la generación de voz. Para abordar este problema, se presenta Audio-FLAN, un gran conjunto de datos de fine-tuning de instancias. Audio-FLAN incluye 80 tareas diferentes y abarca el campo de voz, música y sonido, con más de 100 mil instancias. Audio-FLAN sirve como base para construir modelos de lenguaje de voz integrados que pueden tratar tanto la comprensión (por ejemplo, traducción, comprensión) como la generación (por ejemplo, voz, música, sonido). El conjunto de datos Audio-FLAN está disponible en HuggingFace y GitHub y se actualiza regularmente.",
      "upvotes": 18,
      "discussionId": "67bd423b6959e61abd265b88"
    },
    "publishedAt": "2025-02-24T23:14:20.487Z",
    "title": "Audio-FLAN: A Preliminary Release",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fd6f670053c8345eddc1b68",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd6f670053c8345eddc1b68/cuTsu2krRYHC6zYGD2dpQ.jpeg",
      "fullname": "Ruibin Yuan",
      "name": "a43992899",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17435",
      "authors": [
        {
          "_id": "67bd6b4b8edd1ce8ad5603a0",
          "name": "Chen-Wei Chang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a1",
          "name": "Cheng-De Fan",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a2",
          "name": "Chia-Che Chang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a3",
          "name": "Yi-Chen Lo",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a4",
          "name": "Yu-Chee Tseng",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a5",
          "name": "Jiun-Long Huang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a6",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:59:54.000Z",
      "title": "GCC: Se expande el color checker y se normaliza el color generado.",
      "summary": "La ley de la constante de tono de color reconoce frecuentemente que es difícil generalizar entre diferentes sensores de cámara. Presentamos el GCC (Constante de Color Global) utilizando modelos de difusión. El GCC realiza la constante de tono de color interpolando marcadores de color en las imágenes. Nuestra innovación principal consiste en: (1) un enfoque de inferencia seguro que refleja la luz del panorama en un solo paso, (2) un método de diferenciación de Laplace que permite adaptación de tono de color dependiente de la luz mientras se mantiene la estructura de los marcadores, y (3) una estrategia de expansión de datos basada en máscaras para manejar anotaciones incorrectas de marcadores de color. El GCC muestra la mayor robustez en escenarios de cámaras múltiples, con un porcentaje de error máximo del 25% de 5.15° y 4.32°. Estos resultados demuestran que el modelo no necesita considerar las características de la cámara ni necesita entrenamiento específico para los sensores, lo que lo convierte en una solución amplia de aplicaciones en la realidad.",
      "upvotes": 16,
      "discussionId": "67bd6b4d8edd1ce8ad560401"
    },
    "publishedAt": "2025-02-25T02:06:00.809Z",
    "title": "GCC: Generative Color Constancy via Diffusing a Color Checker",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/gDAYQUcbNE2Ps2pQFxg_m.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6459d5da3b6fafd9664807ab",
      "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
      "fullname": "Yu-Lun Liu",
      "name": "yulunliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16614",
      "authors": [
        {
          "_id": "67bd36334a9a04b9ca9bbb68",
          "name": "Alexander Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb69",
          "name": "Marcus Dong",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6a",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6b",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6c",
          "name": "Yejie Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6d",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6e",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6f",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb70",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb71",
          "name": "Yingshui Tan",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb72",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb73",
          "name": "Zhexu Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb74",
          "name": "Weixun Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb75",
          "name": "Yancheng He",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb76",
          "name": "Ken Deng",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb77",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb78",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb79",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T15:36:43.000Z",
      "title": "CodeCriticBench: Modelo de lenguaje de programación que proporciona un marco de referencia para evaluar código de manera general.",
      "summary": "La capacidad crítica de los LLMs es fundamental para proporcionar habilidades lógicas y proporcionar sugerencias necesarias (por ejemplo, análisis detallado y retroalimentación práctica). Por lo tanto, los métodos de evaluación de la capacidad crítica de los LLMs reciben mucha atención y se han propuesto varios marcadores críticos. Sin embargo, los actuales marcadores críticos tienen limitaciones: (1) se enfocan en tareas lógicas generales en diferentes áreas, pero la evaluación en tareas de código es insuficiente (por ejemplo, solo se evalúan tareas de generación de código). Estas tareas tienen un nivel de dificultad relativamente sencillo (por ejemplo, las consultas de código en CriticBench se toman de Humaneval y MBPP). (2) se falta una evaluación detallada en varios aspectos. Para enfrentar estas limitaciones, presentamos un nuevo marcador crítico basado en mapas llamado CodeCriticBench. En particular, nuestro CodeCriticBench incluye dos tareas de código principales (generación de código y cuestionamiento de código) y su protocolo de evaluación incluye evaluaciones críticas básicas y desarrolladas, además de diseñar listas de chequeo detalladas para las evaluaciones desarrolladas, que son adecuadas para estas configuraciones. Finalmente, hemos realizado experimentos con diferentes LLMs y demostramos la efectividad de CodeCriticBench.",
      "upvotes": 13,
      "discussionId": "67bd36354a9a04b9ca9bbc16"
    },
    "publishedAt": "2025-02-24T22:17:28.937Z",
    "title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65377c30e48353201e6fdda0",
      "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
      "fullname": "Jiaheng Liu",
      "name": "CheeryLJH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16033",
      "authors": [
        {
          "_id": "67bd31d0d055a27740b16a30",
          "name": "Qianqi Yan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a31",
          "name": "Yue Fan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a32",
          "name": "Hongquan Li",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a33",
          "name": "Shan Jiang",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a34",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a35",
          "name": "Xinze Guan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a36",
          "name": "Ching-Chen Kuo",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a37",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-22T01:52:37.000Z",
      "title": "Polymorphism Inappropriateness Inference (MMIR): Nuevo Benchmark para Modelos de Inferencia de Polimorfismo",
      "summary": "Actualmente, los modelos de lenguaje multimodal (MLLM) se entrenan y se validan principalmente con entradas de imagen-texto coherentes, pero se plantea la pregunta de si pueden manejar adecuadamente la inadecuación en contenidos muy ricos y variados como los de Ruffled Room. Para remediar esta inadecuación, proponemos el Multimodal Inconsistency Reasoning (MMIR) como un marco de evaluación para la capacidad de los MLLM en detectar y justificar la semántica inadecuada. MMIR incluye 534 muestras difíciles y incorpora errores sintéticos en cinco áreas causantes: contrarios factuales, identidad no definida, incoherencia contextual, desacuerdo cuantitativo y desacuerdo temporal/espacial. Evaluamos seis modelos de MLLM óptimos y observamos que los modelos con capacidades de razonamiento semántico (como o1) superan significativamente a sus modelos de contenedor, pero los modelos abierto-código son particularmente vulnerables a errores de inadecuación. El análisis detallado muestra que la capacidad de detectar la inadecuación semántica es especialmente desafiante en el texto, mientras que los modelos experimentan conflictos entre modelos y complicaciones en contenidos de Ruffled Room. Experimentos exploratorios demostraron efectos micro en la prompting de modelos únicos que incluyen métodos como Chain-of-Thought (CoT) o Set-of-Mark (SoM), y mostraron un registro de razonamiento cruzado de modelos. Nuestros hallazgos demuestran la necesidad de avances en la razonamiento multimodal y orientan futuras investigaciones hacia el estudio de la inadecuación multimodal.",
      "upvotes": 11,
      "discussionId": "67bd31d2d055a27740b16ad9"
    },
    "publishedAt": "2025-02-24T21:59:50.456Z",
    "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16033.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16894",
      "authors": [
        {
          "_id": "67bd396ea06bae99f3866911",
          "user": {
            "_id": "641aa5e391e3376a057bbd4c",
            "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
            "isPro": false,
            "fullname": "Chenghao Fan",
            "user": "Facico",
            "type": "user"
          },
          "name": "Chenghao Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:38.942Z",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866912",
          "name": "Zhenyi Lu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866913",
          "name": "Sichen Liu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866914",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866915",
          "name": "Wei Wei",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866916",
          "name": "Chengfeng Gu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866917",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T06:48:13.000Z",
      "title": "La mejora de LoRA utilizando optimización de valores propios adaptativos y alineamiento de expertos en el optimización de LoRA",
      "summary": "Low-Rank Adaptation (LoRA) permite una micro-ajuste parámetros eficiente en los Grandes Modelos de Lenguaje (LLMs). Sin embargo, este rendimiento generalmente es inferior al de Fine-Tuning Completo (Full FT). Los métodos actuales inicializan y optimizan LoRA a través de subconjuntos de SVD (Decomposición de Valores Singulares), pero esta forma no se conecta con el aprovechamiento óptimo de conocimientos previamente entrenados. Una de las formas adicionales para mejorar LoRA es introducir la arquitectura Mixture-of-Experts (MoE). Sin embargo, introducir SVD en una arquitectura MoE presenta problemas como la mal distribución de pesos y la complejidad de la dinámica de gradientes. Para resolver estos problemas, proponemos el marco de trabajo Great LoRA Mixture-of-Expert (GOAT). Este marco proporciona: (1) la integración adaptativa de líderes relacionados utilizando una arquitectura MoE estructurada por SVD y (2) la calculación de un factor de escalación teórico para ofrecer funcionalidades para el MoE Fine-Tuned Completo y la optimización. Este enfoque puede mejorar la eficiencia y el rendimiento de LoRA MoE sin necesidad de cambiar la arquitectura o el algoritmo de entrenamiento. Los experimentos realizados en 25 conjuntos de datos de procesamiento de lenguaje natural, inferencia de conocimiento general, clasificación de imágenes y generación de lenguaje natural muestran el rendimiento reciente del GOAT y reducen la diferencia con el Full FT.",
      "upvotes": 10,
      "discussionId": "67bd396fa06bae99f3866964"
    },
    "publishedAt": "2025-02-24T22:35:41.042Z",
    "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16894.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641aa5e391e3376a057bbd4c",
      "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
      "fullname": "Chenghao Fan",
      "name": "Facico",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17407",
      "authors": [
        {
          "_id": "67bd48d4becb766415a5d19d",
          "name": "Guijin Son",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d19e",
          "name": "Jiwoo Hong",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d19f",
          "user": {
            "_id": "63e087b6a98d931aa90c1b9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e087b6a98d931aa90c1b9c/96c6IT3f1pWGLbRdRDB2U.png",
            "isPro": false,
            "fullname": "Hyunwoo Ko",
            "user": "Cartinoe5930",
            "type": "user"
          },
          "name": "Hyunwoo Ko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:12.933Z",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d1a0",
          "name": "James Thorne",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:36:15.000Z",
      "title": "El contenido sobre la generalización lingüística de la escala en el momento de la medida",
      "summary": "El escalado de pruebas ha demostrado ser efectivo para la alcanzamiento de multilingüismo, pero su efectividad en escalar con el mismo impacto durante la prueba es incierta. En este artículo, se presenta MCLM (Benchmark Multilingual de Matemática), una versión multilingüe que aborda problemas de nivel competitivo en 55 idiomas. Se verificaron tres métodos de escalado durante las pruebas: Outcome Reward Modeling (ORM), Process Reward Modeling (ORM) y Budget Forcing (BF) para Qwen2.5-1.5B Math y MR1-1.5B. Al aplicar ORM a Qwen2.5-1.5B Math, se obtuvo un puntaje de 35.8 puntos, mientras que BF aplicado a MR1-1.5B resultó en un puntaje de 35.2 puntos. \"Thinking LLMs\" ha recibido atención reciente, pero su rendimiento es relativamente bueno cuando se limita la cantidad de FLOPs de inferencia, comparado con métodos tradicionales de escalado. Además, BF demostró un aumento de 20 puntos en AIME en inglés, pero solo un aumento promedio de 1.94 puntos en otros idiomas. Este patrón se mantiene en otros métodos de escalado durante las pruebas. La escalado durante las pruebas no se expande efectivamente para tareas multilingües. Para avanzar, se propone MCLM, MR1-1.5B y se publican los resultados de evaluación.",
      "upvotes": 9,
      "discussionId": "67bd48d5becb766415a5d1e9"
    },
    "publishedAt": "2025-02-24T23:37:53.138Z",
    "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17407.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d3e619b8448e1785bbda2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
      "fullname": "GUIJIN SON",
      "name": "amphora",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17110",
      "authors": [
        {
          "_id": "67bd3936daef22cbce6d7ef2",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef3",
          "user": {
            "_id": "645b10e80c73ea27d13f7aca",
            "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
            "isPro": false,
            "fullname": "xuhaiyang",
            "user": "xhyandwyy",
            "type": "user"
          },
          "name": "Haiyang Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:41.528Z",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef4",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef5",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef6",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef7",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef8",
          "name": "Jitao Sang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T12:51:23.000Z",
      "title": "Mobile-Agent-V: Aprendizaje de la acción de dispositivos móviles mediante la colaboración de múltiples agentes en un guión de video",
      "summary": "Con el aumento del uso de dispositivos móviles, es necesario una mejora continua en la gestión de tareas. Sin embargo, muchos marcos de trabajo dirigidos por IA sufren desafíos debido a la falta de conocimientos operativos. El conocimiento manual puede ser útil, pero es costoso y no eficiente. Para resolver estos problemas, se presenta el marco de trabajo Mobile-Agent-V. Este marco de trabajo proporciona conocimientos operativos para la automatización móvil ricos y costo eficientes utilizando guías de video. Mobile-Agent-V no requiere entrenamiento o preprocesamiento y mejora la capacidad de ejecución de tareas utilizando entradas de video. Adopta una estrategia de ventanas deslizantes y combina agentes de video con agentes de reflexión profunda, asegurando que las acciones coincidan con las instrucciones del usuario. Con esta innovadora metodología, los usuarios pueden recibir guías para registrar procesos de trabajo y el sistema puede aprender automáticamente y ejecutar tareas de manera eficiente. Los resultados de las pruebas muestran que Mobile-Agent-V logra un aumento del 30% en rendimiento en comparación con otros marcos de trabajo actuales.",
      "upvotes": 8,
      "discussionId": "67bd3938daef22cbce6d7f9d"
    },
    "publishedAt": "2025-02-24T22:31:17.771Z",
    "title": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/mshxtP77rrnN07f6ux6_0.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17110.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16922",
      "authors": [
        {
          "_id": "67bd3d6b60186d7478467208",
          "user": {
            "_id": "6643261b8876db14227eeb19",
            "avatarUrl": "/avatars/67428c9e37a2273697c0547e1783ec6b.svg",
            "isPro": false,
            "fullname": "Zhenglin Wang",
            "user": "wzl0228",
            "type": "user"
          },
          "name": "Zhenglin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:15.633Z",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d7478467209",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720a",
          "name": "Pengfei LI",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720b",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720c",
          "name": "Deyu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T07:27:54.000Z",
      "title": "China Era-Based Time-Series Benchmark and Arrayment Benchmark",
      "summary": "La inferencia temporal es la base cognitiva humana y es importante en diversas aplicaciones de la realidad. El desarrollo reciente de los grandes modelos de lenguaje ha demostrado capacidades esperables en la inferencia temporal, pero los actuales benchmarks se construyen principalmente basándose en reglas, con una profundidad de contexto insuficiente y un rango limitado de entidades temporales. Para resolver estas limitaciones, presentamos el Chinese Temporal Inference (CTM). El CTM es un benchmark diseñado para evaluar la inferencia temporal en una amplia gama de secuencias temporales en China. El CTM enfatiza relaciones entre entidades cruzadas, enunciados de fechas en tiempo reverso, inferencias contextualizadas y culturalmente fundamentadas, y ofrece evaluaciones detalladas. A través de experimentos expandidos, el CTM clarifica los problemas y presenta posibilidades de mejora.",
      "upvotes": 7,
      "discussionId": "67bd3d6c60186d7478467249"
    },
    "publishedAt": "2025-02-24T22:48:30.357Z",
    "title": "Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16922.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15894",
      "authors": [
        {
          "_id": "67bd3bd26faf9f04b2170f61",
          "name": "Min Zhao",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f62",
          "name": "Guande He",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f63",
          "name": "Yixiao Chen",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f64",
          "user": {
            "_id": "64c269a52d73768f07ac266c",
            "avatarUrl": "/avatars/d497a960f8aef6a974907b68ed750c1c.svg",
            "isPro": false,
            "fullname": "Zhu Hongzhou",
            "user": "zhuhz22",
            "type": "user"
          },
          "name": "Hongzhou Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:23.502Z",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f65",
          "name": "Chongxuan Li",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f66",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T19:28:05.000Z",
      "title": "RIFLEx: Lanzamiento Gratuito para la Estimación de Longitud en la División de Imágenes con Transformers",
      "summary": "El reciente avance en la generación de imágenes ha permitido que modelos puedan sintetizar imágenes de alta calidad en un minuto. Sin embargo, la generación de imágenes largas y la mantener la continuidad temporal es un gran desafío, y los métodos actuales de estimación de longitud presentan problemas debido a la recreación temporal o la lentitud del movimiento. En este artículo, se analiza sistemáticamente el papel de las componentes de frecuencia de la ubicación y se identifica una frecuencia única que principalmente controla la acción de estimación de longitud. Basándose en esta observación, se propone una aproximación mínima y efectiva llamada RIFLEx. RIFLEx reduce la frecuencia única para mantener la coherencia del movimiento mientras se inhibe la recreación, sin necesidad de cambios adicionales. RIFLEx realiza una estimación de longitud de la calidad máxima en doble mediante un entrenamiento completo con los transformadores de difusión de imágenes más recientes. Además, con un ajuste mínimo, es posible alcanzar una estimación de longitud tres veces más larga, sin necesidad de imágenes largas. El sitio web del proyecto y el código están disponibles en la URL proporcionada.",
      "upvotes": 6,
      "discussionId": "67bd3bd66faf9f04b21710d1"
    },
    "publishedAt": "2025-02-25T00:09:04.483Z",
    "title": "RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15894.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6205
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16707",
      "authors": [
        {
          "_id": "67bd3bcc797e4d53ce0bc70d",
          "user": {
            "_id": "64f8fbd95515d7dcceb906b1",
            "avatarUrl": "/avatars/1c7d034de408930b166592465e65fc31.svg",
            "isPro": false,
            "fullname": "Yunhai Feng",
            "user": "yunhaif",
            "type": "user"
          },
          "name": "Yunhai Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:31.085Z",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc70e",
          "user": {
            "_id": "62318c0386753f5f41d0e261",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
            "isPro": false,
            "fullname": "Jiaming Han",
            "user": "csuhan",
            "type": "user"
          },
          "name": "Jiaming Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:28.772Z",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc70f",
          "name": "Zhuoran Yang",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc710",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc711",
          "name": "Sergey Levine",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc712",
          "name": "Jianlan Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T20:42:15.000Z",
      "title": "Replicable Planning: Vision-Language Models for Multi-Stage Long-Term Robot Operations",
      "summary": "Para resolver problemas complejos y a largo plazo de manipulación de robots, se requieren habilidades de planificación altas, comprensión del mundo físico y capacidades de predicción a largo plazo para abordar la acumulación de errores predictivos a largo plazo. Modelos de lenguaje visual (VLMs) se han entrenado previamente con datos de internet, lo que les permite proporcionar un marco para resolver estos problemas. Sin embargo, en su forma actual, los VLMs no comprenden los detalles físicos complejos necesarios para la manipulación automática ni tienen capacidades de predicción a largo plazo para abordar la acumulación de errores predictivos a largo plazo. En este artículo, se presenta un nuevo marco de cálculo para fortalecer la comprensión de los modelos de VLMs, que permite resolver tareas de manipulación multi-etapa. El enfoque central es un enfoque que utiliza la estructura \"reflexiva\" para mejorar recurrentemente los VLMs entrenados. Este enfoque utiliza modelos generativos que imaginan el futuro, guiando las decisiones de acción basadas en estas predicciones y mejorando críticamente cualquier potencial inadecuación. Los resultados de los experimentos muestran que nuestro enfoque supera significativamente a los VLMs comerciales avanzados y aproximaciones post-entrenamiento como la búsqueda de árboles de Monte Carlo (MCTS). Se puede ver el vídeo en https://reflect-vlm.github.io.",
      "upvotes": 5,
      "discussionId": "67bd3bcf797e4d53ce0bc7ff"
    },
    "publishedAt": "2025-02-25T01:02:05.395Z",
    "title": "Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16707.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f8cb8ed04a890f5380d9a4",
      "avatarUrl": "/avatars/d6fdfdbb0c10141aa3b4c832d928121b.svg",
      "fullname": "Jianlan Luo",
      "name": "jianlanluo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15987",
      "authors": [
        {
          "_id": "67bd46ea3e090b402d70f1f4",
          "user": {
            "_id": "64dfbcb18e2084e1d7b51b46",
            "avatarUrl": "/avatars/fafe30beea2d7e8eec3f3ba985c582f7.svg",
            "isPro": false,
            "fullname": "Kushal Raj Bhandari",
            "user": "KBhandari11",
            "type": "user"
          },
          "name": "Kushal Raj Bhandari",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-25T04:30:32.676Z",
          "hidden": false
        },
        {
          "_id": "67bd46ea3e090b402d70f1f5",
          "name": "Pin-Yu Chen",
          "hidden": false
        },
        {
          "_id": "67bd46ea3e090b402d70f1f6",
          "name": "Jianxi Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T22:52:19.000Z",
      "title": "Predicción del crecimiento de modelos de IA abierto de Hugging Face",
      "summary": "El desarrollo de la capacidad de peso abierto en el mapa de escala de IA, junto con el crecimiento de la investigación de modelos, la inversión masiva y el aumento de interés de los usuarios, ha llevado a que sea crucial que los modelos lideren la innovación y forman la ecosistema de IA. Se propone un marco para cuantificar cómo evoluciona el impacto de los modelos de peso abierto, basándose en la tendencia de citaciones y la similitud de la literatura científica. En particular, se utilizan tres factores: immediacy, longevity y relative fitness, aplicados a un modelo de citación científica introducido por Wang et al. para seguir la acumulación de modelos de ajuste micro de los modelos de peso abierto. Nuestros resultados muestran que el enfoque de citaciones es efectivo para comprender las diferentes rutas de adopción de los modelos de peso abierto, que la mayoría de los modelos están adaptados a estas rutas y que los auctores muestran patrones característicos o un rápido aumento de uso.",
      "upvotes": 4,
      "discussionId": "67bd46ee3e090b402d70f317"
    },
    "publishedAt": "2025-02-24T23:30:36.556Z",
    "title": "Forecasting Open-Weight AI Model Growth on Hugging Face",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e67bdd61009063689407479/kQHArNjaT0CM1KCujtDc1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15987.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "5e67bdd61009063689407479",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857146757-5e67bdd61009063689407479.jpeg",
      "fullname": "Clem 🤗",
      "name": "clem",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isMod": false,
      "followerCount": 2052
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16701",
      "authors": [
        {
          "_id": "67bd31d6bf6d46017e515a58",
          "user": {
            "_id": "62543749b777cd32720675c2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658760912583-62543749b777cd32720675c2.jpeg",
            "isPro": false,
            "fullname": "Irene Solaiman",
            "user": "irenesolaiman",
            "type": "user"
          },
          "name": "Irene Solaiman",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-25T03:43:21.348Z",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a59",
          "name": "Rishi Bommasani",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5a",
          "name": "Dan Hendrycks",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5b",
          "name": "Ariel Herbert-Voss",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5c",
          "name": "Yacine Jernite",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5d",
          "name": "Aviya Skowron",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5e",
          "name": "Andrew Trask",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T20:06:12.000Z",
      "title": "Más allá de la Liberación: Revisión del Acceso a Sistemas de IA de Generación",
      "summary": "La decisión de lanzamiento de un AI generativo se basa en el determinar si los componentes del sistema están disponibles para su uso, pero el lanzamiento no puede procesar las variaciones de múltiples elementos del sistema que cambian cómo el usuario colabora con el sistema. En lugar de eso, la accesibilidad a los componentes del sistema proporciona información sobre los riesgos y beneficios potenciales. La accesibilidad incluye los requisitos prácticos, infraestructura, técnicas y condiciones sociales necesarios para el uso de los componentes. La accesibilidad se divide en tres ejes: recursos, disponibilidad técnica y utilidad. En cada categoría, se utilizan continuas de configuración para explicar la pérdida. Por ejemplo, los recursos requieren acceso a la infraestructura de cálculo para almacenar los pesos del modelo en un servidor. Además, se comparan las accesibilidades de cuatro modelos de lenguaje de alta eficiencia, incluyendo dos modelos abiertos y dos modelos cerrados, y se aplican los mismos criterios a todos los modelos. Las variables de accesibilidad son la base para crear estructuras que pueden expandir o incrementar la accesibilidad para los usuarios, y se investigan cómo la escala de accesibilidad afecta la gestión de riesgos y la mitigación de riesgos. Este marco tiene como objetivo mejorar la comprensión de los riesgos y beneficios del lanzamiento del sistema, y proporciona información para la decisión de lanzamiento del sistema, la investigación y las políticas.",
      "upvotes": 4,
      "discussionId": "67bd31d7bf6d46017e515a7e"
    },
    "publishedAt": "2025-02-24T21:59:15.571Z",
    "title": "Beyond Release: Access Considerations for Generative AI Systems",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62543749b777cd32720675c2/LwZmJUoXiJriC_c1DZ7qM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16701.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62543749b777cd32720675c2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658760912583-62543749b777cd32720675c2.jpeg",
      "fullname": "Irene Solaiman",
      "name": "irenesolaiman",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 79
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17258",
      "authors": [
        {
          "_id": "67bd515c0417e7f92283d3b8",
          "name": "Xiangpeng Yang",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3b9",
          "name": "Linchao Zhu",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3ba",
          "name": "Hehe Fan",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3bb",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T15:39:14.000Z",
      "title": "VideoGrain: Regulación del atención en el tiempo y el espacio para edición de video multigranular",
      "summary": "El desarrollo reciente de modelos Difeasion ha notablemente mejorado la capacidad de generación y edición de películas. Sin embargo, la edición de películas en múltiples niveles (clase, instancia, partida) representa un desafío complejo. Los principales desafíos de la edición multi-granularidad radican en el ajuste significativo de la control de las regiones y la combinación de características dentro del modelo Difeasion. Para abordar estos desafíos, proponemos VideoGrain. VideoGrain es un enfoque de 0-shot que permite un control preciso en el contenido de las películas mediante la regulación de las instituciones de atención espacial-temporal (cruzada y auto). Nos esforzamos por fortalecer el control de las regiones y minimizar la interacción entre regiones irrelevantes en la atención cruzada. Además, reducimos la interferencia entre regiones en la atención auto y fortalecemos la reconocimiento dentro de las regiones. Extensos experimentos demuestran que nuestro método logra los mejores resultados en escenarios reales. Nuestro código, datos y demo están disponibles en https://knightyxp.github.io/VideoGrain_project_page/.",
      "upvotes": 3,
      "discussionId": "67bd51620417e7f92283d4e9"
    },
    "publishedAt": "2025-02-25T00:13:12.214Z",
    "title": "VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6205
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14132",
      "authors": [
        {
          "_id": "67b86819d00e69f10c1f31b9",
          "user": {
            "_id": "6231d3ce86753f5f41d39c6f",
            "avatarUrl": "/avatars/9b18f368e5f80cfc935b2e339d42a85f.svg",
            "isPro": false,
            "fullname": "Nadav Borenstein",
            "user": "Nadav",
            "type": "user"
          },
          "name": "Nadav Borenstein",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:52.278Z",
          "hidden": false
        },
        {
          "_id": "67b86819d00e69f10c1f31ba",
          "user": {
            "_id": "6698cffdb2ebada9f4a7e7d7",
            "avatarUrl": "/avatars/e66d946c14595d3b008185f2be8d2f57.svg",
            "isPro": false,
            "fullname": "Greta Warren",
            "user": "gretawarren",
            "type": "user"
          },
          "name": "Greta Warren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T14:42:45.791Z",
          "hidden": false
        },
        {
          "_id": "67b86819d00e69f10c1f31bb",
          "name": "Desmond Elliott",
          "hidden": false
        },
        {
          "_id": "67b86819d00e69f10c1f31bc",
          "name": "Isabelle Augenstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T22:26:39.000Z",
      "title": "¿Puede la comunidad reemplazar a los verificadores de hechos profesionales?",
      "summary": "En las redes sociales, dos estrategias comúnmente utilizadas para prevenir la propagación de rumores son: (i) la verificación de hechos realizada por organizaciones profesionales y (ii) la moderación comunitaria realizada por los usuarios del platformo. Las cambios en las políticas de Twitter/X y Meta han evolucionado hacia una mayor dependencia de la colaboración con grupos de verificación de hechos, a través de asociaciones con ellos. Sin embargo, la naturaleza y grado de esta dependencia entre verificación de hechos y moderación comunitaria no son claros. Para abordar este problema, se registró un gran corpus de Twitter/X utilizando modelos de lenguaje para clasificar propiedades como la negación de afirmaciones relacionadas con amplias explicaciones de rumores, basadas en temas, fuentes citadas y notas que se refieren a amplias explicaciones de rumores. Los resultados de la analisis muestran que las notas comunitarias citan fuentes de verificación de hechos en 5 veces más frecuencia que en los reportes anteriores, y que las notas de verificación de hechos son especialmente importantes en los posts que contienen amplias explicaciones, referenciando fuentes de verificación de hechos con dos veces más probabilidad que otras fuentes. En conclusión, nuestros resultados demuestran que una moderación comunitaria exitosa depende fuertemente de la verificación de hechos profesionales.",
      "upvotes": 2,
      "discussionId": "67b8681bd00e69f10c1f3267"
    },
    "publishedAt": "2025-02-25T04:11:18.915Z",
    "title": "Can Community Notes Replace Professional Fact-Checkers?",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6231d3ce86753f5f41d39c6f/CwWaf1c9-jOzJ-gD5lvCH.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/6231d3ce86753f5f41d39c6f/WrrBClUkuDsXHcfxP_N8B.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14132.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6231d3ce86753f5f41d39c6f",
      "avatarUrl": "/avatars/9b18f368e5f80cfc935b2e339d42a85f.svg",
      "fullname": "Nadav Borenstein",
      "name": "Nadav",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15122",
      "authors": [
        {
          "_id": "67bbd6d5ba0bb31293e11210",
          "user": {
            "_id": "675f68e3074ff89c5c078bf3",
            "avatarUrl": "/avatars/e3b78d90f032659d411761f47c3cf43e.svg",
            "isPro": false,
            "fullname": "Angus",
            "user": "angus924",
            "type": "user"
          },
          "name": "Angus Dempster",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-24T02:18:57.914Z",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11211",
          "name": "Navid Mohammadi Foumani",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11212",
          "name": "Chang Wei Tan",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11213",
          "name": "Lynn Miller",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11214",
          "name": "Amish Mishra",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11215",
          "name": "Mahsa Salehi",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11216",
          "name": "Charlotte Pelletier",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11217",
          "name": "Daniel F. Schmidt",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11218",
          "name": "Geoffrey I. Webb",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T00:54:40.000Z",
      "title": "Monstruo: Monstra escalable del tiempo serie evaluación repositorio",
      "summary": "Monster (MONSTER) - Repositorio de evaluación de la serie de tiempo de la escuela de Monash Escalable Time Series Series - Presenta la recopilación de un gran conjunto de datos en el campo de la clasificación de series de tiempo. Este campo recibe retroalimentación según los comunes marcos de referencia establecidos por UCR y UEA. Sin embargo, los conjuntos de datos incluidos en estos marcos de referencia son pequeños, con valores medios de 217 y 255. Esto tiende a reducir el espacio de modelos que pueden lograr bajos errores de clasificación en una amplia variedad de pequeños conjuntos de datos. Esta tendencia se atribuye a la priorización de modelos que minimizan la variación y que no se enfoquen suficientemente en aspectos como la escalabilidad y problemas de cálculo. Nuestra esperanza es construir estos marcos de referencia con grandes conjuntos de datos para diversificar el campo. Participamos en los desafíos teóricos y prácticos de entrenar modelos con grandes conjuntos de datos, creyendo en el gran potencial de nuevos desarrollos para el campo.",
      "upvotes": 2,
      "discussionId": "67bbd6d6ba0bb31293e11258"
    },
    "publishedAt": "2025-02-25T00:37:53.138Z",
    "title": "MONSTER: Monash Scalable Time Series Evaluation Repository",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15122.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "675f68e3074ff89c5c078bf3",
      "avatarUrl": "/avatars/e3b78d90f032659d411761f47c3cf43e.svg",
      "fullname": "Angus",
      "name": "angus924",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17414",
      "authors": [
        {
          "_id": "67bd526001d5bfa0abfcc5ba",
          "name": "Zeyuan Chen",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bb",
          "name": "Hongyi Xu",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bc",
          "name": "Guoxian Song",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bd",
          "name": "You Xie",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5be",
          "name": "Chenxu Zhang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bf",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c0",
          "name": "Chao Wang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c1",
          "name": "Di Chang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c2",
          "name": "Linjie Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:47:54.000Z",
      "title": "X-Dancer: Generación de vídeos de música de expresión y baile humano",
      "summary": "X-Dancer es una nueva tecnología de Pipeline de Animación de Imágenes de Danza con Motivos Musicales que genera videos de baile de personas humanas vivos de larga duración a partir de una sola imagen fija. Su esencia radica en un único canal de dispersión integrado, donde se aplica un modelo de recuperación automática para sintetizar una secuencia de términos de posición de cuerpo, cabeza y manos, extendidas y motivadas por música. Estas secuencias se utilizan a través de un modelo de dispersión para generar imágenes de baile realistas. A diferencia de los métodos existentes, X-Dancer no se centra en la generación de movimientos 3D de cuerpos humanos, sino en abordar las limitaciones de los datos, modelar un amplio rango de movimientos 2D de baile y ajustar con precisión el ritmo de la música en vídeos monocromáticos, mejorando la calidad de escalabilidad. Para lograr esto, se construye una representación estructurada espacial de términos que combina etiquetas de posición 2D de cuerpos humanos y la confianza de los puntos clave. Se diseña un modelo de canales que transforman la música en acciones, se genera automáticamente una secuencia de términos de posición de baile motivada por música, y se aplica una atención global que incluye el estilo de la música y el contexto de acciones previas. Finalmente, se agrega esta posición de términos sintetizada a la Imagen de Referencia para completar un marco completamente diferenciable final. Los resultados de las pruebas demuestran que X-Dancer puede generar videos de baile con diversas características, superando significativamente los métodos más avanzados en diversidad, expresividad y realismo. El código y los modelos están disponibles para la investigación.",
      "upvotes": 2,
      "discussionId": "67bd526101d5bfa0abfcc62c"
    },
    "publishedAt": "2025-02-25T00:17:51.431Z",
    "title": "X-Dancer: Expressive Music to Human Dance Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17414.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6205
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13074",
      "authors": [
        {
          "_id": "67bd8759fdecc637bd621e6b",
          "name": "Omer Angel",
          "hidden": false
        },
        {
          "_id": "67bd8759fdecc637bd621e6c",
          "name": "Emmanuel Jacob",
          "hidden": false
        },
        {
          "_id": "67bd8759fdecc637bd621e6d",
          "name": "Brett Kolesnik",
          "hidden": false
        },
        {
          "_id": "67bd8759fdecc637bd621e6e",
          "name": "Grégory Miermont",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T17:21:44.000Z",
      "title": "La serpiente en el cuerno de Brown",
      "summary": "La Broxñar espefia es un fenómeno que aparece como el límite de escala de varios tipos de mapas planos aleatorios en espacios bidimensionales y homogéneos. La estructura directa de la Broxñar espefia se realiza mediante una correspondencia directa ANALOGICA con la correspondencia Cori-Vauquelin-Schaeffer (CVS). Esta correspondencia CVS permite asociar árboles estandarizados a mapas planos y se utiliza para asociar los árboles aleatorios continuos en el tiempo (Broxñar espefia) con la Broxñar espefia. En este artículo, se explica cómo construir la Broxñar espefia como una función medida que representa el Broxñar espefia, para explicar la inversa de la correspondencia CVS continua. Es importante tener en cuenta la tratamiento de la dirección en la Broxñar espefia.",
      "upvotes": 0,
      "discussionId": "67bd875afdecc637bd621e95"
    },
    "publishedAt": "2025-02-25T04:03:39.758Z",
    "title": "The snake in the Brownian sphere",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13074.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636d12455aaed143cd665607",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679399015950-636d12455aaed143cd665607.png",
      "fullname": "ZLW",
      "name": "ZarkLngeW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15167",
      "authors": [
        {
          "_id": "67bc7ea06f88ef9a2b8283d3",
          "name": "Chuan Cui",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d4",
          "name": "Kejiang Chen",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d5",
          "name": "Zhihua Wei",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d6",
          "name": "Wen Shen",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d7",
          "name": "Weiming Zhang",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d8",
          "name": "Nenghai Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T03:05:45.000Z",
      "title": "M3-AGIQA: Evaluación de la calidad de imágenes generadas por AI con visión multimodal, multirutina y multiaspecto",
      "summary": "El rápido desarrollo de modelos de imágenes generadas por IA (AGI) está generando importantes problemas en la evaluación de su calidad. Para resolver estos problemas, proponemos M3-AGIQA, un marco de evaluación que realiza evaluaciones multimodales, multiturnadas y multiaspecto. Nuestro enfoque utiliza las capacidades de modelos grandes de lenguaje multimodal (MLLMs), funcionando con un encoder de texto y de imagen, y integra la alta capacidad de captura de los MLLMs en modelos locales a través de Low-Rank Adaptation (LoRA). Este marco incluye una estructura de evaluación estructurada, generando descripciones de imágenes intermedias y ofreciendo perspectivas profundas en calidad, correspondencia y accesibilidad. Para predecir juicios visuales humanos, utilizamos un predictor compuesto por xLSTM y un cabeza de regresión, que procesan la lógica de secuencia y predicen puntuaciones de Opinión Media (MOS). Este marco se ha expandido en varios conjuntos de datos de prueba y ha demostrado un fuerte rendimiento de generalización en pruebas de validación cruzada. El código está disponible en https://github.com/strawhatboy/M3-AGIQA.",
      "upvotes": 0,
      "discussionId": "67bc7ea26f88ef9a2b828473"
    },
    "publishedAt": "2025-02-25T03:36:50.480Z",
    "title": "M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 771
    },
    "isAuthorParticipating": false
  }
]