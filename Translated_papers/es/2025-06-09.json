[
  {
    "paper": {
      "id": "2505.21115",
      "authors": [
        {
          "_id": "68372d97e4af3c39dcec8e65",
          "user": {
            "_id": "5dfa8e07da6d0311fd3d5430",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651090418656-5dfa8e07da6d0311fd3d5430.png",
            "isPro": false,
            "fullname": "Sergey Pletenev",
            "user": "memyprokotow",
            "type": "user"
          },
          "name": "Sergey Pletenev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:55.604Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e66",
          "user": {
            "_id": "660ee18e2dcd816ad14b3739",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
            "isPro": false,
            "fullname": "Maria Marina",
            "user": "zlatamaria",
            "type": "user"
          },
          "name": "Maria Marina",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:59.278Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e67",
          "user": {
            "_id": "6682607ece294ddc5e72f4fb",
            "avatarUrl": "/avatars/2a304bc3eb56ec7d13297d28fbb062ae.svg",
            "isPro": false,
            "fullname": "Ivanov",
            "user": "VirVen",
            "type": "user"
          },
          "name": "Nikolay Ivanov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:48:42.811Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e68",
          "name": "Daria Galimzianova",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e69",
          "user": {
            "_id": "643010b2ff56d6c2004699a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/OYsf1Hp-KAievw_M8XBG9.jpeg",
            "isPro": false,
            "fullname": "Krayko Nikita",
            "user": "nakrayko",
            "type": "user"
          },
          "name": "Nikita Krayko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:53.523Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6a",
          "name": "Mikhail Salnikov",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6b",
          "name": "Vasily Konovalov",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6c",
          "name": "Alexander Panchenko",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6d",
          "user": {
            "_id": "63bbfd74141c7d395c471768",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
            "isPro": false,
            "fullname": "Viktor Moskvoretskii",
            "user": "VityaVitalich",
            "type": "user"
          },
          "name": "Viktor Moskvoretskii",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:57.325Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T12:35:13.000Z",
      "submittedOnDailyAt": "2025-06-09T07:27:12.232Z",
      "title": "¿También será lo mismo el día de mañana? Problema de la ópera multilingüe\nClasificación para mejorar la confianza en la respuesta de pregunta\n\nEsta traducción mantiene la profesionalidad y la precisión.",
      "submittedOnDailyBy": {
        "_id": "660ee18e2dcd816ad14b3739",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
        "isPro": false,
        "fullname": "Maria Marina",
        "user": "zlatamaria",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) a menudo realizan 'hallucinaciones' (es decir, el contenido de la salida es diferente al hecho o información). Este fenómeno no solo es un elemento importante, sino también una de las causas por la cual aún no se ha realizado suficiente investigación, debido a la característica temporal de identificar el problema. Esta característica se divide en dos tipos: los respuestas que no cambian con el tiempo, los 'evergreen', y las respuestas que cambian con el tiempo, los 'mutables'. En este estudio, se presenta EverGreenQA, el primer conjunto de preguntas y respuestas multilingües, que proporciona etiquetas de 'evergreen' para evaluar y entrenar. Usando EverGreenQA, se evalua a 12 modelos de LLMs modernos, y se examina si estos modelos expresan claramente la característica temporal de las preguntas (expresadas de manera explícita o oculta, según sea el criterio de juicio o señales de incertidumbre). Además, se entrena un ligero clasificador multilingüe denominado EG-E5, para alcanzar los mejores resultados en esta tarea. Finalmente, se muestran tres aplicaciones prácticas que demuestran la utilidad de la clasificación 'evergreen': mejorar la estimación de la autopercepción, filtrar conjuntos de datos de preguntas y respuestas, y explicar las acciones de búsqueda de GPT-4o.",
      "upvotes": 41,
      "discussionId": "68372d98e4af3c39dcec8e88",
      "githubRepo": "https://github.com/s-nlp/Evergreen-classification",
      "ai_summary": "EverGreenQA, a multilingual QA dataset with evergreen labels, is introduced to benchmark LLMs on temporality encoding and assess their performance through verbalized judgments and uncertainty signals.",
      "ai_keywords": [
        "Large Language Models",
        "QA",
        "evergreen",
        "mutable",
        "temporality",
        "Multilingual QA dataset",
        "EG-E5",
        "lightweight multilingual classifier",
        "SoTA performance",
        "self-knowledge estimation",
        "filtering QA datasets",
        "GPT-4o retrieval behavior"
      ]
    },
    "publishedAt": "2025-05-27T08:35:13.000Z",
    "title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question\n  Classification to Improve Trustworthy QA",
    "summary": "Large Language Models (LLMs) often hallucinate in question answering (QA)\ntasks. A key yet underexplored factor contributing to this is the temporality\nof questions -- whether they are evergreen (answers remain stable over time) or\nmutable (answers change). In this work, we introduce EverGreenQA, the first\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\nencode question temporality explicitly (via verbalized judgments) or implicitly\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\nthe practical utility of evergreen classification across three applications:\nimproving self-knowledge estimation, filtering QA datasets, and explaining\nGPT-4o retrieval behavior.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21115.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "660ee18e2dcd816ad14b3739",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
      "fullname": "Maria Marina",
      "name": "zlatamaria",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01111",
      "authors": [
        {
          "_id": "6845b6a33ec10bdd8ab4da1b",
          "name": "Shunian Chen",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1c",
          "user": {
            "_id": "66440e86bfe15e84d369cb03",
            "avatarUrl": "/avatars/d15b3b3831bc74138206071612169f64.svg",
            "isPro": false,
            "fullname": "Xinyuan Xie",
            "user": "SatsukiVie",
            "type": "user"
          },
          "name": "Xinyuan Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:42.759Z",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1d",
          "name": "Zheshu Chen",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1e",
          "name": "Liyan Zhao",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1f",
          "name": "Owen Lee",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da20",
          "name": "Zhan Su",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da21",
          "name": "Qilin Sun",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da22",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T18:29:17.000Z",
      "submittedOnDailyAt": "2025-06-09T01:59:54.914Z",
      "title": "FusionAudio-1.2M: Fusión de contexto multimodal para captura de voz con detalles finos",
      "submittedOnDailyBy": {
        "_id": "623be9e1d1eb227788764959",
        "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
        "isPro": false,
        "fullname": "Shunian Chen",
        "user": "Shunian",
        "type": "user"
      },
      "summary": "La captura de alta calidad de grandes sonidos es crucial para el desarrollo de la comprensión de voz, pero los métodos actuales de automatización generan muchas capturas que descuidan detalles minuciosos y la precisión del contexto. Esto se debe a la relación con modos únicos limitados o información multimodal superficial. Inspirados en la percepción auditiva humana y integrando diferentes cursos adecuadamente, se introduce una nueva pipila en dos etapas para lograr un alto nivel de reconocimiento auditivo complejo. Esta pipila utiliza modelos especialmente entrenados para extraer diferentes cursos de contexto (por ejemplo, voz de conversación, música, sonidos generales, información visual proveniente de vídeos relacionados). Luego, los grandes modelos de lenguaje (LLM) combinan estas entradas multimodales ricas para generar capturas de voz detalladas y contextuales. Las principales contribuciones de este estudio son: (1) un nuevo método de generación de capturas de voz detalladas y escalables, (2) la adición de funciones, un nuevo conjunto de datos grande que incluye 1.2 millones de estas capturas detalladas, combinado con 6 millones de pares de preguntas y respuestas (QA), y (3) la mejora del modelo de voz, especialmente la mejora de la correspondencia sonido-documento y la capacidad de seguimiento de instrucciones en el encoder de voz basado en CLAP. Este artículo presenta un juego de películas para la comprensión automática precisa en entornos complejos de voz. El código y los datos pueden encontrarse en https://github.com/satsuki2486441738/FusionAudio.",
      "upvotes": 21,
      "discussionId": "6845b6a43ec10bdd8ab4da23",
      "githubRepo": "https://github.com/FreedomIntelligence/FusionAudio",
      "ai_summary": "A novel two-stage pipeline using specialized pretrained models and a large language model enhances audio caption quality by integrating diverse multimodal cues and contextual information.",
      "ai_keywords": [
        "audio captioning",
        "auditory perception",
        "auditory scene analysis",
        "pretrained models",
        "large language model",
        "FusionAudio",
        "CLAP-based audio encoder",
        "audio-text alignment",
        "instruction following"
      ]
    },
    "publishedAt": "2025-06-01T14:29:17.000Z",
    "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion",
    "summary": "High-quality, large-scale audio captioning is crucial for advancing audio\nunderstanding, yet current automated methods often generate captions that lack\nfine-grained detail and contextual accuracy, primarily due to their reliance on\nlimited unimodal or superficial multimodal information. Drawing inspiration\nfrom human auditory perception, which adeptly integrates cross-modal cues and\nperforms sophisticated auditory scene analysis, we introduce a novel two-stage\nautomated pipeline. This pipeline first employs specialized pretrained models\nto extract diverse contextual cues (e.g., speech, music, general sounds, and\nvisual information from associated video). A large language model (LLM) then\nsynthesizes these rich, multimodal inputs to generate detailed and\ncontext-aware audio captions. Key contributions of this work include: (1) the\nproposed scalable method for fine-grained audio caption generation; (2)\nFusionAudio, a new large-scale dataset comprising 1.2 million such detailed\ncaptions, combined with 6 million QA pairs; and (3) enhanced audio models\ndeveloped using FusionAudio, specifically a CLAP-based audio encoder with\nsuperior audio-text alignment and instruction following. This paper paves the\nway for more nuanced and accurate automated understanding of complex audio\nenvironments. Code and data can be found in\nhttps://github.com/satsuki2486441738/FusionAudio.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01111.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623be9e1d1eb227788764959",
      "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
      "fullname": "Shunian Chen",
      "name": "Shunian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05984",
      "authors": [
        {
          "_id": "68463ee43ec10bdd8ab4da6f",
          "user": {
            "_id": "622326ae0129f2097d69a3e2",
            "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
            "isPro": false,
            "fullname": "Cheng-Han Chiang",
            "user": "dcml0714",
            "type": "user"
          },
          "name": "Cheng-Han Chiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:26.041Z",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da70",
          "user": {
            "_id": "64dc191bc307ee5369fbcb04",
            "avatarUrl": "/avatars/5a8a0db63a187e85d4ae2fff93a838f0.svg",
            "isPro": false,
            "fullname": "Xiaofei Wang",
            "user": "xiaofei-wang",
            "type": "user"
          },
          "name": "Xiaofei Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T01:54:46.319Z",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da71",
          "name": "Chung-Ching Lin",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da72",
          "name": "Kevin Lin",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da73",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da74",
          "name": "Radu Kopetz",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da75",
          "name": "Yao Qian",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da76",
          "name": "Zhendong Wang",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da77",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da78",
          "name": "Hung-yi Lee",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da79",
          "name": "Lijuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T11:05:48.000Z",
      "submittedOnDailyAt": "2025-06-09T00:28:11.753Z",
      "title": "Los grandes modelos de lenguaje relacionados con el lenguaje de voz desempeñan un papel decisivo en la forma de la conversación.",
      "submittedOnDailyBy": {
        "_id": "622326ae0129f2097d69a3e2",
        "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
        "isPro": false,
        "fullname": "Cheng-Han Chiang",
        "user": "dcml0714",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grandes de voz (ALLMs) relacionados con el lenguaje son capaces de comprender tanto la información textual como la no textual de los datos de voz. En este artículo, se investiga cómo utilizar los ALLMs como jurados automáticos para evaluar la expresión de discursos. Se utilizan jurados de ALLMs para evaluar los discursos generados por los modelos de lenguaje grandes (SLMs). Esta evaluación utiliza dos tareas: la ejecución del estilo de voz y la interpretación de papel. Las expresiones que se evalúan incluyen emociones, volumen, velocidad de habla, peso de las palabras, control de la melodía, elementos no lingüísticos y más. Se utilizan cuatro modelos de lenguaje (SLMs) para realizar estas dos tareas y se comparan los juicios de los humanos y los ALLMs sobre las respuestas de los SLMs. Se compara la tasa de concordancia entre los jurados de Gemini y los humanos con la tasa de concordancia entre los jurados humanos, y se observa que la tasa de concordancia entre Gemini y los humanos es relativamente alta. Estos resultados esperados demuestran que los ALLMs pueden ser utilizados como jurados para evaluar los SLMs. Además, se observa claramente que los SLMs actuales, especialmente GPT-4o-audio, tienen ventajas en el control de la expresión y la generación de diálogos naturales.",
      "upvotes": 9,
      "discussionId": "68463ee43ec10bdd8ab4da7a",
      "ai_summary": "Audio-aware large language models can assess speaking styles in audio inputs, demonstrating performance comparable to human judges in evaluating synthesized speech along dimensions like emotion, volume, and pitch.",
      "ai_keywords": [
        "audio-aware large language models",
        "ALLMs",
        "speaking styles",
        "SLMs",
        "voice style instruction",
        "role-playing",
        "emotion",
        "volume",
        "speaking pace",
        "word emphasis",
        "pitch control",
        "non-verbal elements",
        "GPT-4o-audio",
        "Gemini-2.5-pro",
        "human evaluation",
        "agreement",
        "speaking style control",
        "natural dialogues"
      ]
    },
    "publishedAt": "2025-06-06T07:05:48.000Z",
    "title": "Audio-Aware Large Language Models as Judges for Speaking Styles",
    "summary": "Audio-aware large language models (ALLMs) can understand the textual and\nnon-textual information in the audio input. In this paper, we explore using\nALLMs as an automatic judge to assess the speaking styles of speeches. We use\nALLM judges to evaluate the speeches generated by SLMs on two tasks: voice\nstyle instruction following and role-playing. The speaking style we consider\nincludes emotion, volume, speaking pace, word emphasis, pitch control, and\nnon-verbal elements. We use four spoken language models (SLMs) to complete the\ntwo tasks and use humans and ALLMs to judge the SLMs' responses. We compare two\nALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and\nshow that the agreement between Gemini and human judges is comparable to the\nagreement between human evaluators. These promising results show that ALLMs can\nbe used as a judge to evaluate SLMs. Our results also reveal that current SLMs,\neven GPT-4o-audio, still have room for improvement in controlling the speaking\nstyle and generating natural dialogues.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622326ae0129f2097d69a3e2",
      "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
      "fullname": "Cheng-Han Chiang",
      "name": "dcml0714",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05629",
      "authors": [
        {
          "_id": "68464b273ec10bdd8ab4da86",
          "user": {
            "_id": "64a6518132cf858d6386ac52",
            "avatarUrl": "/avatars/4cabf3dab8b1ba06245ad8024f334181.svg",
            "isPro": false,
            "fullname": "Ananth Muppidi",
            "user": "ananthmuppidi",
            "type": "user"
          },
          "name": "Ananth Muppidi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T02:47:03.971Z",
          "hidden": false
        },
        {
          "_id": "68464b273ec10bdd8ab4da87",
          "user": {
            "_id": "5f89da6c5d083370c711f37c",
            "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
            "isPro": false,
            "fullname": "Abhilash Nandy",
            "user": "abhi1nandy2",
            "type": "user"
          },
          "name": "Abhilash Nandy",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-09T03:49:29.446Z",
          "hidden": false
        },
        {
          "_id": "68464b273ec10bdd8ab4da88",
          "user": {
            "_id": "65238ea295df08170c93933d",
            "avatarUrl": "/avatars/8364301e324274a550d12f2b184ea10e.svg",
            "isPro": false,
            "fullname": "Sambaran Bandyopadhyay",
            "user": "sambaran",
            "type": "user"
          },
          "name": "Sambaran Bandyopadhyay",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T02:47:03.971Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T23:13:22.000Z",
      "submittedOnDailyAt": "2025-06-09T01:27:41.831Z",
      "title": "Plan de Promoción para la Ecologización del Impacto Ambiental",
      "submittedOnDailyBy": {
        "_id": "5f89da6c5d083370c711f37c",
        "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
        "isPro": false,
        "fullname": "Abhilash Nandy",
        "user": "abhi1nandy2",
        "type": "user"
      },
      "summary": "En ciertos campos de tareas, el ajuste de modelos de lenguaje de gran escala es costoso computacionalmente y técnicamente difícil. Este artículo enfoca en un ajuste eficiente de parámetros para adaptar modelos pretrenados a tareas de bajo nivel, utilizando un soft pruning. Proponemos un nuevo método de soft pruning basado en tokens de entrada (ID-SPAM) para procesar tokens de diferentes importancia de manera diferente. Nuestro método reduce el número de parámetros de entrenamiento, es sencillo y eficiente. Mostramos los beneficios comparativos con la tecnología más reciente y demostramos una mejor capacidad de movimiento en el dominio 0 shot.",
      "upvotes": 9,
      "discussionId": "68464b273ec10bdd8ab4da89",
      "ai_summary": "A new method using input-dependent soft prompting with a self-attention mechanism improves parameter-efficient fine-tuning for large language models, enhancing zero-shot domain transfer.",
      "ai_keywords": [
        "soft prompting",
        "parameter-efficient fine-tuning",
        "pre-trained models",
        "downstream tasks",
        "Input Dependent Soft Prompting technique",
        "self-Attention Mechanism",
        "zero shot domain transfer"
      ]
    },
    "publishedAt": "2025-06-05T19:13:22.000Z",
    "title": "Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs",
    "summary": "The performance of large language models in domain-specific tasks\nnecessitates fine-tuning, which is computationally expensive and technically\nchallenging. This paper focuses on parameter-efficient fine-tuning using soft\nprompting, a promising approach that adapts pre-trained models to downstream\ntasks by learning a small set of parameters. We propose a novel Input Dependent\nSoft Prompting technique with a self-Attention Mechanism (ID-SPAM) that\ngenerates soft prompts based on the input tokens and attends different tokens\nwith varying importance. Our method is simple and efficient, keeping the number\nof trainable parameters small. We show the merits of the proposed approach\ncompared to state-of-the-art techniques on various tasks and show the improved\nzero shot domain transfer capability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05629.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f89da6c5d083370c711f37c",
      "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
      "fullname": "Abhilash Nandy",
      "name": "abhi1nandy2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01872",
      "authors": [
        {
          "_id": "683e77d41417d107337abf6e",
          "user": {
            "_id": "643f9e2288d9d4488fd81c52",
            "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
            "isPro": false,
            "fullname": "Tinghui Zhu",
            "user": "DarthZhu",
            "type": "user"
          },
          "name": "Tinghui Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:47.572Z",
          "hidden": false
        },
        {
          "_id": "683e77d41417d107337abf6f",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "683e77d41417d107337abf70",
          "name": "Muhao Chen",
          "hidden": false
        },
        {
          "_id": "683e77d41417d107337abf71",
          "name": "Yu Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:01:40.000Z",
      "submittedOnDailyAt": "2025-06-09T07:51:42.399Z",
      "title": "¿El diseño de la extensión de modelo se ajusta a todos los diseños de modelo correctamente?",
      "submittedOnDailyBy": {
        "_id": "643f9e2288d9d4488fd81c52",
        "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
        "isPro": false,
        "fullname": "Tinghui Zhu",
        "user": "DarthZhu",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje multimodal (OLMs) tienen como objetivo proporcionar explicaciones para diferentes modalidades de entrada, como texto, imágenes, vídeos y voz, mientras mantienen una fuerte capacidad lingüística. A pesar de los recientes avances, los modelos actuales, especialmente los abiertos, aún están lejos de lograr una verdadera multimodalidad. En particular, se enfrentan desafíos al generalizar sobre pares de modalidades que no fueron entrenados y al procesar entradas multimodales con alto rendimiento. En este trabajo, se investiga uno de los principales métodos de entrenamiento de modelos: la extensión de modalidades. Específicamente, se examina si la extensión de modalidades complementa las capacidades lingüísticas esenciales, si se puede integrar efectivamente modelos de modalidades entrenados de manera independiente, y si este efecto se logra mejor con la extensión secuencial en comparación con la mejora de la compartibilidad y generalización. A través de experimentos detallados, se analizan estos trade-offs y se proporcionan feedbacks sobre la posibilidad de lograr una verdadera multimodalidad en los métodos actuales de entrenamiento.",
      "upvotes": 9,
      "discussionId": "683e77d41417d107337abf8f",
      "projectPage": "https://darthzhu.github.io/lm-extend-page/",
      "githubRepo": "https://github.com/DarthZhu/lm-extend",
      "ai_summary": "Research investigates the impact of extending modality and model merging on maintaining language abilities and generalization in omni-modal language models.",
      "ai_keywords": [
        "omni-modal language models",
        "modality extension",
        "fine-tuning",
        "language abilities",
        "model merging",
        "generalization",
        "true omni-modality"
      ]
    },
    "publishedAt": "2025-06-02T13:01:40.000Z",
    "title": "Is Extending Modality The Right Path Towards Omni-Modality?",
    "summary": "Omni-modal language models (OLMs) aim to integrate and reason over diverse\ninput modalities--such as text, images, video, and audio--while maintaining\nstrong language capabilities. Despite recent advancements, existing models,\nespecially open-source ones, remain far from true omni-modality, struggling to\ngeneralize beyond the specific modality pairs they are trained on or to achieve\nstrong performance when processing multi-modal inputs. We study the effect of\nextending modality, the dominant technique for training multimodal models,\nwhere an off-the-shelf language model is fine-tuned on target-domain and\nlanguage data. Specifically, we investigate three key questions: (1) Does\nmodality extension compromise core language abilities? (2) Can model merging\neffectively integrate independently fine-tuned modality-specific models to\nachieve omni-modality? (3) Does omni-modality extension lead to better\nknowledge sharing and generalization compared to sequential extension? Through\nextensive experiments, we analyze these trade-offs and provide insights into\nthe feasibility of achieving true omni-modality using current approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643f9e2288d9d4488fd81c52",
      "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
      "fullname": "Tinghui Zhu",
      "name": "DarthZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06276",
      "authors": [
        {
          "_id": "68466dfb3ec10bdd8ab4dae2",
          "name": "Jiatao Gu",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae3",
          "name": "Tianrong Chen",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae4",
          "name": "David Berthelot",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae5",
          "name": "Huangjie Zheng",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae6",
          "name": "Yuyang Wang",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae7",
          "name": "Ruixiang Zhang",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae8",
          "name": "Laurent Dinh",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae9",
          "name": "Miguel Angel Bautista",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4daea",
          "name": "Josh Susskind",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4daeb",
          "name": "Shuangfei Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:58:39.000Z",
      "submittedOnDailyAt": "2025-06-09T03:58:52.022Z",
      "title": "STARFlow: Escalado de formas de normalización potencial para la síntesis de imágenes de alta resolución",
      "submittedOnDailyBy": {
        "_id": "6164e72d73996c363c52e66d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png",
        "isPro": false,
        "fullname": "Jiatao Gu",
        "user": "thomagram",
        "type": "user"
      },
      "summary": "STARFlow es un modelo generativo escalable. Este modelo se construye basándose en flujos de normalización y muestra un excelente rendimiento en la síntesis de imágenes de alta resolución. El núcleo de STARFlow es el Transformer Autoregressive Flow (TARFlow). TARFlow combina la capacidad de representación de los flujos de normalización y la estructurada modelación de los canales de monitoreo autónomo. Primero, se demuestra la generalidad teórica de TARFlow. Sobre esta base, se introducen las siguientes tres innovaciones arquitectónicas y algorítmicas esenciales para mejorar significativamente la escalabilidad: 1) un diseño profundo para capturar al máximo la representación del modelo con bloques de Transformer profundos, mientras que los bloques de Transformer superficiales más eficientes son los más ventajosos. 2) Se modela en el espacio potencial de un codificador entrenado, lo que es más efectivo que la modelación a nivel de píxel. 3) Se introduce un nuevo algoritmo de guiado para significativamente mejorar la calidad de las muestras. Es importante destacar que el modelo mantiene la coherencia desde principio hasta fin como flujo de normalización, permitiendo la entrenamiento de probabilidad máxima precisa en el espacio continuo. STARFlow muestra un excelente rendimiento en tareas de generación de imágenes condicionadas por clase y contexto, alcanzando una calidad de muestras similar a los mejores modelos difuë, y, según nuestra información, constituye el primer éxito significativo en demostrar que los flujos de normalización funcionan efectivamente en esta escala y resolución.",
      "upvotes": 5,
      "discussionId": "68466dfb3ec10bdd8ab4daec",
      "ai_summary": "STARFlow, a generative model combining normalizing flows with autoregressive Transformers, achieves competitive image synthesis performance with innovations in architecture and latent space modeling.",
      "ai_keywords": [
        "normalizing flows",
        "Transformer Autoregressive Flow",
        "TARFlow",
        "theoretical universality",
        "deep-shallow design",
        "pretrained autoencoders",
        "latent space",
        "guidance algorithm",
        "end-to-end normalizing flow",
        "exact maximum likelihood training",
        "class-conditional",
        "text-conditional image generation",
        "state-of-the-art diffusion models"
      ]
    },
    "publishedAt": "2025-06-06T13:58:39.000Z",
    "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\n  Synthesis",
    "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06276.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6164e72d73996c363c52e66d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png",
      "fullname": "Jiatao Gu",
      "name": "thomagram",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06253",
      "authors": [
        {
          "_id": "68469b773ec10bdd8ab4db88",
          "name": "Yuping He",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db89",
          "name": "Yifei Huang",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8a",
          "user": {
            "_id": "6392c73390b8e99a6779a7b0",
            "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
            "isPro": false,
            "fullname": "Guo Chen",
            "user": "cg1177",
            "type": "user"
          },
          "name": "Guo Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:02.757Z",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8b",
          "name": "Lidong Lu",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8c",
          "name": "Baoqi Pei",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8d",
          "name": "Jilan Xu",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8e",
          "name": "Tong Lu",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8f",
          "name": "Yoichi Sato",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:25:48.000Z",
      "submittedOnDailyAt": "2025-06-09T07:00:04.801Z",
      "title": "Biraldía Poertreid: Echoscóntribísión y Echoscóntribísión: Colóroración de la Sinecísha Kaintek Intelligencia Investigación",
      "submittedOnDailyBy": {
        "_id": "6392c73390b8e99a6779a7b0",
        "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
        "isPro": false,
        "fullname": "Guo Chen",
        "user": "cg1177",
        "type": "user"
      },
      "summary": "El mundo percibido desde dos puntos de vista virtuales (primero desde un centro virtual propio y, posteriormente, desde un segundo centro virtual) forma la base de la percepción humana y permite una comprensión rica y complementaria de entornos dinámicos. Recientemente, el desarrollo de funciones de complementación de estas dos perspectivas por parte de la inteligencia artificial ha emergido como un interesante campo de investigación en el entendimiento de imágenes. Este estudio explora en profundidad el entendimiento de imágenes desde estas dos perspectivas. Primero, se presentan ejemplos prácticos de aplicaciones que integran estos métodos de dos centros virtuales y se anticipan potenciales minerías de datos. Luego, se identifican los desafíos cruciales para lograr estas aplicaciones. A continuación, se agrupan los últimos avances en tres direcciones principales de investigación y se analizan los diversos trabajos relacionados con tareas específicas de cada una de estas direcciones. Además, se discuten conjuntamente los conjuntos de datos de referencia que apoyan la investigación de estas dos perspectivas y se evaluan su alcance, diversidad y posibilidad de aplicación. Finalmente, se discuten las limitaciones actuales de la investigación y se proponen futuras direcciones de estudio. La integración de estas dos perspectivas permite fomentar el entendimiento de imágenes y el desarrollo de la inteligencia artificial, con el objetivo de que la inteligencia artificial pueda ver el mundo como la humanidad lo hace. Para investigar estos temas, se recomienda el repositorio GitHub https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.",
      "upvotes": 5,
      "discussionId": "68469b773ec10bdd8ab4db90",
      "projectPage": "https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision",
      "githubRepo": "https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision",
      "ai_summary": "A survey on leveraging both egocentric and exocentric video understanding for enhancing complementary tasks with a focus on three research directions and benchmark datasets.",
      "ai_keywords": [
        "egocentric",
        "exocentric",
        "video understanding",
        "research tasks",
        "benchmark datasets"
      ]
    },
    "publishedAt": "2025-06-06T13:25:48.000Z",
    "title": "Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence\n  with Egocentric-Exocentric Vision",
    "summary": "Perceiving the world from both egocentric (first-person) and exocentric\n(third-person) perspectives is fundamental to human cognition, enabling rich\nand complementary understanding of dynamic environments. In recent years,\nallowing the machines to leverage the synergistic potential of these dual\nperspectives has emerged as a compelling research direction in video\nunderstanding. In this survey, we provide a comprehensive review of video\nunderstanding from both exocentric and egocentric viewpoints. We begin by\nhighlighting the practical applications of integrating egocentric and\nexocentric techniques, envisioning their potential collaboration across\ndomains. We then identify key research tasks to realize these applications.\nNext, we systematically organize and review recent advancements into three main\nresearch directions: (1) leveraging egocentric data to enhance exocentric\nunderstanding, (2) utilizing exocentric data to improve egocentric analysis,\nand (3) joint learning frameworks that unify both perspectives. For each\ndirection, we analyze a diverse set of tasks and relevant works. Additionally,\nwe discuss benchmark datasets that support research in both perspectives,\nevaluating their scope, diversity, and applicability. Finally, we discuss\nlimitations in current works and propose promising future research directions.\nBy synthesizing insights from both perspectives, our goal is to inspire\nadvancements in video understanding and artificial intelligence, bringing\nmachines closer to perceiving the world in a human-like manner. A GitHub repo\nof related works can be found at\nhttps://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6392c73390b8e99a6779a7b0",
      "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
      "fullname": "Guo Chen",
      "name": "cg1177",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05523",
      "authors": [
        {
          "_id": "6846657d3ec10bdd8ab4daca",
          "user": {
            "_id": "630bc5ae86b8b9904c33e94b",
            "avatarUrl": "/avatars/b176d9b1691c05cc941409dd6c2b2228.svg",
            "isPro": false,
            "fullname": "Zikui Cai",
            "user": "Zikui",
            "type": "user"
          },
          "name": "Zikui Cai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:17.551Z",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacb",
          "name": "Andrew Wang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacc",
          "name": "Anirudh Satheesh",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacd",
          "name": "Ankit Nakhawa",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dace",
          "name": "Hyunwoo Jae",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacf",
          "name": "Keenan Powell",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad0",
          "name": "Minghui Liu",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad1",
          "name": "Neel Jay",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad2",
          "name": "Sungbin Oh",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad3",
          "name": "Xiyao Wang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad4",
          "name": "Yongyuan Liang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad5",
          "name": "Tom Goldstein",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad6",
          "name": "Furong Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T19:12:45.000Z",
      "submittedOnDailyAt": "2025-06-09T03:10:23.755Z",
      "title": "MORSE-500: Marco de video controlable programaticamente que realiza pruebas de tipo para lógica dual-camino.",
      "submittedOnDailyBy": {
        "_id": "655fed9fdef5905d38b84af3",
        "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
        "isPro": false,
        "fullname": "Xiyao Wang",
        "user": "russwang",
        "type": "user"
      },
      "summary": "Aunque el crecimiento rápido de los modelos de video de vídeo (VLMs) continúa, los actuales marcadores de lógica multiformes carecen en tres aspectos importantes. Primero, dependen principalmente de imágenes estáticas y no comprenden la complejidad temporal del mundo real. Segundo, se centran en la resolución de problemas matemáticos, ignorando ampliamente el vasto campo de técnicas lógicas abstractas, físicas, planificatorias, espaciales y temporales. Tercero, muchos marcadores se desactivan rápidamente y están limitados en la diagnóstico de fallos y la evaluación de progreso. Para abordar estos problemas, se presenta MORSE-500 (entorno de prueba de estrés para lógica multiforme), que es un marcador de 500 vídeos que extiende las seis categorías de lógica intermedia. Cada instancia se genera con un script de Python determinado, utilizando MANIM, Matplotlib, MoviePy, modelos de vídeo generados y alimentos de vida editados. Este script permite controlar la complejidad visual, la densidad de detección y la dinámica temporal, y puede escalar la dificultad sistemáticamente según el progreso del modelo. A diferencia de los marcadores estáticos, MORSE-500 está diseñado para fomentar el desarrollo: la generación de nuevas instancias con una pipeline controlable permite testear a los modelos de siguientes generaciones. Los experimentos iniciales, que incluyeron a los modelos más potentes como GEMINI 2.5 Pro, OpenAI o3 y modelos abiertos de código fuertes, mostraron grandes diferencias de rendimiento en todas las categorías, con particulares pérdidas en tareas abstractas y planificatorias. Se publican el conjunto de datos, los scripts de generación y los herramientas de evaluación para apoyar una investigación multiforme lógica transparente y reproducible.",
      "upvotes": 5,
      "discussionId": "6846657d3ec10bdd8ab4dad7",
      "projectPage": "https://morse-500.github.io/",
      "githubRepo": "https://github.com/morse-benchmark/morse-500",
      "ai_summary": "MORSE-500, a video benchmark with 500 scripted clips, evaluates multimodal reasoning across six categories, highlighting performance gaps in abstract and planning tasks.",
      "ai_keywords": [
        "Vision-language models",
        "MORSE-500",
        "multimodal reasoning",
        "video benchmark",
        "scripted clips",
        "reasoning categories",
        "Manim",
        "Matplotlib",
        "MoviePy",
        "generative video models",
        "controllable generation",
        "spatial capabilities",
        "temporal capabilities",
        "abstract reasoning",
        "planning tasks"
      ]
    },
    "publishedAt": "2025-06-05T15:12:45.000Z",
    "title": "MORSE-500: A Programmatically Controllable Video Benchmark to\n  Stress-Test Multimodal Reasoning",
    "summary": "Despite rapid advances in vision-language models (VLMs), current benchmarks\nfor multimodal reasoning fall short in three key dimensions. First, they\noverwhelmingly rely on static images, failing to capture the temporal\ncomplexity of real-world environments. Second, they narrowly focus on\nmathematical problem-solving, neglecting the broader spectrum of reasoning\nskills -- including abstract, physical, planning, spatial, and temporal\ncapabilities -- required for robust multimodal intelligence. Third, many\nbenchmarks quickly saturate, offering limited headroom for diagnosing failure\nmodes or measuring continued progress. We introduce MORSE-500 (Multimodal\nReasoning Stress-test Environment), a video benchmark composed of 500 fully\nscripted clips with embedded questions spanning six complementary reasoning\ncategories. Each instance is programmatically generated using deterministic\nPython scripts (via Manim, Matplotlib, MoviePy), generative video models, and\ncurated real footage. This script-driven design allows fine-grained control\nover visual complexity, distractor density, and temporal dynamics -- enabling\ndifficulty to be scaled systematically as models improve. Unlike static\nbenchmarks that become obsolete once saturated, MORSE-500 is built to evolve:\nits controllable generation pipeline supports the creation of arbitrarily\nchallenging new instances, making it ideally suited for stress-testing\nnext-generation models. Initial experiments with state-of-the-art systems --\nincluding various Gemini 2.5 Pro and OpenAI o3 which represent the strongest\navailable at the time, alongside strong open-source models -- reveal\nsubstantial performance gaps across all categories, with particularly large\ndeficits in abstract and planning tasks. We release the full dataset,\ngeneration scripts, and evaluation harness to support transparent,\nreproducible, and forward-looking multimodal reasoning research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05523.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655fed9fdef5905d38b84af3",
      "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
      "fullname": "Xiyao Wang",
      "name": "russwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05573",
      "authors": [
        {
          "_id": "6846902a3ec10bdd8ab4db61",
          "name": "Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db62",
          "user": {
            "_id": "62e18206926f4892a4c782bd",
            "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
            "isPro": false,
            "fullname": "Chenguo Lin",
            "user": "chenguolin",
            "type": "user"
          },
          "name": "Chenguo Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:09.640Z",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db63",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db64",
          "name": "Honglei Yan",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db65",
          "name": "Yiqiang Feng",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db66",
          "name": "Yadong Mu",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db67",
          "name": "Katerina Fragkiadaki",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/iNc42ij-Kj0Z4V0jD5lCp.mp4"
      ],
      "publishedAt": "2025-06-05T20:30:28.000Z",
      "submittedOnDailyAt": "2025-06-09T06:14:01.450Z",
      "title": "PartCrafter: Generación estructurada de 3D mecánicas mediante el Transformador de Potencial Distributivo Constitucional",
      "submittedOnDailyBy": {
        "_id": "62e18206926f4892a4c782bd",
        "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
        "isPro": false,
        "fullname": "Chenguo Lin",
        "user": "chenguolin",
        "type": "user"
      },
      "summary": "PartCrafter es el primer modelo de generación 3D estructurado que synthetiza de una sola imagen RGB 3D meshes significativos y geométricamente diferentes. A diferencia de los métodos existentes, en lugar de crear una sola forma 3D o dividir la imagen para reconstruir cada segmento en un proceso de dos etapas, PartCrafter adopta una arquitectura unificada y estructurada sin depender de los segmentos previamente divididos. Basándose en una sola imagen, PartCrafter permite la generación simultánea de múltiples partes 3D, eliminando ruido y reconociendo las relaciones de partes de objetos individuales o espacios multi-objetos complejos. PartCrafter hereda los pesos, encoder y decoder de un 3D meshDIFUージョントランスフォーマー (DiT) entrenado con el objeto completo, introduciendo dos innovaciones: (1) un espacio de potenciales estructurados, donde cada parte 3D se representa como un conjunto de tokens de potencial separados; y (2) una estructura de capas de atención que permite un flujo de información estructurado, asegurando una consistencia global y manteniendo detalles a nivel de parte durante la generación. Para apoyar la supervisión a nivel de parte, se han colectado notas a nivel de parte en una gran colección de datos de objetos 3D y se ha creado un nuevo conjunto de datos. Las experimentaciones muestran que PartCrafter realiza la generación de 3D meshes más factibles que los métodos existentes, incluyendo partes que no son visibles directamente en la imagen de entrada, y que lidera en la generación de información estructurada y la comprensión 3D. El código y los datos de entrenamiento se lanzan.",
      "upvotes": 4,
      "discussionId": "6846902a3ec10bdd8ab4db68",
      "projectPage": "https://wgsxm.github.io/projects/partcrafter",
      "githubRepo": "https://github.com/wgsxm/PartCrafter",
      "ai_summary": "PartCrafter is a unified 3D generative model that synthesizes multiple semantically meaningful 3D meshes from a single image using a compositional latent space and hierarchical attention mechanism.",
      "ai_keywords": [
        "3D generative model",
        "multiple 3D meshes",
        "RGB image",
        "unified compositional generation architecture",
        "denoising",
        "3D diffusion transformer (DiT)",
        "compositional latent space",
        "disentangled latent tokens",
        "hierarchical attention mechanism",
        "part-level supervision",
        "part-aware generative priors"
      ]
    },
    "publishedAt": "2025-06-05T16:30:28.000Z",
    "title": "PartCrafter: Structured 3D Mesh Generation via Compositional Latent\n  Diffusion Transformers",
    "summary": "We introduce PartCrafter, the first structured 3D generative model that\njointly synthesizes multiple semantically meaningful and geometrically distinct\n3D meshes from a single RGB image. Unlike existing methods that either produce\nmonolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an\nimage and then reconstructing each segment, PartCrafter adopts a unified,\ncompositional generation architecture that does not rely on pre-segmented\ninputs. Conditioned on a single image, it simultaneously denoises multiple 3D\nparts, enabling end-to-end part-aware generation of both individual objects and\ncomplex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh\ndiffusion transformer (DiT) trained on whole objects, inheriting the pretrained\nweights, encoder, and decoder, and introduces two key innovations: (1) A\ncompositional latent space, where each 3D part is represented by a set of\ndisentangled latent tokens; (2) A hierarchical attention mechanism that enables\nstructured information flow both within individual parts and across all parts,\nensuring global coherence while preserving part-level detail during generation.\nTo support part-level supervision, we curate a new dataset by mining part-level\nannotations from large-scale 3D object datasets. Experiments show that\nPartCrafter outperforms existing approaches in generating decomposable 3D\nmeshes, including parts that are not directly visible in input images,\ndemonstrating the strength of part-aware generative priors for 3D understanding\nand synthesis. Code and training data will be released.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/iNc42ij-Kj0Z4V0jD5lCp.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05573.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e18206926f4892a4c782bd",
      "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
      "fullname": "Chenguo Lin",
      "name": "chenguolin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06199",
      "authors": [
        {
          "_id": "684637733ec10bdd8ab4da66",
          "user": {
            "_id": "674b2406591d7232820252cd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
            "isPro": false,
            "fullname": "Hongyan Zhi",
            "user": "Hoyard",
            "type": "user"
          },
          "name": "Hongyan Zhi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:27.821Z",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da67",
          "name": "Peihao Chen",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da68",
          "name": "Siyuan Zhou",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da69",
          "name": "Yubo Dong",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6a",
          "name": "Quanxi Wu",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6b",
          "name": "Lei Han",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6c",
          "name": "Mingkui Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T16:00:31.000Z",
      "submittedOnDailyAt": "2025-06-09T01:42:52.611Z",
      "title": "3DFlowAction: Modelo de aprendizaje para operaciones de cruzado emodimención en el mundo 3D del flujo",
      "submittedOnDailyBy": {
        "_id": "674b2406591d7232820252cd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
        "isPro": false,
        "fullname": "Hongyan Zhi",
        "user": "Hoyard",
        "type": "user"
      },
      "summary": "La operación era duraderamente difícil para los robots, mientras que los humanos pueden interactuar fácilmente con objetos complejos. Por ejemplo, podemos pegar un vaso en un micróscopio. La causa de esta dificultad radica en la falta de grandes conjuntos de datos únicos necesarios para enseñar habilidades de movimiento a los robots. Los conjuntos de datos actuales de robots registran acciones de robots en espacios de acción sencillos. Esto impide el aprendizaje de representaciones continuas y fuertes de acciones en otros escenarios. Se ha descubierto que la comprensión de tareas de movimiento por los humanos y cómo objetos se mueven en el espacio 3D son esenciales para la instrucción de acciones. Esta instrucción no depende de la máquina y es adecuada para humanos y diferentes robots. Por lo tanto, el objetivo es aprender un modelo de mundo 3D de flujo a partir de datos de movimiento de humanos y robots. Este modelo predice el movimiento futuro de objetos en el espacio 3D y planifica las acciones. Específicamente, se genera un gran conjunto de datos de flujo óptico 3D mediante un proceso de detección automática de movimientos grandes. Posteriormente, se crea un modelo de mundo basado en difusión de video que aprende la física de movimiento a partir de estos datos y genera un entrenador de flujo óptico 3D basado en instrucciones lingüísticas. A través del flujo óptico 3D generado, se propone una estructura de renderización de guía de flujo y se renderiza el estado final predicho, evaluando si el flujo predicho coincide con la descripción de la tarea utilizando GPT-4o. De esta manera, los robots adquieren la capacidad de planificar planes de acción en ambientes cerrados. Finalmente, se optimiza el flujo óptico 3D predecido bajo las restricciones de políticas de optimización, considerando las acciones de robot necesarias para la tarea. Los experimentos extendidos muestran una generalización robusta en diferentes tareas de movimiento de robots y una adaptación emocional de formato cerrado confiable, sin necesidad de entrenamiento en hardware específico.",
      "upvotes": 3,
      "discussionId": "684637733ec10bdd8ab4da6d",
      "githubRepo": "https://github.com/Hoyyyaard/3DFlowAction/",
      "ai_summary": "A 3D flow world model learned from human and robot manipulation data, using video diffusion and GPT-4o, enables robots to perform diverse manipulation tasks with strong generalization and cross-embodiment adaptation.",
      "ai_keywords": [
        "3D flow world model",
        "moving object auto-detect pipeline",
        "video diffusion-based world model",
        "3D optical flow dataset",
        "ManiFlow-110k",
        "3D optical flow trajectories",
        "flow-guided rendering mechanism",
        "GPT-4o",
        "closed-loop planning",
        "optimization policy",
        "cross-embodiment adaptation"
      ]
    },
    "publishedAt": "2025-06-06T12:00:31.000Z",
    "title": "3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\n  Model",
    "summary": "Manipulation has long been a challenging task for robots, while humans can\neffortlessly perform complex interactions with objects, such as hanging a cup\non the mug rack. A key reason is the lack of a large and uniform dataset for\nteaching robots manipulation skills. Current robot datasets often record robot\naction in different action spaces within a simple scene. This hinders the robot\nto learn a unified and robust action representation for different robots within\ndiverse scenes. Observing how humans understand a manipulation task, we find\nthat understanding how the objects should move in the 3D space is a critical\nclue for guiding actions. This clue is embodiment-agnostic and suitable for\nboth humans and different robots. Motivated by this, we aim to learn a 3D flow\nworld model from both human and robot manipulation data. This model predicts\nthe future movement of the interacting objects in 3D space, guiding action\nplanning for manipulation. Specifically, we synthesize a large-scale 3D optical\nflow dataset, named ManiFlow-110k, through a moving object auto-detect\npipeline. A video diffusion-based world model then learns manipulation physics\nfrom these data, generating 3D optical flow trajectories conditioned on\nlanguage instructions. With the generated 3D object optical flow, we propose a\nflow-guided rendering mechanism, which renders the predicted final state and\nleverages GPT-4o to assess whether the predicted flow aligns with the task\ndescription. This equips the robot with a closed-loop planning ability.\nFinally, we consider the predicted 3D optical flow as constraints for an\noptimization policy to determine a chunk of robot actions for manipulation.\nExtensive experiments demonstrate strong generalization across diverse robotic\nmanipulation tasks and reliable cross-embodiment adaptation without\nhardware-specific training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06199.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674b2406591d7232820252cd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
      "fullname": "Hongyan Zhi",
      "name": "Hoyard",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05433",
      "authors": [
        {
          "_id": "68469df13ec10bdd8ab4db92",
          "name": "Zikang Liu",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db93",
          "name": "Tongtian Yue",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db94",
          "name": "Yepeng Tang",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db95",
          "name": "Longteng Guo",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db96",
          "name": "Junxian Cai",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db97",
          "name": "Qingbin Liu",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db98",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db99",
          "name": "Jing Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T09:13:37.000Z",
      "submittedOnDailyAt": "2025-06-09T07:17:09.252Z",
      "title": "Prefix Grouper: Aprovechamiento de Prefijos Comunes para Entrenamiento Eficiente de GRPO",
      "submittedOnDailyBy": {
        "_id": "6448dcf1b6ac93fe6512e342",
        "avatarUrl": "/avatars/a6441f89eabd156181bafc47c0b2f8c8.svg",
        "isPro": false,
        "fullname": "Zikang Liu",
        "user": "JohnCage",
        "type": "user"
      },
      "summary": "El Grupo de Comparación de Políticas Optimizadas (GRPO) calcula las ventajas de manera eficiente a partir de la comparación relativa de entradas de prefijo común compartido. GRPO es efectivo, pero cuando se trata de procesar largos prefijos comunes, cada miembro del grupo debe codificarlos de manera extensa, lo que aumenta significativamente la carga computacional. Este inconveniente es particularmente significativo en el aprendizaje de contextos largos, donde se vuelve una de las principales limitaciones de escalabilidad. Proponemos el algoritmo de entrenamiento eficiente GRPO llamado Prefix Grouper, utilizando la estrategia de Forward con Prefijo Compartido para eliminar los cálculos extensos de prefijos. Específicamente, reconfiguramos la atención automática en dos partes, lo que permite codificar un solo vez el prefijo común y mantiene la compatibilidad con la entrenamiento de todos los gradientes y puntos de salida. Teóricamente y experimentalmente, Prefix Grouper se comporta como el GRPO estándar: obtiene el mismo salida de forward y retropropagación, y garantiza que el rendimiento de la cálculo final del valor no se ve afectado. Experimentalmente, nuestros resultados muestran que Prefix Grouper obtiene resultados consistentes, reduciendo significativamente los costos computacionales de entrenamiento, especialmente cuando se trata de largos prefijos. El método es completamente pluggable: coincide con la arquitectura GRPO actual, requiere solo mínimos cambios en la configuración de entrada y cálculo de atención, y no necesita modificaciones estructurales. Prefix Grouper permite el uso de grandes grupos dentro de la misma computación, mejorando la escalabilidad de GRPO para tareas complejas o modelos grandes. El código está disponible en https://github.com/johncaged/PrefixGrouper.",
      "upvotes": 2,
      "discussionId": "68469df13ec10bdd8ab4db9a",
      "githubRepo": "https://github.com/johncaged/PrefixGrouper",
      "ai_summary": "Prefix Grouper reduces computational overhead in GRPO by encoding shared prefixes only once, improving scalability in long-context scenarios without altering training dynamics or policy performance.",
      "ai_keywords": [
        "Group Relative Policy Optimization (GRPO)",
        "self-attention",
        "Shared-Prefix Forward strategy",
        "computational overhead",
        "long-context learning scenarios",
        "differentiability",
        "end-to-end training",
        "training-equivalent"
      ]
    },
    "publishedAt": "2025-06-05T05:13:37.000Z",
    "title": "Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward",
    "summary": "Group Relative Policy Optimization (GRPO) enhances policy learning by\ncomputing gradients from relative comparisons among candidate outputs that\nshare a common input prefix. Despite its effectiveness, GRPO introduces\nsubstantial computational overhead when processing long shared prefixes, which\nmust be redundantly encoded for each group member. This inefficiency becomes a\nmajor scalability bottleneck in long-context learning scenarios. We propose\nPrefix Grouper, an efficient GRPO training algorithm that eliminates redundant\nprefix computation via a Shared-Prefix Forward strategy. In particular, by\nrestructuring self-attention into two parts, our method enables the shared\nprefix to be encoded only once, while preserving full differentiability and\ncompatibility with end-to-end training. We provide both theoretical and\nempirical evidence that Prefix Grouper is training-equivalent to standard GRPO:\nit yields identical forward outputs and backward gradients, ensuring that the\noptimization dynamics and final policy performance remain unchanged.\nEmpirically, our experiments confirm that Prefix Grouper achieves consistent\nresults while significantly reducing the computational cost of training,\nparticularly in long-prefix scenarios. The proposed method is fully\nplug-and-play: it is compatible with existing GRPO-based architectures and can\nbe seamlessly integrated into current training pipelines as a drop-in\nreplacement, requiring no structural modifications and only minimal changes to\ninput construction and attention computation. Prefix Grouper enables the use of\nlarger group sizes under the same computational budget, thereby improving the\nscalability of GRPO to more complex tasks and larger models. Code is now\navailable at https://github.com/johncaged/PrefixGrouper",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05433.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6448dcf1b6ac93fe6512e342",
      "avatarUrl": "/avatars/a6441f89eabd156181bafc47c0b2f8c8.svg",
      "fullname": "Zikang Liu",
      "name": "JohnCage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04255",
      "authors": [
        {
          "_id": "684312988f9ec8394c514883",
          "user": {
            "_id": "65c43d6d2b723dbc4ddc29d2",
            "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
            "isPro": false,
            "fullname": "Kunal Pai",
            "user": "guineapig",
            "type": "user"
          },
          "name": "Kunal Pai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-07T05:45:12.032Z",
          "hidden": false
        },
        {
          "_id": "684312988f9ec8394c514884",
          "user": {
            "_id": "62a0dbe7bff710e3fb05f9ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a0dbe7bff710e3fb05f9ae/uZK0Zkv7YG7jWbweh5tQb.png",
            "isPro": false,
            "fullname": "Parth Shah",
            "user": "helloparthshah",
            "type": "user"
          },
          "name": "Parth Shah",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T16:08:57.423Z",
          "hidden": false
        },
        {
          "_id": "684312988f9ec8394c514885",
          "name": "Harshil Patel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T17:33:16.000Z",
      "submittedOnDailyAt": "2025-06-09T03:49:09.306Z",
      "title": "Hyirae Key Agent System Hybrid Intelligent Resource Utilization",
      "submittedOnDailyBy": {
        "_id": "65c43d6d2b723dbc4ddc29d2",
        "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
        "isPro": false,
        "fullname": "Kunal Pai",
        "user": "guineapig",
        "type": "user"
      },
      "summary": "El rápido desarrollo de los Modelos de Lenguaje Grandes (LLM) está impulsando el desarrollo de sistemas de conversación multi-agente (MAS). Sin embargo, los actuales marcos están limitados en flexibilidad, reconocimiento de recursos, diversidad de modelos y desarrollo de herramientas de conversión automática. En este artículo, se presenta \"HASHIRU\" (Sistema de Agentes Heurísticos para la Utilización Inteligente de Recursos), un nuevo marco de trabajo de MAS. Este marco está diseñado para mejorar la flexibilidad, eficiencia de recursos y adaptabilidad. HASHIRU se caracteriza por el manejo dinámico de agentes \"EMPOI\" (Experts Multi-Purpose) basados en la necesidad de la tarea y las limitaciones de recursos (costo, memoria). Este híbrido de inteligencia utiliza de manera flexible pequeños modelos locales y, según sea necesario, API externos y grandes modelos, proporcionados a través de Ollama. Un modelo económico de contratación/liberación incentiva la estabilidad del equipo y la eficientia de la distribución de recursos. Este sistema incluye herramientas de conversión automática y funciones de memoria. Se evaluó HASHIRU en diferentes tareas, demostrando sus capacidades en la evaluación académica (58% de éxito), seguridad (100% de éxito en ciertos puntos de JailbreakBench), evaluación de razonamiento complejo (GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%). Los estudios de caso muestran la generación del modelo de costo de conversión automática, la integración de herramientas y la mejora automática mediante gestión de versiones. HASHIRU ofrece un control heurístico dinámico, una inteligencia híbrida de reconocimiento de recursos y una extensión de funciones de conversión automática, proporcionando un MAS más robusto, eficiente y adaptable. El código fuente y los benchmarks están disponibles en https://github.com/HASHIRU-AI/HASHIRU y https://github.com/HASHIRU-AI/HASHIRUBench. Para consultar sobre la demostración de libre acceso, puede contactarse a través de https://hashiruagentx-hashiruai.hf.space.",
      "upvotes": 2,
      "discussionId": "684312998f9ec8394c514886",
      "githubRepo": "https://github.com/HASHIRU-AI/HASHIRU",
      "ai_summary": "HASHIRU, a novel MAS framework, enhances flexibility, resource efficiency, and adaptability by dynamically managing specialized agents and using a hybrid intelligence approach with smaller, local LLMs and external APIs.",
      "ai_keywords": [
        "Hierarchical Agent System",
        "Hybrid Intelligent Resource Utilization",
        "HASHIRU",
        "CEO agent",
        "employee agents",
        "Ollama",
        "external APIs",
        "economic model",
        "hiring/firing costs",
        "autonomous API tool creation",
        "academic paper review",
        "safety assessments",
        "GSM8K",
        "JEEBench",
        "SVAMP",
        "Gemini 2.0 Flash",
        "self-improvement",
        "autonomous cost model generation",
        "tool integration",
        "budget management"
      ]
    },
    "publishedAt": "2025-06-01T13:33:16.000Z",
    "title": "HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\n  Utilization",
    "summary": "Rapid Large Language Model (LLM) advancements are fueling autonomous\nMulti-Agent System (MAS) development. However, current frameworks often lack\nflexibility, resource awareness, model diversity, and autonomous tool creation.\nThis paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent\nResource Utilization), a novel MAS framework enhancing flexibility, resource\nefficiency, and adaptability. HASHIRU features a \"CEO\" agent dynamically\nmanaging specialized \"employee\" agents, instantiated based on task needs and\nresource constraints (cost, memory). Its hybrid intelligence prioritizes\nsmaller, local LLMs (via Ollama) while flexibly using external APIs and larger\nmodels when necessary. An economic model with hiring/firing costs promotes team\nstability and efficient resource allocation. The system also includes\nautonomous API tool creation and a memory function. Evaluations on tasks like\nacademic paper review (58% success), safety assessments (100% on a\nJailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash\non GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate\nHASHIRU's capabilities. Case studies illustrate its self-improvement via\nautonomous cost model generation, tool integration, and budget management.\nHASHIRU offers a promising approach for more robust, efficient, and adaptable\nMAS through dynamic hierarchical control, resource-aware hybrid intelligence,\nand autonomous functional extension. Source code and benchmarks are available\nat https://github.com/HASHIRU-AI/HASHIRU and\nhttps://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is\navailable at https://hashiruagentx-hashiruai.hf.space upon request.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04255.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c43d6d2b723dbc4ddc29d2",
      "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
      "fullname": "Kunal Pai",
      "name": "guineapig",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]