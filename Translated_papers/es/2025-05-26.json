[
  {
    "paper": {
      "id": "2505.18125",
      "authors": [
        {
          "_id": "6833f8b419852283c4b3bbd6",
          "name": "Alan Arazi",
          "hidden": false
        },
        {
          "_id": "6833f8b419852283c4b3bbd7",
          "user": {
            "_id": "64802fb6c57f629056c59966",
            "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
            "isPro": false,
            "fullname": "Eilam Shapira",
            "user": "EilamSha",
            "type": "user"
          },
          "name": "Eilam Shapira",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:08.206Z",
          "hidden": false
        },
        {
          "_id": "6833f8b419852283c4b3bbd8",
          "name": "Roi Reichart",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:34:28.000Z",
      "submittedOnDailyAt": "2025-05-26T03:50:53.260Z",
      "title": "TabSTAR: Modelo básico de tabla centrado en el contexto de un objetivo de interés",
      "submittedOnDailyBy": {
        "_id": "64802fb6c57f629056c59966",
        "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
        "isPro": false,
        "fullname": "Eilam Shapira",
        "user": "EilamSha",
        "type": "user"
      },
      "summary": "Deep learning ha sido exitosa en varias áreas, pero en tareas de aprendizaje de tablas, los Gradient Boosting Decision Trees (GBDTs) han liderado el campo históricamente. Sin embargo, los avances recientes están preparando los modelos de aprendizaje de tablas basados en texto. Estos pueden utilizar conocimientos reales y generalizar en diferentes conjuntos de datos, especialmente cuando se incluye texto. Sin embargo, en los métodos actuales, se intenta integrar la capacidad de modelos de lenguaje en tareas de tablas, pero casi todos utilizan representaciones de texto estáticas y independientes del objetivo, lo que limita su efectividad. Se presenta TabSTAR: TabSTAR es un modelo de tablas basado en semántica con representaciones relacionadas con el objetivo. TabSTAR permite entrenamiento de transición en datos de tablas que incluyen características de texto. TabSTAR está diseñado con una arquitectura que no depende del conjunto de datos. Este modelo libera el encoder preentrenado de texto y recibe el token de objetivo como entrada. Esto proporciona al modelo el contexto necesario para aprender las características propias de la tarea. TabSTAR ha alcanzado el mejor rendimiento en las marcos de referencia de tareas de clasificación, así como en conjuntos de datos de tamaño intermedio y grande, y muestra una escalabilidad según el número de conjuntos de datos preentrenados. TabSTAR ofrece una ruta para mejorar el rendimiento final.",
      "upvotes": 66,
      "discussionId": "6833f8b419852283c4b3bc02",
      "ai_summary": "TabSTAR, a tabular foundation model with semantically target-aware representations, achieves state-of-the-art performance in classification tasks with text features through transfer learning without dataset-specific parameters.",
      "ai_keywords": [
        "TabSTAR",
        "foundation tabular model",
        "semantically target-aware representations",
        "transfer learning",
        "pretrained text encoder",
        "target tokens",
        "task-specific embeddings",
        "scaling laws"
      ]
    },
    "publishedAt": "2025-05-23T13:34:28.000Z",
    "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations",
    "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18125.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64802fb6c57f629056c59966",
      "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
      "fullname": "Eilam Shapira",
      "name": "EilamSha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17667",
      "authors": [
        {
          "_id": "6833d7c5a3262d6b1e4d358e",
          "user": {
            "_id": "62ecbffd99112e99c5f7fded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
            "isPro": false,
            "fullname": "Fanqi Wan",
            "user": "Wanfq",
            "type": "user"
          },
          "name": "Fanqi Wan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:44.880Z",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d358f",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3590",
          "name": "Shengyi Liao",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3591",
          "name": "Yingcheng Shi",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3592",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3593",
          "name": "Ziyi Yang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3594",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3595",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3596",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3597",
          "name": "Ming Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T09:31:55.000Z",
      "submittedOnDailyAt": "2025-05-26T03:36:36.885Z",
      "title": "QwenLong-L1: Modelo de lógica de largo contexto contra el aprendizaje por refuerzo",
      "submittedOnDailyBy": {
        "_id": "62ecbffd99112e99c5f7fded",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
        "isPro": false,
        "fullname": "Fanqi Wan",
        "user": "Wanfq",
        "type": "user"
      },
      "summary": "Recientemente, los grandes modelos de lógica (LRMs) han demostrado una potente capacidad de lógica y han sido probados a través del aprendizaje por refuerzo (RL). Estas mejoras principalmente se aplican a tareas de lógica en contextos cortos. Por otro lado, la efectiva procesamiento de LRMs con entradas de contexto largo y la realización de RL para lógica constituyen desafíos importantes que no han sido resueltos. Para abordar estos desafíos, primero formalizamos el paradigma de RL para lógica en contextos largos y identificamos los desafíos clave de la eficiencia de entrenamiento óptima y el proceso de optimización instable. Para resolver estos problemas, proponemos el marco de trabajo QwenLong-L1. Este marco basa el desarrollo en escalado contextual avanzado para aplicar LRMs cortos a escenarios de contexto largo. En particular, utilizamos un paso de ajuste de normalización acelerado (SFT) para construir una política inicial fuerte y, posteriormente, aplicamos técnicas de RL guiadas por un currículo para estabilizar el desarrollo de la política y fortalecer la búsqueda de la política a través de una revisión de la dificultad. Los experimentos en los 7 pruebas de respuestas a preguntas de documentos de contexto largo en el marco de referencia benchmark demostraron que QwenLong-L1-32B supera a otros LRMs como OpenAI-o3-mini y Qwen3-235B-A22B, alcanza un rendimiento similar a Claude-3.7-Sonnet-Thinking y muestra un rendimiento especializado entre los mejores LRMs. Esta investigación ha convertidose en el desarrollo de LRMs de contexto largo prácticos que permiten una fuerte lógica en entornos de alta densidad de información.",
      "upvotes": 42,
      "discussionId": "6833d7c6a3262d6b1e4d35c5",
      "githubRepo": "https://github.com/Tongyi-Zhiwen/QwenLong-L1",
      "ai_summary": "A framework called QwenLong-L1 enhances large reasoning models for long-context reasoning through reinforcement learning, achieving leading performance on document question-answering benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "long-context reasoning",
        "short-context reasoning",
        "training efficiency",
        "optimization process",
        "QwenLong-L1",
        "progressive context scaling",
        "supervised fine-tuning",
        "curriculum-guided phased RL",
        "difficulty-aware retrospective sampling",
        "document question-answering benchmarks",
        "OpenAI-o3-mini",
        "Qwen3-235B-A22B",
        "Claude-3.7-Sonnet-Thinking"
      ]
    },
    "publishedAt": "2025-05-23T05:31:55.000Z",
    "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning",
    "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17667.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ecbffd99112e99c5f7fded",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
      "fullname": "Fanqi Wan",
      "name": "Wanfq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17612",
      "authors": [
        {
          "_id": "6833c9fd298a7bec9c3da3b0",
          "name": "Minki Kang",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b1",
          "name": "Jongwon Jeong",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b2",
          "name": "Seanie Lee",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b3",
          "name": "Jaewoong Cho",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b4",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T08:20:15.000Z",
      "submittedOnDailyAt": "2025-05-26T00:25:44.604Z",
      "title": "Metodo para integrar un pequeño modelo usando LLM outputs y un retailización y herramientas de código",
      "submittedOnDailyBy": {
        "_id": "64b74920fe6a108d03fed767",
        "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
        "isPro": false,
        "fullname": "Minki Kang",
        "user": "Nardien",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grandes (LLMs) muestran excelentes resultados en tareas lógicas complejas, pero su costo de cálculo es alto, lo que lo hace poco práctico para su utilización en la mayoría de los casos. Para resolver este problema, recientes investigaciones han centrado su estudio en el uso de la técnica de \"Chain-of-Thought\" (CoT) para transmitir a pequeños modelos de lenguaje (sLMs) la capacidad de realizar tareas lógicas. Sin embargo, este enfoque suele fallar cuando se necesita conocimientos factuales específicos o cálculos precisos, ya que los sLMs tienen limitaciones en estas áreas. En este estudio, se propone un marco de trabajo llamado \"Agent Distillation\", cuyo objetivo es transmitir a los sLMs no solo la capacidad para realizar tareas lógicas, sino también todas las capacidades de los agentes basados en LLMs. El Agent Distillation se mejora en dos direcciones: (1) mejora de la calidad de los trabajos elaborados por el modelo docente utilizando técnicas de pregunta; (2) mejora de la robustez de los pequeños agentes durante el test utilizando la generación automática de acciones. Se evaluaron 8 tareas lógicas, incluyendo áreas de conocimientos factuales y matemáticos, tanto dentro como fuera de su dominio. Finalmente, sLMs con 0.5B, 1.5B y 3B parámetros, utilizando el método de CoT distillado, demostraron competencia con los modelos más grandes de 1.5B, 3B y 7B, mostrando así la posibilidad de construir pequeños agentes prácticos y útiles. El código está disponible en https://github.com/Nardien/agent-distillation.",
      "upvotes": 34,
      "discussionId": "6833ca00298a7bec9c3da444",
      "githubRepo": "https://github.com/Nardien/agent-distillation",
      "ai_summary": "Agent Distillation transfers reasoning and task-solving capabilities from large language models to smaller models using enhanced prompts and self-consistent actions, matching performance of larger models on various reasoning tasks.",
      "ai_keywords": [
        "Large language models",
        "small language models",
        "chain-of-thought",
        "agent distillation",
        "prompting method",
        "first-thought prefix",
        "self-consistent action generation",
        "task-solving behavior",
        "retrieval tools",
        "code tools",
        "in-domain generalization",
        "out-of-domain generalization"
      ]
    },
    "publishedAt": "2025-05-23T04:20:15.000Z",
    "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
    "summary": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17612.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64b74920fe6a108d03fed767",
      "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
      "fullname": "Minki Kang",
      "name": "Nardien",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15929",
      "authors": [
        {
          "_id": "6830404effb59afb6569273a",
          "name": "Hui Shen",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273b",
          "user": {
            "_id": "6621cea88850e38ffbb1854f",
            "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
            "isPro": false,
            "fullname": "Taki WU",
            "user": "taki555",
            "type": "user"
          },
          "name": "Taiqiang Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:14:30.851Z",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273c",
          "name": "Qi Han",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273d",
          "name": "Yunta Hsieh",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273e",
          "user": {
            "_id": "67fe265f9698ae4f5f4db718",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0PLJEbUyJbM9BacFxcScP.png",
            "isPro": false,
            "fullname": "Jizhou Wang",
            "user": "John-ai-bee",
            "type": "user"
          },
          "name": "Jizhou Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:14:33.171Z",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273f",
          "name": "Yuyue Zhang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692740",
          "name": "Yuxin Cheng",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692741",
          "name": "Zijian Hao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692742",
          "name": "Yuansheng Ni",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692743",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692744",
          "name": "Zhongwei Wan",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692745",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692746",
          "name": "Wendong Xu",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692747",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692748",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692749",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274a",
          "name": "Chaofan Tao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274b",
          "name": "Zhuoqing Mao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274c",
          "name": "Ngai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T18:33:50.000Z",
      "submittedOnDailyAt": "2025-05-26T05:21:02.238Z",
      "title": "¿Tiene la \"sabiduría\" de la teoría de la física?",
      "submittedOnDailyBy": {
        "_id": "6621cea88850e38ffbb1854f",
        "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
        "isPro": false,
        "fullname": "Taki WU",
        "user": "taki555",
        "type": "user"
      },
      "summary": "Los actuales marcos de referencia no capturan aspectos importantes de la mente: la teoría física, el conocimiento de dominio y la teoría simbólica, así como la capacidad de entender las restricciones reales. Para resolver esta deficiencia, presentamos PhyX: el primer marco de referencia de gran escala que evalúa la capacidad de un modelo para aplicar la teoría física en escenarios visuales. PhyX incluye una variedad de preguntas de diferentes profundidades y profundidades, combinando 6 tipos de teoría y evaluando 25 sub-dominios y 6 dominios físicos clave (termodinámica, electromagnetismo, mecánica, física moderna, óptica, ondas y acústica). Según nuestra evaluación detallada, los modelos más avanzados también enfrentan grandes desafíos en la teoría física. GPT-4o, Claude3.7-Sonnet y GPT-o4-mini alcanzan precisiónes de 32.5%, 42.2% y 45.8% respectivamente, pero el intervalo de rendimiento de un experto humano supera el 29%. Nuestro análisis revela limitaciones importantes en los modelos actuales: dependencia de la memoria para conocimientos académicos, excesiva dependencia de fórmulas matemáticas y un enfoque en el patrón visual superficial más que en la comprensión verdaderamente física. Evaluamos la capacidad de teoría física con gran detalle a través de estados detallados, estudios de casos específicos y varios protocolos de evaluación. Para garantizar la reproducibilidad, implementamos protocolos de evaluación compatibles basados en herramientas de paquete ampliamente utilizados como VLMEvalKit.",
      "upvotes": 34,
      "discussionId": "68304052ffb59afb6569282f",
      "projectPage": "https://phyx-bench.github.io/",
      "githubRepo": "https://github.com/NastyMarcus/PhyX",
      "ai_summary": "A new benchmark, PhyX, evaluates models' physics-grounded reasoning in visual scenarios, revealing significant limitations in current models' physical understanding compared to human experts.",
      "ai_keywords": [
        "multimodal questions",
        "reasoning types",
        "sub-domains",
        "core physics domains",
        "thermodynamics",
        "electromagnetism",
        "mechanics",
        "modern physics",
        "optics",
        "wave\\&acoustics",
        "fine-grained statistics",
        "case studies",
        "evaluation paradigms",
        "VLMEvalKit"
      ]
    },
    "publishedAt": "2025-05-21T14:33:50.000Z",
    "title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?",
    "summary": "Existing benchmarks fail to capture a crucial aspect of intelligence:\nphysical reasoning, the integrated ability to combine domain knowledge,\nsymbolic reasoning, and understanding of real-world constraints. To address\nthis gap, we introduce PhyX: the first large-scale benchmark designed to assess\nmodels capacity for physics-grounded reasoning in visual scenarios. PhyX\nincludes 3K meticulously curated multimodal questions spanning 6 reasoning\ntypes across 25 sub-domains and 6 core physics domains: thermodynamics,\nelectromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In\nour comprehensive evaluation, even state-of-the-art models struggle\nsignificantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and\nGPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy\nrespectively-performance gaps exceeding 29\\% compared to human experts. Our\nanalysis exposes critical limitations in current models: over-reliance on\nmemorized disciplinary knowledge, excessive dependence on mathematical\nformulations, and surface-level visual pattern matching rather than genuine\nphysical understanding. We provide in-depth analysis through fine-grained\nstatistics, detailed case studies, and multiple evaluation paradigms to\nthoroughly examine physical reasoning capabilities. To ensure reproducibility,\nwe implement a compatible evaluation protocol based on widely-used toolkits\nsuch as VLMEvalKit, enabling one-click evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15929.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6621cea88850e38ffbb1854f",
      "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
      "fullname": "Taki WU",
      "name": "taki555",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18129",
      "authors": [
        {
          "_id": "6833cf89df7cbb5c087a4caa",
          "name": "Yan Ma",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cab",
          "name": "Linge Du",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cac",
          "user": {
            "_id": "642e4d4d6748dd4f8eeb7732",
            "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
            "isPro": false,
            "fullname": "Xuyang Shen",
            "user": "Ryan1122",
            "type": "user"
          },
          "name": "Xuyang Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:03.920Z",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cad",
          "name": "Shaoxiang Chen",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cae",
          "name": "Pengfei Li",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4caf",
          "name": "Qibing Ren",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb0",
          "name": "Lizhuang Ma",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb1",
          "name": "Yuchao Dai",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb2",
          "name": "Pengfei Liu",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb3",
          "name": "Junjie Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:41:14.000Z",
      "submittedOnDailyAt": "2025-05-26T00:54:44.420Z",
      "title": "Una RL que puede ver todo: el aprendizaje de refuerzo de integración tridimensional visual",
      "submittedOnDailyBy": {
        "_id": "642e4d4d6748dd4f8eeb7732",
        "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
        "isPro": false,
        "fullname": "Xuyang Shen",
        "user": "Ryan1122",
        "type": "user"
      },
      "summary": "El aprendizaje por refuerzo (RL) ha mejorado significativamente la capacidad lógica de los modelos de lenguaje visual (VLMs). Sin embargo, el uso de RL en tareas lógicas es menos explorado que en tareas de detección de objetos o segmentación, que enfatizan la observación. Proponemos el sistema de aprendizaje por refuerzo unificado Visual Triple, llamado V-Triune. Este sistema permite que los VLMs aprendan lógica visual y observación en un solo flujo de entrenamiento. V-Triune consta de tres componentes complementarios: formateo de datos a nivel de muestra (unificación de entradas de diferentes tareas), cálculo de recompensa a nivel de datos de validación (provisión de recompensas personalizadas a través de datos especializados), y monitoreo de métricas a nivel de fuente (diagnóstico de problemas a nivel de fuente de datos). Además, presentamos una nueva recompensa de IoU dinámico que proporciona retroalimentación adaptativa, evolutiva y decisiva para tareas de observación. Nuestro enfoque se ha implementado dentro del marco de entrenamiento de RL de off-policy utilizando modelos de 7B y 32B de código abierto. Los modelos resultantes, denominados Logical Vision (LV), muestran mejoras en tareas lógicas y observación. Estas capacidades se han desarrollado en conjunto con diversos conjuntos de datos centrados en tareas visuales lógicas (matemáticas, puzzles, gráficos, ciencias) y tareas de observación (detección, segmentación, contador, OCR). Posteriormente, LV demostró notables resultados en MEGA-Bench Core, con mejoras en el rango de +2.1 a +14.1 para las versiones de 7B y 32B, y ampliada aplicación en tareas inferiores. Estos resultados confirman la efectividad y escalabilidad de un enfoque de RL integrado para VLMs. El sistema V-Triune y los modelos LV están disponibles en https://github.com/MiniMax-AI.",
      "upvotes": 33,
      "discussionId": "6833cf8adf7cbb5c087a4d0c",
      "githubRepo": "https://github.com/MiniMax-AI/One-RL-to-See-Them-All",
      "ai_summary": "A unified reinforcement learning system, V-Triune, combines visual reasoning and perception tasks in vision-language models through a single training pipeline, achieving significant improvements across various tasks.",
      "ai_keywords": [
        "visual triple unified reinforcement learning",
        "sample-level data formatting",
        "verifier-level reward computation",
        "source-level metric monitoring",
        "dynamic IoU reward",
        "reinforcement learning",
        "vision-language models",
        "object detection",
        "grounding",
        "Orsta",
        "MEGA-Bench Core"
      ]
    },
    "publishedAt": "2025-05-23T13:41:14.000Z",
    "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
    "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e4d4d6748dd4f8eeb7732",
      "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
      "fullname": "Xuyang Shen",
      "name": "Ryan1122",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18092",
      "authors": [
        {
          "_id": "6833ea049f968fc5c6b64486",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64487",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64488",
          "user": {
            "_id": "62ecbffd99112e99c5f7fded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
            "isPro": false,
            "fullname": "Fanqi Wan",
            "user": "Wanfq",
            "type": "user"
          },
          "name": "Fanqi Wan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:25.991Z",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64489",
          "name": "Shengyi Liao",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448a",
          "name": "Shaopeng Lai",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448b",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448c",
          "name": "Yingcheng Shi",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448d",
          "name": "Yuning Wu",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448e",
          "name": "Gang Fu",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448f",
          "name": "Zhansheng Li",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64490",
          "name": "Bin Yang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64491",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64492",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64493",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64494",
          "name": "Ming Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T16:47:00.000Z",
      "submittedOnDailyAt": "2025-05-26T04:43:04.143Z",
      "title": "QwenLong-CPRS: QwenLong-CPRS: Optimización de Contexto Dinámico para LLMs Infinitos",
      "submittedOnDailyBy": {
        "_id": "64777a346e6c7ac608c1e9bf",
        "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
        "isPro": false,
        "fullname": "Weizhou Shen",
        "user": "shenwzh3",
        "type": "user"
      },
      "summary": "Este informe técnico describe cómo QwenLong-CPRS utiliza un marco de compresión de contexto para optimizar claramente el contexto largo, y aborda el sobre cargo computacional del paso de rellenamiento previo y el fenómeno de \"perdida intermedia\" en modelos de lenguaje grandes (LLMs) al procesar largos contextos. Se implementa una nueva estructura dinámica de optimización de contexto, que permite compresión de contexto multigranular por instrucciones de lenguaje natural, y logra mejoras en eficiencia y rendimiento.\n\nQwenLong-CPRS, desarrollado en la serie de arquitectura Qwen, introduce 4 innovaciones: optimización dinámica basada en lenguaje, capa bidireccional para mejorar la polaridad, estructura tokenica con cabezas de modelado de lenguaje, y inferencia paralela de ventanas.\n\nLos resultados de evaluación en 5 benchmarks (con contextos de 4K a 2M palabras) muestran que QwenLong-CPRS presenta 3 efectos: supera a otros métodos de gestión de contexto (RAG, atención esparsa) en precisión y eficiencia. Al ser arquitecturalmente independiente, integra a todos los LLMs de flash como GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3 y Qwen2.5-max, logrando una compresión de contexto de 21.59 veces y un aumento promedio de rendimiento de 19.15 puntos. La introducción de Qwen2.5-32B-Instruct permite que QwenLong-CPRS supere a Ruler-128K y a InfiniteBench en 4.85 y 10.88 puntos, respectivamente, estableciendo una nueva línea de rendimiento estandar.",
      "upvotes": 31,
      "discussionId": "6833ea059f968fc5c6b644c1",
      "ai_summary": "QwenLong-CPRS enhances large language models with multi-granularity context compression, dynamic optimization guided by natural language, and efficient bidirectional reasoning and parallel inference, achieving superior performance and context management.",
      "ai_keywords": [
        "context compression",
        "dynamic context optimization",
        "bidirectional reasoning layers",
        "token critic mechanisms",
        "window-parallel inference",
        "Qwen",
        "RAG",
        "sparse attention",
        "large language models",
        "SOTA performance"
      ]
    },
    "publishedAt": "2025-05-23T12:47:00.000Z",
    "title": "QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization",
    "summary": "This technical report presents QwenLong-CPRS, a context compression framework\ndesigned for explicit long-context optimization, addressing prohibitive\ncomputation overhead during the prefill stage and the \"lost in the middle\"\nperformance degradation of large language models (LLMs) during long sequence\nprocessing. Implemented through a novel dynamic context optimization mechanism,\nQwenLong-CPRS enables multi-granularity context compression guided by natural\nlanguage instructions, achieving both efficiency gains and improved\nperformance.\n  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key\ninnovations: (1) Natural language-guided dynamic optimization, (2)\nBidirectional reasoning layers for enhanced boundary awareness, (3) Token\ncritic mechanisms with language modeling heads, and (4) Window-parallel\ninference.\n  Comprehensive evaluations across five benchmarks (4K-2M word contexts)\ndemonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority\nover other context management methods like RAG and sparse attention in both\naccuracy and efficiency. (2) Architecture-agnostic integration with all\nflagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,\nand Qwen2.5-max, achieves 21.59times context compression alongside\n19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,\nQwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on\nRuler-128K and InfiniteBench, establishing new SOTA performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18092.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64777a346e6c7ac608c1e9bf",
      "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
      "fullname": "Weizhou Shen",
      "name": "shenwzh3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17225",
      "authors": [
        {
          "_id": "6833c65d49b9e903d3ddbd11",
          "user": {
            "_id": "62845957b410bd779033759c",
            "avatarUrl": "/avatars/4feef73c06f2f7de6abf7a4789ac13f9.svg",
            "isPro": false,
            "fullname": "Doohyuk Jang",
            "user": "jadohu",
            "type": "user"
          },
          "name": "Doohyuk Jang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:25.026Z",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd12",
          "user": {
            "_id": "61b15ce1a5dd7dc7024406dc",
            "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
            "isPro": false,
            "fullname": "Yoonjeon Kim",
            "user": "yjyjyj98",
            "type": "user"
          },
          "name": "Yoonjeon Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:27.112Z",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd13",
          "name": "Chanjae Park",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd14",
          "name": "Hyun Ryu",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd15",
          "name": "Eunho Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T19:00:01.000Z",
      "submittedOnDailyAt": "2025-05-26T00:11:09.797Z",
      "title": "La línea de código es:\n\n```\nria_ning_module_is_stubborn: diagnosing command overrides in the ria_ning module\n```",
      "submittedOnDailyBy": {
        "_id": "61b15ce1a5dd7dc7024406dc",
        "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
        "isPro": false,
        "fullname": "Yoonjeon Kim",
        "user": "yjyjyj98",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje general muestran un excelente rendimiento en tareas de lógica compleja y larga. Sin embargo, estos modelos muestran una inflexibilidad lógica problemática, que se denomina \"rigidez lógica\", y a menudo priorizan patrones lógicos generales, incluso cuando se les da instrucciones claras, lo que a menudo lleva a conclusiones erróneas. Esta conducta es especialmente problemaática en áreas como la matemática y los rompecabezas lógicos, donde el cumplimiento de restricciones específicas es crucial. Para investigar sistemáticamente esta rigidez lógica, se presenta un conjunto de diagnósticos puntuales de expertos, que se han investigado poco en estudios previos. Este conjunto de datos permite identificar patrones sibéricos que se repiten recurrentemente en lógica general. En particular, estos patrones se clasifican en tres modos: (i) sobrecarga de interpretación, (ii) desconfianza de entrada, y (iii) atención parcial del proyecto, que incitan a que el modelo ignore o distorsione sus instrucciones. Se publica este conjunto de datos con el objetivo de fomentar futuras investigaciones que busquen mitigar esta rigidez lógica.",
      "upvotes": 30,
      "discussionId": "6833c65e49b9e903d3ddbd6a",
      "projectPage": "https://reasoningtrap.github.io/",
      "githubRepo": "https://github.com/ReasoningTrap/ReasoningTrap",
      "ai_summary": "A diagnostic set examines and categorizes reasoning rigidity in large language models, identifying patterns where models ignore instructions and default to familiar reasoning.",
      "ai_keywords": [
        "reasoning rigidity",
        "large language models",
        "long and complex reasoning tasks",
        "reasoning trajectories",
        "diagnostic set",
        "AIME",
        "MATH500",
        "Interpretation Overload",
        "Input Distrust",
        "Partial Instruction Attention"
      ]
    },
    "publishedAt": "2025-05-22T15:00:01.000Z",
    "title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in\n  Reasoning Models",
    "summary": "Large language models have demonstrated remarkable proficiency in long and\ncomplex reasoning tasks. However, they frequently exhibit a problematic\nreliance on familiar reasoning patterns, a phenomenon we term reasoning\nrigidity. Despite explicit instructions from users, these models often\noverride clearly stated conditions and default to habitual reasoning\ntrajectories, leading to incorrect conclusions. This behavior presents\nsignificant challenges, particularly in domains such as mathematics and logic\npuzzle, where precise adherence to specified constraints is critical. To\nsystematically investigate reasoning rigidity, a behavior largely unexplored in\nprior work, we introduce a expert-curated diagnostic set, . Our\ndataset includes specially modified variants of existing mathematical\nbenchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately\nredesigned to require deviation from familiar reasoning strategies. Using this\ndataset, we identify recurring contamination patterns that occur when models\ndefault to ingrained reasoning. Specifically, we categorize this contamination\ninto three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,\nand (iii) Partial Instruction Attention, each causing models to ignore or\ndistort provided instructions. We publicly release our diagnostic set to\nfacilitate future research on mitigating reasoning rigidity in language models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b15ce1a5dd7dc7024406dc",
      "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
      "fullname": "Yoonjeon Kim",
      "name": "yjyjyj98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17941",
      "authors": [
        {
          "_id": "6833cc35015eb19058ed83d9",
          "user": {
            "_id": "65811eeaa2284a018e51f1ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
            "isPro": true,
            "fullname": "Zigeng Chen",
            "user": "Zigeng",
            "type": "user"
          },
          "name": "Zigeng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:12.285Z",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83da",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83db",
          "name": "Gongfan Fang",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83dc",
          "name": "Ruonan Yu",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83dd",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T14:17:56.000Z",
      "submittedOnDailyAt": "2025-05-26T00:36:09.618Z",
      "title": "VeriThinker: Optimización de modelos de inferencia mediante aprendizaje de verificación",
      "submittedOnDailyBy": {
        "_id": "65811eeaa2284a018e51f1ba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
        "isPro": true,
        "fullname": "Zigeng Chen",
        "user": "Zigeng",
        "type": "user"
      },
      "summary": "Los modelos lógicos de razonamiento (LRMs) superan tareas complejas utilizando la lógica de la cadena de pensamiento (CoT). Sin embargo, el pensamiento excesivo puede llevar a cadenas de razonamiento innecesariamente largas, lo que aumenta significativamente los costos de inferencia. Para mitigar este problema, presentamos un nuevo enfoque llamado VeriThinker. En contraste con los métodos tradicionales que ajustan directamente a los LRMs utilizando datos CoT sintéticos y sencillos, VeriThinker ajusta los modelos utilizando solo tareas de prueba de demostración auxiliares. Al demostrar con precisión la exactitud de las soluciones CoT, los LRMs pueden más claramente determinar la necesidad de un estado posterior de reflexión y se ven más capaces de restringir el pensamiento excesivo. Los experimentos expandidos demuestran que VeriThinker reduce significativamente la longitud de las cadenas de razonamiento, manteniendo o ligeramente aumentando la precisión. En el caso de DeepSeek-R1-Distill-Qwen-7B, en el conjunto de datos MATH500, el número de tokens de razonamiento se reduce de 3790 a 2125, lo que implica un aumento de precisión del 0.8% (94.0% a 94.8%), y en AIME25, el número de tokens se reduce de 14321 a 10287, lo que implica un aumento de precisión del 2.1% (38.7% a 40.8%). Además, los experimentos demuestran que VeriThinker es capaz de generalizar en el dominio de la lógica de la hipótesis de forma 0-shot. El código está disponible en https://github.com/czg1225/VeriThinker.",
      "upvotes": 20,
      "discussionId": "6833cc36015eb19058ed8419",
      "githubRepo": "https://github.com/czg1225/VeriThinker",
      "ai_summary": "VeriThinker reduces the length of complex reasoning chains in Large Reasoning Models (LRMs) by fine-tuning them on a verification task, thereby decreasing inference costs without significantly sacrificing accuracy.",
      "ai_keywords": [
        "Large Reasoning Models (LRMs)",
        "Chain-of-Thought (CoT) reasoning",
        "CoT compression",
        "verification task",
        "reasoning chain lengths",
        "reasoning tokens",
        "accuracy",
        "DeepSeek-R1-Distill-Qwen-7B",
        "MATH500",
        "AIME25",
        "speculative reasoning"
      ]
    },
    "publishedAt": "2025-05-23T10:17:56.000Z",
    "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
    "summary": "Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought\n(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily\nlengthy reasoning chains, dramatically increasing inference costs. To mitigate\nthis issue, we introduce VeriThinker, a novel approach for CoT compression.\nUnlike conventional methods that fine-tune LRMs directly on the original\nreasoning task using synthetic concise CoT data, we innovatively fine-tune the\nmodel solely through an auxiliary verification task. By training LRMs to\naccurately verify the correctness of CoT solutions, the LRMs inherently become\nmore discerning about the necessity of subsequent self-reflection steps,\nthereby effectively suppressing overthinking. Extensive experiments validate\nthat VeriThinker substantially reduces reasoning chain lengths while\nmaintaining or even slightly improving accuracy. When applied to\nDeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500\nfrom 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on\nAIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to\n40.8%). Additionally, our experiments demonstrate that VeriThinker can also be\nzero-shot generalized to speculative reasoning. Code is available at\nhttps://github.com/czg1225/VeriThinker",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65811eeaa2284a018e51f1ba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
      "fullname": "Zigeng Chen",
      "name": "Zigeng",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17561",
      "authors": [
        {
          "_id": "6833cb9030cd9df52a117557",
          "name": "Kwanyoung Kim",
          "hidden": false
        },
        {
          "_id": "6833cb9030cd9df52a117558",
          "name": "Sanghyun Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T07:09:10.000Z",
      "submittedOnDailyAt": "2025-05-26T00:33:24.403Z",
      "title": "El modelo ya conoce la mejor ruido: el modelo de video depilación selecciona el ruido activo bayesiano a través de la atención.",
      "submittedOnDailyBy": {
        "_id": "63973ee44e7b4959dc98028f",
        "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
        "isPro": false,
        "fullname": "Kwanyoung",
        "user": "kwanyoung",
        "type": "user"
      },
      "summary": "La elección del ruido inicial tiene un impacto significativo en la calidad del modelo de difusor de video y en la disposición de los patrones. Si se utiliza un ruido diferente para el mismo patron, la video generada puede variar considerablemente. Los métodos recientes dependen de diseños adyacentes como filtros de frecuencia o la planificación de los frames adyacentes, pero no abren los señales dentro del modelo. Para resolver esto, se propone ANSE (Framework para la Selección de Ruido Activa para la Generación). ANSE depende de un modelo que selecciona ruido de alta calidad mediante la cuantificación de la incertidumbre basada en atención. Su punto clave es BANSA (Bayesian Active Noise Selection via Attention). BANSA mide la incertidumbre de entropia de múltiples muestras de atención de estorque para estimar la confianza y la coherencia del modelo. Para procesamiento eficiente durante la inferencia, se introduce aproximación de mascara de Bernoulli normalizada para BANSA, y se utilizan un difusor de un paso y algunas capas de atención para realizar la estimación de puntuación. En los experimentos con CogVideoX-2B y 5B, ANSE mejoró la calidad y la coherencia temporal de la videografía con un aumento del tiempo de inferencia del 8% y 13%, proporcionando una abordaje racional y generalizable en la selección de ruido en difusores de video. Puede consultar el sitio web del proyecto en: https://anse-project.github.io/anse-project/",
      "upvotes": 18,
      "discussionId": "6833cb9430cd9df52a11765d",
      "projectPage": "https://anse-project.github.io/anse-project/",
      "ai_summary": "ANSE enhances video diffusion models by selecting noise seeds based on model confidence, improving video quality and temporal coherence with minimal increase in inference time.",
      "ai_keywords": [
        "video diffusion models",
        "noise seeds",
        "prompt alignment",
        "external priors",
        "frequency filters",
        "inter-frame smoothing",
        "ANSE",
        "Active Noise Selection for Generation",
        "BANSA",
        "Bayesian Active Noise Selection via Attention",
        "acquisition function",
        "entropy disagreement",
        "stochastic attention samples",
        "score estimation",
        "diffusion step",
        "temporal coherence"
      ]
    },
    "publishedAt": "2025-05-23T03:09:10.000Z",
    "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via\n  Attention in Video Diffusion Model",
    "summary": "The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17561.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63973ee44e7b4959dc98028f",
      "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
      "fullname": "Kwanyoung",
      "name": "kwanyoung",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17873",
      "authors": [
        {
          "_id": "68341f661d53989a8ecb685d",
          "user": {
            "_id": "6684b284dc7b0ae2cc67660c",
            "avatarUrl": "/avatars/54b3c0c4d808f293d78085d4d504570a.svg",
            "isPro": false,
            "fullname": "liuwanhao",
            "user": "wanhaoliu",
            "type": "user"
          },
          "name": "Wanhao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:54.880Z",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb685e",
          "user": {
            "_id": "646a11791556443f24b582e9",
            "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
            "isPro": false,
            "fullname": "Zonglin Yang",
            "user": "ZonglinY",
            "type": "user"
          },
          "name": "Zonglin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:08:35.046Z",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb685f",
          "name": "Jue Wang",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6860",
          "name": "Lidong Bing",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6861",
          "user": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:40:05.397Z",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6862",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6863",
          "name": "Yuqiang Li",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6864",
          "name": "Houqiang Li",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6865",
          "name": "Erik Cambria",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6866",
          "name": "Wanli Ouyang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T13:24:50.000Z",
      "submittedOnDailyAt": "2025-05-26T06:33:21.775Z",
      "title": "MOOSE-Chem3: Método para establecer la clasificación de las hipótesis a través de retroalimentación sobre experimentos",
      "submittedOnDailyBy": {
        "_id": "646a11791556443f24b582e9",
        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
        "isPro": false,
        "fullname": "Zonglin Yang",
        "user": "ZonglinY",
        "type": "user"
      },
      "summary": "La hipótesis de ordenamiento es un componente importante en la automatización del descubrimiento científico, especialmente en las áreas de la ciencia natural donde los experimentos de laboratorio son costosos y están limitados por el tipo de experimento. El enfoque actual se centra en el ordenamiento previo de los experimentos, dependiendo solo de las razones internas del modelo de lenguaje y sin incluir los resultados reales de los experimentos. Presentamos la tarea de ordenar las hipótesis guiadas por el experimento, con el objetivo de priorizar las hipótesis basadas en resultados previamente medidos. Sin embargo, el desarrollo de esta estrategia es difícil en las áreas de la ciencia natural debido a la ineficiencia de repetir experimentos en la práctica. Para enfrentar esta situación, proponemos un virtual agente basado en información de tres áreas, donde se decide la prioridad de las hipótesis basada en su similitud con hipótesis conocidas y se modela su sensibilidad a la ruido. Construimos un conjunto de datos basado en 124 hipótesis químicas y desarrollamos un método de ordenamiento guiado por experimentos a través de este virtual agente. Agrupamos las hipótesis que comparten características funcionales y priorizamos las hipótesis basadas en las insights obtenidos de la retroalimentación de los experimentos simulados. Los experimentos muestran que nuestro método supera los estándares previos y las limitaciones fuertes antes de los experimentos.",
      "upvotes": 14,
      "discussionId": "68341f671d53989a8ecb68b8",
      "ai_summary": "A novel simulator and experiment-guided ranking method improve hypothesis prioritization in scientific discovery by incorporating simulated experimental outcomes.",
      "ai_keywords": [
        "hypothesis ranking",
        "automated scientific discovery",
        "natural sciences",
        "wet-lab experiments",
        "large language model",
        "pre-experiment ranking",
        "experiment-guided ranking",
        "hypothesis performance",
        "similarity",
        "noise",
        "dataset",
        "pseudo experiment-guided ranking",
        "clustering",
        "functional characteristics",
        "simulated experimental feedback"
      ]
    },
    "publishedAt": "2025-05-23T09:24:50.000Z",
    "title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated\n  Experimental Feedback",
    "summary": "Hypothesis ranking is a crucial component of automated scientific discovery,\nparticularly in natural sciences where wet-lab experiments are costly and\nthroughput-limited. Existing approaches focus on pre-experiment ranking,\nrelying solely on large language model's internal reasoning without\nincorporating empirical outcomes from experiments. We introduce the task of\nexperiment-guided ranking, which aims to prioritize candidate hypotheses based\non the results of previously tested ones. However, developing such strategies\nis challenging due to the impracticality of repeatedly conducting real\nexperiments in natural science domains. To address this, we propose a simulator\ngrounded in three domain-informed assumptions, modeling hypothesis performance\nas a function of similarity to a known ground truth hypothesis, perturbed by\nnoise. We curate a dataset of 124 chemistry hypotheses with experimentally\nreported outcomes to validate the simulator. Building on this simulator, we\ndevelop a pseudo experiment-guided ranking method that clusters hypotheses by\nshared functional characteristics and prioritizes candidates based on insights\nderived from simulated experimental feedback. Experiments show that our method\noutperforms pre-experiment baselines and strong ablations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17873.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "646a11791556443f24b582e9",
      "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
      "fullname": "Zonglin Yang",
      "name": "ZonglinY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16211",
      "authors": [
        {
          "_id": "6833d9cfdf7cbb5c087cb9cd",
          "name": "Kai Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ce",
          "name": "Can Shen",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9cf",
          "name": "Yile Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d0",
          "name": "Jirui Han",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d1",
          "name": "Kelong Zheng",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d2",
          "name": "Xuechao Zou",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d3",
          "name": "Zhe Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d4",
          "name": "Xingjian Du",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d5",
          "name": "Shun Zhang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d6",
          "name": "Hanjun Luo",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d7",
          "name": "Yingbin Jin",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d8",
          "name": "Xinxin Xing",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d9",
          "name": "Ziyang Ma",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9da",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9db",
          "user": {
            "_id": "64c6627d5671d42e0adfad56",
            "avatarUrl": "/avatars/8b98054b2911b86dcc4856a15306e60f.svg",
            "isPro": false,
            "fullname": "jiaxiaojunQAQ",
            "user": "jiaxiaojunQAQ",
            "type": "user"
          },
          "name": "Xiaojun Jia",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:37.847Z",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9dc",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9dd",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9de",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9df",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e0",
          "name": "Haoyang Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e1",
          "name": "Yiming Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e2",
          "name": "Xiaobin Zhuang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e3",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e4",
          "name": "Haibo Hu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e5",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e6",
          "name": "Zhizheng Wu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e7",
          "name": "Xiaolin Hu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e8",
          "name": "Eng-Siong Chng",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e9",
          "name": "XiaoFeng Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ea",
          "name": "Wenyuan Xu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9eb",
          "name": "Wei Dong",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ec",
          "name": "Xinfeng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T04:27:46.000Z",
      "submittedOnDailyAt": "2025-05-26T01:33:43.107Z",
      "title": "AudioTrust: Marca de confiabilidad multifacética del modelo de lenguaje de voz",
      "submittedOnDailyBy": {
        "_id": "6387676c23da90491eb9fb16",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
        "isPro": false,
        "fullname": "Kai Li",
        "user": "JusperLee",
        "type": "user"
      },
      "summary": "El rápido desarrollo y la expansión de aplicaciones de los modelos de lenguaje de audio (ALLMs) ha impulsado la necesidad de entender su confianza con rigor. Sin embargo, la investigación sistemática sobre la evaluación de estos modelos, especialmente sobre los riesgos propios de los modelos de audio, ha sido poco explorada. Los actuales marcos de evaluación se centran principalmente en modelos de texto o en dimensiones limitadas de seguridad, y no se adaptan adecuadamente a las características únicas y los escenarios de aplicación de los modelos de audio. En este contexto, se presenta AudioTrust, el primer marco de evaluación de confianza multidimensional y un benchmark diseñado específicamente para los ALLMs. AudioTrust fomenta la evaluación en seis dimensiones clave: justicia, hacking, seguridad, privacidad, robustez y autentificación. Para evaluar estas dimensiones, AudioTrust configura 18 configuraciones experimentales diferentes, con un conjunto de datos exquisito construido con más de 4,420 muestras de audio/texto. Este conjunto de datos se ha diseñado para extraer escenarios reales como conversaciones diarias, llamadas emergentes y interacciones con asistentes de voz, para investigar la confianza multidimensional de los ALLMs. Para la evaluación, el benchmark establece 9 puntos de evaluación únicos de audio y utiliza un proceso automático a gran escala para proporcionar puntuaciones objetivas y escalables. Los resultados de los experimentos demuestran que los más avanzados modelos de ALLMs, tanto abiertos como cerrados, al enfrentar escenarios de alto riesgo de audio, se encuentran en límites y limitaciones claros de confianza, y proporcionan una privacidad end-to-end valiosa para la introducción de confianza segura en modelos de audio futuros. El plataforma y el benchmark están disponibles en https://github.com/JusperLee/AudioTrust.",
      "upvotes": 14,
      "discussionId": "6833d9d1df7cbb5c087cba85",
      "githubRepo": "https://github.com/JusperLee/AudioTrust",
      "ai_summary": "AudioTrust evaluates the trustworthiness of Audio Large Language Models across multifaceted dimensions, using a comprehensive dataset and specific metrics to assess their performance in real-world audio scenarios.",
      "ai_keywords": [
        "Audio Large Language Models",
        "ALLMs",
        "trustworthiness",
        "fairness",
        "hallucination",
        "safety",
        "privacy",
        "robustness",
        "authentication",
        "AudioTrust",
        "experimental setups",
        "audio-specific evaluation metrics",
        "automated pipeline"
      ]
    },
    "publishedAt": "2025-05-22T00:27:46.000Z",
    "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models",
    "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16211.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6387676c23da90491eb9fb16",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
      "fullname": "Kai Li",
      "name": "JusperLee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17618",
      "authors": [
        {
          "_id": "6833eeaf98515618764fc204",
          "user": {
            "_id": "6672937ceac0fb1b9e516595",
            "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg",
            "isPro": false,
            "fullname": "haoran he",
            "user": "haoranhe",
            "type": "user"
          },
          "name": "Haoran He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:19.593Z",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc205",
          "name": "Jiajun Liang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc206",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc207",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc208",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc209",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc20a",
          "name": "Ling Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T08:25:46.000Z",
      "submittedOnDailyAt": "2025-05-26T04:32:07.257Z",
      "title": "Generación de escalas de imagenes y videos durante el tiempo de prueba utilizando cálculo evolutivo",
      "submittedOnDailyBy": {
        "_id": "667187ba9ab144eb3ac43a1b",
        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
        "isPro": false,
        "fullname": "Runze Liu",
        "user": "RyanLiu112",
        "type": "user"
      },
      "summary": "Cuando el costo de la expansión del cálculo (datos y parámetros) aumenta significativamente durante el entrenamiento de un modelo, la expansión del cálculo en el inferencia (TTS) se presenta como una dirección potencial para mejorar el rendimiento del modelo de generación al asignar una cantidad adicional de cálculos. TTS ha demostrado éxito en varios tareas de lenguaje, pero su comprensión en el funcionamiento de expansión para modelos de generación de imágenes o videos (basados en difusión o en flujos) ha sido retrasada. Recientes estudios han intentado explorar estrategias para la inferencia en tareas visuales, pero estos enfoques tienen limitaciones importantes y se ven limitados a dominios específicos, lo que puede llevar a una pérdida de diversidad o a un óptimo excesivo. En este artículo, se propone un nuevo método eficiente generalizado de TTS llamado \"EvoSearch\". Este método mejora eficazmente la capacidad de expansión de modelos de difusión o flujos para la generación de imágenes o videos, y no requiere adicionales entrenamientos o expansiones del modelo. EvoSearch utiliza los principios de evolución para redefinir el problema de expansión en modelos de difusión o flujos como un problema de búsqueda evolutiva, explorando y mejorando rutas de cálculo generales y eficientes. Incluye diseños apropiados y estructuras de mutación que se ajustan a la proceso de difusión, manteniendo la diversidad de diferentes poblaciones mientras genera continuamente alta calidad de descendientes. En una amplia evaluación de modelos de difusión o flujos para tareas de generación de imágenes o videos, nuestro método supera experiencialmente los métodos existentes, ofrece alta diversidad y muestra una excelente generalización bajo nuevas evaluaciones. Este proyecto puede acceder a través de https://tinnerhrhe.github.io/evosearch.",
      "upvotes": 11,
      "discussionId": "6833eeb198515618764fc277",
      "projectPage": "https://tinnerhrhe.github.io/evosearch/",
      "githubRepo": "https://github.com/tinnerhrhe/EvoSearch-codes",
      "ai_summary": "EvoSearch, an evolutionary search method, enhances test-time scaling for diffusion and flow-based generative models, improving image and video generation quality, diversity, and generalizability.",
      "ai_keywords": [
        "test-time scaling",
        "TTS",
        "image generation",
        "video generation",
        "diffusion models",
        "flow-based models",
        "denoising trajectory",
        "stochastic differential equation",
        "selection",
        "mutation",
        "EvoSearch"
      ]
    },
    "publishedAt": "2025-05-23T04:25:46.000Z",
    "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
    "summary": "As the marginal cost of scaling computation (data and parameters) during\nmodel pre-training continues to increase substantially, test-time scaling (TTS)\nhas emerged as a promising direction for improving generative model performance\nby allocating additional computation at inference time. While TTS has\ndemonstrated significant success across multiple language tasks, there remains\na notable gap in understanding the test-time scaling behaviors of image and\nvideo generative models (diffusion-based or flow-based models). Although recent\nworks have initiated exploration into inference-time strategies for vision\ntasks, these approaches face critical limitations: being constrained to\ntask-specific domains, exhibiting poor scalability, or falling into reward\nover-optimization that sacrifices sample diversity. In this paper, we propose\nEvolutionary Search (EvoSearch), a novel, generalist, and\nefficient TTS method that effectively enhances the scalability of both image\nand video generation across diffusion and flow models, without requiring\nadditional training or model expansion. EvoSearch reformulates test-time\nscaling for diffusion and flow models as an evolutionary search problem,\nleveraging principles from biological evolution to efficiently explore and\nrefine the denoising trajectory. By incorporating carefully designed selection\nand mutation mechanisms tailored to the stochastic differential equation\ndenoising process, EvoSearch iteratively generates higher-quality offspring\nwhile preserving population diversity. Through extensive evaluation across both\ndiffusion and flow architectures for image and video generation tasks, we\ndemonstrate that our method consistently outperforms existing approaches,\nachieves higher diversity, and shows strong generalizability to unseen\nevaluation metrics. Our project is available at the website\nhttps://tinnerhrhe.github.io/evosearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667187ba9ab144eb3ac43a1b",
      "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
      "fullname": "Runze Liu",
      "name": "RyanLiu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15692",
      "authors": [
        {
          "_id": "68306ffdff038ca6400a153a",
          "user": {
            "_id": "6747de57f8cab58c22ec94a2",
            "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
            "isPro": false,
            "fullname": "Jinyang Wu",
            "user": "Jinyang23",
            "type": "user"
          },
          "name": "Jinyang Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:13:57.397Z",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153b",
          "user": {
            "_id": "667fdaee20ee9ac417c7708c",
            "avatarUrl": "/avatars/69dfba6ff392643af1dcfe8af0a42ae9.svg",
            "isPro": false,
            "fullname": "Chonghua Liao",
            "user": "ChonghuaLiao",
            "type": "user"
          },
          "name": "Chonghua Liao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:16:00.056Z",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153c",
          "name": "Mingkuan Feng",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153d",
          "name": "Shuai Zhang",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153e",
          "name": "Zhengqi Wen",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153f",
          "name": "Pengpeng Shao",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a1540",
          "name": "Huazhe Xu",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a1541",
          "name": "Jianhua Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T16:06:10.000Z",
      "submittedOnDailyAt": "2025-05-26T01:19:22.736Z",
      "title": "\"Optimización de la Política de Sindre: Una Amplia Conexión entre Guías Exteriores y Capacidades Internas\"",
      "submittedOnDailyBy": {
        "_id": "6747de57f8cab58c22ec94a2",
        "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
        "isPro": false,
        "fullname": "Jinyang Wu",
        "user": "Jinyang23",
        "type": "user"
      },
      "summary": "El aprendizaje por refuerzo (RL) ha aparecido como un método efectivo para el entrenamiento de modelos de lógica, pero los enfoques actuales de RL suelen orientar la distribución de salidas hacia el camino de maximización de la recompensa, sin incorporar conocimientos externos. Esto limita la capacidad de exploración y restringe el desarrollo de habilidades lógicas en comparación con modelos básicos. Para resolver estos limitaciones, proponemos un nuevo marco llamado TAPO (Optimización de Política por Agregación de Patrones de Pensamiento). TAPO fortalece el RL mediante la integración de guías de alto nivel externas, conocidas como \"patrones de pensamiento\". Al adaptar estructurados pensamientos durante el entrenamiento, TAPO equilibra mejor la exploración interna y la utilización de las guías externas. A través de experimentos ampliados, nuestro enfoque ha mejorado el rendimiento en AIME en más del 99%, en AMC en 41% y en Minerva Math en 17%. Aunque estos patrones de pensamiento se han abstractado a partir de 500 muestras, se han generalizado ampliamente en diferentes tareas y modelos, demostrando la aplicabilidad de TAPO en diversos dominios y tareas. Nuestras análisis en curso indican que la introducción de guías externas mejora significativamente la explicabilidad de las acciones de inferencia y la comprensión de los resultados, permitiendo la creación de modelos lógicos fuertes.",
      "upvotes": 11,
      "discussionId": "68306ffeff038ca6400a1569",
      "ai_summary": "A novel RL framework, TAPO, integrates external guidance to enhance model performance and exploration compared to existing methods.",
      "ai_keywords": [
        "reinforcement learning",
        "TAPO",
        "Thought-Augmented Policy Optimization",
        "high-level guidance",
        "thought patterns",
        "model exploration",
        "AIME",
        "AMC",
        "Minerva Math",
        "reasoning models",
        "explainability",
        "output readability"
      ]
    },
    "publishedAt": "2025-05-21T12:06:10.000Z",
    "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities",
    "summary": "Reinforcement learning (RL) has emerged as an effective method for training\nreasoning models. However, existing RL approaches typically bias the model's\noutput distribution toward reward-maximizing paths without introducing external\nknowledge. This limits their exploration capacity and results in a narrower\nreasoning capability boundary compared to base models. To address this\nlimitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel\nframework that augments RL by incorporating external high-level guidance\n(\"thought patterns\"). By adaptively integrating structured thoughts during\ntraining, TAPO effectively balances model-internal exploration and external\nguidance exploitation. Extensive experiments show that our approach\nsignificantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva\nMath. Notably, these high-level thought patterns, abstracted from only 500\nprior samples, generalize effectively across various tasks and models. This\nhighlights TAPO's potential for broader applications across multiple tasks and\ndomains. Our further analysis reveals that introducing external guidance\nproduces powerful reasoning models with superior explainability of inference\nbehavior and enhanced output readability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15692.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6747de57f8cab58c22ec94a2",
      "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
      "fullname": "Jinyang Wu",
      "name": "Jinyang23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17399",
      "authors": [
        {
          "_id": "6833fd69fe87d9433d098068",
          "name": "Haoyu Sun",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d098069",
          "name": "Huichen Will Wang",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d09806a",
          "user": {
            "_id": "645b4819f9d4ec91fdd54852",
            "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
            "isPro": false,
            "fullname": "Kuvvi Gu",
            "user": "Kuvvi",
            "type": "user"
          },
          "name": "Jiawei Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:01.542Z",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d09806b",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d09806c",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T02:16:11.000Z",
      "submittedOnDailyAt": "2025-05-26T08:05:22.618Z",
      "title": "FullFront: FullFront: Workflow de la Neurociencia Frontal MLLM Benchmark",
      "submittedOnDailyBy": {
        "_id": "645b4819f9d4ec91fdd54852",
        "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
        "isPro": false,
        "fullname": "Kuvvi Gu",
        "user": "Kuvvi",
        "type": "user"
      },
      "summary": "Front-end engineering es un flujo de trabajo complejo que incluye la conceptualización de diseños, su traducción a código y la mejora iterativa de la implementación. Los recientes benchmarks han centrado principalmente en la conversión de diseños visuales a código, pero presentamos \"FullFront\", un benchmark que evalúa el proceso completo del desarrollo de una pantalla. FullFront evalúa tres tareas básicas: el diseño de página web (etapa de conceptualización), la prueba de accesibilidad y QA (comprensión de la organización visual y de los elementos), y la generación de código de página (etapa de ejecución). Los actuales benchmarks utilizan códigos extensos, haciendo uso de scraping de sitios web o HTML simplificados generados por LLMs, mientras que FullFront utiliza un nuevo proceso de dos etapas para convertir páginas web reales en HTML claro y estándar, manteniendo diversos diseños visuales y evitando problemas de derechos de autor. Los experimentos de expansión de los MLLMs han revelado limitaciones significativas en la reconocimiento de páginas, generación de código (especialmente en procesamiento de imágenes y diseño de layout), y implementación de interacciones. Nuestros resultados muestran diferencias cuantitativas en el rendimiento de los modelos y tareas, y claramente destacan la gran diferencia entre las capacidades actuales de los MLLMs y el rendimiento de un experto en front-end engineering. El benchmark FullFront y su código están disponibles en https://github.com/Mikivishy/FullFront.",
      "upvotes": 10,
      "discussionId": "6833fd6bfe87d9433d0980c2",
      "githubRepo": "https://github.com/Mikivishy/FullFront",
      "ai_summary": "FullFront is a benchmark evaluating Multimodal Large Language Models across conceptualization, comprehension, and implementation phases in front-end engineering.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "Webpage Design",
        "Webpage Perception QA",
        "Webpage Code Generation",
        "front-end engineering"
      ]
    },
    "publishedAt": "2025-05-22T22:16:11.000Z",
    "title": "FullFront: Benchmarking MLLMs Across the Full Front-End Engineering\n  Workflow",
    "summary": "Front-end engineering involves a complex workflow where engineers\nconceptualize designs, translate them into code, and iteratively refine the\nimplementation. While recent benchmarks primarily focus on converting visual\ndesigns to code, we present FullFront, a benchmark designed to evaluate\nMultimodal Large Language Models (MLLMs) across the full front-end\ndevelopment pipeline. FullFront assesses three fundamental tasks that map\ndirectly to the front-end engineering pipeline: Webpage Design\n(conceptualization phase), Webpage Perception QA (comprehension of visual\norganization and elements), and Webpage Code Generation (implementation phase).\nUnlike existing benchmarks that use either scraped websites with bloated code\nor oversimplified LLM-generated HTML, FullFront employs a novel, two-stage\nprocess to transform real-world webpages into clean, standardized HTML while\nmaintaining diverse visual designs and avoiding copyright issues. Extensive\ntesting of state-of-the-art MLLMs reveals significant limitations in page\nperception, code generation (particularly for image handling and layout), and\ninteraction implementation. Our results quantitatively demonstrate performance\ndisparities across models and tasks, and highlight a substantial gap between\ncurrent MLLM capabilities and human expert performance in front-end\nengineering. The FullFront benchmark and code are available in\nhttps://github.com/Mikivishy/FullFront.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17399.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b4819f9d4ec91fdd54852",
      "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
      "fullname": "Kuvvi Gu",
      "name": "Kuvvi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14669",
      "authors": [
        {
          "_id": "682da9d3781210358218a950",
          "name": "Roberto L. Castro",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a951",
          "name": "Andrei Panferov",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a952",
          "name": "Soroush Tabesh",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a953",
          "name": "Oliver Sieberling",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a954",
          "name": "Jiale Chen",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a955",
          "name": "Mahdi Nikdan",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a956",
          "name": "Saleh Ashkboos",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a957",
          "name": "Dan Alistarh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/N9QxR8-CzRd2UcOj9uwRR.png"
      ],
      "publishedAt": "2025-05-20T17:55:50.000Z",
      "submittedOnDailyAt": "2025-05-26T08:34:57.302Z",
      "title": "Capitán: Entrenamiento nativo FP4 es la opción más adecuada para modelos de lenguaje de gran escala.",
      "submittedOnDailyBy": {
        "_id": "623753b5eddd7763adc9346a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
        "isPro": false,
        "fullname": "Andrei Panferov",
        "user": "BlackSamorez",
        "type": "user"
      },
      "summary": "El rápido desarrollo de los modelos de lenguaje grande (LLMs) se está realizando junto con un aumento sin precedentes de las demandas de cálculo. El costo de entrenamiento de los modelos más avanzados duplica en pocos meses. La utilización directa de cálculos de baja precisión para entrenar los modelos es una solución que mejora tanto la velocidad de cálculo como la eficiencia energética. En particular, la arquitectura Blackwell reciente de NVIDIA promueve especialmente la operación de baja precisión FP4 y busca un gran aumento de eficiencia. Sin embargo, los algoritmos utilizados actualmente para el entrenamiento de precisión FP4 en los LLMs presentan grandes problemas y generalmente dependen de retrocesos de precisión mixta. En este artículo, se investiga sistemáticamente el entrenamiento con FP4 soportado por hardware y se presenta un nuevo enfoque llamado Quartet. Este enfoque permite entrenar de manera precisa desde el principio con FP4, haciendo que todas las principales operaciones (por ejemplo, capas lineales) se realicen a baja precisión. Mediante evaluaciones de extensión para modelos de tipo Llama, se cuantifica el equilibrio entre rendimiento y cambios en la profundidad de bits, y se descubre un nuevo escalador que define un método de entrenamiento de baja precisión que se denomina aproximación óptima. Esto se llama Quartet, se implementa utilizando CUDA caneros optimizados para NVIDIA Blackwell GPU y se muestra que puede alcanzar la precisión más avanzada en FP4 y entrenar exitosamente modelos de brillantes escala. Nuestro método es una competencia con el entrenamiento de precisión estándar y FP8, y muestra un entrenamiento completo basado en FP4. Nuestro código está disponible en https://github.com/IST-DASLab/Quartet.",
      "upvotes": 10,
      "discussionId": "682da9d4781210358218a982",
      "ai_summary": "Quartet, a hardware-supported FP4 training approach for large language models, demonstrates state-of-the-art accuracy while significantly reducing computational costs compared to standard or FP8 precision.",
      "ai_keywords": [
        "large language models",
        "low-precision arithmetic",
        "Blackwell architecture",
        "FP4",
        "mixed-precision",
        "linear layers",
        "low-precision scaling law",
        "CUDA kernels"
      ]
    },
    "publishedAt": "2025-05-20T13:55:50.000Z",
    "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
    "summary": "The rapid advancement of large language models (LLMs) has been paralleled by\nunprecedented increases in computational demands, with training costs for\nstate-of-the-art models doubling every few months. Training models directly in\nlow-precision arithmetic offers a solution, by improving both computational\nthroughput and energy efficiency. Specifically, NVIDIA's recent Blackwell\narchitecture facilitates extremely low-precision operations, specifically FP4\nvariants, promising substantial efficiency gains. Yet, current algorithms for\ntraining LLMs in FP4 precision face significant accuracy degradation and often\nrely on mixed-precision fallbacks. In this paper, we systematically investigate\nhardware-supported FP4 training and introduce Quartet, a new approach enabling\naccurate, end-to-end FP4 training with all the major computations (in e.g.\nlinear layers) being performed in low precision. Through extensive evaluations\non Llama-type models, we reveal a new low-precision scaling law that quantifies\nperformance trade-offs across varying bit-widths and allows us to identify a\n\"near-optimal\" low-precision training technique in terms of\naccuracy-vs-computation, called Quartet. We implement Quartet using optimized\nCUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve\nstate-of-the-art accuracy for FP4 precision, successfully training\nbillion-scale models. Our method demonstrates that fully FP4-based training is\na competitive alternative to standard-precision and FP8 training. Our code is\navailable at https://github.com/IST-DASLab/Quartet.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/N9QxR8-CzRd2UcOj9uwRR.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623753b5eddd7763adc9346a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
      "fullname": "Andrei Panferov",
      "name": "BlackSamorez",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17558",
      "authors": [
        {
          "_id": "6833c8af029c4a53a60a5dfa",
          "user": {
            "_id": "648749094dea003c6dae810f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
            "isPro": false,
            "fullname": "Shrey Pandit",
            "user": "SP2001",
            "type": "user"
          },
          "name": "Shrey Pandit",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-26T01:49:36.568Z",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfb",
          "user": {
            "_id": "62fa7294363251ee40a41dba",
            "avatarUrl": "/avatars/869c6de9a1cb2ded690ae56559916cae.svg",
            "isPro": false,
            "fullname": "Ashwin V",
            "user": "ashwinnv",
            "type": "user"
          },
          "name": "Ashwin Vinod",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:22.347Z",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfc",
          "name": "Liu Leqi",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfd",
          "name": "Ying Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T07:05:09.000Z",
      "submittedOnDailyAt": "2025-05-26T00:20:35.511Z",
      "title": "Librería para enseñanza: detección de imágenes basada en datos de sonidos sintéticos en el currículo DPO",
      "submittedOnDailyBy": {
        "_id": "648749094dea003c6dae810f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
        "isPro": false,
        "fullname": "Shrey Pandit",
        "user": "SP2001",
        "type": "user"
      },
      "summary": "El aviso de noticias de desastres en lenguaje general (LLMs) sigue siendo un problema grave debido a las características complejas de los textos de desastres. Los ejemplos de desastres presentan una calidad falsa más alta que los ejemplos tradicionales de negativos, y estos bien planeados son utilizados como ejemplos de falsos en los procedimientos de DPO. Nuestro método introduce un paso de entrenamiento intermedio en el CLE, adaptándose de manera gradual desde muestras más sencillas a muestras más difíciles basándose en la reducción máxima de los puntajes de verificación de hechos obtenidos de un modelo independiente. Esta escalación estructurada de dificultad garantiza un aprendizaje estable y gradual. Según la evaluación experimental, nuestro modelo HaluCheck, entrenado con el enfoque DPO CLE y muestras de falsos de alta calidad, mejoró significativamente el rendimiento del modelo en diferentes métricas, con un aumento de aproximadamente 24% en los benchmarks difíciles de MedHallu y HaluEval. Además, el modelo HaluCheck muestra robustez en la configuración de 0 shot y supera significativamente a los modelos más recientes en diversos benchmarks.",
      "upvotes": 9,
      "discussionId": "6833c8b0029c4a53a60a5e3a",
      "ai_summary": "The use of carefully crafted hallucinations in a curriculum learning approach within the DPO alignment procedure significantly enhances LLMs' hallucination detection abilities.",
      "ai_keywords": [
        "LLMs",
        "hallucinations",
        "DPO alignment procedure",
        "curriculum learning",
        "probability scores",
        "fact checking models",
        "HaluCheck models",
        "MedHallu",
        "HaluEval",
        "zero-shot settings"
      ]
    },
    "publishedAt": "2025-05-23T03:05:09.000Z",
    "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for\n  Hallucination Detection",
    "summary": "Aligning large language models (LLMs) to accurately detect hallucinations\nremains a significant challenge due to the sophisticated nature of hallucinated\ntext. Recognizing that hallucinated samples typically exhibit higher deceptive\nquality than traditional negative samples, we use these carefully engineered\nhallucinations as negative examples in the DPO alignment procedure. Our method\nincorporates a curriculum learning strategy, gradually transitioning the\ntraining from easier samples, identified based on the greatest reduction in\nprobability scores from independent fact checking models, to progressively\nharder ones. This structured difficulty scaling ensures stable and incremental\nlearning. Experimental evaluation demonstrates that our HaluCheck models,\ntrained with curriculum DPO approach and high quality negative samples,\nsignificantly improves model performance across various metrics, achieving\nimprovements of upto 24% on difficult benchmarks like MedHallu and HaluEval.\nAdditionally, HaluCheck models demonstrate robustness in zero-shot settings,\nsignificantly outperforming larger state-of-the-art models across various\nbenchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17558.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648749094dea003c6dae810f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
      "fullname": "Shrey Pandit",
      "name": "SP2001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16479",
      "authors": [
        {
          "_id": "682fdc63bf762029ddcad451",
          "user": {
            "_id": "6640c647acae6bb179eedff5",
            "avatarUrl": "/avatars/bcaafaaa1d4b4c241d72a886401772e3.svg",
            "isPro": false,
            "fullname": "Yuetong Liu",
            "user": "YuetongLiu",
            "type": "user"
          },
          "name": "Yuetong Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:16:39.979Z",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad452",
          "user": {
            "_id": "646c77911ee398a4e9404b8b",
            "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
            "isPro": false,
            "fullname": "Yunqiu Xu",
            "user": "Yunqiu",
            "type": "user"
          },
          "name": "Yunqiu Xu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T04:01:10.107Z",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad453",
          "name": "Yang Wei",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad454",
          "name": "Xiuli Bi",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad455",
          "name": "Bin Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T10:06:35.000Z",
      "submittedOnDailyAt": "2025-05-26T05:13:10.671Z",
      "title": "Claros Noches Ahora: Hacia la Restauración de Imágenes Nocturnas en Tiempos de Clima Diverso",
      "submittedOnDailyBy": {
        "_id": "646c77911ee398a4e9404b8b",
        "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
        "isPro": false,
        "fullname": "Yunqiu Xu",
        "user": "Yunqiu",
        "type": "user"
      },
      "summary": "Las investigaciones sobre la pérdida causada por efectos de la mala meteorología en imágenes nocturnas son prácticas pero carecen de un amplio estudio. La mala meteorología existe con gran iluminación durante la noche. En este estudio, se revisa el problema de la creación de imágenes nocturnas que combinan el impacto de la mala meteorología y el efecto de la llama. Además, proporcionaremos todos los conjuntos de datos de Waiver. Este conjunto de datos incluye imágenes nocturnas de alta calidad y diferentes tipos de daños estructurales. Estas imágenes fueron sintetizadas mediante el daño generado por la iluminación que proponemos. También presentaremos nuestro marco de trabajo unificado para la creación de imágenes nocturnas llamado ClearNight. Este marco de trabajo es capaz de eliminar complejos daños de manera simultánea. En particular, ClearNight extrae líneas de borde basadas en ritmos y enfoca en regiones de iluminación inecuada y contenido de textura propio, lo que proporciona un alto efecto en las noches. Además, proponeremos un método de cooperación específico-común para mejorar la representación de las similitudes y diferencias entre diferentes climas. Este método permite identificar el daño causado por la mala meteorología y seleccionar unidades óptimas relacionadas con el clima. Nuestro ClearNight presenta los mejores resultados en imágenes sintéticas y reales. Los experimentos de eliminación detallados demuestran la necesidad de todos los conjuntos de datos de Waiver y la efectividad de ClearNight. El sitio web del proyecto está disponible en https://henlyta.github.io/ClearNight/mainpage.html.",
      "upvotes": 9,
      "discussionId": "682fdc67bf762029ddcad58c",
      "projectPage": "https://henlyta.github.io/ClearNight/mainpage.html",
      "githubRepo": "https://github.com/henlyta/ClearNight",
      "ai_summary": "A unified framework for restoring nighttime images under diverse weather conditions using dual priors and adaptive collaboration.",
      "ai_keywords": [
        "Retinex-based dual priors",
        "illumination-aware degradation generation",
        "weather-aware dynamic specific-commonality collaboration",
        "nighttime image restoration"
      ]
    },
    "publishedAt": "2025-05-22T06:06:35.000Z",
    "title": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration",
    "summary": "Restoring nighttime images affected by multiple adverse weather conditions is\na practical yet under-explored research problem, as multiple weather conditions\noften coexist in the real world alongside various lighting effects at night.\nThis paper first explores the challenging multi-weather nighttime image\nrestoration task, where various types of weather degradations are intertwined\nwith flare effects. To support the research, we contribute the AllWeatherNight\ndataset, featuring large-scale high-quality nighttime images with diverse\ncompositional degradations, synthesized using our introduced illumination-aware\ndegradation generation. Moreover, we present ClearNight, a unified nighttime\nimage restoration framework, which effectively removes complex degradations in\none go. Specifically, ClearNight extracts Retinex-based dual priors and\nexplicitly guides the network to focus on uneven illumination regions and\nintrinsic texture contents respectively, thereby enhancing restoration\neffectiveness in nighttime scenarios. In order to better represent the common\nand unique characters of multiple weather degradations, we introduce a\nweather-aware dynamic specific-commonality collaboration method, which\nidentifies weather degradations and adaptively selects optimal candidate units\nassociated with specific weather types. Our ClearNight achieves\nstate-of-the-art performance on both synthetic and real-world images.\nComprehensive ablation experiments validate the necessity of AllWeatherNight\ndataset as well as the effectiveness of ClearNight. Project page:\nhttps://henlyta.github.io/ClearNight/mainpage.html",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16479.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646c77911ee398a4e9404b8b",
      "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
      "fullname": "Yunqiu Xu",
      "name": "Yunqiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16483",
      "authors": [
        {
          "_id": "6833cb27e10e89e250a6d9ae",
          "name": "Shuzheng Si",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9af",
          "user": {
            "_id": "63ff09a02098b9ad105a09f6",
            "avatarUrl": "/avatars/4409ca5d320050cf4c3df05962c7ff58.svg",
            "isPro": false,
            "fullname": "Hans Zhao",
            "user": "BleachNick",
            "type": "user"
          },
          "name": "Haozhe Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:14.872Z",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b0",
          "name": "Cheng Gao",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b1",
          "name": "Yuzhuo Bai",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b2",
          "name": "Zhitong Wang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b3",
          "name": "Bofei Gao",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b4",
          "name": "Kangyang Luo",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b5",
          "name": "Wenhao Li",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b6",
          "name": "Yufei Huang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b7",
          "name": "Gang Chen",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b8",
          "name": "Fanchao Qi",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b9",
          "name": "Minjia Zhang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9ba",
          "name": "Baobao Chang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9bb",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T10:10:07.000Z",
      "submittedOnDailyAt": "2025-05-26T00:36:30.930Z",
      "title": "Utilizando tareas de síntesis y aprendizaje por refuerzo para mantener la dependencia contextual de modelos de lenguaje de gran escala",
      "submittedOnDailyBy": {
        "_id": "637c99bbfe115289cfedfb44",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
        "isPro": false,
        "fullname": "ssz",
        "user": "ssz1111",
        "type": "user"
      },
      "summary": "La construcción de un sistema de exploración de información confiable es esencial para proporcionar respuestas precisas a los modelos de lenguaje de texto (LLMs) como el Teocodoro Jungle Model (LLMs) en el contexto proporcionado. Por consiguiente, proponemos un marco sistemático llamado CANOE para mejorar la precisión en tareas de generación corta y larga, sin el uso de anóticación humana. Concretamente, hemos sintetizado datos de respuestas a preguntas cortas que incluyen cuatro tareas diferentes, y hemos construido un conjunto de datos de entrenamiento de alta calidad y fácil verificación, excluyendo la anóticación humana. Además, proponemos un método de aprendizaje por refuerzo basado en reglas de recompensa, Dual-GRPO, que incluye tres reglas de recompensa basadas en los datos de respuestas a preguntas cortas sin anóticación humana. Este método optimiza tanto la generación de respuestas cortas como largas. En particular, Dual-GRPO no requiere etiquetados datos manuales directos para el entrenamiento de modelos de recompensa, sino que depende exclusivamente de los datos de respuestas a preguntas cortas sin anóticación humana, evitando así la optimización excesiva de la generación corta. Los resultados de los experimentos muestran que CANOE mejora significativamente la precisión de los LLMs en 11 tareas de generación de respuestas a preguntas, demostrando que supera a los modelos más recientes de GPT-4o y OpenAI o1.",
      "upvotes": 8,
      "discussionId": "6833cb28e10e89e250a6da0a",
      "projectPage": "https://github.com/S1s-Z/CANOE",
      "githubRepo": "https://github.com/S1s-Z/CANOE",
      "ai_summary": "CANOE improves LLM faithfulness in generation tasks using synthetic QA data and Dual-GRPO reinforcement learning without human annotations.",
      "ai_keywords": [
        "teaching large language models",
        "faithfulness",
        "context",
        "CANOE",
        "short-form generation",
        "long-form generation",
        "question-answering",
        "Dual-GRPO",
        "rule-based reinforcement learning",
        "preference data",
        "reward models"
      ]
    },
    "publishedAt": "2025-05-22T06:10:07.000Z",
    "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via\n  Synthetic Tasks and Reinforcement Learning",
    "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16483.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "637c99bbfe115289cfedfb44",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
      "fullname": "ssz",
      "name": "ssz1111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13508",
      "authors": [
        {
          "_id": "683148c0018bba5b656c94e3",
          "user": {
            "_id": "65d188a4aa309d842e438ef1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
            "isPro": false,
            "fullname": "Zijia Liu",
            "user": "m-serious",
            "type": "user"
          },
          "name": "Zijia Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:18.318Z",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e4",
          "name": "Peixuan Han",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e5",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e6",
          "name": "Haoru Li",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e7",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T13:46:28.000Z",
      "submittedOnDailyAt": "2025-05-26T06:47:47.579Z",
      "title": "Tiempo R1: Discusión sobre soluciones previas a la lógica de series temporales",
      "submittedOnDailyBy": {
        "_id": "65d188a4aa309d842e438ef1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
        "isPro": false,
        "fullname": "Zijia Liu",
        "user": "m-serious",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) han demostrado capacidades sorprendentes, pero su conocimiento temporal es débil y es difícil que los modelos piensen en causas pasadas o predeen futuros de manera adecuada. Además, los métodos actuales se centran generalmente en desarrollar habilidades temporales separadas, incluyendo respuestas a preguntas sobre hechos pasados y predicciones básicas, pero su capacidad para generalizar es limitada, especialmente cuando se trata de eventos más alejados del conocimiento o visión creativa. Para superar estas limitaciones, presentamos Time-R1. Time-R1 es el primer marco de trabajo y proporciona a un modelo de LLM de tamaño intermedio (3B parámetros) habilidades temporales (reconocimiento, predicción, generación creativa). Nuestro enfoque se caracteriza por un nuevo paso de desarrollo en tres etapas, donde las primeras dos etapas están basadas en un sistema de recompensas basado en reglas dinámicas y se utilizan para un currículo de aprendizaje por refuerzo (RL). Este marco permite: (1) una comprensión básica del tiempo a partir de datos históricos y la mapeo de relaciones temporales de eventos, (2) una capacidad para predecir eventos más alejados del conocimiento, y (3) una excelente generalización en la generación de escenarios creativos del futuro. En particular, los experimentos muestran que Time-R1 supera a modelos de más de 200 veces su tamaño, incluyendo el líder en el campo, el 671B DeepSeek-R1, en benchmarks muy difíciles de predicción de eventos futuros y generación de escenarios creativos. Esta investigación es una prueba fuerte de que una precisa ingeniería y un ajuste avanzado de RL pueden lograr excelentes resultados temporales a través de pequeños y eficientes modelos. Además, para facilitar la investigación, hemos publicado Time-Bench, un conjunto de datos de inferencia temporal multitarea a partir de 10 años de datos de noticias, y proporcionamos una serie de checkpoints de Time-R1.",
      "upvotes": 8,
      "discussionId": "683148c1018bba5b656c9511",
      "githubRepo": "https://github.com/ulab-uiuc/Time-R1",
      "ai_summary": "A novel framework, Time-R1, enhances moderate-sized LLMs with comprehensive temporal abilities through a reinforcement learning curriculum, outperforming larger models on future event prediction and creative scenario generation benchmarks.",
      "ai_keywords": [
        "Large Language Models",
        "reinforcement learning",
        "RL curriculum",
        "rule-based reward system",
        "temporal understanding",
        "event-time mappings",
        "future event prediction",
        "creative scenario generation",
        "Time-Bench",
        "Time-R1 checkpoints"
      ]
    },
    "publishedAt": "2025-05-16T09:46:28.000Z",
    "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs",
    "summary": "Large Language Models (LLMs) demonstrate impressive capabilities but lack\nrobust temporal intelligence, struggling to integrate reasoning about the past\nwith predictions and plausible generations of the future. Meanwhile, existing\nmethods typically target isolated temporal skills, such as question answering\nabout past events or basic forecasting, and exhibit poor generalization,\nparticularly when dealing with events beyond their knowledge cutoff or\nrequiring creative foresight. To address these limitations, we introduce\nTime-R1, the first framework to endow a moderate-sized (3B-parameter)\nLLM with comprehensive temporal abilities: understanding, prediction, and\ncreative generation. Our approach features a novel three-stage development\npath; the first two constitute a reinforcement learning (RL)\ncurriculum driven by a meticulously designed dynamic rule-based reward system.\nThis framework progressively builds (1) foundational temporal understanding and\nlogical event-time mappings from historical data, (2) future event prediction\nskills for events beyond its knowledge cutoff, and finally (3) enables\nremarkable generalization to creative future scenario generation without any\nfine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms\nmodels over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,\non highly challenging future event prediction and creative scenario generation\nbenchmarks. This work provides strong evidence that thoughtfully engineered,\nprogressive RL fine-tuning allows smaller, efficient models to achieve superior\ntemporal performance, offering a practical and scalable path towards truly\ntime-aware AI. To foster further research, we also release Time-Bench,\na large-scale multi-task temporal reasoning dataset derived from 10 years of\nnews data, and our series of Time-R1 checkpoints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13508.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65d188a4aa309d842e438ef1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
      "fullname": "Zijia Liu",
      "name": "m-serious",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17417",
      "authors": [
        {
          "_id": "6833d2df73bebebe5cd6604e",
          "user": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "isPro": false,
            "fullname": "Alan Dao",
            "user": "alandao",
            "type": "user"
          },
          "name": "Alan Dao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-26T02:38:31.379Z",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd6604f",
          "name": "Dinh Bach Vu",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66050",
          "name": "Huy Hoang Ha",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66051",
          "name": "Tuan Le Duc Anh",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66052",
          "name": "Shreyas Gopal",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66053",
          "name": "Yue Heng Yeo",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66054",
          "name": "Warren Keng Hoong Low",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66055",
          "name": "Eng Siong Chng",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66056",
          "name": "Jia Qi Yip",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T03:05:47.000Z",
      "submittedOnDailyAt": "2025-05-26T01:03:18.385Z",
      "title": "Lenguaje sin lenguaje de clase: entrenamiento de clase de lenguaje de la lengua original en el entrenamiento de instrucciones sin lenguaje",
      "submittedOnDailyBy": {
        "_id": "62d7b2339b629105a5d6888a",
        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
        "isPro": false,
        "fullname": "Alan Dao",
        "user": "alandao",
        "type": "user"
      },
      "summary": "El rápido crecimiento de los sistemas de asistencia vocal con lenguaje generalizado (LLM) ha claramente mostrado la necesidad de datos de instrucciones vocales para su entrenamiento. Mientras la abundancia de datos de reconocimiento de lenguaje contrasta con la escasez de datos de instrucciones, estos son particularmente importantes para ajustar modelos que puedan entender y ejecutar lenguajes. Para generar una calidad de lenguaje sintético, es necesario un buen modelo de conversión de texto a voz (TTS), pero estos modelos no son visibles en lenguajes con pocos recursos. Nuestro nuevo enfoque resuelve este problema al detenerse en la representación significativa del lenguaje sintético y evitando la necesidad de TTS. Para lograrlo, se combinan las representaciones significativas sintéticas con un codificador pre-entrenado de Whisper y se entrena el modelo LLM para que pueda entrenarse con instrucciones de frases, mientras mantiene la capacidad de comprender instrucciones de lenguaje en el momento de la inferencia. Este proceso de entrenamiento simplificado es una prometedora aproximación para construir sistemas de asistencia vocal en lenguajes con pocos recursos.",
      "upvotes": 6,
      "discussionId": "6833d2df73bebebe5cd66074",
      "githubRepo": "https://github.com/menloresearch/ichigo",
      "ai_summary": "A method bypasses the need for TTS models by aligning semantic representations with a Whisper encoder, enabling LLMs to understand both text and spoken instructions for low-resource languages.",
      "ai_keywords": [
        "large language models",
        "LLM",
        "speech instruction data",
        "TTS",
        "semantic representation",
        "Whisper encoder",
        "fine-tuning",
        "low-resource languages"
      ]
    },
    "publishedAt": "2025-05-22T23:05:47.000Z",
    "title": "Speechless: Speech Instruction Training Without Speech for Low Resource\n  Languages",
    "summary": "The rapid growth of voice assistants powered by large language models (LLM)\nhas highlighted a need for speech instruction data to train these systems.\nDespite the abundance of speech recognition data, there is a notable scarcity\nof speech instruction data, which is essential for fine-tuning models to\nunderstand and execute spoken commands. Generating high-quality synthetic\nspeech requires a good text-to-speech (TTS) model, which may not be available\nto low resource languages. Our novel approach addresses this challenge by\nhalting synthesis at the semantic representation level, bypassing the need for\nTTS. We achieve this by aligning synthetic semantic representations with the\npre-trained Whisper encoder, enabling an LLM to be fine-tuned on text\ninstructions while maintaining the ability to understand spoken instructions\nduring inference. This simplified training process is a promising approach to\nbuilding voice assistant for low-resource languages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d7b2339b629105a5d6888a",
      "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
      "fullname": "Alan Dao",
      "name": "alandao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16770",
      "authors": [
        {
          "_id": "68308e2697d9a81c8521bc6a",
          "user": {
            "_id": "62145614b670cb63a38075ba",
            "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
            "isPro": false,
            "fullname": "MenghaoGuo",
            "user": "MenghaoGuo",
            "type": "user"
          },
          "name": "Meng-Hao Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:57.462Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6b",
          "user": {
            "_id": "66b711f9512dac2ac08bc5e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b711f9512dac2ac08bc5e5/n2kSqNhg-TE56iN_V0xHm.png",
            "isPro": false,
            "fullname": "Xuanyu Chu",
            "user": "CXY07",
            "type": "user"
          },
          "name": "Xuanyu Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:48.661Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6c",
          "name": "Qianrui Yang",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6d",
          "user": {
            "_id": "6816bd8e0499f6c7c7b89601",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6816bd8e0499f6c7c7b89601/-dkIPxjOGbdwDZFxhkBMC.jpeg",
            "isPro": false,
            "fullname": "Zhe-Han Mo",
            "user": "Mo-ZheHan",
            "type": "user"
          },
          "name": "Zhe-Han Mo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:52.048Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6e",
          "name": "Yiqing Shen",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6f",
          "name": "Pei-lin Li",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc70",
          "name": "Xinjie Lin",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc71",
          "name": "Jinnian Zhang",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc72",
          "name": "Xin-Sheng Chen",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc73",
          "user": {
            "_id": "63b2efb5922f26a27e76381c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b2efb5922f26a27e76381c/zOQAt_xywiY8eTvvQOrmQ.png",
            "isPro": false,
            "fullname": "Yi Zhang",
            "user": "uyzhang",
            "type": "user"
          },
          "name": "Yi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:54.379Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc74",
          "name": "Kiyohiro Nakayama",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc75",
          "name": "Zhengyang Geng",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc76",
          "name": "Houwen Peng",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc77",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc78",
          "name": "Shi-Nin Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T15:11:57.000Z",
      "submittedOnDailyAt": "2025-05-26T06:55:20.321Z",
      "title": "RBench-V: Salida multimodal para la evaluación básica de modelos de razonamiento visual",
      "submittedOnDailyBy": {
        "_id": "62145614b670cb63a38075ba",
        "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
        "isPro": false,
        "fullname": "MenghaoGuo",
        "user": "MenghaoGuo",
        "type": "user"
      },
      "summary": "Nota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice\n\nNota Índice",
      "upvotes": 6,
      "discussionId": "68308e2797d9a81c8521bca5",
      "ai_summary": "A benchmark called RBench-V evaluates multi-modal models' vision-indispensable reasoning through image manipulation and auxiliary line construction, demonstrating that current models struggle with multi-modal outputs.",
      "ai_keywords": [
        "multi-modal models",
        "omni-models",
        "GPT-4o",
        "Gemini",
        "o3",
        "multi-modal chain of thought",
        "M-CoT",
        "RBench-V",
        "image manipulation",
        "auxiliary lines"
      ]
    },
    "publishedAt": "2025-05-22T11:11:57.000Z",
    "title": "RBench-V: A Primary Assessment for Visual Reasoning Models with\n  Multi-modal Outputs",
    "summary": "The rapid advancement of native multi-modal models and omni-models,\nexemplified by GPT-4o, Gemini, and o3, with their capability to process and\ngenerate content across modalities such as text and images, marks a significant\nmilestone in the evolution of intelligence. Systematic evaluation of their\nmulti-modal output capabilities in visual thinking processes (also known as\nmulti-modal chain of thought, M-CoT) becomes critically important. However,\nexisting benchmarks for evaluating multi-modal models primarily focus on\nassessing multi-modal inputs and text-only reasoning while neglecting the\nimportance of reasoning through multi-modal outputs. In this paper, we present\na benchmark, dubbed RBench-V, designed to assess models' vision-indispensable\nreasoning abilities. To construct RBench-V, we carefully hand-pick 803\nquestions covering math, physics, counting, and games. Unlike previous\nbenchmarks that typically specify certain input modalities, RBench-V presents\nproblems centered on multi-modal outputs, which require image manipulation such\nas generating novel images and constructing auxiliary lines to support the\nreasoning process. We evaluate numerous open- and closed-source models on\nRBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the\nbest-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below\nthe human score of 82.3%, highlighting that current models struggle to leverage\nmulti-modal reasoning. Data and code are available at\nhttps://evalmodels.github.io/rbenchv",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16770.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62145614b670cb63a38075ba",
      "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
      "fullname": "MenghaoGuo",
      "name": "MenghaoGuo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15389",
      "authors": [
        {
          "_id": "682f518184a99219c4b3090c",
          "user": {
            "_id": "6540fbf9cb7fffd683942b43",
            "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
            "isPro": false,
            "fullname": "DongGeon Lee",
            "user": "oneonlee",
            "type": "user"
          },
          "name": "DongGeon Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:17:11.519Z",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090d",
          "name": "Joonwon Jang",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090e",
          "name": "Jihae Jeong",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090f",
          "name": "Hwanjo Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T11:26:40.000Z",
      "submittedOnDailyAt": "2025-05-26T00:35:59.051Z",
      "title": "El modelo de visión-LUNGRAZ es seguro en su estado natural? Studio de marcos de prueba basado en memoria",
      "submittedOnDailyBy": {
        "_id": "6540fbf9cb7fffd683942b43",
        "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
        "isPro": false,
        "fullname": "DongGeon Lee",
        "user": "oneonlee",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje visual (VLMs) que se están distribuyendo rápidamente amplían el riesgo de seguridad pero la mayoría de las evaluaciones dependen de imágenes artificiales. Este estudio investiga la seguridad de los VLMs frente a imágenes de memes que comparten los usuarios generales. Para ello, introducimos MemeSafetyBench, que es una base de prueba estándar con 50,430 ejemplos, combinando imágenes reales de memes y comandos nocivos o inocentes. Utilizamos métodos de clasificación de seguridad general y la generación de comandos basada en modelos de lenguaje para evaluar varios VLMs con interacciones de un y múltiples ciclos. Estudiamos cómo los memes de la realidad afectan a los comandos nocivos, el efecto de la contexto de conversación, y la relación entre el tamaño del modelo y los indicadores de seguridad. Nuestros resultados muestran que los VLMs son más fácilmente atacados por comandos nocivos basados en memes que por imágenes sintéticas o de impresión. En casos de entrada de texto, los memes aumentan la respuesta nociva y reducen la rechazo. Las interacciones de múltiples ciclos ofrecen alguna mejora, pero el alto riesgo sigue existiendo. Estos resultados subrayan la necesidad de evaluaciones ecológicamente efectivas y la fortalecimiento de mecanismos de seguridad.",
      "upvotes": 6,
      "discussionId": "682f518184a99219c4b30956",
      "ai_summary": "VLMs are more vulnerable to harmful meme-based prompts than to synthetic images, and while multi-turn interactions offer some protection, significant vulnerabilities remain.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "MemeSafetyBench",
        "safety taxonomy",
        "LLM-based instruction generation",
        "single and multi-turn interactions"
      ]
    },
    "publishedAt": "2025-05-21T07:26:40.000Z",
    "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
    "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs show greater vulnerability to\nmeme-based harmful prompts than to synthetic or typographic images. Memes\nsignificantly increase harmful responses and decrease refusals compared to\ntext-only inputs. Though multi-turn interactions provide partial mitigation,\nelevated vulnerability persists. These results highlight the need for\necologically valid evaluations and stronger safety mechanisms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15389.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6540fbf9cb7fffd683942b43",
      "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
      "fullname": "DongGeon Lee",
      "name": "oneonlee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17826",
      "authors": [
        {
          "_id": "6833ce1bd5c438959f750d57",
          "name": "Xuchen Pan",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d58",
          "name": "Yanxi Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d59",
          "name": "Yushuo Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5a",
          "name": "Yuchang Sun",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5b",
          "name": "Daoyuan Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5c",
          "name": "Wenhao Zhang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5d",
          "name": "Yuexiang Xie",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5e",
          "name": "Yilun Huang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5f",
          "name": "Yilei Zhang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d60",
          "name": "Dawei Gao",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d61",
          "name": "Yaliang Li",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d62",
          "name": "Bolin Ding",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d63",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6576f9f4654561a1b345610b/kDRshLvxO0EynfeH0BOZK.png"
      ],
      "publishedAt": "2025-05-23T12:41:09.000Z",
      "submittedOnDailyAt": "2025-05-26T00:50:50.296Z",
      "title": "TRINITY-RFT: Un marco de trabajo unificado para el ajuste de modelos de lenguaje grandes como un marco general de aprendizaje por refuerzo",
      "submittedOnDailyBy": {
        "_id": "6576f9f4654561a1b345610b",
        "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
        "isPro": false,
        "fullname": "Yanxi Chen",
        "user": "yanxi-chen",
        "type": "user"
      },
      "summary": "TRINITY-RFT es un marco general, flexible y escalable adecuado para ajustes micro de aprendizaje reforzado (RFT). Este marco utiliza diseños desacoplados. Los componentes incluyen: (1) el núcleo RFT-core, que permite la normalización y generalización del RFT para sincronizaciones/asincronizaciones, en línea/fuera de línea, y en línea/fuera de línea. (2) la infinita integración de interacción entre agente y entorno, que enfatiza eficiencia y robustez. (3) la configuración de un sistema de flujos de datos optimizados para RFT. TRINITY-RFT puede aplicarse en diversos entornos de aplicación y sirve como un plataforma integrada para explorar paradigmas avanzados de aprendizaje reforzado. En este informe técnico, se describen en detalle el punto de vista, funciones, diseño e implementación de TRINITY-RFT, y se muestran diversos ejemplos para demostrar la utilidad y la amigabilidad con el usuario del propuesto marco.",
      "upvotes": 5,
      "discussionId": "6833ce1cd5c438959f750dab",
      "projectPage": "https://github.com/modelscope/Trinity-RFT",
      "githubRepo": "https://github.com/modelscope/Trinity-RFT",
      "ai_summary": "Trinity-RFT is a flexible and scalable framework for reinforcement fine-tuning of large language models, supporting various interaction modes and data pipelines.",
      "ai_keywords": [
        "reinforcement fine-tuning",
        "RFT-core",
        "synchronous/asynchronous",
        "on-policy/off-policy",
        "online/offline",
        "agent-environment interaction",
        "data pipelines",
        "reinforcement learning paradigms"
      ]
    },
    "publishedAt": "2025-05-23T08:41:09.000Z",
    "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models",
    "summary": "Trinity-RFT is a general-purpose, flexible and scalable framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na decoupled design, consisting of (1) an RFT-core that unifies and generalizes\nsynchronous/asynchronous, on-policy/off-policy, and online/offline modes of\nRFT, (2) seamless integration for agent-environment interaction with high\nefficiency and robustness, and (3) systematic data pipelines optimized for RFT.\nTrinity-RFT can be easily adapted for diverse application scenarios, and serves\nas a unified platform for exploring advanced reinforcement learning paradigms.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples demonstrating the utility and\nuser-friendliness of the proposed framework.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6576f9f4654561a1b345610b/kDRshLvxO0EynfeH0BOZK.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17826.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576f9f4654561a1b345610b",
      "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
      "fullname": "Yanxi Chen",
      "name": "yanxi-chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17508",
      "authors": [
        {
          "_id": "6833cf5a2d728e2330d572e3",
          "user": {
            "_id": "647bf082aba7062fe5c51ca9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
            "isPro": false,
            "fullname": "Yifan Zhang",
            "user": "yifAI",
            "type": "user"
          },
          "name": "Yifan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:09.930Z",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e4",
          "user": {
            "_id": "653d276681f52ceb4d12bd85",
            "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
            "isPro": false,
            "fullname": "Yifeng Liu",
            "user": "Lewis-Lau",
            "type": "user"
          },
          "name": "Yifeng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:06.913Z",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e5",
          "name": "Huizhuo Yuan",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e6",
          "name": "Yang Yuan",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e7",
          "name": "Quanquan Gu",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e8",
          "name": "Andrew C Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/mGUuNpUhjfafWqcJJZ1V1.png"
      ],
      "publishedAt": "2025-05-23T06:01:21.000Z",
      "submittedOnDailyAt": "2025-05-26T00:50:14.655Z",
      "title": "La política de normalización KL para algoritmos genéticos de descenso de gradiente: razones de diseño",
      "submittedOnDailyBy": {
        "_id": "647bf082aba7062fe5c51ca9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
        "isPro": false,
        "fullname": "Yifan Zhang",
        "user": "yifAI",
        "type": "user"
      },
      "summary": "El algoritmo de gradiente de políticas se ha aplicado con éxito para mejorar la capacidad lógica de modelos de lenguaje grandes (LLMs). Sin embargo, la normalización KL se utiliza ampliamente en este algoritmo, lo que ayuda a regular el aprendizaje, pero también implica explorar un espacio de diseño complejo y sistemático, ya que se deben evaluar las diferentes matrices de datos KL y integrarlas como funciones de pérdida en aprendizaje por refuerzo (RL) en línea. En este artículo, se propone un marco sistemático para la generalización y análisis de métodos de políticas normalizadas, llamado Policy Gradient Normalized (RPG). Se revisa tanto las distribuciones de políticas normalizadas como no normalizadas, y se extraen funciones de política y funciones de pérdida de agente relativo que se ajustan a objetivos normalizados con matrices de datos KL bidireccionales y unidireccionales. Además, se realizan cálculos de todas las funciones de pérdida diferenciables y estimaciones de gradiente de tipo REINFORCE, y se ajustan para satisfacer las necesidades de diferentes algoritmos. Estas metodologías se expandieron para llevar a cabo experimentos en RL de lógica de LLMs, demostrando resultados excelentes o competitivos en términos de estabilidad del aprendizaje y rendimiento. El código está disponible en https://github.com/complex-reasoning/RPG.",
      "upvotes": 4,
      "discussionId": "6833cf5b2d728e2330d57313",
      "projectPage": "https://complex-reasoning.github.io/RPG",
      "githubRepo": "https://github.com/complex-reasoning/RPG",
      "ai_summary": "A regularized policy gradient framework is introduced to explore KL divergence formulations for enhancing the reasoning capabilities of LLMs in online reinforcement learning, demonstrating improved training stability and performance.",
      "ai_keywords": [
        "policy gradient algorithms",
        "KL regularization",
        "KL divergence",
        "surrogate loss functions",
        "online reinforcement learning",
        "full differentiable loss functions",
        "REINFORCE-style gradient estimators",
        "GRPO",
        "REINFORCE++",
        "DAPO",
        "regularized policy gradient (RPG)",
        "forward KL divergence",
        "reverse KL divergence",
        "normalized policy distributions",
        "unnormalized policy distributions"
      ]
    },
    "publishedAt": "2025-05-23T02:01:21.000Z",
    "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM\n  Reasoning",
    "summary": "Policy gradient algorithms have been successfully applied to enhance the\nreasoning capabilities of large language models (LLMs). Despite the widespread\nuse of Kullback-Leibler (KL) regularization in policy gradient algorithms to\nstabilize training, the systematic exploration of how different KL divergence\nformulations can be estimated and integrated into surrogate loss functions for\nonline reinforcement learning (RL) presents a nuanced and systematically\nexplorable design space. In this paper, we propose regularized policy gradient\n(RPG), a systematic framework for deriving and analyzing KL-regularized policy\ngradient methods in the online RL setting. We derive policy gradients and\ncorresponding surrogate loss functions for objectives regularized by both\nforward and reverse KL divergences, considering both normalized and\nunnormalized policy distributions. Furthermore, we present derivations for\nfully differentiable loss functions as well as REINFORCE-style gradient\nestimators, accommodating diverse algorithmic needs. We conduct extensive\nexperiments on RL for LLM reasoning using these methods, showing improved or\ncompetitive results in terms of training stability and performance compared to\nstrong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at\nhttps://github.com/complex-reasoning/RPG.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/mGUuNpUhjfafWqcJJZ1V1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647bf082aba7062fe5c51ca9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
      "fullname": "Yifan Zhang",
      "name": "yifAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17412",
      "authors": [
        {
          "_id": "6833e93697966d18e7c1e4d7",
          "name": "Shuang Wu",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4d8",
          "name": "Youtian Lin",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4d9",
          "name": "Feihu Zhang",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4da",
          "name": "Yifei Zeng",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4db",
          "name": "Yikang Yang",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4dc",
          "name": "Yajie Bao",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4dd",
          "name": "Jiachen Qian",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4de",
          "name": "Siyu Zhu",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4df",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4e0",
          "name": "Xun Cao",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4e1",
          "name": "Yao Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T02:58:01.000Z",
      "submittedOnDailyAt": "2025-05-26T03:12:06.518Z",
      "title": "Direct3D-S2: Facilita la generación de escalas de gigantes 3D con la atención espectral sparce.",
      "submittedOnDailyBy": {
        "_id": "645a24779f06c5897254d14b",
        "avatarUrl": "/avatars/dd0a635674025dcc9a94ee0f4c952083.svg",
        "isPro": false,
        "fullname": "Youtian Lin",
        "user": "LoYoT",
        "type": "user"
      },
      "summary": "Direct3D S2 es un framework de generación de 3D que utiliza representaciones volumétricas como funciones de distancia signed para crear 3D Shapes, pero enfrenta problemas significativos de cálculo y memoria al generar formas 3D de alta resolución. Hemos introducido una estructura de atención espacialmente esparsa dentro de volúmenes esparsos eficientes para procesar un gran conjunto de tokens, lo que ha aumentado la velocidad de propagación en un 3.9 veces (propagación) y 9.6 veces (retropropagación), y reducido significativamente el overhead de cálculo. Además, incluimos un encoder distribuido que mantiene el formato de volúmenes esparsos consistente en las etapas de entrada, potencial y salida, lo que ha mejorado significativamente la eficiencia y estabilidad del entrenamiento en comparación con métodos anteriores que utilizaban otras representaciones de VAE 3D. Nuestro modelo se ha entrenado con conjuntos de datos disponibles, y los experimentos muestran que Direct3D S2 supera a los métodos más avanzados en cuanto a calidad de generación y eficiencia, logrando generar formas 3D de 1024 resolución con solo 8 kernels, y realizar representaciones volumétricas de 256 resolución con un número de kernels significativamente menor a los necesarios anteriormente, haciendo que la generación de 3D sea práctica y accesible. Página del proyecto: https://nju3dv.github.io/projects/Direct3D-S2/",
      "upvotes": 4,
      "discussionId": "6833e93b97966d18e7c1e676",
      "projectPage": "https://nju-3dv.github.io/projects/Direct3D-S2/",
      "githubRepo": "https://github.com/DreamTechAI/Direct3D-S2",
      "ai_summary": "A scalable 3D shape generation framework using sparse volumes and spatial sparse attention, enabling high-resolution generation with reduced computational requirements.",
      "ai_keywords": [
        "Signed Distance Functions",
        "sparse volumes",
        "Spatial Sparse Attention",
        "Diffusion Transformer",
        "variational autoencoder",
        "gigascale 3D generation"
      ]
    },
    "publishedAt": "2025-05-22T22:58:01.000Z",
    "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse\n  Attention",
    "summary": "Generating high resolution 3D shapes using volumetric representations such as\nSigned Distance Functions presents substantial computational and memory\nchallenges. We introduce Direct3D S2, a scalable 3D generation framework based\non sparse volumes that achieves superior output quality with dramatically\nreduced training costs. Our key innovation is the Spatial Sparse Attention\nmechanism, which greatly enhances the efficiency of Diffusion Transformer\ncomputations on sparse volumetric data. SSA allows the model to effectively\nprocess large token sets within sparse volumes, significantly reducing\ncomputational overhead and achieving a 3.9x speedup in the forward pass and a\n9.6x speedup in the backward pass. Our framework also includes a variational\nautoencoder that maintains a consistent sparse volumetric format across input,\nlatent, and output stages. Compared to previous methods with heterogeneous\nrepresentations in 3D VAE, this unified design significantly improves training\nefficiency and stability. Our model is trained on public available datasets,\nand experiments demonstrate that Direct3D S2 not only surpasses\nstate-of-the-art methods in generation quality and efficiency, but also enables\ntraining at 1024 resolution using only 8 GPUs, a task typically requiring at\nleast 32 GPUs for volumetric representations at 256 resolution, thus making\ngigascale 3D generation both practical and accessible. Project page:\nhttps://nju3dv.github.io/projects/Direct3D-S2/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17412.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645a24779f06c5897254d14b",
      "avatarUrl": "/avatars/dd0a635674025dcc9a94ee0f4c952083.svg",
      "fullname": "Youtian Lin",
      "name": "LoYoT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17091",
      "authors": [
        {
          "_id": "6833cb25fe87d9433dfd2b1c",
          "name": "Prateek Verma",
          "hidden": false
        },
        {
          "_id": "6833cb25fe87d9433dfd2b1d",
          "name": "Mert Pilanci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T22:20:16.000Z",
      "submittedOnDailyAt": "2025-05-26T00:30:41.320Z",
      "title": "Los modelos de lenguaje grandes están aprendiendo las habilidades visual y auditiva automáticamente a través de la lectura.",
      "submittedOnDailyBy": {
        "_id": "62d7f1119b629105a5d84aad",
        "avatarUrl": "/avatars/c74045063e7c06cb7be0fa41ebb1d824.svg",
        "isPro": false,
        "fullname": "Prateek Verma",
        "user": "prateekv",
        "type": "user"
      },
      "summary": "En este artículo se presenta un interesante hallazgo: se entrena un modelo de LLM de recuperación automática para palabras, lo que permite al modelo de palabras desarrollar la capacidad de comprender imágenes y sonidos internamente y adquirir habilidades en lectura y escucha. Los modelos de LLM avanzados de sonido y visión proporcionan salidas de palabras basadas en los embeddings de imágenes y sonidos, fine-tuniendo así los modelos de palabras de texto. Por otro lado, nuestra arquitectura acepta como entrada imágenes de patches, sonidos en formato de onda o tokens. También proporciona un embarazado típico y etiquetas de categoría como en un sistema de clasificación general. Demostramos que los pesos de palabras de nuestro modelo pueden ser efectivamente utilizados en la clasificación de sonidos con los conjuntos de datos FSD-50K y GTZAN. Además, obtenemos los mismos resultados en la clasificación de imágenes en CIFAR-10 y Fashion-MNIST. Esto subraya que el LLM de palabras aprende una fuerte ruta interna que activa las conexiones necesarias y, por lo tanto, no es necesario crear un nuevo modelo para cada aplicación.",
      "upvotes": 4,
      "discussionId": "6833cb26fe87d9433dfd2b64",
      "ai_summary": "Auto-regressive text LLMs trained on text can develop internal capabilities for understanding images and audio, enabling them to perform classification tasks across different modalities without fine-tuning.",
      "ai_keywords": [
        "auto-regressive",
        "LLM",
        "text tokens",
        "audio",
        "visual",
        "embeddings",
        "category labels",
        "classification",
        "FSD-50K",
        "GTZAN",
        "CIFAR-10",
        "Fashion-MNIST",
        "image patches"
      ]
    },
    "publishedAt": "2025-05-20T18:20:16.000Z",
    "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading",
    "summary": "This paper presents a fascinating find: By training an auto-regressive LLM\nmodel on text tokens, the text model inherently develops internally an ability\nto understand images and audio, thereby developing the ability to see and hear\njust by reading. Popular audio and visual LLM models fine-tune text LLM models\nto give text output conditioned on images and audio embeddings. On the other\nhand, our architecture takes in patches of images, audio waveforms or tokens as\ninput. It gives us the embeddings or category labels typical of a\nclassification pipeline. We show the generality of text weights in aiding audio\nclassification for datasets FSD-50K and GTZAN. Further, we show this working\nfor image classification on CIFAR-10 and Fashion-MNIST, as well on image\npatches. This pushes the notion of text-LLMs learning powerful internal\ncircuits that can be utilized by activating necessary connections for various\napplications rather than training models from scratch every single time.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17091.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62d7f1119b629105a5d84aad",
      "avatarUrl": "/avatars/c74045063e7c06cb7be0fa41ebb1d824.svg",
      "fullname": "Prateek Verma",
      "name": "prateekv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16270",
      "authors": [
        {
          "_id": "6833d08edf7cbb5c087a8bf1",
          "user": {
            "_id": "65c288280aa2d53135734a42",
            "avatarUrl": "/avatars/960422a1482ac8b4a52dd08c02d901f6.svg",
            "isPro": false,
            "fullname": "Jiaru Zou",
            "user": "jiaruz2",
            "type": "user"
          },
          "name": "Jiaru Zou",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-26T02:41:41.249Z",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf2",
          "name": "Yikun Ban",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf3",
          "name": "Zihao Li",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf4",
          "name": "Yunzhe Qi",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf5",
          "name": "Ruizhong Qiu",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf6",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf7",
          "name": "Jingrui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T06:00:45.000Z",
      "submittedOnDailyAt": "2025-05-26T00:53:39.701Z",
      "title": "Transformer Copilot: Logs del ajuste de micro entrenador en un laberinto",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande se introducen generalmente mediante ajustes micro basados en datos de aprendizaje profundo para aplicaciones específicas de suma de aprendizaje profundo. El ajuste micro estándar se centra en minimizar la pérdida generada para optimizar los parámetros del modelo, pero nos esfuerzamos por conservar el señal de aprendizaje del modelo y utilizarlo de manera efectiva. Esto puede ser visto como un concepto similar al de reflexión sobre errores pasados para mejorar los resultados futuros, como en el aprendizaje humano.\n\nPrimero, presentamos la idea de \"MISTIE LOG\", un registro sistemático para registrar la acción de aprendizaje del modelo y los errores de reproducción. Llamamos a los modelos originales basados en transformers a \"PREIO\" y diseñamos un modelo llamado \"COPIO\" para mejorar la eficiencia de inferencia de los PREIO mediante ajustes de logística. Añadimos el nombre \"Transformer COPIO\" al conjunto de frameworks para PREIO y COPIO, introduciendo: (i) el diseño de nuevos modelos COPIO, (ii) el paradigma de entrenamiento conjunto en el registro MISTIE LOG, y (iii) el paradigma de inferencia funcional que mejora la generación mediante ajustes de logística. Ofrecemos un análisis teórico y experimental del nuevo framework de entrenamiento. Los experimentos en 12 benchmarks (conocimiento general, aritmética, trabajos de recomendación) muestran que el Transformer COPIO, comparado con los modelos PREIO, mejora su rendimiento en un promedio del 34.5% a través de ajustes micro, mostrando una fuerte capacidad de escalabilidad y transferencia.",
      "upvotes": 3,
      "discussionId": "6833d08fdf7cbb5c087a8c29",
      "ai_summary": "The Transformer Copilot framework enhances large language model performance through a Copilot model that refines the Pilot's logits based on a Mistake Log, leading to consistent performance improvements across various benchmarks.",
      "ai_keywords": [
        "large language models",
        "supervised fine-tuning",
        "domain-specific data",
        "generation loss",
        "model parameters",
        "learning signals",
        "Mistake Log",
        "transformer-based model",
        "Copilot model",
        "logits rectification",
        "joint training paradigm",
        "fused inference paradigm",
        "performance improvements",
        "computational overhead",
        "scalability",
        "transferability"
      ]
    },
    "publishedAt": "2025-05-22T02:00:45.000Z",
    "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
    "summary": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17063",
      "authors": [
        {
          "_id": "6833e65bf9ae3819ea4c568e",
          "name": "Yiduo Guo",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c568f",
          "user": {
            "_id": "638e4e66629b4d0a62ce1bf3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
            "isPro": false,
            "fullname": "Zhen Guo",
            "user": "zguo0525",
            "type": "user"
          },
          "name": "Zhen Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:28.399Z",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5690",
          "name": "Chuanwei Huang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5691",
          "name": "Zi-Ang Wang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5692",
          "name": "Zekai Zhang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5693",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5694",
          "name": "Huishuai Zhang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5695",
          "name": "Yikang Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T05:35:13.000Z",
      "submittedOnDailyAt": "2025-05-26T02:26:50.238Z",
      "title": "Synthetic Data RL: Task Definition Is All You Need\n\nDatos Síneticos RL: La Definición de Tarea Es Todo lo que Necesitas",
      "submittedOnDailyBy": {
        "_id": "638e4e66629b4d0a62ce1bf3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
        "isPro": false,
        "fullname": "Zhen Guo",
        "user": "zguo0525",
        "type": "user"
      },
      "summary": "El aprendizaje por refuerzo (RL) es uno de los métodos potentes para aplicar modelos básicos a tareas específicas, pero puede introducir una dependencia extensa en grandes conjuntos de datos etiquetados por humanos. Presentamos un simple y general marco llamado \"RL con Datos Sintéticos\" para ajustar modelos usando solo datos sintéticos generados a partir de la definición de la tarea. Nuestro método genera pares de preguntas y respuestas a partir de la definición de la tarea y artículos encontrados, ajusta la dificultad de las preguntas basándose en la capacidad de resolución de problemas del modelo y selecciona preguntas para entrenamiento de RL basándose en la tasa de aprobación promedio del modelo. Con Qwen-2.5-7B, logramos mejorar la base del modelo en GSM8K en un 29.2% absoluto (en realidad +2.9pp para el modelo entrenado y +6.6pp para Self-Instruct), 8.7% en MATH, 7.0pp en GPQA (comparado con SynthLLM), 8.9% en MedQA, 17.7% en CQA (derecho) y 13.7% en CFA (finanzas). En el mismo bucket de datos, superamos el aprendizaje supervisado y alcanzamos una rendición cercana a la de un aprendizaje por refuerzo con datos humanos (en GSM8K, +17.2pp). Añadiendo 100 demostraciones humanas solo mejoró la rendición en GSM8K en un 0.4pp. Reduciendo los datos humanos, el aprendizaje por refuerzo con datos sintéticos permite una aplicación scalable y eficiente de modelos. El código y demostraciones están disponibles en https://github.com/gydpku/Data_Synthesis_RL.",
      "upvotes": 3,
      "discussionId": "6833e65cf9ae3819ea4c56c9",
      "projectPage": "https://github.com/gydpku/Data_Synthesis_RL",
      "githubRepo": "https://github.com/gydpku/Data_Synthesis_RL",
      "ai_summary": "Synthetic Data RL enhances foundation models through reinforcement learning using only synthetic data, achieving performance comparable to models trained with full human-labeled data.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "synthetic data",
        "reinforcement fine-tuning",
        "question and answer pairs",
        "model solvability",
        "average pass rate",
        "data budget",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-05-18T01:35:13.000Z",
    "title": "Synthetic Data RL: Task Definition Is All You Need",
    "summary": "Reinforcement learning (RL) is a powerful way to adapt foundation models to\nspecialized tasks, but its reliance on large-scale human-labeled data limits\nbroad adoption. We introduce Synthetic Data RL, a simple and general framework\nthat reinforcement fine-tunes models using only synthetic data generated from a\ntask definition. Our method first generates question and answer pairs from the\ntask definition and retrieved documents, then adapts the difficulty of the\nquestion based on model solvability, and selects questions using the average\npass rate of the model across samples for RL training. On Qwen-2.5-7B, our\nmethod achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9\npp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on\nGPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA\n(finance). It surpasses supervised fine-tuning under the same data budget and\nnearly matches RL with full human data across datasets (e.g., +17.2 pp on\nGSM8K). Adding 100 human demonstrations improves the performance of GSM8K only\nby 0.4 pp, showing a limited added value. By reducing human data annotation,\nSynthetic Data RL enables scalable and efficient RL-based model adaptation.\nCode and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17063.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638e4e66629b4d0a62ce1bf3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
      "fullname": "Zhen Guo",
      "name": "zguo0525",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17540",
      "authors": [
        {
          "_id": "683409de1869c47bd0c423a4",
          "name": "Mingrui Wu",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a5",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a6",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a7",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a8",
          "name": "Jianjin Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a9",
          "name": "Jianfeng Liu",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423aa",
          "name": "Yuefeng Zhan",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ab",
          "name": "Weihao Han",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ac",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ad",
          "name": "Jiayi Ji",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ae",
          "name": "Xiaoshuai Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423af",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b0",
          "name": "Weiwei Deng",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b1",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b2",
          "name": "Feng Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b3",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b4",
          "name": "Rongrong Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T06:44:26.000Z",
      "submittedOnDailyAt": "2025-05-26T04:59:40.648Z",
      "title": "Generación de imágenes mediante ejecución de reprogramación basada en teoría y aprendizaje por refuerzo",
      "submittedOnDailyBy": {
        "_id": "6416d0b2058f65de43191027",
        "avatarUrl": "/avatars/2d99114e5cff39dccc385adfad7032c5.svg",
        "isPro": false,
        "fullname": "Mingrui Wu",
        "user": "mrwu",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de la generación de imágenes (T2I) ha llevado a que los modelos actuales tiendan a tener dificultades para comprender precisamente la intención del usuario a partir de pequeños regalos o prompts con defectos. En estudios previos, se intentó mejorar la función de los prompts utilizando modelos de lenguaje de gran escala (LLMs), pero estos métodos presentan una base insuficiente para la combinación de significado y realidad de las imágenes y generan muchas veces contenidos estilísticos e irrealistas. Basándonos en los recientes avances en la inferencia de modelos de lenguaje, proponemos un nuevo marco de expansión de prompts llamado RePrompt. Este método introduce una inferencia explícita durante el proceso de expansión de los prompts mediante aprendizaje por refuerzo. Nuestro método no depende de reglas handcrafted o cambios estilísticos, sino que entrena un modelo de etiquetas que genera prompts estructurados y subjetivos para optimizar resultados a nivel de imagen. El modelo de recompensa creado mejora la generación de prompts mediante evaluaciones de la preferencia humana, la coincidencia semántica y la combinación de la imagen. Nuestro enfoque no utiliza datos del usuario y permite entrenamiento en el terminal. Los experimentos en GenEval y T2I-Compbench demuestran que RePrompt mejora significativamente la precisión de la disposición espacial y la generalización de la combinación en diferentes backends de T2I, obteniendo resultados líder del momento.",
      "upvotes": 2,
      "discussionId": "683409e21869c47bd0c4248e",
      "ai_summary": "RePrompt, a reprompting framework using reinforcement learning, enhances text-to-image generation by optimizing for image-level outcomes, significantly improving spatial layout and compositional generalization.",
      "ai_keywords": [
        "text-to-image",
        "T2I",
        "large language models",
        "LLMs",
        "reinforcement learning",
        "structured prompts",
        "self-reflective prompts",
        "reward models",
        "human preference",
        "semantic alignment",
        "visual composition",
        "GenEval",
        "T2I-Compbench",
        "spatial layout fidelity",
        "compositional generalization"
      ]
    },
    "publishedAt": "2025-05-23T02:44:26.000Z",
    "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation\n  via Reinforcement Learning",
    "summary": "Despite recent progress in text-to-image (T2I) generation, existing models\noften struggle to faithfully capture user intentions from short and\nunder-specified prompts. While prior work has attempted to enhance prompts\nusing large language models (LLMs), these methods frequently generate stylistic\nor unrealistic content due to insufficient grounding in visual semantics and\nreal-world composition. Inspired by recent advances in reasoning for language\nmodel, we propose RePrompt, a novel reprompting framework that introduces\nexplicit reasoning into the prompt enhancement process via reinforcement\nlearning. Instead of relying on handcrafted rules or stylistic rewrites, our\nmethod trains a language model to generate structured, self-reflective prompts\nby optimizing for image-level outcomes. The tailored reward models assesse the\ngenerated images in terms of human preference, semantic alignment, and visual\ncomposition, providing indirect supervision to refine prompt generation. Our\napproach enables end-to-end training without human-annotated data. Experiments\non GenEval and T2I-Compbench show that RePrompt significantly boosts spatial\nlayout fidelity and compositional generalization across diverse T2I backbones,\nestablishing new state-of-the-art results.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17540.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6416d0b2058f65de43191027",
      "avatarUrl": "/avatars/2d99114e5cff39dccc385adfad7032c5.svg",
      "fullname": "Mingrui Wu",
      "name": "mrwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17016",
      "authors": [
        {
          "_id": "6833f7847e0c637c71de0ec6",
          "user": {
            "_id": "64b64debeb9a833e08d079fd",
            "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
            "isPro": false,
            "fullname": "Shuhan Tan",
            "user": "tanshh97",
            "type": "user"
          },
          "name": "Shuhan Tan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:10.881Z",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec7",
          "name": "Kairan Dou",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec8",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec9",
          "name": "Philipp Krähenbühl",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:45.000Z",
      "submittedOnDailyAt": "2025-05-26T03:40:36.660Z",
      "title": "Modelo de lenguaje visual interactivo y acción posteriormente entrenado",
      "submittedOnDailyBy": {
        "_id": "64b64debeb9a833e08d079fd",
        "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
        "isPro": false,
        "fullname": "Shuhan Tan",
        "user": "tanshh97",
        "type": "user"
      },
      "summary": "RIPT-VLA es un paradigma interactivo de aprendizaje por refuerzo sencillo y expandible. Este paradigma se utiliza para fine-tunar modelos de visión-lenguaje-acción (Vision-Language-Action) entrenados previamente con recompensas binarias de éxito raras. El proceso actual de entrenamiento de VLA utiliza demostraciones de EXPERTO en entornos offline y aprendizaje por refuerzo para adaptarse a nuevas tareas y entornos. RIPT-VLA aborda estos problemas utilizando un algoritmo de optimización de políticas estables basado en muestreo dinámico de rollout y evaluación de prioridad leave-one-out.\n\nRIPT-VLA presenta las siguientes características:\n\n1. **Aplicabilidad a diversos modelos VLA**: Aumenta la eficiencia del modelo ligero QueST en un 21.2%, y alcanza un 97.5% de éxito sin precedentes en el modelo OpenVLA-OFT de 7B.\n2. **Eficiencia computacional y de datos**: Con solo una demostración de datos, RIPT-VLA puede entrenar un modelo de SFT inútil (4%) en 15 iteraciones para alcanzar un 97% de éxito.\n3. **Generalización y robustez**: La política entrenada por RIPT-VLA se adapta ampliamente a diferentes tareas y escenarios, y es resistente a los estados iniciales.\n\nEstos resultados demuestran la importancia del paradigma RIPT-VLA para entrenar eficazmente y de manera práctica modelos de visión-lenguaje-acción posteriormente.",
      "upvotes": 1,
      "discussionId": "6833f7857e0c637c71de0f07",
      "projectPage": "https://ariostgx.github.io/ript_vla/",
      "githubRepo": "https://github.com/Ariostgx/ript-vla",
      "ai_summary": "RIPT-VLA is a reinforcement learning-based interactive post-training paradigm that enhances pretrained Vision-Language-Action models using sparse binary success rewards, improving adaptability and generalization.",
      "ai_keywords": [
        "reinforcement-learning-based",
        "interactive post-training",
        "Vision-Language-Action (VLA) models",
        "sparse binary success rewards",
        "offline expert demonstration",
        "supervised imitation",
        "dynamic rollout sampling",
        "leave-one-out advantage estimation",
        "policy optimization",
        "lightweight QueST model",
        "OpenVLA-OFT model",
        "success rate",
        "computational efficiency",
        "data-efficient",
        "policy learned",
        "generalization",
        "initial state context"
      ]
    },
    "publishedAt": "2025-05-22T13:59:45.000Z",
    "title": "Interactive Post-Training for Vision-Language-Action Models",
    "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17016.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b64debeb9a833e08d079fd",
      "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
      "fullname": "Shuhan Tan",
      "name": "tanshh97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16293",
      "authors": [
        {
          "_id": "683400b5231225ee202c20b7",
          "user": {
            "_id": "645c26d423ed9b7788d5e24b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
            "isPro": false,
            "fullname": "Rishabh Maheshwary",
            "user": "rmahesh",
            "type": "user"
          },
          "name": "Rishabh Maheshwary",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:08:47.558Z",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20b8",
          "name": "Masoud Hashemi",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20b9",
          "name": "Khyati Mahajan",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20ba",
          "name": "Shiva Krishna Reddy Malay",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bb",
          "name": "Sai Rajeswar",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bc",
          "name": "Sathwik Tejaswi Madhusudhan",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bd",
          "name": "Spandana Gella",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20be",
          "name": "Vikas Yadav",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T06:45:05.000Z",
      "submittedOnDailyAt": "2025-05-26T04:22:12.337Z",
      "title": "Solución para el problema de la complejidad de la resolución de preguntas basada en la teoría lógica de los LLM fortalecida por la escritura dinámica de notas",
      "submittedOnDailyBy": {
        "_id": "645c26d423ed9b7788d5e24b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
        "isPro": false,
        "fullname": "Rishabh Maheshwary",
        "user": "rmahesh",
        "type": "user"
      },
      "summary": "Utilizando el Iterative RAG para responder preguntas multi-turn, se enfrentan desafíos como el acumulación de largos contextos e información inadecuada. Esto limita la capacidad del modelo para procesar y inferir sobre el contenido recuperado. Los métodos recientes se centran en la compresión de la información recuperada, pero están limitados a un solo round RAG o requieren fine-tuning, o presentan una escasa capacidad de extensión en un RAG iterativo. Para resolver estos problemas, proponemos Notes Writing. Notes Writing genera notas claras y relevantes a partir de documentos recuperados en cada etapa, reduciendo el ruido y manteniendo solo la información esencial. Esto indirectamente aumenta la longitud de contexto válido de los Grandes Modelos de Lenguaje (LLMs), permitiendo que el modelo infera y planifique más efectivamente con mayores cantidades de texto de entrada. Notes Writing es independiente del marco y puede ser integrado en diversos métodos RAG iterativos. Hemos probado la eficacia de esta metodología utilizando tres métodos RAG iterativos, logrando un aumento promedio de 15.6 puntos porcentuales, minimizando al mismo tiempo el aumento de tokens de salida.",
      "upvotes": 1,
      "discussionId": "683400b6231225ee202c20e3",
      "ai_summary": "Notes Writing enhances iterative RAG by generating concise notes at each step, improving reasoning and performance while minimizing output increase.",
      "ai_keywords": [
        "Iterative RAG",
        "multi-hop question answering",
        "context length",
        "irrelevant information",
        "dimensionality reduction",
        "Notes Writing",
        "Large Language Models",
        "LLMs",
        "framework agnostic",
        "evaluation datasets"
      ]
    },
    "publishedAt": "2025-05-22T02:45:05.000Z",
    "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA",
    "summary": "Iterative RAG for multi-hop question answering faces challenges with lengthy\ncontexts and the buildup of irrelevant information. This hinders a model's\ncapacity to process and reason over retrieved content and limits performance.\nWhile recent methods focus on compressing retrieved information, they are\neither restricted to single-round RAG, require finetuning or lack scalability\nin iterative RAG. To address these challenges, we propose Notes Writing, a\nmethod that generates concise and relevant notes from retrieved documents at\neach step, thereby reducing noise and retaining only essential information.\nThis indirectly increases the effective context length of Large Language Models\n(LLMs), enabling them to reason and plan more effectively while processing\nlarger volumes of input text. Notes Writing is framework agnostic and can be\nintegrated with different iterative RAG methods. We demonstrate its\neffectiveness with three iterative RAG methods, across two models and four\nevaluation datasets. Notes writing yields an average improvement of 15.6\npercentage points overall, with minimal increase in output tokens.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16293.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645c26d423ed9b7788d5e24b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
      "fullname": "Rishabh Maheshwary",
      "name": "rmahesh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16022",
      "authors": [
        {
          "_id": "68342cb2924393051af84722",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84723",
          "name": "Siya Qi",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84724",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84725",
          "name": "Chen Qian",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84726",
          "name": "Yali Du",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84727",
          "name": "Yulan He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T21:12:35.000Z",
      "submittedOnDailyAt": "2025-05-26T07:38:18.450Z",
      "title": "NOVER: Entrenamiento visual de modelos de lenguaje por aprendizaje reforzado sin evaluadores",
      "submittedOnDailyBy": {
        "_id": "66e2932e5c100c12aa2def39",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FiQ5Fap-qVqnXeULGPYs6.png",
        "isPro": false,
        "fullname": "weiliu",
        "user": "thinkwee",
        "type": "user"
      },
      "summary": "En el desarrollo reciente, funciones como DeepSeek R1-Zero destacan la eficiencia del paradigma de aprendizaje de recompensas basado en la parte final de la respuesta del modelo de lenguaje. Estos métodos están limitados a áreas como matemáticas o programación, ya que calculan las recompensas con base en datos de verificación externos, que son fáciles de obtener. Los modelos de recompensas requieren de altos costos de entrenamiento y de alta calidad de datos, lo que dificulta su solución. En este artículo, se propone un marco de aprendizaje de recompensas general NOVER (Reinforcement Learning sin Verificador), que no necesita de datos de verificación externos. NOVER requiere solo de datos de fininimiento estándar y permite el aprendizaje de recompensas sin necesidad de datos de verificación externos. En tareas de resolución de problemas desde texto a texto, NOVER permite el aprendizaje de recompensas y supera un rendimiento de al menos 7.7% sobre modelos del mismo tamaño, como DeepSeek R1 671B. Además, la flexibilidad de NOVER ofrece nuevas posibilidades para la optimización de grandes modelos de lenguaje y explora nuevas oportunidades en el aprendizaje de recompensas inverso.",
      "upvotes": 1,
      "discussionId": "68342cb3924393051af8476b",
      "githubRepo": "https://github.com/thinkwee/NOVER",
      "ai_summary": "NOVER, a reinforcement learning framework that eliminates the need for external verifiers, enhances language model performance across text-to-text tasks.",
      "ai_keywords": [
        "incentive training",
        "reinforcement learning",
        "NOVER",
        "NO-VERifier Reinforcement Learning",
        "DeepSeek R1-Zero",
        "DeepSeek R1 671B",
        "inverse incentive training"
      ]
    },
    "publishedAt": "2025-05-21T17:12:35.000Z",
    "title": "NOVER: Incentive Training for Language Models via Verifier-Free\n  Reinforcement Learning",
    "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e2932e5c100c12aa2def39",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FiQ5Fap-qVqnXeULGPYs6.png",
      "fullname": "weiliu",
      "name": "thinkwee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15805",
      "authors": [
        {
          "_id": "682eeb06720821973d643576",
          "user": {
            "_id": "647c4a2692182942d7c2e698",
            "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
            "isPro": false,
            "fullname": "HWANCHANG",
            "user": "HwanChang0106",
            "type": "user"
          },
          "name": "Hwan Chang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:18:42.752Z",
          "hidden": false
        },
        {
          "_id": "682eeb06720821973d643577",
          "name": "Yumin Kim",
          "hidden": false
        },
        {
          "_id": "682eeb06720821973d643578",
          "name": "Yonghyun Jun",
          "hidden": false
        },
        {
          "_id": "682eeb06720821973d643579",
          "name": "Hwanhee Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:58:11.000Z",
      "submittedOnDailyAt": "2025-05-26T07:18:19.930Z",
      "title": "Seguir la seguridad! Benchmark para el ataque indirecto de tipo pregunta en el contexto de grandes modelos de lenguaje: conservación de políticas de seguridad.",
      "submittedOnDailyBy": {
        "_id": "647c4a2692182942d7c2e698",
        "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
        "isPro": false,
        "fullname": "HWANCHANG",
        "user": "HwanChang0106",
        "type": "user"
      },
      "summary": "El desarrollo de LLMs en áreas sensibles como empresas y gobierno se expande cada vez más, lo que hace importante la inducción de políticas de seguridad personalizadas que se cumplan en el contexto, especialmente en la protección de la información. El estudio previo de LLMs ha centrado su atención principalmente en la seguridad general y en datos socialmente sensibles, pero ha tenido poca atención a la mantener la seguridad contextual frente a ataques. En este sentido, presentamos un nuevo grande conjunto de datos de benchmark llamado CoPriva, que evalúa en qué medida LLMs cumplen con políticas de no publicación contextuales personalizadas. Este conjunto de datos se genera en contextos reales, incluye políticas y preguntas específicas, y explora información prohibida frente a ataques directos e indirectos. Evaluamos 10 modelos de LLM en nuestro benchmark y descubrimos importantes vulnerabilidades: muchos modelos destruyen políticas personalizadas y exponen información sensible. Estos fallos son particularmente graves frente a ataques indirectos. Nuestro análisis revela que las aplicaciones sensibles actuales en la seguridad de los LLMs tienen importantes debilidades. Los modelos muestran la capacidad de proporcionar respuestas correctas, pero presentan dificultades al aplicar las restricciones de las políticas durante la generación. Por otro lado, demostraron su capacidad para modificar la salida cuando se le proporcionan directamente los prompts. Estos hallazgos subrayan la necesidad de desarrollar métodos más robustos para garantizar la seguridad contextual.",
      "upvotes": 1,
      "discussionId": "682eeb07720821973d6435ec",
      "githubRepo": "https://github.com/hwanchang00/CoPriva",
      "ai_summary": "LLMs frequently violate contextual security policies by leaking sensitive information, particularly under indirect attacks, indicating a critical gap in current safety mechanisms.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "security policies",
        "information non-disclosure",
        "CoPriva",
        "contextual security preservation",
        "question answering",
        "explicit policies",
        "indirect attacks",
        "virus",
        "policy constraints",
        "output revision"
      ]
    },
    "publishedAt": "2025-05-21T13:58:11.000Z",
    "title": "Keep Security! Benchmarking Security Policy Preservation in Large\n  Language Model Contexts Against Indirect Attacks in Question Answering",
    "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15805.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647c4a2692182942d7c2e698",
      "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
      "fullname": "HWANCHANG",
      "name": "HwanChang0106",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12891",
      "authors": [
        {
          "_id": "683059e8e2f446ed653e8512",
          "name": "Shaohang Wei",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8513",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8514",
          "user": {
            "_id": "6447ca6ca478b20f1755b294",
            "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
            "isPro": false,
            "fullname": "Feifan Song",
            "user": "songff",
            "type": "user"
          },
          "name": "Feifan Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:14:11.572Z",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8515",
          "name": "Wen Luo",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8516",
          "name": "Tianyi Zhuang",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8517",
          "name": "Haochen Tan",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8518",
          "name": "Zhijiang Guo",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8519",
          "name": "Houfeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T09:22:02.000Z",
      "submittedOnDailyAt": "2025-05-26T03:54:28.256Z",
      "title": "TIME: Escenario real de la lógica temporal de los modelos de lenguaje LLMs\n\n(Nota: Se mantienen los nombres propios y términos técnicos del original para asegurar la profundidad y precisión.)",
      "submittedOnDailyBy": {
        "_id": "6447ca6ca478b20f1755b294",
        "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
        "isPro": false,
        "fullname": "Feifan Song",
        "user": "songff",
        "type": "user"
      },
      "summary": "La lógica temporal es esencial para que los modelos de lenguaje grandes (LLMs) entiendan la realidad. Sin embargo, la investigación actual no entiende correctamente los problemas de la realidad en términos temporales. Esto se debe a tres factores: (1) la información temporal fuerte, (2) la dinámica de eventos que cambian rápidamente, y (3) las complejas relaciones temporales que se desarrollan en la sociedad. Para resolver esto, proponemos un marco de evaluación de escala temporal llamado TIME. TIME incluye 38,522 pares de preguntas y respuestas y se divide en tres niveles con 11 tareas detalladas. Este marco de evaluación incluye tres subconjuntos de datos que reflejan diferentes problemas de la realidad: TIME-Wiki, TIME-News y TIME-Dial. Hemos realizado experimentos ampliados con modelos de inferencia y modelos no inferenciales, analizando el rendimiento en términos de lógica temporal y resumiendo el impacto del escalado temporal en la capacidad de lógica temporal. Además, para fomentar la investigación futura y la evaluación estándar, hemos lanzado TIME-Lite. El código está disponible en https://github.com/sylvain-wei/TIME y los conjuntos de datos en https://huggingface.co/datasets/SylvainWei/TIME.",
      "upvotes": 1,
      "discussionId": "683059eae2f446ed653e85d7",
      "ai_summary": "A benchmark called TIME assesses temporal reasoning in LLMs across varied real-world challenges, including intensive temporal information, fast-changing event dynamics, and complex social interactions, and evaluates the impact of test-time scaling.",
      "ai_keywords": [
        "Temporal reasoning",
        "Large Language Models (LLMs)",
        "QA pairs",
        "benchmark",
        "TIME-Wiki",
        "TIME-News",
        "TIME-Dial",
        "reasoning models",
        "non-reasoning models",
        "TIME-Lite"
      ]
    },
    "publishedAt": "2025-05-19T05:22:02.000Z",
    "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in\n  Real-World Scenarios",
    "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12891.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6447ca6ca478b20f1755b294",
      "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
      "fullname": "Feifan Song",
      "name": "songff",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16056",
      "authors": [
        {
          "_id": "6830894db51948863e05b68c",
          "user": {
            "_id": "64bfa1401d40292dd32f93d7",
            "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
            "isPro": false,
            "fullname": "Leo Liang",
            "user": "ljcleo",
            "type": "user"
          },
          "name": "Jingcong Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:13:21.361Z",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b68d",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b68e",
          "name": "Miren Tian",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b68f",
          "name": "Yitong Li",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b690",
          "name": "Duyu Tang",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b691",
          "name": "Zhongyu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T22:13:09.000Z",
      "submittedOnDailyAt": "2025-05-26T07:53:13.385Z",
      "title": "Todos los modelos no son adecuados para el sobrecarga de especialistas: La consistencia de la ruta local en modelos Mixture-of-Experts",
      "submittedOnDailyBy": {
        "_id": "64bfa1401d40292dd32f93d7",
        "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
        "isPro": false,
        "fullname": "Leo Liang",
        "user": "ljcleo",
        "type": "user"
      },
      "summary": "Mixture-of-Experts (MoE) permite una escalabilidad eficiente de grandes modelos de lenguaje natural (LLMs) utilizando expertos raramente activados durante la inferencia. Para aplicar efectivamente MoE en dispositivos con limitaciones de memoria, varios sistemas han introducido *overload de expertos*, diseñando para almacenar parte de los expertos en memoria rápida y ejecutar el resto en el CPU o cargarlo cuando sea necesario. A pesar de esto, también existen investigaciones que explotan la localidad de la activación de los expertos (donde tokens continuos activan similares expertos), aunque la **coherencia de la ruta local** varía entre modelos y aún no se ha investigado su impacto. En este trabajo, se proponen dos métricas para evaluar la coherencia de la ruta local en modelos MoE: 1. **Segment Routing Best Performance (SRP)** evalúa a qué medida un grupo fijo de expertos responde a las necesidades de segmentos de tokens. 2. **Segment Cache Best Hit Rate (SCH)** evalúa el mejor tasa de hito en cache para un nivel de segmentos dado bajo un límite de tamaño de cache. Se analizaron 20 modelos MoE de diferentes tamaños y estructuras, y se encontró que los modelos que aplican MoE en cada capa y no comparten expertos mostraron la mayor coherencia de la ruta local. Además, los expertos especializados en áreas contribuyen más a la coherencia de la ruta local, y casi todos los modelos alcanzan un equilibrio entre el tamaño de cache y su eficiencia al usar un tamaño de cache aproximadamente el doble de los expertos activos. Estos hallazgos se conectan con la diseña y aplicación de MoE eficientes en memoria, sin perder velocidad de inferencia. En este trabajo, se publica el código para reproducir los experimentos (https://github.com/ljcleo/moe-lrc).",
      "upvotes": 0,
      "discussionId": "6830894eb51948863e05b6e8",
      "ai_summary": "MoE models achieve efficient scaling in LLMs with expert offloading, emphasizing the importance of local routing consistency and cache effectiveness.",
      "ai_keywords": [
        "Mixture-of-Experts (MoE)",
        "large language models (LLMs)",
        "expert offloading",
        "fast memory",
        "slow memory",
        "local routing consistency",
        "Segment Routing Best Performance (SRP)",
        "Segment Cache Best Hit Rate (SCH)",
        "domain-specialized experts",
        "vocabulary-specialized experts"
      ]
    },
    "publishedAt": "2025-05-21T18:13:09.000Z",
    "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
    "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16056.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bfa1401d40292dd32f93d7",
      "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
      "fullname": "Leo Liang",
      "name": "ljcleo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11881",
      "authors": [
        {
          "_id": "6833ebdb142b0e50399413d3",
          "name": "Giyeong Oh",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d4",
          "name": "Woohyun Cho",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d5",
          "name": "Siyeol Kim",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d6",
          "name": "Suhwan Choi",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d7",
          "name": "Younjae Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-17T07:16:11.000Z",
      "submittedOnDailyAt": "2025-05-26T02:52:51.028Z",
      "title": "Retornado el residuo de la conexión: actualización de normalización eficiente en una gran red profunda establecida",
      "submittedOnDailyBy": {
        "_id": "63d93667255ef6add20f9272",
        "avatarUrl": "/avatars/99a3aeadcc81ef85164cdfb6ab186b17.svg",
        "isPro": false,
        "fullname": "Giyeong Oh",
        "user": "BootsofLagrangian",
        "type": "user"
      },
      "summary": "Los residuos de conexión son un elemento importante en redes neuronales profundas y pueden mitigar el fenómeno de la pérdida de gradiente durante el proceso de entrenamiento, permitiendo así aumentar la profundidad. Sin embargo, el método general de actualización de residuos agrega directamente el output del módulo al flujo de entrada. En este caso, la actualización principal se centra en fortalecer o regular la dirección del flujo existente, lo que puede reducir la capacidad de aprendizaje del módulo. En este artículo, se introduce una actualización de residuos ortogonal: se decompone el output del módulo con respecto al flujo de entrada y se agrega solo la componente ortogonal a este flujo. Esta diseñación fomenta que el módulo proporcione principalmente nuevas direcciones de representación, promueve un aprendizaje de características rico y fomenta un entrenamiento eficiente. La actualización ortogonal ha demostrado mejorar la precisión de generalización y la estabilidad del entrenamiento en diferentes arquitecturas (ResNetV2, Vision Transformers) y conjuntos de datos (CIFARs, TinyImageNet, ImageNet-1k). Por ejemplo, la precisión top-1 de ViT-B en ImageNet-1k se ha mejorado en +4.3%p.",
      "upvotes": 0,
      "discussionId": "6833ebdc142b0e5039941420",
      "ai_summary": "Orthogonal Residual Updates enhance feature learning and training stability by decomposing module outputs to contribute primarily novel features.",
      "ai_keywords": [
        "residual connections",
        "vanishing gradients",
        "orthogonal update",
        "ResNetV2",
        "Vision Transformers",
        "CIFARs",
        "TinyImageNet",
        "ImageNet-1k",
        "top-1 accuracy"
      ]
    },
    "publishedAt": "2025-05-17T03:16:11.000Z",
    "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and\n  Efficient Deep Networks",
    "summary": "Residual connections are pivotal for deep neural networks, enabling greater\ndepth by mitigating vanishing gradients. However, in standard residual updates,\nthe module's output is directly added to the input stream. This can lead to\nupdates that predominantly reinforce or modulate the existing stream direction,\npotentially underutilizing the module's capacity for learning entirely novel\nfeatures. In this work, we introduce Orthogonal Residual Update: we decompose\nthe module's output relative to the input stream and add only the component\northogonal to this stream. This design aims to guide modules to contribute\nprimarily new representational directions, fostering richer feature learning\nwhile promoting more efficient training. We demonstrate that our orthogonal\nupdate strategy improves generalization accuracy and training stability across\ndiverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,\nTinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy\ngain for ViT-B on ImageNet-1k.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11881.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d93667255ef6add20f9272",
      "avatarUrl": "/avatars/99a3aeadcc81ef85164cdfb6ab186b17.svg",
      "fullname": "Giyeong Oh",
      "name": "BootsofLagrangian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]