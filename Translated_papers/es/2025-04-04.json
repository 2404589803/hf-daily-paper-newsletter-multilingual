[
  {
    "paper": {
      "id": "2504.01990",
      "authors": [
        {
          "_id": "67ef8723d325fe100f36107e",
          "user": {
            "_id": "654a97282d2fcd6bf2851173",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png",
            "isPro": false,
            "fullname": "Bang Liu",
            "user": "Bang-UdeM-Mila",
            "type": "user"
          },
          "name": "Bang Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-04T07:15:51.456Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36107f",
          "user": {
            "_id": "65af5f8f3db2280ece7fac79",
            "avatarUrl": "/avatars/66d88b2d744c8d00e11d39a55ab86c2e.svg",
            "isPro": false,
            "fullname": "Xin-Feng Li",
            "user": "xinfeng1i",
            "type": "user"
          },
          "name": "Xinfeng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:45.785Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361080",
          "user": {
            "_id": "64b78a39954ae43365984448",
            "avatarUrl": "/avatars/6de9d4bf320a69eca6b758e718ee116c.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Peiyan",
            "type": "user"
          },
          "name": "Jiayi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:49.444Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361081",
          "user": {
            "_id": "64324bb3034ecbefddd99863",
            "avatarUrl": "/avatars/3b8cdc2066251999a3a7e6d5565dceb5.svg",
            "isPro": false,
            "fullname": "Jinlin Wang",
            "user": "JinlinW",
            "type": "user"
          },
          "name": "Jinlin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:27:02.727Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361082",
          "name": "Tanjin He",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361083",
          "name": "Sirui Hong",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361084",
          "name": "Hongzhang Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361085",
          "name": "Shaokun Zhang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361086",
          "user": {
            "_id": "5fc0b2b61160c47d1d438568",
            "avatarUrl": "/avatars/90beea6b452c662d579197dbf592423a.svg",
            "isPro": false,
            "fullname": "Kaitao Song",
            "user": "KaitaoSong",
            "type": "user"
          },
          "name": "Kaitao Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:27:47.151Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361087",
          "user": {
            "_id": "64c090a9f613170e7be93d2f",
            "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
            "isPro": false,
            "fullname": "KunlunZhu",
            "user": "KunlunZhu",
            "type": "user"
          },
          "name": "Kunlun Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:03.582Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361088",
          "user": {
            "_id": "6664783b0ab8b63cbb4a3156",
            "avatarUrl": "/avatars/71859e6f76c157191bd2e968061f08b0.svg",
            "isPro": false,
            "fullname": "cyh",
            "user": "chengyuheng",
            "type": "user"
          },
          "name": "Yuheng Cheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:14.543Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361089",
          "user": {
            "_id": "62bb1e0f3ff437e49a3088e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62bb1e0f3ff437e49a3088e5/bcUQmH8tKfI6DIWH9IcYp.jpeg",
            "isPro": true,
            "fullname": "Suyuchen Wang",
            "user": "sheryc",
            "type": "user"
          },
          "name": "Suyuchen Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:21.351Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108a",
          "user": {
            "_id": "655c092183186f133f959108",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WMVgGhjQbVJXK9eh4EuT9.jpeg",
            "isPro": false,
            "fullname": "Xiaoqiang Wang",
            "user": "qindomitable",
            "type": "user"
          },
          "name": "Xiaoqiang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:26.707Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108b",
          "name": "Yuyu Luo",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108c",
          "user": {
            "_id": "648ee5fe9ae7cc4fcffa9aef",
            "avatarUrl": "/avatars/9bcc5eb91452c1360b9a0a4f9def8af8.svg",
            "isPro": false,
            "fullname": "Haibo Jin",
            "user": "Nick233",
            "type": "user"
          },
          "name": "Haibo Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:28:40.177Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108d",
          "user": {
            "_id": "64b78a39954ae43365984448",
            "avatarUrl": "/avatars/6de9d4bf320a69eca6b758e718ee116c.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Peiyan",
            "type": "user"
          },
          "name": "Peiyan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:02.679Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108e",
          "user": {
            "_id": "66197a8afeb55cbe39e50ae8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pcSS1TsCoHxRcAEkcMNm0.png",
            "isPro": false,
            "fullname": "Ollie Liu",
            "user": "oliu-io",
            "type": "user"
          },
          "name": "Ollie Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:10.114Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36108f",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361090",
          "user": {
            "_id": "6719d581a6cad13741b8bc7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719d581a6cad13741b8bc7f/w4EttqfXRgWZJc6HpYOS9.jpeg",
            "isPro": false,
            "fullname": "Huan Zhang",
            "user": "huanzhang12",
            "type": "user"
          },
          "name": "Huan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:24.614Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361091",
          "user": {
            "_id": "640dc84b474aa6f89554d518",
            "avatarUrl": "/avatars/64f47f76d97c5e91b7ab8380bcada61c.svg",
            "isPro": false,
            "fullname": "Zhaoyang Yu",
            "user": "MoshiQAQ",
            "type": "user"
          },
          "name": "Zhaoyang Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:29:38.969Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361092",
          "name": "Haochen Shi",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361093",
          "name": "Boyan Li",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361094",
          "name": "Dekun Wu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361095",
          "user": {
            "_id": "6402e8fb06c715b93407442d",
            "avatarUrl": "/avatars/12b67f0632be5a53b56d8a68586a7f98.svg",
            "isPro": false,
            "fullname": "Fengwei Teng",
            "user": "leavendough",
            "type": "user"
          },
          "name": "Fengwei Teng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:09.492Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361096",
          "name": "Xiaojun Jia",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361097",
          "name": "Jiawei Xu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361098",
          "name": "Jinyu Xiang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f361099",
          "name": "Yizhang Lin",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109a",
          "name": "Tianming Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109b",
          "name": "Tongliang Liu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109c",
          "name": "Yu Su",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109d",
          "name": "Huan Sun",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109e",
          "user": {
            "_id": "66a8fa5fd909c30167f1f5cd",
            "avatarUrl": "/avatars/c9b26d5b2dd78bed9661df429012fd97.svg",
            "isPro": false,
            "fullname": "Glen Berseth",
            "user": "gberseth",
            "type": "user"
          },
          "name": "Glen Berseth",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:19.433Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f36109f",
          "name": "Jianyun Nie",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a0",
          "name": "Ian Foster",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a1",
          "name": "Logan Ward",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a2",
          "name": "Qingyun Wu",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a3",
          "user": {
            "_id": "5e7e595230dc073f817a2bb5",
            "avatarUrl": "/avatars/d5ff36e45555d9e169cf56c845736444.svg",
            "isPro": false,
            "fullname": "Yu Gu",
            "user": "entslscheia",
            "type": "user"
          },
          "name": "Yu Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:50:40.603Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a4",
          "user": {
            "_id": "64403daae44f30a72323e4ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
            "isPro": false,
            "fullname": "mingchen zhuge",
            "user": "tjpxiaoming",
            "type": "user"
          },
          "name": "Mingchen Zhuge",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:26:28.011Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a5",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a6",
          "name": "Haohan Wang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a7",
          "name": "Jiaxuan You",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a8",
          "name": "Chi Wang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610a9",
          "user": {
            "_id": "670918b02806bda07e44780c",
            "avatarUrl": "/avatars/c08ba5048d9e911ef488862e8869792f.svg",
            "isPro": false,
            "fullname": "Jian Pei",
            "user": "StrawHat2333",
            "type": "user"
          },
          "name": "Jian Pei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:36:21.496Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610aa",
          "name": "Qiang Yang",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610ab",
          "user": {
            "_id": "6342829eb9454d65a2e7a4c4",
            "avatarUrl": "/avatars/4438abdf189dbe26a52948800d79a7c5.svg",
            "isPro": false,
            "fullname": "Xiaoliang Qi",
            "user": "phynics",
            "type": "user"
          },
          "name": "Xiaoliang Qi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:30:59.175Z",
          "hidden": false
        },
        {
          "_id": "67ef8723d325fe100f3610ac",
          "name": "Chenglin Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T18:00:29.000Z",
      "submittedOnDailyAt": "2025-04-04T05:46:58.338Z",
      "title": "El desarrollo de agentes básicos y los problemas: evolución hacia un sistema cooperativo y seguro inspirado en el cerebro",
      "submittedOnDailyBy": {
        "_id": "64403daae44f30a72323e4ca",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
        "isPro": false,
        "fullname": "mingchen zhuge",
        "user": "tjpxiaoming",
        "type": "user"
      },
      "summary": "El surgimiento de los LLM ha impulsado cambios innovadores en el campo de la inteligencia artificial, ofreciendo oportunidades para desarrollar inteligencias artificiales de alto rendimiento que puedan realizar acciones transformadoras en diversas áreas, así como desarrollar teorías lógicas complejas y fortalecer la percepción. Asumiendo que estas inteligencias podrán tener un mayor impacto en la investigación y aplicaciones prácticas de la inteligencia artificial, el diseño, evaluación y mejora continua de las IAs se han convertido en desafíos complejos y multifacéticos. En esta investigación, se propone la configuración de una inteligencia artificial modularizada basada en una arquitectura similar a la función de la mente humana, integrando principios de ciencia cognitiva, neurociencia y investigación computacional. Se proporciona una descripción detallada del diseño, evaluación y mejora continua de esta inteligencia artificial. Nuestro trabajo se divide en cuatro partes interrelacionadas. Primero, se examina la base modularizada de la inteligencia artificial, mapeando módulos de cognición, percepción y acción correspondientes a funciones del cerebro humano, y se identifican elementos clave como memoria, modelado del mundo, procesamiento de recompensas y sistemas emocionales. Segundo, se discuten estructuras evolutivas autómatas y el proceso de adaptación, investigando cómo las IAs pueden evaluar automáticamente sus habilidades y adaptarse a entornos dinámicos, alcanzando un aprendizaje continuo a través del paradigma de optimización automática. Esto incluye el desarrollo de nuevas estrategias de optimización lideradas por los LLM. Tercero, se investiga sistemas de múltiples IAs cooperativas y evolutivas, explorando la inteligencia colectiva generada por el intercambio, colaboración y estructuras sociales de las IAs, y se establece una relación paralela con la dinámica de la sociedad humana. Finalmente, se discuten las responsabilidades fundamentales para la construcción de sistemas de IA seguras, éticas y beneficiosas, destacando estrategias prácticas para mitigar amenazas internas y externas, asegurar acuerdos éticos, fortalecer robustez y confianza.",
      "upvotes": 55,
      "discussionId": "67ef8727d325fe100f3611aa",
      "githubRepo": "https://github.com/FoundationAgents/awesome-foundation-agents"
    },
    "publishedAt": "2025-03-31T14:00:29.000Z",
    "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems",
    "summary": "The advent of large language models (LLMs) has catalyzed a transformative\nshift in artificial intelligence, paving the way for advanced intelligent\nagents capable of sophisticated reasoning, robust perception, and versatile\naction across diverse domains. As these agents increasingly drive AI research\nand practical applications, their design, evaluation, and continuous\nimprovement present intricate, multifaceted challenges. This survey provides a\ncomprehensive overview, framing intelligent agents within a modular,\nbrain-inspired architecture that integrates principles from cognitive science,\nneuroscience, and computational research. We structure our exploration into\nfour interconnected parts. First, we delve into the modular foundation of\nintelligent agents, systematically mapping their cognitive, perceptual, and\noperational modules onto analogous human brain functionalities, and elucidating\ncore components such as memory, world modeling, reward processing, and\nemotion-like systems. Second, we discuss self-enhancement and adaptive\nevolution mechanisms, exploring how agents autonomously refine their\ncapabilities, adapt to dynamic environments, and achieve continual learning\nthrough automated optimization paradigms, including emerging AutoML and\nLLM-driven optimization strategies. Third, we examine collaborative and\nevolutionary multi-agent systems, investigating the collective intelligence\nemerging from agent interactions, cooperation, and societal structures,\nhighlighting parallels to human social dynamics. Finally, we address the\ncritical imperative of building safe, secure, and beneficial AI systems,\nemphasizing intrinsic and extrinsic security threats, ethical alignment,\nrobustness, and practical mitigation strategies necessary for trustworthy\nreal-world deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64403daae44f30a72323e4ca",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
      "fullname": "mingchen zhuge",
      "name": "tjpxiaoming",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02826",
      "authors": [
        {
          "_id": "67ef4be0985aa66b67021ddc",
          "user": {
            "_id": "6530e62f536dbca918e71c3e",
            "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
            "isPro": false,
            "fullname": "Xiangyu Z",
            "user": "PhoenixZ",
            "type": "user"
          },
          "name": "Xiangyu Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:07.389Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021ddd",
          "user": {
            "_id": "6710be3e6d1b33cf24417e38",
            "avatarUrl": "/avatars/f60bc9a67bb58f5997cbcc28cb93c079.svg",
            "isPro": false,
            "fullname": "zpy",
            "user": "zpy777",
            "type": "user"
          },
          "name": "Peiyuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:11.816Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021dde",
          "user": {
            "_id": "662516d72419feed62fb3a0a",
            "avatarUrl": "/avatars/24c4157829e70a4e346aa984885aa5ad.svg",
            "isPro": false,
            "fullname": "Dian",
            "user": "KexianTang",
            "type": "user"
          },
          "name": "Kexian Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:21:18.584Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021ddf",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de0",
          "name": "Zicheng Zhang",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de1",
          "user": {
            "_id": "65535b125413c1a54e6fb243",
            "avatarUrl": "/avatars/03bcf1d58865f5406aff49a415e78bdc.svg",
            "isPro": false,
            "fullname": "Guangtao Zhai",
            "user": "GTZhai",
            "type": "user"
          },
          "name": "Guangtao Zhai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:23.057Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de2",
          "user": {
            "_id": "667289f903c802764985d8c6",
            "avatarUrl": "/avatars/916befcbf0e52ce56be49617f31c7bb2.svg",
            "isPro": false,
            "fullname": "Junchi Yan",
            "user": "Rethinker",
            "type": "user"
          },
          "name": "Junchi Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:30.311Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de3",
          "name": "Hua Yang",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de4",
          "user": {
            "_id": "648e77184cae4f6921dbb382",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648e77184cae4f6921dbb382/zAAJRvOStC0wZplqVWrk_.jpeg",
            "isPro": false,
            "fullname": "Xue Yang",
            "user": "yangxue",
            "type": "user"
          },
          "name": "Xue Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:09.267Z",
          "hidden": false
        },
        {
          "_id": "67ef4be0985aa66b67021de5",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:22:37.146Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/PQFr3_6S3BeUSNb79jMSO.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/0rGBBJT_JcUPBk5cE_Te0.png"
      ],
      "publishedAt": "2025-04-03T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-04T01:35:35.280Z",
      "title": "Pixeles tras la imaginación: un marco de referencia para el edición visual basada en evidencias.",
      "submittedOnDailyBy": {
        "_id": "63ee1379190ddd6214efd73a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
        "isPro": false,
        "fullname": "HAODONG DUAN",
        "user": "KennyUTC",
        "type": "user"
      },
      "summary": "Los grandes modelos multimodel (LMMs) están evolucionando en el campo de la comprensión visual y la generación, pero en el ámbito general de la edición visual, aún presentan desafíos como la seguimiento de instrucciones complejas, la mantención de coherencia externa y la soporte a formatos de entrada flexibles. Para cerrar estos espacios, presentamos RISEBench, un marco de evaluación. RISEBench enfatiza los principios temporales, causales, espaciales y lógicos. Proporciona test cases de alta calidad para cada categoría y propone un marco para evaluar la razón de las instrucciones, la coherencia externa y la posibilidad visual, utilizando un enfoque de jurado de humanos y de modelos LMM-as-a-judge. Según los resultados de los experimentos, GPT-4o-Native supera significativamente otros modelos abiertos y propietarios, pero también reconoce dificultades en el campo de los principios lógicos, lo que indica una investigación insuficiente en esta área. A través de los esfuerzos iniciales, RISEBench proporciona una base de conocimientos fundamentales sobre la edición visual basada en lógica y estimula futuras investigaciones. Aunque está en su inicio, estamos dedicados a expandir y mejorar continuamente el marco de evaluación para apoyar una evaluación más estricta y escalable en los próximos sistemas multimodel. Los códigos y datos están disponibles en https://github.com/PhoenixZ810/RISEBench.",
      "upvotes": 40,
      "discussionId": "67ef4be4985aa66b67021ef2",
      "githubRepo": "https://github.com/PhoenixZ810/RISEBench",
      "ai_keywords": [
        "Large Multi-modality Models (LMMs)",
        "General Visual Editing",
        "Temporal Reasoning",
        "Causal Reasoning",
        "Spatial Reasoning",
        "Logical Reasoning",
        "RISEBench",
        "Instruction Reasoning",
        "Appearance Consistency",
        "Visual Plausibility",
        "GPT-4o-Native",
        "multimodal systems"
      ]
    },
    "publishedAt": "2025-04-03T13:59:56.000Z",
    "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
    "summary": "Large Multi-modality Models (LMMs) have made significant progress in visual\nunderstanding and generation, but they still face challenges in General Visual\nEditing, particularly in following complex instructions, preserving appearance\nconsistency, and supporting flexible input formats. To address this gap, we\nintroduce RISEBench, the first benchmark for evaluating Reasoning-Informed\nviSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal,\nCausal, Spatial, and Logical Reasoning. We curate high-quality test cases for\neach category and propose an evaluation framework that assesses Instruction\nReasoning, Appearance Consistency, and Visual Plausibility with both human\njudges and an LMM-as-a-judge approach. Our experiments reveal that while\nGPT-4o-Native significantly outperforms other open-source and proprietary\nmodels, even this state-of-the-art system struggles with logical reasoning\ntasks, highlighting an area that remains underexplored. As an initial effort,\nRISEBench aims to provide foundational insights into reasoning-aware visual\nediting and to catalyze future research. Though still in its early stages, we\nare committed to continuously expanding and refining the benchmark to support\nmore comprehensive, reliable, and scalable evaluations of next-generation\nmultimodal systems. Our code and data will be released at\nhttps://github.com/PhoenixZ810/RISEBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/PQFr3_6S3BeUSNb79jMSO.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/0rGBBJT_JcUPBk5cE_Te0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02826.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ee1379190ddd6214efd73a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
      "fullname": "HAODONG DUAN",
      "name": "KennyUTC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02782",
      "authors": [
        {
          "_id": "67ef502ce803d818f00e1b94",
          "name": "Zhiyuan Yan",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b95",
          "user": {
            "_id": "66978ee0b8656f6506b4acb2",
            "avatarUrl": "/avatars/298acb8222e189fce4368985ee5374a1.svg",
            "isPro": false,
            "fullname": "Junyan Ye",
            "user": "Yejy53",
            "type": "user"
          },
          "name": "Junyan Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:10.032Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b96",
          "user": {
            "_id": "66d5b56c77a026c3d2086a79",
            "avatarUrl": "/avatars/45da07fd82fd455955faa05b27a6393f.svg",
            "isPro": false,
            "fullname": "Weijia Li",
            "user": "liweijia",
            "type": "user"
          },
          "name": "Weijia Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:16:44.819Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b97",
          "user": {
            "_id": "6487e158f675b4a7867f45fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg",
            "isPro": false,
            "fullname": "Zilong Huang",
            "user": "SereinH",
            "type": "user"
          },
          "name": "Zilong Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:56.501Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b98",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:05.246Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b99",
          "user": {
            "_id": "67ef53656c7ba428e7c2e605",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/fFVJTpMRKF15Bf63yZEG_.png",
            "isPro": false,
            "fullname": "He",
            "user": "shawnxyh",
            "type": "user"
          },
          "name": "Xiangyang He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:02.573Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9a",
          "user": {
            "_id": "6459a47e4fe72fae522b4fc9",
            "avatarUrl": "/avatars/a4139f8e348081e45b28dd99d96908d3.svg",
            "isPro": false,
            "fullname": "Kaiqing.Lin",
            "user": "lin6123",
            "type": "user"
          },
          "name": "Kaiqing Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:16:28.452Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9b",
          "user": {
            "_id": "670ddb69d6ac6394419d88c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XxnGNaX3FWug4aiZVjg93.png",
            "isPro": false,
            "fullname": "Jun He",
            "user": "JunHe0915",
            "type": "user"
          },
          "name": "Jun He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:59.877Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9c",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:18.123Z",
          "hidden": false
        },
        {
          "_id": "67ef502ce803d818f00e1b9d",
          "user": {
            "_id": "66135a5e50350afe76beebce",
            "avatarUrl": "/avatars/370a4b83949355feb050c2cb0425c264.svg",
            "isPro": false,
            "fullname": "yl2488",
            "user": "yl2488",
            "type": "user"
          },
          "name": "Li Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:40.281Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:23:16.000Z",
      "submittedOnDailyAt": "2025-04-04T01:51:34.697Z",
      "title": "GPT-ImgEval: Marcos detallados de evaluación para la generación de imágenes de GPT4o",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo del modelo GPT4o de OpenAI ha demostrado capacidades excepcionales en la generación y edición de imágenes, obteniendo resultados muy interesantes en la comunidad. En este informe técnico, se utiliza el primer benchmark de evaluación de proyecciones llamado GPT-ImgEval para evaluar de manera cuantitativa y sincera el rendimiento de GPT-4o en tres aspectos importantes: generación de imágenes, edición y síntesis de significado basada en el mundo. En las tres tareas, GPT-4o muestra un gran rendimiento, superando significativamente los métodos existentes en la control de la generación y la calidad de la salida, y en particular, demostra una capacidad de inferencia de conocimiento. Además, se propone un enfoque basado en modelos de clasificación para investigar la arquitectura potencial del modelo a partir de los datos generados por GPT-4o, y los resultados de los experimentos muestran que el modelo tiene una estructura combinando la auto-regresión (AR) y cabezas basadas en ramificación. También se identificaron y visualizaron los límites concretos que se observan en la generación de imágenes de GPT-4o y se proporcionaron feedback sintéticos. Además, se realizó una comparativa de estudio sobre la edición de imágenes iterativa entre GPT-4o y Gemini 2.0 Flash, y se discutió el impacto en la seguridad de los resultados de GPT-4o. Los códigos y conjuntos de datos utilizados en la evaluación de GPT-4o se pueden obtener en https://github.com/PicoTrex/GPT-ImgEval.",
      "upvotes": 28,
      "discussionId": "67ef502fe803d818f00e1c70",
      "githubRepo": "https://github.com/PicoTrex/GPT-ImgEval",
      "ai_keywords": [
        "auto-regressive (AR)",
        "diffusion-based head",
        "VAR-like architectures",
        "multi-round image editing",
        "image forensic models"
      ]
    },
    "publishedAt": "2025-04-03T13:23:16.000Z",
    "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation",
    "summary": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated\nsurprisingly good capabilities in image generation and editing, resulting in\nsignificant excitement in the community. This technical report presents the\nfirst-look evaluation benchmark (named GPT-ImgEval), quantitatively and\nqualitatively diagnosing GPT-4o's performance across three critical dimensions:\n(1) generation quality, (2) editing proficiency, and (3) world\nknowledge-informed semantic synthesis. Across all three tasks, GPT-4o\ndemonstrates strong performance, significantly surpassing existing methods in\nboth image generation control and output quality, while also showcasing\nexceptional knowledge reasoning capabilities. Furthermore, based on the\nGPT-4o's generated data, we propose a classification-model-based approach to\ninvestigate the underlying architecture of GPT-4o, where our empirical results\nsuggest the model consists of an auto-regressive (AR) combined with a\ndiffusion-based head for image decoding, rather than the VAR-like\narchitectures. We also provide a complete speculation on GPT-4o's overall\narchitecture. In addition, we conduct a series of analyses to identify and\nvisualize GPT-4o's specific limitations and the synthetic artifacts commonly\nobserved in its image generation. We also present a comparative study of\nmulti-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the\nsafety implications of GPT-4o's outputs, particularly their detectability by\nexisting image forensic models. We hope that our work can offer valuable\ninsight and provide a reliable benchmark to guide future research, foster\nreproducibility, and accelerate innovation in the field of image generation and\nbeyond. The codes and datasets used for evaluating GPT-4o can be found at\nhttps://github.com/PicoTrex/GPT-ImgEval.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02587",
      "authors": [
        {
          "_id": "67ef3f9804be7fba0c882738",
          "user": {
            "_id": "633fc70529b5a95f6e15a6b7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
            "isPro": false,
            "fullname": "Yan Ma",
            "user": "ManTle",
            "type": "user"
          },
          "name": "Yan Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:53.820Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c882739",
          "user": {
            "_id": "64b370fe6d953e7c75ede314",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b370fe6d953e7c75ede314/RdP2q3hGXWE4E2zfSv0KU.png",
            "isPro": false,
            "fullname": "Steffi Chern",
            "user": "steffichern",
            "type": "user"
          },
          "name": "Steffi Chern",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:14.660Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273a",
          "user": {
            "_id": "642e4d4d6748dd4f8eeb7732",
            "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
            "isPro": false,
            "fullname": "Xuyang Shen",
            "user": "Ryan1122",
            "type": "user"
          },
          "name": "Xuyang Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:40.397Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273b",
          "user": {
            "_id": "64c525e4d68946edad6c7067",
            "avatarUrl": "/avatars/1b108661634af602717a4ab4b66a151f.svg",
            "isPro": false,
            "fullname": "Yiran Zhong",
            "user": "IanZhong",
            "type": "user"
          },
          "name": "Yiran Zhong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:16.707Z",
          "hidden": false
        },
        {
          "_id": "67ef3f9804be7fba0c88273c",
          "user": {
            "_id": "6144a0c4ff1146bbd84d9865",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661715958139-6144a0c4ff1146bbd84d9865.png",
            "isPro": false,
            "fullname": "Pengfei Liu",
            "user": "Pengfei",
            "type": "user"
          },
          "name": "Pengfei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:17:34.472Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T13:53:28.000Z",
      "submittedOnDailyAt": "2025-04-04T00:42:23.044Z",
      "title": "Reconsidering RL Scaling Applied to Visual Language Models: A Transparent, End-to-End Framework and Detailed Evaluation Scripts",
      "submittedOnDailyBy": {
        "_id": "633fc70529b5a95f6e15a6b7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
        "isPro": false,
        "fullname": "Yan Ma",
        "user": "ManTle",
        "type": "user"
      },
      "summary": "El aprendizaje por refuerzo (RL) ha ganado notoriedad recientemente como una potente posibilidad para mejorar el rendimiento de los modelos de lenguaje de gran escala, y se está desarrollando hacia su aplicación en modelos de lenguaje visual (VLMs). Sin embargo, la aplicación de RL en los VLMs actuales se basa principalmente en arquitecturas de ingeniería complejas que comprometen la reproducibilidad y la accesibilidad, y la falta de protocolos de evaluación estándar ha dificultado la comparación de resultados y la interpretación de los dinámicos de aprendizaje. En este artículo, proponemos una sencilla y funcional pipeline de 4 etapas para el RL en VLMs, proporcionando un marco transparente verificado en diferentes modelos y conjuntos de datos para estandarizar los protocolos de evaluación y proponer técnicas de evaluación estándares para la dinámica de aprendizaje y el comportamiento reflexivo. En experimentos distribuidos sobre tareas de razonamiento visual, se han descubierto los siguientes hallazgos principales: la longitud de la respuesta es sensible a la aleatoriedad, la reflexión tiene una correlación con la longitud del output, y el RL muestra un rendimiento más consistente en la generalización en datos de alta calidad frente a la adaptación de fine-tuning (SFT). Estos hallazgos y el marco propuesto se centran en establecer una línea base reproducible y apoyar una amplia colaboración en la investigación de VLMs basada en RL.",
      "upvotes": 19,
      "discussionId": "67ef3f9904be7fba0c882772",
      "ai_keywords": [
        "reinforcement learning",
        "reasoning capabilities",
        "large language models",
        "vision-language models",
        "reproducibility",
        "accessibility",
        "standardized evaluation protocols",
        "transparent framework",
        "four-step pipeline",
        "training dynamics",
        "reflective behaviors",
        "visual reasoning tasks",
        "response length",
        "reflection",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-04-03T09:53:28.000Z",
    "title": "Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme",
    "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02587.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "633fc70529b5a95f6e15a6b7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
      "fullname": "Yan Ma",
      "name": "ManTle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02436",
      "authors": [
        {
          "_id": "67ef3dfae8b932ae7a832950",
          "user": {
            "_id": "617ba1820e4237bd1731b867",
            "avatarUrl": "/avatars/f9de06363e64bddd7dc977e96e85df8a.svg",
            "isPro": false,
            "fullname": "zhengcong fei",
            "user": "onion",
            "type": "user"
          },
          "name": "Zhengcong Fei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:16.548Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832951",
          "user": {
            "_id": "65dc3a850af7e21ba40e939f",
            "avatarUrl": "/avatars/e129c64617675edd05d4317d39604318.svg",
            "isPro": false,
            "fullname": "Li",
            "user": "Debang",
            "type": "user"
          },
          "name": "Debang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:27.042Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832952",
          "user": {
            "_id": "65bef422fdb8d33cefeaccc3",
            "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
            "isPro": false,
            "fullname": "Qiu Di",
            "user": "diqiu7",
            "type": "user"
          },
          "name": "Di Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:19:41.458Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832953",
          "name": "Jiahua Wang",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832954",
          "name": "Yikun Dou",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832955",
          "user": {
            "_id": "62e0f1314db2175cd270ad08",
            "avatarUrl": "/avatars/1d3d6af6c63557f4abf0484e028fa942.svg",
            "isPro": false,
            "fullname": "Rui Wang",
            "user": "ruiwang",
            "type": "user"
          },
          "name": "Rui Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:11.206Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832956",
          "user": {
            "_id": "666a674967c686801acf25bb",
            "avatarUrl": "/avatars/c1f3edd63fd378dfb555e6413a966932.svg",
            "isPro": false,
            "fullname": "jingtao xu",
            "user": "raul678",
            "type": "user"
          },
          "name": "Jingtao Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:20.880Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832957",
          "user": {
            "_id": "634672bfb7b4e71c7f45360f",
            "avatarUrl": "/avatars/4b646fc3e271be90b9ec619d42ce3e99.svg",
            "isPro": false,
            "fullname": "Fan Mingyuan",
            "user": "MichaelFan",
            "type": "user"
          },
          "name": "Mingyuan Fan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:20:32.597Z",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832958",
          "name": "Guibin Chen",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a832959",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "67ef3dfae8b932ae7a83295a",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T09:50:50.000Z",
      "submittedOnDailyAt": "2025-04-04T00:33:57.000Z",
      "title": "SkyReels-A2: SkyReels-A2: SkyReels-A2: Movie Diffusion Transformer to Create Everything.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "En este artículo se presenta SkyReels-A2, un marco de creación de videos con control. Este marco permite la combinación de cualquier elemento visual (por ejemplo, personajes, objetos, fondos) en un video mediante un programa de cadenas de caracteres, manteniendo una precisa coincidencia con las imágenes de referencia. Este trabajo se conoce como Element-to-Video (E2V) y presenta principalmente los problemas de mantener la autenticidad de los elementos, garantizar la continuidad de las imágenes y lograr un resultado natural. Para abordar estos problemas, primero se diseña una pipeline de datos detallada para construir tuplas de programa de cadenas de caracteres-imágenes de referencia-videos para el entrenamiento del modelo. Luego, se propone un modelo de mapeo común entre imágenes y texto, y se introducen representaciones multi-elemento en el proceso de generación para equilibrar la coincidencia específica de los elementos, la continuidad global y la correspondencia del contexto. Además, se optimiza la pipeline de inferencia para mayor velocidad y estabilidad de salida. Además, se presenta A2 Bench, un marco de evaluación probado, con el objetivo de evaluar el sistema. Las experimentaciones muestran que nuestro marco de trabajo puede generar videos de diferentes calidades y controlar precisamente los elementos. SkyReels-A2 es el primer modelo de nivel comercial abierto en la creación de E2V, y se valora bien comparado con modelos comerciales cerrados desarrollados. Esperamos que SkyReels-A2 supere los límites de la posibilidad de control en la generación de videos en aplicaciones creativas como dramas o negocios virtuales.",
      "upvotes": 17,
      "discussionId": "67ef3dfee8b932ae7a832a97",
      "ai_keywords": [
        "elements-to-video (E2V)",
        "image-text joint embedding model",
        "prompt-reference-video triplets",
        "generative process",
        "multi-element representations",
        "strict consistency",
        "coherent composition",
        "natural outputs",
        "output stability",
        "A2 Bench (benchmark)",
        "high-quality videos",
        "precise element control",
        "open-source commercial grade model"
      ]
    },
    "publishedAt": "2025-04-03T05:50:50.000Z",
    "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
    "summary": "This paper presents SkyReels-A2, a controllable video generation framework\ncapable of assembling arbitrary visual elements (e.g., characters, objects,\nbackgrounds) into synthesized videos based on textual prompts while maintaining\nstrict consistency with reference images for each element. We term this task\nelements-to-video (E2V), whose primary challenges lie in preserving the\nfidelity of each reference element, ensuring coherent composition of the scene,\nand achieving natural outputs. To address these, we first design a\ncomprehensive data pipeline to construct prompt-reference-video triplets for\nmodel training. Next, we propose a novel image-text joint embedding model to\ninject multi-element representations into the generative process, balancing\nelement-specific consistency with global coherence and text alignment. We also\noptimize the inference pipeline for both speed and output stability. Moreover,\nwe introduce a carefully curated benchmark for systematic evaluation, i.e, A2\nBench. Experiments demonstrate that our framework can generate diverse,\nhigh-quality videos with precise element control. SkyReels-A2 is the first\nopen-source commercial grade model for the generation of E2V, performing\nfavorably against advanced closed-source commercial models. We anticipate\nSkyReels-A2 will advance creative applications such as drama and virtual\ne-commerce, pushing the boundaries of controllable video generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02436.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6573
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00502",
      "authors": [
        {
          "_id": "67ef72898667ee5c99026d16",
          "user": {
            "_id": "67014d33126f9ab39fc52481",
            "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
            "isPro": false,
            "fullname": "Qianhao Yuan",
            "user": "yuanqianhao",
            "type": "user"
          },
          "name": "Qianhao Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:23:28.149Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d17",
          "name": "Qingyu Zhang",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d18",
          "name": "Yanjiang Liu",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d19",
          "user": {
            "_id": "654c7fbe6b51714c2a6ff590",
            "avatarUrl": "/avatars/db217415c56730872b9a807f3afb4e5b.svg",
            "isPro": false,
            "fullname": "Jiawei Chen",
            "user": "chenjiawei-icip",
            "type": "user"
          },
          "name": "Jiawei Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:04.134Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1a",
          "user": {
            "_id": "6216496a9b34d2fb49144599",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg",
            "isPro": false,
            "fullname": "Yaojie Lu",
            "user": "luyaojie",
            "type": "user"
          },
          "name": "Yaojie Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:09.511Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1b",
          "user": {
            "_id": "6711c702f858a456b4b9f3a4",
            "avatarUrl": "/avatars/178e9567c3111ab22717c3c0dd003a6a.svg",
            "isPro": false,
            "fullname": "Hongyu  Lin",
            "user": "sanmusunrise",
            "type": "user"
          },
          "name": "Hongyu Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:15.188Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1c",
          "name": "Jia Zheng",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1d",
          "user": {
            "_id": "65e99a77e71555ed193609cf",
            "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
            "isPro": false,
            "fullname": "Xianpei Han",
            "user": "xphan",
            "type": "user"
          },
          "name": "Xianpei Han",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:23.046Z",
          "hidden": false
        },
        {
          "_id": "67ef72898667ee5c99026d1e",
          "name": "Le Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T07:47:55.000Z",
      "submittedOnDailyAt": "2025-04-04T04:19:46.946Z",
      "title": "Inespecíficamente efectivos capas de imagen token de freo para modelos de lenguaje multimodelo eficiente",
      "submittedOnDailyBy": {
        "_id": "67014d33126f9ab39fc52481",
        "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
        "isPro": false,
        "fullname": "Qianhao Yuan",
        "user": "yuanqianhao",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande de Diálogo Multimodal (MLLMs) suelen incurrir en altos costos de cálculo debido a su gran escala y el uso de tokens visuales extensos. En este artículo, se introduce un nuevo métrico llamado \"Contribución de la Capa (LC)\" para cuantificar el impacto que tiene la transformación de una capa en los tokens visuales y contextuales. El cálculo de LC se realiza mediante la medida de la varianza entre el modelo y la transformación de una capa que elimina los tokens específicos. Las pruebas de fatiga demuestran claramente que muchas capas de MLLMs contribuyen mínimamente al procesamiento de tokens visuales. Basándose en estas observaciones, se propone un método de \"ShortV\" que utiliza LC para identificar capas inútiles y detener la actualización de tokens visuales en estas capas, sin necesidad de entrenamiento adicional. Los resultados de los experimentos muestran que ShortV puede detener la actualización de tokens visuales en aproximadamente el 60% de las capas de MLLM, reduciendo significativamente los costos de cálculo asociados. Por ejemplo, en el modelo LLaVA-NeXT-13B, se puede reducir los FLOPs en un 50% mientras se mantiene la calidad del rendimiento. El código está disponible en https://github.com/icip-cas/ShortV.",
      "upvotes": 11,
      "discussionId": "67ef728a8667ee5c99026d69",
      "githubRepo": "https://github.com/icip-cas/ShortV",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Layer Contribution (LC)",
        "visual tokens",
        "transformations",
        "layer-wise redundancy",
        "model output",
        "divergence",
        "ineffective layers",
        "training-free method",
        "visual token updates",
        "computational costs",
        "FLOPs",
        "LLaVA-NeXT-13B"
      ]
    },
    "publishedAt": "2025-04-01T03:47:55.000Z",
    "title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual\n  Tokens in Ineffective Layers",
    "summary": "Multimodal Large Language Models (MLLMs) suffer from high computational costs\ndue to their massive size and the large number of visual tokens. In this paper,\nwe investigate layer-wise redundancy in MLLMs by introducing a novel metric,\nLayer Contribution (LC), which quantifies the impact of a layer's\ntransformations on visual and text tokens, respectively. The calculation of LC\ninvolves measuring the divergence in model output that results from removing\nthe layer's transformations on the specified tokens. Our pilot experiment\nreveals that many layers of MLLMs exhibit minimal contribution during the\nprocessing of visual tokens. Motivated by this observation, we propose ShortV,\na training-free method that leverages LC to identify ineffective layers, and\nfreezes visual token updates in these layers. Experiments show that ShortV can\nfreeze visual token in approximately 60\\% of the MLLM layers, thereby\ndramatically reducing computational costs related to updating visual tokens.\nFor example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while\nmaintaining superior performance. The code will be publicly available at\nhttps://github.com/icip-cas/ShortV",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67014d33126f9ab39fc52481",
      "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
      "fullname": "Qianhao Yuan",
      "name": "yuanqianhao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02542",
      "authors": [
        {
          "_id": "67ef3773ac0c701df7fd98aa",
          "user": {
            "_id": "6264a7dfc39850dc093eb68a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650763566575-noauth.png",
            "isPro": false,
            "fullname": "Fa-Ting Hong",
            "user": "HarlanHong",
            "type": "user"
          },
          "name": "Fa-Ting Hong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:38.641Z",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ab",
          "user": {
            "_id": "6481523b3fb124fc9850afed",
            "avatarUrl": "/avatars/ddde178c88713662800aafd2343647a4.svg",
            "isPro": false,
            "fullname": "Zunnan Xu",
            "user": "xuzn",
            "type": "user"
          },
          "name": "Zunnan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:23.733Z",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ac",
          "name": "Zixiang Zhou",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ad",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98ae",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98af",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98b0",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "67ef3773ac0c701df7fd98b1",
          "user": {
            "_id": "66feab48651e00e22f33222e",
            "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
            "isPro": false,
            "fullname": "Dan Xu",
            "user": "danxuhk",
            "type": "user"
          },
          "name": "Dan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:20.987Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/VOMaQBXsDjs1295R5NVOh.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/tksGfiG1zEDZ4QUg4IorF.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/aUp8tmNM2pdPYCa3A3Noh.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/MpCNSPEP6OSjZR_0m70PP.mp4"
      ],
      "publishedAt": "2025-04-03T12:44:41.000Z",
      "submittedOnDailyAt": "2025-04-04T01:37:45.934Z",
      "title": "Modelado de Vídeo con Control de Voz por Estados Escondidos, Estados de Selección Espacial Mascarados, Modelado de Token Naturales con Cabeza de Generación",
      "submittedOnDailyBy": {
        "_id": "66feab48651e00e22f33222e",
        "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
        "isPro": false,
        "fullname": "Dan Xu",
        "user": "danxuhk",
        "type": "user"
      },
      "summary": "Talking head synthesis es crucial en video-to-video y en la interacción humano-computadora, pero los métodos actuales suelen estar limitados por su control basado en una única modelo principal, lo que dificulta su aplicación práctica. En este sentido, presentamos ACTalker. ACTalker es un marco de trabajo para la generación de vídeos que apoya tanto control multi-signal como control uni-signal. Para el control multi-signal, diseñamos una estructura mamba paralela, permitiendo que cada sección utilice un diferente señal de control para manipular específicamente regiones faciales. Aplicamos una estructura de gate en cada sección para proporcionar un control flexible en la generación de vídeos. Para garantizar asociaciones naturales en el tiempo y espacio, utilizamos la estructura mamba, lo que permite a cada sección manipular caracteres de tokens en ambos ejes. Además, introducimos la estrategia de mask drop para permitir que cada señal de control pueda controlar independientemente las regiones faciales correspondientes dentro de la estructura mamba, evitando así conflictos. A través de los resultados experimentales, demostramos que nuestro método genera vídeos de caras naturales moviles con múltiples señales de control, y que las capas mamba pueden integrar de manera continua y sin conflictos múltiples modelos de control.",
      "upvotes": 8,
      "discussionId": "67ef3775ac0c701df7fd994c",
      "projectPage": "https://harlanhong.github.io/publications/actalker/index.html",
      "githubRepo": "https://github.com/harlanhong/ACTalker",
      "ai_keywords": [
        "ACTalker",
        "video diffusion framework",
        "multi-signals control",
        "parallel mamba structure",
        "driving signals",
        "gate mechanism",
        "temporal coordination",
        "spatial coordination",
        "feature tokens",
        "mask-drop strategy",
        "facial videos",
        "multiple driving modalities"
      ]
    },
    "publishedAt": "2025-04-03T08:44:41.000Z",
    "title": "Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation",
    "summary": "Talking head synthesis is vital for virtual avatars and human-computer\ninteraction. However, most existing methods are typically limited to accepting\ncontrol from a single primary modality, restricting their practical utility. To\nthis end, we introduce ACTalker, an end-to-end video diffusion\nframework that supports both multi-signals control and single-signal control\nfor talking head video generation. For multiple control, we design a parallel\nmamba structure with multiple branches, each utilizing a separate driving\nsignal to control specific facial regions. A gate mechanism is applied across\nall branches, providing flexible control over video generation. To ensure\nnatural coordination of the controlled video both temporally and spatially, we\nemploy the mamba structure, which enables driving signals to manipulate feature\ntokens across both dimensions in each branch. Additionally, we introduce a\nmask-drop strategy that allows each driving signal to independently control its\ncorresponding facial region within the mamba structure, preventing control\nconflicts. Experimental results demonstrate that our method produces\nnatural-looking facial videos driven by diverse signals and that the mamba\nlayer seamlessly integrates multiple driving modalities without conflict.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/VOMaQBXsDjs1295R5NVOh.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/tksGfiG1zEDZ4QUg4IorF.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/aUp8tmNM2pdPYCa3A3Noh.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/MpCNSPEP6OSjZR_0m70PP.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02542.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66feab48651e00e22f33222e",
      "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
      "fullname": "Dan Xu",
      "name": "danxuhk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02507",
      "authors": [
        {
          "_id": "67ef5a3d4417508df8d99dad",
          "user": {
            "_id": "62cd4b03c5cc157be82f0b56",
            "avatarUrl": "/avatars/351e963c1c763d507ae78cbcd62966a3.svg",
            "isPro": false,
            "fullname": "Abhay kumar",
            "user": "akanyaani",
            "type": "user"
          },
          "name": "Abhay Kumar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:18.512Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99dae",
          "user": {
            "_id": "6071c4b270e11b30cfcfd7a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
            "isPro": false,
            "fullname": "Louis Owen",
            "user": "louisowen6",
            "type": "user"
          },
          "name": "Louis Owen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:21.051Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99daf",
          "user": {
            "_id": "645a0d3dd6648853107c5fdc",
            "avatarUrl": "/avatars/1e3b6a4f5ce81a707ba7cbdf81631091.svg",
            "isPro": false,
            "fullname": "Nilabhra Roy Chowdhury",
            "user": "nilabhra",
            "type": "user"
          },
          "name": "Nilabhra Roy Chowdhury",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:12.764Z",
          "hidden": false
        },
        {
          "_id": "67ef5a3d4417508df8d99db0",
          "user": {
            "_id": "65e4be59e8b017ee1310a1b6",
            "avatarUrl": "/avatars/c3f7cdf5d0859cb80bfb2b970a675dfa.svg",
            "isPro": false,
            "fullname": "Fabian",
            "user": "gueraf",
            "type": "user"
          },
          "name": "Fabian Güra",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:16.132Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T11:41:55.000Z",
      "submittedOnDailyAt": "2025-04-04T02:34:36.631Z",
      "title": "ZClip: Adaptación del estrés de habla en el aprendizaje previo de modelos de LLM",
      "submittedOnDailyBy": {
        "_id": "6071c4b270e11b30cfcfd7a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
        "isPro": false,
        "fullname": "Louis Owen",
        "user": "louisowen6",
        "type": "user"
      },
      "summary": "Los grandes modelos de lenguaje (LLMs) en su entrenamiento enfrentan múltiples problemas como la instabilidad de gradientes y picos de pérdida. Estos fenómenos pueden provocar desastres de gradientes, lo que requiere la recuperación de puntos de control de costo alto y el salto de batches de datos. Las técnicas de clipping del gradiente basadas en umbrales fijos o heurísticas no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales fijos o heurísticas, no pueden resolver efectivamente estos problemas. Estas técnicas, basadas en umbrales",
      "upvotes": 7,
      "discussionId": "67ef5a3e4417508df8d99dfc",
      "githubRepo": "https://github.com/bluorion-com/ZClip/",
      "ai_keywords": [
        "large language models (LLMs)",
        "gradient instability",
        "loss spikes",
        "catastrophic divergence",
        "checkpoint restoration",
        "data batch skipping",
        "traditional gradient clipping techniques",
        "norm-based methods",
        "adaptive gradient clipping",
        "clipping threshold",
        "statistical properties of gradient norms",
        "z-score-based anomaly detection",
        "malignant loss spikes",
        "convergence"
      ]
    },
    "publishedAt": "2025-04-03T07:41:55.000Z",
    "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
    "summary": "Training large language models (LLMs) presents numerous challenges, including\ngradient instability and loss spikes. These phenomena can lead to catastrophic\ndivergence, requiring costly checkpoint restoration and data batch skipping.\nTraditional gradient clipping techniques, such as constant or norm-based\nmethods, fail to address these issues effectively due to their reliance on\nfixed thresholds or heuristics, leading to inefficient learning and requiring\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\ngradient clipping algorithm that dynamically adjusts the clipping threshold\nbased on statistical properties of gradient norms over time. Unlike prior\nreactive strategies, ZClip proactively adapts to training dynamics without\nmaking any prior assumptions on the scale and the temporal evolution of\ngradient norms. At its core, it leverages z-score-based anomaly detection to\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\nwhile not interfering with convergence otherwise. Our code is available at:\nhttps://github.com/bluorion-com/ZClip.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6071c4b270e11b30cfcfd7a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
      "fullname": "Louis Owen",
      "name": "louisowen6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02398",
      "authors": [
        {
          "_id": "67ef63b5e8b932ae7a8d3043",
          "user": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "isPro": false,
            "fullname": "Gallil Maimon",
            "user": "gallilmaimon",
            "type": "user"
          },
          "name": "Gallil Maimon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:06.509Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3044",
          "user": {
            "_id": "6547411a9295970f878aa52e",
            "avatarUrl": "/avatars/6e240f0add27bf1a6c04a9618eccdf83.svg",
            "isPro": false,
            "fullname": "Michael Hassid",
            "user": "hassid",
            "type": "user"
          },
          "name": "Michael Hassid",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:39.934Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3045",
          "user": {
            "_id": "64b7b7b38ba7d6c922d753d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7b7b38ba7d6c922d753d6/rt0thjYa84VZHy1BEcW4p.jpeg",
            "isPro": false,
            "fullname": "Amit Roth",
            "user": "MajoRoth",
            "type": "user"
          },
          "name": "Amit Roth",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:48.483Z",
          "hidden": false
        },
        {
          "_id": "67ef63b5e8b932ae7a8d3046",
          "user": {
            "_id": "6481e135578646b5c2386728",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6481e135578646b5c2386728/SPva4iNw0pORiCXD45cx9.jpeg",
            "isPro": false,
            "fullname": "Yossi Adi",
            "user": "adiyoss",
            "type": "user"
          },
          "name": "Yossi Adi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:24:54.851Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/wqUq-bT-DvKoNybPX46uL.png"
      ],
      "publishedAt": "2025-04-03T08:46:56.000Z",
      "submittedOnDailyAt": "2025-04-04T03:52:15.607Z",
      "title": "Intralíver de la Speech - Análisis de Escalado de Modelos de Lenguaje Textual",
      "submittedOnDailyBy": {
        "_id": "66b9bc2dacdbc1d0b39c3b50",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
        "isPro": false,
        "fullname": "Gallil Maimon",
        "user": "gallilmaimon",
        "type": "user"
      },
      "summary": "Actualmente, el análisis de escalado de modelos de lenguaje (SLM) está describiendo jugadores oscuros. Ellos predicen que los SLM necesitarán una gran cantidad de cálculos y datos en comparación con los documentos, lo que genera dudas sobre la posibilidad de entrenar SLMs de alta calidad. Sin embargo, los SLMs modernos se inicializan generalmente mediante un método que cruza lenguaje y texto, desde modelos preentrenados TextLM, y están diseñados para permitir la transmisión de conocimiento. Esto plantea la pregunta de si un SLM sin interrupciones puede escalarse de manera más eficiente que un SLM sin texto. Este artículo resuelve estos problemas de manera estricta y muestra resultados positivos. Hemos realizado un análisis de escalado de un SLM sin interrupciones, entrenamos decenas de modelos y analizamos las tendencias de la escalado. En esta configuración, hemos descubierto que los SLMs pueden escalarse de manera más eficiente con respecto a la cantidad de cálculos. Además, nuestros resultados muestran que la escalado es dinámicamente diferente a la de un SLM sin texto, lo que indica que es necesario manejar la cantidad de cálculos al expandir el tamaño del modelo. También hemos investigado el papel de los datos sintéticos y los familias de modelos TextLM, intentando desarrollar estas potencialidades. Los resultados muestran que nuestros modelos de escalado logran alcanzar rendimientos similares a los líderes como el modelo de evaluación de lenguaje, utilizando un mínimo de cálculos y datos. Los modelos, muestras y datos están disponibles - https://pages.cs.huji.ac.il/adiyoss-lab/sims.",
      "upvotes": 7,
      "discussionId": "67ef63b6e8b932ae7a8d306d",
      "projectPage": "https://pages.cs.huji.ac.il/adiyoss-lab/sims/",
      "githubRepo": "https://github.com/slp-rl/slamkit",
      "ai_keywords": [
        "Speech Language Model (SLM)",
        "TextLMs",
        "speech-text interleaving",
        "scaling analysis",
        "compute",
        "knowledge transfer",
        "textless-SLMs",
        "scaling trends",
        "scaling-dynamics",
        "training tokens",
        "synthetic data",
        "model families",
        "speech semantic metrics"
      ]
    },
    "publishedAt": "2025-04-03T04:46:56.000Z",
    "title": "Scaling Analysis of Interleaved Speech-Text Language Models",
    "summary": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture.\nThey predict that SLMs require much more compute and data compared to text,\nleading some to question the feasibility of training high-quality SLMs.\nHowever, modern SLMs are often initialised from pre-trained TextLMs using\nspeech-text interleaving to allow knowledge transfer. This raises the question\n- Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper\nwe answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by\ntraining several dozen and analysing the scaling trends. We see that under this\nsetup SLMs scale more efficiently with compute. Additionally, our results\nindicate that the scaling-dynamics are significantly different than\ntextless-SLMs, suggesting one should allocate notably more of the compute\nbudget for increasing model size over training tokens. We also study the role\nof synthetic data and TextLM model families in unlocking this potential.\nResults suggest, that our scaled up model achieves comparable performance with\nleading models on speech semantic metrics while using less compute and data\nthan other approaches. We open source models, samples, and data -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/sims.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/wqUq-bT-DvKoNybPX46uL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02398.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b9bc2dacdbc1d0b39c3b50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
      "fullname": "Gallil Maimon",
      "name": "gallilmaimon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02012",
      "authors": [
        {
          "_id": "67ef5af0724d484dd41afe5c",
          "user": {
            "_id": "66189b980da4c017c401fb5d",
            "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
            "isPro": false,
            "fullname": "soro bedio",
            "user": "bedio",
            "type": "user"
          },
          "name": "Soro Bedionita",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:51:20.621Z",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5d",
          "name": "Bruno Andreis",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5e",
          "name": "Song Chong",
          "hidden": false
        },
        {
          "_id": "67ef5af0724d484dd41afe5f",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T05:50:19.000Z",
      "submittedOnDailyAt": "2025-04-04T02:38:11.321Z",
      "title": "Instrucciones de Guía para la Creación de Parámetros de Red Aggregate Network del Dolar Norlive",
      "submittedOnDailyBy": {
        "_id": "66189b980da4c017c401fb5d",
        "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
        "isPro": false,
        "fullname": "soro bedio",
        "user": "bedio",
        "type": "user"
      },
      "summary": "Según las reglas del aprendizaje de Turkers, la generación de parámetros de red neuronal en condiciones específicas es crucial para la adaptabilidad del modelo y el desarrollo de aprendizaje de patrones. Los métodos actuales, especialmente los basados en redes neuronales profundas, presentan limitaciones en la escalabilidad hacia grandes arquitecturas, inflexibilidad al manejar diferentes profundidades de redes y la separación de la generación de parámetros que destruye la colaboración entre capas indirectas. En este estudio, se propone un marco de trabajo automático de recuperación uniforme para la generación de parámetros, denominado IGPG (Generación de Parámetros de Instrucción de Parámetros), que se aplica a diferentes tareas y arquitecturas. IGPG utiliza VQ-VAE y modelos de recuperación automático para generar parámetros de red neuronal basándose en instrucciones de tarea, conjuntos de datos y detalles de arquitectura. Funciona de manera automática de recuperación creando pesos de la red como tokens. IGPG asegura la colaboración entre capas indirectas y permite una adaptación eficiente entre el modelo y los conjuntos de datos. Operando a nivel de tokens, IGPG captura efectivamente la distribución compleja de parámetros reducidas a partir de modelos bien entrenados. Experimentos difusos en conjuntos de datos visuales muestran que IGPG integra modelos bien entrenados en un solo marco de generación flexible, obteniendo resultados competitivos o superiores a los métodos de vanguardia. En particular, en términos de escalabilidad y eficiencia para grandes arquitecturas, IGPG destaca la posibilidad de ser un potente instrumento para la búsqueda de pesos entrenados, selección de modelos y ajuste rápido de tareas específicas.",
      "upvotes": 5,
      "discussionId": "67ef5af1724d484dd41afef3",
      "ai_keywords": [
        "diffusion models",
        "IGPG (Instruction Guided Parameter Generation)",
        "VQ-VAE",
        "autoregressive framework",
        "token level",
        "parameter synthesis",
        "inter-layer coherence",
        "vision datasets",
        "pretrained models",
        "pretrained weight retrieval",
        "model selection",
        "task-specific fine-tuning"
      ]
    },
    "publishedAt": "2025-04-02T01:50:19.000Z",
    "title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
    "summary": "Learning to generate neural network parameters conditioned on task\ndescriptions and architecture specifications is pivotal for advancing model\nadaptability and transfer learning. Existing methods especially those based on\ndiffusion models suffer from limited scalability to large architectures,\nrigidity in handling varying network depths, and disjointed parameter\ngeneration that undermines inter-layer coherence. In this work, we propose IGPG\n(Instruction Guided Parameter Generation), an autoregressive framework that\nunifies parameter synthesis across diverse tasks and architectures. IGPG\nleverages a VQ-VAE and an autoregressive model to generate neural network\nparameters, conditioned on task instructions, dataset, and architecture\ndetails. By autoregressively generating neural network weights' tokens, IGPG\nensures inter-layer coherence and enables efficient adaptation across models\nand datasets. Operating at the token level, IGPG effectively captures complex\nparameter distributions aggregated from a broad spectrum of pretrained models.\nExtensive experiments on multiple vision datasets demonstrate that IGPG\nconsolidates diverse pretrained models into a single, flexible generative\nframework. The synthesized parameters achieve competitive or superior\nperformance relative to state-of-the-art methods, especially in terms of\nscalability and efficiency when applied to large architectures. These results\nunderscore ICPG potential as a powerful tool for pretrained weight retrieval,\nmodel selection, and rapid task-specific fine-tuning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66189b980da4c017c401fb5d",
      "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
      "fullname": "soro bedio",
      "name": "bedio",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02119",
      "authors": [
        {
          "_id": "67ef41e7efcb0a2fbfbb6a32",
          "user": {
            "_id": "670826649e319cca029ff240",
            "avatarUrl": "/avatars/6d12b3abf75f714d75d1775d88885345.svg",
            "isPro": false,
            "fullname": "rtfvbhkuj",
            "user": "wwdd7718",
            "type": "user"
          },
          "name": "Wang Wei",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-04T02:20:24.253Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a33",
          "user": {
            "_id": "66e4e50a52356419c4a1ad14",
            "avatarUrl": "/avatars/4be3ce17671785cbe7126b9c1141478b.svg",
            "isPro": false,
            "fullname": "Tiankai Yang",
            "user": "tiankaiy",
            "type": "user"
          },
          "name": "Tiankai Yang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-04T02:25:15.747Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a34",
          "name": "Hongjie Chen",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a35",
          "user": {
            "_id": "62a3ab83e4dd6252344d27cd",
            "avatarUrl": "/avatars/7ca8510f70a58dc207b104240e30c35c.svg",
            "isPro": false,
            "fullname": "Ryan A. Rossi",
            "user": "ryanrossi",
            "type": "user"
          },
          "name": "Ryan A. Rossi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:23:05.421Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a36",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a37",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-04T02:34:51.212Z",
          "hidden": false
        },
        {
          "_id": "67ef41e7efcb0a2fbfbb6a38",
          "name": "Hoda Eldardiry",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T20:33:27.000Z",
      "submittedOnDailyAt": "2025-04-04T00:50:35.167Z",
      "title": "\"Método Eficiente para la Selección de Modelos en Predicción de Serie Temporal Utilizando LLMs\"",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "La selección de modelos es uno de los pasos importantes en la predicción de secuencias temporales, y hasta ahora ha sido necesario una evaluación muy estricta de rendimiento para diferentes conjuntos de datos. El enfoque de meta aprendizaje ha intentado automatizar este proceso, pero generalmente depende de la construcción previa de métricas de rendimiento, lo cual es costoso. En este estudio, proponemos utilizar Grandes Modelos de Lenguaje (LLMs) como alternativas ligeras para la selección de modelos. Nuestro método utiliza los conocimientos y capacidades de inferencia propias de los LLMs para eliminar la necesidad de métricas de rendimiento explícitas. A través de experimentos extendidos con LLaMA, GPT y Gemini, demostramos que nuestro enfoque supera los métodos tradicionales de meta aprendizaje y de línea de trabajo heurístico, y reduce significativamente el sobrecargo computacional. Estos hallazgos subrayan la posibilidad de que los LLMs puedan ser eficientes en la selección de modelos para la predicción de secuencias temporales.",
      "upvotes": 4,
      "discussionId": "67ef41e8efcb0a2fbfbb6a93",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "model selection",
        "time series forecasting",
        "meta-learning approaches",
        "pre-constructed performance matrices",
        "reasoning capabilities",
        "experiments",
        "LLaMA",
        "GPT",
        "Gemini",
        "heuristic baselines",
        "computational overhead"
      ]
    },
    "publishedAt": "2025-04-02T16:33:27.000Z",
    "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
    "summary": "Model selection is a critical step in time series forecasting, traditionally\nrequiring extensive performance evaluations across various datasets.\nMeta-learning approaches aim to automate this process, but they typically\ndepend on pre-constructed performance matrices, which are costly to build. In\nthis work, we propose to leverage Large Language Models (LLMs) as a lightweight\nalternative for model selection. Our method eliminates the need for explicit\nperformance matrices by utilizing the inherent knowledge and reasoning\ncapabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini,\nwe demonstrate that our approach outperforms traditional meta-learning\ntechniques and heuristic baselines, while significantly reducing computational\noverhead. These findings underscore the potential of LLMs in efficient model\nselection for time series forecasting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02119.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00891",
      "authors": [
        {
          "_id": "67ef62342a18e60aeee0ea02",
          "name": "Jian Zhao",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea03",
          "user": {
            "_id": "667187ba9ab144eb3ac43a1b",
            "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
            "isPro": false,
            "fullname": "Runze Liu",
            "user": "RyanLiu112",
            "type": "user"
          },
          "name": "Runze Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:08:09.923Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea04",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:22.709Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea05",
          "name": "Zhimu Zhou",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea06",
          "user": {
            "_id": "67ab05fe4c6ca2d5db4c0c52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QpGUNDkeuKjX71s2GXlXF.png",
            "isPro": false,
            "fullname": "Junqi Gao",
            "user": "ChetKao",
            "type": "user"
          },
          "name": "Junqi Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:38.624Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea07",
          "name": "Dong Li",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea08",
          "user": {
            "_id": "6562db314e8918182da42706",
            "avatarUrl": "/avatars/b113bbbb496bf4dac254f0e840f08e10.svg",
            "isPro": false,
            "fullname": "Jiafei Lyu",
            "user": "dmux",
            "type": "user"
          },
          "name": "Jiafei Lyu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:25:45.394Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea09",
          "user": {
            "_id": "65b34c5785b6c2144807db37",
            "avatarUrl": "/avatars/4c1cb03cda250d4ec760ebf7815a3bce.svg",
            "isPro": false,
            "fullname": "Qianzhouyi",
            "user": "Saputello",
            "type": "user"
          },
          "name": "Zhouyi Qian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:00.763Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0a",
          "user": {
            "_id": "645d9c3058f9ee315148116d",
            "avatarUrl": "/avatars/165e18f27b5a50738bf1d22857118478.svg",
            "isPro": false,
            "fullname": "Biqing Qi",
            "user": "jackqi7",
            "type": "user"
          },
          "name": "Biqing Qi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:06.517Z",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0b",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67ef62342a18e60aeee0ea0c",
          "user": {
            "_id": "669f614b59adf5b56e05bce3",
            "avatarUrl": "/avatars/ffd4189efbceb0e63a03db273065a44b.svg",
            "isPro": false,
            "fullname": "BowenZhou",
            "user": "bowenZhou",
            "type": "user"
          },
          "name": "Bowen Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-04T07:26:14.312Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T15:21:05.000Z",
      "submittedOnDailyAt": "2025-04-04T03:13:15.991Z",
      "title": "GenPRM: Teoría de la Compensación de Procesos para Ampliar la Cantidad de Cálculos en la Prueba de la Teoría de Razones Generativas",
      "submittedOnDailyBy": {
        "_id": "667187ba9ab144eb3ac43a1b",
        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
        "isPro": false,
        "fullname": "Runze Liu",
        "user": "RyanLiu112",
        "type": "user"
      },
      "summary": "Recientemente, se ha sugerido que utilizar modelos de recompensa de proceso (PRMs) como validadores para mejorar el rendimiento de modelos grandes de lenguaje (LLMs) es adecuado. Sin embargo, los PRMs actuales enfrentan tres problemas importantes: 1) limitaciones en el control y la capacidad de generalización del proceso, 2) dependencia en la predicción escalar sin aprovechar la capacidad generativa de los LLMs, y 3) no siempre es posible escalar la cantidad de cálculos en el test. En este artículo, se presenta un modelo de recompensa de proceso generativo (GenPRM) que realiza la verificación de la lógica de cadena de pensamiento (CoT) y el código, y proporciona una evaluación de cada etapa lógica, permitiendo obtener etiquetas de control del proceso y datos de razonamiento de alta calidad. Se propone el Evaluación de Progreso Relativo (RPE) y un marco de síntesis de razones para demostrar que GenPRM, utilizando 23K datos de entrenamiento en el conjunto de datos MATH, supera significativamente a los PRMs existentes. En función de la escalabilidad en el test, el GenPRM de 1.5B supera a GPT-4o, mientras que el GenPRM de 7B supera a Qwen2.5-Math-PRM-72B en el ProcessBench. Además, GenPRM muestra una fuerte capacidad como modelo de evaluación para el entrenamiento de modelos de política. Este artículo recomienda un nuevo paradigma para cerrar la brecha entre PRMs y modelos de evaluación, y los códigos, modelos y datos están disponibles en https://ryanliu112.github.io/GenPRM.",
      "upvotes": 4,
      "discussionId": "67ef62352a18e60aeee0ea4b",
      "projectPage": "https://ryanliu112.github.io/GenPRM",
      "githubRepo": "https://github.com/RyanLiu112/GenPRM",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Process Reward Models (PRMs)",
        "Chain-of-Thought (CoT) reasoning",
        "Relative Progress Estimation (RPE)",
        "ProcessBench",
        "MATH dataset",
        "GPT-4",
        "Qwen2.5-Math-PRM-72B",
        "critic model",
        "policy model refinement"
      ]
    },
    "publishedAt": "2025-04-01T11:21:05.000Z",
    "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning",
    "summary": "Recent advancements in Large Language Models (LLMs) have shown that it is\npromising to utilize Process Reward Models (PRMs) as verifiers to enhance the\nperformance of LLMs. However, current PRMs face three key challenges: (1)\nlimited process supervision and generalization capabilities, (2) dependence on\nscalar value prediction without leveraging the generative abilities of LLMs,\nand (3) inability to scale the test-time compute of PRMs. In this work, we\nintroduce GenPRM, a generative process reward model that performs explicit\nChain-of-Thought (CoT) reasoning with code verification before providing\njudgment for each reasoning step. To obtain high-quality process supervision\nlabels and rationale data, we propose Relative Progress Estimation (RPE) and a\nrationale synthesis framework that incorporates code verification. Experimental\nresults on ProcessBench and several mathematical reasoning tasks show that\nGenPRM significantly outperforms prior PRMs with only 23K training data from\nMATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and\na 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally,\nGenPRM demonstrates strong abilities to serve as a critic model for policy\nmodel refinement. This work establishes a new paradigm for process supervision\nthat bridges the gap between PRMs and critic models in LLMs. Our code, model,\nand data will be available in https://ryanliu112.github.io/GenPRM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00891.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667187ba9ab144eb3ac43a1b",
      "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
      "fullname": "Runze Liu",
      "name": "RyanLiu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.22444",
      "authors": [
        {
          "_id": "67ef33a4456bcf30fa95b2f1",
          "user": {
            "_id": "655fb8a122ce47e5fa491c72",
            "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
            "isPro": false,
            "fullname": "Pengsong Zhang",
            "user": "universea",
            "type": "user"
          },
          "name": "Pengsong Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-04T07:09:42.152Z",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f2",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f3",
          "name": "Huazhe Xu",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f4",
          "name": "Renjun Xu",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f5",
          "name": "Zhenting Wang",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f6",
          "name": "Cong Wang",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f7",
          "name": "Animesh Garg",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f8",
          "name": "Zhibin Li",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2f9",
          "name": "Arash Ajoudani",
          "hidden": false
        },
        {
          "_id": "67ef33a4456bcf30fa95b2fa",
          "name": "Xinyu Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655fb8a122ce47e5fa491c72/d-38XQKRw3vnyFhzZFTEd.jpeg"
      ],
      "publishedAt": "2025-03-28T14:00:27.000Z",
      "submittedOnDailyAt": "2025-04-04T06:29:27.852Z",
      "title": "La ley de la escala de los descubrimientos científicos hechos por IA y científicos de robots",
      "submittedOnDailyBy": {
        "_id": "655fb8a122ce47e5fa491c72",
        "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
        "isPro": false,
        "fullname": "Pengsong Zhang",
        "user": "universea",
        "type": "user"
      },
      "summary": "La investigación científica se espera que avance rápidamente debido a la alta tecnología de los robots y la inteligencia artificial. Actualmente, la investigación científica consume mucho tiempo y recursos en experimentos manuales, y la integración de conocimientos en diversas áreas científicas requiere conocimientos que exceden el dominio de un solo investigador. Aquí imaginamos el concepto de un científico general automático (AGS). Este concepto tiene como objetivo automatizar completamente la ciclo de investigación mediante la combinación efectiva de IA y robots abstractos. Este sistema promueve la interacción física y visual y apoya la integración de conocimientos en diversas áreas científicas. Estas tecnologías pueden expandir directamente todas las etapas de la investigación (busqueda de literatura, generación de hipótesis, experimentos, redacción de informes de investigación) y, con la introducción de reflexión interna y retroalimentación externa, pueden reducir significativamente el tiempo y recursos necesarios para una descubrimiento científico. Un científico virtual de IA puede evolucionar hacia un científico robotico basado en IA general, y el AGS ofrece una posibilidad global. Se espera que estos sistemas automáticos se integren más estrictamente en los procesos de investigación, y se asume que las descubrimientos científicos pueden llegar a un nuevo tamaño, considerando la creación y evolución de conocimientos desde una nueva perspectiva. La adaptación a los entornos extremos de los robots abstractos y el efecto de nivel de libertad debido al aumento activo del conocimiento científico tienen como objetivo superar continuamente las fronteras físicas e intelectuales.",
      "upvotes": 4,
      "discussionId": "67ef33a5456bcf30fa95b35e",
      "githubRepo": "https://github.com/openags/Awesome-AI-Scientist-Papers"
    },
    "publishedAt": "2025-03-28T10:00:27.000Z",
    "title": "Scaling Laws in Scientific Discovery with AI and Robot Scientists",
    "summary": "Scientific discovery is poised for rapid advancement through advanced\nrobotics and artificial intelligence. Current scientific practices face\nsubstantial limitations as manual experimentation remains time-consuming and\nresource-intensive, while multidisciplinary research demands knowledge\nintegration beyond individual researchers' expertise boundaries. Here, we\nenvision an autonomous generalist scientist (AGS) concept combines agentic AI\nand embodied robotics to automate the entire research lifecycle. This system\ncould dynamically interact with both physical and virtual environments while\nfacilitating the integration of knowledge across diverse scientific\ndisciplines. By deploying these technologies throughout every research stage --\nspanning literature review, hypothesis generation, experimentation, and\nmanuscript writing -- and incorporating internal reflection alongside external\nfeedback, this system aims to significantly reduce the time and resources\nneeded for scientific discovery. Building on the evolution from virtual AI\nscientists to versatile generalist AI-based robot scientists, AGS promises\ngroundbreaking potential. As these autonomous systems become increasingly\nintegrated into the research process, we hypothesize that scientific discovery\nmight adhere to new scaling laws, potentially shaped by the number and\ncapabilities of these autonomous systems, offering novel perspectives on how\nknowledge is generated and evolves. The adaptability of embodied robots to\nextreme environments, paired with the flywheel effect of accumulating\nscientific knowledge, holds the promise of continually pushing beyond both\nphysical and intellectual frontiers.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655fb8a122ce47e5fa491c72/d-38XQKRw3vnyFhzZFTEd.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22444.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655fb8a122ce47e5fa491c72",
      "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
      "fullname": "Pengsong Zhang",
      "name": "universea",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.01871",
      "authors": [
        {
          "_id": "67eea9e5117231f8bb04402b",
          "user": {
            "_id": "65d0c00b0954f06e472909f4",
            "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
            "isPro": false,
            "fullname": "tom bush",
            "user": "tuphs",
            "type": "user"
          },
          "name": "Thomas Bush",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T19:20:34.085Z",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402c",
          "name": "Stephen Chung",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402d",
          "name": "Usman Anwar",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402e",
          "user": {
            "_id": "645ecd18f0f92653b9f33d4e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645ecd18f0f92653b9f33d4e/nHDMWtM9ZHrji0c4Y4XW1.jpeg",
            "isPro": false,
            "fullname": "Adrià Garriga-Alonso",
            "user": "agaralon",
            "type": "user"
          },
          "name": "Adrià Garriga-Alonso",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-03T15:31:53.577Z",
          "hidden": false
        },
        {
          "_id": "67eea9e5117231f8bb04402f",
          "name": "David Krueger",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T16:24:23.000Z",
      "submittedOnDailyAt": "2025-04-04T07:00:29.802Z",
      "title": "Planificación retrospectiva en aprendizaje por refuerzo sin modelo",
      "submittedOnDailyBy": {
        "_id": "65d0c00b0954f06e472909f4",
        "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
        "isPro": false,
        "fullname": "tom bush",
        "user": "tuphs",
        "type": "user"
      },
      "summary": "Aquí se proporciona un modelo de aprendizaje por refuerzo sin modelo que realiza el aprendizaje de planes basado en evidencias mecánicas. Este método se ha aplicado en el juego de computadora \"SoCoBaN\" para impulsar la explicabilidad basada en conceptos en agentes sin modelo. En particular, el DRC (Guez et al. (2019)), un agente sin modelo del género introducido, utiliza representaciones de conceptos aprendidas para la configuración de planes internos, predeciendo los efectos a largo plazo del entorno y influyendo en la elección de acciones. Nuestro método se compone de tres etapas: (1) detección de conceptos relacionados con los planes, (2) investigación de la formación de planes dentro de la representación del agente, y (3) confirmación de que los planes encontrados (dentro de la representación del agente) tienen un impacto causal en las acciones del agente. Además, la descubrimiento de estos planes permite identificar capacidades como el rendimiento planificado y obtener ventajas en términos de computación adicional durante el test. Finalmente, se realiza un análisis cualitativo sobre el algoritmo de planes aprendidos por el agente, y se encuentra una fuerte similitud con exploraciones bidireccionales paralelas. Estos hallazgos permiten entender mejor la estructura interna de los comportamientos planificados del agente y son cruciales para comprender fenómenos como la capacidad de planificación y inferencia en el aprendizaje por refuerzo de los LLM recientes.",
      "upvotes": 3,
      "discussionId": "67eea9e9117231f8bb044167",
      "ai_keywords": [
        "model-free reinforcement learning",
        "concept-based interpretability",
        "Sokoban",
        "DRC",
        "learned concept representations",
        "plan formation",
        "causal effect",
        "parallelized bidirectional search",
        "LLMs",
        "emergent planning",
        "reasoning capabilities"
      ]
    },
    "publishedAt": "2025-04-02T12:24:23.000Z",
    "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
    "summary": "We present the first mechanistic evidence that model-free reinforcement\nlearning agents can learn to plan. This is achieved by applying a methodology\nbased on concept-based interpretability to a model-free agent in Sokoban -- a\ncommonly used benchmark for studying planning. Specifically, we demonstrate\nthat DRC, a generic model-free agent introduced by Guez et al. (2019), uses\nlearned concept representations to internally formulate plans that both predict\nthe long-term effects of actions on the environment and influence action\nselection. Our methodology involves: (1) probing for planning-relevant\nconcepts, (2) investigating plan formation within the agent's representations,\nand (3) verifying that discovered plans (in the agent's representations) have a\ncausal effect on the agent's behavior through interventions. We also show that\nthe emergence of these plans coincides with the emergence of a planning-like\nproperty: the ability to benefit from additional test-time compute. Finally, we\nperform a qualitative analysis of the planning algorithm learned by the agent\nand discover a strong resemblance to parallelized bidirectional search. Our\nfindings advance understanding of the internal mechanisms underlying planning\nbehavior in agents, which is important given the recent trend of emergent\nplanning and reasoning capabilities in LLMs through RL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01871.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d0c00b0954f06e472909f4",
      "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
      "fullname": "tom bush",
      "name": "tuphs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02821",
      "authors": [
        {
          "_id": "67ef9a8885ea9d1d7db3fdb5",
          "name": "Mateusz Pach",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb6",
          "name": "Shyamgopal Karthik",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb7",
          "name": "Quentin Bouniot",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb8",
          "name": "Serge Belongie",
          "hidden": false
        },
        {
          "_id": "67ef9a8885ea9d1d7db3fdb9",
          "name": "Zeynep Akata",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:58:35.000Z",
      "submittedOnDailyAt": "2025-04-04T07:08:47.568Z",
      "title": "Spas Encoder aprende características de un solo significado en modelos de lenguaje visual.",
      "submittedOnDailyBy": {
        "_id": "6254599b6e36fe62e141c8f9",
        "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
        "isPro": false,
        "fullname": "Shyamgopal Karthik",
        "user": "shyamgopal",
        "type": "user"
      },
      "summary": "Los Autoencoders Esparsos (AEs) han demostrado ser útiles para mejorar la interpretabilidad y control de los Modelos de Lenguaje Grandes (LLMs) recientemente. En este artículo, se extiende el ámbito de aplicación de los AEs a los Modelos de Visión-Lenguaje (VLMs) y se propone un marco detallado adecuado para evaluar la unicidad de la representación visual. Los resultados de los experimentos muestran que los AEs entrenados en VLMs mejoran significativamente la unicidad de la representación de cada neurona y producen una representación estratificada que coincide con la definida por expertos (por ejemplo, la clasificación de iNaturalist). En particular, se muestra que aplicando AEs al encoder visual de CLIP es posible manipular directamente los resultados de modelos multifásicos como LLaVA. Estos hallazgos subrayan la utilidad y eficacia de los AEs como una forma práctica y efectiva de mejorar la interpretabilidad y control de los VLMs.",
      "upvotes": 2,
      "discussionId": "67ef9a8985ea9d1d7db3fe20",
      "ai_keywords": [
        "Sparse Autoencoders (SAEs)",
        "Large Language Models (LLMs)",
        "Vision-Language Models (VLMs)",
        "CLIP",
        "monosemanticity",
        "vision representations",
        "hierarchical representations",
        "iNaturalist taxonomy",
        "multimodal LLMs",
        "LLaVA",
        "unsupervised approach"
      ]
    },
    "publishedAt": "2025-04-03T13:58:35.000Z",
    "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models",
    "summary": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6254599b6e36fe62e141c8f9",
      "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
      "fullname": "Shyamgopal Karthik",
      "name": "shyamgopal",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]