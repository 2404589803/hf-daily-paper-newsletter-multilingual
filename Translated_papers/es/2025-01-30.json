[
  {
    "paper": {
      "id": "2501.17703",
      "authors": [
        {
          "_id": "679ae76cf211c66bd702f5d5",
          "user": {
            "_id": "636a35eff8d9af4aea181608",
            "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
            "isPro": false,
            "fullname": "yubo",
            "user": "ubowang",
            "type": "user"
          },
          "name": "Yubo Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-30T08:39:49.375Z",
          "hidden": false
        },
        {
          "_id": "679ae76cf211c66bd702f5d6",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "679ae76cf211c66bd702f5d7",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-30T02:43:59.302Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-29T15:20:30.000Z",
      "title": "Criticism Training: El entrenamiento crítico es más efectivo que el aprendizaje por imitación.",
      "summary": "Supervised Fine-Tuning (SFT) es un método general para entrenar modelos de lenguaje que imitan respuestas según señales dadas. En este artículo, desafíamos este patrón y proponemos Critique Fine-Tuning (CFT). CFT es una estrategia en la que el modelo aprende a criticar respuestas con ruido, reforzando el pensamiento crítico humano en el modelado. Esta metodología fomenta características como el análisis profundo y la comprensión compleja, que son difíciles de lograr en el SFT estándar. Para evaluar el efecto de CFT, construimos un conjunto de datos de 50K muestras en WebInstruct y generamos críticas usando GPT-4o como modelo de enseñanza (input=[pregunta; respuesta con ruido], output=crítica). El entrenamiento con CFT en este conjunto de datos mejoró en un 4-10% en 6 benchmarks de matemáticas frente al SFT, utilizando modelos como Qwen2.5, Qwen2.5-Math y DeepSeek-Math. Además, extendimos los conjuntos de datos MetaMath y NuminaMath para obtener el mismo efecto que el SFT. En particular, el modelo Qwen2.5-Math-CFT mostró mejores resultados con 2M muestras, superando o igualando en muchos benchmarks. Los estudios sobre el desarrollo de la lógica en modelos de lenguaje sugieren que el entrenamiento crítico es una alternativa más efectiva, basada en el pensamiento crítico humano.",
      "upvotes": 12,
      "discussionId": "679ae770f211c66bd702f697"
    },
    "publishedAt": "2025-01-29T21:51:11.227Z",
    "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636a35eff8d9af4aea181608",
      "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
      "fullname": "yubo",
      "name": "ubowang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.14334",
      "authors": [
        {
          "_id": "679a7546805383520ce065af",
          "user": {
            "_id": "644156da1a80f6d83cb1667c",
            "avatarUrl": "/avatars/106d30a576b0fb58118ac4333b17260b.svg",
            "isPro": false,
            "fullname": "Clement Desroches",
            "user": "clementdesroches",
            "type": "user"
          },
          "name": "Clément Desroches",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-29T21:06:17.418Z",
          "hidden": false
        },
        {
          "_id": "679a7546805383520ce065b0",
          "user": {
            "_id": "66221f6295e8f09a668f07f0",
            "avatarUrl": "/avatars/f7c943996c814630ab5dcfaaaba01a83.svg",
            "isPro": false,
            "fullname": "Martin Chauvin",
            "user": "Neyri56",
            "type": "user"
          },
          "name": "Martin Chauvin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-30T09:38:17.235Z",
          "hidden": false
        },
        {
          "_id": "679a7546805383520ce065b1",
          "name": "Louis Ladan",
          "hidden": false
        },
        {
          "_id": "679a7546805383520ce065b2",
          "name": "Caroline Vateau",
          "hidden": false
        },
        {
          "_id": "679a7546805383520ce065b3",
          "name": "Simon Gosset",
          "hidden": false
        },
        {
          "_id": "679a7546805383520ce065b4",
          "name": "Philippe Cordier",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-24T08:58:49.000Z",
      "title": "Exploración del escalado sostenible de la IA: estudio de la predicción del impacto en el entorno de la empresa de la IA",
      "summary": "El rápido desarrollo de la inteligencia artificial (IA), especialmente el de los grandes modelos de lenguaje (LLMs), ha aumentado las preocupaciones por los efectos ambientales, afectando no solo la emisión de gases pero también la fabricación y el ciclo de vida hasta su finalización. La opacidad de los principales proveedores dificulta la evaluación de los impactos ambientales relacionados con la IA y la alcanzamiento de los objetivos de cero emisiones.\n\nEn este artículo, se propone un método para evaluar el impacto ambiental de la cartera de IA de una empresa, y se ofrecen perspectivas que permiten realizar la evaluación de AI y el ciclo de vida (LCA) sin necesidad de conocer conocimientos específicos en estos campos. Los resultados muestran que los modelos de IA de generación grandes consumen hasta 4.600 veces más energía que los modelos tradicionales. Nuestro enfoque de modelado predice la cantidad de electricidad consumida por la IA hasta el año 2030, considerando la variación del mix de electricidad según los escenarios de IPCC, el aumento en la utilización de IA, la eficiencia de cálculo y el impacto ambiental de las emisiones de gases. En el escenario de alta introducción, se espera que la utilización de electricidad por la IA aumente en un 24.4 veces debido a la introducción amplia de IA generativa y de agentes inteligentes, lo que lleva a un aumento significativo en el uso de modelos y frameworks complejos.\n\nPara mitigar el impacto ambiental de la IA hasta el año 2030, es necesario una colaboración global en toda la cadena de valor de la IA. Las medidas independientes para mejorar la eficiencia de cálculo, la eficiencia del modelo y la reducción de las emisiones de gases no son suficientes. Proponemos la implementación de un marco de referencia estándarizado para la evaluación ambiental, mayor transparencia de todos los propietarios de la cadena de valor, y la introducción de métricas de \"compensación ambiental\", así como la promoción de que el desarrollo de la IA se alinee con los objetivos de cero emisiones.",
      "upvotes": 11,
      "discussionId": "679a7548805383520ce065f5"
    },
    "publishedAt": "2025-01-30T03:05:08.789Z",
    "title": "Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.14334.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "644156da1a80f6d83cb1667c",
      "avatarUrl": "/avatars/106d30a576b0fb58118ac4333b17260b.svg",
      "fullname": "Clement Desroches",
      "name": "clementdesroches",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.17749",
      "authors": [
        {
          "_id": "679ae5eab898ac90bf4480b6",
          "user": {
            "_id": "657b3a44de028a439ea2ed9d",
            "avatarUrl": "/avatars/9f05e8eb6809a0ce1b50cd1fc9b5a044.svg",
            "isPro": false,
            "fullname": "Aitor Arrieta",
            "user": "aitorarrieta",
            "type": "user"
          },
          "name": "Aitor Arrieta",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-01-30T08:45:20.561Z",
          "hidden": false
        },
        {
          "_id": "679ae5eab898ac90bf4480b7",
          "name": "Miriam Ugarte",
          "hidden": false
        },
        {
          "_id": "679ae5eab898ac90bf4480b8",
          "user": {
            "_id": "65001514f322f9156663f096",
            "avatarUrl": "/avatars/e8712f60d4e8b7c70ac02c532ad547ef.svg",
            "isPro": false,
            "fullname": "Pablo Valle",
            "user": "pablovalle",
            "type": "user"
          },
          "name": "Pablo Valle",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-30T09:39:30.629Z",
          "hidden": false
        },
        {
          "_id": "679ae5eab898ac90bf4480b9",
          "user": {
            "_id": "63527de67e4cc3135fd16651",
            "avatarUrl": "/avatars/5eb8076d448d0b6746e256c24e1440e0.svg",
            "isPro": false,
            "fullname": "José Antonio Parejo Maestre",
            "user": "japarejo",
            "type": "user"
          },
          "name": "José Antonio Parejo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-30T09:39:06.958Z",
          "hidden": false
        },
        {
          "_id": "679ae5eab898ac90bf4480ba",
          "user": {
            "_id": "6790d642a1863df579840ae3",
            "avatarUrl": "/avatars/a10a6f4af327c1bb67513c56d7f84820.svg",
            "isPro": false,
            "fullname": "Sergio Segura",
            "user": "ssegura",
            "type": "user"
          },
          "name": "Sergio Segura",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-30T02:37:35.516Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-29T16:36:53.000Z",
      "title": "OpenAI's o3-mini's initial external safety verification: feedback from pre-deployment evaluation",
      "summary": "Los modelos de lenguaje grande (LLMs) forman parte importante de nuestra vida diaria. Sin embargo, estos modelos pueden dañar la privacidad de las personas, mantener prejuicios y propagar información inacurrada. Estos riesgos subrayan la necesidad de estructuras de seguridad fuertes, pautas éticas y una amplia gama de pruebas para garantizar funciones responsables. La seguridad de los LLMs es crucial, ya que debe ser evaluada en detalle antes de que estos modelos sean accesibles a usuarios generales. En este artículo, se presenta el experiencia de los investigadores de la universidad de Stanford y la universidad de Cambridge en la prueba externa de seguridad de un nuevo LLM de OpenAI, llamado o3-mini. En particular, se utiliza nuestro herramienta ASTRAL para generar automáticamente y sistemáticamente entradas de prueba no estables (por ejemplo, prompts) y para evaluar y clasificar la seguridad de los LLMs de manera diversa. Se generaron y ejecutaron automaticamente las pruebas para la versión beta de o3-mini. Se verificaron directamente las pruebas no estables clasificadas por ASTRAL y se encontraron 87 instancias de comportamiento no estable de LLMs. Se mencionan especialmente las importantes descubrimientos y observaciones encontradas antes de la implementación de estas nuevas funciones del LLM.",
      "upvotes": 7,
      "discussionId": "679ae5f0b898ac90bf44826c"
    },
    "publishedAt": "2025-01-29T21:38:42.464Z",
    "title": "Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17749.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5860
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.17433",
      "authors": [
        {
          "_id": "679b1319f87b99a2a7c41e36",
          "user": {
            "_id": "67325283b318faa97f7ae5f7",
            "avatarUrl": "/avatars/2f83452768148b323c540c43ad695ee6.svg",
            "isPro": false,
            "fullname": "TianshengHuang",
            "user": "TianshengHuang",
            "type": "user"
          },
          "name": "Tiansheng Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-30T08:39:47.548Z",
          "hidden": false
        },
        {
          "_id": "679b1319f87b99a2a7c41e37",
          "user": {
            "_id": "6539cab119c3ef6679794706",
            "avatarUrl": "/avatars/a88691ff5a547c7a1384edcc615c8209.svg",
            "isPro": false,
            "fullname": "Sihao Hu",
            "user": "SihaoHu",
            "type": "user"
          },
          "name": "Sihao Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-30T09:39:58.723Z",
          "hidden": false
        },
        {
          "_id": "679b1319f87b99a2a7c41e38",
          "user": {
            "_id": "647615b995a4dc98e58c24f2",
            "avatarUrl": "/avatars/7f73999246526c1aef4d019d5f5595ad.svg",
            "isPro": false,
            "fullname": "Fatih Ilhan",
            "user": "tawreos",
            "type": "user"
          },
          "name": "Fatih Ilhan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-30T09:40:06.004Z",
          "hidden": false
        },
        {
          "_id": "679b1319f87b99a2a7c41e39",
          "user": {
            "_id": "65aae89948c718a57434db6f",
            "avatarUrl": "/avatars/6c0fae8dafad9b9265098a9bc3bfc102.svg",
            "isPro": false,
            "fullname": "selim tekin",
            "user": "sftekin25",
            "type": "user"
          },
          "name": "Selim Furkan Tekin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-30T09:40:16.339Z",
          "hidden": false
        },
        {
          "_id": "679b1319f87b99a2a7c41e3a",
          "user": {
            "_id": "65c998005e17dbeaf147db84",
            "avatarUrl": "/avatars/6fb47b1e095971b93ff7dcd10369f926.svg",
            "isPro": false,
            "fullname": "Ling Liu",
            "user": "ling1119",
            "type": "user"
          },
          "name": "Ling Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-30T09:40:37.075Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-29T06:24:58.000Z",
      "title": "Virus: Ataques de micro-ajustes harminos a través de modelos de lenguaje de gran escala",
      "summary": "Según los últimos estudios, los modelos de lenguaje de gran escala (LLMs) son vulnerables a ataques pequeños y perjudiciales, y al ajustar con muestras peligrosas, la capacidad de ajuste de seguridad puede disminuir. Para reducir el riesgo, generalmente se utiliza la técnica de filtrar muestras peligrosas antes del ajuste mediante líneas de protección. En este artículo, se diseña una nueva misión de equipo de redada y se sostiene que no se puede confiar en que se pueda filtrar datos mediante modelado de líneas de protección. El método de ataque propuesto en este artículo, llamado Virus, permite evitar el modelado de líneas de protección al hacer ligeramente cambios en los datos peligrosos. Los resultados de los experimentos muestran que los datos peligrosos optimizados por Virus presentan un 100% de fallo de detección en las líneas de protección y al mismo tiempo logran un buen rendimiento de ataque. Finalmente, la principal mensaje transmitido en este artículo es que considerar el modelado de líneas de protección para resolver ataques de ajuste pequeño y peligroso es peligroso. El modelado de líneas de protección no puede resolver los problemas de seguridad propios de los LLMs pre-entrenados. El código está disponible en GitHub: https://github.com/git-disl/Virus.",
      "upvotes": 2,
      "discussionId": "679b131bf87b99a2a7c41ede"
    },
    "publishedAt": "2025-01-30T01:30:18.013Z",
    "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67325283b318faa97f7ae5f7/1hJo5gEfGEXAwYB5a6yWY.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67325283b318faa97f7ae5f7/8SaMXA1izw5vcfwtU2Nhj.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17433.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67325283b318faa97f7ae5f7",
      "avatarUrl": "/avatars/2f83452768148b323c540c43ad695ee6.svg",
      "fullname": "TianshengHuang",
      "name": "TianshengHuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.17195",
      "authors": [
        {
          "_id": "679ae7655c55250b48483742",
          "name": "Andrei Alexandru",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b48483743",
          "user": {
            "_id": "66e184e86048d62cd8fb4e52",
            "avatarUrl": "/avatars/dc459c692fe9fce0911fa1229df0aeee.svg",
            "isPro": false,
            "fullname": "Antonia Calvi",
            "user": "NinaCalvi",
            "type": "user"
          },
          "name": "Antonia Calvi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-30T09:40:54.827Z",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b48483744",
          "name": "Henry Broomfield",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b48483745",
          "name": "Jackson Golden",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b48483746",
          "name": "Kyle Dai",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b48483747",
          "name": "Mathias Leys",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b48483748",
          "name": "Maurice Burger",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b48483749",
          "name": "Max Bartolo",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b4848374a",
          "name": "Roman Engeler",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b4848374b",
          "name": "Sashank Pisupati",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b4848374c",
          "name": "Toby Drane",
          "hidden": false
        },
        {
          "_id": "679ae7655c55250b4848374d",
          "name": "Young Sun Park",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T15:09:08.000Z",
      "title": "Atla Selene Mini: Modelo de Evaluación General",
      "summary": "Atla・Selene・Mini, SLMJ (Small Language Model as a Judge) a través del cual se proporciona la evaluación de los mejores pequeños modelos de lenguaje. Selene・Mini es un evaluador adecuado para su uso general, superando en el rendimiento general a SLMJ y GPT-4o-mini en 11 marcos de referencia. Esto incluye puntuaciones absolutas, clasificación de clases y tareas de preferencia de parejas. Ha registrado la puntuación más alta en RewardBench para generadores de 8B, y supera a GPT-4o y a evaluadores especializados en su fuerte criterio. Para lograr esto, se ha desarrollado una estrategia de carga de datos que genera datasetes sintéticos a partir de datos públicos, asegurando alta calidad a través de filtrados y eliminaciones. El modelo se entrenó combinando la función de pérdida de optimización de preferencias directas (DPO) y la aprendizaje de fines de la filosofía (SFT). Selene・Mini mejora significativamente la acuerdo con expertos en conjuntos de datos de financiamiento y salud, y es resistente a cambios en el formato de los prompts. A partir de los resultados iniciales, Selene・Mini ha sido evaluada como el mejor evaluador en el Arena de Jurados, liderando la comunidad y la vida pública. El peso del modelo está disponible en HuggingFace (https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) y Ollama, impulsando la introducción de una comunidad amplia.",
      "upvotes": 2,
      "discussionId": "679ae76b5c55250b484838e0"
    },
    "publishedAt": "2025-01-29T21:44:37.041Z",
    "title": "Atla Selene Mini: A General Purpose Evaluation Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17195.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5860
    },
    "isAuthorParticipating": false
  }
]