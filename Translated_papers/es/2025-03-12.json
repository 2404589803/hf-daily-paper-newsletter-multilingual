[
  {
    "paper": {
      "id": "2503.07920",
      "authors": [
        {
          "_id": "67d0f9c95f0fcc0c38902b8e",
          "name": "Samuel Cahyawijaya",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b8f",
          "name": "Holy Lovenia",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b90",
          "name": "Joel Ruben Antony Moniz",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b91",
          "name": "Tack Hwa Wong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b92",
          "name": "Mohammad Rifqi Farhansyah",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b93",
          "name": "Thant Thiri Maung",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b94",
          "name": "Frederikus Hudi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b95",
          "name": "David Anugraha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b96",
          "user": {
            "_id": "63ddfced5ea8577c8d5fb421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677144169806-63ddfced5ea8577c8d5fb421.jpeg",
            "isPro": false,
            "fullname": "Muhammad Ravi Shulthan Habibi",
            "user": "muhammadravi251001",
            "type": "user"
          },
          "name": "Muhammad Ravi Shulthan Habibi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:20.672Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b97",
          "name": "Muhammad Reza Qorib",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b98",
          "name": "Amit Agarwal",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b99",
          "name": "Joseph Marvin Imperial",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9a",
          "name": "Hitesh Laxmichand Patel",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9b",
          "user": {
            "_id": "67d1039a3e0dca11407f9460",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tN5K_Gc8oAlw0ADYuyc1s.png",
            "isPro": false,
            "fullname": "Vicky Feliren",
            "user": "feliren",
            "type": "user"
          },
          "name": "Vicky Feliren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:30.804Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9c",
          "name": "Bahrul Ilmi Nasution",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9d",
          "user": {
            "_id": "67559e52860bd4d8f4e9beeb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/I_SNcfwTifgHtL9NFLLli.jpeg",
            "isPro": false,
            "fullname": "Manuel Antonio Rufino",
            "user": "antonrufino",
            "type": "user"
          },
          "name": "Manuel Antonio Rufino",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:33.476Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9e",
          "name": "Genta Indra Winata",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9f",
          "name": "Rian Adam Rajagede",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba0",
          "name": "Carlos Rafael Catalan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba1",
          "name": "Mohamed Fazli Imam",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba2",
          "name": "Priyaranjan Pattnayak",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba3",
          "name": "Salsabila Zahirah Pranida",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba4",
          "name": "Kevin Pratama",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba5",
          "name": "Yeshil Bangera",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba6",
          "user": {
            "_id": "66d03a984505b8d635183aaa",
            "avatarUrl": "/avatars/0eab10dfad243d9dc19318b0f88de496.svg",
            "isPro": false,
            "fullname": "Adisai Na-Thalang",
            "user": "ensmart72",
            "type": "user"
          },
          "name": "Adisai Na-Thalang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:50:17.122Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba7",
          "name": "Patricia Nicole Monderin",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba8",
          "name": "Yueqi Song",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba9",
          "name": "Christian Simon",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902baa",
          "name": "Lynnette Hui Xian Ng",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bab",
          "name": "Richardy Lobo' Sapan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bac",
          "name": "Taki Hasan Rafi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bad",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bae",
          "name": "Supryadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902baf",
          "name": "Kanyakorn Veerakanjana",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb0",
          "name": "Piyalitt Ittichaiwong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb1",
          "name": "Matthew Theodore Roque",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb2",
          "name": "Karissa Vincentio",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb3",
          "name": "Takdanai Kreangphet",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb4",
          "user": {
            "_id": "631a4855300a072a8da70abd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631a4855300a072a8da70abd/jRnzdW5JBjICYKCmkUFI-.jpeg",
            "isPro": false,
            "fullname": "phakphum artkaew",
            "user": "pakphum",
            "type": "user"
          },
          "name": "Phakphum Artkaew",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:41.811Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb5",
          "name": "Kadek Hendrawan Palgunadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb6",
          "name": "Yanzhi Yu",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb7",
          "name": "Rochana Prih Hastuti",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb8",
          "name": "William Nixon",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb9",
          "name": "Mithil Bangera",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bba",
          "name": "Adrian Xuan Wei Lim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbb",
          "user": {
            "_id": "64f2e3b87244601d8f4365cf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f2e3b87244601d8f4365cf/QHKB8DOMBoKSXgMo6nY6z.jpeg",
            "isPro": false,
            "fullname": "Aye Hninn Khine",
            "user": "ayehninnkhine",
            "type": "user"
          },
          "name": "Aye Hninn Khine",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:27.900Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbc",
          "name": "Hanif Muhammad Zhafran",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbd",
          "name": "Teddy Ferdinan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbe",
          "name": "Audra Aurora Izzani",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbf",
          "name": "Ayushman Singh",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc0",
          "name": "Evan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc1",
          "name": "Jauza Akbar Krito",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc2",
          "name": "Michael Anugraha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc3",
          "name": "Fenal Ashokbhai Ilasariya",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc4",
          "name": "Haochen Li",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc5",
          "name": "John Amadeo Daniswara",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc6",
          "name": "Filbert Aurelian Tjiaranata",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc7",
          "name": "Eryawan Presma Yulianrifat",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc8",
          "name": "Can Udomcharoenchaikit",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc9",
          "name": "Fadil Risdian Ansori",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bca",
          "name": "Mahardika Krisna Ihsani",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcb",
          "name": "Giang Nguyen",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcc",
          "name": "Anab Maulana Barik",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcd",
          "name": "Dan John Velasco",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bce",
          "name": "Rifo Ahmad Genadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcf",
          "name": "Saptarshi Saha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd0",
          "user": {
            "_id": "66a31819b839c8994e5c3815",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a31819b839c8994e5c3815/ARVZtfxJYyGvZ0zHyaaBP.png",
            "isPro": false,
            "fullname": "Chengwei Wei",
            "user": "amao0o0",
            "type": "user"
          },
          "name": "Chengwei Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:39.101Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd1",
          "name": "Isaiah Flores",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd2",
          "name": "Kenneth Ko Han Chen",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd3",
          "name": "Anjela Gail Santos",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd4",
          "name": "Wan Shen Lim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd5",
          "name": "Kaung Si Phyo",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd6",
          "name": "Tim Santos",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd7",
          "name": "Meisyarah Dwiastuti",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd8",
          "name": "Jiayun Luo",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd9",
          "name": "Jan Christian Blaise Cruz",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bda",
          "name": "Ming Shan Hee",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdb",
          "name": "Ikhlasul Akmal Hanif",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdc",
          "name": "M. Alif Al Hakim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdd",
          "name": "Muhammad Rizky Sya'ban",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bde",
          "name": "Kun Kerdthaisong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdf",
          "name": "Lester James V. Miranda",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be0",
          "name": "Fajri Koto",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be1",
          "name": "Tirana Noor Fatyanosa",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be2",
          "name": "Alham Fikri Aji",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be3",
          "name": "Jostin Jerico Rosal",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be4",
          "name": "Jun Kevin",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be5",
          "user": {
            "_id": "640ead243830fd441c2e9838",
            "avatarUrl": "/avatars/4083942ce6b432a4cfb3524f72bcffb0.svg",
            "isPro": false,
            "fullname": "Robert Wijaya",
            "user": "wijayarobert",
            "type": "user"
          },
          "name": "Robert Wijaya",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:17.433Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be6",
          "name": "Onno P. Kampman",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be7",
          "name": "Ruochen Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be8",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:36.440Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be9",
          "name": "Peerat Limkonchotiwat",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T23:54:52.000Z",
      "title": "Cloud source, Croil, or generative? SEA-VL, un conjunto de datos de lenguaje de visión para múltiples características de la región sudeste asiática.",
      "summary": "Sure, here is the translation of the provided text into Spanish:\n\n\"Southeast Asia (SEA) es caracterizado por la diversidad de lenguas y culturas, pero no es un lugar representativo para la investigación de lenguajes visuales (VL). Esto es debido a que los modelos de IA no pueden comprender los nuances subtilísimos de la cultura SEA, lo que lleva a fallos. Para remediar esto, se propone el proyecto abierto-source SEA-VL. Este proyecto tiene como objetivo el desarrollo de alta calidad de datos relacionados con los idiomas de SEA. SEA-VL garantiza la contribución de los participantes de los países de SEA, asegurando la relevancia cultural y la diversidad, y promueve la ampliación de lenguas representativas en la investigación de VL. Es más costo-eficiente que soluciones de nube, alcanzando aproximadamente el 85% de la relevancia cultural, pero también se destaca que los modelos generativos de imágenes no pueden reflejar precisamente la cultura SEA. Las imágenes generadas no pueden reflejar el contexto de las tradiciones y culturas locales. En SEA-VL, se tiene como objetivo recopilar 1.28 millones de imágenes relacionadas con la cultura SEA, con un tamaño que es más de cinco veces mayor que otros conjuntos de datos. SEA-VL busca llenar las deficiencias representativas de SEA y fomentar el desarrollo de un sistema AI que represente de manera más amplia y realidad diversas culturas.\"",
      "upvotes": 44,
      "discussionId": "67d0f9cd5f0fcc0c38902cdf",
      "ai_keywords": [
        "vision-language (VL) research",
        "cultural relevance",
        "crowdsourcing",
        "image crawling",
        "image generation",
        "generative vision models",
        "synthesized images",
        "datasets"
      ]
    },
    "publishedAt": "2025-03-10T19:54:52.000Z",
    "title": "Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia",
    "summary": "Southeast Asia (SEA) is a region of extraordinary linguistic and cultural\ndiversity, yet it remains significantly underrepresented in vision-language\n(VL) research. This often results in artificial intelligence (AI) models that\nfail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an\nopen-source initiative dedicated to developing high-quality, culturally\nrelevant data for SEA languages. By involving contributors from SEA countries,\nSEA-VL aims to ensure better cultural relevance and diversity, fostering\ngreater inclusivity of underrepresented languages in VL research. Beyond\ncrowdsourcing, our initiative goes one step further in the exploration of the\nautomatic collection of culturally relevant images through crawling and image\ngeneration. First, we find that image crawling achieves approximately ~85%\ncultural relevance while being more cost- and time-efficient than\ncrowdsourcing. Second, despite the substantial progress in generative vision\nmodels, synthetic images remain unreliable in accurately reflecting SEA\ncultures. The generated images often fail to reflect the nuanced traditions and\ncultural contexts of the region. Collectively, we gather 1.28M SEA\nculturally-relevant images, more than 50 times larger than other existing\ndatasets. Through SEA-VL, we aim to bridge the representation gap in SEA,\nfostering the development of more inclusive AI systems that authentically\nrepresent diverse cultures across SEA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07920.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07536",
      "authors": [
        {
          "_id": "67d04f248f79213c2fc0ba04",
          "name": "Yingzhe Peng",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba05",
          "name": "Gongrui Zhang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba06",
          "name": "Miaosen Zhang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba07",
          "name": "Zhiyuan You",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba08",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba09",
          "name": "Qipeng Zhu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0a",
          "name": "Kai Yang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0b",
          "name": "Xingzhong Xu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0c",
          "name": "Xin Geng",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0d",
          "name": "Xu Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:04:14.000Z",
      "title": "LMM-R1: Se fortalece un modelo de 30 millones de palabras con fuertes capacidades de lógica basado en un aprendizaje reforzado de dos etapas.",
      "summary": "Para fortalecer la teoría de los modelos de memoria multimodal (LMMs), es necesario resolver los problemas inherentes a la compleja interacción entre el reconocimiento visual y la teoría racional, especialmente en arquitecturas con 300 millones de parámetros que limitan la capacidad teórica del modelo.\n\nEl aprendizaje por refuerzo basado en reglas (RL) muestra excelentes resultados en dominios contextuales, pero presenta dos grandes obstáculos al ser aplicado a modelos de memoria multimodal: 1. La limitación de datos que impide obtener respuestas inciertas o ejemplos complejos de teoría racional. 2. La degradación de la teoría racional debido al aprendizaje previo del modelo.\n\nPara resolver estos problemas, proponemos un marco de trabajo de dos etapas llamado \\method. Este marco mejora la teoría racional mediante el aprendizaje por refuerzo (FRE) y generaliza el aprendizaje de memoria multimodal (MGT). En la etapa de FRE, se mejora la capacidad teórica usando datos contextuales, y en la etapa de MGT, se generaliza esta capacidad a dominios de memoria multimodal.\n\nLos experimentos en Qwen2.5-VL-Instruct-3B demostraron que \\method logró un aumento promedio del 4.83% en los benchmarks de memoria multimodal y del 4.5% en los benchmarks cerebrales, y un efecto del 3.63% en tareas de juego de fútbol complejos. Estos resultados prueban que la mejora de la teoría racional basada en la mente es efectiva para la generalización de la memoria multimodal y ofrece un enfoque de datos eficientes que evita el alto costo de obtener datos de alta calidad.",
      "upvotes": 41,
      "discussionId": "67d04f268f79213c2fc0ba8b",
      "projectPage": "https://forjadeforest.github.io/LMM-R1-ProjectPage",
      "githubRepo": "https://github.com/TideDra/lmm-r1",
      "ai_keywords": [
        "Large Multimodal Models (LMMs)",
        "visual perception",
        "logical reasoning",
        "3B-parameter architectures",
        "rule-based reinforcement learning (RL)",
        "multimodal extension",
        "ambiguous answers",
        "complex reasoning examples",
        "degraded foundational reasoning",
        "multimodal pretraining",
        "Foundational Reasoning Enhancement (FRE)",
        "Multimodal Generalization Training (MGT)",
        "Qwen2.5-VL-Instruct-3B",
        "multimodal benchmarks",
        "text-only benchmarks",
        "complex Football Game tasks",
        "text-based reasoning enhancement",
        "data-efficient paradigm"
      ]
    },
    "publishedAt": "2025-03-10T13:04:14.000Z",
    "title": "LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through\n  Two-Stage Rule-Based RL",
    "summary": "Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges\nfrom the complex interplay between visual perception and logical reasoning,\nparticularly in compact 3B-parameter architectures where architectural\nconstraints limit reasoning capacity and modality alignment.\n  While rule-based reinforcement learning (RL) excels in text-only domains, its\nmultimodal extension confronts two critical barriers: (1) data limitations due\nto ambiguous answers and scarce complex reasoning examples, and (2) degraded\nfoundational reasoning induced by multimodal pretraining.\n  To address these challenges, we propose \\method, a two-stage\nframework adapting rule-based RL for multimodal reasoning through\nFoundational Reasoning Enhancement (FRE) followed by\nMultimodal Generalization Training (MGT). The FRE stage first\nstrengthens reasoning abilities using text-only data with rule-based RL, then\nthe MGT stage generalizes these reasoning capabilities to multimodal domains.\n  Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves\n4.83\\% and 4.5\\% average improvements over baselines in multimodal and\ntext-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game\ntasks. These results validate that text-based reasoning enhancement enables\neffective multimodal generalization, offering a data-efficient paradigm that\nbypasses costly high-quality multimodal training data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07536.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08638",
      "authors": [
        {
          "_id": "67d1027435066eade61549ae",
          "user": {
            "_id": "5fd6f670053c8345eddc1b68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd6f670053c8345eddc1b68/cuTsu2krRYHC6zYGD2dpQ.jpeg",
            "isPro": false,
            "fullname": "Ruibin Yuan",
            "user": "a43992899",
            "type": "user"
          },
          "name": "Ruibin Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:33.054Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549af",
          "name": "Hanfeng Lin",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b0",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b1",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-12T06:24:13.961Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b2",
          "name": "Jiahao Pan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b3",
          "name": "Yongyi Zang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b4",
          "name": "Haohe Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b5",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b6",
          "name": "Wenye Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b7",
          "user": {
            "_id": "654907a4a1faff97850c4eff",
            "avatarUrl": "/avatars/458c90151614bc7f116943b6e67d6b8a.svg",
            "isPro": false,
            "fullname": "du",
            "user": "dododododo",
            "type": "user"
          },
          "name": "Xingjian Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:36.330Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b8",
          "name": "Xinrun Du",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b9",
          "name": "Zhen Ye",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ba",
          "name": "Tianyu Zheng",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bb",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bc",
          "user": {
            "_id": "6417d9ea8f689506e7148417",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
            "isPro": false,
            "fullname": "minghao",
            "user": "Liam-Liu",
            "type": "user"
          },
          "name": "Minghao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:39.193Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bd",
          "name": "Zeyue Tian",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549be",
          "name": "Ziya Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bf",
          "name": "Liumeng Xue",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c0",
          "user": {
            "_id": "6628adb14277eae0da5eee28",
            "avatarUrl": "/avatars/6cb41b80cc5e014e455dfc2a22682e64.svg",
            "isPro": true,
            "fullname": "HKUST Audio",
            "user": "HKUST-Audio",
            "type": "user"
          },
          "name": "Xingwei Qu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-12T03:41:43.139Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c1",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c2",
          "name": "Shangda Wu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c3",
          "name": "Tianhao Shen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c4",
          "name": "Ziyang Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c5",
          "name": "Jun Zhan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c6",
          "name": "Chunhui Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c7",
          "name": "Yatian Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c8",
          "name": "Xiaowei Chi",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c9",
          "name": "Xinyue Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ca",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cb",
          "name": "Xiangzhou Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cc",
          "name": "Shansong Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cd",
          "name": "Lingrui Mei",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ce",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cf",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d0",
          "name": "Jianwei Yu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d1",
          "name": "Guojian Pang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d2",
          "name": "Xu Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d3",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d4",
          "name": "Xiaohuan Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d5",
          "name": "Lijun Yu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d6",
          "name": "Emmanouil Benetos",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d7",
          "name": "Yong Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d8",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d9",
          "name": "Xie Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549da",
          "name": "Gus Xia",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549db",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549dc",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549dd",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549de",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549df",
          "name": "Xipeng Qiu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e0",
          "name": "Roger Dannenberg",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e1",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e2",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e3",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e4",
          "name": "Wei Xue",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e5",
          "name": "Xu Tan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e6",
          "name": "Yike Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:26:50.000Z",
      "title": "Scaling Open-Ended Foundation Models for Music Generation",
      "summary": "Nosotros presentamos \"YuE\", una familia de modelos basados en la arquitectura de la base abierta LLaMA2, diseñados para abordar los desafíos particulares de la generación musical de largas frases como canciones. Específicamente, YuE supera los tres mil millones de tokens y puede generar música que ocupe el 5% de un disco, manteniendo al mismo tiempo la consistencia de la letra, la estructura musical coherente y una melodía bocarral emocionante. Esto se logra a través de: (1) la predicción del siguiente token para separaciones de trayecto, (2) la mejora de la consistencia de la letra en largas contextos mediante condicionales avanzados, y (3) aprendizaje previo multi-tarea y multi-etapa. Además, YuE redesigna las técnicas de generación musical y permite transformaciones de estilo bidireccionales (por ejemplo, convertir música pop de ciudad japonesa en rap en inglés mientras se mantiene el acompañamiento original). Según evaluaciones detalladas, YuE supera a algunos sistemas propiaís en la flexibilidad musical y bocarral. Además, en el ajuste fino de YuE, se pueden obtener controles adicionales y soporte para lenguajes adicionales. Además, los resultados de generación muestran que las representaciones aprendidas de YuE también demostrando excelentes resultados en tareas de comprensión musical, superando los mejores métodos en el marco de referencia MARBLE. Palabras clave: letra a canción, generación de música, largas frases, modelo de base, generación musical.",
      "upvotes": 39,
      "discussionId": "67d1027735066eade6154a7e",
      "ai_keywords": [
        "track-decoupled next-token prediction",
        "dense mixture signals",
        "structural progressive conditioning",
        "long-context lyrical alignment",
        "multitask, multiphase pre-training",
        "in-context learning",
        "versatile style transfer",
        "bidirectional generation",
        "musicality",
        "vocal agility",
        "tail languages",
        "music understanding tasks",
        "MARBLE benchmark"
      ]
    },
    "publishedAt": "2025-03-11T13:26:50.000Z",
    "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
    "summary": "We tackle the task of long-form music generation--particularly the\nchallenging lyrics-to-song problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08638.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08120",
      "authors": [
        {
          "_id": "67d0f5ace3c8042929eea946",
          "user": {
            "_id": "64c860d23a3f428da65ea499",
            "avatarUrl": "/avatars/f0bcc6ae7e558babe691b6bbf1059c9d.svg",
            "isPro": false,
            "fullname": "lijunzhe",
            "user": "tulvgengenr",
            "type": "user"
          },
          "name": "Junzhe Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:39:26.197Z",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea947",
          "name": "Xuerui Qiu",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea948",
          "name": "Linrui Xu",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea949",
          "name": "Liya Guo",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94a",
          "user": {
            "_id": "64daecec888b7e9c400f59b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png",
            "isPro": false,
            "fullname": "Delin Qu",
            "user": "delinqu",
            "type": "user"
          },
          "name": "Delin Qu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:39:29.349Z",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94b",
          "name": "Tingting Long",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94c",
          "name": "Chun Fan",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94d",
          "name": "Ming Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T07:34:59.000Z",
      "title": "UniF^2ace: Modelo monomodal unificado para comprensión y generación de caras en la nube",
      "summary": "Los modelos multimodal unificados (UMMs) desempeñan un papel crucial como paradigma fundamental en la investigación de visión computacional, demostrando una potencia significativa en la comprensión e generación de imágenes. Sin embargo, la investigación previa en el ámbito de los dominios faciales ha centrado su enfoque en la comprensión de características faciales básicas, limitando su capacidad para procesar características faciales subtilmente, lo que ha sido considerado un problema en su capacidad de generación. Para superar estas limitaciones, proponemos UniF^2ace, el primer UMM especializado en la comprensión y generación de características faciales subtil. Generalmente, utilizamos dos métodos de división que ofrecen dos beneficios mutuamente y una arquitectura de explorador de micros en dos etapas para entrenar UniF^2ace con un conjunto de datos especialmente construido. En particular, hemos construido el primer conjunto de datos faciales de escala grande, UniF^2ace-130K, que incluye 130K pares de imágenes-texto, y este conjunto ha sido ampliado con 1 millón de pares de preguntas-respuestas para expandir diversas características faciales. Además, hemos establecido la conexión teórica entre el método de punto de división y el modelo de generación con máscara, optimizando ambos límites inferiores para mejorar significativamente la capacidad del modelo para sintetizar información detallada de las caras. Finalmente, hemos introducido exploradores de micros a nivel de token y de secuencia, lo que permite el aprendizaje de representaciones subtilmente eficientes para ambas tareas de comprensión y generación. Los experimentos extendidos en UniF^2ace-130K demuestran que UniF^2ace supera a los UMMs y a los modelos de generación existentes, al lograr un excelente desempeño en ambas tareas de comprensión y generación.",
      "upvotes": 23,
      "discussionId": "67d0f5b4e3c8042929eeab49",
      "ai_keywords": [
        "diffusion score matching",
        "masked generative models",
        "evidence lower bounds",
        "mixture-of-experts",
        "token-level",
        "sequence-level"
      ]
    },
    "publishedAt": "2025-03-11T03:34:59.000Z",
    "title": "UniF^2ace: Fine-grained Face Understanding and Generation\n  with Unified Multimodal Models",
    "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm in\nfoundational computer vision research, demonstrating significant potential in\nboth image understanding and generation. However, existing research in the face\ndomain primarily focuses on coarse facial attribute understanding,\nwith limited capacity to handle fine-grained facial attributes and\nwithout addressing generation capabilities. To overcome these limitations, we\npropose UniF^2ace, the first UMM tailored specifically for\nfine-grained face understanding and generation. In general, we train\nUniF^2ace on a self-constructed, specialized dataset utilizing two\nmutually beneficial diffusion techniques and a two-level mixture-of-experts\narchitecture. Specifically, we first build a large-scale facial dataset,\nUniF^2ace-130K, which contains 130K image-text pairs with one\nmillion question-answering pairs that span a wide range of facial attributes.\nSecond, we establish a theoretical connection between discrete diffusion score\nmatching and masked generative models, optimizing both evidence lower bounds\nsimultaneously, which significantly improves the model's ability to synthesize\nfacial details. Finally, we introduce both token-level and sequence-level\nmixture-of-experts, enabling efficient fine-grained representation learning for\nboth understanding and generation tasks. Extensive experiments on\nUniF^2ace-130K demonstrate that UniF^2ace outperforms\nexisting UMMs and generative models, achieving superior performance across both\nunderstanding and generation tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08120.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07703",
      "authors": [
        {
          "_id": "67d0f422a3158b8e55d3562f",
          "name": "Lixue Gong",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35630",
          "name": "Xiaoxia Hou",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35631",
          "name": "Fanshi Li",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35632",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35633",
          "name": "Xiaochen Lian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35634",
          "name": "Fei Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35635",
          "name": "Liyang Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35636",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35637",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35638",
          "name": "Yichun Shi",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35639",
          "name": "Shiqi Sun",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563a",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563b",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563c",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563d",
          "name": "Xun Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563e",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563f",
          "name": "Guofeng Wu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35640",
          "user": {
            "_id": "6381c5d63680a7cf34e08ca9",
            "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
            "isPro": false,
            "fullname": "wujie10558@gmail.com",
            "user": "wujie10",
            "type": "user"
          },
          "name": "Jie Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:40:44.088Z",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35641",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35642",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35643",
          "name": "Linjie Yang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35644",
          "name": "Zhonghua Zhai",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35645",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35646",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35647",
          "name": "Yuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35648",
          "name": "Shijia Zhao",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35649",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3564a",
          "name": "Weilin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:33.000Z",
      "title": "Seedream 2.0: シードリーム 2.0: Modelo básico de generación de imágenes entre lenguas de idiomas chino y inglés",
      "summary": "El avance de modelos de diferenciación rápida está impulsando un progreso sorprendente en el campo de la generación de imágenes. Sin embargo, modelos generales como Flux, SD3.5 y Midjourney enfrentan problemas como el bias del modelo, capacidades de entrenamiento textual limitadas y una insuficiente comprensión de la nuancia cultural china. Para resolver estos limites, presentamos Seedream 2.0, un modelo de generación de imágenes bilingüe en múltiples dimensiones que excelve en el idioma materno y el inglés. Este modelo soporta la generación de imágenes bilingües y la entrenamiento textual en ambos idiomas, permitiendo una gestión efectiva de los prompts. Hemos desarrollado un sistema de integración de conocimientos potente y un sistema de captura que equilibra la precisión y la riqueza de la explicación de las imágenes. Seedream incorpora un modelo de lenguaje bilingüe autónomo como encoder de texto, lo que permite que aprenda directamente el conocimiento materno del idioma desde los datos masivos. De esta manera, puede generar con alta precisión la nuancia y la expresión artística de la cultura en chino o inglés. Además, Glyph-Aligned ByT5 apoya un entrenamiento flexible a nivel de palabra y Scaled ROPE ofrece una buena extensibilidad para resoluciones no entrenadas. La fortaleza de la edición del modelo final, que incluye optimizaciones posteriores multi-etapa y iteraciones de SFT y RLHF, ha permitido alcanzar el rendimiento líder en múltiples aspectos, como la conformidad con los prompts, la belleza, la precisión textual y la exactitud estructural. A través de experimentos extensos, Seedream 2.0 ha demostrado su capacidad de alcanzar el rendimiento líder en múltiples aspectos, incluyendo la conformidad con los prompts, la belleza, la precisión textual y la exactitud estructural, lo que lo convierte en un modelo de alto rendimiento en varios aspectos.",
      "upvotes": 21,
      "discussionId": "67d0f42fa3158b8e55d358ea",
      "projectPage": "https://team.doubao.com/zh/tech/seedream",
      "ai_keywords": [
        "diffusion models",
        "Flux",
        "SD3.5",
        "Midjourney",
        "model bias",
        "Seedream 2.0",
        "bilingual image generation",
        "text prompt",
        "data system",
        "caption system",
        "bilingual large language model",
        "high-fidelity images",
        "cultural nuances",
        "aesthetic expressions",
        "Glyph-Aligned ByT5",
        "character-level text rendering",
        "Scaled ROPE",
        "multi-phase post-training optimizations",
        "SFT",
        "RLHF",
        "prompt-following",
        "structural correctness",
        "ELO score",
        "instruction-based image editing model",
        "SeedEdit"
      ]
    },
    "publishedAt": "2025-03-10T13:58:33.000Z",
    "title": "Seedream 2.0: A Native Chinese-English Bilingual Image Generation\n  Foundation Model",
    "summary": "Rapid advancement of diffusion models has catalyzed remarkable progress in\nthe field of image generation. However, prevalent models such as Flux, SD3.5\nand Midjourney, still grapple with issues like model bias, limited text\nrendering capabilities, and insufficient understanding of Chinese cultural\nnuances. To address these limitations, we present Seedream 2.0, a native\nChinese-English bilingual image generation foundation model that excels across\ndiverse dimensions, which adeptly manages text prompt in both Chinese and\nEnglish, supporting bilingual image generation and text rendering. We develop a\npowerful data system that facilitates knowledge integration, and a caption\nsystem that balances the accuracy and richness for image description.\nParticularly, Seedream is integrated with a self-developed bilingual large\nlanguage model as a text encoder, allowing it to learn native knowledge\ndirectly from massive data. This enable it to generate high-fidelity images\nwith accurate cultural nuances and aesthetic expressions described in either\nChinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible\ncharacter-level text rendering, while a Scaled ROPE generalizes well to\nuntrained resolutions. Multi-phase post-training optimizations, including SFT\nand RLHF iterations, further improve the overall capability. Through extensive\nexperimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art\nperformance across multiple aspects, including prompt-following, aesthetics,\ntext rendering, and structural correctness. Furthermore, Seedream 2.0 has been\noptimized through multiple RLHF iterations to closely align its output with\nhuman preferences, as revealed by its outstanding ELO score. In addition, it\ncan be readily adapted to an instruction-based image editing model, such as\nSeedEdit, with strong editing capability that balances instruction-following\nand image consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07703.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05978",
      "authors": [
        {
          "_id": "67d129d732b4bbfb938321a1",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a2",
          "user": {
            "_id": "66015e8aa4d296af07de538e",
            "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
            "isPro": false,
            "fullname": "Ye",
            "user": "Owen777",
            "type": "user"
          },
          "name": "Tian Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:50:12.319Z",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a3",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a4",
          "name": "Xuancheng Yang",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a5",
          "name": "Jiantong Zhao",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a6",
          "name": "Hanzhong Guo",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a7",
          "name": "Terrance Wang",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a8",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a9",
          "name": "Zeke Xie",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321aa",
          "name": "Lei Zhu",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ab",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ac",
          "name": "Michael Lingelbach",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ad",
          "name": "Daquan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T23:21:11.000Z",
      "title": "Crea imágenes de infinitas historias con palabras y voz.",
      "summary": "MagicInfinity utiliza el marco de trabajo Difu-sion Transfomer (DiT) para superar las limitaciones de las animaciones de personajes anteriores y ofrece resultados de alta calidad para diferentes tipos de personajes. Esto incluye personajes humanos realistas, personajes de pieza, y personajes animados estilizados. Esta función soporta variaciones faciales, perspectivas panorámicas y permite la especificación precisa de los determinantes de la animación de múltiples personajes utilizando una máscara de entrada. Nuestro enfoque resuelve principales problemas mediante tres innovaciones implementaciones: 1. La función de atención 3D y el deshacer de ventanas de sliding window para generar videos de larga duración, manteniendo la cooperación secuencial y la calidad visual de diferentes estilos de personajes. 2. Introducción del aprendizaje de clasificación de dos etapas para permitir el control multimodal a largo plazo, utilizando sincronización de voz con labios, acciones representativas de frases y la preservación de identidad a través de imágenes de referencia. 3. Uso de máscaras por áreas y funciones de pérdida adaptativas para equilibrar el control global de la frase y el guía vocal local, y apoyar la animación de determinados elementos. La optimización se logra a través de nuestra técnica innovadora de paso integrado y distillación de cfg, logrando una velocidad de inferencia 20 veces más rápida que el modelo base. Con 8 GPU H100, se pueden generar videos de 540x540p en 10 segundos y videos de 720x720p en 30 segundos, sin pérdida de calidad. Nuevas evaluaciones de benchmark muestran excelentes resultados en sincronización de voz y labios, preservación de identidad y movimientos naturales, y están disponibles para uso público. https://www.hedra.com/, ejemplo: https://magicinfinite.github.io/",
      "upvotes": 19,
      "discussionId": "67d129e332b4bbfb938324a0",
      "projectPage": "https://magicinfinite.github.io/",
      "ai_keywords": [
        "diffusion Transformer (DiT)",
        "3D full-attention mechanisms",
        "sliding window denoising strategy",
        "infinite video generation",
        "temporal coherence",
        "two-stage curriculum learning scheme",
        "audio for lip sync",
        "text for expressive dynamics",
        "reference images for identity preservation",
        "region-specific masks",
        "adaptive loss functions",
        "unified step and cfg distillation techniques",
        "inference speed",
        "audio-lip synchronization",
        "identity preservation",
        "motion naturalness"
      ]
    },
    "publishedAt": "2025-03-07T18:21:11.000Z",
    "title": "MagicInfinite: Generating Infinite Talking Videos with Your Words and\n  Voice",
    "summary": "We present MagicInfinite, a novel diffusion Transformer (DiT) framework that\novercomes traditional portrait animation limitations, delivering high-fidelity\nresults across diverse character types-realistic humans, full-body figures, and\nstylized anime characters. It supports varied facial poses, including\nback-facing views, and animates single or multiple characters with input masks\nfor precise speaker designation in multi-character scenes. Our approach tackles\nkey challenges with three innovations: (1) 3D full-attention mechanisms with a\nsliding window denoising strategy, enabling infinite video generation with\ntemporal coherence and visual quality across diverse character styles; (2) a\ntwo-stage curriculum learning scheme, integrating audio for lip sync, text for\nexpressive dynamics, and reference images for identity preservation, enabling\nflexible multi-modal control over long sequences; and (3) region-specific masks\nwith adaptive loss functions to balance global textual control and local audio\nguidance, supporting speaker-specific animations. Efficiency is enhanced via\nour innovative unified step and cfg distillation techniques, achieving a 20x\ninference speed boost over the basemodel: generating a 10 second 540x540p video\nin 10 seconds or 720x720p in 30 seconds on 8 H100 GPUs, without quality loss.\nEvaluations on our new benchmark demonstrate MagicInfinite's superiority in\naudio-lip synchronization, identity preservation, and motion naturalness across\ndiverse scenarios. It is publicly available at https://www.hedra.com/, with\nexamples at https://magicinfinite.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05978.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08625",
      "authors": [
        {
          "_id": "67d0fd74f8595b656f921a48",
          "user": {
            "_id": "632179745fc60c44fd91fc33",
            "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
            "isPro": false,
            "fullname": "zhumuzhi",
            "user": "Z-MU-Z",
            "type": "user"
          },
          "name": "Muzhi Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:42.495Z",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a49",
          "name": "Yuzhuo Tian",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4a",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4b",
          "name": "Chunluan Zhou",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4c",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4d",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4e",
          "name": "Ming Yang",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4f",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:08:54.000Z",
      "title": "SegAgent: Para explorar el entendimiento de píxeles en la línea de modelos de Múltiple Modalidad, se imita el camino del Human Annotationist.",
      "summary": "Los modelos de línea de múltiples personajes (MLLMs) muestran capacidades de comprensión de imágenes, pero están limitados en aplicaciones prácticas debido a dificultades en la comprensión a nivel de píxeles. Actualmente, tareas de evaluación como VQA y visual georeferencing se encargan de evaluar con exceso la comprensión a nivel de píxeles, lo que no refleja la capacidad real de los MLLMs. La base de la comprensión a nivel de píxeles está en la segmentación, pero los métodos actuales requieren que los MLLMs generen tokens ocultos interpretables por un decodificador externo de píxeles. Este enfoque destruye el espacio de salida de texto de los MLLMs, puede perder la capacidad lingüística potencial, reduce la eficacia y la extensibilidad del modelo, y no refleja su comprensión propia a nivel de píxeles.\n\nPor lo tanto, presentamos la Tarea de Análisis de Mascaras Humanas (HLMAT). Esta es un nuevo paradigma en el que los MLLMs se comportan como anónitadores humanos, utilizando herramientas de análisis interactivo para realizar análisis. La segmentación se modela en un proceso de decisión marcoviano multinivel, y HLMAT modela cambios estructurales o utiliza tokens ocultos, permitiendo que los MLLMs generen puntos de clic basados en texto para crear altas calidades de mascaras. En este sistema, se desarrolló al SegAgent, un modelo finalmente entrenado como un anónitador humano. Este modelo logra rendimientos de la tecnología más reciente (SOTA) y apoya tareas adicionales como la optimización de las mascaras y filtrado de análisis.\n\nHLMAT proporciona un protocolo para evaluar la comprensión a nivel de píxeles de los MLLMs, introduciendo tareas de decisión multinivel visual y ayudando a explorar la capacidad de inferencia visual de los MLLMs. La mejora en la búsqueda de árboles de nuestra política de mejora StaR y el guía PRM mejoran la robustez del modelo en tareas complejas de segmentación, y se basan en la precisión del reconocimiento visual y el futuro de sistemas de decisión multinivel de los MLLMs.",
      "upvotes": 18,
      "discussionId": "67d0fd76f8595b656f921ae8",
      "projectPage": "https://aim-uofa.github.io/SegAgent/",
      "githubRepo": "https://github.com/aim-uofa/SegAgent",
      "ai_keywords": [
        "Human-Like Mask Annotation Task (HLMAT)",
        "Markov Decision Process",
        "multi-step decision-making",
        "click points",
        "masks",
        "policy improvement method StaR",
        "PRM-guided tree search",
        "mask refinement",
        "annotation filtering",
        "fine-grained pixel understanding",
        "vision-centric task",
        "visual reasoning abilities"
      ]
    },
    "publishedAt": "2025-03-11T13:08:54.000Z",
    "title": "SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by\n  Imitating Human Annotator Trajectories",
    "summary": "While MLLMs have demonstrated adequate image understanding capabilities, they\nstill struggle with pixel-level comprehension, limiting their practical\napplications. Current evaluation tasks like VQA and visual grounding remain too\ncoarse to assess fine-grained pixel comprehension accurately. Though\nsegmentation is foundational for pixel-level understanding, existing methods\noften require MLLMs to generate implicit tokens, decoded through external pixel\ndecoders. This approach disrupts the MLLM's text output space, potentially\ncompromising language capabilities and reducing flexibility and extensibility,\nwhile failing to reflect the model's intrinsic pixel-level understanding.\n  Thus, we introduce the Human-Like Mask Annotation Task (HLMAT), a new\nparadigm where MLLMs mimic human annotators using interactive segmentation\ntools. Modeling segmentation as a multi-step Markov Decision Process, HLMAT\nenables MLLMs to iteratively generate text-based click points, achieving\nhigh-quality masks without architectural changes or implicit tokens. Through\nthis setup, we develop SegAgent, a model fine-tuned on human-like annotation\ntrajectories, which achieves performance comparable to state-of-the-art (SOTA)\nmethods and supports additional tasks like mask refinement and annotation\nfiltering.\n  HLMAT provides a protocol for assessing fine-grained pixel understanding in\nMLLMs and introduces a vision-centric, multi-step decision-making task that\nfacilitates exploration of MLLMs' visual reasoning abilities. Our adaptations\nof policy improvement method StaR and PRM-guided tree search further enhance\nmodel robustness in complex segmentation tasks, laying a foundation for future\nadvancements in fine-grained visual perception and multi-step decision-making\nfor MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08625.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07604",
      "authors": [
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b5",
          "name": "Tianhe Lin",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b6",
          "user": {
            "_id": "62d65139667051e0a29bffe7",
            "avatarUrl": "/avatars/0252aa2bcd4cf1c8e4b87e5f164b6da5.svg",
            "isPro": false,
            "fullname": "Jian Xie",
            "user": "hsaest",
            "type": "user"
          },
          "name": "Jian Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:42:36.765Z",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b7",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b8",
          "name": "Deqing Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:31.000Z",
      "title": "La inicialización de entrada en Transformer es por inferencia mediante sorting.",
      "summary": "Los cálculos durante el test han sido identificados como un nuevo paradigma que mejora la capacidad de inferencia multi-nivel compleja del modelo de lenguaje. Esto se ha demostrado claramente con el éxito de OpenAI's o1, o3 y DeepSeek's R1. En comparación con la inferencia explícita en el test, la inferencia oculta es más eficiente y genera menos tokens. Sin embargo, la razón por la que la capacidad de inferencia evolutiva aparece de manera oculta no es clara. En este artículo, se realizan experimentos analíticos para investigar cómo funciona la inferencia oculta en tareas multi-nivel, al entrenar el modelo GPT-2 con un conjunto de datos de matemáticas multi-nivel. Nuestros hallazgos son los siguientes: 1) Modelos de lenguaje pueden alcanzar altas precisiones en pruebas dentro y fuera del dominio, al entrenarse con datos de patrones fijos y realizar inferencia en pasos. Sin embargo, esta capacidad solo se manifiesta con los datos de patrones fijos. 2) Por otro lado, la capacidad de inferencia oculta entrenada con patrones inestables se sobreajusta a estos patrones y falla en aplicaciones evolutivas. Este límite también se observa en los modelos de lenguaje más avanzados de gran escala. Estos hallazgos muestran que los modelos de lenguaje pueden obtener inferencia oculta a través de aprendizaje corto, ganando excelente rendimiento en tareas similares pero perdiendo capacidades de generalización.",
      "upvotes": 14,
      "discussionId": "67cfa4edd8cb8688d7d6d908",
      "githubRepo": "https://github.com/TianheL/LM-Implicit-Reasoning",
      "ai_keywords": [
        "test-time compute",
        "multi-step reasoning",
        "OpenAI's o1",
        "OpenAI's o3",
        "DeepSeek's R1",
        "implicit reasoning",
        "inference-efficient",
        "generated tokens",
        "explicit reasoning",
        "GPT-2",
        "multi-step mathematical reasoning dataset",
        "step-by-step reasoning",
        "in-domain tests",
        "out-of-domain tests",
        "fixed-pattern data",
        "unfixed-pattern data",
        "overfit",
        "generalization",
        "shortcut learning"
      ]
    },
    "publishedAt": "2025-03-10T13:58:31.000Z",
    "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
    "summary": "Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07604.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08605",
      "authors": [
        {
          "_id": "67d0ed0877b0c8ac3f304ef1",
          "name": "Subin Kim",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef2",
          "name": "Seoung Wug Oh",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef3",
          "name": "Jui-Hsien Wang",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef4",
          "name": "Joon-Young Lee",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef5",
          "name": "Jinwoo Shin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:43:45.000Z",
      "title": "Generación de videos de evento para amostrado de cópia sincrónico con ajustes libres",
      "summary": "Recientemente, el desarrollo de modelos de expansión de vídeo ha permitido la generación de alta calidad de vídeos cortos a partir de un solo prompt, pero la limitada cantidad de datos y los altos costos de cálculo hacen que la generación de vídeos largos de la realidad sea una tarea difícil. En respuesta a esto, varios estudios han propuesto un enfoque sin tuning, extendiendo modelos existentes para la generación de vídeos largos y permitiendo cambios de contenido dinámicos mediante múltiples prompts. Sin embargo, estos métodos se centran principalmente en garantizar un movimiento suave entre frames adyacentes, lo que resulta en una pérdida gradual de coherencia significativa en secuencias largas, lo que ha sido un problema principal. Para resolver esto, proponemos la Synchronized Coupled Sampling (SynCoS). SynCoS sincroniza el paso de expansión de todo el vídeo y introduce un nuevo método de inferencia que asegura la coherencia a distancias considerables, no solo entre frames adyacentes. Nuestro enfoque combina dos estrategias de sampling: sampling inverso y sampling basado en optimización, garantizando un movimiento suave entre frames adyacentes y forzando una coherencia general. Sin embargo, intercambiar directamente estos samples puede crear tildes de expansión asimétricas, destruir el prompt guide y generar cambios de contenido impredecibles. Para resolver esto, SynCoS utiliza pasos de tiempo básicos y ruidos fijos para sincronizar el sampling, asegurando que las muestras sean coherentes y que las tildes de expansión se ajusten al prompt guide. Los experimentos extensos muestran que SynCoS mejora significativamente la generación de vídeos largos, logra un movimiento suave y una coherencia a distancias considerables, superando de manera notable los métodos anteriores.",
      "upvotes": 13,
      "discussionId": "67d0ed0b77b0c8ac3f304f7c",
      "projectPage": "https://syncos2025.github.io/",
      "githubRepo": "https://github.com/subin-kim-cv/SynCoS",
      "ai_keywords": [
        "text-to-video diffusion models",
        "high-quality short video generation",
        "long video generation",
        "tuning-free approaches",
        "multiple prompts",
        "dynamic content changes",
        "smooth transitions",
        "content drift",
        "semantic coherence",
        "Synchronized Coupled Sampling (SynCoS)",
        "denoising paths",
        "reverse sampling",
        "optimization-based sampling",
        "seamless local transitions",
        "global coherence",
        "grounded timestep",
        "fixed baseline noise",
        "multi-event long video generation",
        "long-range consistency"
      ]
    },
    "publishedAt": "2025-03-11T12:43:45.000Z",
    "title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling",
    "summary": "While recent advancements in text-to-video diffusion models enable\nhigh-quality short video generation from a single prompt, generating real-world\nlong videos in a single pass remains challenging due to limited data and high\ncomputational costs. To address this, several works propose tuning-free\napproaches, i.e., extending existing models for long video generation,\nspecifically using multiple prompts to allow for dynamic and controlled content\nchanges. However, these methods primarily focus on ensuring smooth transitions\nbetween adjacent frames, often leading to content drift and a gradual loss of\nsemantic coherence over longer sequences. To tackle such an issue, we propose\nSynchronized Coupled Sampling (SynCoS), a novel inference framework that\nsynchronizes denoising paths across the entire video, ensuring long-range\nconsistency across both adjacent and distant frames. Our approach combines two\ncomplementary sampling strategies: reverse and optimization-based sampling,\nwhich ensure seamless local transitions and enforce global coherence,\nrespectively. However, directly alternating between these samplings misaligns\ndenoising trajectories, disrupting prompt guidance and introducing unintended\ncontent changes as they operate independently. To resolve this, SynCoS\nsynchronizes them through a grounded timestep and a fixed baseline noise,\nensuring fully coupled sampling with aligned denoising paths. Extensive\nexperiments show that SynCoS significantly improves multi-event long video\ngeneration, achieving smoother transitions and superior long-range coherence,\noutperforming previous approaches both quantitatively and qualitatively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08605.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07891",
      "authors": [
        {
          "_id": "67d108c56bd6c57bab0b6f07",
          "name": "Jinhyuk Lee",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f08",
          "name": "Feiyang Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f09",
          "name": "Sahil Dua",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0a",
          "name": "Daniel Cer",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0b",
          "name": "Madhuri Shanbhogue",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0c",
          "name": "Iftekhar Naim",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0d",
          "name": "Gustavo Hernández Ábrego",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0e",
          "name": "Zhe Li",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0f",
          "name": "Kaifeng Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f10",
          "name": "Henrique Schechter Vera",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f11",
          "name": "Xiaoqi Ren",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f12",
          "name": "Shanfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f13",
          "name": "Daniel Salz",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f14",
          "name": "Michael Boratko",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f15",
          "name": "Jay Han",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f16",
          "name": "Blair Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f17",
          "name": "Shuo Huang",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f18",
          "name": "Vikram Rao",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f19",
          "name": "Paul Suganthan",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1a",
          "name": "Feng Han",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1b",
          "name": "Andreas Doumanoglou",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1c",
          "name": "Nithi Gupta",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1d",
          "name": "Fedor Moiseev",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1e",
          "name": "Cathy Yip",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1f",
          "name": "Aashi Jain",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f20",
          "name": "Simon Baumgartner",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f21",
          "name": "Shahrokh Shahi",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f22",
          "name": "Frank Palma Gomez",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f23",
          "name": "Sandeep Mariserla",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f24",
          "name": "Min Choi",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f25",
          "name": "Parashar Shah",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f26",
          "name": "Sonam Goenka",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f27",
          "name": "Ke Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f28",
          "name": "Ye Xia",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f29",
          "name": "Koert Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2a",
          "name": "Sai Meher Karthik Duddu",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2b",
          "name": "Yichang Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2c",
          "name": "Trevor Walker",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2d",
          "name": "Wenlei Zhou",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2e",
          "name": "Rakesh Ghiya",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2f",
          "name": "Zach Gleicher",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f30",
          "name": "Karan Gill",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f31",
          "name": "Zhe Dong",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f32",
          "name": "Mojtaba Seyedhosseini",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f33",
          "name": "Yunhsuan Sung",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f34",
          "name": "Raphael Hoffmann",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f35",
          "name": "Tom Duerig",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T22:16:45.000Z",
      "title": "Gemini Embedding: Gemini's Extensibility Embedding",
      "summary": "En este informe, se presenta la más avanzada y poderosa herramienta de modelado interno de Google, llamada \"Gemini Embedding\". Esta herramienta utiliza las capacidades únicas de comprensión multilingüe y comprensión de código de Gemini para generar una representación de alto nivel de generalización en textos multilingües y de diferentes contextos. Las representaciones generadas por Gemini Embedding pueden aplicarse en diversas tareas de procesamiento posterior, como clasificación, similitud, clustering, ordenamiento y búsqueda. Evaluada en el benchmark de text embedding multilingüe de gran escala (MMTEB), este modelo muestra un gran avance sobre los anteriores, ya que el benchmark incluye más de 100 tareas en más de 250 idiomas. Gemini Embedding ha demostrado ser el mejor en los benchmarks multilingüe, inglés y de código, y nuestro modelo de integración muestra una fuerza significativa en una amplia gama de tareas, superando los modelos de dominio específico.",
      "upvotes": 12,
      "discussionId": "67d108c66bd6c57bab0b6f6e",
      "ai_keywords": [
        "Gemini Embedding",
        "large language model",
        "multilingual",
        "code understanding",
        "representations",
        "downstream tasks",
        "classification",
        "similarity",
        "clustering",
        "ranking",
        "retrieval",
        "Massive Multilingual Text Embedding Benchmark (MMTEB)",
        "embedding quality",
        "specialized domain-specific models"
      ]
    },
    "publishedAt": "2025-03-10T18:16:45.000Z",
    "title": "Gemini Embedding: Generalizable Embeddings from Gemini",
    "summary": "In this report, we introduce Gemini Embedding, a state-of-the-art embedding\nmodel leveraging the power of Gemini, Google's most capable large language\nmodel. Capitalizing on Gemini's inherent multilingual and code understanding\ncapabilities, Gemini Embedding produces highly generalizable embeddings for\ntext spanning numerous languages and textual modalities. The representations\ngenerated by Gemini Embedding can be precomputed and applied to a variety of\ndownstream tasks including classification, similarity, clustering, ranking, and\nretrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark\n(MMTEB), which includes over one hundred tasks across 250+ languages, Gemini\nEmbedding substantially outperforms prior state-of-the-art models,\ndemonstrating considerable improvements in embedding quality. Achieving\nstate-of-the-art performance across MMTEB's multilingual, English, and code\nbenchmarks, our unified model demonstrates strong capabilities across a broad\nselection of tasks and surpasses specialized domain-specific models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07891.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08619",
      "authors": [
        {
          "_id": "67d0eb9cec69694dca382208",
          "name": "Xianfeng Wu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382209",
          "name": "Yajing Bai",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220a",
          "name": "Haoze Zheng",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220b",
          "name": "Harold Haodong Chen",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220c",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220d",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220e",
          "name": "Xuran Ma",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220f",
          "name": "Wen-Jie Shu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382210",
          "name": "Xianzu Wu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382211",
          "name": "Harry Yang",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382212",
          "name": "Ser-Nam Lim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:58:02.000Z",
      "title": "LightGen: Transferencia de conocimiento y optimización de preferencias directas para la generación eficiente de imágenes",
      "summary": "El desarrollo de la tecnología que transforma texto en imágenes ha sido impulsado principalmente por la disponibilidad de grandes conjuntos de datos y la arquitectura rica en parámetros. Estas necesidades imponen un estricto límite de accesibilidad para investigadores o profesionales con recursos computacionales limitados. En este artículo, se presenta LightGen, un modelo de entrenamiento eficiente que se ha desarrollado utilizando el conocimiento de distillación (KD) y la optimización de preferencias directas (DPO). LightGen adopta el éxito de la técnica de KD en modelos de lenguaje grandes de múltiples tipos (MLLM) para absorber el conocimiento del modelo de transformación de texto a imágenes de alto rendimiento (SOTA) en una arquitectura sencilla y poco parametrizada, la Masked Autoregressive Model (MAR). Utiliza un conjunto de datos de simple síntesis compuesto de 2M imágenes de alta calidad, demostrando que la diversidad de los datos es más importante que su cantidad para determinar el rendimiento del modelo. Esta estrategia reduce significativamente las necesidades de recursos computacionales y puede limitar el tiempo de entrenamiento previo a 88 GPU días en lugar de miles de GPU días. Además, se integra la técnica DPO para mejorar la fidelidad y la precisión de la posición de las imágenes, resuelviendo los defectos característicos de los datos de síntesis. Los experimentos detallados muestran que LightGen, al reducir significativamente los recursos computacionales y expandir el acceso en entornos con limitaciones de recursos, logra un calidad de generación de imágenes equivalente a los modelos de SOTA. El código está disponible en https://github.com/XianfengWu01/LightGen.",
      "upvotes": 11,
      "discussionId": "67d0eba3ec69694dca3823a0",
      "ai_keywords": [
        "knowledge distillation (KD)",
        "Direct Preference Optimization (DPO)",
        "Multi-Modal Large Language Models (MLLMs)",
        "Masked Autoregressive (MAR)",
        "synthetic dataset",
        "data diversity",
        "data volume",
        "model performance",
        "computational demands",
        "pre-training time",
        "synthetic data",
        "high-frequency details",
        "spatial inaccuracies",
        "image fidelity",
        "positional accuracy"
      ]
    },
    "publishedAt": "2025-03-11T12:58:02.000Z",
    "title": "LightGen: Efficient Image Generation through Knowledge Distillation and\n  Direct Preference Optimization",
    "summary": "Recent advances in text-to-image generation have primarily relied on\nextensive datasets and parameter-heavy architectures. These requirements\nseverely limit accessibility for researchers and practitioners who lack\nsubstantial computational resources. In this paper, we introduce \\model, an\nefficient training paradigm for image generation models that uses knowledge\ndistillation (KD) and Direct Preference Optimization (DPO). Drawing inspiration\nfrom the success of data KD techniques widely adopted in Multi-Modal Large\nLanguage Models (MLLMs), LightGen distills knowledge from state-of-the-art\n(SOTA) text-to-image models into a compact Masked Autoregressive (MAR)\narchitecture with only 0.7B parameters. Using a compact synthetic dataset of\njust 2M high-quality images generated from varied captions, we demonstrate\nthat data diversity significantly outweighs data volume in determining model\nperformance. This strategy dramatically reduces computational demands and\nreduces pre-training time from potentially thousands of GPU-days to merely 88\nGPU-days. Furthermore, to address the inherent shortcomings of synthetic data,\nparticularly poor high-frequency details and spatial inaccuracies, we integrate\nthe DPO technique that refines image fidelity and positional accuracy.\nComprehensive experiments confirm that LightGen achieves image generation\nquality comparable to SOTA models while significantly reducing computational\nresources and expanding accessibility for resource-constrained environments.\nCode is available at https://github.com/XianfengWu01/LightGen",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08619.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08686",
      "authors": [
        {
          "_id": "67d0f892a189f3978638e154",
          "name": "Jialv Zou",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e155",
          "name": "Bencheng Liao",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e156",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e157",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e158",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:46.000Z",
      "title": "OmniMamba: Modelo de Unidades Eficiente basado en Modelos de Espacio-Tiempo para la Comprensión y Generación de Demostraciones",
      "summary": "Recientemente, el desarrollo de modelos de generación de comprensión y visualización monomodal (o generación monomodal) ha estado limitado por el sobrecarga de complejidad computacional y la dependencia de grandes conjuntos de datos de entrenamiento. Presentamos el modelo de generación monomodal \"OmniMamba\", basado en arquitectura lineal. Este modelo genera texto e imágenes simultáneamente a través de un único paso de predicción del siguiente token. OmniMamba maximiza la eficiencia computacional y memoria de Mamba-2 para expandir la generación monomodal en la generación de texto. Para abordar los problemas de eficiencia de datos en modelos integrados, proponemos dos ideas clave: (1) separar la generación de módulos guiados por modelos de no-Ocasionalidad y (2) aplicar LoRA de manera eficiente en las secuencias de tareas. Además, introducimos una estrategia de entrenamiento en dos etapas para mitigar la desigualdad de datos en dos tareas. Con estas tecnologías, OmniMamba demostró un rendimiento competitivo en comparación con JanusFlow y superó a Show-o, entrenado con solo 2M pares de imágenes y texto, lo que representa un 1/1000 de la cantidad de datos de Show-o. En particular, OmniMamba presenta una velocidad de inferencia 119.2 veces más rápida y una reducción del 63% en memoria de GPU en comparación con competentes modelos basados en Transformer. El código y el modelo están disponibles en https://github.com/hustvl/OmniMamba.",
      "upvotes": 9,
      "discussionId": "67d0f894a189f3978638e1b7",
      "ai_keywords": [
        "OmniMamba",
        "linear-architecture-based multimodal generation model",
        "unified next-token prediction paradigm",
        "Mamba-2",
        "computational efficiency",
        "memory efficiency",
        "decoupled vocabularies",
        "task-specific LoRA",
        "parameter-efficient adaptation",
        "decoupled two-stage training strategy",
        "data imbalance",
        "Show-o",
        "benchmark",
        "inference efficiency",
        "Transformer-based counterparts"
      ]
    },
    "publishedAt": "2025-03-11T13:59:46.000Z",
    "title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation\n  via State Space Models",
    "summary": "Recent advancements in unified multimodal understanding and visual generation\n(or multimodal generation) models have been hindered by their quadratic\ncomputational complexity and dependence on large-scale training data. We\npresent OmniMamba, the first linear-architecture-based multimodal generation\nmodel that generates both text and images through a unified next-token\nprediction paradigm. The model fully leverages Mamba-2's high computational and\nmemory efficiency, extending its capabilities from text generation to\nmultimodal generation. To address the data inefficiency of existing unified\nmodels, we propose two key innovations: (1) decoupled vocabularies to guide\nmodality-specific generation, and (2) task-specific LoRA for\nparameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage\ntraining strategy to mitigate data imbalance between two tasks. Equipped with\nthese techniques, OmniMamba achieves competitive performance with JanusFlow\nwhile surpassing Show-o across benchmarks, despite being trained on merely 2M\nimage-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba\nstands out with outstanding inference efficiency, achieving up to a 119.2 times\nspeedup and 63% GPU memory reduction for long-sequence generation compared to\nTransformer-based counterparts. Code and models are released at\nhttps://github.com/hustvl/OmniMamba",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08686.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07860",
      "authors": [
        {
          "_id": "67d0e915d0038007e5a75178",
          "user": {
            "_id": "650871aeb44445e9b3625c7b",
            "avatarUrl": "/avatars/c35bd3e4a851389a4b6898a5a51e2219.svg",
            "isPro": false,
            "fullname": "James Burgess",
            "user": "jmhb",
            "type": "user"
          },
          "name": "James Burgess",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:41:30.547Z",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a75179",
          "user": {
            "_id": "65703fab7f50602340d23704",
            "avatarUrl": "/avatars/324c45f5fba9cd8c38a89b30427c06b4.svg",
            "isPro": false,
            "fullname": "Xiaohan Wang",
            "user": "nicholswang",
            "type": "user"
          },
          "name": "Xiaohan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:40:56.564Z",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517a",
          "name": "Yuhui Zhang",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517b",
          "name": "Anita Rau",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517c",
          "name": "Alejandro Lozano",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517d",
          "name": "Lisa Dunlap",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517e",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517f",
          "name": "Serena Yeung-Levy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T21:18:32.000Z",
      "title": "Vídeo Acción Dispersión",
      "summary": "¿Qué métodos se utilizan para verificar las diferencias que se presentan en un video cuando dos personas realizan la misma acción? En este artículo se presenta un nuevo desafío llamado \"Video Action Differencing (VidDiff)\" para reconocer las pequeñas diferencias en videos de la misma acción. Este desafío tiene aplicaciones en diversas áreas como el entrenamiento de jugadores o la cocina. Para facilitar el desarrollo de este nuevo desafío, se crearon primero los datos VidDiffBench. Este conjunto de datos incluye 549 pares de videos, con 4,469 diferencias específicas de acciones y 2,075 explicaciones humanas de los momentos en los que ocurren dichas diferencias. Nuestros experimentos muestran que los modelos grandes de multimodalidad (LMMs) como GPT-4o y Qwen2-VL son afectados significativamente por VidDiffBench. Al analizar los casos de fracaso de los LMMs en VidDiffBench, se identifican dos desafíos importantes: la localización y la comparación de subacciones relacionadas con dos videos y la comparación de detalles en los frames. Para abordar estos desafíos, se propone el método VidDiff, que está compuesto de tres etapas: la proposición de las diferencias de acción, la localización de los frames clave y la comparación de frames. Cada etapa utiliza un modelo base especializado. Para promover futuras investigaciones en este nuevo desafío, el benchmark se puede acceder en https://huggingface.co/datasets/jmhb/VidDiffBench y el código en http://jmhb0.github.io/viddiff.",
      "upvotes": 9,
      "discussionId": "67d0e917d0038007e5a751e9",
      "projectPage": "https://jmhb0.github.io/viddiff/",
      "githubRepo": "https://github.com/jmhb0/viddiff",
      "ai_keywords": [
        "Video Action Differencing (VidDiff)",
        "VidDiffBench",
        "multimodal models (LMMs)",
        "GPT-4o",
        "Qwen2-VL",
        "action difference proposal",
        "keyframe localization",
        "frame differencing",
        "agentic workflow",
        "fine-grained action differences",
        "localization timestamps"
      ]
    },
    "publishedAt": "2025-03-10T17:18:32.000Z",
    "title": "Video Action Differencing",
    "summary": "How do two individuals differ when performing the same action? In this work,\nwe introduce Video Action Differencing (VidDiff), the novel task of identifying\nsubtle differences between videos of the same action, which has many\napplications, such as coaching and skill learning. To enable development on\nthis new task, we first create VidDiffBench, a benchmark dataset containing 549\nvideo pairs, with human annotations of 4,469 fine-grained action differences\nand 2,075 localization timestamps indicating where these differences occur. Our\nexperiments demonstrate that VidDiffBench poses a significant challenge for\nstate-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL.\nBy analyzing failure cases of LMMs on VidDiffBench, we highlight two key\nchallenges for this task: localizing relevant sub-actions over two videos and\nfine-grained frame comparison. To overcome these, we propose the VidDiff\nmethod, an agentic workflow that breaks the task into three stages: action\ndifference proposal, keyframe localization, and frame differencing, each stage\nutilizing specialized foundation models. To encourage future research in this\nnew task, we release the benchmark at\nhttps://huggingface.co/datasets/jmhb/VidDiffBench and code at\nhttp://jmhb0.github.io/viddiff.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07860.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07572",
      "authors": [
        {
          "_id": "67d0e38171b6b577dbb8c72c",
          "user": {
            "_id": "6500bbf5e102da55f9ed43fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6500bbf5e102da55f9ed43fc/QZ6EAFV2CStFsILmTJw5D.jpeg",
            "isPro": true,
            "fullname": "Yuxiao Qu",
            "user": "CohenQu",
            "type": "user"
          },
          "name": "Yuxiao Qu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:41:33.926Z",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72d",
          "name": "Matthew Y. R. Yang",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72e",
          "name": "Amrith Setlur",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72f",
          "name": "Lewis Tunstall",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c730",
          "name": "Edward Emanuel Beeching",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c731",
          "name": "Ruslan Salakhutdinov",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c732",
          "name": "Aviral Kumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:40:43.000Z",
      "title": "Optimización de cálculos en pruebas mediante ajustes micro-regulaciones meta-recompensas",
      "summary": "El uso eficiente del cálculo computacional durante el entrenamiento de modelos es crucial para mejorar el rendimiento lógico de los LLMs. Actualmente, los métodos predominantes se centran en fine-tuning en tráficos de búsqueda o en el aprendizaje por refuerzo (RL) con resultados binarios (0/1), pero no está claro si estos enfoques eficientemente utilizan el cálculo computacional durante el test o si pueden ser ampliados cuando se mejora la gestión. En este artículo, respondemos a estas preguntas. Formulamos el problema de optimizar el cálculo computacional durante el test como un problema de aprendizaje meta-recursivo (RL), lo que permite observar de manera fundamental cómo se utiliza el cálculo computacional durante el test. Desde esta perspectiva, podemos configurar las largas secuencias de salida generadas por los LLMs durante el test como varias episodios de entrenamiento, evaluando la eficiencia del cálculo computacional mediante la pérdida acumulada de los tokens. El algoritmo de RL puede ayudar a encontrar un equilibrio entre la exploración óptima y la utilización. La minimización de la pérdida acumulada proporciona un equilibrio entre la exploración y la utilización de los tokens, demostrando que los modelos más recientes pueden reducir la pérdida, y combinando el RL con un bonus de recompensa densa, podemos realizar estos beneficios. Este bonus representa el \"progreso\" en cada bloque posterior de la secuencia de salida y cuantifica la variación en la probabilidad de éxito final. Utilizando esta perspectiva, desarrollamos una nueva clase de métodos de fine-tuning para optimizar el cálculo computacional durante el test, llamados Meta-Recursive Training Fine-tuning (MRT). El MRT proporciona un efecto relativo de 2-3 veces mejor que el RL de recompensas, y un efecto de aproximadamente 1.5 veces mejor en la eficiencia de los tokens matemáticos.",
      "upvotes": 8,
      "discussionId": "67d0e38271b6b577dbb8c7b7",
      "projectPage": "https://cohenqu.github.io/mrt.github.io/",
      "ai_keywords": [
        "meta-reinforcement learning (RL)",
        "cumulative regret",
        "token stream",
        "exploration and exploitation",
        "dense reward bonus",
        "likelihood of eventual success",
        "Meta Reinforcement Fine-Tuning (MRT)"
      ]
    },
    "publishedAt": "2025-03-10T13:40:43.000Z",
    "title": "Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning",
    "summary": "Training models to effectively use test-time compute is crucial for improving\nthe reasoning performance of LLMs. Current methods mostly do so via fine-tuning\non search traces or running RL with 0/1 outcome reward, but do these approaches\nefficiently utilize test-time compute? Would these approaches continue to scale\nas the budget improves? In this paper, we try to answer these questions. We\nformalize the problem of optimizing test-time compute as a meta-reinforcement\nlearning (RL) problem, which provides a principled perspective on spending\ntest-time compute. This perspective enables us to view the long output stream\nfrom the LLM as consisting of several episodes run at test time and leads us to\nuse a notion of cumulative regret over output tokens as a way to measure the\nefficacy of test-time compute. Akin to how RL algorithms can best tradeoff\nexploration and exploitation over training, minimizing cumulative regret would\nalso provide the best balance between exploration and exploitation in the token\nstream. While we show that state-of-the-art models do not minimize regret, one\ncan do so by maximizing a dense reward bonus in conjunction with the outcome\n0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in\nthe output stream, quantified by the change in the likelihood of eventual\nsuccess. Using these insights, we develop Meta Reinforcement Fine-Tuning, or\nMRT, a new class of fine-tuning methods for optimizing test-time compute. MRT\nleads to a 2-3x relative gain in performance and roughly a 1.5x gain in token\nefficiency for math reasoning compared to outcome-reward RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07572.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08588",
      "authors": [
        {
          "_id": "67d125362da8f91f8ef0412f",
          "user": {
            "_id": "6190ab805ca89a28e9f66873",
            "avatarUrl": "/avatars/a677a8401360be473895494e5fb267bb.svg",
            "isPro": false,
            "fullname": "Xin Xu",
            "user": "XinXuNLPer",
            "type": "user"
          },
          "name": "Xin Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:07.130Z",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04130",
          "name": "Wei Xu",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04131",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04132",
          "name": "Julian McAuley",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:25:36.000Z",
      "title": "BiasEdit: Método para modificar modelos de lenguaje entrenados para eliminar sesgos",
      "summary": "En los estudios anteriores, se ha confirmado claramente que modelos de lenguaje muestran sesgos de tipo escalar en sus unidades de procesamiento. En la estrategia actual de sesgos de dispositivo en las unidades de procesamiento, la reentrenamiento de modelos utilizando factores de configuración, la proyección de representaciones y la generación no son efectivas para eliminar los sesgos o cambiar directamente las representaciones internas sesgadas del modelo. Para enfrentar estas problemas, proponemos un método eficiente para editar modelos que elimina sesgos de tipo escalar llamado BiasEdit. BiasEdit utiliza una red editora que ejecuta ediciones de modelo a través de una red ligera, generando actualizaciones de parámetros para eliminar el sesgo. BiasEdit mantiene la capacidad de modelar lenguaje mientras edita parcialmente algunos parámetros del modelo de lenguaje utilizando la pérdida de sesgo de dispositivo. En los experimentos con StereoSet y Crows-Pairs, demostramos la eficacia, eficiencia y robustez de la eliminación de sesgos, y que la edición de sesgos en el modelo de lenguaje tiene un impacto muy reducido en su capacidad general. Además, realizamos entrenamientos de sesgos para explorar los sesgos de cada módulo y investigar el impacto de la edición de sesgos en diferentes componentes del modelo de lenguaje.",
      "upvotes": 5,
      "discussionId": "67d125382da8f91f8ef041d0",
      "githubRepo": "https://github.com/zjunlp/BiasEdit",
      "ai_keywords": [
        "language models",
        "counterfactual data",
        "representation projection",
        "prompting",
        "BiasEdit",
        "parameter updates",
        "debiasing loss",
        "retention loss",
        "StereoSet",
        "Crows-Pairs",
        "language modeling abilities",
        "bias tracing"
      ]
    },
    "publishedAt": "2025-03-11T12:25:36.000Z",
    "title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing",
    "summary": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08588.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08689",
      "authors": [
        {
          "_id": "67d0f759cb5bf46c22ac8af1",
          "name": "Yongdong Luo",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af2",
          "name": "Wang Chen",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af3",
          "name": "Xiawu Zheng",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af4",
          "name": "Weizhong Huang",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af5",
          "name": "Shukang Yin",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af6",
          "name": "Haojia Lin",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af7",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af8",
          "name": "Jinfa Huang",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af9",
          "name": "Jiayi Ji",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8afa",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8afb",
          "name": "Rongrong Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:57.000Z",
      "title": "Para lograr una estructura de distribución de tokens para la consulta, se basa en la comprensión de largos videos y se realiza una formación de contexto.",
      "summary": "El desarrollo reciente del entendimiento de largas vídeos se realiza a través de la reducción de los tokens visuales, lo que reduce la información no necesaria visual. Sin embargo, los métodos actuales utilizan la reducción de los tokens de respuesta posterior en las capas de decodificador, pero dejan de lado la relación significativa entre los tokens visuales y las consultas (query) en el nivel de entrada. En este artículo, proponemos un nuevo módulo de entrenamiento con restricciones lógicas anteriores llamado \"QuoTA\" para expandir los grandes modelos de lenguaje de vídeo (LVLMs). Este módulo expande a los LVLMs utilizando la asignación de tokens visuales basada en la evaluación del nivel de importancia de los frames relacionados con la consulta. La selección de tokens es crucial, y el proceso optimiza la utilización del bucket de tokens para la visual procesamiento, conservando contenido significativo. Específicamente, (i) QuoTA asigna puntuaciones de importancia de frames basadas en la relación con la consulta, permitiendo una asignación de tokens visuales antes de la interacción cruzada de modos en la capa de decodificador, (ii) separa la consulta en una inferencia de \"Chain-of-Thoughts\" para evaluar de manera más precisa la importancia de los frames usando el LVLM, y (iii) proporciona funciones de puerto y tubo para expandir los LVLMs. Los resultados experimentales expandidos, combinados con LLaVA-Video-7B, muestran un aumento promedio del rendimiento en 6 benchmarks (incluyendo Video-MME y MLVU) del 3.2%, utilizando el mismo bucket de tokens visuales. El código está disponible en https://github.com/MAC-AutoML/QuoTA.",
      "upvotes": 4,
      "discussionId": "67d0f75bcb5bf46c22ac8b70",
      "githubRepo": "https://github.com/MAC-AutoML/QuoTA",
      "ai_keywords": [
        "QuoTA",
        "ante-hoc",
        "training-free",
        "modular",
        "long video understanding",
        "visual token pruning",
        "attention distribution",
        "decoder layers",
        "input-level semantic correlation",
        "visual tokens",
        "instructions",
        "query",
        "frame-level importance assessment",
        "task-specific requirements",
        "token budget utilization",
        "semantically relevant content",
        "Chain-of-Thoughts reasoning",
        "cross-modal interactions",
        "plug-and-play functionality",
        "LLaVA-Video-7B",
        "Video-MME",
        "MLVU"
      ]
    },
    "publishedAt": "2025-03-11T13:59:57.000Z",
    "title": "QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long\n  Video Comprehension",
    "summary": "Recent advances in long video understanding typically mitigate visual\nredundancy through visual token pruning based on attention distribution.\nHowever, while existing methods employ post-hoc low-response token pruning in\ndecoder layers, they overlook the input-level semantic correlation between\nvisual tokens and instructions (query). In this paper, we propose QuoTA, an\nante-hoc training-free modular that extends existing large video-language\nmodels (LVLMs) for visual token assignment based on query-oriented frame-level\nimportance assessment. The query-oriented token selection is crucial as it\naligns visual processing with task-specific requirements, optimizing token\nbudget utilization while preserving semantically relevant content.\nSpecifically, (i) QuoTA strategically allocates frame-level importance scores\nbased on query relevance, enabling one-time visual token assignment before\ncross-modal interactions in decoder layers, (ii) we decouple the query through\nChain-of-Thoughts reasoning to facilitate more precise LVLM-based frame\nimportance scoring, and (iii) QuoTA offers a plug-and-play functionality that\nextends to existing LVLMs. Extensive experimental results demonstrate that\nimplementing QuoTA with LLaVA-Video-7B yields an average performance\nimprovement of 3.2% across six benchmarks (including Video-MME and MLVU) while\noperating within an identical visual token budget as the baseline. Codes are\nopen-sourced at https://github.com/MAC-AutoML/QuoTA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08689.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08685",
      "authors": [
        {
          "_id": "67d0f7032eaba9be7bf76e0e",
          "user": {
            "_id": "63483629ac5172169929da0e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665676793089-noauth.jpeg",
            "isPro": false,
            "fullname": "Xin Wen",
            "user": "xwen99",
            "type": "user"
          },
          "name": "Xin Wen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:39:00.455Z",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e0f",
          "user": {
            "_id": "62dcd71075e9787ec5aa41ba",
            "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
            "isPro": true,
            "fullname": "Bingchen Zhao",
            "user": "tennant",
            "type": "user"
          },
          "name": "Bingchen Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:56.945Z",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e10",
          "name": "Ismail Elezi",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e11",
          "name": "Jiankang Deng",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e12",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:41.000Z",
      "title": "\"Análisis de Componentes Principales\" permite que las imágenes tengan un nuevo lenguaje.",
      "summary": "Presentamos un nuevo marco de trabajo de tokenización visual. Este marco de trabajo inserta una estructura similar a la PCA provable en el espacio de tokens potenciales. La tokenización visual existente se centra principalmente en optimizar la precisión de reconstrucción, pero frecuentemente ignora las características estructurales del espacio potencial, lo cual es un elemento importante en la interpretación y en las tareas posteriores. Nuestro método genera una secuencia de tokens causales unidimensionales para las imágenes. Cada token continuo tiene una variabilidad explicada decreciente matemáticamente garantizada, proporcionando información no repetitiva. Esta estructura es similar a la análisis de componentes principales. Esta restricción estructural garantiza que los tokens extraen primero las características visuales más salientes, y cada siguiente token agrega información complementaria de manera gradual decreciente. Además, hemos identificado y resolvido el efecto de la combinación espectral semántica, donde el contenido semántico de alto nivel y los detalles espectrales de bajo nivel se mezclan de manera inesperada en los tokens. Las experimentaciones muestran que nuestro enfoque logra un rendimiento de reconstrucción superior al actual y mejora la interpretabilidad para que se ajuste mejor a el sistema visual humano. Además, los modelos automáticos de regresión que se entrenan y inferen utilizando nuestra secuencia de tokens alcanzan el rendimiento más alto actual pero requieren menos tokens.",
      "upvotes": 4,
      "discussionId": "67d0f7052eaba9be7bf76eac",
      "projectPage": "https://visual-gen.github.io/semanticist/",
      "githubRepo": "https://github.com/visual-gen/semanticist",
      "ai_keywords": [
        "visual tokenization framework",
        "PCA-like structure",
        "latent token space",
        "reconstruction fidelity",
        "structural properties",
        "interpretabiliy",
        "1D causal token sequence",
        "explained variance",
        "principal component analysis",
        "salient visual features",
        "semantic-spectrum coupling effect",
        "diffusion decoder",
        "reconstruction performance",
        "human vision system",
        "auto-regressive models",
        "state-of-the-art methods"
      ]
    },
    "publishedAt": "2025-03-11T13:59:41.000Z",
    "title": "\"Principal Components\" Enable A New Language of Images",
    "summary": "We introduce a novel visual tokenization framework that embeds a provable\nPCA-like structure into the latent token space. While existing visual\ntokenizers primarily optimize for reconstruction fidelity, they often neglect\nthe structural properties of the latent space -- a critical factor for both\ninterpretability and downstream tasks. Our method generates a 1D causal token\nsequence for images, where each successive token contributes non-overlapping\ninformation with mathematically guaranteed decreasing explained variance,\nanalogous to principal component analysis. This structural constraint ensures\nthe tokenizer extracts the most salient visual features first, with each\nsubsequent token adding diminishing yet complementary information.\nAdditionally, we identified and resolved a semantic-spectrum coupling effect\nthat causes the unwanted entanglement of high-level semantic content and\nlow-level spectral details in the tokens by leveraging a diffusion decoder.\nExperiments demonstrate that our approach achieves state-of-the-art\nreconstruction performance and enables better interpretability to align with\nthe human vision system. Moreover, auto-regressive models trained on our token\nsequences achieve performance comparable to current state-of-the-art methods\nwhile requiring fewer tokens for training and inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08685.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07699",
      "authors": [
        {
          "_id": "67d114912264403cbf39d0ba",
          "name": "Huiyang Shao",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bb",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bc",
          "name": "Yuhong Yang",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bd",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0be",
          "name": "Xing Wang",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bf",
          "name": "Xuefeng Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:20:52.000Z",
      "title": "RayFlow: Reconoce la información de instancias para acelerar el procesamiento de tráfico de forma adaptativa",
      "summary": "El modelo de difusión ha tenido éxito en diversas áreas de manera impresionante. Sin embargo, la lentitud de la generación sigue siendo un problema importante. Los métodos de aceleración actuales pierden la calidad de las muestras o introducen complejidades en el entrenamiento. En este sentido, se propone RayFlow, un nuevo marco de trabajo de difusión. RayFlow guia cada muestra a través de diferentes rutas correspondientes a distribuciones objetivo únicas para cada instancia, lo que mantiene la diversidad y la estabilidad de la generación mientras minimiza los pasos de muestreo. Además, se presenta el método de muestreo Time Sampler, que se centra en mejorar la eficiencia del entrenamiento al enfocarse en los momentos temporales más importantes. Los experimentos ampliados muestran que RayFlow mejora la velocidad, la control y la eficiencia del entrenamiento comparado con los métodos de aceleración existentes, demostrando su capacidad para generar imágenes de alta calidad.",
      "upvotes": 3,
      "discussionId": "67d114922264403cbf39d0f8",
      "ai_keywords": [
        "diffusion models",
        "generation speed",
        "RayFlow",
        "instance-specific target distribution",
        "sampling steps",
        "generation diversity",
        "stability",
        "Time Sampler",
        "importance sampling",
        "training efficiency",
        "high-quality images"
      ]
    },
    "publishedAt": "2025-03-10T13:20:52.000Z",
    "title": "RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow\n  Trajectories",
    "summary": "Diffusion models have achieved remarkable success across various domains.\nHowever, their slow generation speed remains a critical challenge. Existing\nacceleration methods, while aiming to reduce steps, often compromise sample\nquality, controllability, or introduce training complexities. Therefore, we\npropose RayFlow, a novel diffusion framework that addresses these limitations.\nUnlike previous methods, RayFlow guides each sample along a unique path towards\nan instance-specific target distribution. This method minimizes sampling steps\nwhile preserving generation diversity and stability. Furthermore, we introduce\nTime Sampler, an importance sampling technique to enhance training efficiency\nby focusing on crucial timesteps. Extensive experiments demonstrate RayFlow's\nsuperiority in generating high-quality images with improved speed, control, and\ntraining efficiency compared to existing acceleration techniques.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07699.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18858",
      "authors": [
        {
          "_id": "67d1080b2264403cbf36b0ad",
          "name": "Jingtao Zhan",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0ae",
          "name": "Jiahao Zhao",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0af",
          "name": "Jiayu Li",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b0",
          "name": "Yiqun Liu",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b1",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b2",
          "name": "Qingyao Ai",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b3",
          "name": "Jiaxin Mao",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b4",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b5",
          "name": "Min Zhang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b6",
          "name": "Shaoping Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T05:59:45.000Z",
      "title": "Evaluación de Inteligencia a través de Pruebas y Errores",
      "summary": "Inteligencia es una característica importante que limita errores y busca soluciones. Basándonos en este concepto, proponemos un juego de supervivencia (Survival Game) para evaluar la inteligencia basándonos en la cantidad de fallos. Una menor cantidad de fallos indica una mayor inteligencia. La expectativa y la varianza de los fallos están limitadas, lo que refleja la capacidad de resolver problemas continuamente y se define como el nivel autónomo (Autonomous Level). Usando este juego, evaluamos estructuralmente los sistemas AI actuales. Resulta que los sistemas AI alcanzan el nivel autónomo en tareas simples, pero están lejos de lograrlo en tareas complejas como la visión, la búsqueda, el reconocimiento de imágenes, el lenguaje y más. Para alcanzar el nivel autónomo en tareas generales, se predice que se necesiten 10^26 parámetros. Esto implica que el número de GPU H100 necesarios es excesivamente grande, y el costo total es 10^7 veces mayor que el valor de mercado de Apple Inc. Según la ley de Moore, se necesitarían 70 años para soportar este escala de parámetros. Estos altos costos resaltan la complejidad de las tareas humanas y la insuficiencia de las tecnologías AI actuales. Para profundizar en estos fenómenos, realizamos un análisis teórico y experimentos del juego de supervivencia. Nuestros hallazgos muestran que las tareas humanas son extremadamente complejas, y se requieren modelos de gran escala. Esta complejidad es evidente en la necesidad de modelos de gran escala, lo que implica costos astronómicos y una falta de capacidad de resolución de problemas complejos en las tecnologías actuales.",
      "upvotes": 3,
      "discussionId": "67d108112264403cbf36b1e9",
      "githubRepo": "https://github.com/jingtaozhan/IntelligenceTest"
    },
    "publishedAt": "2025-02-26T00:59:45.000Z",
    "title": "Evaluating Intelligence via Trial and Error",
    "summary": "Intelligence is a crucial trait for species to find solutions within a\nlimited number of trial-and-error attempts. Building on this idea, we introduce\nSurvival Game as a framework to evaluate intelligence based on the number of\nfailed attempts in a trial-and-error process. Fewer failures indicate higher\nintelligence. When the expectation and variance of failure counts are both\nfinite, it signals the ability to consistently find solutions to new\nchallenges, which we define as the Autonomous Level of intelligence. Using\nSurvival Game, we comprehensively evaluate existing AI systems. Our results\nshow that while AI systems achieve the Autonomous Level in simple tasks, they\nare still far from it in more complex tasks, such as vision, search,\nrecommendation, and language. While scaling current AI technologies might help,\nthis would come at an astronomical cost. Projections suggest that achieving the\nAutonomous Level for general tasks would require 10^{26} parameters. To put\nthis into perspective, loading such a massive model requires so many H100 GPUs\nthat their total value is 10^{7} times that of Apple Inc.'s market value.\nEven with Moore's Law, supporting such a parameter scale would take 70 years.\nThis staggering cost highlights the complexity of human tasks and the\ninadequacies of current AI technologies. To further investigate this\nphenomenon, we conduct a theoretical analysis of Survival Game and its\nexperimental results. Our findings suggest that human tasks possess a\ncriticality property. As a result, Autonomous Level requires a deep\nunderstanding of the task's underlying mechanisms. Current AI systems, however,\ndo not fully grasp these mechanisms and instead rely on superficial mimicry,\nmaking it difficult for them to reach an autonomous level. We believe Survival\nGame can not only guide the future development of AI but also offer profound\ninsights into human intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18858.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07639",
      "authors": [
        {
          "_id": "67d0e3ede3afecf451915d0a",
          "name": "Xingyi Yang",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0b",
          "name": "Constantin Venhoff",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0c",
          "name": "Ashkan Khakzar",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0d",
          "name": "Christian Schroeder de Witt",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0e",
          "name": "Puneet K. Dokania",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0f",
          "name": "Adel Bibi",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d10",
          "name": "Philip Torr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T17:40:54.000Z",
      "title": "Mixture of Experts Made Intrinsically Interpretable",
      "summary": "El neuron representa múltiples significados en modelos de lenguaje de gran escala y frecuentemente codifica conceptos irrelevantes simultáneamente, ocultando su interpretabilidad. En lugar de depender de métodos de postprocesado, proponemos un modelo de lenguaje de Expertos Mixto (MoE) que tiene propias interpretabilidad. Nuestro enfoque se basa en la observación de que redes amplias con actividades raras en el modelo de lenguaje son fáciles de detectar causas interpretativas. Sin embargo, entrenar estas grandes redes raras directamente es computacionalmente impracticable. La arquitectura MoE activa a algunos expertos para procesar entradas de manera escalable, lo cual coincide con nuestro objetivo de interpretabilidad. En MoE-X, reemplazamos las capas MoE con grandes MLPs raras equivalentes, estableciendo esta conexión. Esta aproximación permite una escalabilidad eficiente de la dimensión de entrada y mantiene la raridad. Además, para mejorar la interpretabilidad, forzamos actividades raras en cada experto, redesignamos la estructura de raíces y priorizamos a los expertos con la actividad más rara. Esto asegura que solo se procesen características claras. MoE-X ha sido evaluado en tareas de ajedrez y procesamiento de lenguaje natural, demostrando un rendimiento comparable a modelos densos y un gran aumento en la interpretabilidad. Comparado con GPT-2, MoE-X es más permutativa y su interpretabilidad es superior a los enfoques basados en codificadores autóctonos raros (SAE).",
      "upvotes": 2,
      "discussionId": "67d0e3f0e3afecf451915dfa",
      "ai_keywords": [
        "polysemanticity",
        "Mixture-of-Experts (MoE)",
        "interpretable",
        "sparse activations",
        "sparsity",
        "sparse networks",
        "hidden size",
        "sparse activation",
        "routing mechanism",
        "salient features",
        "perplexity",
        "GPT-2",
        "sparse autoencoder (SAE)"
      ]
    },
    "publishedAt": "2025-03-05T12:40:54.000Z",
    "title": "Mixture of Experts Made Intrinsically Interpretable",
    "summary": "Neurons in large language models often exhibit polysemanticity,\nsimultaneously encoding multiple unrelated concepts and obscuring\ninterpretability. Instead of relying on post-hoc methods, we present\nMoE-X, a Mixture-of-Experts (MoE) language model designed to be\nintrinsically interpretable. Our approach is motivated by the\nobservation that, in language models, wider networks with sparse activations\nare more likely to capture interpretable factors. However, directly training\nsuch large sparse networks is computationally prohibitive. MoE architectures\noffer a scalable alternative by activating only a subset of experts for any\ngiven input, inherently aligning with interpretability objectives. In MoE-X, we\nestablish this connection by rewriting the MoE layer as an equivalent sparse,\nlarge MLP. This approach enables efficient scaling of the hidden size while\nmaintaining sparsity. To further enhance interpretability, we enforce sparse\nactivation within each expert and redesign the routing mechanism to prioritize\nexperts with the highest activation sparsity. These designs ensure that only\nthe most salient features are routed and processed by the experts. We evaluate\nMoE-X on chess and natural language tasks, showing that it achieves performance\ncomparable to dense models while significantly improving interpretability.\nMoE-X achieves a perplexity better than GPT-2, with interpretability surpassing\neven sparse autoencoder (SAE)-based approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07639.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08507",
      "authors": [
        {
          "_id": "67d15293e3afecf451aceab7",
          "name": "Qing Jiang",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceab8",
          "name": "Lin Wu",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceab9",
          "name": "Zhaoyang Zeng",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceaba",
          "name": "Tianhe Ren",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabb",
          "name": "Yuda Xiong",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabc",
          "name": "Yihao Chen",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabd",
          "name": "Qin Liu",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabe",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T14:57:14.000Z",
      "title": "「Sin importancia de quién」",
      "summary": "El ser humano es el más importante participante en la visión por computadora, y el trabajo de detectar a ciertas personas definido a través de la capacidad de entender lenguaje natural tiene valor práctico. Sin embargo, los modelos actuales a menudo no logran alcanzar la posibilidad de uso real, y los benchmarks actuales impiden su desarrollo al centrarse en un solo criterio. En este artículo, se revisaron tres aspectos cruciales de este trabajo: la definición del trabajo, el diseño de los conjuntos de datos y la arquitectura del modelo. Primero, se identificaron las cinco características posibles de entidades y las tres propiedades del trabajo, y luego se estableció como objetivo superar estos problemas y mejorar la simulación de aplicaciones reales mediante el diseño de un nuevo conjunto de datos llamado HumanRef. Desde el punto de vista de la diseño del modelo, se integraron modelos de lenguaje y un marco de trabajo para la detección de objetos para construir un modelo de referencia fuerte llamado RexSeek. Los resultados de los experimentos muestran que los modelos más avanzados que se comportan bien en generales de benchmark (RefCOCO/+/g) fallan en detectar muchas personas en HumanRef. Por otro lado, RexSeek demostró un excelente rendimiento en criterios humanos, al mismo tiempo que puede ampliarse efectivamente a objetos generales y aplicarse ampliamente a diferentes tareas de observación. El código está disponible en https://github.com/IDEA-Research/RexSeek.",
      "upvotes": 1,
      "discussionId": "67d15294e3afecf451aceb29",
      "ai_keywords": [
        "referable entities",
        "multimodal large language model",
        "object detection framework",
        "HumanRef",
        "RexSeek",
        "RefCOCO/+/g"
      ]
    },
    "publishedAt": "2025-03-11T10:57:14.000Z",
    "title": "Referring to Any Person",
    "summary": "Humans are undoubtedly the most important participants in computer vision,\nand the ability to detect any individual given a natural language description,\na task we define as referring to any person, holds substantial practical value.\nHowever, we find that existing models generally fail to achieve real-world\nusability, and current benchmarks are limited by their focus on one-to-one\nreferring, that hinder progress in this area. In this work, we revisit this\ntask from three critical perspectives: task definition, dataset design, and\nmodel architecture. We first identify five aspects of referable entities and\nthree distinctive characteristics of this task. Next, we introduce HumanRef, a\nnovel dataset designed to tackle these challenges and better reflect real-world\napplications. From a model design perspective, we integrate a multimodal large\nlanguage model with an object detection framework, constructing a robust\nreferring model named RexSeek. Experimental results reveal that\nstate-of-the-art models, which perform well on commonly used benchmarks like\nRefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple\nindividuals. In contrast, RexSeek not only excels in human referring but also\ngeneralizes effectively to common object referring, making it broadly\napplicable across various perception tasks. Code is available at\nhttps://github.com/IDEA-Research/RexSeek",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08507.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08478",
      "authors": [
        {
          "_id": "67d12d0b44be28339053b965",
          "user": {
            "_id": "64a3eb280111d5ff6c4849fd",
            "avatarUrl": "/avatars/3a9000393b8d200418bae5fe7d902e4d.svg",
            "isPro": false,
            "fullname": "Han-Wei Kung",
            "user": "hkung",
            "type": "user"
          },
          "name": "Han-Wei Kung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:50:08.946Z",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b966",
          "name": "Tuomas Varanka",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b967",
          "name": "Terence Sim",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b968",
          "name": "Nicu Sebe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T14:29:37.000Z",
      "title": "NullFace: Aproximación sin entrenamiento a la cara localizada",
      "summary": "La cantidad de cámaras continúa aumentando, y con ello, se incrementan los preocupaciones de privacidad en la era digital actual. Los métodos anti-identificación existentes pueden ocultar la información personal de las personas, pero mantener la utilidad de las imágenes es difícil. Este artículo presenta una metodología que permite el anímicación facial mientras se mantiene la propiedad de los características no relacionadas, aplicada sin entrenamiento. Nuestro enfoque utiliza un modelo expandido desde imágenes de texto pre-entrenado, sin necesidad de optimización o entrenamiento adicional. Primero, se reconstruye el ruido inicial en las imágenes ingresadas. Luego, se reduce el ruido oculto para mantener las características del carácter, y se utiliza el ruido oculto modificado del rostro para reducir el ruido oculto. Nuestro enfoque permite la anímicación selectiva de la región facial, y los usuarios pueden controlar si anímican o mantienen la región facial. Comparado con los métodos más recientes, nuestro enfoque supera en anímicación, mantenimiento de características del carácter y calidad de la imagen. Su flexibilidad, robustez y características prácticas lo hacen adecuado para aplicaciones realistas. El código y los datos se pueden encontrar en https://github.com/hanweikung/nullface.",
      "upvotes": 1,
      "discussionId": "67d12d1044be28339053baab",
      "ai_keywords": [
        "text-to-image diffusion model",
        "identity-conditioned diffusion",
        "identity embeddings",
        "localized anonymization"
      ]
    },
    "publishedAt": "2025-03-11T10:29:37.000Z",
    "title": "NullFace: Training-Free Localized Face Anonymization",
    "summary": "Privacy concerns around ever increasing number of cameras are increasing in\ntoday's digital age. Although existing anonymization methods are able to\nobscure identity information, they often struggle to preserve the utility of\nthe images. In this work, we introduce a training-free method for face\nanonymization that preserves key non-identity-related attributes. Our approach\nutilizes a pre-trained text-to-image diffusion model without requiring\noptimization or training. It begins by inverting the input image to recover its\ninitial noise. The noise is then denoised through an identity-conditioned\ndiffusion process, where modified identity embeddings ensure the anonymized\nface is distinct from the original identity. Our approach also supports\nlocalized anonymization, giving users control over which facial regions are\nanonymized or kept intact. Comprehensive evaluations against state-of-the-art\nmethods show our approach excels in anonymization, attribute preservation, and\nimage quality. Its flexibility, robustness, and practicality make it\nwell-suited for real-world applications. Code and data can be found at\nhttps://github.com/hanweikung/nullface .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08478.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08307",
      "authors": [
        {
          "_id": "67d140378cb4592900a1a75e",
          "user": {
            "_id": "66895b3d41fcf83c026b5dca",
            "avatarUrl": "/avatars/f1104041ee3445024f05d5d0b4d1550b.svg",
            "isPro": false,
            "fullname": "Alex Ergasti",
            "user": "MaverickAlex",
            "type": "user"
          },
          "name": "Alex Ergasti",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:03.978Z",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a75f",
          "name": "Giuseppe Gabriele Tarollo",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a760",
          "name": "Filippo Botti",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a761",
          "name": "Tomaso Fontanini",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a762",
          "name": "Claudio Ferrari",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a763",
          "name": "Massimo Bertozzi",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a764",
          "name": "Andrea Prati",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T11:18:47.000Z",
      "title": "^RFLAV: Flujo de flujo de enrollamiento para la generación de videos de voz infinita",
      "summary": "La generación de AV sigue siendo un problema importante en la generación de IA, complicado principalmente por tres factores cruciales: la calidad de los ejemplos generados, la sincronización motivada por la motivación y la coherencia temporal, y la necesidad de que el tracto sonoro coincida o sea opuesto al datos visuales, o coincida sin limitaciones durante largos tiempos de video. En este artículo, se presenta una nueva arquitectura basada en transformers para resolver todos estos problemas importantes en la generación de AV. Se revisarán tres módulos de interacción cruzados diferentes, y se evaluará que el módulo ligero de funciones temporales es la forma más efectiva y eficiente en términos de cálculo para la configuración de los módulos sonoro y visual. Los resultados de los experimentos muestran que esta arquitectura supera a los modelos más avanzados actuales en tareas de generación de AV multimódulo. Los códigos y los puntos de chequeo están disponibles en https://github.com/ErgastiAlex/R-FLAV.",
      "upvotes": 1,
      "discussionId": "67d1403b8cb4592900a1a868",
      "githubRepo": "https://github.com/ErgastiAlex/R-FLAV",
      "ai_keywords": [
        "transformer-based architecture",
        "cross modality interaction modules",
        "lightweight temporal fusion module",
        "audio and visual modalities",
        "multimodal AV generation tasks"
      ]
    },
    "publishedAt": "2025-03-11T07:18:47.000Z",
    "title": "^RFLAV: Rolling Flow matching for infinite Audio Video generation",
    "summary": "Joint audio-video (AV) generation is still a significant challenge in\ngenerative AI, primarily due to three critical requirements: quality of the\ngenerated samples, seamless multimodal synchronization and temporal coherence,\nwith audio tracks that match the visual data and vice versa, and limitless\nvideo duration. In this paper, we present , a novel transformer-based\narchitecture that addresses all the key challenges of AV generation. We explore\nthree distinct cross modality interaction modules, with our lightweight\ntemporal fusion module emerging as the most effective and computationally\nefficient approach for aligning audio and visual modalities. Our experimental\nresults demonstrate that  outperforms existing state-of-the-art models\nin multimodal AV generation tasks. Our code and checkpoints are available at\nhttps://github.com/ErgastiAlex/R-FLAV.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08307.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08102",
      "authors": [
        {
          "_id": "67d12c32428a3d8d5281f310",
          "name": "Jiale Wei",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f311",
          "name": "Xiang Ying",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f312",
          "name": "Tao Gao",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f313",
          "name": "Felix Tao",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f314",
          "name": "Jingbo Shang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T07:05:52.000Z",
      "title": "AI-native Memory 2.0: Segundo Momento",
      "summary": "La humanidad intercambia recuerdos personales a través de la interacción con el mundo exterior. Esta interacción incluye la interacción con otras personas, sitios web, aplicaciones y futuros agentes de IA. Una parte de esta interacción es que los usuarios deben proporcionar la misma información repetidamente en diferentes contextos, lo que puede hacer que la información se vuelva compleja debido a datos innecesarios. Las soluciones actuales, como la información de autenticación guardada en navegadores, estructuras de autocompletado o sistemas de autenticación integrados, ayudan a reducir la cantidad de información innecesaria al almacenar y extraer datos del usuario. El desarrollo de modelos de lenguaje grandes (LLM) ofrece una oportunidad para reestructurar la gestión de la memoria en el paradigma de la base de conocimiento de la IA, y SECOND ME aprovecha esta oportunidad para promover un enfoque más sistemático y inteligente en la gestión de la memoria. SECOND ME es un sistema de carga de memoria inteligente y continuo que mantiene, organiza y utiliza conocimiento dinámicamente en relación con el conocimiento del usuario. Actúa como un mediador en las interacciones del usuario, genera respuestas automáticamente en contextos adecuados, prepara información necesaria y promueve comunicación sin intermediarios con sistemas externos, reduciendo significativamente el carga cognitiva y el fricción en las interacciones. En comparación con las soluciones tradicionales de almacenamiento de memoria, SECOND ME utiliza la parametrización de memoria basada en LLM para permitir una organización estructurada, conexiones de contexto y búsqueda de conocimiento adaptativo, lo que promueve un enfoque más sistemático y inteligente en la gestión de la memoria. Como un agente personal de IA, SECOND ME se integra más en la ecosistema digital y se representa como un paso importante para fortalecer la interacción entre la humanidad y el mundo. Estamos en proceso de abrir completamente el sistema de depósito distribuido localizable en GitHub: https://github.com/Mindverse/Second-Me.",
      "upvotes": 1,
      "discussionId": "67d12c33428a3d8d5281f346",
      "ai_keywords": [
        "large language models (LLMs)",
        "intelligent, persistent memory offload system",
        "context-aware responses",
        "structured organization",
        "contextual reasoning",
        "adaptive knowledge retrieval",
        "self-optimizing memory systems"
      ]
    },
    "publishedAt": "2025-03-11T03:05:52.000Z",
    "title": "AI-native Memory 2.0: Second Me",
    "summary": "Human interaction with the external world fundamentally involves the exchange\nof personal memory, whether with other individuals, websites, applications, or,\nin the future, AI agents. A significant portion of this interaction is\nredundant, requiring users to repeatedly provide the same information across\ndifferent contexts. Existing solutions, such as browser-stored credentials,\nautofill mechanisms, and unified authentication systems, have aimed to mitigate\nthis redundancy by serving as intermediaries that store and retrieve commonly\nused user data. The advent of large language models (LLMs) presents an\nopportunity to redefine memory management through an AI-native paradigm: SECOND\nME. SECOND ME acts as an intelligent, persistent memory offload system that\nretains, organizes, and dynamically utilizes user-specific knowledge. By\nserving as an intermediary in user interactions, it can autonomously generate\ncontext-aware responses, prefill required information, and facilitate seamless\ncommunication with external systems, significantly reducing cognitive load and\ninteraction friction. Unlike traditional memory storage solutions, SECOND ME\nextends beyond static data retention by leveraging LLM-based memory\nparameterization. This enables structured organization, contextual reasoning,\nand adaptive knowledge retrieval, facilitating a more systematic and\nintelligent approach to memory management. As AI-driven personal agents like\nSECOND ME become increasingly integrated into digital ecosystems, SECOND ME\nfurther represents a critical step toward augmenting human-world interaction\nwith persistent, contextually aware, and self-optimizing memory systems. We\nhave open-sourced the fully localizable deployment system at GitHub:\nhttps://github.com/Mindverse/Second-Me.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08102.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06594",
      "authors": [
        {
          "_id": "67cfd77ff8ee57c14450221b",
          "user": {
            "_id": "6440b38d3e0374802e1acc5e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440b38d3e0374802e1acc5e/w-ZpW_9gCSHUeDKyGSeMt.jpeg",
            "isPro": false,
            "fullname": "luoyingfeng",
            "user": "luoyingfeng",
            "type": "user"
          },
          "name": "Yingfeng Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:42:33.649Z",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221c",
          "name": "Tong Zheng",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221d",
          "name": "Yongyu Mu",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221e",
          "name": "Bei Li",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221f",
          "name": "Qinghong Zhang",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502220",
          "name": "Yongqi Gao",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502221",
          "name": "Ziqiang Xu",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502222",
          "name": "Peinan Feng",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502223",
          "name": "Xiaoqian Liu",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502224",
          "name": "Tong Xiao",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502225",
          "name": "Jingbo Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T12:54:05.000Z",
      "title": "No solo decoder: modelos de lenguaje grandes pueden ser utilizados como encoders para la traducción automática.",
      "summary": "La área de la traducción automática basada en redes neuronales (NMT) ha experimentado cambios con la aparición de modelos de lenguaje grande (LLM). En los últimos debates sobre procesamiento del lenguaje natural (NLP), se ha centrado en usar solo modelos transformadores preentrenados para la traducción automática y la modelación de diversas tareas. En los modelos NMT de la semana pasada, la arquitectura estándar de encoder-decoder recibió menos atención. En este artículo, se revisa cómo conectar el mundo de los LLM y NMT para desarrollar modelos generales, eficientes y fáciles de optimizar. Se aplica un LLM como encoder en NMT, mientras que el decoder NMT se mantiene intacto. Además, se desarrollan métodos para mejorar la colaboración entre el LLM y el decoder NMT. Se construye un nuevo conjunto de datos que incluye diversas tareas para evaluar la capacidad de extensión de sistemas de traducción automática en múltiples tareas. En las evaluaciones de WMT y nuestro conjunto de datos, los resultados obtenidos mediante nuestro método son comparables o mejores en cuanto a calidad de traducción, con un aumento de velocidad de inferencia del 2.4 a 6.5 veces y una reducción del 75% en el uso de memoria por caché KV. Además, muestra una fuerte capacidad de extensión en varias tareas relacionadas con la traducción.",
      "upvotes": 1,
      "discussionId": "67cfd780f8ee57c144502268",
      "githubRepo": "https://github.com/NiuTrans/LaMaTE/",
      "ai_keywords": [
        "large language models (LLMs)",
        "neural machine translation (NMT)",
        "natural language processing (NLP)",
        "Transformer decoder",
        "encoder-decoder architectures",
        "pre-trained Transformer decoder",
        "LLMs",
        "NMT encoding",
        "NMT decoder",
        "KV cache"
      ]
    },
    "publishedAt": "2025-03-09T08:54:05.000Z",
    "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
    "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve 2.4 sim 6.5 times inference speedups and a 75% reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06594.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06492",
      "authors": [
        {
          "_id": "67cfe557ad91643b5cb7d2c6",
          "user": {
            "_id": "67cd327432668b04f4555270",
            "avatarUrl": "/avatars/15e2cef976cbe05c4c5858c88dccf4af.svg",
            "isPro": false,
            "fullname": "Yanling Wang",
            "user": "WYLing",
            "type": "user"
          },
          "name": "Yanling Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T16:09:28.071Z",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c7",
          "name": "Yihan Zhao",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c8",
          "name": "Xiaodong Chen",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c9",
          "name": "Shasha Guo",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2ca",
          "name": "Lixin Liu",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cb",
          "name": "Haoyang Li",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cc",
          "name": "Yong Xiao",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cd",
          "name": "Jing Zhang",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2ce",
          "name": "Qi Li",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cf",
          "name": "Ke Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T07:25:32.000Z",
      "title": "VisualSimpleQA: Normas de Evaluación para la Exploración de Hechos en Preguntas de Modelos de Visión-Lenguaje de Gran Escala",
      "summary": "Los modelos de lenguaje visual y lingüístico (LVLMs) han demostrado resultados impresionantes, pero aún existen muchos problemas en la generación de respuestas factuales frente a consultas de exploración de hechos. Los actuales marcos de evaluación de exploración de hechos multimodal se centran principalmente en la comparación entre el output del modelo y las respuestas reales, lo que limita la comprensión específica de su rendimiento. Para resolver esto, se presenta el marco de evaluación de exploración de hechos multimodal llamado VisualSimpleQA. Este marco tiene dos características destacadas: 1. Permite evaluar el flujo y el carga del decodificador de los modelos visuales y lingüísticos de LVLMs. 2. Define claramente la dificultad y guia la anotación humana, extrayendo un subconjunto llamado VisualSimpleQA-hard. Según los experimentos con 15 modelos, incluyendo modelos como GPT-4o, la precisión en la exploración de hechos multimodal de VisualSimpleQA es superior a 60%, y en VisualSimpleQA-hard a 30%. Además, la evaluación del carga del decodificador muestra una gran posibilidad de mejora en ambos módulos visual y lingüístico. El dataset está disponible en https://huggingface.co/datasets/WYLing/VisualSimpleQA.",
      "upvotes": 1,
      "discussionId": "67cfe55bad91643b5cb7d3fb",
      "ai_keywords": [
        "Large vision-language models",
        "fact-seeking question answering",
        "multimodal benchmarks",
        "visual modality",
        "linguistic modality",
        "VisualSimpleQA",
        "VisualSimpleQA-hard",
        "GPT-4",
        "multimodal fact-seeking QA",
        "decoupled evaluation"
      ]
    },
    "publishedAt": "2025-03-09T03:25:32.000Z",
    "title": "VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large\n  Vision-Language Models in Fact-Seeking Question Answering",
    "summary": "Large vision-language models (LVLMs) have demonstrated remarkable\nachievements, yet the generation of non-factual responses remains prevalent in\nfact-seeking question answering (QA). Current multimodal fact-seeking\nbenchmarks primarily focus on comparing model outputs to ground truth answers,\nproviding limited insights into the performance of modality-specific modules.\nTo bridge this gap, we introduce VisualSimpleQA, a multimodal fact-seeking\nbenchmark with two key features. First, it enables streamlined and decoupled\nevaluation of LVLMs in visual and linguistic modalities. Second, it\nincorporates well-defined difficulty criteria to guide human annotation and\nfacilitates the extraction of a challenging subset, VisualSimpleQA-hard.\nExperiments on 15 LVLMs show that even state-of-the-art models such as GPT-4o\nachieve merely 60%+ correctness in multimodal fact-seeking QA on VisualSimpleQA\nand 30%+ on VisualSimpleQA-hard. Furthermore, the decoupled evaluation across\nthese models highlights substantial opportunities for improvement in both\nvisual and linguistic modules. The dataset is available at\nhttps://huggingface.co/datasets/WYLing/VisualSimpleQA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06492.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05860",
      "authors": [
        {
          "_id": "67d0a239967ead9b5aff9883",
          "user": {
            "_id": "655a627aab0644b531a02eb1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/9rW6X1idfx1p5omky67D6.jpeg",
            "isPro": false,
            "fullname": "Roham Koohestani",
            "user": "RohamKoohestani",
            "type": "user"
          },
          "name": "Roham Koohestani",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:41:54.151Z",
          "hidden": false
        },
        {
          "_id": "67d0a239967ead9b5aff9884",
          "user": {
            "_id": "655213d1968a2554a5e8212a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3XM_b9imWk-pwoueJwAZB.jpeg",
            "isPro": false,
            "fullname": "Philippe de Bekker",
            "user": "philippedebekker",
            "type": "user"
          },
          "name": "Philippe de Bekker",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T20:51:06.669Z",
          "hidden": false
        },
        {
          "_id": "67d0a239967ead9b5aff9885",
          "name": "Maliheh Izadi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T18:44:32.000Z",
      "title": "Software Engineering AI Model Benchmarks: Review, Search Tools, and Extension Protocols",
      "summary": "El benchmark es crucial para evaluaciones consistentes y garantizar la reproducibilidad. La integración de la inteligencia artificial (IA) en el software engineering (SE) ha aumentado considerablemente el número de benchmarks para tareas como la generación de código o la corrección de errores. Sin embargo, este aumento ha asociado con varios problemas: 1. la dispersión de conocimientos sobre benchmarks para cada tarea, 2. la dificultad de seleccionar benchmarks relacionados, 3. la falta de estándares uniformes en el desarrollo de benchmarks, y 4. las limitaciones actuales de los benchmarks. En este artículo, se revisaron 173 estudios y se identificaron 204 benchmarks de IA para el SE. Estos benchmarks fueron clasificados, analizados y sus deficiencias prácticas reveladas. Basado en este estudio, se desarrolló \"BenchScout\", una herramienta de búsqueda semántica para encontrar contextos relacionados utilizando clustering automático. Se evaluaron la disponibilidad, efectividad y intuitividad de BenchScout con 22 participantes, obteniendo puntuaciones promedio de 4.5, 4.0 y 4.1 en cada criterio. Para mejorar los estándares de benchmarks, se propone BenchFrame, un método único. En un estudio de caso, se aplicó BenchFrame al benchmark HumanEval, resolviendo principalmente sus limitaciones. Como resultado, HumanEvalNext adquirió las siguientes características: 1. mejora en la corrección de errores, 2. mejora en la conversión de lenguaje, 3. extensión de la cobertura de pruebas, y 4. mejora en el nivel de dificultad. Posteriormente, se evaluaron HumanEval, HumanEvalPlus y HumanEvalNext con 10 modelos de lenguaje de código de vanguardia. En HumanEvalNext, las puntuaciones para HumanEval y HumanEvalPlus disminuyeron a 31.22% y 19.94%, respectivamente.",
      "upvotes": 1,
      "discussionId": "67d0a23a967ead9b5aff98da",
      "projectPage": "https://evalpro.online/",
      "githubRepo": "https://github.com/AISE-TUDelft/AI4SE-benchmarks",
      "ai_keywords": [
        "AI4SE (Artificial Intelligence in Software Engineering)",
        "BenchScout",
        "semantic search tool",
        "automated clustering",
        "BenchFrame",
        "HumanEval",
        "HumanEvalNext",
        "pass@1 score"
      ]
    },
    "publishedAt": "2025-03-07T13:44:32.000Z",
    "title": "Benchmarking AI Models in Software Engineering: A Review, Search Tool,\n  and Enhancement Protocol",
    "summary": "Benchmarks are essential for consistent evaluation and reproducibility. The\nintegration of Artificial Intelligence into Software Engineering (AI4SE) has\ngiven rise to numerous benchmarks for tasks such as code generation and bug\nfixing. However, this surge presents challenges: (1) scattered benchmark\nknowledge across tasks, (2) difficulty in selecting relevant benchmarks, (3)\nthe absence of a uniform standard for benchmark development, and (4)\nlimitations of existing benchmarks. In this paper, we review 173 studies and\nidentify 204 AI4SE benchmarks. We classify these benchmarks, analyze their\nlimitations, and expose gaps in practices. Based on our review, we created\nBenchScout, a semantic search tool to find relevant benchmarks, using automated\nclustering of the contexts from associated studies. We conducted a user study\nwith 22 participants to evaluate BenchScout's usability, effectiveness, and\nintuitiveness which resulted in average scores of 4.5, 4.0, and 4.1 out of 5.\nTo advance benchmarking standards, we propose BenchFrame, a unified method to\nenhance benchmark quality. As a case study, we applied BenchFrame to the\nHumanEval benchmark and addressed its main limitations. This led to\nHumanEvalNext, featuring (1) corrected errors, (2) improved language\nconversion, (3) expanded test coverage, and (4) increased difficulty. We then\nevaluated ten state-of-the-art code language models on HumanEval,\nHumanEvalPlus, and HumanEvalNext. On HumanEvalNext, models showed a pass@1\nscore reduction of 31.22% and 19.94% compared to HumanEval and HumanEvalPlus,\nrespectively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05860.png",
    "numComments": 1,
    "isAuthorParticipating": true
  }
]