[
  {
    "paper": {
      "id": "2504.13835",
      "authors": [
        {
          "_id": "6805b38355d3c792e1a9d0dd",
          "name": "Yicheng Chen",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0de",
          "name": "Yining Li",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0df",
          "name": "Kai Hu",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0e0",
          "name": "Zerun Ma",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0e1",
          "name": "Haochen Ye",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0e2",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T17:59:46.000Z",
      "submittedOnDailyAt": "2025-04-21T01:39:08.191Z",
      "title": "MIG: Maximización de beneficios informativos mediante la selección automática de datos en el espacio de significado para la optimización de instrucciones",
      "submittedOnDailyBy": {
        "_id": "649988726677f66c2b486392",
        "avatarUrl": "/avatars/b649a77370660e129726e29504daba34.svg",
        "isPro": false,
        "fullname": "Yining Li",
        "user": "ly015",
        "type": "user"
      },
      "summary": "La calidad y diversidad de los datos son esenciales para la construcción de un conjunto de datos de tunelamiento de instancias válidos. A medida que la utilización de conjuntos de datos de tunelamiento de instancias abiertos aumenta, es beneficioso elegir automáticamente subconjuntos de alta calidad y diversidad en muchos datos. Los métodos actuales generalmente priorizan la calidad de la instancia y mantienen la diversidad utilizando reglas heurísticas, pero esto puede resultar insuficiente para alcanzar resultados óptimos debido a la falta de visión global de la colección. Además, las reglas heurísticas suelen centrarse en la distancia en el espacio de codificación o en la clustering, lo que impide detectar con precisión el propósito de instancias complejas. Para complementar esto, se propone una serie de métodos para cuantificar la información de los conjuntos de datos. Este método construye grafos de etiquetas para modelar el espacio de notaciones y cuantifica la diversidad basándose en la distribución de la información dentro del grafo. Basándose en esta evaluación, se introducen métodos eficientes de muestreo de datos y se realizan muestreos que maximicen el ganancia de información en el espacio de notaciones. Los experimentos en diferentes conjuntos de datos y modelos base muestran que este método supera los métodos de estado de la arte. En particular, un modelo fine-tunado con 5% de datos de Tulu3 muestra un rendimiento relativo comparable al modelo SFT entrenado con el conjunto completo de datos, con mejoras de +5.73% en AlpacaEval y +6.89% en Wildbench.",
      "upvotes": 26,
      "discussionId": "6805b38555d3c792e1a9d155",
      "projectPage": "https://yichengchen24.github.io/projects/mig",
      "githubRepo": "https://github.com/yichengchen24/MIG",
      "ai_keywords": [
        "label graph",
        "semantic space",
        "information content",
        "Maximize the Information Gain (MIG)"
      ]
    },
    "publishedAt": "2025-04-18T13:59:46.000Z",
    "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space",
    "summary": "Data quality and diversity are key to the construction of effective\ninstruction-tuning datasets. % With the increasing availability of open-source\ninstruction-tuning datasets, it is advantageous to automatically select\nhigh-quality and diverse subsets from a vast amount of data. % Existing methods\ntypically prioritize instance quality and use heuristic rules to maintain\ndiversity. % However, this absence of a comprehensive view of the entire\ncollection often leads to suboptimal results. % Moreover, heuristic rules\ngenerally focus on distance or clustering within the embedding space, which\nfails to accurately capture the intent of complex instructions in the semantic\nspace. % To bridge this gap, we propose a unified method for quantifying the\ninformation content of datasets. This method models the semantic space by\nconstructing a label graph and quantifies diversity based on the distribution\nof information within the graph. % Based on such a measurement, we further\nintroduce an efficient sampling method that selects data samples iteratively to\nMaximize the Information Gain (MIG) in semantic\nspace. % Experiments on various datasets and base models demonstrate that MIG\nconsistently outperforms state-of-the-art methods. % Notably, the model\nfine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance\nto the official SFT model trained on the full dataset, with improvements of\n+5.73\\% on AlpacaEval and +6.89\\% on Wildbench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13835.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649988726677f66c2b486392",
      "avatarUrl": "/avatars/b649a77370660e129726e29504daba34.svg",
      "fullname": "Yining Li",
      "name": "ly015",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13837",
      "authors": [
        {
          "_id": "6805b9ec7c5fa8020f595642",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595643",
          "name": "Zhiqi Chen",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595644",
          "name": "Rui Lu",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595645",
          "name": "Andrew Zhao",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595646",
          "name": "Zhaokai Wang",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595647",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595648",
          "name": "Shiji Song",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595649",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/2KWQqFdVDUCAu-kSu87fa.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/rF4cAa6DAI3EaejDV_dgG.mp4"
      ],
      "publishedAt": "2025-04-18T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-21T01:54:36.096Z",
      "title": "Reinforcement Learning promueve la capacidad de juicio lógico de los modelos de lenguaje de alto nivel (LLM) más allá de los modelos básicos.",
      "submittedOnDailyBy": {
        "_id": "649d475111592b1a765ac1a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
        "isPro": false,
        "fullname": "Yang Yue",
        "user": "Yang130",
        "type": "user"
      },
      "summary": "RLVR (Reinforcement Learning with Verifiable Rewards) ha recibido mucha atención recientemente por su capacidad significativa de mejorar la capacidad de resolución de problemas de modelos grandes de lenguaje (LLM), especialmente en tareas de matemáticas o programación. Generalmente, RLVR permite a los modelos de lenguaje mejorar continuamente a través de un proceso de auto-mejoramiento, y se espera que estos modelos adquieran una nueva capacidad de resolución de problemas que supere significativamente la capacidad del modelo base. Sin embargo, en este estudio, se revisa críticamente estas asunciones, y se explora la capacidad de resolución de problemas de diferentes familias de modelos y marcos de evaluación utilizando el métrico pass@k con valores de k grandes. Un hecho sorprendente es que el aprendizaje por refuerzo no ha extraído patrones nuevos de resolución de problemas. Los modelos entrenados con RL superan a los modelos base en pequeños valores de k (por ejemplo, k=1), pero los modelos base pueden alcanzar una puntuación de pass@k más alta en valores de k grandes. Los caminos de resolución de problemas generados por estos modelos se encuentran dentro de la distribución de muestreo del modelo base, lo que demuestra que la capacidad de resolución de problemas de estos modelos ya está presente en el modelo base. Esta investigación muestra que la capacidad de resolución de problemas de estos modelos es ya presente en el modelo base.",
      "upvotes": 24,
      "discussionId": "6805b9ed7c5fa8020f59568c",
      "projectPage": "https://limit-of-rlvr.github.io/",
      "githubRepo": "https://github.com/LeapLabTHU/limit-of-RLVR",
      "ai_keywords": [
        "Reinforcement Learning",
        "Verifiable Rewards",
        "RLVR",
        "LLMs (Large Language Models)",
        "reasoning capabilities",
        "mathematics",
        "programming tasks",
        "pass@\\textit{k}",
        "benchmark",
        "RL-trained models",
        "base models",
        "reasoning paths",
        "sampling distribution",
        "performance",
        "biasing",
        "output distribution",
        "visual reasoning tasks",
        "distillation"
      ]
    },
    "publishedAt": "2025-04-18T13:59:56.000Z",
    "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@k metric with large values of k to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does not, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of k (\\eg, k=1), base models can achieve a comparable or\neven higher pass@k score compared to their RL counterparts at large k\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/2KWQqFdVDUCAu-kSu87fa.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/rF4cAa6DAI3EaejDV_dgG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13837.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "649d475111592b1a765ac1a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
      "fullname": "Yang Yue",
      "name": "Yang130",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11833",
      "authors": [
        {
          "_id": "6805bb01747a412bca737b53",
          "name": "Changjiang Gao",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b54",
          "name": "Xu Huang",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b55",
          "name": "Wenhao Zhu",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b56",
          "name": "Shujian Huang",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b57",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b58",
          "name": "Fei Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T07:45:10.000Z",
      "submittedOnDailyAt": "2025-04-21T01:57:09.327Z",
      "title": "¿Se piensa que la diversidad de lenguajes puede mejorar la capacidad de inferencia de un LLM?",
      "submittedOnDailyBy": {
        "_id": "65fed45b08d35929362dd651",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
        "isPro": false,
        "fullname": "FeiYuan",
        "user": "FeYuan",
        "type": "user"
      },
      "summary": "Anteriores estudios han descubierto que los modelos de lenguaje de gran escala presentan un \"fenómeno de sesgo hacia el inglés\". Es decir, cuando se proporcionan tareas en inglés, su rendimiento se vea significativamente mejorado. Un punto interesante es que se ha observado un mejor rendimiento en tareas de inferencia en un lenguaje específico que supera al inglés. Sin embargo, este fenómeno aún no ha sido suficientemente investigado. En este artículo, se evaluan las limitaciones de las tareas de inferencia en modelos multilingües y se muestra que los límites de inferencia en múltiples lenguas pueden ser más altos y estables que aquellos en solo inglés (calidad de traducción y robustez frente al cambio de lenguaje). Además, se analizan las causas de estos límites y los problemas asociados con alcanzarlos, demostrando que los métodos para elegir respuestas generales no son capaz de superar estos límites. Estos puntos de vista esperan que estos hallazgos sean útiles para futuras investigaciones que busquen aprovechar todas las potencialidades de la inferencia multilingüe en modelos de lenguaje grandes.",
      "upvotes": 14,
      "discussionId": "6805bb02747a412bca737b7e",
      "githubRepo": "https://github.com/CONE-MT/multilingual_reasoning"
    },
    "publishedAt": "2025-04-16T03:45:10.000Z",
    "title": "Could Thinking Multilingually Empower LLM Reasoning?",
    "summary": "Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@k points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11833.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fed45b08d35929362dd651",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
      "fullname": "FeiYuan",
      "name": "FeYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13072",
      "authors": [
        {
          "_id": "6805bfc5e332a61dd90160b0",
          "name": "Wenqi Dong",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b1",
          "name": "Bangbang Yang",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b2",
          "name": "Zesong Yang",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b3",
          "name": "Yuan Li",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b4",
          "name": "Tao Hu",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b5",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b6",
          "name": "Yuewen Ma",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b7",
          "name": "Zhaopeng Cui",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63d748ff6f49aa82306b7e48/DNTOfy5oOkjTJwLBeGOUH.qt"
      ],
      "publishedAt": "2025-04-17T16:33:39.000Z",
      "submittedOnDailyAt": "2025-04-21T03:26:56.809Z",
      "title": "Hiskeín: Creación de líneas de visión 3D estratificada mediante vistas en ángulo igual",
      "submittedOnDailyBy": {
        "_id": "63d748ff6f49aa82306b7e48",
        "avatarUrl": "/avatars/9f0b8b8a09b14d76e52ed1bd312e6b63.svg",
        "isPro": false,
        "fullname": "BB Yang",
        "user": "ybbbbt",
        "type": "user"
      },
      "summary": "La generación 3D desempeña un papel frontera en las áreas de multimedia y graficos computacionales, pero el enfoque actual presenta limitaciones en la clasificación de objetos y la flexibilidad de edición en aplicaciones interactivas. En este trabajo, se propone un nuevo marco heurístico llamado 'HiScene', que intenta reducir significativamente la diferencia entre la generación de imágenes 2D y la de objetos 3D, proporcionando escenas precisas con características de identificación estructural y contenido de escena artístico. Nuestra principal hipótesis es que se trata de considerar a las escenas como 'objetos' en transformaciones isométricas. De esta manera, las escenas pueden ser más complejas y divididas en elementos manipulables y diseñables. Esta aproximación heurística permite la generación de contenido 3D que coincide con representaciones 2D mientras mantiene estructuras estructurales. Para garantizar la completitud y la asignación espacial de cada instancia dividida, se utiliza una tecnología de compilación de ignoración basada en video difusor para efectivamente realizar procesamiento oculto, y se introduce la inyección prioritaria de forma para asegurar la coherencia espacial. Los resultados experimentales muestran que nuestro método genera instancias completas y adecuadas para aplicaciones interactivas, manteniendo la posibilidad física y la asignación con la entrada del usuario.",
      "upvotes": 5,
      "discussionId": "6805bfc9e332a61dd901618b",
      "ai_keywords": [
        "hierarchical framework",
        "2D image generation",
        "3D object generation",
        "video-diffusion-based amodal completion",
        "occlusions",
        "shadows",
        "shape prior injection",
        "spatial coherence",
        "natural object arrangements",
        "complete object instances",
        "interactive applications",
        "physical plausibility",
        "user inputs"
      ]
    },
    "publishedAt": "2025-04-17T12:33:39.000Z",
    "title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation",
    "summary": "Scene-level 3D generation represents a critical frontier in multimedia and\ncomputer graphics, yet existing approaches either suffer from limited object\ncategories or lack editing flexibility for interactive applications. In this\npaper, we present HiScene, a novel hierarchical framework that bridges the gap\nbetween 2D image generation and 3D object generation and delivers high-fidelity\nscenes with compositional identities and aesthetic scene content. Our key\ninsight is treating scenes as hierarchical \"objects\" under isometric views,\nwhere a room functions as a complex object that can be further decomposed into\nmanipulatable items. This hierarchical approach enables us to generate 3D\ncontent that aligns with 2D representations while maintaining compositional\nstructure. To ensure completeness and spatial alignment of each decomposed\ninstance, we develop a video-diffusion-based amodal completion technique that\neffectively handles occlusions and shadows between objects, and introduce shape\nprior injection to ensure spatial coherence within the scene. Experimental\nresults demonstrate that our method produces more natural object arrangements\nand complete object instances suitable for interactive applications, while\nmaintaining physical plausibility and alignment with user inputs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63d748ff6f49aa82306b7e48/DNTOfy5oOkjTJwLBeGOUH.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13072.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d748ff6f49aa82306b7e48",
      "avatarUrl": "/avatars/9f0b8b8a09b14d76e52ed1bd312e6b63.svg",
      "fullname": "BB Yang",
      "name": "ybbbbt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13157",
      "authors": [
        {
          "_id": "6804392129303a3402c4f38e",
          "user": {
            "_id": "631bfb21f6bc4be4a6592afc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
            "isPro": false,
            "fullname": "Khiem Vuong",
            "user": "kvuong2711",
            "type": "user"
          },
          "name": "Khiem Vuong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-20T15:01:35.224Z",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f38f",
          "name": "Anurag Ghosh",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f390",
          "name": "Deva Ramanan",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f391",
          "name": "Srinivasa Narasimhan",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f392",
          "name": "Shubham Tulsiani",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/x2dR6H7Gl0l8qbXY9YxGP.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/Kf60HymR1YXGVqoSmQNYb.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/O0wHR9zr6E56IE7sjs0UI.jpeg"
      ],
      "publishedAt": "2025-04-17T17:57:05.000Z",
      "submittedOnDailyAt": "2025-04-21T01:28:10.375Z",
      "title": "AerialMegaDepth: MegaDepth: Reconstrucción de Superficies y Sintesis de Puntos de Vista de Aprendizaje",
      "submittedOnDailyBy": {
        "_id": "631bfb21f6bc4be4a6592afc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
        "isPro": false,
        "fullname": "Khiem Vuong",
        "user": "kvuong2711",
        "type": "user"
      },
      "summary": "Estamos investigando el trabajo de recomposición gráfica que se compone de una mezcla de imágenes capturadas en tierra y aire. El método de enfoque más avanzado actual basado en aprendizaje no puede manejar los cambios visuales extremos de pares de imágenes tierra-aire. Nuestra hipótesis es que la escasez de conjuntos de datos alta calidad, registrados en aire y tierra, es la principal causa de este fracaso. Estos datos son difíciles de recomponer, lo que dificulta su expansión. Para superar este desafío, proponemos la combinación de dibujos factísticos en el mapa 3D del mundo y imágenes reales de nivel terrestre de la comunidad (por ejemplo, Google Earth, MegaDepth). Los datos factísticos simulan una amplia gama de vistas en el aire, mientras que las imágenes reales de nivel terrestre mejoran la precisión visual de las imágenes de tierra cuando las imágenes de mapa no tienen suficientes detalles. Esta combinación efectivamente reduce el intercambio entre los dos tipos de datos. Usando este conjunto de datos híbrido, hemos ajustado varios algoritmos avanzados y logrado un gran progreso en el trabajo de aire a tierra sin ejemplos, como el DUSt3R, que ahora alcanza una precisión de casi 56%, resolviendo principalmente su fracaso con cambios visuales significativos. Además, nuestro conjunto de datos, excluyendo la medición de la cámara y la reconstrucción del escenario, ha mejorado la eficiencia de trabajos de muestreo de escenarios difíciles como el aire a tierra y ha demostrado la valiosa aplicación práctica de nuestro enfoque en aplicaciones mundiales.",
      "upvotes": 4,
      "discussionId": "6804392329303a3402c4f3e8",
      "projectPage": "https://aerial-megadepth.github.io/",
      "githubRepo": "https://github.com/kvuong2711/aerial-megadepth",
      "ai_keywords": [
        "geometric reconstruction",
        "learning-based approaches",
        "extreme viewpoint variation",
        "co-registered",
        "aerial-ground datasets",
        "pseudo-synthetic renderings",
        "3D city-wide meshes",
        "crowd-sourced images",
        "visual fidelity",
        "domain gap",
        "mesh-based renderings",
        "fine-tuning",
        "DUSt3R",
        "camera rotation error",
        "scene reconstruction",
        "novel-view synthesis",
        "downstream tasks"
      ]
    },
    "publishedAt": "2025-04-17T13:57:05.000Z",
    "title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis",
    "summary": "We explore the task of geometric reconstruction of images captured from a\nmixture of ground and aerial views. Current state-of-the-art learning-based\napproaches fail to handle the extreme viewpoint variation between aerial-ground\nimage pairs. Our hypothesis is that the lack of high-quality, co-registered\naerial-ground datasets for training is a key reason for this failure. Such data\nis difficult to assemble precisely because it is difficult to reconstruct in a\nscalable way. To overcome this challenge, we propose a scalable framework\ncombining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google\nEarth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The\npseudo-synthetic data simulates a wide range of aerial viewpoints, while the\nreal, crowd-sourced images help improve visual fidelity for ground-level images\nwhere mesh-based renderings lack sufficient detail, effectively bridging the\ndomain gap between real images and pseudo-synthetic renderings. Using this\nhybrid dataset, we fine-tune several state-of-the-art algorithms and achieve\nsignificant improvements on real-world, zero-shot aerial-ground tasks. For\nexample, we observe that baseline DUSt3R localizes fewer than 5% of\naerial-ground pairs within 5 degrees of camera rotation error, while\nfine-tuning with our data raises accuracy to nearly 56%, addressing a major\nfailure point in handling large viewpoint changes. Beyond camera estimation and\nscene reconstruction, our dataset also improves performance on downstream tasks\nlike novel-view synthesis in challenging aerial-ground scenarios, demonstrating\nthe practical value of our approach in real-world applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/x2dR6H7Gl0l8qbXY9YxGP.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/Kf60HymR1YXGVqoSmQNYb.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/O0wHR9zr6E56IE7sjs0UI.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13157.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631bfb21f6bc4be4a6592afc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
      "fullname": "Khiem Vuong",
      "name": "kvuong2711",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11544",
      "authors": [
        {
          "_id": "6804ca9fd8538baa1c39ca93",
          "name": "Tianyang Xu",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca94",
          "name": "Haojie Zheng",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca95",
          "name": "Chengze Li",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca96",
          "name": "Haoxiang Chen",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca97",
          "name": "Yixin Liu",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca98",
          "name": "Ruoxi Chen",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca99",
          "name": "Lichao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T18:24:00.000Z",
      "submittedOnDailyAt": "2025-04-21T01:38:46.380Z",
      "title": "NodeRAG: Construcción de RAG basada en grafos con nodos híbridos",
      "submittedOnDailyBy": {
        "_id": "6610fb736504d9bed5890d58",
        "avatarUrl": "/avatars/832b186fc51c639f1709025d442b3f4b.svg",
        "isPro": false,
        "fullname": "Tianyang Xu",
        "user": "TerryXu666",
        "type": "user"
      },
      "summary": "La Revisión Automática de Contenido (RAG) permite que modelos de lenguaje grandes accedan a corporaciones externas e personales, proporcionando respuestas que coincidan de hecho en ciertos dominios. Utilizando la estructura interna del corpus, el enfoque RAG basado en grafos construye un índice de grafos y explota las características estructurales del grafo para realizar este proceso. Sin embargo, el enfoque actual de RAG basado en grafos no prioriza la diseño de la estructura del grafo. Un grafo inadecuado puede impidir la integración fácil de diversos algoritmos de grafos y causar desacordos en el flujo de trabajo y pérdida de eficiencia. Para explotar mejor los potenciales de RAG basado en grafos, proponemos NodeRAG. Este enfoque presenta una estructura gráfica no formal y permite la integración sencilla de métodos basados en grafos en el flujo de trabajo de RAG. Con la estrecha asociación de las capacidades de los modelos de lenguaje grandes (LLM), este marco asegura una serie de procesos continuos y eficientes desde el inicio hasta el final. A través de experimentos concretos, hemos demostrado que NodeRAG presenta mejores rendimientos en respuestas a consultas en comparación con los métodos anteriores (GraphRAG y LightRAG), en términos de tiempo de índice, tiempo de consulta, eficiencia de almacenamiento, marcos de referencia de etapas y evaluación de cabeceras abiertas. Nuestro repositorio en GitHub está disponible en https://github.com/Terry-Xu-666/NodeRAG.",
      "upvotes": 4,
      "discussionId": "6804caa0d8538baa1c39cac2",
      "projectPage": "https://terry-xu-666.github.io/NodeRAG_web/",
      "githubRepo": "https://github.com/Terry-Xu-666/NodeRAG",
      "ai_keywords": [
        "Retrieval-augmented generation (RAG)",
        "external and private corpus",
        "factually consistent responses",
        "knowledge graph index",
        "graph-based RAG methods",
        "heterogeneous graph structures",
        "seamless and holistic integration",
        "end-to-end process",
        "question-answering performance",
        "multi-hop benchmarks",
        "open-ended head-to-head evaluations",
        "retrieval tokens"
      ]
    },
    "publishedAt": "2025-04-15T14:24:00.000Z",
    "title": "NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes",
    "summary": "Retrieval-augmented generation (RAG) empowers large language models to access\nexternal and private corpus, enabling factually consistent responses in\nspecific domains. By exploiting the inherent structure of the corpus,\ngraph-based RAG methods further enrich this process by building a knowledge\ngraph index and leveraging the structural nature of graphs. However, current\ngraph-based RAG approaches seldom prioritize the design of graph structures.\nInadequately designed graph not only impede the seamless integration of diverse\ngraph algorithms but also result in workflow inconsistencies and degraded\nperformance. To further unleash the potential of graph for RAG, we propose\nNodeRAG, a graph-centric framework introducing heterogeneous graph structures\nthat enable the seamless and holistic integration of graph-based methodologies\ninto the RAG workflow. By aligning closely with the capabilities of LLMs, this\nframework ensures a fully cohesive and efficient end-to-end process. Through\nextensive experiments, we demonstrate that NodeRAG exhibits performance\nadvantages over previous methods, including GraphRAG and LightRAG, not only in\nindexing time, query time, and storage efficiency but also in delivering\nsuperior question-answering performance on multi-hop benchmarks and open-ended\nhead-to-head evaluations with minimal retrieval tokens. Our GitHub repository\ncould be seen at https://github.com/Terry-Xu-666/NodeRAG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11544.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6610fb736504d9bed5890d58",
      "avatarUrl": "/avatars/832b186fc51c639f1709025d442b3f4b.svg",
      "fullname": "Tianyang Xu",
      "name": "TerryXu666",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.09621",
      "authors": [
        {
          "_id": "6800ef5509eaa9d1d87a6eaf",
          "name": "Jiuchen Chen",
          "hidden": false
        },
        {
          "_id": "6800ef5509eaa9d1d87a6eb0",
          "user": {
            "_id": "6672c01fa6eb488f049ecb80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
            "isPro": false,
            "fullname": "Xinyu Yan",
            "user": "fengyanzi",
            "type": "user"
          },
          "name": "Xinyu Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-19T15:17:09.181Z",
          "hidden": false
        },
        {
          "_id": "6800ef5509eaa9d1d87a6eb1",
          "name": "Qizhi Xu",
          "hidden": false
        },
        {
          "_id": "6800ef5509eaa9d1d87a6eb2",
          "name": "Kaiqi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T15:41:25.000Z",
      "submittedOnDailyAt": "2025-04-21T05:45:47.747Z",
      "title": "Toknaize Image PatchX: Fusion of Global Context for Effective Patch Removal in Large-Scale Images",
      "submittedOnDailyBy": {
        "_id": "6672c01fa6eb488f049ecb80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
        "isPro": false,
        "fullname": "Xinyu Yan",
        "user": "fengyanzi",
        "type": "user"
      },
      "summary": "En el contexto global y las características detalladas locales son esenciales para el trabajo de eliminación de niebla. Los modelos de aprendizaje profundo son familiares con imágenes de baja resolución pequeñas, pero en imágenes de alta resolución grandes, suelen enfrentar limitaciones de memoria de GPU. Para compensar esto, se opta por cortar o reescalar las imágenes. Estos dos métodos reducen la información global y el detalle alta frecuencia. Para resolver estos problemas, se propone el método de eliminación de niebla llamado DehazeXL. Este método equilibra la extracción de contexto global y características locales, permitiendo así el modelado de imágenes grandes en dispositivos de GPU principales. Además, diseña una distribución visual de responsabilidades adecuada para la tarea de eliminación de niebla y utiliza esto para evaluar el uso eficiente del contexto global. Finalmente, reconoce la escasez de conjuntos de datos de benchmark para la eliminación de niebla en grandes imágenes y desarrolla el conjunto de datos de alta resolución 8KDehaze. Este conjunto incluye 10,000 pares de imágenes de observación remota limpias y con niebla, cada una de las imágenes tiene un tamaño de 8192×8192 píxeles. Los experimentos ampliados muestran que DehazeXL puede realizar inferencia de imágenes de 10240×10240 píxeles con 21 GB de memoria, y que es el método que mejora los resultados en todos los métodos evaluados. El código fuente y los conjuntos de datos experimentales están disponibles en https://github.com/CastleChen339/DehazeXL.",
      "upvotes": 4,
      "discussionId": "6800ef5709eaa9d1d87a6f76",
      "projectPage": "https://castlechen339.github.io/DehazeXL.github.io/",
      "githubRepo": "https://github.com/CastleChen339/DehazeXL",
      "ai_keywords": [
        "haze removal",
        "image slicing",
        "downsampling",
        "DehazeXL",
        "global context",
        "local feature extraction",
        "end-to-end modeling",
        "visual attribution",
        "8KDehaze",
        "ultra-high-resolution haze removal dataset",
        "remote sensing images"
      ]
    },
    "publishedAt": "2025-04-13T11:41:25.000Z",
    "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal\n  in Large Images",
    "summary": "Global contextual information and local detail features are essential for\nhaze removal tasks. Deep learning models perform well on small, low-resolution\nimages, but they encounter difficulties with large, high-resolution ones due to\nGPU memory limitations. As a compromise, they often resort to image slicing or\ndownsampling. The former diminishes global information, while the latter\ndiscards high-frequency details. To address these challenges, we propose\nDehazeXL, a haze removal method that effectively balances global context and\nlocal feature extraction, enabling end-to-end modeling of large images on\nmainstream GPU hardware. Additionally, to evaluate the efficiency of global\ncontext utilization in haze removal performance, we design a visual attribution\nmethod tailored to the characteristics of haze removal tasks. Finally,\nrecognizing the lack of benchmark datasets for haze removal in large images, we\nhave developed an ultra-high-resolution haze removal dataset (8KDehaze) to\nsupport model training and testing. It includes 10000 pairs of clear and hazy\nremote sensing images, each sized at 8192 times 8192 pixels. Extensive\nexperiments demonstrate that DehazeXL can infer images up to 10240 times\n10240 pixels with only 21 GB of memory, achieving state-of-the-art results\namong all evaluated methods. The source code and experimental dataset are\navailable at https://github.com/CastleChen339/DehazeXL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6672c01fa6eb488f049ecb80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
      "fullname": "Xinyu Yan",
      "name": "fengyanzi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13626",
      "authors": [
        {
          "_id": "6805fa66fddd500b98039425",
          "name": "Yule Liu",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b98039426",
          "name": "Jingyi Zheng",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b98039427",
          "name": "Zhen Sun",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b98039428",
          "name": "Zifan Peng",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b98039429",
          "name": "Wenhan Dong",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b9803942a",
          "name": "Zeyang Sha",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b9803942b",
          "name": "Shiwen Cui",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b9803942c",
          "name": "Weiqiang Wang",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b9803942d",
          "name": "Xinlei He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T11:07:19.000Z",
      "submittedOnDailyAt": "2025-04-21T06:29:28.134Z",
      "title": "¡Logueado, puedes dejar comentarios!",
      "submittedOnDailyBy": {
        "_id": "63da3d7ae697e5898cb86854",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675246771355-noauth.jpeg",
        "isPro": false,
        "fullname": "Talha Rüzgar Akkuş",
        "user": "Q-bert",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los grandes modelos lógicos (LRMs) ha expandido la cantidad de cálculos en los procesos de validación, demostrando un aumento efectivo en la capacidad lógica para diversas tareas. Sin embargo, los LRMs frecuentemente enfrentan el problema de \"pensamiento excesivo\", donde el modelo genera pasos lógicos largos y redundantes sin mejoras significativas en su rendimiento. Actualmente, la investigación se centra en la mitigación de este pensamiento excesivo mediante fine-tuning, lo cual implica adicionales datos, configuraciones de entrenamiento no convencionales, ajustes inadecuados de seguridad y una disminución en la capacidad de generalización.\n\nUn análisis experimental ha revelado importantes características de la acción de los LRMs. Este análisis muestra que pequeños modelos pueden recordar y controlar estímulos externos de CoTs (Coherent Thought-Chains) insertando tokens de memoria entre <think> y </think>. Basándose en esta observación, se propone una simple y eficiente pipeline llamada ThoughtMani, que permite a los LRMs saltar pasos intermedios innecesarios, reduciendo significativamente los costos de cálculo. Se verificó la utilidad y eficiencia de ThoughtMani a través de experimentos ampliados. Por ejemplo, cuando se aplica a QwQ-32B en el conjunto de datos LiveBench/Code, ThoughtMani logró reducir la cantidad de tokens de salida en aproximadamente 30% sin perder el rendimiento original, y minimizó o eliminó el overhead de la generación de CoTs. Además, ThoughtMani mejoró en un promedio del 10% en la configuración de seguridad, y permitió la construcción de LRMs más eficientes y accesibles en aplicaciones de gran escala, reduciendo el tamaño del modelo necesario para mantener el rendimiento.",
      "upvotes": 3,
      "discussionId": "6805fa67fddd500b98039461",
      "ai_keywords": [
        "large reasoning models (LRMs)",
        "overthinking problems",
        "fine-tuning",
        "thinking token",
        "external CoTs (Chain of Thought)",
        "ThoughtMani",
        "unnecessary intermediate steps",
        "computational costs",
        "LiveBench/Code dataset",
        "output token counts",
        "safety alignment"
      ]
    },
    "publishedAt": "2025-04-18T07:07:19.000Z",
    "title": "Thought Manipulation: External Thought Can Be Efficient for Large\n  Reasoning Models",
    "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities in multiple tasks. However, LRMs typically suffer from\n\"overthinking\" problems, where models generate significantly redundant\nreasoning steps while bringing limited performance gains. Existing work relies\non fine-tuning to mitigate overthinking, which requires additional data,\nunconventional training setups, risky safety misalignment, and poor\ngeneralization.\n  Through empirical analysis, we reveal an important characteristic of LRM\nbehaviors that placing external CoTs generated by smaller models between the\nthinking token (<think> and </think>) can effectively\nmanipulate the model to generate fewer thoughts. Building on these insights, we\npropose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass\nunnecessary intermediate steps and reduce computational costs significantly. We\nconduct extensive experiments to validate the utility and efficiency of\nThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code\ndataset, ThoughtMani keeps the original performance and reduces output token\ncounts by approximately 30%, with little overhead from the CoT generator.\nFurthermore, we find that ThoughtMani enhances safety alignment by an average\nof 10%. Since model vendors typically serve models of different sizes\nsimultaneously, ThoughtMani provides an effective way to construct more\nefficient and accessible LRMs for real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13626.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63da3d7ae697e5898cb86854",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675246771355-noauth.jpeg",
      "fullname": "Talha Rüzgar Akkuş",
      "name": "Q-bert",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 89
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13173",
      "authors": [
        {
          "_id": "6805c4dab15a57fcb59b6f08",
          "name": "Ali Behrouz",
          "hidden": false
        },
        {
          "_id": "6805c4dab15a57fcb59b6f09",
          "name": "Meisam Razaviyayn",
          "hidden": false
        },
        {
          "_id": "6805c4dab15a57fcb59b6f0a",
          "name": "Peilin Zhong",
          "hidden": false
        },
        {
          "_id": "6805c4dab15a57fcb59b6f0b",
          "name": "Vahab Mirrokni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:59:33.000Z",
      "submittedOnDailyAt": "2025-04-21T02:39:28.607Z",
      "title": "Es todo conectado: memoria en el proceso de verificación, sesgos de la atención, almacenamiento, el deseo de optimizar en línea.",
      "submittedOnDailyBy": {
        "_id": "65cccd5134a5d74cbaa9446c",
        "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
        "isPro": false,
        "fullname": "Ali Behrouz",
        "user": "AliBehrouz",
        "type": "user"
      },
      "summary": "El diseño de eficiencia y arquitectura efectiva son esenciales para mejorar las capacidades de los modelos básicos. Hemos modelado fenómenos cognitivos como la bias de atención (tendencia natural a priorizar ciertos eventos o estímulos), asociando Transformers, Titans y redes neuronales lineales modernas con módulos de memoria asociativa. Proponemos un método para aprender la asignación de clave-valor utilizando objetos internos (bias de atención). Sorprendentemente, hemos encontrado que muchos modelos de secuencia actuales utilizan (1) similitudes similares a productos y (2) objetos de regresión L2 como bias de atención. Además, hemos relacionado la configuración del bias de atención y métodos efectivos de aproximación con la estabilidad del proceso de entrenamiento. También hemos reinterpretado la estructura de olvido en arquitecturas de aprendizaje profundo como normalización de retención y proponemos un nuevo gate de olvido para modelos de secuencia. Basándonos en estos principios, proponemos un marco general llamado Miras, que permite diseñar arquitecturas de aprendizaje profundo basadas en memoria asociativa, objetos de bias de atención, gates de retención y algoritmos de aprendizaje de memoria. Presentamos tres nuevos modelos de secuencia (Moneta, Yaad, Memora) que superan la capacidad de los RNN lineales actuales, manteniendo un proceso de entrenamiento rápido y paralelizable. Nuestros experimentos muestran que diferentes diseños de Miras pueden generar modelos con diferentes niveles de rendimiento. Por ejemplo, en tareas de modelado de lenguaje, inferencia general y en tareas específicas de enfatización de la memoria, los modelos de Transformers y redes neuronales lineales modernas pueden ser superados por Miras.",
      "upvotes": 3,
      "discussionId": "6805c4dbb15a57fcb59b6f3d",
      "ai_keywords": [
        "Transformers",
        "Titans",
        "linear recurrent neural networks",
        "associative memory modules",
        "attentional bias",
        "dot-product similarity",
        "L2 regression",
        "retention regularization",
        "forget gates",
        "Miras",
        "Moneta",
        "Yaad",
        "Memora",
        "parallelizable training process",
        "language modeling",
        "commonsense reasoning",
        "recall intensive tasks"
      ]
    },
    "publishedAt": "2025-04-17T13:59:33.000Z",
    "title": "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization",
    "summary": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13173.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cccd5134a5d74cbaa9446c",
      "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
      "fullname": "Ali Behrouz",
      "name": "AliBehrouz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]