[
  {
    "paper": {
      "id": "2506.17450",
      "authors": [
        {
          "_id": "68620adf9e7509383d29ab98",
          "user": {
            "_id": "655bca95360e4f90cb61ba83",
            "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
            "isPro": true,
            "fullname": "Jiacheng Chen",
            "user": "cccjc",
            "type": "user"
          },
          "name": "Jiacheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-30T06:22:01.226Z",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab99",
          "name": "Ramin Mehran",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9a",
          "name": "Xuhui Jia",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9b",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "68620adf9e7509383d29ab9c",
          "name": "Sanghyun Woo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T19:38:34.000Z",
      "submittedOnDailyAt": "2025-06-30T02:33:26.106Z",
      "title": "BlenderFusion: Visualización y Edición 3D Basada en la Generación y Sintetización Óptica",
      "submittedOnDailyBy": {
        "_id": "655bca95360e4f90cb61ba83",
        "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
        "isPro": true,
        "fullname": "Jiacheng Chen",
        "user": "cccjc",
        "type": "user"
      },
      "summary": "BlenderFusion es un marco de fusión visual generativa que recombina objetos, cámaras y fondos para sintetizar nuevas imágenes. Este proceso se compone de: (i) la conversión de entradas visuales en entidades 3D visuales (procesamiento de capas), (ii) la edición basada en control 3D en Blender (edición), y (iii) la generación de imágenes coherentes utilizando un generador de fusión (fusión). Nuestro generador de fusión expande modelos difusión preentrenados para procesar en paralelo imágenes originales (fuente) y editadas (objetivo). Esto se ajusta a través de dos estrategias principales de entrenamiento: (i) máscaras de fuente y (ii) simulaciones de objetos. BlenderFusion supera significativamente a los métodos existentes en tareas de edición de imágenes complejas.",
      "upvotes": 28,
      "discussionId": "68620adf9e7509383d29ab9d",
      "projectPage": "https://blenderfusion.github.io/",
      "ai_summary": "A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.",
      "ai_keywords": [
        "diffusion model",
        "source masking",
        "simulated object jittering"
      ]
    },
    "publishedAt": "2025-06-20T15:38:34.000Z",
    "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing",
    "summary": "We present BlenderFusion, a generative visual compositing framework that\nsynthesizes new scenes by recomposing objects, camera, and background. It\nfollows a layering-editing-compositing pipeline: (i) segmenting and converting\nvisual inputs into editable 3D entities (layering), (ii) editing them in\nBlender with 3D-grounded control (editing), and (iii) fusing them into a\ncoherent scene using a generative compositor (compositing). Our generative\ncompositor extends a pre-trained diffusion model to process both the original\n(source) and edited (target) scenes in parallel. It is fine-tuned on video\nframes with two key training strategies: (i) source masking, enabling flexible\nmodifications like background replacement; (ii) simulated object jittering,\nfacilitating disentangled control over objects and camera. BlenderFusion\nsignificantly outperforms prior methods in complex compositional scene editing\ntasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17450.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655bca95360e4f90cb61ba83",
      "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
      "fullname": "Jiacheng Chen",
      "name": "cccjc",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.21862",
      "authors": [
        {
          "_id": "6861eea79e7509383d29ab2f",
          "name": "Boyuan Sun",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab30",
          "name": "Jiaxing Zhao",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab31",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "6861eea79e7509383d29ab32",
          "name": "Qibin Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T02:29:58.000Z",
      "submittedOnDailyAt": "2025-06-30T00:31:12.107Z",
      "title": "LLaVA-Scissor: Token Compression Utilizing Meaningful Connective Components for Video LLM",
      "submittedOnDailyBy": {
        "_id": "6686044047f2a33570e59e31",
        "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
        "isPro": false,
        "fullname": "Jiaxing Zhao",
        "user": "StarJiaxing",
        "type": "user"
      },
      "summary": "En este artículo, se propone una estrategia de compresión de tokens adecuada para modelos de lenguaje multimodal de video, llamada LLaVA-Scissor. Los métodos existentes principalmente basan la compresión de tokens en los scores de atención, pero no pueden capturar eficazmente todos los dominios semánticos, lo que lleva a la aparición de tokens redundantes. Además, se utiliza un enfoque de acceso a componentes semánticos (SCC) para asignar tokens a diferentes dominios semánticos dentro de un conjunto de tokens, asegurando así una cobertura semántica general. Consequentemente, se ha implementado una estrategia de compresión de tokens en dos etapas, utilizando la SCC en el espectro y en el dominio temporal. Esta estrategia permite efectivamente compresionar tokens de manera que no se repitan, facilitando la representación de todo el video. La capacidad de compresión de tokens de LLaVA-Scissor se evaluó mediante diversos benchmarks de comprensión del video, incluyendo evaluaciones de respuestas a preguntas de video, comprensión de videos largos y benchmarks de múltiples elecciones detalladas. Los resultados de los experimentos muestran que la propuesta de LLaVA-Scissor presenta un desempeño excepcional comparado con otros métodos de compresión de tokens, y también muestra un desempeño superior en varios benchmarks de comprensión del video, incluso a bajos ratios de mantenimiento de tokens. Página del proyecto: https://github.com/HumanMLLM/LLaVA-Scissor.",
      "upvotes": 26,
      "discussionId": "6861eea89e7509383d29ab33",
      "ai_summary": "LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.",
      "ai_keywords": [
        "token compression strategy",
        "Semantic Connected Components (SCC)",
        "spatio-temporal token compression strategy",
        "video question answering",
        "long video understanding",
        "comprehensive multi-choice benchmarks"
      ]
    },
    "publishedAt": "2025-06-26T22:29:58.000Z",
    "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
    "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21862.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6686044047f2a33570e59e31",
      "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
      "fullname": "Jiaxing Zhao",
      "name": "StarJiaxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21416",
      "authors": [
        {
          "_id": "685e084071131fa43be08acc",
          "user": {
            "_id": "6361dd166945df7441b893fa",
            "avatarUrl": "/avatars/b3ae6888a41aab8c2a7ef9f7320565c4.svg",
            "isPro": false,
            "fullname": "Bowen Chen ",
            "user": "chenbowen",
            "type": "user"
          },
          "name": "Bowen Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-30T06:22:45.351Z",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08acd",
          "name": "Mengyi Zhao",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ace",
          "name": "Haomiao Sun",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08acf",
          "name": "Li Chen",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad0",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad1",
          "name": "Kang Du",
          "hidden": false
        },
        {
          "_id": "685e084071131fa43be08ad2",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T16:04:16.000Z",
      "submittedOnDailyAt": "2025-06-30T04:43:14.606Z",
      "title": "XVerse: Control de múltiples variables en línea con DiT - Coincidencia de atributos identificativos y semánticos",
      "submittedOnDailyBy": {
        "_id": "6498038ece9190ebb8693034",
        "avatarUrl": "/avatars/06ec2457932e05572d917ba286cdef25.svg",
        "isPro": false,
        "fullname": "Zhao",
        "user": "Mengyi",
        "type": "user"
      },
      "summary": "Lograr un control fino sobre la identidad de los sujetos y atributos semánticos (pose, estilo, iluminación) en la generación de imágenes a partir de texto, especialmente para múltiples sujetos, a menudo desminera la edicibilidad y la coherencia de los Transformadores de Difusión (DiTs). Muchas aproximaciones introducen artefactos o sufren de entenamiento de atributos. Para superar estos desafíos, proponemos un nuevo modelo de generación controlada de múltiples sujetos, el XVerse. Al transformar imágenes de referencia en desviaciones para la modulación del flujo de texto específico de tokens, el XVerse permite un control preciso e independiente para sujetos específicos sin perturbar los latentes o características de las imágenes. Consequentemente, el XVerse ofrece una síntesis de imágenes multi-sujeto de alta fidelidad con un control robusto sobre las características individuales de los sujetos y los atributos semánticos. Este avance mejora significativamente las capacidades de generación de escenarios personalizados y complejos.",
      "upvotes": 18,
      "discussionId": "685e084071131fa43be08ad3",
      "projectPage": "https://bytedance.github.io/XVerse/",
      "githubRepo": "https://github.com/bytedance/XVerse",
      "ai_summary": "XVerse enhances text-to-image generation by enabling precise and independent control over multiple subjects using token-specific text-stream modulation, improving image coherence and fidelity.",
      "ai_keywords": [
        "Diffusion Transformers",
        "DiTs",
        "text-to-image generation",
        "multi-subject controlled generation",
        "reference images",
        "token-specific text-stream modulation",
        "image latents",
        "multi-subject image synthesis",
        "semantic attributes"
      ],
      "githubStars": 68
    },
    "publishedAt": "2025-06-26T12:04:16.000Z",
    "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT Modulation",
    "summary": "Achieving fine-grained control over subject identity and semantic attributes\n(pose, style, lighting) in text-to-image generation, particularly for multiple\nsubjects, often undermines the editability and coherence of Diffusion\nTransformers (DiTs). Many approaches introduce artifacts or suffer from\nattribute entanglement. To overcome these challenges, we propose a novel\nmulti-subject controlled generation model XVerse. By transforming reference\nimages into offsets for token-specific text-stream modulation, XVerse allows\nfor precise and independent control for specific subject without disrupting\nimage latents or features. Consequently, XVerse offers high-fidelity, editable\nmulti-subject image synthesis with robust control over individual subject\ncharacteristics and semantic attributes. This advancement significantly\nimproves personalized and complex scene generation capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21416.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6498038ece9190ebb8693034",
      "avatarUrl": "/avatars/06ec2457932e05572d917ba286cdef25.svg",
      "fullname": "Zhao",
      "name": "Mengyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21356",
      "authors": [
        {
          "_id": "6861fb7a9e7509383d29ab4b",
          "name": "Hongbo Liu",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4c",
          "name": "Jingwen He",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4d",
          "name": "Yi Jin",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4e",
          "name": "Dian Zheng",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab4f",
          "name": "Yuhao Dong",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab50",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab51",
          "name": "Ziqi Huang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab52",
          "name": "Yinan He",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab53",
          "name": "Yangguang Li",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab54",
          "name": "Weichao Chen",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab55",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab56",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab57",
          "name": "Shengjie Zhao",
          "hidden": false
        },
        {
          "_id": "6861fb7a9e7509383d29ab58",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T15:09:21.000Z",
      "submittedOnDailyAt": "2025-06-30T04:32:34.261Z",
      "title": "ShotBench: Modelo de lenguaje visual cinematográfico, modelo que posee una comprensión visual de nivel experto.",
      "submittedOnDailyBy": {
        "_id": "652965773a416e1f2173443b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
        "isPro": false,
        "fullname": "Yuhao Dong",
        "user": "THUdyh",
        "type": "user"
      },
      "summary": "La técnica de registro cinematográfico, una lenguaje visual básico de la cinematografía, es esencial para la transmisión de historias, emociones y preguntas artísticas. Los modelos de lenguaje visual (VLMs) recientes han demostrado una amplia comprensión visual, pero faltan investigaciones y evaluaciones fuertes para entender los microtextos de registro cinematográfico dentro de cada escena. Esta falta es crítica para el entendimiento preciso de los detalles visuales y la precisión de la generación de vídeos con IA. En respuesta a esto, presentamos ShotBench, un marco de referencia detallado para la comprensión del lenguaje cinematográfico. Se ha extraído de 200 películas evaluadas (principalmente candidatas a Oscar), y se han preparado más de 3500 pares de preguntas y respuestas evaluadas por expertos en 8 dimensiones importantes de registro cinematográfico. La evaluación de 24 VLMs avanzados en ShotBench ha revelado sus limitaciones: incluso los modelos con mejores resultados tienen un porcentaje de respuestas correctas inferior al 60%, y particularmente, tienen dificultades con el código visual detallado y la lógica espacial compleja. Para fomentar el desarrollo de esta área, hemos construido ShotQA, un grande conjunto de datos diverso que incluye aproximadamente 70,000 pares de preguntas y respuestas. Utilizando ShotQA, hemos desarrollado ShotVL a través de ajustes directoriales y optimización de políticas grupales. ShotVL supera todos los modelos abierto-código y propietario actuales, y establece nuevos rendimientos óptimos en ShotBench. Publicamos los modelos, datos y código para fomentar rápidamente el desarrollo de la comprensión y generación de registro cinematográfico con IA.",
      "upvotes": 15,
      "discussionId": "6861fb7a9e7509383d29ab59",
      "projectPage": "https://vchitect.github.io/ShotBench-project/",
      "githubRepo": "https://github.com/Vchitect/ShotBench/tree/main",
      "ai_summary": "ShotBench and ShotQA datasets, along with ShotVL model, enhance AI's understanding and generation capabilities by specifically targeting nuanced cinematic language comprehension.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "ShotBench",
        "QA pairs",
        "cinematic grammar",
        "fine-grained visual comprehension",
        "AI-assisted video generation",
        "ShotQA",
        "multimodal dataset",
        "supervised fine-tuning",
        "Group Relative Policy Optimization",
        "ShotVL",
        "AI-driven cinematic understanding",
        "state-of-the-art performance"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-06-26T11:09:21.000Z",
    "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language\n  Models",
    "summary": "Cinematography, the fundamental visual language of film, is essential for\nconveying narrative, emotion, and aesthetic quality. While recent\nVision-Language Models (VLMs) demonstrate strong general visual understanding,\ntheir proficiency in comprehending the nuanced cinematic grammar embedded\nwithin individual shots remains largely unexplored and lacks robust evaluation.\nThis critical gap limits both fine-grained visual comprehension and the\nprecision of AI-assisted video generation. To address this, we introduce\nShotBench, a comprehensive benchmark specifically designed for cinematic\nlanguage understanding. It features over 3.5k expert-annotated QA pairs from\nimages and video clips, meticulously curated from over 200 acclaimed\n(predominantly Oscar-nominated) films and spanning eight key cinematography\ndimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their\nsubstantial limitations: even the top-performing model achieves less than 60%\naverage accuracy, particularly struggling with fine-grained visual cues and\ncomplex spatial reasoning. To catalyze advancement in this domain, we construct\nShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic\nQA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning\nand Group Relative Policy Optimization. ShotVL significantly outperforms all\nexisting open-source and proprietary models on ShotBench, establishing new\nstate-of-the-art performance. We open-source our models, data, and code to\nfoster rapid progress in this crucial area of AI-driven cinematic understanding\nand generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21356.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652965773a416e1f2173443b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
      "fullname": "Yuhao Dong",
      "name": "THUdyh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20279",
      "authors": [
        {
          "_id": "686218679e7509383d29abb3",
          "name": "Changliang Xia",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb4",
          "name": "Chengyou Jia",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb5",
          "name": "Zhuohang Dang",
          "hidden": false
        },
        {
          "_id": "686218679e7509383d29abb6",
          "name": "Minnan Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T09:40:50.000Z",
      "submittedOnDailyAt": "2025-06-30T03:24:47.090Z",
      "title": "Ideales hacia la realidad: predicción concentrada de unificación eficiente de datos en el escaneo de la realidad.",
      "submittedOnDailyBy": {
        "_id": "6602548a68d519ed324b47c5",
        "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
        "isPro": false,
        "fullname": "ChengyouJia",
        "user": "ChengyouJia",
        "type": "user"
      },
      "summary": "Predicción densa desempeña un papel importante en el campo de la visión computacional, con el objetivo de entrenar la etiquetado de cada píxel de la imagen de entrada. Aunque ha evolucionado, los métodos actuales se centran en condiciones ideales y tienen una capacidad de generalización limitada en escenarios reales, así como en la falta de datos. Para estudiar estos problemas de manera sistemática, se presenta el benchmark DenseWorld. DenseWorld constituye una amplia gama de tareas de predicción densa que corresponden a 25 aplicaciones reales efectivas y tiene la característica de una evaluación unificada. A continuación, se propone el modelo DenseDiT. DenseDiT se centra en maximizar la utilización de los límites visuales de los modelos generativos, con el objetivo de realizar diversas tareas de predicción densa de manera uniforme. DenseDiT combina una estructura de reutilización de parámetros y dos ramas ligeras, funcionando con menos de 0.1% de parámetros adicionales. En la evaluación en DenseWorld, DenseDiT reduce significativamente el rendimiento de los baselines generales y muestra sus límites en la capacidad de generalización en escenarios reales. Por otro lado, DenseDiT obtiene resultados excelentes utilizando menos del 0.01% de los datos de entrenamiento, destacando la valoración práctica de sus características reales. Los datos, chekpoints y código están disponibles en https://xcltql666.github.io/DenseDiTProj.",
      "upvotes": 13,
      "discussionId": "686218689e7509383d29abb7",
      "projectPage": "https://xcltql666.github.io/DenseDiTProj/",
      "githubRepo": "https://github.com/xcltql666/DenseDiT",
      "ai_summary": "DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.",
      "ai_keywords": [
        "dense prediction",
        "generative models",
        "visual priors",
        "parameter-reuse mechanism",
        "lightweight branches",
        "multi-scale context",
        "DenseWorld",
        "DenseDiT"
      ],
      "githubStars": 15
    },
    "publishedAt": "2025-06-25T05:40:50.000Z",
    "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios",
    "summary": "Dense prediction tasks hold significant importance of computer vision, aiming\nto learn pixel-wise annotated label for an input image. Despite advances in\nthis field, existing methods primarily focus on idealized conditions, with\nlimited generalization to real-world scenarios and facing the challenging\nscarcity of real-world data. To systematically study this problem, we first\nintroduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction\ntasks that correspond to urgent real-world applications, featuring unified\nevaluation across tasks. Then, we propose DenseDiT, which maximally exploits\ngenerative models' visual priors to perform diverse real-world dense prediction\ntasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism\nand two lightweight branches that adaptively integrate multi-scale context,\nworking with less than 0.1% additional parameters. Evaluations on DenseWorld\nreveal significant performance drops in existing general and specialized\nbaselines, highlighting their limited real-world generalization. In contrast,\nDenseDiT achieves superior results using less than 0.01% training data of\nbaselines, underscoring its practical value for real-world deployment. Our\ndata, and checkpoints and codes are available at\nhttps://xcltql666.github.io/DenseDiTProj",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602548a68d519ed324b47c5",
      "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
      "fullname": "ChengyouJia",
      "name": "ChengyouJia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22434",
      "authors": [
        {
          "_id": "686205ad9e7509383d29ab80",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab81",
          "name": "Mingkang Zhu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab82",
          "name": "Shaoteng Liu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab83",
          "name": "Xiaoyang Wu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab84",
          "name": "Xiaogang Xu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab85",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab86",
          "name": "Xiang Bai",
          "hidden": false
        },
        {
          "_id": "686205ad9e7509383d29ab87",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T17:59:27.000Z",
      "submittedOnDailyAt": "2025-06-30T02:04:48.511Z",
      "title": "Comparación de imágenes múltiples para la descripción visual reforzada",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "Este estudio investiga métodos para facilitar la inferencia de la Cadena de Pensamiento (CoT) entre varias imágenes. Una solución intuitiva podría ser el aprendizaje de reglas basado en el aprendizaje por refuerzo aplicado a modelos de lenguaje visión-lenguaje (VLMs). Sin embargo, estos métodos suelen basarse en pares de preguntas y respuestas automáticamente preparados, lo que dificulta particularmente el tratamiento de detalles visuales complejos de las imágenes o la procesamiento lógico entre imágenes. Al referirse a un modelo de aprendizaje de representaciones visuales automáticamente ajustadas, se ha descubierto que las imágenes tienen restricciones implícitas. Según esta guía, se construyeron tuplas de imágenes que consisten en dos expansiones visuales de la misma imagen y una tercera imagen similar. Durante el proceso de entrenamiento, se promueve a el modelo que genere razones para comparar estas imágenes (es decir, determinar si son las mismas o diferentes). Luego, se optimiza el modelo utilizando el aprendizaje por refuerzo basado en reglas. Debido a la alta similitud visual y la existencia de la expansión, el modelo debe interesarse por pequeños cambios visuales y ejecutar lógicas. Los experimentos muestran que el modelo aprende por qué se comparan las imágenes en tareas de comparación visual, lo que se generaliza a una amplia gama de preguntas. Sin depender de pares de preguntas y respuestas registradas por humanos, este método logra mejoras claras en benchmarks de procesamiento de razonamiento entre múltiples imágenes y muestra un rendimiento fuerte en tareas visuales generales.",
      "upvotes": 8,
      "discussionId": "686205ad9e7509383d29ab88",
      "ai_summary": "Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.",
      "ai_keywords": [
        "Vision-Language Models",
        "self-supervised learning",
        "image triplets",
        "reasoning ability",
        "multi-image reasoning benchmarks",
        "general vision tasks"
      ]
    },
    "publishedAt": "2025-06-27T13:59:27.000Z",
    "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
    "summary": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual\ncues across multiple images. A straightforward solution is to adapt rule-based\nreinforcement learning for Vision-Language Models (VLMs). However, such methods\ntypically rely on manually curated question-answer pairs, which can be\nparticularly challenging when dealing with fine grained visual details and\ncomplex logic across images. Inspired by self-supervised visual representation\nlearning, we observe that images contain inherent constraints that can serve as\nsupervision. Based on this insight, we construct image triplets comprising two\naugmented views of the same image and a third, similar but distinct image.\nDuring training, the model is prompted to generate a reasoning process to\ncompare these images (i.e., determine same or different). Then we optimize the\nmodel with rule-based reinforcement learning. Due to the high visual similarity\nand the presence of augmentations, the model must attend to subtle visual\nchanges and perform logical reasoning to succeed. Experiments show that,\nalthough trained solely on visual comparison tasks, the learned reasoning\nability generalizes effectively to a wide range of questions. Without relying\non any human-annotated question-answer pairs, our method achieves significant\nimprovements on multi-image reasoning benchmarks and shows strong performance\non general vision tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22434.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21656",
      "authors": [
        {
          "_id": "6861f2b89e7509383d29ab35",
          "name": "Yifan Shen",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab36",
          "name": "Yuanzhe Liu",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab37",
          "name": "Jingyuan Zhu",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab38",
          "name": "Xu Cao",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab39",
          "name": "Xiaofeng Zhang",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3a",
          "name": "Yixiao He",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3b",
          "name": "Wenming Ye",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3c",
          "name": "James Matthew Rehg",
          "hidden": false
        },
        {
          "_id": "6861f2b89e7509383d29ab3d",
          "name": "Ismini Lourentzou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T18:00:00.000Z",
      "submittedOnDailyAt": "2025-06-30T00:44:37.025Z",
      "title": "Fine-Grained Preference Optimization se utiliza para mejorar la capacidad de reconocimiento espacial de los VLMs.",
      "submittedOnDailyBy": {
        "_id": "65e387095132c2edd193ae49",
        "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
        "isPro": false,
        "fullname": "Yifan Shen",
        "user": "SivanSX",
        "type": "user"
      },
      "summary": "Los modelos actuales de lenguaje visual (VLMs) son particularmente afectados cuando se requieren lógicas en varias etapas y una precisa alineación espacial. En este estudio, se presenta SpatialReasoner-R1, un modelo de lógica visual que busca resolver esta limitación. Para establecer normas de alta calidad en lógica espacial, se diseñó el método de búsqueda de árboles M3CTS con múltiples modelos, y se generaron trayectorias lógicas de conceptos largos y continuos. Además, se propone la optimización directa de fines físicos (fDPO), que introduce la granularidad directa y la naturaleza de los fines de la lógica observacional y espacial, y se evalúan respuestas candidatas basándose en la compensación espacial. Los resultados de los experimentos muestran que fDPO mejora en un promedio del 4.1% en tareas de calidad espacial y del 9.0% en tareas de cantidad espacial. Con fDPO, SpatialReasoner-R1 establece un nuevo estandarte de referencia en SPATIALRGPT-Bench, mejorando la precisión promedio en un 9.8% frente a los mejores estándares, y mantiene una competencia en tareas generales de lenguaje visual.",
      "upvotes": 6,
      "discussionId": "6861f2b99e7509383d29ab3e",
      "ai_summary": "SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.",
      "ai_keywords": [
        "vision-language models",
        "SpatialReasoner-R1",
        "Multi-Model Monte Carlo Tree Search",
        "M3CTS",
        "Long Chain-of-Thought",
        "LongCoT",
        "fine-grained Direct Preference Optimization",
        "fDPO",
        "segment-specific preference granularity",
        "descriptive grounding",
        "logical reasoning",
        "spatial reward mechanism",
        "visual consistency",
        "spatial grounding",
        "logical coherence",
        "SPATIALRGPT-Bench"
      ]
    },
    "publishedAt": "2025-06-26T14:00:00.000Z",
    "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
    "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e387095132c2edd193ae49",
      "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
      "fullname": "Yifan Shen",
      "name": "SivanSX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21628",
      "authors": [
        {
          "_id": "686261739e7509383d29ac6e",
          "name": "Magnus Dierking",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac6f",
          "name": "Christopher E. Mower",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac70",
          "name": "Sarthak Das",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac71",
          "name": "Huang Helong",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac72",
          "name": "Jiacheng Qiu",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac73",
          "name": "Cody Reading",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac74",
          "name": "Wei Chen",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac75",
          "name": "Huidong Liang",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac76",
          "name": "Huang Guowei",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac77",
          "name": "Jan Peters",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac78",
          "name": "Quan Xingyue",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac79",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "686261739e7509383d29ac7a",
          "name": "Haitham Bou-Ammar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T20:23:39.000Z",
      "submittedOnDailyAt": "2025-06-30T08:38:41.700Z",
      "title": "Ark: Framework de Python abierto para el aprendizaje de robots",
      "submittedOnDailyBy": {
        "_id": "631c375768f7da9ad2496bf6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg",
        "isPro": false,
        "fullname": "Haitham Bou Ammar",
        "user": "hba123",
        "type": "user"
      },
      "summary": "La tecnología de robótica ha experimentado un desarrollo sorprendente en hardware. Incluye eventos como el desafío de ciudades y robotes de la DARPA, y el primer concurso de robots humanoides de CAPBOCKSING. Sin embargo, la irregularidad comercial no puede ser atrapada por el desarrollo de aprendizaje automático. El software como bloque lógico forma grandes bloques. El actual stack de robots exige altas curvas de aprendizaje, conocimiento avanzado en C/C++, distribución de entrenamiento y un arquitectura compleja de hardware. Esto contrasta con el ecosistema detallado centrado en Python que impulsa la IA moderna. Se presenta ARK, un marco de referencia de tecnología robótica abierto, que prioriza Python. ARK utiliza algoritmos de aprendizaje avanzados (como ACT y Diffusion Policy) para la recopilación y preprocesamiento de datos, ofreciendo una interfaz de entorno tipo Gym para una transición sin interrupciones entre simulación precisa y robotes físicos. Su arquitectura ligera de cliente-servidor proporciona proveedores de red y comunicación sub-sub, con opciones de binding en C/C++ para garantizar rendimiento real-time. ARK incluye módulos reutilizables para control, SLAM, planificación de movimiento, identificación de sistemas y visualización, además de la naturaleza intercambiable de ROS. Se muestran detalles y estudios de casos, incluyendo prototipos de búsqueda y movimiento, intercambio de hardware sencillo, pipelines construidos en puntos de entrada y una conveniencia comparable al flujo de trabajo predominante de aprendizaje automático. ARK integra la práctica de robótica y IA en Python compartido, acelera el estudio de robots autónomos y el empleo comercial, y reduce el valor de entrada.",
      "upvotes": 4,
      "discussionId": "686261739e7509383d29ac7b",
      "ai_summary": "ARK is an open-source Python-first framework that integrates modern imitation-learning algorithms and seamless simulation-physical robot interactions to simplify robotics development and deployment.",
      "ai_keywords": [
        "Gym-style environment interface",
        "imitation-learning algorithms",
        "ACT",
        "Diffusion Policy",
        "lightweight client-server architecture",
        "publisher-subscriber communication",
        "reusable modules",
        "control",
        "SLAM",
        "motion planning",
        "system identification",
        "visualization",
        "native ROS interoperability",
        "end-to-end pipelines"
      ]
    },
    "publishedAt": "2025-06-24T16:23:39.000Z",
    "title": "Ark: An Open-source Python-based Framework for Robot Learning",
    "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21628.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631c375768f7da9ad2496bf6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg",
      "fullname": "Haitham Bou Ammar",
      "name": "hba123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19741",
      "authors": [
        {
          "_id": "686209869e7509383d29ab92",
          "name": "Yihong Luo",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab93",
          "name": "Shuchen Xue",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab94",
          "name": "Tianyang Hu",
          "hidden": false
        },
        {
          "_id": "686209869e7509383d29ab95",
          "name": "Jing Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T15:58:55.000Z",
      "submittedOnDailyAt": "2025-06-30T02:20:48.977Z",
      "title": "\"Consistency Training: An Additional Controlled Learning Approach in a Single-Stage Generator's Native Access Method\"",
      "submittedOnDailyBy": {
        "_id": "65f7e6856bd4bac5b6a4ecc3",
        "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
        "isPro": false,
        "fullname": "Yihong Luo",
        "user": "Luo-Yihong",
        "type": "user"
      },
      "summary": "La optimización y la posibilidad de control en la generación de contenido de alta calidad son un problema esencial de la creación de contenido generado por la inteligencia artificial (AIGC). Un generador de un paso proporciona una alta calidad de generación y eficiencia computacional a través de la técnica de evaporación de calor, pero presenta grandes problemas para adaptarse a nuevos condiciones de control (restricciones estructurales, guías gramaticales, entradas externas, etc.). Los métodos tradicionales requieren una alta cantidad de cálculos y la necesidad de un evaporador de calor posterior. En este artículo, se presenta un nuevo y ligero enfoque llamado Entrenamiento de Noise Consistency Training (NCT), que permite la aplicación directa de nuevos controles. NCT no requiere acceso a imágenes de entrenamiento o reentrenamiento del modelo de evaporación de calor. Se inserta un módulo adaptativo y se implementa un pérdida de consistencia de ruido en el espacio de ruido del generador. Esta pérdida ajusta la acción de generación del modelo adaptativo a diferentes niveles de ruido condicionado y guia claramente a nuevas condiciones de control. Teóricamente, este objetivo de entrenamiento puede entenderse como minimizar la diferencia de distribución entre el modelo adaptativo y la distribución condicionada por nuevas condiciones. NCT es modular, eficiente en datos y fácil de implementar, dependiendo de un generador de un paso preprocesado y un modelo de control. Los experimentos detallados muestran que NCT realiza la generación controlable más avanzada en un paso y supera los métodos basados en múltiples pasos o evaporación de calor en términos de calidad de generación y eficiencia computacional. El código está disponible en https://github.com/Luo-Yihong/NCT.",
      "upvotes": 4,
      "discussionId": "686209869e7509383d29ab96",
      "ai_summary": "A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.",
      "ai_keywords": [
        "diffusion distillation",
        "Noise Consistency Training",
        "NCT",
        "one-step generators",
        "adapter module",
        "noise consistency loss",
        "noise space",
        "conditional distribution",
        "generative modeling",
        "data-efficient",
        "computational efficiency"
      ]
    },
    "publishedAt": "2025-06-24T11:58:55.000Z",
    "title": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls",
    "summary": "The pursuit of efficient and controllable high-quality content generation\nremains a central challenge in artificial intelligence-generated content\n(AIGC). While one-step generators, enabled by diffusion distillation\ntechniques, offer excellent generation quality and computational efficiency,\nadapting them to new control conditions--such as structural constraints,\nsemantic guidelines, or external inputs--poses a significant challenge.\nConventional approaches often necessitate computationally expensive\nmodifications to the base model and subsequent diffusion distillation. This\npaper introduces Noise Consistency Training (NCT), a novel and lightweight\napproach to directly integrate new control signals into pre-trained one-step\ngenerators without requiring access to original training images or retraining\nthe base diffusion model. NCT operates by introducing an adapter module and\nemploys a noise consistency loss in the noise space of the generator. This loss\naligns the adapted model's generation behavior across noises that are\nconditionally dependent to varying degrees, implicitly guiding it to adhere to\nthe new control. Theoretically, this training objective can be understood as\nminimizing the distributional distance between the adapted generator and the\nconditional distribution induced by the new conditions. NCT is modular,\ndata-efficient, and easily deployable, relying only on the pre-trained one-step\ngenerator and a control signal model. Extensive experiments demonstrate that\nNCT achieves state-of-the-art controllable generation in a single forward pass,\nsurpassing existing multi-step and distillation-based methods in both\ngeneration quality and computational efficiency. Code is available at\nhttps://github.com/Luo-Yihong/NCT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19741.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f7e6856bd4bac5b6a4ecc3",
      "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
      "fullname": "Yihong Luo",
      "name": "Luo-Yihong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21411",
      "authors": [
        {
          "_id": "686237f69e7509383d29abe9",
          "user": {
            "_id": "64d5deb154bb9eb704f83122",
            "avatarUrl": "/avatars/86ce09bcca903319051e2307581a43f4.svg",
            "isPro": false,
            "fullname": "Yehui Tang",
            "user": "tangyehui",
            "type": "user"
          },
          "name": "Yehui Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:35.262Z",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abea",
          "name": "Xiaosong Li",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abeb",
          "user": {
            "_id": "64b78295479b934973e2c40e",
            "avatarUrl": "/avatars/9213e385964132fa50859264a838d891.svg",
            "isPro": false,
            "fullname": "liu",
            "user": "Fangcheng2",
            "type": "user"
          },
          "name": "Fangcheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:52.170Z",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abec",
          "name": "Wei Guo",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abed",
          "name": "Hang Zhou",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abee",
          "name": "Yaoyuan Wang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abef",
          "name": "Kai Han",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf0",
          "name": "Xianzhi Yu",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf1",
          "name": "Jinpeng Li",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf2",
          "name": "Hui Zang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf3",
          "name": "Fei Mi",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf4",
          "name": "Xiaojun Meng",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf5",
          "name": "Zhicheng Liu",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf6",
          "name": "Hanting Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf7",
          "name": "Binfan Zheng",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf8",
          "name": "Can Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abf9",
          "name": "Youliang Yan",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfa",
          "name": "Ruiming Tang",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfb",
          "name": "Peifeng Qin",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfc",
          "name": "Xinghao Chen",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfd",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "686237f69e7509383d29abfe",
          "user": {
            "_id": "658bdf7b925aadd43304f05c",
            "avatarUrl": "/avatars/64d9e9dea27c376c3bc7b2a54efc2a46.svg",
            "isPro": false,
            "fullname": "Yunhe Wang",
            "user": "MightyCrane",
            "type": "user"
          },
          "name": "Yunhe Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-30T07:11:59.146Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T16:40:21.000Z",
      "submittedOnDailyAt": "2025-06-30T05:41:23.309Z",
      "title": "Pangu Pro MoE: Eficiencia por la confusión de expertos por grupos de la hipérsparsity",
      "submittedOnDailyBy": {
        "_id": "63a369d98c0c89dcae3b8329",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
        "isPro": true,
        "fullname": "Adina Yakefu",
        "user": "AdinaY",
        "type": "user"
      },
      "summary": "El modelo de Expertos Mixtos (MoE) ha surgido en los modelos de lenguaje grandes, con un aumento significativo en el número de parámetros y en la capacidad de entrenamiento, lo que sugiere una reducción pequeña en los costos de ejecución. Sin embargo, se observa que la frecuencia de activación de estos Expertos es sesgada, y cuando se ejecutan en dispositivos diferentes de manera paralela, la eficiencia del sistema suele disminuir. En respuesta a esto, se introduce el modelo de Expertos Mixtos de Grupos (MoGE), que muestra cómo se puede equilibrar la carga de los Expertos al seleccionarlos en grupos, lo que proporciona una distribución más equitativa de la carga en comparación con MoE. MoGE activa un número igual de Expertos dentro de cada grupo de Expertos reservados y, en caso de que la ejecución del modelo se distribuya entre varios dispositivos, esta arquitectura garantiza que se mantenga un equilibrio en el carga de cálculo entre los dispositivos, especialmente mejorando significativamente la ejecución en la fase de inferencia. Además, se ha construido el MoE de Pangu Pro en NPUs Ascend, y se ha implementado un modelo raro basado en MoGE que satisface un total de 720 billones de parámetros y una cantidad de parámetros activos en cada token de 160 billones. El diseño de Pangu Pro MoE se ha optimizado a través de simulaciones de sistemas ampliados con Ascend 300I Duo y 800I A2. Los resultados de los experimentos muestran que MoGE mejora aún más la distribución de la carga de los Expertos en la entrenamiento y la inferencia, logrando una ejecución eficiente. La eficiencia de Pangu Pro MoE en la inferencia alcanza 1148 tokens/segundo por cada tarjeta, y con la aceleración espectral, se alcanza hasta 1528 tokens/segundo, superando los modelos densos. Además, el rendimiento de costo-eficiencia de la inferencia en Ascend 300I Duo es muy bueno, y Pangu Pro MoE es capaz de entrenarse utilizando la Magic Space Parallelization, ocupando una posición de liderazgo en el clase de modelos de 100 billones de parámetros, superando modelos abiertos como GLM-Z1-32B y Qwen3-32B.",
      "upvotes": 4,
      "discussionId": "686237f79e7509383d29abff",
      "ai_summary": "Mixture of Grouped Experts (MoGE) improves expert load balancing and execution efficiency for large language models, enhancing throughput and cost-to-performance on Ascend NPUs.",
      "ai_keywords": [
        "Mixture of Experts (MoE)",
        "Mixture of Grouped Experts (MoGE)",
        "large language models",
        "expert load balancing",
        "computational load",
        "inference phase",
        "sparse model",
        "Ascend NPUs",
        "system simulation",
        "speculative acceleration",
        "Dense models",
        "GLM-Z1-32B",
        "Qwen3-32B"
      ]
    },
    "publishedAt": "2025-05-27T12:40:21.000Z",
    "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
    "summary": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo. Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 774
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22419",
      "authors": [
        {
          "_id": "686229249e7509383d29abd0",
          "name": "Bingchen Zhao",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd1",
          "name": "Despoina Magka",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd2",
          "name": "Minqi Jiang",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd3",
          "name": "Xian Li",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd4",
          "name": "Roberta Raileanu",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd5",
          "name": "Tatiana Shavrina",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd6",
          "name": "Jean-Christophe Gagnon-Audet",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd7",
          "name": "Kelvin Niu",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd8",
          "name": "Shagun Sodhani",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abd9",
          "name": "Michael Shvartsman",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abda",
          "name": "Andrei Lupu",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdb",
          "name": "Alisia Lupidi",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdc",
          "name": "Edan Toledo",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdd",
          "name": "Karen Hambardzumyan",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abde",
          "name": "Martin Josifoski",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abdf",
          "name": "Thomas Foster",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe0",
          "name": "Lucia Cipolina-Kun",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe1",
          "name": "Abhishek Charnalia",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe2",
          "name": "Derek Dunfield",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe3",
          "name": "Alexander H. Miller",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe4",
          "name": "Oisin Mac Aodha",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe5",
          "name": "Jakob Foerster",
          "hidden": false
        },
        {
          "_id": "686229249e7509383d29abe6",
          "name": "Yoram Bachrach",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T17:44:32.000Z",
      "submittedOnDailyAt": "2025-06-30T06:29:15.385Z",
      "title": "Automatización de la velocidad de entrenamiento de LLM: Replicación y mejora de NanoGPT",
      "submittedOnDailyBy": {
        "_id": "62dcd71075e9787ec5aa41ba",
        "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
        "isPro": true,
        "fullname": "Bingchen Zhao",
        "user": "tennant",
        "type": "user"
      },
      "summary": "El rápido desarrollo de los modelos de lenguaje grande (LLMs) puede ser de gran ayuda para el desarrollo científico. Una de las capacidades importantes de este es el poder de recrear actividades existentes. Para evaluar la capacidad de recrear resultados en diversas áreas de investigación, utilizamos la contribución de la comunidad de NanoGPT speedrun para introducir un marco de referencia automatizado de speedrunning de LLMs. En este concurso, el objetivo es entrenar el modelo GPT-2 en el menor tiempo posible. Para cada de las 19 tareas de speedrun, la entidad recibe los anteriores registros y scripts de entrenamiento, y puede optar por recibir una de tres formatos de pistas. Estas pistas pueden ser descritas como código de lenguaje de programación, similar a un artículo. Los registros se ejecutan de manera diseñada para ser rápidos, y la mejora en los speedruns incluye desarrollos algoritmicos avanzados hasta optimizaciones relacionadas con el hardware, así como cambios en diferentes niveles de código. Estas capacidades demuestran el valor y la utilidad de mejorar el entrenamiento de los LLMs, pero también que la tecnología necesaria, aunque necesaria, no es suficiente. La combinación de modelos recientes y los mejores de la técnica (SoTA) es difícil de recrear, incluso con pistas disponibles, en nuestro marco de referencia. Este marco de referencia muestra que la capacidad de recrear ciencia científica automáticamente en los LLMs es simple, pero no suficiente.",
      "upvotes": 2,
      "discussionId": "686229249e7509383d29abe7",
      "ai_summary": "An Automated LLM Speedrunning Benchmark evaluates AI agents' ability to reproduce scientific results by leveraging NanoGPT speedrun tasks, indicating that even recent reasoning LLMs struggle with re-implementing known improvements.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "AI agents",
        "Automated LLM Speedrunning Benchmark",
        "NanoGPT speedrun",
        "GPT-2",
        "high-level algorithmic advancements",
        "hardware-aware optimizations"
      ]
    },
    "publishedAt": "2025-06-27T13:44:32.000Z",
    "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements",
    "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62dcd71075e9787ec5aa41ba",
      "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
      "fullname": "Bingchen Zhao",
      "name": "tennant",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21594",
      "authors": [
        {
          "_id": "68625a4c9e7509383d29ac4c",
          "name": "Ahmed M. Adly",
          "hidden": false
        },
        {
          "_id": "68625a4c9e7509383d29ac4d",
          "name": "Mostafa Samy",
          "hidden": false
        },
        {
          "_id": "68625a4c9e7509383d29ac4e",
          "name": "Amr Fawzy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T09:44:21.000Z",
      "submittedOnDailyAt": "2025-06-30T08:07:09.494Z",
      "title": "Gazelle-R1: El más avanzado en lógica médica alcanzado con parámetros eficientes de entrenamiento en dos etapas",
      "submittedOnDailyBy": {
        "_id": "63aca106e3b217fb36cf1950",
        "avatarUrl": "/avatars/b37cc9102f875b6ce0c55a294c052078.svg",
        "isPro": false,
        "fullname": "Ahmed Mostafa",
        "user": "AhmedMostafa",
        "type": "user"
      },
      "summary": "GAZAL-R1 se presenta como un modelo de lenguaje con 320 billones de parámetros. Este modelo alcanza el mejor rendimiento en lógica médica y proporciona una explicación paso a paso para la toma de decisiones clínicas. Basado en Qwen3 32B, GAZAL-R1 ha demostrado que un modelo de tamaño intermedio puede superar a modelos mucho más grandes en especialidades específicas a través de una entrenamiento estratégico. Se desarrolló un nuevo proceso de entrenamiento en dos etapas. Primero, se realizó una ajuste de sub-promoción mediante una colección de 107,033 ejemplos de lógica médica sintética, enseñando pensamiento clínico estructurado y agregando técnicas eficientes en parámetros como Weight-Decomposed Low-Rank Adaptation (DoRA) y Rank-Stabilized LoRA (rsLoRA). Luego, se utilizó el Group Relative Policy Optimization (GRPO) para realizar aprendizaje por refuerzo y se implementó un complejo sistema de entrenamiento multi-componente que mejora la precisión, la conformidad formal y la calidad lógica. GAZAL-R1 ha registrado un excelente rendimiento en benchmarks médicos, alcanzando 87.1% en MedQA, 81.6% en MMLU Pro (Medicina) y 79.6% en PubMedQA, superando significativamente modelos mucho más grandes. Este estudio ofrece una profunda comprensión de los desafíos en el entrenamiento de modelos de lógica en especialidades, explicando problemas como la instabilidad del entrenamiento, la memoria factual y la tensión básica entre detalles lógicos. Nuestro enfoque proporciona un marco reproducible para el desarrollo de modelos de lenguaje de alto rendimiento en especialidades, equilibrando rendimiento, eficiencia y explicabilidad.",
      "upvotes": 2,
      "discussionId": "68625a4c9e7509383d29ac4f",
      "ai_summary": "Gazal-R1, a 32-billion-parameter language model, achieves top performance in medical reasoning through strategic training, including advanced parameter-efficient techniques and reinforcement learning, providing detailed explanations for clinical decisions.",
      "ai_keywords": [
        "Weight-Decomposed Low-Rank Adaptation (DoRA)",
        "Rank-Stabilized LoRA (rsLoRA)",
        "Group Relative Policy Optimization (GRPO)",
        "MedQA",
        "MMLU Pro (Medical)",
        "PubMedQA",
        "reasoning-capable models",
        "reward hacking",
        "training instability"
      ]
    },
    "publishedAt": "2025-06-18T05:44:21.000Z",
    "title": "Gazal-R1: Achieving State-of-the-Art Medical Reasoning with\n  Parameter-Efficient Two-Stage Training",
    "summary": "We present Gazal-R1, a 32-billion-parameter language model that achieves\nstate-of-the-art performance in medical reasoning while providing transparent,\nstep-by-step explanations for clinical decision-making. Built upon Qwen3 32B,\nour model demonstrates that strategic training can enable mid-sized models to\noutperform significantly larger counterparts in specialized domains. We\ndeveloped a novel two-stage training pipeline: first, supervised fine-tuning on\na carefully curated dataset of 107,033 synthetic medical reasoning examples\nthat teaches structured clinical thinking, enhanced by advanced\nparameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation\n(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using\nGroup Relative Policy Optimization (GRPO) with a sophisticated multi-component\nreward system that refines accuracy, format adherence, and reasoning quality.\nGazal-R1 achieves exceptional performance across medical benchmarks, scoring\n87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing\nmodels up to 12x larger. Beyond its strong empirical results, this work\nprovides detailed insights into the challenges of training reasoning-capable\nmodels in specialized domains, including issues with reward hacking, training\ninstability, and the fundamental tension between factual recall and detailed\nreasoning. Our methodology offers a reproducible framework for developing\nhigh-capability, domain-specific language models that balance performance,\nefficiency, and explainability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21594.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63aca106e3b217fb36cf1950",
      "avatarUrl": "/avatars/b37cc9102f875b6ce0c55a294c052078.svg",
      "fullname": "Ahmed Mostafa",
      "name": "AhmedMostafa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22149",
      "authors": [
        {
          "_id": "68625f5d9e7509383d29ac62",
          "name": "Ronald Fecso",
          "hidden": false
        },
        {
          "_id": "68625f5d9e7509383d29ac63",
          "name": "José Morano",
          "hidden": false
        },
        {
          "_id": "68625f5d9e7509383d29ac64",
          "name": "Ursula Schmidt-Erfurth",
          "hidden": false
        },
        {
          "_id": "68625f5d9e7509383d29ac65",
          "name": "Hrvoje Bogunović",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-27T11:53:54.000Z",
      "submittedOnDailyAt": "2025-06-30T08:31:01.610Z",
      "title": "RetFiner: Estrategia de mejora del lenguaje visual basada en modelos de capa de leche",
      "submittedOnDailyBy": {
        "_id": "655b3383ed8df831286969f0",
        "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
        "isPro": false,
        "fullname": "José Morano",
        "user": "j-morano",
        "type": "user"
      },
      "summary": "El desarrollo de la tecnología de incentivos vídeo y del aprendizaje profundo (DL) ha permitido a los médicos y investigadores clasificar las enfermedades de la membrana limitante en tiempo real. La popularidad del aprendizaje basado en privacidad (SSL) para el entrenamiento de la red neuronal es notable, ya que permite reducir los costos al utilizar datos sin etiquetas. Aunque SSL ha facilitado el desarrollo de modelos básicos (FM), los actuales FM para OCT están entrenados solo con datos de imagen, lo que limita su comprensión de los detalles de las imágenes. Esto se manifiesta claramente en la performance de los datos de entrenamiento, especialmente en tareas complejas. Estos FM deben ser ajustados para ser efectivos en aplicaciones específicas o poblaciones, lo que requiere retroalimentación (aunque en algunos casos puede ser impracticable). Para resolver estos problemas, proponemos RetFiner. RetFiner es un ajuste de lenguaje visual con aprendizaje basado en privacidad, que mejora la representación de los FM actuales y los aplica de manera eficiente a poblaciones específicas. Nuestro método utiliza señales ricas de subobjetos de datos de texto ricos para entrenar diferentes objetivos. RetFiner ha sido validado para RETFound, UrFound y VisionFM de OCT, y ha demostrado un significativo aumento en la performance de aprendizaje lineal en 7 tareas muy diversas de clasificación de OCT. En promedio, se observa un aumento del 5.8, 3.9 y 2.1 puntos porcentuales respecto a los datos de referencia. Nuestro código y los pesos del modelo están disponibles en https://github.com/ronnief1/RetFiner.",
      "upvotes": 1,
      "discussionId": "68625f5d9e7509383d29ac66",
      "githubRepo": "https://github.com/ronnief1/RetFiner",
      "ai_summary": "RetFiner, a vision-language refinement scheme, enhances self-supervised foundation models for OCT by leveraging textual data, improving their downstream performance in retinal disease classification tasks.",
      "ai_keywords": [
        "optical coherence tomography (OCT)",
        "deep learning (DL)",
        "self-supervised learning (SSL)",
        "foundation models (FMs)",
        "supervised fine-tuning",
        "RetFiner",
        "vision-language refinement",
        "linear probing performance"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-06-27T07:53:54.000Z",
    "title": "RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation\n  Models",
    "summary": "The rise of imaging techniques such as optical coherence tomography (OCT) and\nadvances in deep learning (DL) have enabled clinicians and researchers to\nstreamline retinal disease staging. A popular DL approach is self-supervised\nlearning (SSL), where models learn from vast amounts of unlabeled data,\navoiding costly annotation. SSL has allowed the development of foundation\nmodels (FMs), large models that can be used for a variety of downstream tasks.\nHowever, existing FMs for OCT, trained solely on image data, lack a\ncomprehensive and robust semantic understanding of images, as evidenced by\ntheir downstream performance (especially for complex tasks), and thus require\nsupervised fine-tuning (which may be unfeasible) to better adapt to specific\napplications and populations. To address this, we propose RetFiner, an SSL\nvision-language refinement scheme that improves the representations of existing\nFMs and enables their efficient and direct adaptation to specific populations\nfor improved downstream performance. Our method uses a diverse set of training\nobjectives which take advantage of the rich supervisory signal found in textual\ndata. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM,\nshowing significant improvements in linear probing performance on seven highly\ndiverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1\npercentage points over their baselines, respectively. Our code and model\nweights are publicly available at https://github.com/ronnief1/RetFiner.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22149.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655b3383ed8df831286969f0",
      "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
      "fullname": "José Morano",
      "name": "j-morano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]