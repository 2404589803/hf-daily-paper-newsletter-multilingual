[
  {
    "paper": {
      "id": "2504.13161",
      "authors": [
        {
          "_id": "6801d661ed5fc062197db592",
          "user": {
            "_id": "633bd54b00732349209a18fe",
            "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
            "isPro": false,
            "fullname": "Shizhe Diao",
            "user": "shizhediao",
            "type": "user"
          },
          "name": "Shizhe Diao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:38:46.555Z",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db593",
          "name": "Yu Yang",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db594",
          "name": "Yonggan Fu",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db595",
          "name": "Xin Dong",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db596",
          "name": "Dan Su",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db597",
          "name": "Markus Kliegl",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db598",
          "name": "Zijia Chen",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db599",
          "name": "Peter Belcak",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db59a",
          "name": "Yoshi Suhara",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db59b",
          "name": "Hongxu Yin",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db59c",
          "name": "Mostofa Patwary",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db59d",
          "name": "Yingyan",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db59e",
          "name": "Lin",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db59f",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "6801d661ed5fc062197db5a0",
          "name": "Pavlo Molchanov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:58:13.000Z",
      "submittedOnDailyAt": "2025-04-18T03:05:25.298Z",
      "title": "Clustering-based Iteration Data Micro-batched Bootstrapping Language Model Pre-training",
      "submittedOnDailyBy": {
        "_id": "633bd54b00732349209a18fe",
        "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
        "isPro": false,
        "fullname": "Shizhe Diao",
        "user": "shizhediao",
        "type": "user"
      },
      "summary": "Los conjuntos de datos de entrenamiento previamente limpiados se recopilan generalmente de contenido web y no tienen un dominio específico. Por ejemplo, el dataset comúnmente utilizado como Common Crawl no incluye etiquetas de dominio explícitas, mientras que el dataset manualmente etiquetado como The Pile es costoso en términos de trabajo humano. Por lo tanto, es difícil determinar la mejor mezcla de datos de entrenamiento, aunque este problema es crucial para mejorar significativamente el rendimiento del entrenamiento. A pesar de que este problema es difícil de resolver, hemos propuesto CLustering-based Iterative Data Mixture Bootstrapping (CLIMB). CLIMB es un marco de trabajo automatizado para encontrar, evaluar y mejorar la mezcla de datos de entrenamiento, especialmente, CLIMB embedea grandes conjuntos de datos en un espacio semántico y los divide mediante clustering, utilizando pequeños modelos profesionales y predictores para explorar múltiples mezclas óptimas. Esta mezcla se entrena continuamente con 400B tokens, y nuestro modelo de 1B supera a Llama-3.2-1B en un 2.0% adicional. Además, CLIMB puede optimizarse para dominios específicos, observando un mejoramiento del 5% en comparación con muestreo aleatorio, como en el caso de las ciencias sociales. Finalmente, se presentan ClimbLab y ClimbMix. ClimbLab ofrece un corpus filtrado de 1.2 trilion de tokens con 20 clústeres como un playland para la investigación, mientras que ClimbMix proporciona un conjunto de datos de aprendizaje eficiente con 400 billones de tokens, ofreciendo también altos rendimientos. La mezcla final de datos se analiza y se revelan las características de la mezcla de datos óptima. Nuestros datos están disponibles en el siguiente URL: https://research.nvidia.com/labs/lpr/climb/",
      "upvotes": 48,
      "discussionId": "6801d663ed5fc062197db631",
      "ai_keywords": [
        "CLIMB",
        "semantic space",
        "proxy model",
        "predictor",
        "ClimbLab",
        "ClimbMix"
      ]
    },
    "publishedAt": "2025-04-17T13:58:13.000Z",
    "title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training",
    "summary": "Pre-training datasets are typically collected from web content and lack\ninherent domain divisions. For instance, widely used datasets like Common Crawl\ndo not include explicit domain labels, while manually curating labeled datasets\nsuch as The Pile is labor-intensive. Consequently, identifying an optimal\npre-training data mixture remains a challenging problem, despite its\nsignificant benefits for pre-training performance. To address these challenges,\nwe propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an\nautomated framework that discovers, evaluates, and refines data mixtures in a\npre-training setting. Specifically, CLIMB embeds and clusters large-scale\ndatasets in a semantic space and then iteratively searches for optimal mixtures\nusing a smaller proxy model and a predictor. When continuously trained on 400B\ntokens with this mixture, our 1B model exceeds the state-of-the-art\nLlama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific\ndomain (e.g., Social Sciences) yields a 5% improvement over random sampling.\nFinally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20\nclusters as a research playground, and ClimbMix, a compact yet powerful\n400-billion-token dataset designed for efficient pre-training that delivers\nsuperior performance under an equal token budget. We analyze the final data\nmixture, elucidating the characteristics of an optimal data mixture. Our data\nis available at: https://research.nvidia.com/labs/lpr/climb/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13161.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633bd54b00732349209a18fe",
      "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
      "fullname": "Shizhe Diao",
      "name": "shizhediao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13146",
      "authors": [
        {
          "_id": "6801b77dcb758561ae26997e",
          "user": {
            "_id": "647c0564001553a39c38e79e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t5KF0Vp7kCk-5EZRWhG8i.jpeg",
            "isPro": false,
            "fullname": "Yash Savani",
            "user": "yashsavani",
            "type": "user"
          },
          "name": "Yash Savani",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:39:03.683Z",
          "hidden": false
        },
        {
          "_id": "6801b77dcb758561ae26997f",
          "name": "Asher Trockman",
          "hidden": false
        },
        {
          "_id": "6801b77dcb758561ae269980",
          "name": "Zhili Feng",
          "hidden": false
        },
        {
          "_id": "6801b77dcb758561ae269981",
          "name": "Avi Schwarzschild",
          "hidden": false
        },
        {
          "_id": "6801b77dcb758561ae269982",
          "user": {
            "_id": "63458a32d54fb141deda949d",
            "avatarUrl": "/avatars/fc4b59cd009075ac7987c6cdddbe3fea.svg",
            "isPro": false,
            "fullname": "Alex Robey",
            "user": "arobey1",
            "type": "user"
          },
          "name": "Alexander Robey",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:39:01.839Z",
          "hidden": false
        },
        {
          "_id": "6801b77dcb758561ae269983",
          "name": "Marc Finzi",
          "hidden": false
        },
        {
          "_id": "6801b77dcb758561ae269984",
          "name": "J. Zico Kolter",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:54:14.000Z",
      "submittedOnDailyAt": "2025-04-18T00:59:33.586Z",
      "title": "\"Sampling de Bendiostetil\"",
      "submittedOnDailyBy": {
        "_id": "6570917c0ea91e592aff0b8c",
        "avatarUrl": "/avatars/529e9713e6ac835e11599ea7070a9603.svg",
        "isPro": false,
        "fullname": "Avi Schwarzschild",
        "user": "schwarzschild",
        "type": "user"
      },
      "summary": "Los rastros de la teoría extendida generados por modelos avanzados son ricos en secuencias de tokens sin intención, lo que puede ayudar a hacer más precisos los resultados del modelo. Reconociendo estas debilidades, los propietarios de los modelos están explorando estrategias de muestreo que permitan mejorar la precisión de los resultados sin dañar el rendimiento del modelo. La muestración antidistiló es la solución perfecta a estas cuestiones. Modificando estratégicamente la distribución de probabilidades de los siguientes tokens, la muestración antidistiló desvanece los rastros de la teoría extendida, evitando que estos se conviertan en un factor negativo en la mejora de los resultados, y mantiene el papel efectivo del modelo. Para más detalles, consulte https://antidistillation.com.",
      "upvotes": 41,
      "discussionId": "6801b77ecb758561ae269a19",
      "projectPage": "https://antidistillation.com",
      "ai_keywords": [
        "antidistillation sampling",
        "next-token probability distribution",
        "reasoning traces"
      ]
    },
    "publishedAt": "2025-04-17T13:54:14.000Z",
    "title": "Antidistillation Sampling",
    "summary": "Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\nAntidistillation sampling provides exactly this capability. By\nstrategically modifying a model's next-token probability distribution,\nantidistillation sampling poisons reasoning traces, rendering them\nsignificantly less effective for distillation while preserving the model's\npractical utility. For further details, see https://antidistillation.com.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13146.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6570917c0ea91e592aff0b8c",
      "avatarUrl": "/avatars/529e9713e6ac835e11599ea7070a9603.svg",
      "fullname": "Avi Schwarzschild",
      "name": "schwarzschild",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12322",
      "authors": [
        {
          "_id": "6801d3de81552de84a537dd5",
          "user": {
            "_id": "652f9a74c22d404ebfa9f51d",
            "avatarUrl": "/avatars/8959d312b7c4c28952d4a26bb67f82ea.svg",
            "isPro": false,
            "fullname": "gaoxin",
            "user": "GX-XinGao",
            "type": "user"
          },
          "name": "Xin Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:38:50.872Z",
          "hidden": false
        },
        {
          "_id": "6801d3de81552de84a537dd6",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "6801d3de81552de84a537dd7",
          "name": "Zinan Tang",
          "hidden": false
        },
        {
          "_id": "6801d3de81552de84a537dd8",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "6801d3de81552de84a537dd9",
          "name": "Honglin Lin",
          "hidden": false
        },
        {
          "_id": "6801d3de81552de84a537dda",
          "name": "Jiang Wu",
          "hidden": false
        },
        {
          "_id": "6801d3de81552de84a537ddb",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "6801d3de81552de84a537ddc",
          "name": "Lijun Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T06:13:43.000Z",
      "submittedOnDailyAt": "2025-04-18T03:58:23.748Z",
      "title": "La estrategia de colaboración de pequeños LLMs se enfrenta al desafío de la síntesis de datos contra grandes LLMs",
      "submittedOnDailyBy": {
        "_id": "6397f6081323f19c578f142e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
        "isPro": false,
        "fullname": "QizhiPei",
        "user": "QizhiPei",
        "type": "user"
      },
      "summary": "La síntesis de datos y desidise son estratégias prometedoras para fortalecer modelos de lenguaje pequeños, pero los métodos actuales en uso implican altos costos computacionales, desafíos de sostenibilidad ambiental y potenciales sesgos en sistemas micro. En contraste, los modelos pequeños de LLMs enfrentan dificultades para generar datos de alta calidad, diversidad y confiabilidad. Proponemos una estructura GRA (Generative, Review, Adjudicate) basada en el proceso de colaboración con humanos (por ejemplo, revisión por parejas). La GRA permite que los modelos pequeños de LLMs desempeñen roles específicos, y que varios modelos pequeños colaboren para lograr una precisión y gestión de calidad equivalentes a los modelos de LLM de mayor tamaño a través de un proceso iterativo. En este marco colaborativo, varios modelos pequeños de LLMs asumen roles como generadores, revisores y arbitradores, simulando un proceso de síntesis de datos basado en revisión por parejas. Los generadores proponen muestras de datos iniciales, los revisores evalúan su calidad y diversidad, y los arbitradores resuelven conflictos para determinar el output final. Al decomposir el proceso de síntesis en tareas subespecíficas, los modelos pequeños de LLMs colaboran para lograr un equilibrio en la calidad de datos comparado con los modelos de LLM de mayor tamaño. Los datos generados a través de múltiples evaluaciones mostraron que pueden superar la calidad de los resultados de un solo modelo de LLM de gran tamaño, como Qwen-2.5-72B-Instruct. Nuestros resultados sugieren que la necesidad de un solo sistema micro para la síntesis de datos de alta calidad debe ser dudada, y proponemos una estrategia de colaboración estratégica de modelos pequeños. Nuestro dataset, modelo y código están disponibles en https://github.com/GX-XinGao/GRA.",
      "upvotes": 19,
      "discussionId": "6801d3df81552de84a537e20",
      "githubRepo": "https://github.com/GX-XinGao/GRA",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "multiple small LLMs involved framework",
        "GRA",
        "Generator",
        "Reviewer",
        "Adjudicator",
        "peer-review-inspired data synthesis pipeline",
        "data-level parity"
      ]
    },
    "publishedAt": "2025-04-11T02:13:43.000Z",
    "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis",
    "summary": "While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12322.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6397f6081323f19c578f142e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
      "fullname": "QizhiPei",
      "name": "QizhiPei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13169",
      "authors": [
        {
          "_id": "6801bcd484335da5c3e32d0b",
          "user": {
            "_id": "644a767044b75fd95805d232",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a767044b75fd95805d232/vHA2vI_B3CpXapdBEwspB.jpeg",
            "isPro": false,
            "fullname": "Patrick (Tsung-Han) Wu",
            "user": "tsunghanwu",
            "type": "user"
          },
          "name": "Tsung-Han Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:38:59.626Z",
          "hidden": false
        },
        {
          "_id": "6801bcd484335da5c3e32d0c",
          "name": "Heekyung Lee",
          "hidden": false
        },
        {
          "_id": "6801bcd484335da5c3e32d0d",
          "name": "Jiaxin Ge",
          "hidden": false
        },
        {
          "_id": "6801bcd484335da5c3e32d0e",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "6801bcd484335da5c3e32d0f",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "6801bcd484335da5c3e32d10",
          "name": "David M. Chan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:59:22.000Z",
      "submittedOnDailyAt": "2025-04-18T01:16:30.770Z",
      "title": "Generación y validación: Sampling de re-resampling de ribayes debido a la reducción de harrowering en modelos de visualización de longitudes",
      "submittedOnDailyBy": {
        "_id": "6388f68c43d8b0797a09ff84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
        "isPro": false,
        "fullname": "David Chan",
        "user": "davidchan",
        "type": "user"
      },
      "summary": "Modelo de Visión y Lenguaje (VLMs) trata especialmente la comprensión visual, pero presenta un problema con los hallucinaciones visuales. Esto implica explicar objetos, acciones o conceptos que no existen, lo que puede generar riesgos graves en aplicaciones inseguras. Los métodos actuales para suprimir los hallucinaciones generalmente se reducen a dos patrones: ajuste de generación, que cambia el comportamiento de decodificación para alinear texto y entrada visual, y verificación posterior, donde modelos externos evalúan y corregen los resultados. Entre estos, el ajuste de generación depende frecuentemente de heurísticas y puede no tener una estructura de corrección, mientras que la verificación posterior es compleja y a menudo requiere múltiples modelos, con un mayor riesgo de rechazo de resultados. En este artículo, se presenta un marco integrado llamado REVERSE, que combina la entrenamiento frente a los hallucinaciones y la auto-verificación en línea. Utilizando un nuevo conjunto de datos de verificación de hallucinaciones (con más de 1.3M muestras de entrenamiento) y una nueva técnica de resampling en tiempo de inferencia, los VLMs pueden detectar y corregir hallucinaciones durante la generación. Los resultados de evaluación muestran que REVERSE logra la reducción más avanzada de hallucinaciones, con mejoras del 12% en CHAIR-MSCOCO y más del 28% en HaloQuest. Los conjuntos de datos, modelos y código están disponibles en la siguiente URL: https://reverse-vlm.github.io.",
      "upvotes": 18,
      "discussionId": "6801bcd684335da5c3e32db7",
      "projectPage": "https://reverse-vlm.github.io",
      "githubRepo": "https://github.com/tsunghan-wu/reverse_vlm",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "visual hallucinations",
        "generation adjustment",
        "post-hoc verification",
        "hallucination-aware training",
        "on-the-fly self-verification",
        "hallucination-verification dataset",
        "semi-synthetic samples",
        "inference-time retrospective resampling",
        "CHAIR-MSCOCO",
        "HaloQuest",
        "state-of-the-art hallucination reduction"
      ]
    },
    "publishedAt": "2025-04-17T13:59:22.000Z",
    "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling",
    "summary": "Vision-Language Models (VLMs) excel at visual understanding but often suffer\nfrom visual hallucinations, where they generate descriptions of nonexistent\nobjects, actions, or concepts, posing significant risks in safety-critical\napplications. Existing hallucination mitigation methods typically follow one of\ntwo paradigms: generation adjustment, which modifies decoding behavior to align\ntext with visual inputs, and post-hoc verification, where external models\nassess and correct outputs. While effective, generation adjustment methods\noften rely on heuristics and lack correction mechanisms, while post-hoc\nverification is complicated, typically requiring multiple models and tending to\nreject outputs rather than refine them. In this work, we introduce REVERSE, a\nunified framework that integrates hallucination-aware training with on-the-fly\nself-verification. By leveraging a new hallucination-verification dataset\ncontaining over 1.3M semi-synthetic samples, along with a novel inference-time\nretrospective resampling technique, our approach enables VLMs to both detect\nhallucinations during generation and dynamically revise those hallucinations.\nOur evaluations show that REVERSE achieves state-of-the-art hallucination\nreduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO\nand 28% on HaloQuest. Our dataset, model, and code are available at:\nhttps://reverse-vlm.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13169.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6388f68c43d8b0797a09ff84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
      "fullname": "David Chan",
      "name": "davidchan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.12626",
      "authors": [
        {
          "_id": "6801b65181552de84a4b7e29",
          "name": "Lvmin Zhang",
          "hidden": false
        },
        {
          "_id": "6801b65181552de84a4b7e2a",
          "name": "Maneesh Agrawala",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T04:02:31.000Z",
      "submittedOnDailyAt": "2025-04-18T00:47:56.027Z",
      "title": "Método de empaquetado del contexto de la frase de entrada en un modelo de predicción de frames para la generación de imágenes",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Se propone una arquitectura de redes neuronales para entrenar un modelo de predicción del siguiente frame (o sección de frames) utilizando un Frame Pack. El Frame Pack reduce los frames de entrada y los convierte en un número fijo, independientemente de la longitud de la video. Esto permite procesar grandes cantidades de frames y evita problemas de cálculo como el botón de fusión de videos. Este método puede aumentar significativamente el tamaño de los conjuntos de entrenamiento de videos (en comparación con la entrenamiento de imágenes). Además, se propone generar frames en orden inverso del tiempo, establecer un punto de finalización inicial y utilizar un método de muestreo sin desplazamiento y con retroalimentación para evitar la acumulación de errores. Finalmente, el Frame Pack permite ajustar modelos de video de difusión y proporciona un scheduler de difusión equilibrado para la predicción de frames siguientes, reduciendo el número de pasos de tiempo de flujo extremos y mejorando la calidad de las imágenes.",
      "upvotes": 18,
      "discussionId": "6801b65281552de84a4b7e42",
      "projectPage": "https://lllyasviel.github.io/frame_pack_gitpage/",
      "githubRepo": "https://github.com/lllyasviel/FramePack",
      "ai_keywords": [
        "FramePack",
        "next-frame prediction",
        "transformer context length",
        "video diffusion",
        "computation bottleneck",
        "image diffusion",
        "anti-drifting sampling",
        "inverted temporal order",
        "exposure bias",
        "existing video diffusion models",
        "parameter-efficient fine-tuning",
        "visual quality",
        "diffusion schedulers",
        "extreme flow shift timesteps"
      ]
    },
    "publishedAt": "2025-04-17T00:02:31.000Z",
    "title": "Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation",
    "summary": "We present a neural network structure, FramePack, to train next-frame (or\nnext-frame-section) prediction models for video generation. The FramePack\ncompresses input frames to make the transformer context length a fixed number\nregardless of the video length. As a result, we are able to process a large\nnumber of frames using video diffusion with computation bottleneck similar to\nimage diffusion. This also makes the training video batch sizes significantly\nhigher (batch sizes become comparable to image diffusion training). We also\npropose an anti-drifting sampling method that generates frames in inverted\ntemporal order with early-established endpoints to avoid exposure bias (error\naccumulation over iterations). Finally, we show that existing video diffusion\nmodels can be finetuned with FramePack, and their visual quality may be\nimproved because the next-frame prediction supports more balanced diffusion\nschedulers with less extreme flow shift timesteps.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12626.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 48
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13122",
      "authors": [
        {
          "_id": "6801b62608d748addc187952",
          "name": "Haojian Huang",
          "hidden": false
        },
        {
          "_id": "6801b62608d748addc187953",
          "user": {
            "_id": "6570450a78d7aca0c361a177",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg",
            "isPro": false,
            "fullname": "Harold Chen",
            "user": "Harold328",
            "type": "user"
          },
          "name": "Haodong Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:39:11.374Z",
          "hidden": false
        },
        {
          "_id": "6801b62608d748addc187954",
          "user": {
            "_id": "64c139d867eff857ea51caa8",
            "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
            "isPro": false,
            "fullname": "Shengqiong Wu",
            "user": "ChocoWu",
            "type": "user"
          },
          "name": "Shengqiong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:39:08.857Z",
          "hidden": false
        },
        {
          "_id": "6801b62608d748addc187955",
          "name": "Meng Luo",
          "hidden": false
        },
        {
          "_id": "6801b62608d748addc187956",
          "name": "Jinlan Fu",
          "hidden": false
        },
        {
          "_id": "6801b62608d748addc187957",
          "name": "Xinya Du",
          "hidden": false
        },
        {
          "_id": "6801b62608d748addc187958",
          "name": "Hanwang Zhang",
          "hidden": false
        },
        {
          "_id": "6801b62608d748addc187959",
          "user": {
            "_id": "647773a1168cb428e00e9a8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
            "isPro": false,
            "fullname": "Hao Fei",
            "user": "scofield7419",
            "type": "user"
          },
          "name": "Hao Fei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:39:06.521Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:39:41.000Z",
      "submittedOnDailyAt": "2025-04-18T00:47:30.566Z",
      "title": "VistaDPO: Optimización Directa de la Preferencia Espacial-Temporal en Video\n\nEsta traducción representa la optimización directa de la preferencia espacial-temporal en video para la expansión de la escala del modelo de video.",
      "submittedOnDailyBy": {
        "_id": "6570450a78d7aca0c361a177",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg",
        "isPro": false,
        "fullname": "Harold Chen",
        "user": "Harold328",
        "type": "user"
      },
      "summary": "Los grandes modelos de vídeo (LVMs) se construyen en base a los grandes modelos de lenguaje (LLMs), pero presentan suele tener la posibilidad de comprender vídeos, sin embargo, enfrentan generalmente desafíos con la asimetría con la intuición humana y los problemas de acuerdo con el vídeo. Para enfrentar estos desafíos, presentamos un nuevo marco de trabajo llamado VistaDPO. VistaDPO utiliza la estructura jerárquica del vídeo para ajustar directamente la preferencia espacial-temporal del vídeo. VistaDPO fortalece la ajuste de contexto-vídeo en tres niveles: i) el nivel de instancia, donde se ajustan el contenido completo del vídeo y la respuesta, ii) el nivel temporal, donde se ajusta la significado temporal del vídeo y la explicación de eventos, y iii) el nivel visual, donde se ajustan los objetos espaciales y los tokens de lenguaje. Dado que no existe un conjunto de datos para ajustar la preferencia de contexto-vídeo de manera detallada, construimos el conjunto de datos VistaDPO-7k. Este conjunto de datos incluye 7.2K pares de respuestas seleccionadas y rechazadas, y contiene información espacial-temporal como marcas de tiempo, frame clave, y cajas de borde. Las experimentaciones extendidas en los benchmarks de acuerdo con la comprensión del vídeo, la respuesta a preguntas sobre el vídeo y el rendimiento de captura muestran que VistaDPO significativamente mejora el rendimiento de los LVMs actuales, mitiga efectivamente la asimetría entre el vídeo y la lenguaje y la acuerdo, y los códigos y datos están disponibles en https://github.com/HaroldChen19/VistaDPO.",
      "upvotes": 15,
      "discussionId": "6801b62808d748addc1879b2",
      "githubRepo": "https://github.com/HaroldChen19/VistaDPO",
      "ai_keywords": [
        "Large Video Models (LVMs)",
        "Large Language Models (LLMs)",
        "Video Hierarchical Spatial-Temporal Direct Preference Optimization",
        "Instance Level",
        "Temporal Level",
        "Perceptive Level",
        "QA pairs",
        "timestamps",
        "keyframes",
        "bounding boxes",
        "Video Hallucination",
        "Video QA",
        "Captioning"
      ]
    },
    "publishedAt": "2025-04-17T13:39:41.000Z",
    "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models",
    "summary": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13122.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6570450a78d7aca0c361a177",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg",
      "fullname": "Harold Chen",
      "name": "Harold328",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12369",
      "authors": [
        {
          "_id": "6801a8453c431c2bbe3b5f94",
          "name": "Zeqi Xiao",
          "hidden": false
        },
        {
          "_id": "6801a8453c431c2bbe3b5f95",
          "name": "Yushi Lan",
          "hidden": false
        },
        {
          "_id": "6801a8453c431c2bbe3b5f96",
          "name": "Yifan Zhou",
          "hidden": false
        },
        {
          "_id": "6801a8453c431c2bbe3b5f97",
          "name": "Wenqi Ouyang",
          "hidden": false
        },
        {
          "_id": "6801a8453c431c2bbe3b5f98",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "6801a8453c431c2bbe3b5f99",
          "name": "Yanhong Zeng",
          "hidden": false
        },
        {
          "_id": "6801a8453c431c2bbe3b5f9a",
          "name": "Xingang Pan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/650e37cc11f3210cf7910501/YSoQPv8_5FCfCdk2bZXxq.mp4"
      ],
      "publishedAt": "2025-04-16T17:59:30.000Z",
      "submittedOnDailyAt": "2025-04-18T00:24:00.506Z",
      "title": "WORLDMEM: Guardar memorias y simulación de mundos consistentes a largo plazo",
      "submittedOnDailyBy": {
        "_id": "650e37cc11f3210cf7910501",
        "avatarUrl": "/avatars/dab5c9d647cfa97c59f5170216673a20.svg",
        "isPro": false,
        "fullname": "zeqixiao",
        "user": "zeqixiao",
        "type": "user"
      },
      "summary": "La simulación del mundo es un lugar donde se modelan entornos hermosos desde una línea base y tiene la capacidad de predecir los resultados de las acciones. Esta técnica está ganando popularidad, pero su ventana de contexto temporal está limitada, lo que impide mantener una consistencia a largo plazo, especialmente en el mantenimiento de la coherencia en espacios 3D. En este artículo, se propone un marco de trabajo llamado WorldMem, que fortalece la generación de escenas mediante unidades de memoria que forman el banco de memoria. Este banco de memoria almacena memorias y estados (por ejemplo, posturas y marcos de tiempo). Mediante la estructura de atención de memoria, nuestro método extrae información adecuada de estas memorias basándose en el estado, lo que permite reconstruir con precisión escenas vistas anteriormente, incluso desde diferentes perspectivas y intervalos de tiempo. Además, insertando un marco de tiempo en el estado, nuestro marco de trabajo puede modelar no solo mundos estáticos, sino también detectar la evolución dinámica causada por el tiempo y facilitar la interacción y la percepción en el mundo simulado. Los experimentos ampliados tanto para mundos virtuales como reales demuestran la efectividad de nuestro enfoque.",
      "upvotes": 15,
      "discussionId": "6801a8463c431c2bbe3b5fe4",
      "projectPage": "https://xizaoqu.github.io/worldmem/",
      "githubRepo": "https://github.com/xizaoqu/WorldMem",
      "ai_keywords": [
        "WorldMem",
        "memory bank",
        "memory units",
        "memory frames",
        "states",
        "poses",
        "timestamps",
        "memory attention mechanism",
        "scene generation",
        "long-term consistency",
        "3D spatial consistency",
        "dynamic evolution"
      ]
    },
    "publishedAt": "2025-04-16T13:59:30.000Z",
    "title": "WORLDMEM: Long-term Consistent World Simulation with Memory",
    "summary": "World simulation has gained increasing popularity due to its ability to model\nvirtual environments and predict the consequences of actions. However, the\nlimited temporal context window often leads to failures in maintaining\nlong-term consistency, particularly in preserving 3D spatial consistency. In\nthis work, we present WorldMem, a framework that enhances scene generation with\na memory bank consisting of memory units that store memory frames and states\n(e.g., poses and timestamps). By employing a memory attention mechanism that\neffectively extracts relevant information from these memory frames based on\ntheir states, our method is capable of accurately reconstructing previously\nobserved scenes, even under significant viewpoint or temporal gaps.\nFurthermore, by incorporating timestamps into the states, our framework not\nonly models a static world but also captures its dynamic evolution over time,\nenabling both perception and interaction within the simulated world. Extensive\nexperiments in both virtual and real scenarios validate the effectiveness of\nour approach.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650e37cc11f3210cf7910501/YSoQPv8_5FCfCdk2bZXxq.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12369.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650e37cc11f3210cf7910501",
      "avatarUrl": "/avatars/dab5c9d647cfa97c59f5170216673a20.svg",
      "fullname": "zeqixiao",
      "name": "zeqixiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.12364",
      "authors": [
        {
          "_id": "6801d913a0cf74448f93d5c8",
          "user": {
            "_id": "649e7693a83143427691769c",
            "avatarUrl": "/avatars/8eecb0423e36797ee592b35407aa7978.svg",
            "isPro": false,
            "fullname": "Tianhui Song",
            "user": "sthuihui",
            "type": "user"
          },
          "name": "Tianhui Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:38:44.267Z",
          "hidden": false
        },
        {
          "_id": "6801d913a0cf74448f93d5c9",
          "name": "Weixin Feng",
          "hidden": false
        },
        {
          "_id": "6801d913a0cf74448f93d5ca",
          "user": {
            "_id": "66615c855fd9d736e670e0a9",
            "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
            "isPro": false,
            "fullname": "wangshuai",
            "user": "wangsssssss",
            "type": "user"
          },
          "name": "Shuai Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:38:42.164Z",
          "hidden": false
        },
        {
          "_id": "6801d913a0cf74448f93d5cb",
          "name": "Xubin Li",
          "hidden": false
        },
        {
          "_id": "6801d913a0cf74448f93d5cc",
          "name": "Tiezheng Ge",
          "hidden": false
        },
        {
          "_id": "6801d913a0cf74448f93d5cd",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "6801d913a0cf74448f93d5ce",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T15:09:45.000Z",
      "submittedOnDailyAt": "2025-04-18T03:19:21.541Z",
      "title": "DMM: Evaluación de modelos basados en destilación para la construcción de modelos generadores de imágenes ricas en funciones",
      "submittedOnDailyBy": {
        "_id": "649e7693a83143427691769c",
        "avatarUrl": "/avatars/8eecb0423e36797ee592b35407aa7978.svg",
        "isPro": false,
        "fullname": "Tianhui Song",
        "user": "sthuihui",
        "type": "user"
      },
      "summary": "El éxito del modelo generador T2I ha llevado a que varios puntos de control de un mismo modelo base se ajusten a diferentes conjuntos de datos profesionales, lo que aumenta la cantidad de modelos y plantea problemas de desgaste de hiperparámetros y altos costos de almacenamiento. La producción en masa de modelos especializados requiere la desarrollo de métodos efectivos para integrar las capacidades de múltiples modelos en un solo. En la integración de modelos, la práctica general utiliza interpolación lineal estática en el espacio de parámetros para combinar estilos, pero esta técnica ignora las características específicas de la tarea de generación de imágenes T2I y puede generar modelos incompatibles o confusos. Para abordar estos problemas, se propone la propiedad de generación de imágenes con prompts de estilo, que permite la creación precisa de imágenes de cualquier estilo mediante el control de vectores de estilo. Basándonos en esta propiedad, se propone un paradigma de integración de modelos basado en red de puntos de evaluación estilizados (DMM), que permite la compresión de varios modelos en un sólo potente modelo T2I. Además, se reinicia la tarea de integración de modelos en el contexto de la generación de imágenes T2I y se presentan nuevos objetivos de integración y protocolos de evaluación. Nuestros experimentos muestran que el DMM absorbe conocimiento de múltiples modelos de entrenamiento y logra la generación de estilos controlables.",
      "upvotes": 9,
      "discussionId": "6801d91ba0cf74448f93d7f5",
      "githubRepo": "https://github.com/MCG-NJU/DMM",
      "ai_keywords": [
        "text-to-image (T2I) generation models",
        "model checkpoints",
        "fine-tuned",
        "parameter redundancy",
        "storage cost",
        "model merging",
        "static linear interpolation",
        "parameter space",
        "style mixing",
        "style-promptable image generation pipeline",
        "style vectors",
        "score distillation",
        "compressed models",
        "versatile T2I model",
        "merging goals",
        "evaluation protocols",
        "teacher models",
        "controllable arbitrary-style generation"
      ]
    },
    "publishedAt": "2025-04-16T11:09:45.000Z",
    "title": "DMM: Building a Versatile Image Generation Model via Distillation-Based\n  Model Merging",
    "summary": "The success of text-to-image (T2I) generation models has spurred a\nproliferation of numerous model checkpoints fine-tuned from the same base model\non various specialized datasets. This overwhelming specialized model production\nintroduces new challenges for high parameter redundancy and huge storage cost,\nthereby necessitating the development of effective methods to consolidate and\nunify the capabilities of diverse powerful models into a single one. A common\npractice in model merging adopts static linear interpolation in the parameter\nspace to achieve the goal of style mixing. However, it neglects the features of\nT2I generation task that numerous distinct models cover sundry styles which may\nlead to incompatibility and confusion in the merged model. To address this\nissue, we introduce a style-promptable image generation pipeline which can\naccurately generate arbitrary-style images under the control of style vectors.\nBased on this design, we propose the score distillation based model merging\nparadigm (DMM), compressing multiple models into a single versatile T2I model.\nMoreover, we rethink and reformulate the model merging task in the context of\nT2I generation, by presenting new merging goals and evaluation protocols. Our\nexperiments demonstrate that DMM can compactly reorganize the knowledge from\nmultiple teacher models and achieve controllable arbitrary-style generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12364.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "649e7693a83143427691769c",
      "avatarUrl": "/avatars/8eecb0423e36797ee592b35407aa7978.svg",
      "fullname": "Tianhui Song",
      "name": "sthuihui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05506",
      "authors": [
        {
          "_id": "6801a137a199bc0f78da6930",
          "user": {
            "_id": "63efd75a5c2ceb16fc6e98fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63efd75a5c2ceb16fc6e98fc/qoA4LKuLTEr7hx90i90UK.jpeg",
            "isPro": true,
            "fullname": "Ahmed Masry",
            "user": "ahmed-masry",
            "type": "user"
          },
          "name": "Ahmed Masry",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:39:16.613Z",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6931",
          "user": {
            "_id": "624eb4c9058568da72ac0964",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649325306950-624eb4c9058568da72ac0964.png",
            "isPro": false,
            "fullname": "Saidul Islam",
            "user": "38saidul",
            "type": "user"
          },
          "name": "Mohammed Saidul Islam",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:39:14.462Z",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6932",
          "name": "Mahir Ahmed",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6933",
          "name": "Aayush Bajaj",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6934",
          "name": "Firoz Kabir",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6935",
          "name": "Aaryaman Kartha",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6936",
          "name": "Md Tahmid Rahman Laskar",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6937",
          "name": "Mizanur Rahman",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6938",
          "name": "Shadikur Rahman",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da6939",
          "name": "Mehrad Shahmohammadi",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da693a",
          "name": "Megh Thakkar",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da693b",
          "name": "Md Rizwan Parvez",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da693c",
          "name": "Enamul Hoque",
          "hidden": false
        },
        {
          "_id": "6801a137a199bc0f78da693d",
          "name": "Shafiq Joty",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T21:05:06.000Z",
      "submittedOnDailyAt": "2025-04-18T00:21:30.532Z",
      "title": "ChartQAPro: Benchmark de preguntas de gráficos más diversas y difíciles",
      "submittedOnDailyBy": {
        "_id": "63efd75a5c2ceb16fc6e98fc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63efd75a5c2ceb16fc6e98fc/qoA4LKuLTEr7hx90i90UK.jpeg",
        "isPro": true,
        "fullname": "Ahmed Masry",
        "user": "ahmed-masry",
        "type": "user"
      },
      "summary": "El gráfico se utiliza frecuentemente en diversas situaciones y proporciona mucha ayuda en el análisis de datos, la resolución de problemas y la descubrimiento de interesantes insights. Sin embargo, cuando se realizan tareas de análisis complejas, se requiere esfuerzo visual y cognitivo. El Sistema de Respuesta a Preguntas en Gráficos (CQA) automatiza este proceso al que el modelo puede interpretar la representación visual de los datos y proporcionar razones. Sin embargo, actualmente, el benchmark ChartQA presenta una limitada diversidad y, recientemente, se han reportado intentos de mejorar su rendimiento al integrar modelos de lenguaje visual grande (LVLMs). Para resolver estas limitaciones, presentamos ChartQAPro, un nuevo benchmark que incluye 1,341 gráficos de 157 fuentes diferentes. Este benchmark registra diferentes tipos de gráficos, como gráficos de información y paneles de control, y incluye 1,948 preguntas, lo que mejora la simulación de problemas reales. Según la evaluación de nuestros 21 modelos, el rendimiento de los LVLMs ha disminuido significativamente; por ejemplo, Claude Sonnet 3.5 obtuvo un 90.5% en ChartQA pero disminuyó a 55.81% en ChartQAPro, lo que resalta la complejidad de la comprensión de las razones en gráficos. Estamos realizando estudios detallados de errores y investigaciones para eliminar ellos, y estamos explorando los principales desafíos y oportunidades para el desarrollo de comprensión de gráficos y razones en LVLMs. Presentamos ChartQAPro.",
      "upvotes": 9,
      "discussionId": "6801a147a199bc0f78da6d4a",
      "githubRepo": "https://github.com/vis-nlp/ChartQAPro",
      "ai_keywords": [
        "Chart Question Answering (CQA)",
        "visual representations",
        "large vision-language models (LVLMs)",
        "ChartQA",
        "ChartQAPro",
        "infographics",
        "dashboards",
        "multiple-choice",
        "conversational",
        "hypothetical",
        "unanswerable questions",
        "Claude Sonnet 3.5",
        "error analyses",
        "ablation studies",
        "chart reasoning"
      ]
    },
    "publishedAt": "2025-04-07T17:05:06.000Z",
    "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question\n  Answering",
    "summary": "Charts are ubiquitous, as people often use them to analyze data, answer\nquestions, and discover critical insights. However, performing complex\nanalytical tasks with charts requires significant perceptual and cognitive\neffort. Chart Question Answering (CQA) systems automate this process by\nenabling models to interpret and reason with visual representations of data.\nHowever, existing benchmarks like ChartQA lack real-world diversity and have\nrecently shown performance saturation with modern large vision-language models\n(LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark\nthat includes 1,341 charts from 157 diverse sources, spanning various chart\ntypes, including infographics and dashboards, and featuring 1,948 questions in\nvarious types, such as multiple-choice, conversational, hypothetical, and\nunanswerable questions, to better reflect real-world challenges. Our\nevaluations with 21 models show a substantial performance drop for LVLMs on\nChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on\nChartQAPro, underscoring the complexity of chart reasoning. We complement our\nfindings with detailed error analyses and ablation studies, identifying key\nchallenges and opportunities for advancing LVLMs in chart understanding and\nreasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05506.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63efd75a5c2ceb16fc6e98fc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63efd75a5c2ceb16fc6e98fc/qoA4LKuLTEr7hx90i90UK.jpeg",
      "fullname": "Ahmed Masry",
      "name": "ahmed-masry",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 69
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13145",
      "authors": [
        {
          "_id": "6801cbcc382250483109ddd4",
          "name": "Li-Cheng Lan",
          "hidden": false
        },
        {
          "_id": "6801cbcc382250483109ddd5",
          "name": "Andrew Bai",
          "hidden": false
        },
        {
          "_id": "6801cbcc382250483109ddd6",
          "name": "Minhao Cheng",
          "hidden": false
        },
        {
          "_id": "6801cbcc382250483109ddd7",
          "name": "Ruochen Wang",
          "hidden": false
        },
        {
          "_id": "6801cbcc382250483109ddd8",
          "name": "Cho-Jui Hsieh",
          "hidden": false
        },
        {
          "_id": "6801cbcc382250483109ddd9",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:38:55.704Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:53:54.000Z",
      "submittedOnDailyAt": "2025-04-18T02:26:03.331Z",
      "title": "Encontrar métodos para mejorar la configuración de agentes de IA basados en modelos grandes a través de los fracasos de expertos en exploración",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) muestran un potencial potente como potentes agentes efectivos en situaciones complejas que requieren interacción. La Rejection Sampling Fine-Tuning (RFT) ha emergecido como una métrica efectiva para fine-tunar LLMs como agentes, permitiendo a los expertos modelar trajes exitosos y a desarrollar habilidades del agente a través de una micro-entrenamiento iterativo basado en trajes exitosos generados por el modelo. Sin embargo, los expertos (como GPT-4) principalmente son exitosos en tareas sencillas, lo que hace que RFT sea más adecuado para escenarios sencillos, y en muchos casos, no puede resolver tareas sub-tasas complejas, lo que puede llevar a situaciones de out-of-distribution (OOD). Esta investigación adicional explora cómo se pueden obtener instrucciones válidas a partir de los trajes fallidos de expertos, descubriendo que, por ejemplo, habilidades como planeación y acciones clave pueden tener un gran impacto en la eficiencia de exploración del agente y en la adquisición de habilidades. Estos hallazgos han llevado a la propuesta de Exploring Expert Failures (EEF). La EEF identifica acciones útiles a partir de los trajes fallidos de expertos y las incorpora a los conjuntos de datos de entrenamiento. Las acciones potencialmente nocivas son excluidas con precisión para evitar contaminación durante el proceso de entrenamiento del modelo. Utilizando acciones útiles derivadas de la falta de expertos, la EEF ha logrado resolver sub-tareas que anteriormente no habían sido resueltas y ha mejorado la eficiencia del fine-tuning del agente. En particular, nuestro enfoque ha alcanzado un 62% de éxito en WebShop, superando a RFT (53.6%) y GPT-4 (35.6%), y ha alcanzado puntuaciones superiores a 0.81 en WebShop, y también superando en SciWorld.",
      "upvotes": 6,
      "discussionId": "6801cbcd382250483109de04",
      "ai_keywords": [
        "Rejection Sampling Fine-Tuning (RFT)",
        "expert-generated successful trajectories",
        "self-generated trajectories",
        "out-of-distribution (OOD)",
        "agent exploration efficiency",
        "acquisition of critical skills",
        "Exploring Expert Failures (EEF)",
        "beneficial actions",
        "harmful actions",
        "agent tuning performance",
        "WebShop",
        "SciWorld"
      ]
    },
    "publishedAt": "2025-04-17T13:53:54.000Z",
    "title": "Exploring Expert Failures Improves LLM Agent Tuning",
    "summary": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13145.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13055",
      "authors": [
        {
          "_id": "6801eb7e3881da18a86691c3",
          "name": "Xiangyan Liu",
          "hidden": false
        },
        {
          "_id": "6801eb7e3881da18a86691c4",
          "name": "Jinjie Ni",
          "hidden": false
        },
        {
          "_id": "6801eb7e3881da18a86691c5",
          "name": "Zijian Wu",
          "hidden": false
        },
        {
          "_id": "6801eb7e3881da18a86691c6",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "6801eb7e3881da18a86691c7",
          "user": {
            "_id": "6214e4ee1e35c843d42d1f88",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
            "isPro": true,
            "fullname": "Longxu Dou",
            "user": "dreamerdeo",
            "type": "user"
          },
          "name": "Longxu Dou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:38:28.304Z",
          "hidden": false
        },
        {
          "_id": "6801eb7e3881da18a86691c8",
          "name": "Haonan Wang",
          "hidden": false
        },
        {
          "_id": "6801eb7e3881da18a86691c9",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "6801eb7e3881da18a86691ca",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T16:10:13.000Z",
      "submittedOnDailyAt": "2025-04-18T04:35:54.079Z",
      "title": "NoisyRollout: Fortalecimiento de la Inferencia Visual con Análisis de Datos",
      "submittedOnDailyBy": {
        "_id": "6214e4ee1e35c843d42d1f88",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
        "isPro": true,
        "fullname": "Longxu Dou",
        "user": "dreamerdeo",
        "type": "user"
      },
      "summary": "El reciente desarrollo del aprendizaje reforzado (RL) ha fortalecido la capacidad de entendimiento de los modelos de lenguaje visual (VLM), pero el aumento de la eficiencia en la expansión de la cantidad de cálculos durante la búsqueda de políticas en VLM ha sido poco investigado. Además, los VLM siguen luchando con un reconocimiento visual incompleto, lo cual afecta a los procesos posteriores de comprensión. En este sentido, proponemos un enfoque RL sencillo y efectivo llamado NoisyRollout. Este método introduce diversidad objetiva en la reconocimiento visual y en los patrones de comprensión a través de la mezcla de proyecciones de imágenes compactas y de imágenes con cierto grado de deformación. El NoisyRollout mejora la capacidad de búsqueda de VLM sin aumentar los costos de entrenamiento adicionales, introduciendo una inclinación visual. Además, durante el entrenamiento, el NoisyRollout utiliza un escalonamiento de ruido que reduce gradualmente la intensidad de la deformación, garantizando el beneficio de los señales de ruido en las fases iniciales y manteniendo la estabilidad y la escalabilidad del entrenamiento en las fases finales. Con 2.1K muestras de entrenamiento, el NoisyRollout logra los mejores resultados en cinco benchmarks de dominios, expandiendo las tareas de comprensión y reconocimiento, y manteniendo una performance relativamente óptima o mejor en dominios más desfavorables o más favorables.",
      "upvotes": 6,
      "discussionId": "6801eb7f3881da18a8669226",
      "githubRepo": "https://github.com/John-AI-Lab/NoisyRollout",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "vision-language models (VLMs)",
        "policy exploration",
        "NoisyRollout",
        "visual perception",
        "reasoning process",
        "trajectories",
        "clean and moderately distorted images",
        "targeted diversity",
        "convolutional manner",
        "inductive bias",
        "noise annealing schedule",
        "training samples",
        "out-of-domain benchmarks",
        "reasoning tasks",
        "perception tasks",
        "in-domain performance"
      ]
    },
    "publishedAt": "2025-04-17T12:10:13.000Z",
    "title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation",
    "summary": "Recent advances in reinforcement learning (RL) have strengthened the\nreasoning capabilities of vision-language models (VLMs). However, enhancing\npolicy exploration to more effectively scale test-time compute remains\nunderexplored in VLMs. In addition, VLMs continue to struggle with imperfect\nvisual perception, which in turn affects the subsequent reasoning process. To\nthis end, we propose NoisyRollout, a simple yet effective RL approach that\nmixes trajectories from both clean and moderately distorted images to introduce\ntargeted diversity in visual perception and the resulting reasoning patterns.\nWithout additional training cost, NoisyRollout enhances the exploration\ncapabilities of VLMs by incorporating a vision-oriented inductive bias.\nFurthermore, NoisyRollout employs a noise annealing schedule that gradually\nreduces distortion strength over training, ensuring benefit from noisy signals\nearly while maintaining training stability and scalability in later stages.\nWith just 2.1K training samples, NoisyRollout achieves state-of-the-art\nperformance among open-source RL-tuned models on 5 out-of-domain benchmarks\nspanning both reasoning and perception tasks, while preserving comparable or\neven better in-domain performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6214e4ee1e35c843d42d1f88",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
      "fullname": "Longxu Dou",
      "name": "dreamerdeo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12395",
      "authors": [
        {
          "_id": "6801f92ac9953c32ecda5c12",
          "name": "Jiale Tao",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c13",
          "name": "Yanbing Zhang",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c14",
          "name": "Qixun Wang",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c15",
          "name": "Yiji Cheng",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c16",
          "user": {
            "_id": "637745113a63a2983ffbde13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
            "isPro": false,
            "fullname": "Haofan Wang",
            "user": "wanghaofan",
            "type": "user"
          },
          "name": "Haofan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:38:16.438Z",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c17",
          "name": "Xu Bai",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c18",
          "name": "Zhengguang Zhou",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c19",
          "name": "Ruihuang Li",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c1a",
          "name": "Linqing Wang",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c1b",
          "name": "Chunyu Wang",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c1c",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "6801f92ac9953c32ecda5c1d",
          "name": "Qinglin Lu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/637745113a63a2983ffbde13/d00ZBp12WFtZTsnetcf4u.png"
      ],
      "publishedAt": "2025-04-16T18:01:59.000Z",
      "submittedOnDailyAt": "2025-04-18T05:34:45.132Z",
      "title": "InstantCharacter: Usando un marco de difusión escalable, se puede automáticamente proyectar cualquier personaje.",
      "submittedOnDailyBy": {
        "_id": "637745113a63a2983ffbde13",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
        "isPro": false,
        "fullname": "Haofan Wang",
        "user": "wanghaofan",
        "type": "user"
      },
      "summary": "Actualmente, los métodos de acceso basados en aprendizaje para temas personalizados se basan principalmente en la arquitectura U-Net, pero enfrentan limitaciones en la capacidad de generalización y una degradación en la calidad de las imágenes. Por otro lado, los métodos basados en optimización requieren ajustes específicos por tema, lo que disminuye la control del contexto. Para resolver estos problemas, proponemos InstantCharacter. InstantCharacter es un flexible marco de trabajo de personalización basado en transformadores de difusión. Demostra a tres ventajas fundamentales: 1. Realiza la potencialización de páginas de dominio abierto para tratar diferentes características de apariencia, postura y estilo de temas, manteniendo resultados de alta calidad. 2. Funciona con un Adapter escalable que explota el espacio potencial de los transformadores de difusión, procesando efectivamente las características del dominio abierto. 3. Para el entrenamiento efectivo del marco, construimos un conjunto de datos de temas de gran escala, con más de 10 millones de muestras. Este conjunto de datos está organizado sistemáticamente en subconjuntos de combinación (diferentes ángulos de vista) y sin combinación (combinación de imágenes de contexto). Esta estructura de datos básica proporciona diferentes pasos de entrenamiento para optimizar la consistencia de la identidad y la edicibilidad del contexto. Los experimentos de calidad muestran que InstantCharacter muestra una alta capacidad para generar imágenes de alta calidad, controladas del contexto y coherentes con el tema, estableciendo un nuevo estándar en la generación de imágenes dirigidas por temas. El código fuente está disponible en https://github.com/Tencent/InstantCharacter.",
      "upvotes": 6,
      "discussionId": "6801f92ec9953c32ecda5d95",
      "projectPage": "https://instantcharacter.github.io/",
      "githubRepo": "https://github.com/Tencent/InstantCharacter",
      "ai_keywords": [
        "foundation diffusion transformer",
        "InstantCharacter",
        "high-fidelity",
        "scalable adapter",
        "stacked transformer encoders",
        "latent space",
        "textual controllability",
        "open-domain personalization",
        "character appearances",
        "poses",
        "styles",
        "large-scale character dataset",
        "paired (multi-view character)",
        "unpaired (text-image combinations)",
        "identity consistency",
        "textual editability",
        "high-fidelity images",
        "character-driven image generation"
      ]
    },
    "publishedAt": "2025-04-16T14:01:59.000Z",
    "title": "InstantCharacter: Personalize Any Characters with a Scalable Diffusion\n  Transformer Framework",
    "summary": "Current learning-based subject customization approaches, predominantly\nrelying on U-Net architectures, suffer from limited generalization ability and\ncompromised image quality. Meanwhile, optimization-based methods require\nsubject-specific fine-tuning, which inevitably degrades textual\ncontrollability. To address these challenges, we propose InstantCharacter, a\nscalable framework for character customization built upon a foundation\ndiffusion transformer. InstantCharacter demonstrates three fundamental\nadvantages: first, it achieves open-domain personalization across diverse\ncharacter appearances, poses, and styles while maintaining high-fidelity\nresults. Second, the framework introduces a scalable adapter with stacked\ntransformer encoders, which effectively processes open-domain character\nfeatures and seamlessly interacts with the latent space of modern diffusion\ntransformers. Third, to effectively train the framework, we construct a\nlarge-scale character dataset containing 10-million-level samples. The dataset\nis systematically organized into paired (multi-view character) and unpaired\n(text-image combinations) subsets. This dual-data structure enables\nsimultaneous optimization of identity consistency and textual editability\nthrough distinct learning pathways. Qualitative experiments demonstrate the\nadvanced capabilities of InstantCharacter in generating high-fidelity,\ntext-controllable, and character-consistent images, setting a new benchmark for\ncharacter-driven image generation. Our source code is available at\nhttps://github.com/Tencent/InstantCharacter.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/637745113a63a2983ffbde13/d00ZBp12WFtZTsnetcf4u.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12395.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637745113a63a2983ffbde13",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
      "fullname": "Haofan Wang",
      "name": "wanghaofan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 75
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12157",
      "authors": [
        {
          "_id": "6801d4446d2188af01a9b6b6",
          "name": "Xiaojun Ye",
          "hidden": false
        },
        {
          "_id": "6801d4446d2188af01a9b6b7",
          "name": "Chun Wang",
          "hidden": false
        },
        {
          "_id": "6801d4446d2188af01a9b6b8",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "6801d4446d2188af01a9b6b9",
          "name": "Sheng Zhou",
          "hidden": false
        },
        {
          "_id": "6801d4446d2188af01a9b6ba",
          "name": "Liangcheng Li",
          "hidden": false
        },
        {
          "_id": "6801d4446d2188af01a9b6bb",
          "name": "Jiajun Bu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T15:04:14.000Z",
      "submittedOnDailyAt": "2025-04-18T02:56:22.746Z",
      "title": "FocusedAD: Sección de consejos de película basada en personajes\n\n**Nota:** La traducción se ha realizado manteniendo la precisión y la profundidad del contenido original.",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": false,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "El AD de cine (AD) tiene como objetivo proporcionar explicaciones sobre el contenido visual en el contexto de la división de diaologos. En particular, es más efectivo para los espectadores con discapacidad visual (BVI) que las captiones generales de video. El AD presenta problemas específicos relacionados con la comprensión del plot y la inclusión de nombres claros de personajes. Para abordar estos problemas, se propone un nuevo marco de trabajo llamado FocusedAD. Este marco incluye: (i) un módulo de reconocimiento de personajes (CPM), que traza las áreas de personajes y los asocia con sus nombres; (ii) un módulo de procesamiento dinámico (DPM), que utiliza soft promotores aprendidos a partir de AD previos y subtítulos para introducir categorías contextuales; (iii) un módulo de captura de enfoque (FCM), que genera explicaciones con nombres de personajes que incluyen detalles del plot. Para superar las limitaciones de la identificación de personajes, se introduce una automatización de flujo para la construcción de un personaje-server. FocusedAD obtiene resultados fuertes en 0-shot con los conjuntos de datos MAD-eval-Named y Cinepile-AD, y alcanza los mejores rendimientos en varios benchmarks. El código y los datos están disponibles en https://github.com/Thorin215/FocusedAD.",
      "upvotes": 4,
      "discussionId": "6801d4466d2188af01a9b756",
      "ai_keywords": [
        "Character Perception Module (CPM)",
        "Dynamic Prior Module (DPM)",
        "Focused Caption Module (FCM)",
        "soft prompts",
        "zero-shot results",
        "MAD-eval-Named",
        "Cinepile-AD dataset"
      ]
    },
    "publishedAt": "2025-04-16T11:04:14.000Z",
    "title": "FocusedAD: Character-centric Movie Audio Description",
    "summary": "Movie Audio Description (AD) aims to narrate visual content during\ndialogue-free segments, particularly benefiting blind and visually impaired\n(BVI) audiences. Compared with general video captioning, AD demands\nplot-relevant narration with explicit character name references, posing unique\nchallenges in movie understanding.To identify active main characters and focus\non storyline-relevant regions, we propose FocusedAD, a novel framework that\ndelivers character-centric movie audio descriptions. It includes: (i) a\nCharacter Perception Module(CPM) for tracking character regions and linking\nthem to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues\nfrom prior ADs and subtitles via learnable soft prompts; and (iii) a Focused\nCaption Module(FCM) that generates narrations enriched with plot-relevant\ndetails and named characters. To overcome limitations in character\nidentification, we also introduce an automated pipeline for building character\nquery banks. FocusedAD achieves state-of-the-art performance on multiple\nbenchmarks, including strong zero-shot results on MAD-eval-Named and our newly\nproposed Cinepile-AD dataset. Code and data will be released at\nhttps://github.com/Thorin215/FocusedAD .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12157.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.07959",
      "authors": [
        {
          "_id": "6801eaa05246a16677d1f2d9",
          "user": {
            "_id": "645dcc0da19f3e64bbf36492",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dcc0da19f3e64bbf36492/SPGOweEnV5syFFpf40niQ.png",
            "isPro": false,
            "fullname": "Dongyoung Kim",
            "user": "dongyong2",
            "type": "user"
          },
          "name": "Dongyoung Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:38:39.520Z",
          "hidden": false
        },
        {
          "_id": "6801eaa05246a16677d1f2da",
          "name": "Mahmoud Afifi",
          "hidden": false
        },
        {
          "_id": "6801eaa05246a16677d1f2db",
          "name": "Dongyun Kim",
          "hidden": false
        },
        {
          "_id": "6801eaa05246a16677d1f2dc",
          "name": "Michael S. Brown",
          "hidden": false
        },
        {
          "_id": "6801eaa05246a16677d1f2dd",
          "name": "Seon Joo Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:59:31.000Z",
      "submittedOnDailyAt": "2025-04-18T08:18:48.467Z",
      "title": "CCMNet: CCMNet utiliza una matriz de corrección de color ajustada para garantizar la constancia de color en el problema de la constancia de color cruzada entre cámaras.",
      "submittedOnDailyBy": {
        "_id": "645dcc0da19f3e64bbf36492",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dcc0da19f3e64bbf36492/SPGOweEnV5syFFpf40niQ.png",
        "isPro": false,
        "fullname": "Dongyoung Kim",
        "user": "dongyong2",
        "type": "user"
      },
      "summary": "La corrección de color y el equilibrio de blanco son módulos importantes en el procesamiento de señales de imagen del ISP (Image Signal Processor) de la cámara, y se utilizan para ajustar los especificaciones de color. Esta tarea se realiza en el espacio de color RAW propio de la cámara, por lo que el algoritmo de equilibrio de blanco debe ser aplicable a diferentes cámaras. En este artículo, se presenta un nuevo algoritmo de corrección de color basado en aprendizaje que puede aplicarse a nuevas cámaras, evitando el necesario de reentrenamiento. Nuestro método utiliza matrices de corrección de color pre-ajustadas (CCMs) que pueden ser usadas en el ISP, para convertir el espacio de color RAW de la cámara en un espacio estándar (por ejemplo, CIE XYZ), transformando así la iluminación en el espacio de color RAW de la cámara de prueba. Esta iluminación se codifica en el Embedding de Fingerprint de la cámara (CFE), permitiendo que la red sea aplicada a cámaras que no han sido vistas originalmente. Para evitar la sobreajuste de las CCMs y la limitación de la entrenamiento en cámaras específicas, se introdujo una técnica de expansión de datos que realiza la interpolación entre cámaras y entre CCMs. A través de los resultados experimentales obtenidos en varios conjuntos de datos y blanco de prueba, nuestro método se ha demostrado como una mejora en la corrección de color cruzada, más ligero y dependiente únicamente de los datos que se usan en el ISP de la cámara.",
      "upvotes": 4,
      "discussionId": "6801eaa25246a16677d1f37f",
      "projectPage": "https://www.dykim.me/projects/ccmnet",
      "ai_keywords": [
        "computational color constancy",
        "white balancing",
        "image signal processor (ISP)",
        "color casts",
        "camera-specific raw color space",
        "learning-based method",
        "cross-camera color constancy",
        "pre-calibrated color correction matrices (CCMs)",
        "CIE XYZ",
        "Planckian locus",
        "camera fingerprint embedding (CFE)",
        "data augmentation",
        "interpolation"
      ]
    },
    "publishedAt": "2025-04-10T13:59:31.000Z",
    "title": "CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera\n  Color Constancy",
    "summary": "Computational color constancy, or white balancing, is a key module in a\ncamera's image signal processor (ISP) that corrects color casts from scene\nlighting. Because this operation occurs in the camera-specific raw color space,\nwhite balance algorithms must adapt to different cameras. This paper introduces\na learning-based method for cross-camera color constancy that generalizes to\nnew cameras without retraining. Our method leverages pre-calibrated color\ncorrection matrices (CCMs) available on ISPs that map the camera's raw color\nspace to a standard space (e.g., CIE XYZ). Our method uses these CCMs to\ntransform predefined illumination colors (i.e., along the Planckian locus) into\nthe test camera's raw space. The mapped illuminants are encoded into a compact\ncamera fingerprint embedding (CFE) that enables the network to adapt to unseen\ncameras. To prevent overfitting due to limited cameras and CCMs during\ntraining, we introduce a data augmentation technique that interpolates between\ncameras and their CCMs. Experimental results across multiple datasets and\nbackbones show that our method achieves state-of-the-art cross-camera color\nconstancy while remaining lightweight and relying only on data readily\navailable in camera ISPs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07959.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645dcc0da19f3e64bbf36492",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dcc0da19f3e64bbf36492/SPGOweEnV5syFFpf40niQ.png",
      "fullname": "Dongyoung Kim",
      "name": "dongyong2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13079",
      "authors": [
        {
          "_id": "6801c2a379ba651f02e807ba",
          "user": {
            "_id": "617df9bb402d4d8f8eee3737",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635645820205-noauth.jpeg",
            "isPro": false,
            "fullname": "Han Wang",
            "user": "HanNight",
            "type": "user"
          },
          "name": "Han Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:38:57.694Z",
          "hidden": false
        },
        {
          "_id": "6801c2a379ba651f02e807bb",
          "name": "Archiki Prasad",
          "hidden": false
        },
        {
          "_id": "6801c2a379ba651f02e807bc",
          "name": "Elias Stengel-Eskin",
          "hidden": false
        },
        {
          "_id": "6801c2a379ba651f02e807bd",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T16:46:11.000Z",
      "submittedOnDailyAt": "2025-04-18T01:51:26.288Z",
      "title": "Rebuia Auction and False Evidence",
      "submittedOnDailyBy": {
        "_id": "617df9bb402d4d8f8eee3737",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635645820205-noauth.jpeg",
        "isPro": false,
        "fullname": "Han Wang",
        "user": "HanNight",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLM) como agentes tienen como objetivo mejorar la precisión al ampliar la generación literal de RAG (Retrieval Augmented Generation). Sin embargo, en realidad, estos sistemas deben procesar solicitudes de usuario inciertas y información potencialmente contradictoria provenientes de diversas fuentes, lo que requiere eliminar información de ruido o documentos irrelevantes. Los estudios previos han tratado estas problemas de manera separada, revisando, por ejemplo, el tratamiento de la incertidumbre o la información no adecuada desde un solo punto de vista. En este contexto, proponemos abordar múltiples causas y presentamos las siguientes propuestas:\n\n(i) RAMDocs (Literalización de información incierta y no adecuada en los documentos): un nuevo conjunto de datos que simula un escaneo complejo y realista de evidencias que conflictuan con las solicitudes del usuario, incluyendo incertidumbre, información no adecuada y ruido.\n\n(ii) MADAM-RAG: un enfoque de múltiples agentes que discuten las ventajas de la respuesta del modelo de LLM y recopilan respuestas a entidades inciertas, eliminando información no adecuada y ruido, y tratando de manera simultánea varios elementos de conflicto.\n\nLos efectos de MADAM-RAG se han demostrado con AmbigDocs (respuestas precisas para todas las solicitudes inciertas) y FaithEval (eliminación de información no adecuada), mejorando en un 11.40% o más los modelos cerrados y abiertos, basándose en un RAG fuerte. Además, en Llama3.3-70B-Instruct, se ha logrado una mejora del 15.80% en la eliminación de información no adecuada (valor absoluto). RAMDocs destaca problemas en los sistemas RAG actuales (Llama3.3-70B-Instruct no ha alcanzado un puntaje de coincidencia completa de 32.60). MADAM-RAG resuelve estas causas de conflicto, pero también destaca claramente que puede quedar errores significativos cuando la evidencia apoyada y la información no adecuada se desbalanceen.",
      "upvotes": 3,
      "discussionId": "6801c2a479ba651f02e807df",
      "githubRepo": "https://github.com/HanNight/RAMDocs",
      "ai_keywords": [
        "retrieval-augmented generation (RAG)",
        "RAMDocs",
        "MADAM-RAG",
        "multi-agent approach",
        "aggregator",
        "disambiguated entities",
        "AmbigDocs",
        "FaithEval",
        "Llama3.3-70B-Instruct",
        "exact match score"
      ]
    },
    "publishedAt": "2025-04-17T12:46:11.000Z",
    "title": "Retrieval-Augmented Generation with Conflicting Evidence",
    "summary": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "617df9bb402d4d8f8eee3737",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635645820205-noauth.jpeg",
      "fullname": "Han Wang",
      "name": "HanNight",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12782",
      "authors": [
        {
          "_id": "6801cd2966aeef19a5cec2a4",
          "name": "Leyang Li",
          "hidden": false
        },
        {
          "_id": "6801cd2966aeef19a5cec2a5",
          "user": {
            "_id": "631c4a23aa346997917bcb89",
            "avatarUrl": "/avatars/4a562ba78e6be289049027c27798cc6e.svg",
            "isPro": false,
            "fullname": "Shilin Lu",
            "user": "Shilin-LU",
            "type": "user"
          },
          "name": "Shilin Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-18T09:38:53.383Z",
          "hidden": false
        },
        {
          "_id": "6801cd2966aeef19a5cec2a6",
          "name": "Yan Ren",
          "hidden": false
        },
        {
          "_id": "6801cd2966aeef19a5cec2a7",
          "name": "Adams Wai-Kin Kong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T09:29:30.000Z",
      "submittedOnDailyAt": "2025-04-18T02:25:50.855Z",
      "title": "Simplemente dicho: Configuramos el proyecto de eliminación de ruido por auto-steaming para evitar conceptos inadecuados.",
      "submittedOnDailyBy": {
        "_id": "631c4a23aa346997917bcb89",
        "avatarUrl": "/avatars/4a562ba78e6be289049027c27798cc6e.svg",
        "isPro": false,
        "fullname": "Shilin Lu",
        "user": "Shilin-LU",
        "type": "user"
      },
      "summary": "Para garantizar la implementación ética de modelos que convierten texto en imágenes, es necesario desarrollar métodos efectivos para prevenir la generación de contenido dañino o inapropiado. La técnica de eliminación de conceptos ha sido propuesta como buena solución, pero los enfoques actuales basados en ajustes micro presentan limitaciones claras. Los métodos sin ancla destruyen proyectos de muestreo y presentan el riesgo de generar artefactos visuales. Los métodos basados en ancla dependen de la elección heurística de conceptos anclados. Para superar estas limitaciones, presentamos el \"ANT\", un marco de ajuste micro. El ANT guia proyectos de eliminación de ruido automáticamente y se centra en evitar conceptos inapropiados. En las etapas de desdenoise inicial y final, el ANT invierte la condición de orientación del clasificador Guided DRACO sin limites, permitiendo cambios precisos en el contenido sin perder la estabilidad estructural inicial. Esto mantiene la estabilidad del dominio de función de puntuación inicial y guia la muestración hacia una imagen natural. En el caso de la eliminación de un solo concepto, proponemos un mapa de pesos de Hénean extendido para especificar parámetros importantes, lo que permite una eliminación más estricta y eficiente al especificar precisamente los parámetros que contribuyen más a conceptos inapropiados. Para la eliminación de varios conceptos, nuestra función objetivo proporciona soluciones funcionales PORT AND PARSE para mejorar significativamente el rendimiento. Los experimentos extendidos muestran que el ANT realiza los mejores resultados en ambas aplicaciones de eliminación de conceptos, manteniendo la precisión de la generación y proporcionando salidas de alta calidad y seguras. El código está disponible en https://github.com/lileyang1210/ANT.",
      "upvotes": 2,
      "discussionId": "6801cd2b66aeef19a5cec330",
      "ai_keywords": [
        "ANT",
        "deNoising Trajectories",
        "classifier-free guidance",
        "score function field",
        "natural image manifold",
        "augmentation-enhanced weight saliency map",
        "trajectory-aware objective"
      ]
    },
    "publishedAt": "2025-04-17T05:29:30.000Z",
    "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts",
    "summary": "Ensuring the ethical deployment of text-to-image models requires effective\ntechniques to prevent the generation of harmful or inappropriate content. While\nconcept erasure methods offer a promising solution, existing finetuning-based\napproaches suffer from notable limitations. Anchor-free methods risk disrupting\nsampling trajectories, leading to visual artifacts, while anchor-based methods\nrely on the heuristic selection of anchor concepts. To overcome these\nshortcomings, we introduce a finetuning framework, dubbed ANT, which\nAutomatically guides deNoising Trajectories to avoid unwanted concepts. ANT is\nbuilt on a key insight: reversing the condition direction of classifier-free\nguidance during mid-to-late denoising stages enables precise content\nmodification without sacrificing early-stage structural integrity. This\ninspires a trajectory-aware objective that preserves the integrity of the\nearly-stage score function field, which steers samples toward the natural image\nmanifold, without relying on heuristic anchor concept selection. For\nsingle-concept erasure, we propose an augmentation-enhanced weight saliency map\nto precisely identify the critical parameters that most significantly\ncontribute to the unwanted concept, enabling more thorough and efficient\nerasure. For multi-concept erasure, our objective function offers a versatile\nplug-and-play solution that significantly boosts performance. Extensive\nexperiments demonstrate that ANT achieves state-of-the-art results in both\nsingle and multi-concept erasure, delivering high-quality, safe outputs without\ncompromising the generative fidelity. Code is available at\nhttps://github.com/lileyang1210/ANT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631c4a23aa346997917bcb89",
      "avatarUrl": "/avatars/4a562ba78e6be289049027c27798cc6e.svg",
      "fullname": "Shilin Lu",
      "name": "Shilin-LU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13171",
      "authors": [
        {
          "_id": "680210b4b2ae01ba08b04189",
          "name": "Kevin Lin",
          "hidden": false
        },
        {
          "_id": "680210b4b2ae01ba08b0418a",
          "name": "Charlie Snell",
          "hidden": false
        },
        {
          "_id": "680210b4b2ae01ba08b0418b",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "680210b4b2ae01ba08b0418c",
          "name": "Charles Packer",
          "hidden": false
        },
        {
          "_id": "680210b4b2ae01ba08b0418d",
          "name": "Sarah Wooders",
          "hidden": false
        },
        {
          "_id": "680210b4b2ae01ba08b0418e",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "680210b4b2ae01ba08b0418f",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:59:25.000Z",
      "submittedOnDailyAt": "2025-04-18T07:14:31.051Z",
      "title": "Shiftime Computing: Mejora en la Escalabilidad de Inferencia Durante el Tiempo de Prueba",
      "submittedOnDailyBy": {
        "_id": "65097423e64ee37323bd2def",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65097423e64ee37323bd2def/PTEwbfafNI88gdX1VVmIn.jpeg",
        "isPro": false,
        "fullname": "Hao Jiang",
        "user": "TechxGenus",
        "type": "user"
      },
      "summary": "El cálculo de tiempo de prueba ha surgido como un elemento crucial para resolver problemas complejos en modelos de lenguaje grandes (LLMs), pero también genera altos costos en términos latinos y de inferencia. Hemos introducido \"LeafTime Compute\" para permitir que el modelo considere el contexto en tiempo real antes de recibir una consulta: predeciendo lo que podría preguntar el usuario y calculando de antemano la cantidad útil, podemos reducir significativamente el cálculo de tiempo de prueba. Para demostrar el efecto de nuestro método, hemos mejorado las versiones actuales de GSM-Symbolic y AIME con estado. Reducimos aproximadamente en 5 veces el cálculo de tiempo de prueba necesario para alcanzar la misma precisión en GSM-Symbolic y AIME, y aumentamos la precisión en GSM-Symbolic en un 13% y en AIME en un 18%. Además, introducimos Multi-Query GSM-Symbolic para extender GSM-Symbolic para incluir múltiples consultas relacionadas en el mismo contexto. Asignando el LeafTime Compute a múltiples consultas relacionadas en el mismo contexto, reducimos el costo promedio en un 2.5 veces. Finalmente, hemos investigado la relación entre la predicción de las consultas del usuario y el efecto del LeafTime Compute para entender mejor las condiciones óptimas de su aplicación. También hemos realizado un estudio de caso para aplicar el LeafTime Compute en tareas de SWE de un sistema ágil de inferencia en tiempo real.",
      "upvotes": 1,
      "discussionId": "680210b6b2ae01ba08b04219",
      "ai_keywords": [
        "large language models (LLMs)",
        "sleep-time compute",
        "anticipation",
        "pre-computing",
        "test-time compute",
        "Stateful GSM-Symbolic",
        "Stateful AIME",
        "Multi-Query GSM-Symbolic",
        "amortization",
        "predictability"
      ]
    },
    "publishedAt": "2025-04-17T13:59:25.000Z",
    "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time",
    "summary": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13171.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65097423e64ee37323bd2def",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65097423e64ee37323bd2def/PTEwbfafNI88gdX1VVmIn.jpeg",
      "fullname": "Hao Jiang",
      "name": "TechxGenus",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 57
    },
    "isAuthorParticipating": false
  }
]