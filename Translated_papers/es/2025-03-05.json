[
  {
    "paper": {
      "id": "2503.02682",
      "authors": [
        {
          "_id": "67c7c3d073299239b63f5378",
          "name": "Weimin Xiong",
          "hidden": false
        },
        {
          "_id": "67c7c3d073299239b63f5379",
          "name": "Yifan Song",
          "hidden": false
        },
        {
          "_id": "67c7c3d073299239b63f537a",
          "name": "Qingxiu Dong",
          "hidden": false
        },
        {
          "_id": "67c7c3d073299239b63f537b",
          "name": "Bingchan Zhao",
          "hidden": false
        },
        {
          "_id": "67c7c3d073299239b63f537c",
          "name": "Feifan Song",
          "hidden": false
        },
        {
          "_id": "67c7c3d073299239b63f537d",
          "name": "Xun Wang",
          "hidden": false
        },
        {
          "_id": "67c7c3d073299239b63f537e",
          "name": "Sujian Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T14:54:45.000Z",
      "title": "MPO: Meta Plan Optimization to Enhance LLM Agents",
      "summary": "Recientemente, el desarrollo de los grandes modelos de lenguaje (LLMs) ha permitido a los agentes basados en LLMs resolver eficazmente tareas de planificación interactiva. Sin embargo, su éxito ha sido limitado por problemas como las hallucinaciones de planificación, lo cual requiere retrenamiento de agentes nuevos. Para abordar estos problemas, proponemos el marco de optimización de meta-planes (MPO). Este método fortalece la capacidad de planificación de los agentes directamente a través de instrucciones explícitas. En contraste con métodos anteriores que pueden basarse en conocimientos complejos que requieren gran esfuerzo humano o no garantizan la calidad, el MPO utiliza instrucciones generales de alto nivel a través de meta-planes, optimizando continuamente la meta-planea basándose en retroalimentación de ejecución de tareas. Experimentos en dos tareas representativas muestran que el MPO produce resultados significativamente mejores en comparación con los estándares actuales. Además, el análisis revela que el MPO mejora tanto la eficiencia de trabajo como la capacidad de generalización en escenarios no vistos antes, lo cual es una mejora bidireccional.",
      "upvotes": 12,
      "discussionId": "67c7c3d173299239b63f53d6",
      "githubRepo": "https://github.com/WeiminXiong/MPO"
    },
    "publishedAt": "2025-03-04T22:30:53.253Z",
    "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02682.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6225a9983207dfc568407204",
      "avatarUrl": "/avatars/c970db6232d84ae8c0fa5f11d561d67c.svg",
      "fullname": "xwm",
      "name": "xwm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02846",
      "authors": [
        {
          "_id": "67c7c3ce6f68759bf368533c",
          "name": "Yuzhe Gu",
          "hidden": false
        },
        {
          "_id": "67c7c3ce6f68759bf368533d",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "67c7c3ce6f68759bf368533e",
          "name": "Chengqi Lyu",
          "hidden": false
        },
        {
          "_id": "67c7c3ce6f68759bf368533f",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67c7c3ce6f68759bf3685340",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T18:20:24.000Z",
      "title": "Mask-DPO: Presentación de arrays de generalización y detalles de la realidad del modelo",
      "summary": "Los modelos de lenguaje grande (LLMs) desempeñan un papel de asistente en diversas áreas, pero también presentan la husillozación, es decir, información falsa o irrelevante. Debido a que los respuestas de los LLMs siempre contienen contenido factual, los métodos de consistencia factual en el pasado no pudieron capturar el ruido durante el proceso de entrenamiento. Por lo tanto, en este artículo, se propone una metodología de ajuste de consistencia factual basada en la Optimización de Preferencia Directa (DPO), que se denomina Mask-DPO. Se utiliza la máscara de documento para la consistencia factual, y Mask-DPO aprende documentos factuales solo en muestras de preferencia, evitando contenido factual en muestras no de preferencia, así como resolver la incertidumbre de la preferencia. Los resultados de experimentos amplios muestran que Mask-DPO mejora significativamente la consistencia factual de las respuestas de los LLMs y efectivamente trata preguntas y temas que no fueron vistas durante el entrenamiento, tanto dentro como fuera del dominio del conjunto de datos. Además, el puntaje de Llama3.1-8B-Instruct en el conjunto de pruebas ANAH se mejoró del 49.19% al 77.53%, y la consistencia factual en el conjunto de datos de biografía, un dominio extra, se mejoró del 30.29% al 39.39%. Además, se investigó la generalización de Mask-DPO utilizando diferentes estrategias de escalamiento de muestras de entrenamiento y concluimos que escalar el número de temas es más efectivo que escalar el problema. Además, se realizaron experimentos para comprender cómo se consigue la consistencia factual en los LLMs y para probar su significado. Estos estudios abren nuevas perspectivas para la investigación sobre la escalabilidad de la consistencia factual.",
      "upvotes": 11,
      "discussionId": "67c7c3d06f68759bf3685489",
      "githubRepo": "https://github.com/open-compass/ANAH"
    },
    "publishedAt": "2025-03-04T22:25:15.163Z",
    "title": "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01935",
      "authors": [
        {
          "_id": "67c7ba7f19b236e0564d1172",
          "user": {
            "_id": "66554507e6ea63012f35824c",
            "avatarUrl": "/avatars/b82de75bd60890e7bb524fc3754b131c.svg",
            "isPro": false,
            "fullname": "Kunlun_Zhu",
            "user": "Leozkl",
            "type": "user"
          },
          "name": "Kunlun Zhu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-05T02:44:18.739Z",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d1173",
          "name": "Hongyi Du",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d1174",
          "name": "Zhaochen Hong",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d1175",
          "name": "Xiaocheng Yang",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d1176",
          "name": "Shuyi Guo",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d1177",
          "name": "Zhe Wang",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d1178",
          "name": "Zhenhailong Wang",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d1179",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d117a",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d117b",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "67c7ba7f19b236e0564d117c",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T05:18:50.000Z",
      "title": "MultiAgentBench: Evaluación de Colaboración y Competencia de Agentes de IA",
      "summary": "Los modelos de lenguaje grande (LLMs) muestran una excelente capacidad como transductores automáticos, pero los actuales benchmarks se centran únicamente en tareas de un solo agente o están limitados a un espacio reducido, no pueden capturar la dinámica de la cooperación y la competencia entre múltiples agentes. En este artículo, se presenta el benchmark de agentes múltiples (MultiAgentBench). Este benchmark está diseñado para evaluar el rendimiento de sistemas basados en LLMs en escenarios de interacción múltiple, proporcionando una evaluación detallada. El marco utiliza nuevas y estrategias basadas en margin-stone como parámetros principales para evaluar la calidad de la cooperación y la competencia. Además, evalúa protocolos de cooperación como tipos, cadenas, árboles y grafos, así como estrategias innovadoras como discusiones grupales y planificación cognitiva. Específicamente, gpt-4o-mini logra un puntaje promedio superior en las tareas, y en escenarios de investigación, la estructura grafica es la más efectiva, y la planificación cognitiva aumenta en un 3% la tasa de logro de margin-stone. El código y los conjuntos de datos están disponibles en https://github.com/MultiagentBench/MARBLE.",
      "upvotes": 7,
      "discussionId": "67c7ba8219b236e0564d124a",
      "githubRepo": "https://github.com/MultiagentBench/MARBLE"
    },
    "publishedAt": "2025-03-04T21:46:46.873Z",
    "title": "MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01935.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64c090a9f613170e7be93d2f",
      "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
      "fullname": "KunlunZhu",
      "name": "KunlunZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01328",
      "authors": [
        {
          "_id": "67c7b5900b05ab9c7e805433",
          "name": "Xinyi Wan",
          "hidden": false
        },
        {
          "_id": "67c7b5900b05ab9c7e805434",
          "name": "Penghui Qi",
          "hidden": false
        },
        {
          "_id": "67c7b5900b05ab9c7e805435",
          "name": "Guangxing Huang",
          "hidden": false
        },
        {
          "_id": "67c7b5900b05ab9c7e805436",
          "name": "Jialin Li",
          "hidden": false
        },
        {
          "_id": "67c7b5900b05ab9c7e805437",
          "name": "Min Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T09:11:06.000Z",
      "title": "PipeOffload: Mejora de la escalabilidad de la paralelización de pipelines mediante operaciones de memoria",
      "summary": "PP se utiliza ampliamente en el entrenamiento de modelos de lenguaje grandes (LLMs), pero la escalabilidad de PP está limitada por la alta consumo de ACTIVE_MEMORY cuando el número de gradientes de lote alcanza el nivel de PP. En este artículo, se investiga la estrategia de MEMORY_OFFLOAD en PP, que no ha sido suficientemente estudiada. A través de estudios experimentales, se ha demostrado que, en muchos casos de configuración estándar, se puede OFFLOAD al menos la mitad de ACTIVE_MEMORY, y en algunos casos, todo, con un micro-overhead potencialmente mínimo. En caso de que no sea posible OFFLOAD todo, se introduce una nueva estrategia de OFFLOAD selectiva que reduce de manera lineal la cantidad máxima de ACTIVE_MEMORY. Además, se integra la MEMORY_OFFLOAD con otras tecnologías y se consideran las restricciones globales de transacciones y memoria. Los experimentos muestran que la ACTIVE_MEMORY de cada dispositivo se reduce eficazmente en función de la cantidad de etapas, y que PP se convierte en una opción más potente que TP, demostrando un menor consumo de memoria y un aumento de 19%. La implementación está disponible en el repositorio abierto en la siguiente URL:\nhttps://github.com/sail-sg/zero-bubble-pipeline-parallelism{Esta URL}",
      "upvotes": 7,
      "discussionId": "67c7b5970b05ab9c7e8055a1",
      "githubRepo": "https://github.com/sail-sg/zero-bubble"
    },
    "publishedAt": "2025-03-04T21:30:49.808Z",
    "title": "PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01328.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63510eea0b94548566dad923",
      "avatarUrl": "/avatars/629eaaf810718259bf7588dc2e6cc0d5.svg",
      "fullname": "Xinyi Wan",
      "name": "ufotalent",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02879",
      "authors": [
        {
          "_id": "67c7c42269d99dd25c5ba0ce",
          "name": "Siming Huang",
          "hidden": false
        },
        {
          "_id": "67c7c42269d99dd25c5ba0cf",
          "name": "Yuliang Xu",
          "hidden": false
        },
        {
          "_id": "67c7c42269d99dd25c5ba0d0",
          "user": {
            "_id": "67890323f8796231c857231e",
            "avatarUrl": "/avatars/f5ccd5186968d880fee9c36324a5f713.svg",
            "isPro": false,
            "fullname": "Mingmeng Geng",
            "user": "mgeng",
            "type": "user"
          },
          "name": "Mingmeng Geng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-05T03:25:23.013Z",
          "hidden": false
        },
        {
          "_id": "67c7c42269d99dd25c5ba0d1",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67c7c42269d99dd25c5ba0d2",
          "name": "Dongping Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T18:58:13.000Z",
      "title": "El tiempo de los documentos de Wikipedia: la evolución y el riesgo",
      "summary": "Este artículo analiza en detalle la influencia que los Modelos de Lenguaje Grande (LLMs) tienen sobre Wikipedia, utilizando datos actuales para investigar su desarrollo y explorar posibles riesgos mediante simulaciones. Primero, se analizan las páginas y los contenidos de los artículos para estudiar las recientes transformaciones en Wikipedia y evaluar el impacto de los LLMs. Luego, se evalúan los efectos que los LLMs tienen en diversas tareas de procesamiento del lenguaje natural (NLP) relacionadas con Wikipedia. De estos hallazgos y resultados de simulación, se concluye que los artículos de Wikipedia son afectados por los LLMs, con un impacto de aproximadamente 1% a 2% en categorías específicas. También se considera la posibilidad de que los marcos de evaluación de traducción automática basados en Wikipedia se vean afectados por los LLMs, lo que podría aumentar los puntajes de los modelos y cambiar los resultados de comparación entre modelos. Además, se considera que el efecto de la RAG (Retrieval-Augmented Generation) puede ser que el contenido generado por los LLMs se vea contaminado por bases de conocimientos contaminadas. Aunque los LLMs no cambian completamente la lengua y estructura de conocimientos de Wikipedia, creemos que es necesario una revisión rigurosa de los riesgos futuros basada en las descubrimientos experimentales.",
      "upvotes": 6,
      "discussionId": "67c7c42369d99dd25c5ba103",
      "githubRepo": "https://github.com/HSM316/LLM_Wikipedia"
    },
    "publishedAt": "2025-03-04T22:25:53.653Z",
    "title": "Wikipedia in the Era of LLMs: Evolution and Risks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02368",
      "authors": [
        {
          "_id": "67c80f94ccc2e04adfa67079",
          "name": "Zhenhua Liu",
          "hidden": false
        },
        {
          "_id": "67c80f94ccc2e04adfa6707a",
          "name": "Lijun Li",
          "hidden": false
        },
        {
          "_id": "67c80f94ccc2e04adfa6707b",
          "name": "Ruizhe Chen",
          "hidden": false
        },
        {
          "_id": "67c80f94ccc2e04adfa6707c",
          "name": "Yuxian Jiang",
          "hidden": false
        },
        {
          "_id": "67c80f94ccc2e04adfa6707d",
          "name": "Tong Zhu",
          "hidden": false
        },
        {
          "_id": "67c80f94ccc2e04adfa6707e",
          "name": "Wenliang Chen",
          "hidden": false
        },
        {
          "_id": "67c80f94ccc2e04adfa6707f",
          "name": "Jing Shao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T07:49:10.000Z",
      "title": "Iteración de decodificación de guía con optimización de la función de valor de iteración",
      "summary": "RLHF (Aprendizaje por Refuerzo con Retroalimentación Humana) se ha convertido en uno de los principales métodos para controlar la salida de modelos de lenguaje, pero enfrenta desafíos debido a altos costos computacionales y inestabilidades durante el entrenamiento. La decodificación guiada, especialmente los métodos inducidos por valores, han sido propuestos como alternativas eficientes para controlar la salida sin requerir reentrenamiento del modelo. Sin embargo, la precisión de la función de valor es crucial para la precisión de la decodificación inducida por valor. Una incertidumbre puede llevar a decisiones óptimas y a una pérdida de rendimiento. Los métodos actuales enfrentan dificultades para estimar precisamente la función de valor, lo que no ha resultado exitoso. Proponemos un nuevo marco de trabajo llamado Iterative Value Function Optimization, introduciendo dos elementos clave: Monte Carlo Value Estimation y Iterative On-Policy Optimization. Estos elementos reducen la varianza de la estimación mediante exploración de múltiples direcciones y mejoran la estimación de valor a través de la recolección de trayectorias desde la política inducida por valor. Demostramos la efectividad de esta abordaje a través de una amplia gama de experimentos, que incluyen resumen de documentos, diálogos de turnos y instrucciones, y que utilizan la optimización fundamental de la función de valor para lograr un control eficiente y efectivo, reduciendo significativamente los costos computacionales.",
      "upvotes": 5,
      "discussionId": "67c80f94ccc2e04adfa670b2"
    },
    "publishedAt": "2025-03-05T03:48:51.408Z",
    "title": "Iterative Value Function Optimization for Guided Decoding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02368.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619b00dd4b0db5ca9d3ea35f",
      "avatarUrl": "/avatars/fce5ac388b7f10cbbc63e9992a5a799f.svg",
      "fullname": "Zhenhua Liu",
      "name": "zhliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14856",
      "authors": [
        {
          "_id": "67bee83509a4524abf899511",
          "name": "Weilin Zhao",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899512",
          "name": "Tengyu Pan",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899513",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899514",
          "name": "Yudi Zhang",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899515",
          "name": "Ao Sun",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899516",
          "name": "Yuxiang Huang",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899517",
          "name": "Kaihuo Zhang",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899518",
          "name": "Weilun Zhao",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf899519",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf89951a",
          "name": "Jianyong Wang",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf89951b",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67bee83509a4524abf89951c",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:58:10.000Z",
      "title": "FR-Spec: Accelerando el modelo de lenguaje mediante la muestración predictiva de un gran diccionario de palabras ordenada por puntuación.",
      "summary": "La especificación de muestras es un método importante que ha surgido para acelerar el proceso de generación automática de regresión en modelos de lenguaje de gran escala (LLMs). Este método utiliza un mecanismo que verifica el drop de múltiples tokens en un solo paso para generar. El método de especificación de muestras más avanzado utiliza un modelo de drop que elimina una capa y el cabeza de modelado de lenguaje (LM) para lograr una compresión impresionante de capas, pero su mejora de eficiencia se reduce significativamente en modelos de grandes escalas de video (por ejemplo, Llama-3-8B, 128k tokens de video). En respuesta a esto, proponemos un marco de especificación de muestras FR-Spec, que optimiza la selección de candidatos de drop en el espacio de video a través de la asignación de prioridades de frecuencia. Limitando la búsqueda de drop en subconjuntos de tokens de frecuencia, nuestro método reduce el sobrecargo de cálculo del cabeza de modelado de lenguaje en un 75%, asegurando la equivalencia de la distribución final de salida. Los experimentos en muchos conjuntos de datos han mostrado un aumento promedio del 1.12 veces más rápido que el método de especificación de muestras más avanzado EAGLE-2.",
      "upvotes": 5,
      "discussionId": "67bee83609a4524abf899550",
      "githubRepo": "https://github.com/thunlp/FR-Spec"
    },
    "publishedAt": "2025-03-05T00:36:34.146Z",
    "title": "FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14856.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b8ff3d95bd42c770878042",
      "avatarUrl": "/avatars/564a4dccdf9e5b813a99979b0ef58183.svg",
      "fullname": "Weilin Zhao",
      "name": "Achazwl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.00955",
      "authors": [
        {
          "_id": "67c7dbff25d0b3348ddace44",
          "name": "Nam V. Nguyen",
          "hidden": false
        },
        {
          "_id": "67c7dbff25d0b3348ddace45",
          "name": "Dien X. Tran",
          "hidden": false
        },
        {
          "_id": "67c7dbff25d0b3348ddace46",
          "name": "Thanh T. Tran",
          "hidden": false
        },
        {
          "_id": "67c7dbff25d0b3348ddace47",
          "name": "Anh T. Hoang",
          "hidden": false
        },
        {
          "_id": "67c7dbff25d0b3348ddace48",
          "name": "Tai V. Duong",
          "hidden": false
        },
        {
          "_id": "67c7dbff25d0b3348ddace49",
          "name": "Di T. Le",
          "hidden": false
        },
        {
          "_id": "67c7dbff25d0b3348ddace4a",
          "name": "Phuc-Lu Le",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T16:22:46.000Z",
      "title": "SemViQA: Sistema de Respuesta a Preguntas para Comprender el Significado de la Información en Vietnamita Şnkrkeck",
      "summary": "La creciente cantidad de datos incorrectos ha llevado a que se necesite una solución fuerte de verificación de hechos para lenguajes como el vietnamés, especialmente en modelos de lenguaje grandes como GPT y Gemini. Los métodos actuales enfrentan dificultades con la incertidumbre de significado, las homofones y las estructuras lingüísticas complejas, manteniendo un equilibrio entre precisión y eficiencia. Presentamos SemViQA, un nuevo marco de verificación de hechos para el vietnamés que integra búsqueda de evidencias basada en significado (SER) y clasificación de juicios en dos etapas (TVC). Nuestro enfoque logra un equilibrio entre precisión y velocidad, registrando un 78.97% de precisión estricta en ISE-DSC01 y un 80.82% en ViWikiFC, así como ganando el primer lugar en el UIT Data Science Challenge. Además, SemViQA Faster aumenta la velocidad de inferencia en 7 veces mientras mantiene la precisión. SemViQA establece un nuevo estándar en la verificación de hechos del vietnamés y impulsa la lucha contra los datos incorrectos. El código fuente está disponible en la siguiente URL: https://github.com/DAVID-NGUYEN-S16/SemViQA.",
      "upvotes": 5,
      "discussionId": "67c7dc0025d0b3348ddace64",
      "githubRepo": "https://github.com/DAVID-NGUYEN-S16/SemViQA"
    },
    "publishedAt": "2025-03-05T00:08:53.214Z",
    "title": "SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c2bea2ada7df214276913b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
      "fullname": "Nguyen Van Nam",
      "name": "DavidNguyen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01342",
      "authors": [
        {
          "_id": "67c6b46e8389d77f5ba87179",
          "user": {
            "_id": "6585493b53c37507639fe3ba",
            "avatarUrl": "/avatars/b7e71d4fa5ebb89a7ed6b2a8313687b5.svg",
            "isPro": false,
            "fullname": "Hao Tang",
            "user": "kanashi6",
            "type": "user"
          },
          "name": "Hao Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:34:43.034Z",
          "hidden": false
        },
        {
          "_id": "67c6b46e8389d77f5ba8717a",
          "name": "Chenwei Xie",
          "hidden": false
        },
        {
          "_id": "67c6b46e8389d77f5ba8717b",
          "name": "Haiyang Wang",
          "hidden": false
        },
        {
          "_id": "67c6b46e8389d77f5ba8717c",
          "name": "Xiaoyi Bao",
          "hidden": false
        },
        {
          "_id": "67c6b46e8389d77f5ba8717d",
          "name": "Tingyu Weng",
          "hidden": false
        },
        {
          "_id": "67c6b46e8389d77f5ba8717e",
          "name": "Pandeng Li",
          "hidden": false
        },
        {
          "_id": "67c6b46e8389d77f5ba8717f",
          "name": "Yun Zheng",
          "hidden": false
        },
        {
          "_id": "67c6b46e8389d77f5ba87180",
          "name": "Liwei Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T09:27:24.000Z",
      "title": "UFO: Método Integrado para la Recognición Visual Detallada a través de Interfaces de Lenguaje con Rango Abierto",
      "summary": "Los modelos generalizados han logrado un éxito notable en tareas de lenguaje y visión-lengua, demostrando la posibilidad de modelado continuo. Sin embargo, integrar eficazmente tareas de detección o segmentación y otras tareas de reconocimiento visual preciso en estos modelos es una tarea muy difícil. Esto se debe a que la mayoría de estas tareas dependen de diseños específicos o arquitecturas que complican el proceso de modelado. Para resolver estos problemas, proponemos un marco que utiliza interfaces de lenguaje abiertamente para integrar tareas de reconocimiento visual preciso. Todos los objetivos de reconocimiento se convierten en espacios de lenguaje, y nuestro marco integra la detección a nivel de objeto, el segmentación a nivel de píxel y tareas de visión-lengua a nivel de imagen en un solo modelo. Además, introducimos una nueva aproximación de búsqueda de embedding que depende únicamente de la interfaz de lenguaje y apoya la tarea de segmentación. Nuestro marco conecta las brechas entre tareas de reconocimiento visual preciso y visión-lengua, simplifica significativamente la diseño estructural y la estrategia de entrenamiento, y alcanza una rendición relativamente alta comparado con diseños especializados para tareas complejas. Después de entrenar nuestro marco en 5 conjuntos de datos estándar de reconocimiento visual, supera el mejor rendimiento de modelos generalizados en la segmentación de instancias del COCO con un mAP de 12.3 y en la segmentación de significado de ADE20K con un mIoU de 3.3. Además, nuestro método se integra fácilmente con los MLLM existentes y con aspectos semánticos, combinando efectivamente la capacidad de reconocimiento visual preciso y el desarrollo de habilidades lingüísticas, lo que permite realizar tareas difíciles como la segmentación por razones. Los códigos y modelos están disponibles públicamente.",
      "upvotes": 4,
      "discussionId": "67c6b4728389d77f5ba8724d",
      "githubRepo": "https://github.com/nnnth/UFO"
    },
    "publishedAt": "2025-03-04T23:55:08.057Z",
    "title": "UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01342.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6585493b53c37507639fe3ba",
      "avatarUrl": "/avatars/b7e71d4fa5ebb89a7ed6b2a8313687b5.svg",
      "fullname": "Hao Tang",
      "name": "kanashi6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02197",
      "authors": [
        {
          "_id": "67c7c183203958ca9c09171a",
          "name": "Zhixun Chen",
          "hidden": false
        },
        {
          "_id": "67c7c183203958ca9c09171b",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67c7c183203958ca9c09171c",
          "name": "Yuxuan Huang",
          "hidden": false
        },
        {
          "_id": "67c7c183203958ca9c09171d",
          "name": "Yali Du",
          "hidden": false
        },
        {
          "_id": "67c7c183203958ca9c09171e",
          "name": "Meng Fang",
          "hidden": false
        },
        {
          "_id": "67c7c183203958ca9c09171f",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:14:55.000Z",
      "title": "ATLaS: Fase importante del aprendizaje mediante ajuste de agentes",
      "summary": "Los modelos de lenguaje grande (LLM) de agentes han demostrado una impresionante capacidad de generalización en diversas tareas de datos. El enfoque actual para ajustar a los agentes generalmente realiza una ajuste normativo para todos los proyectos de expertos. Sin embargo, el rastro visual de todos los proyectos puede generar sesgos hacia los expertos y puede debilitar la capacidad de generalización hacia datos de expertos. Además, la planificación, las sub-tareas complejas y las etapas de decisiones estratégicas son fundamentales para el éxito de las tareas de los agentes. La aprendizaje de estas etapas es clave para mejorar los agentes de LLM. Para lograr una ajuste más efectivo y eficiente, se propone ATLaS. ATLaS identifica las etapas importantes de los proyectos de expertos y ajusta los LLMs para reducir los costos. Enfocar la entrenamiento en al menos estas etapas importantes es fundamental para reducir el riesgo de overfitting en todos los proyectos y promover la generalización en diferentes entornos y tareas. En experimentos extendidos, los LLMs ajustados por ATLaS en 30% de las etapas importantes superan a los LLMs ajustados en todas las etapas y a los agentes de LLM abiertos de reciente desarrollo. ATLaS mantiene y mejora las técnicas básicas y fundamentales para interactuar con diferentes entornos y ecosistemas de los chatbots.",
      "upvotes": 4,
      "discussionId": "67c7c185203958ca9c091751"
    },
    "publishedAt": "2025-03-04T22:17:48.386Z",
    "title": "ATLaS: Agent Tuning via Learning Critical Steps",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02197.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02878",
      "authors": [
        {
          "_id": "67c7bf7c40de8b1b534d23fa",
          "user": {
            "_id": "635d76ce94e5b275ca74b967",
            "avatarUrl": "/avatars/5d9a389e5fd558c0b8f0724bf0838a3e.svg",
            "isPro": false,
            "fullname": "Ethan Mendes",
            "user": "emendes3",
            "type": "user"
          },
          "name": "Ethan Mendes",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-05T03:06:00.588Z",
          "hidden": false
        },
        {
          "_id": "67c7bf7c40de8b1b534d23fb",
          "name": "Alan Ritter",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T18:58:11.000Z",
      "title": "El modelo de lenguaje mejora automáticamente la evaluación del valor del estado y puede realizar búsquedas más eficientes.",
      "summary": "Los tareas de inferencia multi-nivel para recopilar recompensas de tareas de datos verdes o guíamiento humano generalmente requieren costos y tiempo, especialmente en áreas de interacción como las tareas web, donde pueden ser aún más difíciles. Para resolver estos problemas, proponemos un método de aprendizaje automático de transiciones basado en dinámicas de cambio de estado. Este método ayuda a entrenar un modelo de valor que guia efectivamente la exploración controlada por modelos de lenguaje. Hemos confirmado que un modelo de valor de tamaño intermedio (800 millones de parámetros) de una red neuronal abierta de pesos, entrenado mediante aprendizaje automático de transiciones, puede lograr rendimientos comparables a los modelos de valor de líder en el frente, como los modelos de LLM (por ejemplo, gpt-4o). Además, se ha observado un aumento de 20% en el rendimiento comparado con las exploraciones de árbol basadas en modelos de LLM anteriores, así como una reducción del costo en un factor de 37, mejorando la dependencia de las recompensas de datos verdes.",
      "upvotes": 4,
      "discussionId": "67c7bf7e40de8b1b534d2491",
      "githubRepo": "https://github.com/ethanm88/self-taught-lookahead"
    },
    "publishedAt": "2025-03-04T22:07:18.480Z",
    "title": "Language Models can Self-Improve at State-Value Estimation for Better Search",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02878.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635d76ce94e5b275ca74b967",
      "avatarUrl": "/avatars/5d9a389e5fd558c0b8f0724bf0838a3e.svg",
      "fullname": "Ethan Mendes",
      "name": "emendes3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02876",
      "authors": [
        {
          "_id": "67c7bd7149b52e85403758f8",
          "user": {
            "_id": "6308bae5c038bf42d56a98e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yrslTwUe_vy_ZJha1H83m.png",
            "isPro": false,
            "fullname": "Dmitry Nechaev",
            "user": "mgvz",
            "type": "user"
          },
          "name": "Dmitry Nechaev",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-05T03:07:52.944Z",
          "hidden": false
        },
        {
          "_id": "67c7bd7149b52e85403758f9",
          "user": {
            "_id": "6655b0b9d6c043f39719eaaf",
            "avatarUrl": "/avatars/66138e67ef3be41f29857b285b37adff.svg",
            "isPro": false,
            "fullname": "Alex Pchelnikov",
            "user": "alpchel",
            "type": "user"
          },
          "name": "Alexey Pchelnikov",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-05T03:07:35.092Z",
          "hidden": false
        },
        {
          "_id": "67c7bd7149b52e85403758fa",
          "name": "Ekaterina Ivanova",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T18:57:12.000Z",
      "title": "El araña: un conjunto detallado de datos de subestructuras multiarticulares y un modelo de referencia",
      "summary": "El desarrollo de la patología computacional de la inteligencia artificial requiere de conjuntos de datos de gran escala de alta calidad, pero los conjuntos de datos publicados actualmente tienen limitaciones en la diversidad a largo plazo, la cobertura de clases y la calidad de las anotaciones. Para remediar esto, se presenta SPIDER (SUB-JUTUDE PATCHNESS IMAGE DISCLAIMER REPOSITORY). SPIDER es el conjunto de datos más grande publicado, que incluye diversas tipologías de órganos, como piel, colarónico y sebáceo. SPIDER proporciona anotaciones de alta calidad a través de revisiones de patchness por expertos, y ofrece contexto espacial al incluir patchness cercanos, lo que mejora el rendimiento de clasificación.\n\nJunto con el conjunto de datos de SPIDER, se presenta un modelo básico de SPIDER, que utiliza el modelo de base de HIBOR-L como características de extracción, y se combina con un cabeza de clasificación basada en atención. Este modelo logra los mejores resultados en la clasificación de diferentes órganos y puede ser utilizado como un fuerte benchmark para futuras investigaciones digitales en patchness. Más allá de la clasificación de patchness, este modelo permite la rápida reconocimiento de áreas importantes, la medición cuantitativa de métricas de tejido y la construcción de una base para un acceso multimodal.\n\nEl conjunto de datos y el modelo entrenado están disponibles para la investigación, la reproducibilidad y el desarrollo de la inteligencia artificial en patchness. Para acceder, utilice el siguiente URL: https://github.com/HistAI/SPIDER",
      "upvotes": 2,
      "discussionId": "67c7bd7649b52e8540375a34"
    },
    "publishedAt": "2025-03-04T22:08:26.811Z",
    "title": "SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and Baseline Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02876.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6308bae5c038bf42d56a98e5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yrslTwUe_vy_ZJha1H83m.png",
      "fullname": "Dmitry Nechaev",
      "name": "mgvz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02268",
      "authors": [
        {
          "_id": "67c7ebafdf15f5978ac987c3",
          "name": "Wenjia Jiang",
          "hidden": false
        },
        {
          "_id": "67c7ebafdf15f5978ac987c4",
          "name": "Yangyang Zhuang",
          "hidden": false
        },
        {
          "_id": "67c7ebafdf15f5978ac987c5",
          "name": "Chenxi Song",
          "hidden": false
        },
        {
          "_id": "67c7ebafdf15f5978ac987c6",
          "name": "Xu Yang",
          "hidden": false
        },
        {
          "_id": "67c7ebafdf15f5978ac987c7",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T04:34:09.000Z",
      "title": "El Desarrollo de los Agentes de Interfaz - Funcionalidad como Especialista en Dispositivos Móviles",
      "summary": "Recientemente, el desarrollo de los grandes modelos de lenguaje (LLM) ha llevado a un avance en el desarrollo de agentes basados en LLM que pueden interactuar con interfaces gráficas (GUI). Estos agentes muestran una potente capacidad de inteligencia artificial y adaptabilidad, permitiendo realizar tareas complejas que requieren reglas definidas. Sin embargo, los agentes basados en LLM presentan una eficiencia reducida en tareas generales debido a la falta de relación causal en los pasos, especialmente en tareas rutinarias. Por otro lado, los sistemas tradicionales basados en reglas son eficientes pero carecen de adaptabilidad y inteligencia para nuevos escenarios. Para resolver estos problemas, proponemos un nuevo marco evolutivo para los agentes GUI que busca aumentar la eficiencia mientras mantiene la inteligencia y flexibilidad. Nuestro enfoque utiliza una función de memoria para registrar los registros de ejecución de tareas de los agentes. A través de la análisis de estos registros, los agentes pueden identificar secuencias de acciones reproducibles y evolucionar hacia acciones de alto nivel, reemplazando acciones de bajo nivel para mejorar la eficiencia. Así, los agentes pueden centrarse en tareas que requieren razones complejas y realizar acciones rutinarias de manera eficiente. Los resultados de experimentos en varios tareas de benchmark demuestran que nuestro enfoque mejora significativamente la eficiencia y la precisión en comparación con los métodos actuales. El código está disponible como código abierto y apoya la investigación en desarrollo.",
      "upvotes": 1,
      "discussionId": "67c7ebb5df15f5978ac98975"
    },
    "publishedAt": "2025-03-05T01:15:42.467Z",
    "title": "AppAgentX: Evolving GUI Agents as Proficient Smartphone Users",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64196320ed725fef64419c2a/dUDWK6xfRd9uVZz77V0K6.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02268.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64196320ed725fef64419c2a",
      "avatarUrl": "/avatars/96feb22fb5e8931d6c9e0ea06148266f.svg",
      "fullname": "Chi Zhang",
      "name": "DrChiZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]