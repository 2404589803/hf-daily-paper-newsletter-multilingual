[
  {
    "paper": {
      "id": "2502.01362",
      "authors": [
        {
          "_id": "67a2ad6ac7caec9bf5a45e61",
          "name": "Nikita Gushchin",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e62",
          "name": "David Li",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e63",
          "name": "Daniil Selikhanovych",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e64",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e65",
          "name": "Dmitry Baranchuk",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e66",
          "name": "Alexander Korotin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T13:56:03.000Z",
      "title": "Reverso de Bridging Matching Distillation",
      "summary": "El modelo de transición de DiPi genético Bridge es sencillo, pero transformarlo en algo práctico rápido es artístico. Los modelos de DiPi genético Bridge (DBMs) se evalúan como una prometedora extensión de los modelos de DiPi genético para el campo de la transformación de imágenes. Sin embargo, como muchos otros modelos de DiPi genético y flujo modernos, los DBMs sufren de un lento velocidad de inferencia. En respuesta a esto, proponemos una nueva técnica de transición basada en la formulación de la correspondencia inversa de Bridge y calculamos funciones objetivo computables para encontrar soluciones prácticas. Una diferencia con las tecnologías de transición de DBMs desarrolladas anteriormente es que la propuesta puede transicionar tanto modelos condicionales como no condicionales, generando una transición en un generador de un solo paso y, además, entrenando con solo imágenes destruidas. Se evaluaron las técnicas de transición condicional y no condicional en una amplia gama de configuraciones, incluyendo la conversión de imágenes a super resolución, JPEG lifting, de escurrimiento a imágenes y otras tareas. Nuestra tecnología de transición aceleró la velocidad de inferencia de los DBMs en un rango de 4 a 100 veces, y en ciertas configuraciones mostró una calidad de generación que supera a los modelos de entrenador.",
      "upvotes": 16,
      "discussionId": "67a2ad70c7caec9bf5a45fb0"
    },
    "publishedAt": "2025-02-05T03:01:40.464Z",
    "title": "Inverse Bridge Matching Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01362.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "672503c59f68afdd63cc81a2",
      "avatarUrl": "/avatars/91207207b56a1fc2b4a8197b1ab3a7f9.svg",
      "fullname": "Nikita Gushchin",
      "name": "ngushchin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01718",
      "authors": [
        {
          "_id": "67a2d995c97974764a8c294c",
          "name": "Huaye Zeng",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294d",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294e",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294f",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c2950",
          "name": "Xiaotong Chen",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c2951",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:46:04.000Z",
      "title": "ACECODER: Código de prueba automático para mejorar la RL de buenos códigos",
      "summary": "El desarrollo reciente de modelos de código está impulsado por varios tipos de aprendizaje supervisado (SFT), pero la posibilidad de aprendizaje por refuerzo (RL) en el dominio de código se explora principalmente debido a la falta de datos de recompensa confiables y a la escasez de modelos. En este artículo, se aborda estas desafíos utilizando la síntesis de casos de prueba de gran escala para mejorar el entrenamiento de modelos de código. Específicamente, se diseña una pipeline para generar amplias parejas (problema, caso de prueba) a partir de los datos de código actuales. Con estos casos de prueba, se construyen parejas de preferencia basadas en el rendimiento del programa y se entrena un modelo de recompensa utilizando el pérdida de libertad. Esto ha resultado en un aumento promedio de 10 puntos en Llama-3.1-8B-Ins y de 5 puntos en Qwen2.5-Coder-7B-Ins (seleccionando los mejores 3 de 32 muestras). Además, se realiza aprendizaje por refuerzo utilizando la recompensa de paso de los casos de prueba y los modelos de recompensa, observando mejoras consistentes en las pruebas de HumanEval, MBPP, BigCodeBench y LiveCodeBench (V4). En particular, iniciando con el entrenamiento de R1 y continuando con el entrenamiento directo en Qwen2.5-Coder-base, se observó un aumento de más del 25% en HumanEval-plus y de 6% en MBPP-plus (con 80 etapas de optimización). Creemos que el aprendizaje por refuerzo tiene un gran potencial en los modelos de código.",
      "upvotes": 12,
      "discussionId": "67a2d996c97974764a8c29a1"
    },
    "publishedAt": "2025-02-04T22:23:07.858Z",
    "title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01718.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5946
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.02492",
      "authors": [
        {
          "_id": "67a2ec904ea0e3138ac966f2",
          "user": {
            "_id": "6181c72cdcc1df2c9de8a4d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
            "isPro": false,
            "fullname": "Hila Chefer",
            "user": "Hila",
            "type": "user"
          },
          "name": "Hila Chefer",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-05T04:44:03.218Z",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f3",
          "name": "Uriel Singer",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f4",
          "name": "Amit Zohar",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f5",
          "name": "Yuval Kirstain",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f6",
          "name": "Adam Polyak",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f7",
          "name": "Yaniv Taigman",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f8",
          "name": "Lior Wolf",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f9",
          "name": "Shelly Sheynin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T17:07:10.000Z",
      "title": "VideoJAM: Método común de expresión para el surgimiento y funcionamiento de mejoras funcionales en modelos de vídeo",
      "summary": "Aunque ha experimentado un desarrollo sorprendente recientemente, los modelos de video generación carecen de la capacidad de capturar fácilmente la movimiento, la mecánica y la física del mundo real. Demostramos que estas limitaciones se priorizan en los objetivos de reconstrucción de píxeles tradicionales, lo que afecta la fidelidad exterior y la coherencia del movimiento del modelo. Para enfrentar esto, presentamos el nuevo marco de trabajo VideoJAM, que proporciona una orientación efectiva del movimiento a los modelos de video generación, permitiendo que aprendan una representación común de ambos aspectos. VideoJAM está compuesto por dos unidades de interpolación y, durante el entrenamiento, expandimos el funcional objetivo para predecir los píxeles generados y el movimiento relativo, estableciendo objetivos que solo se ejecuten en las representaciones entrenadas. Durante la inferencia, utilizamos los cambios en la predicción del movimiento como señales de guía dinámica para introducir una estructura de Inner-Guidance que guíe la generación de movimiento. En particular, nuestro marco está diseñado de manera que no requiera cambios en los datos de entrenamiento o el tamaño del modelo. VideoJAM demostra una performance líder en la coherencia del movimiento, superando a modelos competitivos de alto nivel y mejorando la calidad visual de los contenidos generados. Estos hallazgos subrayan que la combinación efectiva de la exterioridad y el movimiento en un vídeo generado puede mejorar tanto su calidad visual como su coherencia. Sitio web del proyecto: https://hila-chefer.github.io/videojam-paper.github.io/",
      "upvotes": 11,
      "discussionId": "67a2ec934ea0e3138ac9678e"
    },
    "publishedAt": "2025-02-04T23:46:17.626Z",
    "title": "VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6181c72cdcc1df2c9de8a4d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
      "fullname": "Hila Chefer",
      "name": "Hila",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.02584",
      "authors": [
        {
          "_id": "67a2d59fd5ad3369a66ff394",
          "name": "Zongyu Lin",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff395",
          "name": "Yao Tang",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff396",
          "name": "Xingcheng Yao",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff397",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff398",
          "name": "Ziniu Hu",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff399",
          "name": "Yizhou Sun",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff39a",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T18:58:31.000Z",
      "title": "QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QLASS: QL",
      "summary": "El lenguaje de los agentes inteligentes ha evolucionado como una prometedora solución para tareas de interacción complejas. Uno de los elementos más importantes para el éxito de los agentes es el modelo de recompensa en la ruta del flujo de los agentes. Este modelo proporciona una guía útil durante el proceso de aprendizaje y inferencia. Sin embargo, debido a que no se proporciona una suficiente explicación de las interacciones intermedias, muchos estudios optimizan las políticas que construyen toda la ruta utilizando el modelo de recompensa final del resultado. Esto puede impidir alcanzar una política óptima y disminuir el rendimiento general. Para resolver estos problemas, se propone QLASS (Q-GUIDED LANGUAGE AGENT STEP-WISE STEERING). QLASS calcula los valores Q de manera paso a paso para crear automáticamente explicaciones en los agentes de lenguaje abiertos. Presentando árboles de causas y modelando recompensas en cada paso, QLASS proporciona explicaciones intermedias efectivas en cada etapa. Al recibir explicaciones paso a paso, QLASS adquiere una mejor adaptación a largo plazo, mejorando significativamente el rendimiento en tareas de agentes de interacción complejas. En particular, QLASS mantiene un rendimiento fuerte incluso cuando se usan datos de explicaciones cercanas. Además, QLASS ha demostrado su eficacia mediante análisis cualitativo, permitiendo tomar decisiones más efectivas. Se publican código y datos.",
      "upvotes": 7,
      "discussionId": "67a2d5a0d5ad3369a66ff3d4"
    },
    "publishedAt": "2025-02-04T22:08:25.652Z",
    "title": "QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4670a51d5df8c2d92fce",
      "avatarUrl": "/avatars/c52d7150b4de6a2eb2d83b345d35cbc2.svg",
      "fullname": "Da Yin",
      "name": "DaYin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01941",
      "authors": [
        {
          "_id": "67a2e2a02dd2adbc88755a47",
          "name": "Xiang Liu",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a48",
          "name": "Zhenheng Tang",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a49",
          "name": "Hong Chen",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4a",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4b",
          "name": "Zeyu Li",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4c",
          "name": "Xiuze Zhou",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4d",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4e",
          "name": "Xuming Hu",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4f",
          "name": "Xiaowen Chu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T02:23:06.000Z",
      "title": "¿Los modelos de lenguaje grande (LLM) pueden mantener sus capacidades básicas bajo la compresión de caché KV?",
      "summary": "Este artículo realiza una investigación sobre problemas no investigados en modelos de lenguaje grandes (LLMs) y examina cómo la compresión del caché KV afecta las capacidades básicas de los LLMs. Los métodos existentes logran alcanzar una compresión sorprendente en marcos de evaluación de contextos largos, pero su impacto sobre las capacidades clave del modelo ha sido poco investigado. Presentamos resultados de experimentos detallados que evalúan métodos de compresión del caché KV líder en tareas diversas, como conocimiento mundial, insight, razonamiento aritmético, generación de código, seguridad, comprensión y generación de contextos largos. La análisis muestra que la compresión del caché KV puede disminuir el rendimiento en ciertas tareas. En particular, las tareas de razonamiento aritmético suelen ser especialmente sensibles a la compresión, con un descenso del rendimiento del 17.4% al 43.3%. Notamos que el modelo DeepSeek R1 Distill presenta una mayor resistencia a la compresión en comparación con modelos entrenados de manera directa, con un descenso del rendimiento del 9.67% al 25.53%. Proponemos una nueva aproximación de compresión que mantiene una coherencia significativa mientras se trata especialmente de la fase de pre-procesado y decodificación. Los resultados de los experimentos muestran que, incluso a niveles de compresión severos, se logra un aumento del rendimiento del 9% al 18% en tareas de generación de contextos largos.",
      "upvotes": 6,
      "discussionId": "67a2e2a22dd2adbc88755ab4"
    },
    "publishedAt": "2025-02-04T23:04:25.888Z",
    "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63024676056ec3a2a8714b24/XcgjmhpXd3dH6LnFZGupJ.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63024676056ec3a2a8714b24/hxWz1iVOUcE76E_K5z-B0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.02508",
      "authors": [
        {
          "_id": "67a2d1f9bc9d072d9459e857",
          "user": {
            "_id": "6553c985a7aded0380b5f928",
            "avatarUrl": "/avatars/36109d6f536d2b34d98822b88eac9608.svg",
            "isPro": false,
            "fullname": "Maohao Shen",
            "user": "maohaos2",
            "type": "user"
          },
          "name": "Maohao Shen",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-05T03:00:33.470Z",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e858",
          "name": "Guangtao Zeng",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e859",
          "name": "Zhenting Qi",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85a",
          "name": "Zhang-Wei Hong",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85b",
          "name": "Zhenfang Chen",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85c",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85d",
          "name": "Gregory Wornell",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85e",
          "name": "Subhro Das",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85f",
          "name": "David Cox",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e860",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T17:26:58.000Z",
      "title": "Satori: Utilizando el pensamiento en cadenas de acciones para mejorar el aprendizaje por refuerzo de los LLM. Teoría lógica basada en búsquedas automáticas.",
      "summary": "Los grandes modelos de lenguaje (LLMs) muestran capacidades lógicas impresionantes en diversas áreas. Según recientes estudios, se ha descubierto que el aumento de la cantidad de cálculos mejora la capacidad lógica de los LLMs. Esto generalmente incluye una amplia muestra de datos de validación externa para la inferencia, lo que forma un sistema de 2 jugadores. Sin importar si hay un guía externo, este sistema demuestra la eficiencia de que un solo LLM pueda realizar trabajos complejos. Por lo tanto, proponemos un nuevo problema de investigación: determinar si es posible internalizar la capacidad de exploración de un solo LLM para mejorar sus habilidades lógicas de manera fundamental. Esta investigación se centra en los LLMs posteriores, y se enfoca en un enfoque ortogonal que busca automatizar la colaboración de exploración (incluyendo la autoreflexión y la búsqueda de nuevas estrategias en un proceso de lógica ampliado). Para lograrlo, se utiliza la lógica de acción y pensamiento (COAT) y un paradigma de aprendizaje en dos etapas: (1) un estado de entrenamiento de formación de pequeña escala para internalizar el formato de la COAT y (2) un estado de entrenamiento de gran escala utilizando aprendizaje por refuerzo. Con nuestro enfoque, el 7B LLM Satori se ha entrenado con modelos y datos abiertos. Según las evaluaciones de experimentos extensos, Satori ha alcanzado los mejores resultados en los marcadores lógicos matemáticos y muestra una fuerte capacidad de generalización en tareas fuera del dominio. Todo el código, los datos y los modelos están completamente abiertos.",
      "upvotes": 5,
      "discussionId": "67a2d1fcbc9d072d9459e91b"
    },
    "publishedAt": "2025-02-04T21:55:09.693Z",
    "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60ad0de755f970745d4ec28d",
      "avatarUrl": "/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg",
      "fullname": "GtZeng",
      "name": "chaoscodes",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01720",
      "authors": [
        {
          "_id": "67a2fddb4044bf1c86f765a3",
          "name": "Nupur Kumari",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a4",
          "name": "Xi Yin",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a5",
          "name": "Jun-Yan Zhu",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a6",
          "name": "Ishan Misra",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a7",
          "name": "Samaneh Azadi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:59:41.000Z",
      "title": "Creación adaptada de imágenes - Tecnología para la generación de datos de síntesis multi-imágenes",
      "summary": "Los usuarios de modelos de texto pueden personalizar los modelos para insertar conceptos personalizados y generar esos conceptos con configuraciones que nunca han visto. Actualmente, los métodos dependen de la optimización de pruebas costosas o se entrenan el encoder con solo una imagen, lo que puede degradar la calidad de las imágenes. Proponemos un enfoque sencillo para resolver ambos problemas. Primero, utilizamos modelos de texto y conjuntos de datos 3D para crear un alto rendimiento conjunto de datos de personalización (SynCD) que incluye varias imágenes de la misma objeto con diferentes iluminaciones, fondos y posturas. Luego, proponemos una nueva arquitectura de encoder basada en la atribución compartida para incorporar mejores detalles visuales de la imagen de entrada. Finalmente, proponemos un nuevo método de inferencia para normalizar los vectores de texto y imagen para mitigar el problema de overexposición. En experimentos extendidos, se demostró que los modelos entrenados en el conjunto de datos de personalización con el encoder y algoritmo de inferencia propuestos superan los métodos tono-libres actuales en la personalización estándar.",
      "upvotes": 2,
      "discussionId": "67a2fde34044bf1c86f767ba"
    },
    "publishedAt": "2025-02-05T00:59:11.275Z",
    "title": "Generating Multi-Image Synthetic Data for Text-to-Image Customization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f6a894c3372328414c7021",
      "avatarUrl": "/avatars/e8b10912355712f38f10805c31bea962.svg",
      "fullname": "Nupur Kumari",
      "name": "nupurkmr9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]