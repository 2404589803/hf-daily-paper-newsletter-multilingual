[
  {
    "paper": {
      "id": "2502.01362",
      "authors": [
        {
          "_id": "67a2ad6ac7caec9bf5a45e61",
          "name": "Nikita Gushchin",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e62",
          "name": "David Li",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e63",
          "name": "Daniil Selikhanovych",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e64",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e65",
          "name": "Dmitry Baranchuk",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e66",
          "name": "Alexander Korotin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T13:56:03.000Z",
      "title": "Reverso Bridge Matching Style",
      "summary": "La diseño de la red de bridging modalidad de DeepDream es sencillo, pero transformarlo en algo práctico y rápido es artístico. La red de bridging modalidad de DeepDream (DBMs) es una extensión de la modalidad de DeepDream para la transformación de imágenes. Sin embargo, como otras modalidades de DeepDream modernas o flujos, las DBMs no habían abordado el problema de la velocidad de inferencia lenta. Para resolver esto, proponemos un nuevo método de distillación basado en el enfoque inverso de bridging y calculamos objetos computables para resolver los problemas reales. A diferencia de los métodos de distillación de DBMs anteriormente desarrollados, el método propuesto puede distillar tanto las DBMs condicionales como las no condicionales, y puede distillar un generador en un paso, utilizando solo imágenes destruidas para el entrenamiento. Evaluamos el método de distillación en ambas bridging modalidades condicionales y no condicionales en una amplia gama de configuraciones, y demostramos que nuestro método de distillación acelera la velocidad de inferencia de las DBMs en un factor de 4 a más de 100 veces, y puede superar la calidad de generación de las modalidades de tierra.",
      "upvotes": 16,
      "discussionId": "67a2ad70c7caec9bf5a45fb0"
    },
    "publishedAt": "2025-02-05T03:01:40.464Z",
    "title": "Inverse Bridge Matching Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01362.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "672503c59f68afdd63cc81a2",
      "avatarUrl": "/avatars/91207207b56a1fc2b4a8197b1ab3a7f9.svg",
      "fullname": "Nikita Gushchin",
      "name": "ngushchin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01718",
      "authors": [
        {
          "_id": "67a2d995c97974764a8c294c",
          "name": "Huaye Zeng",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294d",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294e",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294f",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c2950",
          "name": "Xiaotong Chen",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c2951",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:46:04.000Z",
      "title": "ACECODER: El triunfo del código en el bosque aleatorio mediante la síntesis de casos de prueba aleatorios",
      "summary": "El desarrollo reciente de los modelos de código ha sido principalmente impulsado por el ajuste micro observacional (SFT), pero el potencial de la aprendizaje por refuerzo (RL) ha sido muy poco explorado en el ámbito del código debido a la falta de datos de recompensa confiables y la escasez de modelos. En este artículo, resolvemos estas desafíos utilizando la síntesis automática de casos de prueba a gran escala para mejorar el entrenamiento de los modelos de código. En particular, diseñamos una pipeline para generar pares detallados (problema, caso de prueba) a partir de los datos de código existentes. Estos casos de prueba se utilizan para entrenar un modelo de recompensa basado en el porcentaje de éxito de los programas sampling, utilizando la pérdida de Bradley-Lee. Este enfoque ha demostrado mejoras medias de 10 puntos en Llama-3.1-8B-Ins y 5 puntos en Qwen2.5-Coder-7B-Ins, alcanzando un nivel comparable a DeepSeek-V2.5 (236B) en 32 muestras de 32 casos de prueba. Además, realizamos aprendizaje por refuerzo utilizando el modelo de recompensa y la recompensa por paso del caso de prueba, obteniendo mejoras consistentes en HumanEval, MBPP, BigCodeBench y LiveCodeBench (V4). En particular, aplicando el entrenamiento de R1 en Qwen2.5-Coder-base, mostramos mejoras del 25% en HumanEval-plus y 6% en MBPP-plus. Estamos convencidos de que estos resultados demuestran que el aprendizaje por refuerzo tiene un gran potencial en los modelos de código.",
      "upvotes": 12,
      "discussionId": "67a2d996c97974764a8c29a1"
    },
    "publishedAt": "2025-02-04T22:23:07.858Z",
    "title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01718.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5946
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.02492",
      "authors": [
        {
          "_id": "67a2ec904ea0e3138ac966f2",
          "user": {
            "_id": "6181c72cdcc1df2c9de8a4d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
            "isPro": false,
            "fullname": "Hila Chefer",
            "user": "Hila",
            "type": "user"
          },
          "name": "Hila Chefer",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-05T04:44:03.218Z",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f3",
          "name": "Uriel Singer",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f4",
          "name": "Amit Zohar",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f5",
          "name": "Yuval Kirstain",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f6",
          "name": "Adam Polyak",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f7",
          "name": "Yaniv Taigman",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f8",
          "name": "Lior Wolf",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f9",
          "name": "Shelly Sheynin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T17:07:10.000Z",
      "title": "VideoJAM: Expresiones comunes de apariencia-acción que mejoran la funcionalidad en modelos de vídeo",
      "summary": "Aunque se han logrado grandes avances en el pasado reciente, los modelos de video generación aún enfrentan dificultades para comprender la movimiento, la mecánica y la física del mundo real. Estos límites surgen debido a los objetivos tradicionales de reconstrucción de píxeles, que muestran que los modelos pierden la dinámica cohesión. Para resolver esto, presentamos el nuevo marco de trabajo VideoJAM, que se diseña para enseñar a los modelos a aprender representaciones comunes de apariencia-acción, proporcionando movimientos efectivos en los generadores de video. VideoJAM está compuesto por dos unidades de interpolación. Durante el entrenamiento, se expande el objetivo y se predice el movimiento correspondiente a los píxeles generados, lo que permite crear una representación aprendida. En la inferencia, introducimos la estructura Inner-Guidance, que utiliza la predicción de movimientos que evolucionan automáticamente como señales guíadoras dinámicas para controlar la generación de movimientos coherentes. En particular, nuestro marco de trabajo se caracteriza por no requerir cambios en los datos de entrenamiento o en la escala del modelo, permitiendo alcanzar el mejor rendimiento para la dinámica cohesión, superar a los modelos de perfil competitivo y mejorar la calidad visual de los contenidos generados. Estos hallazgos subrayan que la apariencia y el movimiento son interpolativos y se integran efectivamente, mejorando tanto la calidad visual como la cohesión en la generación de video. Sitio web del proyecto: https://hila-chefer.github.io/videojam-paper.github.io/",
      "upvotes": 11,
      "discussionId": "67a2ec934ea0e3138ac9678e"
    },
    "publishedAt": "2025-02-04T23:46:17.626Z",
    "title": "VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6181c72cdcc1df2c9de8a4d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
      "fullname": "Hila Chefer",
      "name": "Hila",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.02584",
      "authors": [
        {
          "_id": "67a2d59fd5ad3369a66ff394",
          "name": "Zongyu Lin",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff395",
          "name": "Yao Tang",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff396",
          "name": "Xingcheng Yao",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff397",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff398",
          "name": "Ziniu Hu",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff399",
          "name": "Yizhou Sun",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff39a",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T18:58:31.000Z",
      "title": "QLASS: Mejora la inferencia de salida de asamblea de lenguaje mediante búsqueda paso a paso guiada por Q",
      "summary": "El lenguaje de la inteligencia artificial se ha consolidado como una prometedora solución para tareas de interacción compleja, y uno de los pilares clave de éxito es el modelo de recompensa en el flujo de la red de agentes. Este modelo proporciona una dirección adecuada durante el entrenamiento o la inferencia. Sin embargo, porque no ofrece una explicación suficiente de las interacciones intermedias, muchos estudios anteriores han utilizado modelos de recompensa, optimizando políticas que forman el conjunto de la trayectoria. Esto puede llevar a políticas no óptimas y a una pérdida de rendimiento global. Para resolver esto, se propone QLASS (Q-GUIDED Language Agent Step-Scale Search), que estima valores de Q en un enfoque paso a paso para generar explicaciones automáticamente, proporcionando una guía intermedia. Al plantear árboles de razonamiento, QLASS permite modelar la recompensa en cada paso, proporcionando una guía efectiva intermedia en cada etapa. Basándose en esta guía, QLASS otorga una mayor adaptabilidad a largo plazo y mejora el rendimiento en la inferencia de modelos de agentes interactivos complejos. En particular, QLASS mantiene un rendimiento fuerte utilizando solo datos de explicación cercanos, demostrando eficiencia con superagentes limitados y probando experimentalmente que promueve decisiones más eficientes a través de un análisis cualitativo. Los códigos y datos están disponibles.",
      "upvotes": 7,
      "discussionId": "67a2d5a0d5ad3369a66ff3d4"
    },
    "publishedAt": "2025-02-04T22:08:25.652Z",
    "title": "QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4670a51d5df8c2d92fce",
      "avatarUrl": "/avatars/c52d7150b4de6a2eb2d83b345d35cbc2.svg",
      "fullname": "Da Yin",
      "name": "DaYin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01941",
      "authors": [
        {
          "_id": "67a2e2a02dd2adbc88755a47",
          "name": "Xiang Liu",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a48",
          "name": "Zhenheng Tang",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a49",
          "name": "Hong Chen",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4a",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4b",
          "name": "Zeyu Li",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4c",
          "name": "Xiuze Zhou",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4d",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4e",
          "name": "Xuming Hu",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4f",
          "name": "Xiaowen Chu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T02:23:06.000Z",
      "title": "¿Los modelos de lenguaje de largo alcance (LLM) pueden mantener sus capacidades básicas bajo la compresión de caché KV?",
      "summary": "Este artículo investiga problemas no examinados en modelos de lenguaje grandes (LLMs): el método de compresión de la caché KV y su impacto en las capacidades básicas de los LLMs. Los métodos existentes logran alcanzar compresiones sorprendentes en marcos de prueba de contexto largo, pero su influencia en las capacidades clave del modelo aún ha sido poco investigada. Proporcionamos un método principal de compresión de la caché KV que evalúa una amplia gama de tareas, incluyendo conocimiento general, inferencia general, inferencia aritmética, generación de código, seguridad, comprensión y generación de contextos largos. El análisis muestra que el método de compresión de la caché KV puede causar una pérdida de rendimiento en ciertas tareas. Las tareas de inferencia aritmética son especialmente sensibles a la compresión, con pérdidas de rendimiento de 17.4% a 43.3%. En particular, el modelo DeepSeek R1 Distill muestra una mayor resistencia a la compresión en comparación con modelos entrenados, con pérdidas de rendimiento de 9.67% a 25.53%. Evaluamos patrones de atención y el rendimiento de compresión cruzado entre tareas, y proponemos un nuevo enfoque de compresión llamado ShotKV. ShotKV trata separadamente las etapas de prefill y decodificación, manteniendo conexiones significativas a nivel de palabra, y logra un aumento del rendimiento de 9% a 18% en tareas de generación de contextos largos bajo una compresión severa.",
      "upvotes": 6,
      "discussionId": "67a2e2a22dd2adbc88755ab4"
    },
    "publishedAt": "2025-02-04T23:04:25.888Z",
    "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63024676056ec3a2a8714b24/XcgjmhpXd3dH6LnFZGupJ.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63024676056ec3a2a8714b24/hxWz1iVOUcE76E_K5z-B0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.02508",
      "authors": [
        {
          "_id": "67a2d1f9bc9d072d9459e857",
          "user": {
            "_id": "6553c985a7aded0380b5f928",
            "avatarUrl": "/avatars/36109d6f536d2b34d98822b88eac9608.svg",
            "isPro": false,
            "fullname": "Maohao Shen",
            "user": "maohaos2",
            "type": "user"
          },
          "name": "Maohao Shen",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-05T03:00:33.470Z",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e858",
          "name": "Guangtao Zeng",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e859",
          "name": "Zhenting Qi",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85a",
          "name": "Zhang-Wei Hong",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85b",
          "name": "Zhenfang Chen",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85c",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85d",
          "name": "Gregory Wornell",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85e",
          "name": "Subhro Das",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85f",
          "name": "David Cox",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e860",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T17:26:58.000Z",
      "title": "Fabricación: Aprendizaje por refuerzo basado en la continuidad de pensamiento se fortalece a los modelos de lenguaje\n\nExploración por inferencia automática\n\n**Nota:** La traducción ha sido realizada manteniendo el nivel de profesionalidad y precisión solicitado.",
      "summary": "Los modelos de lenguaje grande (LLMs) muestran un desvelador talento para la capacidad de juicio lógico en diversas áreas. Según recientes estudios, el aumento de la cantidad de cálculos durante el proceso de validación mejora la capacidad de juicio lógico de los LLMs. Esto generalmente incluye una amplia muestra de datos de LLMs externos durante la inferencia, lo que permite formar un sistema de dos jugadores. Sin tener recourso a guías externas, este sistema demuestra su eficiencia al demostrar que un solo LLM puede resolver tareas complejas. Por lo tanto, proponemos un nuevo problema de investigación: determinar si es posible internalizar la capacidad de exploración en un solo LLM para mejorar sus habilidades de juicio lógico de manera fundamental. Esta investigación se centra en los LLMs posteriormente entrenados, abordando un proceso de juicio lógico ampliado (es decir, la autoreflexión y la exploración automática de nuevas estrategias incluidas) y revisando direcciones perpendiculares. Para lograrlo, proponemos el enfoque de la lógica de acción y pensamiento (Chain-of-Action-Thought, COAT) y un patrón de aprendizaje en dos etapas: 1) un estado de entrenamiento de formación reducido para internalizar el formato de la lógica de COAT y 2) un estado de mejora automático a gran escala utilizando aprendizaje por refuerzo. Nuestro enfoque ha llevado a la obtención de un modelo de 7B LLM entrenado con código abierto y datos, el «Satori». Las evaluaciones de experimentos ampliados muestran que Satori ha alcanzado los mejores resultados en los marcos de referencia lógico-matemático y ha demostrado una fuerte capacidad de extensión para tareas en el exterior. Todo el código, los datos y el modelo están completamente abiertos.",
      "upvotes": 5,
      "discussionId": "67a2d1fcbc9d072d9459e91b"
    },
    "publishedAt": "2025-02-04T21:55:09.693Z",
    "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60ad0de755f970745d4ec28d",
      "avatarUrl": "/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg",
      "fullname": "GtZeng",
      "name": "chaoscodes",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01720",
      "authors": [
        {
          "_id": "67a2fddb4044bf1c86f765a3",
          "name": "Nupur Kumari",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a4",
          "name": "Xi Yin",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a5",
          "name": "Jun-Yan Zhu",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a6",
          "name": "Ishan Misra",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a7",
          "name": "Samaneh Azadi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:59:41.000Z",
      "title": "Generar datos de multi-imagen para realizar la customización de imágenes a partir del texto.",
      "summary": "Texto traducido al español:\n\n\"Adaptar un modelo de imágenes para personalización de usuarios, de manera que los usuarios puedan insertar conceptos personalizados y generar esos conceptos en configuraciones que no han visto antes. Los métodos actuales dependen de optimización costosa en pruebas o se basan en conjuntos de datos de entrenamiento con solo una imagen, lo que puede reducir la calidad de las imágenes si no incluye supervisión de múltiples imágenes. Proponemos una aproximación sencilla para resolver dos restricciones. Primero, utilizamos modelos de texto e imágenes y conjuntos de datos 3D para crear un alto rendimiento dataset de personalización sintética (SynCD) que incluye imágenes de diferentes iluminaciones, fondos y posiciones de objetos. Luego, proponemos una nueva arquitectura de codificador basada en estructuras de atención compartida, con el objetivo de mejorar la integración de detalles visuales minuciosos en las imágenes de entrada. Finalmente, proponemos un nuevo método de inferencia para mitigar el problema de sobrexposición, normalizando los vectores de guía de texto y imágenes. A través de experimentos ampliados, mostramos que nuestro modelo, entrenado con conjuntos de datos sintéticos, supera los métodos actuales en los marcos de referencia estándar de personalización, utilizando el codificador y algoritmos de inferencia propuestos.\"",
      "upvotes": 2,
      "discussionId": "67a2fde34044bf1c86f767ba"
    },
    "publishedAt": "2025-02-05T00:59:11.275Z",
    "title": "Generating Multi-Image Synthetic Data for Text-to-Image Customization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f6a894c3372328414c7021",
      "avatarUrl": "/avatars/e8b10912355712f38f10805c31bea962.svg",
      "fullname": "Nupur Kumari",
      "name": "nupurkmr9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]