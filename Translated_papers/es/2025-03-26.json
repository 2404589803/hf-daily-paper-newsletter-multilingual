[
  {
    "paper": {
      "id": "2503.19325",
      "authors": [
        {
          "_id": "67e35f6fc9d8214b5e1c64c3",
          "name": "Yuchao Gu",
          "hidden": false
        },
        {
          "_id": "67e35f6fc9d8214b5e1c64c4",
          "name": "Weijia Mao",
          "hidden": false
        },
        {
          "_id": "67e35f6fc9d8214b5e1c64c5",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63021630a35b21bd8a53305a/SL0MQs7OvQpNlGBhroTW3.png"
      ],
      "publishedAt": "2025-03-25T03:38:06.000Z",
      "submittedOnDailyAt": "2025-03-26T00:37:14.940Z",
      "title": "Modelo de regresión automática en videos de largo corte y predicción de la siguiente frame",
      "submittedOnDailyBy": {
        "_id": "63021630a35b21bd8a53305a",
        "avatarUrl": "/avatars/7a7e8b39749eda61e57d8a1908726558.svg",
        "isPro": true,
        "fullname": "Gu Yuchao",
        "user": "guyuchao",
        "type": "user"
      },
      "summary": "El modelado automático de contextos largos ha avanzado significativamente en la generación de lenguaje, pero la generación de imágenes ha enfrentado dificultades para utilizar adecuadamente el contexto temporal a lo largo de tiempos prolongados. Para investigar el modelado de contextos largos de imágenes, proponemos un estándar fuerte para el modelado automático de imágenes mediante el Frame AutoRegressive (FAR). Modelos de lenguaje aprenden las relaciones causales fundamentales entre tokens (Token), mientras que FAR modela las relaciones causales temporales entre secuencias de frames continuas, logrando convergencia mejor que modelos AR de tokens o transformers de difusión de imágenes. Basándonos en FAR, el modelado de contextos largos visuales ha mostrado problemas debido a la redundancia visual. Actualmente, RoPE no tiene un enfoque de disipación temporal válido para contextos distantes, lo que limita su capacidad para hacer buenas inferencias en secuencias de imágenes largas. Además, la entrenamiento de imágenes largas es costoso computacionalmente y los tokens visuales crecen rápidamente más que los tokens lingüísticos. Para resolver estos problemas, proponemos FlexRoPE para equilibrar la dependencia local y temporal. FlexRoPE agrega un enfoque flexible de disipación temporal a RoPE, permitiendo inferencias en contextos visuales 16 veces más largos. Además, proponemos el modelado de contextos visuales cortos y largos. El modelado de contextos visuales cortos garantizan la coherencia temporal a gran detalle a través de ventanas de contexto visuales de alta resolución, mientras que los modelados de contextos visuales largos codifican información a largas distancias con pocos tokens, incluso en ventanas de contexto visuales infinitas. Esta aproximación permite entrenar secuencias de imágenes largas. FAR logra los mejores resultados en la generación de imágenes cortas y largas, ofreciendo un estándar sencillo y efectivo para el modelado automático de imágenes.",
      "upvotes": 49,
      "discussionId": "67e35f72c9d8214b5e1c659b",
      "ai_keywords": [
        "Frame AutoRegressive (FAR)",
        "Token AR",
        "video autoregressive modeling",
        "visual redundancy",
        "RoPE (Rotary Position Embedding)",
        "temporal decay",
        "FlexRoPE",
        "long short-term context modeling",
        "high-resolution short-term context window",
        "long-term context window",
        "state-of-the-art performance",
        "video generation"
      ]
    },
    "publishedAt": "2025-03-24T23:38:06.000Z",
    "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
    "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context vision modeling\nfaces challenges due to visual redundancy. Existing RoPE lacks effective\ntemporal decay for remote context and fails to extrapolate well to long video\nsequences. Additionally, training on long videos is computationally expensive,\nas vision tokens grow much faster than language tokens. To tackle these issues,\nwe propose balancing locality and long-range dependency. We introduce FlexRoPE,\nan test-time technique that adds flexible temporal decay to RoPE, enabling\nextrapolation to 16x longer vision contexts. Furthermore, we propose long\nshort-term context modeling, where a high-resolution short-term context window\nensures fine-grained temporal consistency, while an unlimited long-term context\nwindow encodes long-range information using fewer tokens. With this approach,\nwe can train on long video sequences with a manageable token context length. We\ndemonstrate that FAR achieves state-of-the-art performance in both short- and\nlong-video generation, providing a simple yet effective baseline for video\nautoregressive modeling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63021630a35b21bd8a53305a/SL0MQs7OvQpNlGBhroTW3.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19325.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63021630a35b21bd8a53305a",
      "avatarUrl": "/avatars/7a7e8b39749eda61e57d8a1908726558.svg",
      "fullname": "Gu Yuchao",
      "name": "guyuchao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18931",
      "authors": [
        {
          "_id": "67e25c4d1908043170bd551d",
          "user": {
            "_id": "64651db3611ae99d14d392ea",
            "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
            "isPro": false,
            "fullname": "cyt",
            "user": "Row11n",
            "type": "user"
          },
          "name": "Yitong Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:18:45.692Z",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd551e",
          "name": "Lingchen Meng",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd551f",
          "name": "Wujian Peng",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd5520",
          "name": "Zuxuan Wu",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd5521",
          "name": "Yu-Gang Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:52:47.000Z",
      "submittedOnDailyAt": "2025-03-26T01:10:42.553Z",
      "title": "CoMP: Preparación de entrenamiento para modelos continuos basados en visión",
      "submittedOnDailyBy": {
        "_id": "64651db3611ae99d14d392ea",
        "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
        "isPro": false,
        "fullname": "cyt",
        "user": "Row11n",
        "type": "user"
      },
      "summary": "Los modelos de base de visión basados en visión (VFMs) proporcionan una fuerte representación visual para una amplia gama de aplicaciones. En este artículo, se predicen continuamente y de manera multimodal los VFMs para procesar fácilmente entradas visuales de diferentes tamaños y generar representaciones visuales más consistentes con la representación lingüística. Para ello, se introduce el proceso de predicción multimodal continuo (CoMP). CoMP apoya el proceso de predicción continuo de los modelos de origen con la utilización de Embeddings de Posición Rotacional Continuo, y utiliza la pérdida de disposición entre características visuales y textuales para disponer las representaciones multimodales utilizando prototipos lingüísticos. A través de tres etapas de entrenamiento, los VFMs mejoran significativamente en la comprensión multimodal, así como en tareas posteriores como la clasificación de clases y la segmentación. En particular, CoMP-SigLIP logra puntuaciones de 66.7 en ChartQA y 75.9 en DocVQA, manteniendo una precisión de 87.4% en ImageNet-1K y un mIoU de 49.5 en ADE20K, todo esto sin necesidad de usar un modelo de lenguaje de 0.5B.",
      "upvotes": 19,
      "discussionId": "67e25c4f1908043170bd55a8",
      "projectPage": "https://slimm-x.github.io/comp/",
      "githubRepo": "https://github.com/SliMM-X/CoMP-MM",
      "ai_keywords": [
        "Vision Foundation Models (VFMs)",
        "Continual Rotary Position Embedding",
        "Alignment Loss",
        "language prototypes",
        "multimodal pre-training pipeline",
        "three-stage training",
        "multimodal understanding",
        "classification",
        "segmentation",
        "ChartQA",
        "DocVQA",
        "LLM",
        "ImageNet-1K",
        "ADE20K",
        "frozen chunk evaluation"
      ]
    },
    "publishedAt": "2025-03-24T13:52:47.000Z",
    "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models",
    "summary": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18931.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64651db3611ae99d14d392ea",
      "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
      "fullname": "cyt",
      "name": "Row11n",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.19385",
      "authors": [
        {
          "_id": "67e36241d8da46951f858026",
          "name": "Jaihoon Kim",
          "hidden": false
        },
        {
          "_id": "67e36241d8da46951f858027",
          "name": "Taehoon Yoon",
          "hidden": false
        },
        {
          "_id": "67e36241d8da46951f858028",
          "name": "Jisung Hwang",
          "hidden": false
        },
        {
          "_id": "67e36241d8da46951f858029",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T06:30:45.000Z",
      "submittedOnDailyAt": "2025-03-26T00:49:38.583Z",
      "title": "Flujo de modelos que realizan escalado durante la inferencia: generación probabilística y la técnica de la cola de LoRa fuerza",
      "submittedOnDailyBy": {
        "_id": "6342796a0875f2c99cfd313b",
        "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
        "isPro": false,
        "fullname": "Yuseung \"Phillip\" Lee",
        "user": "phillipinseoul",
        "type": "user"
      },
      "summary": "Proponemos un enfoque de escalado durante la inferencia para modelos de flujo predecidos. Recientemente, el escalado durante la inferencia ha despertado interés importante en modelos de LLMs y modelos de difusión, siendo crucial para mejorar la calidad de las muestras y ajustar a las preferencias del usuario mediante cálculos adicionales. En modelos de difusión, la muestración de partículas puede escalar eficientemente a través de la randomización del ruido en las etapas intermedias. Por otro lado, modelos de flujo ofrecen rápida generación y alta calidad de imagenes y videos, lo que los ha convertido en una opción popular en lugar de modelos de difusión, aunque los métodos de escalado de difusión no se aplican directamente en el proceso de generación determinista. Para lograr un escalado eficiente durante la inferencia en modelos de flujo, proponemos tres ideas principales: 1) la generación basada en SDE, que permite la muestración de partículas en modelos de flujo, 2) la transformación entre proteínas, que amplía el espacio de búsqueda para aumentar la diversidad de las muestras, y 3) Forzamiento del Presupuesto de Rollover (RBF), que asigna una distribución adaptativa de canales de cálculo a lo largo de los pasos de tiempo para maximizar su utilización. Según nuestros resultados experimentales, la generación basada en SDE, especialmente la generación basada en proteínas VP (varianza preservando), mejora el rendimiento de la muestración de partículas en el escalado durante la inferencia de modelos de flujo. Además, la combinación de VP-SDE y RBF muestra un rendimiento superior a todos los métodos de escalado durante la inferencia existentes.",
      "upvotes": 17,
      "discussionId": "67e36245d8da46951f85802c",
      "ai_keywords": [
        "flow models",
        "inference-time scaling",
        "LLMs",
        "diffusion models",
        "sample quality",
        "user preferences",
        "particle sampling",
        "stochasticity",
        "denoising steps",
        "generative process",
        "SDE-based generation",
        "interpolant conversion",
        "sample diversity",
        "Rollover Budget Forcing (RBF)",
        "adaptive allocation",
        "computational resources",
        "timesteps",
        "budget utilization",
        "variance-preserving (VP)",
        "VP interpolant-based generation"
      ]
    },
    "publishedAt": "2025-03-25T02:30:45.000Z",
    "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
    "summary": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19385.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6342796a0875f2c99cfd313b",
      "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
      "fullname": "Yuseung \"Phillip\" Lee",
      "name": "phillipinseoul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19622",
      "authors": [
        {
          "_id": "67e3706bc9d8214b5e219149",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914a",
          "name": "Jiashu Qu",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914b",
          "name": "Jingyi Tang",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914c",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914d",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914e",
          "name": "Hongyu Chen",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914f",
          "name": "Li Liang",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e219150",
          "name": "Li Su",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e219151",
          "name": "Qingming Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T13:12:17.000Z",
      "submittedOnDailyAt": "2025-03-26T01:44:03.080Z",
      "title": "Investigación, evaluación, análisis y medidas para el estudio de deslizamiento en videos de grandes escalas de modelos multimodelos",
      "submittedOnDailyBy": {
        "_id": "62728f4f6253fe2068da1021",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
        "isPro": false,
        "fullname": "Hongcheng Gao",
        "user": "HongchengGao",
        "type": "user"
      },
      "summary": "Los problemas de la imaginación en modelos de lenguaje grandes (LMMs) limitan la confianza y la posibilidad de aplicación al proporcionar respuestas incorrectas en la práctica. En este artículo, se estudia este problema y se propone un marco de referencia detallado llamado \"HAVEN\" para evaluar la imaginación en tareas de comprensión de imágenes de LMMs, comparándolos con modelos dinámicos para investigar los problemas de la imaginación en un modelo más complejo como BANK. Este marco de referencia proporciona 6K preguntas tridimensionalmente organizadas según las causas de la imaginación, sus aspectos y la forma de las preguntas, y se estudian cuantitativamente 7 factores influyentes (tiempo de la imagen, tamaño del modelo, razonamiento del modelo) en 16 experimentos de LMMs. Además, se propone un modelo de pensamiento de imágenes que inhibe la imaginación utilizando SRFT (entrenamiento de razonamiento complementario) y TDPO (optimización directa de preferencias), basado en el nuevo algoritmo de pensamiento de modelos como OpenAI o1. SRFT mejora la capacidad de razonamiento, mientras que TDPO reduce la imaginación durante el proceso de pensamiento. Las amplias experimentaciones y análisis me han demostrado que la precisión en la evaluación de la imaginación se ha mejorado en un 7.65% y los puntajes de sesgo se han reducido en un 4.5%, demostrando claramente el efecto del modelo. El código y los datos están disponibles en https://github.com/Hongcheng-Gao/HAVEN.",
      "upvotes": 16,
      "discussionId": "67e3706dc9d8214b5e2191e0",
      "githubRepo": "https://github.com/Hongcheng-Gao/HAVEN",
      "ai_keywords": [
        "multimodal models (LMMs)",
        "hallucination",
        "video modality",
        "video understanding",
        "HAVEN",
        "hallucination causes",
        "hallucination aspects",
        "question formats",
        "duration time",
        "model sizes",
        "model reasoning",
        "supervised reasoning fine-tuning (SRFT)",
        "direct preference optimization (TDPO)",
        "video-thinking model",
        "accuracy",
        "bias score"
      ]
    },
    "publishedAt": "2025-03-25T09:12:17.000Z",
    "title": "Exploring Hallucination of Large Multimodal Models in Video\n  Understanding: Benchmark, Analysis and Mitigation",
    "summary": "The hallucination of large multimodal models (LMMs), providing responses that\nappear correct but are actually incorrect, limits their reliability and\napplicability. This paper aims to study the hallucination problem of LMMs in\nvideo modality, which is dynamic and more challenging compared to static\nmodalities like images and text. From this motivation, we first present a\ncomprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in\nvideo understanding tasks. It is built upon three dimensions, i.e.,\nhallucination causes, hallucination aspects, and question formats, resulting in\n6K questions. Then, we quantitatively study 7 influential factors on\nhallucinations, e.g., duration time of videos, model sizes, and model\nreasoning, via experiments of 16 LMMs on the presented benchmark. In addition,\ninspired by recent thinking models like OpenAI o1, we propose a video-thinking\nmodel to mitigate the hallucinations of LMMs via supervised reasoning\nfine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT\nenhances reasoning capabilities while TDPO reduces hallucinations in the\nthinking process. Extensive experiments and analyses demonstrate the\neffectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on\nhallucination evaluation and reduces the bias score by 4.5%. The code and data\nare public at https://github.com/Hongcheng-Gao/HAVEN.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19622.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "62728f4f6253fe2068da1021",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
      "fullname": "Hongcheng Gao",
      "name": "HongchengGao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14905",
      "authors": [
        {
          "_id": "67e250450487eeecfd9a5880",
          "name": "Siwei Wen",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5881",
          "name": "Junyan Ye",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5882",
          "name": "Peilin Feng",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5883",
          "name": "Hengrui Kang",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5884",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5885",
          "name": "Yize Chen",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5886",
          "name": "Jiang Wu",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5887",
          "name": "Wenjun Wu",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5888",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5889",
          "name": "Weijia Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T05:14:44.000Z",
      "submittedOnDailyAt": "2025-03-26T04:00:13.753Z",
      "title": "Spot the Fake: Detection and Artifact Explanation of Synthetic Images Based on Large-Scale Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "La rápida evolución de la tecnología AIGC está llevando a imágenes sintéticas a ser utilizadas de manera más natural en la vida cotidiana, y esto está planteando nuevos desafíos en la evaluación de la verdad y la detección. Los métodos actuales son efectivos para evaluar la verdad de las imágenes y identificar falsos, pero generalmente tienen un nivel de comprensión humano bajo y no se adaptan suficientemente a la complejidad ampliada de los datos sintéticos. Para resolver estos problemas, presentamos FakeVLM, una herramienta diseñada para la detección de imágenes sintéticas y DeepFake, basada en un grande modelo de lenguaje. FakeVLM no solo distingue entre imágenes verdaderas y falsas, sino que también proporciona una explicación en lenguaje natural sobre los artificios de las imágenes, lo que aumenta la posibilidad de interpretación. Además, presentamos FakeClue, un conjunto de datos detallado que incluye más de 100,000 imágenes en 7 categorías, para ayudar a entender mejor los artificios. FakeVLM muestra un rendimiento comparable a los modelos de expertos, elimina la necesidad de clasificadores adicionales y proporciona una solución sólida para la detección de datos sintéticos. Evaluaciones en diversos conjuntos de datos han demostrado la excelente performance de FakeVLM en tareas de clasificación de verdad e interpretación de artificios, estableciendo nuevos estándares de prueba para la detección de imágenes sintéticas. Los conjuntos de datos y el código están disponibles en la siguiente URL: https://github.com/opendatalab/FakeVLM.",
      "upvotes": 12,
      "discussionId": "67e250490487eeecfd9a599e",
      "githubRepo": "https://github.com/opendatalab/FakeVLM",
      "ai_keywords": [
        "large multimodal model",
        "FakeVLM",
        "DeepFake detection",
        "image artifacts",
        "natural language explanations",
        "FakeClue",
        "fine-grained artifact clues"
      ]
    },
    "publishedAt": "2025-03-19T01:14:44.000Z",
    "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\n  with Artifact Explanation",
    "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) technologies, synthetic images have become increasingly prevalent in\neveryday life, posing new challenges for authenticity assessment and detection.\nDespite the effectiveness of existing methods in evaluating image authenticity\nand locating forgeries, these approaches often lack human interpretability and\ndo not fully address the growing complexity of synthetic data. To tackle these\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\nexcels in distinguishing real from fake images but also provides clear, natural\nlanguage explanations for image artifacts, enhancing interpretability.\nAdditionally, we present FakeClue, a comprehensive dataset containing over\n100,000 images across seven categories, annotated with fine-grained artifact\nclues in natural language. FakeVLM demonstrates performance comparable to\nexpert models while eliminating the need for additional classifiers, making it\na robust solution for synthetic data detection. Extensive evaluations across\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\nclassification and artifact explanation tasks, setting a new benchmark for\nsynthetic image detection. The dataset and code will be released in:\nhttps://github.com/opendatalab/FakeVLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14905.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19903",
      "authors": [
        {
          "_id": "67e375d3cc93cc8c42da7699",
          "name": "Baifeng Shi",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769a",
          "name": "Boyi Li",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769b",
          "name": "Han Cai",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769c",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769d",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769e",
          "name": "Marco Pavone",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769f",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a0",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a1",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a2",
          "name": "Pavlo Molchanov",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a3",
          "name": "Hongxu Yin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649004218f7cbbc94c782db6/F0o61glyGm9fib9Pl1i-L.mp4"
      ],
      "publishedAt": "2025-03-25T17:58:37.000Z",
      "submittedOnDailyAt": "2025-03-26T02:13:20.800Z",
      "title": "Escalado de resolución de 4K similarización de prácticas prácticas",
      "submittedOnDailyBy": {
        "_id": "649004218f7cbbc94c782db6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AdgLVfAIpWlug4jXTaEK-.jpeg",
        "isPro": false,
        "fullname": "Baifeng Shi",
        "user": "bfshi",
        "type": "user"
      },
      "summary": "El reconocimiento de detalles visuales de alta resolución es importante en tareas diarias. Sin embargo, el aprendizaje visual pre-entrenado actual está limitado a imágenes de baja resolución (por ejemplo, 378 x 378 píxeles) debido a los costos asociados con procesar imágenes grandes, que están relacionados con la dimensión bidimensional. Introducimos un método para extender el aprendizaje visual pre-entrenado de estilo CLIP a resoluciones de 4K. PS3 sustituye la representación general de la imagen a través del aprendizaje relativo, procesando selectivamente áreas locales y comparándolos con capturas detalladas locales para permitir el aprendizaje de representaciones de alta resolución. Esto reduce significativamente los costos de cálculo. PS3 puede codificar toda la imagen a baja resolución o procesar selectivamente áreas de alta resolución relacionadas con un prompt de texto. Al aplicar PS3 a un MLLM como DAMO-Llama, se genera el modelo VILA-HD, que mejora significativamente la percepción visual de alta resolución, y comparado con referencias como AnyRes o S^2, que no tienen aprendizaje visual pre-entrenado de alta resolución, aumenta la percepción visual de alta resolución aún cuando se utilizan 4.3 veces más tokens. VILA-HD supera a modelos anteriores como NVILA y Qwen2-VL en términos de tecnologías más recientes, y obtiene excelentes resultados en varios benchmarks. Además, es más eficiente que los métodos de entrenamiento de tokens más recientes. Finalmente, actualmente los benchmarks no necesitan la percepción de resoluciones de 4K, por lo que proponemos un nuevo benchmark llamado 4KPro. En 4KPro, VILA-HD supera a todos los MLLM anteriores, mejora el rendimiento de GPT-4o en un 14.5%, aumenta el rendimiento de Qwen2-VL en un 3.2% y proporciona un aumento de velocidad de 2.96 veces.",
      "upvotes": 9,
      "discussionId": "67e375d9cc93cc8c42da785f",
      "projectPage": "https://nvlabs.github.io/PS3/",
      "githubRepo": "https://github.com/NVlabs/PS3",
      "ai_keywords": [
        "PS3",
        "CLIP-style vision pre-training",
        "contrastive learning",
        "local regions",
        "local detailed captions",
        "high-resolution representation learning",
        "computational overhead",
        "saliency",
        "text prompt",
        "VILA-HD",
        "multi-modal LLM",
        "high-resolution visual perception",
        "AnyRes",
        "S^2",
        "scaling properties",
        "test-time compute",
        "NVILA",
        "Qwen2-VL",
        "benchmarks",
        "token pruning approaches",
        "4KPer",
        "image QA",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-03-25T13:58:37.000Z",
    "title": "Scaling Vision Pre-Training to 4K Resolution",
    "summary": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649004218f7cbbc94c782db6/F0o61glyGm9fib9Pl1i-L.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19903.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649004218f7cbbc94c782db6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AdgLVfAIpWlug4jXTaEK-.jpeg",
      "fullname": "Baifeng Shi",
      "name": "bfshi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19855",
      "authors": [
        {
          "_id": "67e36792a281c900d76a93c8",
          "name": "Xiaoyu Tian",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93c9",
          "name": "Sitong Zhao",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93ca",
          "name": "Haotian Wang",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cb",
          "name": "Shuaiting Chen",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cc",
          "name": "Yunjie Ji",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cd",
          "name": "Yiping Peng",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93ce",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cf",
          "name": "Xiangang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:19:38.000Z",
      "submittedOnDailyAt": "2025-03-26T01:04:39.479Z",
      "title": "「Al considerar de nuevo: el escalado de los retroalimentos de pruebas repetitivos en el proceso de validación para mejorar la lógica del LLM」",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de los modelos de lenguaje grande (LLMs) como OpenAI-o1 y DeepSeek-R1 ha demostrado el efecto de la escalabilidad en los modelos. Esto muestra que un proceso lógico ampliado puede significativamente mejorar el rendimiento del modelo. Sin embargo, los modelos actuales están limitados por el procesamiento de largas oraciones y la eficiencia de la entrenamiento de aprendizaje reforzado (RL). Para enfrentar estos problemas, proponemos un enfoque sencillo y efectivo de escalabilidad durante el test llamado \"Multi-round Thinking\". Este método recoge las respuestas anteriores como prompt para reconfigurar la lógica del modelo, fomentando su evolución y mejorando su rendimiento. Se han realizado amplias pruebas en marcos de referencia como AIME 2024, MATH-500, GPQA-diamond y LiveCodeBench, mostrando un aumento significativo en el rendimiento del modelo. Por ejemplo, la precisión de QwQ-32B en el conjunto de datos AIME 2024 aumentó del 80.3% en la ronda 1 al 82.1% en la ronda 2, y DeepSeek-R1 también aumentó del 79.7% a 82.0%. Estos resultados demuestran que el \"Multi-round Thinking\" es ampliamente aplicable y puede lograr un aumento estable del rendimiento del modelo con un enfoque sencillo. Esta propuesta destaca la posibilidad de desarrollo futuro de tecnologías de escalabilidad durante el test.",
      "upvotes": 7,
      "discussionId": "67e36793a281c900d76a9459",
      "ai_keywords": [
        "large language models",
        "OpenAI-o1",
        "DeepSeek-R1",
        "test-time scaling",
        "extended reasoning processes",
        "reinforcement learning",
        "Multi-round Thinking",
        "iterative refinement",
        "AIME 2024",
        "MATH-500",
        "GPQA-diamond",
        "LiveCodeBench",
        "accuracy",
        "stable enhancements",
        "test-time scaling techniques"
      ]
    },
    "publishedAt": "2025-03-25T13:19:38.000Z",
    "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking",
    "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19855.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6471
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19910",
      "authors": [
        {
          "_id": "67e35e4cff080b9ee71e3295",
          "name": "Chuong Huynh",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3296",
          "name": "Jinyu Yang",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3297",
          "name": "Ashish Tawari",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3298",
          "name": "Mubarak Shah",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3299",
          "name": "Son Tran",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e329a",
          "name": "Raffay Hamid",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e329b",
          "name": "Trishul Chilimbi",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e329c",
          "name": "Abhinav Shrivastava",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:59:50.000Z",
      "submittedOnDailyAt": "2025-03-26T00:26:00.764Z",
      "title": "CoLLM: Modelo de lenguaje grande para búsqueda de imágenes sintéticas",
      "submittedOnDailyBy": {
        "_id": "63a4d196cde2b28f82a56bd9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4d196cde2b28f82a56bd9/iqVFOtDteRMUScFGRcx0L.png",
        "isPro": false,
        "fullname": "Chuong Huynh",
        "user": "chuonghm",
        "type": "user"
      },
      "summary": "Composición de Imágenes de Recuperación (CIR) es una tarea compleja que se basa en la solicitud de varios modelos para buscar imágenes. Los datos de entrenamiento generales están constituidos por tres elementos: imágenes de referencia, descripciones textuales de los cambios deseados y imágenes objetivos. La obtención de estos datos requiere costos y tiempo. La escasez de conjuntos de datos de CIR ha sido abordada mediante el uso de métodos 0-shot que utilizan tres de los elementos mencionados, así como la utilización de pares de comentarios de imágenes web recopiladas con modelos de lenguaje visual (VLMs). Sin embargo, estos métodos tienen limitaciones, como la escasez en escala, la diversidad y la naturaleza no natural de los textos de cambio, así como el impedimiento al aprendizaje compartido para múltiples solicitaciones de modelos debido a la falta de tres datos. Además, los métodos actuales no se adaptan a textos de cambio complejos. Proponemos CoLLM para resolver estas limitaciones. Nuestro enfoque genera automaticamente tres de los elementos a partir de pares de comentarios de imágenes, permitiendo entrenamiento sub-objetivo sin necesidad de explicaciones manuales. Utilizamos modelos de lenguaje de gran escala (LLMs) para generar el aprendizaje compartido de referencias de imágenes y textos de cambio, y promueven la fusión de múltiples modelos. Además, introducimos el Multi-Text CIR (MTCIR), que incluye 3.4 millones de muestras, mejora los benchmarks actuales de CIR (CIRR y Fashion-IQ), y aumenta la confianza en la evaluación. A través de los resultados de experimentos, CoLLM ha alcanzado los mejores rendimientos en varios benchmarks de CIR y configuraciones. El MTCIR ha proporcionado un aumento del 15% en el rendimiento, y nuestros mejoramientos en benchmark han creado criterios de evaluación más confiables para modelos de CIR, contribuyendo a el desarrollo de esta área importante.",
      "upvotes": 6,
      "discussionId": "67e35e4eff080b9ee71e3353",
      "projectPage": "https://collm-cvpr25.github.io/",
      "ai_keywords": [
        "Composed Image Retrieval (CIR)",
        "multimodal query",
        "triplets",
        "reference image",
        "textual description",
        "target image",
        "zero-shot approaches",
        "synthetic triplets",
        "vision-language models (VLMs)",
        "web-crawled image-caption pairs",
        "joint embedding learning",
        "complex and nuanced modification texts",
        "multimodal fusion",
        "CoLLM",
        "Large Language Models (LLMs)",
        "Multi-Text CIR (MTCIR)",
        "CIRR benchmark",
        "Fashion-IQ benchmark",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-03-25T13:59:50.000Z",
    "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
    "summary": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19910.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4d196cde2b28f82a56bd9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4d196cde2b28f82a56bd9/iqVFOtDteRMUScFGRcx0L.png",
      "fullname": "Chuong Huynh",
      "name": "chuonghm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18446",
      "authors": [
        {
          "_id": "67e367ee4363e3c4bbbaca3a",
          "name": "Jinho Jeong",
          "hidden": false
        },
        {
          "_id": "67e367ee4363e3c4bbbaca3b",
          "name": "Sangmin Han",
          "hidden": false
        },
        {
          "_id": "67e367ee4363e3c4bbbaca3c",
          "name": "Jinwoo Kim",
          "hidden": false
        },
        {
          "_id": "67e367ee4363e3c4bbbaca3d",
          "name": "Seon Joo Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T08:50:15.000Z",
      "submittedOnDailyAt": "2025-03-26T01:07:15.007Z",
      "title": "Por favor, aquí está la traducción al español:\n\n\"Por favor, espera por un momento. El trabajo de traducción está en curso.\"",
      "submittedOnDailyBy": {
        "_id": "66b5f733f0c16f37f307f35e",
        "avatarUrl": "/avatars/29a97e10b4d65aa23d7eae238f809499.svg",
        "isPro": false,
        "fullname": "JinHo Jeong",
        "user": "3587jjh",
        "type": "user"
      },
      "summary": "En este artículo se propone un nuevo marco de trabajo llamado LSRNA para la generación de imágenes de alta resolución (superior a 1K) en el espacio potencial. Los modelos de difusión enfrentan dificultades al superar la resolución de entrenamiento, presentando problemas estructurales y una mala representación del contenido. Los métodos basados en referencias resuelven estos problemas guiando la generación de alta resolución a partir de imágenes de baja resolución, pero el upsampling en el espacio potencial introduce un sesgo hacia la variabilidad reducida, lo que afecta la calidad del resultado. Por otro lado, el upsampling en el espacio RGB genera salidas demasiado planificadas. Para superar estas limitaciones, LSRNA combina un upsampling de alta resolución (LSR) en el espacio potencial, con un método de adición de ruido por zonas (RNA) para fortalecer las detalles de alta frecuencia. Comparado con los métodos más recientes basados en referencias en modelos de difusión, LSRNA muestra sus ventajas y destaca la importancia del upsampling en el espacio potencial para mantener la variabilidad y la adición de ruido. El código está disponible en https://github.com/3587jjh/LSRNA.",
      "upvotes": 4,
      "discussionId": "67e367f14363e3c4bbbacae1",
      "ai_keywords": [
        "LSRNA",
        "diffusion models",
        "latent space",
        "super-resolution",
        "structural distortions",
        "content repetition",
        "reference-based methods",
        "manifold deviation",
        "RGB space",
        "manifold alignment",
        "Region-wise Noise Addition (RNA)",
        "high-frequency details"
      ]
    },
    "publishedAt": "2025-03-24T04:50:15.000Z",
    "title": "Latent Space Super-Resolution for Higher-Resolution Image Generation\n  with Diffusion Models",
    "summary": "In this paper, we propose LSRNA, a novel framework for higher-resolution\n(exceeding 1K) image generation using diffusion models by leveraging\nsuper-resolution directly in the latent space. Existing diffusion models\nstruggle with scaling beyond their training resolutions, often leading to\nstructural distortions or content repetition. Reference-based methods address\nthe issues by upsampling a low-resolution reference to guide higher-resolution\ngeneration. However, they face significant challenges: upsampling in latent\nspace often causes manifold deviation, which degrades output quality. On the\nother hand, upsampling in RGB space tends to produce overly smoothed outputs.\nTo overcome these limitations, LSRNA combines Latent space Super-Resolution\n(LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance\nhigh-frequency details. Our extensive experiments demonstrate that integrating\nLSRNA outperforms state-of-the-art reference-based methods across various\nresolutions and metrics, while showing the critical role of latent space\nupsampling in preserving detail and sharpness. The code is available at\nhttps://github.com/3587jjh/LSRNA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18446.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "66b5f733f0c16f37f307f35e",
      "avatarUrl": "/avatars/29a97e10b4d65aa23d7eae238f809499.svg",
      "fullname": "JinHo Jeong",
      "name": "3587jjh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13964",
      "authors": [
        {
          "_id": "67e20852c0c932395394dbb0",
          "name": "Siwei Han",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb1",
          "name": "Peng Xia",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb2",
          "name": "Ruiyi Zhang",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb3",
          "name": "Tong Sun",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb4",
          "name": "Yun Li",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb5",
          "name": "Hongtu Zhu",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb6",
          "name": "Huaxiu Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/smmwVmcnReTUxH6xDnyU1.png"
      ],
      "publishedAt": "2025-03-18T06:57:21.000Z",
      "submittedOnDailyAt": "2025-03-26T03:52:37.520Z",
      "title": "MDocAgent: Marco de Framework Multicuenta Multimodel para la Comprensión de Documentos",
      "submittedOnDailyBy": {
        "_id": "643e9ee6f6bb3c31a26e7bc4",
        "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
        "isPro": false,
        "fullname": "Peng Xia",
        "user": "richardxp888",
        "type": "user"
      },
      "summary": "DocQA es una tarea muy común. Los métodos existentes utilizan grandes modelos de lenguaje (LLMs) o grandes modelos de visión y lenguaje (LVLMs) y realizan un procesamiento de búsqueda reforzado generativo (RAG), pero estos métodos priorizan la información de un solo modelo y no pueden integrar efectivamente el texto y la imagen. Este enfoque se vuelve complejo y tiene límites en el rendimiento de documentos reales. Presentamos un nuevo RAG y un marco de trabajo multi-agente para la comprensión de documentos, llamado MDocAgent (marco de trabajo multi-modelo multi-agente). Este marco de trabajo utiliza tanto texto como imagenes. El sistema utiliza 5 agentes profesionales: agente general, agente evaluador, agente de texto, agente de imagen y agente de resumen. Estos agentes se centran en la búsqueda de contexto multi-modelo y combinan sus perspectivas para comprender mejor el contenido del documento. Esta aproximación cooperativa permite que el sistema sintetice la información de los componentes textuales y visuales y mejore la precisión de las respuestas a las preguntas. Los experimentos iniciales en 5 benchmarks como MMLongBench y LongDocURL muestran el efecto de nuestro MDocAgent, con un mejoramiento promedio de 12.1% en comparación con los métodos más superiores actuales. Este estudio contribuye al desarrollo de un sistema más robusto y detallado para la comprensión de documentos complejos. Los datos y código están disponibles en https://github.com/aiming-lab/MDocAgent.",
      "upvotes": 4,
      "discussionId": "67e20858c0c932395394dde6",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Large Vision Language Models (LVLMs)",
        "Retrieval Augmented Generation (RAG)",
        "multi-modal reasoning",
        "multi-modal multi-agent framework",
        "general agent",
        "critical agent",
        "text agent",
        "image agent",
        "summarizing agent",
        "multi-modal context retrieval"
      ]
    },
    "publishedAt": "2025-03-18T02:57:21.000Z",
    "title": "MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding",
    "summary": "Document Question Answering (DocQA) is a very common task. Existing methods\nusing Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and\nRetrieval Augmented Generation (RAG) often prioritize information from a single\nmodal, failing to effectively integrate textual and visual cues. These\napproaches struggle with complex multi-modal reasoning, limiting their\nperformance on real-world documents. We present MDocAgent (A Multi-Modal\nMulti-Agent Framework for Document Understanding), a novel RAG and multi-agent\nframework that leverages both text and image. Our system employs five\nspecialized agents: a general agent, a critical agent, a text agent, an image\nagent and a summarizing agent. These agents engage in multi-modal context\nretrieval, combining their individual insights to achieve a more comprehensive\nunderstanding of the document's content. This collaborative approach enables\nthe system to synthesize information from both textual and visual components,\nleading to improved accuracy in question answering. Preliminary experiments on\nfive benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of\nour MDocAgent, achieve an average improvement of 12.1% compared to current\nstate-of-the-art method. This work contributes to the development of more\nrobust and comprehensive DocQA systems capable of handling the complexities of\nreal-world documents containing rich textual and visual information. Our data\nand code are available at https://github.com/aiming-lab/MDocAgent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/smmwVmcnReTUxH6xDnyU1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13964.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643e9ee6f6bb3c31a26e7bc4",
      "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
      "fullname": "Peng Xia",
      "name": "richardxp888",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19470",
      "authors": [
        {
          "_id": "67e365b0dcfc2aeae1bf3da2",
          "name": "Mingyang Chen",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da3",
          "name": "Tianpeng Li",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da4",
          "name": "Haoze Sun",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da5",
          "name": "Yijie Zhou",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da6",
          "name": "Chenzheng Zhu",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da7",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da8",
          "name": "Zenan Zhou",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da9",
          "name": "Weipeng Chen",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3daa",
          "name": "Haofen Wang",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3dab",
          "name": "Jeff Z. Pan",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3dac",
          "name": "Wen Zhang",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3dad",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T09:00:58.000Z",
      "submittedOnDailyAt": "2025-03-26T00:56:07.098Z",
      "title": "Estudio: Aprovechar el aprendizaje de inferencia en LLMs mediante el uso de búsqueda y aprendizaje por refuerzo.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje de gran escala (LLMs) muestran excelentes capacidades lógicas, lo que se puede ver en el éxito de OpenAI-o1 y DeepSeek-R1. Sin embargo, la integración de la lógica con procesos de búsqueda externa resulta particularmente compleja cuando se trata de preguntas multietipales que requieren varios pasos de búsqueda. Proponemos un nuevo marco de trabajo llamado ReSearch, en el que desarrollamos una lógica que se aprende mediante aprendizaje por refuerzo, lo que permite que no sea necesario tener datos de supervisión para las etapas lógicas. Nuestro enfoque considera la búsqueda como un componente integral de la cadena lógica, y la búsqueda es guiada por la base de pensamiento en sentencias, con el tiempo y el método de búsqueda influyendo en el pensamiento posterior. ReSearch se ha entrenado utilizando modelos como Qwen2.5-7B(-Instruct) y Qwen2.5-32B(-Instruct), lo que permitió realizar experimentos extensos. A pesar de que se entrenó en un solo conjunto de datos, nuestro modelo demostró una fuerte capacidad de generalización en diferentes benchmarks. El análisis muestra que ReSearch desarrolla naturalmente habilidades lógicas progresivas durante el proceso de aprendizaje por refuerzo. Además, funciona efectivamente en diversos aspectos, incluyendo la naturaleza y la autocorrección.",
      "upvotes": 3,
      "discussionId": "67e365b1dcfc2aeae1bf3df6",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "OpenAI-o1",
        "DeepSeek-R1",
        "complex multi-hop questions",
        "ReSearch",
        "reinforcement learning",
        "text-based thinking",
        "reflection",
        "self-correction",
        "Qwen2.5-7B(-Instruct)",
        "Qwen2.5-32B(-Instruct)"
      ]
    },
    "publishedAt": "2025-03-25T05:00:58.000Z",
    "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning",
    "summary": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19470.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6471
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19041",
      "authors": [
        {
          "_id": "67e35da0b1b97cc3392024b1",
          "name": "Kangwei Liu",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b2",
          "name": "Mengru Wang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b3",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b4",
          "name": "Lin Yuan",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b5",
          "name": "Mengshu Sun",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b6",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b7",
          "name": "Lei Liang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b8",
          "name": "Zhiqiang Zhang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b9",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024ba",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/f0fTHNDhXrS7zWpmuxVU-.png"
      ],
      "publishedAt": "2025-03-24T18:11:42.000Z",
      "submittedOnDailyAt": "2025-03-26T00:22:20.466Z",
      "title": "LookAhead Tuning: Modelos de Lenguaje más Seguros a través de Previsualizaciones Parciales de Respuestas",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "En el fine-tuning, modelos de lenguaje grandes (LLMs) pueden adaptarse a un dominio específico, pero si se utilizan directamente, la regulación de seguridad puede desmoronarse. Para resolver este problema, hemos introducido LookAhead Tuning, que consiste en dos métodos sencillos y de baja demanda de recursos efectivos. Estos métodos buscan mantener la estructura de seguridad propia del modelo y minimizar las variaciones en la distribución de los primeros tokens, cambiando algunas respuestas previamente predecidas en el conjunto de datos de entrenamiento. Los experimentos concretos muestran que LookAhead Tuning puede mantener la seguridad del modelo sin sacrificar el rendimiento en tareas de descarga, demostrando su eficacia. Nuestros hallazgos constituyen una confianza en la capacidad de adaptación segura y efectiva de los LLMs, así como en soluciones eficientes. El código está disponible en https://github.com/zjunlp/LookAheadTuning.",
      "upvotes": 3,
      "discussionId": "67e35da1b1b97cc339202525",
      "ai_keywords": [
        "LookAhead Tuning",
        "safety alignment",
        "data-driven methods",
        "partial answer prefixes",
        "token distributions",
        "robust performance",
        "downstream tasks"
      ]
    },
    "publishedAt": "2025-03-24T14:11:42.000Z",
    "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
    "summary": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often undermines their previously established safety alignment. To\nmitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, which comprises two simple, low-resource, and effective\ndata-driven methods that modify training data by previewing partial answer\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\nby minimizing perturbations to initial token distributions. Comprehensive\nexperiments demonstrate that LookAhead Tuning effectively maintains model\nsafety without sacrificing robust performance on downstream tasks. Our findings\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs. Code is released at\nhttps://github.com/zjunlp/LookAheadTuning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/f0fTHNDhXrS7zWpmuxVU-.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19041.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18783",
      "authors": [
        {
          "_id": "67e2a43d5116df47da357eec",
          "user": {
            "_id": "642438eaa3adbc7142c3ca0f",
            "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
            "isPro": false,
            "fullname": "CharlesChen",
            "user": "CharlesChen2023",
            "type": "user"
          },
          "name": "Linwei Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T14:36:22.430Z",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357eed",
          "name": "Lin Gu",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357eee",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357eef",
          "name": "Chenggang Yan",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357ef0",
          "name": "Ying Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T15:32:06.000Z",
      "submittedOnDailyAt": "2025-03-26T01:08:28.390Z",
      "title": "Frequency Dynamic Convolution for Dense Image Prediction",
      "submittedOnDailyBy": {
        "_id": "642438eaa3adbc7142c3ca0f",
        "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
        "isPro": false,
        "fullname": "CharlesChen",
        "user": "CharlesChen2023",
        "type": "user"
      },
      "summary": "DY-Conv combina múltiples pesos paralelos y funciones de atención para permitir la selección adaptativa de pesos, demostrando un desempeño notable, pero con respuestas frecuenciales muy similares y un alto costo de parámetros asociado con una limitada adaptabilidad. En este artículo, se presenta una nueva aproximación denominada Frequency Dynamic Convolution (FDConv), que aprende vectores de parámetros fijos en el dominio de la frecuencia para mitigar estas limitaciones. FDConv divide los vectores de parámetros en grupos basados en índices de frecuencia diferentes, manteniendo la diversidad frecuencial mientras evita el aumento del costo de parámetros. Para mejorar la adaptabilidad, se propone el Kernel Spatial Modulation (KSM) y el Frequency Band Modulation (FBM). KSM ajusta la respuesta frecuencial de cada filtro de manera dinámica en el nivel espacial, mientras que FBM ajusta los pesos decomposidos en diferentes bandas de frecuencia de acuerdo con el contenido local, mejorando así la adaptabilidad. Se realizan experimentos en diversas áreas como detección de objetos, segmentación y clasificación para demostrar los beneficios de FDConv. Al aplicar FDConv en ResNet-50, se logra un desempeño superior con un poco de aumento en el costo de parámetros (+3.6M parámetros), sin necesidad de un gran incremento como en CondConv (+90M parámetros) o KW (+76.5M parámetros). Además, FDConv se integra fácilmente en diferentes arquitecturas como ConvNeXt y Swin-Transformer, proporcionando soluciones flexibles y eficientes para tareas visuales modernas. El código está disponible en https://github.com/Linwei-Chen/FDConv.",
      "upvotes": 2,
      "discussionId": "67e2a4405116df47da357ff7",
      "ai_keywords": [
        "Dynamic Convolution (DY-Conv)",
        "Frequency Dynamic Convolution (FDConv)",
        "attention mechanism",
        "parameter budget",
        "Fourier domain",
        "frequency-based groups",
        "disjoint Fourier indices",
        "frequency-diverse weights",
        "Kernel Spatial Modulation (KSM)",
        "Frequency Band Modulation (FBM)",
        "frequency response",
        "spatial level",
        "frequency bands",
        "local content",
        "object detection",
        "segmentation",
        "classification",
        "ResNet-50",
        "ConvNeXt",
        "Swin-Transformer",
        "parameter-efficient"
      ]
    },
    "publishedAt": "2025-03-24T11:32:06.000Z",
    "title": "Frequency Dynamic Convolution for Dense Image Prediction",
    "summary": "While Dynamic Convolution (DY-Conv) has shown promising performance by\nenabling adaptive weight selection through multiple parallel weights combined\nwith an attention mechanism, the frequency response of these weights tends to\nexhibit high similarity, resulting in high parameter costs but limited\nadaptability. In this work, we introduce Frequency Dynamic Convolution\n(FDConv), a novel approach that mitigates these limitations by learning a fixed\nparameter budget in the Fourier domain. FDConv divides this budget into\nfrequency-based groups with disjoint Fourier indices, enabling the construction\nof frequency-diverse weights without increasing the parameter cost. To further\nenhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency\nBand Modulation (FBM). KSM dynamically adjusts the frequency response of each\nfilter at the spatial level, while FBM decomposes weights into distinct\nfrequency bands in the frequency domain and modulates them dynamically based on\nlocal content. Extensive experiments on object detection, segmentation, and\nclassification validate the effectiveness of FDConv. We demonstrate that when\napplied to ResNet-50, FDConv achieves superior performance with a modest\nincrease of +3.6M parameters, outperforming previous methods that require\nsubstantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).\nMoreover, FDConv seamlessly integrates into a variety of architectures,\nincluding ConvNeXt, Swin-Transformer, offering a flexible and efficient\nsolution for modern vision tasks. The code is made publicly available at\nhttps://github.com/Linwei-Chen/FDConv.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18783.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642438eaa3adbc7142c3ca0f",
      "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
      "fullname": "CharlesChen",
      "name": "CharlesChen2023",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17361",
      "authors": [
        {
          "_id": "67e35ca7363374850440d91d",
          "name": "Sophia Tang",
          "hidden": false
        },
        {
          "_id": "67e35ca7363374850440d91e",
          "name": "Yinuo Zhang",
          "hidden": false
        },
        {
          "_id": "67e35ca7363374850440d91f",
          "name": "Alexander Tong",
          "hidden": false
        },
        {
          "_id": "67e35ca7363374850440d920",
          "user": {
            "_id": "64cd5b3f0494187a9e8b7c69",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
            "isPro": false,
            "fullname": "Pranam Chatterjee",
            "user": "pranamanam",
            "type": "user"
          },
          "name": "Pranam Chatterjee",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-26T01:57:51.167Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:59:43.000Z",
      "submittedOnDailyAt": "2025-03-26T00:18:51.908Z",
      "title": "Gelmar Soft Matching Flow y GUI directa para la generación de arreglos biológicos controlables",
      "submittedOnDailyBy": {
        "_id": "64cd5b3f0494187a9e8b7c69",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
        "isPro": false,
        "fullname": "Pranam Chatterjee",
        "user": "pranamanam",
        "type": "user"
      },
      "summary": "El flujo de formación en la simplificación de noticias ha aparecido como una prometedora estrategia para el diseño de secuencias de ADN, aunque es difícil escalarlo a simplificaciones de alto nivel necesarias para la generación de peptidos y proteínas. Hemos introducido un nuevo interpretador de gomerbal-softmax con temperatura dependiente como base para un marco de generación en simplificaciones, junto con el flujo de gomerbal-softmax y el score-matching. Usando este interpretador, hemos extraído el flujo de gomerbal-softmax de formación y introducido un campo de velocidad parametrizado que permite mover de una distribución categórica suave hacia una distribución concentrada en un vértice de la simplificación. Además, proponemos el score-matching de gomerbal-softmax, aprendiendo gradientes de densidad de probabilidad para realizar regresión logística. Nuestro marco de trabajo permite generaciones de alta calidad y diversidad, y es escalable a simplificaciones de alto nivel. Proponemos un flujo de guía basado en clasificadores (STGFlow) para facilitar la generación sin entrenamiento, utilizando un clasificador pretrenado con secuencias limpias para ejecutar el flujo de guía eficientemente durante la inferencia. STGFlow puede combinarse con cualquier método de flujo discreto y forma un sólido marco para la generación de secuencias de desnudo controlable. Demostramos la capacidad de nuestro marco para diseños de promotores de ADN condicionales, generación de proteínas a partir de secuencias, y diseño de peptidos de unión objetivo para tratamientos de enfermedades raras.",
      "upvotes": 1,
      "discussionId": "67e35caa363374850440d9df",
      "ai_keywords": [
        "Gumbel-Softmax Flow",
        "Score Matching",
        "simplex",
        "Gumbel-Softmax interpolant",
        "time-dependent temperature",
        "parameterized velocity field",
        "smooth categorical distributions",
        "Gumbel-Softmax Flow Matching",
        "Straight-Through Guided Flows",
        "STGFlow",
        "straight-through estimators",
        "classifiers",
        "de novo sequence generation",
        "conditional DNA promoter design",
        "sequence-only protein generation",
        "target-binding peptide design"
      ]
    },
    "publishedAt": "2025-03-21T13:59:43.000Z",
    "title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
    "summary": "Flow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\nand Score Matching, a generative framework on the simplex based on a novel\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical\ndistributions to distributions concentrated at a single vertex of the simplex.\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\nthe gradient of the probability density. Our framework enables high-quality,\ndiverse generation and scales efficiently to higher-dimensional simplices. To\nenable training-free guidance, we propose Straight-Through Guided Flows\n(STGFlow), a classifier-based guidance method that leverages straight-through\nestimators to steer the unconditional velocity field toward optimal vertices of\nthe simplex. STGFlow enables efficient inference-time guidance using\nclassifiers pre-trained on clean sequences, and can be used with any discrete\nflow method. Together, these components form a robust framework for\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\nperformance in conditional DNA promoter design, sequence-only protein\ngeneration, and target-binding peptide design for rare disease treatment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17361.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cd5b3f0494187a9e8b7c69",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
      "fullname": "Pranam Chatterjee",
      "name": "pranamanam",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17237",
      "authors": [
        {
          "_id": "67e2b68e08c6a250edda264a",
          "user": {
            "_id": "67e2063e1ee7f6db889849d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
            "isPro": false,
            "fullname": "Yu-Hsi Chen",
            "user": "wish44165",
            "type": "user"
          },
          "name": "Yu-Hsi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T14:35:46.455Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67e2063e1ee7f6db889849d6/Dn_qgqu4a6nI8HIAdpDvW.mp4"
      ],
      "publishedAt": "2025-03-21T15:40:18.000Z",
      "submittedOnDailyAt": "2025-03-26T04:35:14.607Z",
      "title": "Baseline fuerte: YOLOv12 y BoT-SORT-ReID para el flirting multi-objecto",
      "submittedOnDailyBy": {
        "_id": "67e2063e1ee7f6db889849d6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
        "isPro": false,
        "fullname": "Yu-Hsi Chen",
        "user": "wish44165",
        "type": "user"
      },
      "summary": "Detectar y seguir una gran cantidad de aviones sin piloto (UAV) en videos de infrarrojo de temperatura es un problema inherentemente difícil debido a bajas contrastes, ruido ambiental y pequeñas dimensiones de los objetivos. Este artículo proporciona una aproximación sencilla para resolver este problema en videos de infrarrojo de temperatura, utilizando los avances recientes en detección y seguimiento. Se propone un marco de seguimiento basado en YOLOv12 y BoT-SORT, sin depender de YOLOv5 y DeepSORT, y se mejoró mediante estrategias de entrenamiento y inferencia adecuadas. Se evaluó con métricas para el desafío de UAV de la cuarta generación, mostrando un excelente rendimiento. En particular, se obtuvieron resultados característicos para seguir múltiples UAV sin necesidad de fortalecer el contraste o integrar información de secuencia, sino fortaleciendo las características propias de los UAV. Se proporcionan detalles de la implementación, un análisis exhaustivo de experimentos y una discusión sobre posibles mejoras. El código está disponible en https://github.com/wish44165/YOLOv12-BoT-SORT-ReID.",
      "upvotes": 1,
      "discussionId": "67e2b69108c6a250edda279f",
      "githubRepo": "https://github.com/wish44165/YOLOv12-BoT-SORT-ReID",
      "ai_keywords": [
        "YOLOv12",
        "BoT-SORT",
        "multi-UAV tracking",
        "thermal infrared video",
        "detection",
        "tracking",
        "tailored training",
        "inference strategies",
        "4th Anti-UAV Challenge",
        "contrast enhancement",
        "temporal information fusion",
        "UAV features",
        "Strong Baseline"
      ]
    },
    "publishedAt": "2025-03-21T11:40:18.000Z",
    "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
    "summary": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\nour approach following the metrics from the 4th Anti-UAV Challenge and\ndemonstrate competitive performance. Notably, we achieve strong results without\nusing contrast enhancement or temporal information fusion to enrich UAV\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\ntracking task. We provide implementation details, in-depth experimental\nanalysis, and a discussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67e2063e1ee7f6db889849d6/Dn_qgqu4a6nI8HIAdpDvW.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17237.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67e2063e1ee7f6db889849d6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
      "fullname": "Yu-Hsi Chen",
      "name": "wish44165",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16965",
      "authors": [
        {
          "_id": "67e35c3bf049c252c672b824",
          "name": "Zhe Hu",
          "hidden": false
        },
        {
          "_id": "67e35c3bf049c252c672b825",
          "name": "Jing Li",
          "hidden": false
        },
        {
          "_id": "67e35c3bf049c252c672b826",
          "name": "Yu Yin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T09:25:23.000Z",
      "submittedOnDailyAt": "2025-03-26T00:20:32.465Z",
      "title": "Cuando una palabra es más poderosa que la visión: Los VLMs pueden realizar entrenamiento centrado en la percepción humana y mejorar automáticamente con solo el texto.",
      "submittedOnDailyBy": {
        "_id": "63999a6fe657365725d0d0a4",
        "avatarUrl": "/avatars/99736de1bc0d5decf4a6eda86e3c7937.svg",
        "isPro": false,
        "fullname": "Derek Zhe Hu",
        "user": "zhehuderek",
        "type": "user"
      },
      "summary": "La decisión de entorno es esencial para que las IAs agentes funcionen en entornos reales. Los modelos de lenguaje visivo (VLMs) han mejorado esta capacidad, pero aún en situaciones humanocentrícas requieren profundas razones para tomar complejos juicios, lo cual les presenta dificultades. En este estudio, se evaluaron de manera sistemática VLMs abiertos en tareas de procesamiento de juicios humanocentrícas. Aunque los modelos de lenguaje grande (LLMs) asumen una escala similar a los VLMs, mostraron un rendimiento sorprendente solo en contextos textuales, lo que indica que su capacidad puede ser afectada por el manejo de imágenes. En respuesta a estas desafíos, se propone un nuevo enfoque de entrenamiento utilizando datos de contexto sintéticos. Este método fortalece los componentes lingüísticos de los VLMs, permite transferir las habilidades aprendidas a diversas inferencias y elimina la necesidad de datos de pares imagen-contexto costosos. Además, los VLMs pueden mejorar su rendimiento utilizando datos de entrenamiento generados por el contexto de un LLM, sin depender de un modelo enseñante más grande como el GPT-4, demostrando una mejora significativa. Estos hallazgos recomiendan un enfoque más eficiente y expandible para fortalecer la capacidad de juicios humanocentrícos de los VLMs y abren una nueva ruta para optimizar los VLMs a través de estructuras de mejora automática.",
      "upvotes": 1,
      "discussionId": "67e35c3cf049c252c672b859",
      "ai_keywords": [
        "Visual Language Models (VLMs)",
        "multimodal human-centered decision-making tasks",
        "Large Language Models (LLMs)",
        "textual descriptions",
        "visual alignment",
        "text-only training approach",
        "synthesized textual data",
        "self-improvement",
        "training data",
        "GPT-4",
        "human-centered decision-making capabilities"
      ]
    },
    "publishedAt": "2025-03-21T05:25:23.000Z",
    "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making",
    "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16965.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63999a6fe657365725d0d0a4",
      "avatarUrl": "/avatars/99736de1bc0d5decf4a6eda86e3c7937.svg",
      "fullname": "Derek Zhe Hu",
      "name": "zhehuderek",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.11849",
      "authors": [
        {
          "_id": "67e3d0ac304f166b665e4a67",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a68",
          "name": "Zhitong Xiong",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a69",
          "name": "Chenying Liu",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6a",
          "name": "Adam J. Stewart",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6b",
          "name": "Thomas Dujardin",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6c",
          "name": "Nikolaos Ioannis Bountos",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6d",
          "name": "Angelos Zavras",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6e",
          "name": "Franziska Gerken",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6f",
          "name": "Ioannis Papoutsis",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a70",
          "name": "Laura Leal-Taixé",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a71",
          "name": "Xiao Xiang Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T20:16:48.000Z",
      "submittedOnDailyAt": "2025-03-26T08:34:37.209Z",
      "title": "A propósito del modelo Copérnico sobre la visión de la Tierra",
      "submittedOnDailyBy": {
        "_id": "64cba974a81988d0734c9925",
        "avatarUrl": "/avatars/645c326ca38eb751144f356076cef60f.svg",
        "isPro": false,
        "fullname": "Yi Wang",
        "user": "wangyi111",
        "type": "user"
      },
      "summary": "El desarrollo de modelos básicos de observación de la Tierra (EO) ha desarrollado la posibilidad de aprender representaciones generales en el espacio a partir de grandes cantidades de datos satelitales, lo que beneficia a diversas aplicaciones importantes sobre la Tierra. Sin embargo, la mayoría de los esfuerzos actuales están limitados a sensores de espectro fijo, se concentran en la superficie de la Tierra y ignoran metadatos más valiosos que las imágenes. En este estudio, se presenta un paso hacia los modelos básicos de EO de la siguiente generación, introduciendo tres componentes clave: 1) Copernicus-Pretrain, un gran conjunto de datos de predicción basado en imágenes de array de 1.870 misiones de sensoristas, que amplían el rango desde la superficie de la Tierra hasta el aire. 2) Copernicus-FM, una red neuronal dinámica extendida y un codificador de metadatos flexible, permitiendo un modelo básico unificado que puede procesar cualquier tipo de sensor de espectro o no espectro. 3) Copernicus-Bench, un marco de referencia que configura 15 tareas de aplicación específicas para cada misión de sensorista, desde el preprocesamiento hasta las tareas de aplicación, y evalúa sistematicamente el sistema. Nuestro conjunto de datos, modelos y marco de referencia mejoran significativamente la escalabilidad, variación y diversidad de los modelos básicos de EO, creando nuevas conexiones con la investigación en EO, meteorología y climatología. Código, conjunto de datos y modelos están disponibles en https://github.com/zhu-xlab/Copernicus-FM.",
      "upvotes": 0,
      "discussionId": "67e3d0af304f166b665e4b68",
      "githubRepo": "https://github.com/zhu-xlab/Copernicus-FM"
    },
    "publishedAt": "2025-03-14T16:16:48.000Z",
    "title": "Towards a Unified Copernicus Foundation Model for Earth Vision",
    "summary": "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps://github.com/zhu-xlab/Copernicus-FM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11849.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cba974a81988d0734c9925",
      "avatarUrl": "/avatars/645c326ca38eb751144f356076cef60f.svg",
      "fullname": "Yi Wang",
      "name": "wangyi111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  }
]