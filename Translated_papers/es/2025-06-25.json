[
  {
    "paper": {
      "id": "2506.19851",
      "authors": [
        {
          "_id": "685b5a46d2ee4fac76521dce",
          "user": {
            "_id": "6375d136dee28348a9c63cbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
            "isPro": false,
            "fullname": "zehuan-huang",
            "user": "huanngzh",
            "type": "user"
          },
          "name": "Zehuan Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-25T08:19:21.031Z",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dcf",
          "user": {
            "_id": "65240d0ca801972b6eb12ed8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65240d0ca801972b6eb12ed8/hl2RAssBperb5JlgOIDvw.jpeg",
            "isPro": false,
            "fullname": "Haoran Feng",
            "user": "fenghora",
            "type": "user"
          },
          "name": "Haoran Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T09:19:31.409Z",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd0",
          "user": {
            "_id": "63a41cb584a6a25c65bd8316",
            "avatarUrl": "/avatars/1d474831c320c7f9ca9e6d88f68acc06.svg",
            "isPro": false,
            "fullname": "Yangtian Sun",
            "user": "Yang-Tian",
            "type": "user"
          },
          "name": "Yangtian Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T09:19:29.490Z",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd1",
          "name": "Yuanchen Guo",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd2",
          "user": {
            "_id": "638066faf022c8a5803f7eb8",
            "avatarUrl": "/avatars/4cfd699c3f6c5461b12b7dc5e3fe183d.svg",
            "isPro": false,
            "fullname": "Yanpei Cao",
            "user": "pookiefoof",
            "type": "user"
          },
          "name": "Yanpei Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-25T08:21:27.467Z",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd3",
          "user": {
            "_id": "65b722dbe02a17f0f8d1cc6b",
            "avatarUrl": "/avatars/65f20601ef9b8ebfdddadd737f9153d6.svg",
            "isPro": false,
            "fullname": "Lu Sheng",
            "user": "lsheng2024",
            "type": "user"
          },
          "name": "Lu Sheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T09:19:33.186Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a96a375a69e2ca889abdff/ttfXuQOJQmOqnO-_-F1op.mp4"
      ],
      "publishedAt": "2025-06-24T17:59:58.000Z",
      "submittedOnDailyAt": "2025-06-25T01:12:40.364Z",
      "title": "アニマX: Video de movimiento de microorganismos en 3D - Modelo de Diferenciación de Posiciones (PoseDIFュージョンモデル)",
      "submittedOnDailyBy": {
        "_id": "64a96a375a69e2ca889abdff",
        "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
        "isPro": false,
        "fullname": "fanhongxing",
        "user": "fanhongxing",
        "type": "user"
      },
      "summary": "AnimaX es un framework de animación 3D en profundidad, que combina el método de síntesis de movimientos en 3D con la estructura controlable de la animación basada en esqueletos. Los métodos de síntesis de movimientos existentes están limitados a esqueletos fijos o requieren un óptimo costoso en el espacio de deformación de alta dimensión. En cambio, AnimaX efectivamente transmite el conocimiento de movimientos basado en videos a las áreas 3D, y apoya artesanales con diferentes esqueletos. En nuestro enfoque, se representan movimientos 3D en mapas de posiciones 2D multi-angulares y multi-frame, y se puede binarizar continuos vídeos y poses utilizando template learning y prompts basados en cadenas de caracteres. Además, se introducen codificaciones de alineación espacial-temporal para garantizar la correspondencia entre secuencias de vídeos y poses, utilizando codificaciones de ubicación compartidas y modelos de reconocimiento. Así, se puede transmitir eficazmente el control de vídeos a la tarea de generación de movimientos. Además, estas secuencias de poses multi-angulares se triangulan y se convierten en posiciones de articulaciones 3D, que luego se transforman en animación de la superficie mediante la cinemática inversa. AnimaX, entrenada con un nuevo dataset (160,000 secuencias de props), obtiene los resultados más avanzados en generalización, precisión y eficiencia en VBench, ofreciendo una solución escalable para la animación 3D sin categoría. La página del proyecto está disponible en https://anima-x.github.io/.",
      "upvotes": 30,
      "discussionId": "685b5a47d2ee4fac76521dd4",
      "projectPage": "https://anima-x.github.io/",
      "githubRepo": "https://github.com/anima-x/anima-x",
      "ai_summary": "AnimaX creates multi-skeleton 3D animations by blending video diffusion model priors with skeleton-based control, using joint video-pose diffusion and shared positional encodings.",
      "ai_keywords": [
        "feed-forward 3D animation framework",
        "video diffusion models",
        "skeleton-based animation",
        "motion synthesis",
        "high-dimensional deformation spaces",
        "2D pose maps",
        "joint video-pose diffusion",
        "template renderings",
        "textual motion prompt",
        "shared positional encodings",
        "modality-aware embeddings",
        "spatial-temporal alignment",
        "inverse kinematics",
        "VBench",
        "category-agnostic 3D animation"
      ],
      "githubStars": 34
    },
    "publishedAt": "2025-06-24T13:59:58.000Z",
    "title": "AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion\n  Models",
    "summary": "We present AnimaX, a feed-forward 3D animation framework that bridges the\nmotion priors of video diffusion models with the controllable structure of\nskeleton-based animation. Traditional motion synthesis methods are either\nrestricted to fixed skeletal topologies or require costly optimization in\nhigh-dimensional deformation spaces. In contrast, AnimaX effectively transfers\nvideo-based motion knowledge to the 3D domain, supporting diverse articulated\nmeshes with arbitrary skeletons. Our method represents 3D motion as multi-view,\nmulti-frame 2D pose maps, and enables joint video-pose diffusion conditioned on\ntemplate renderings and a textual motion prompt. We introduce shared positional\nencodings and modality-aware embeddings to ensure spatial-temporal alignment\nbetween video and pose sequences, effectively transferring video priors to\nmotion generation task. The resulting multi-view pose sequences are\ntriangulated into 3D joint positions and converted into mesh animation via\ninverse kinematics. Trained on a newly curated dataset of 160,000 rigged\nsequences, AnimaX achieves state-of-the-art results on VBench in\ngeneralization, motion fidelity, and efficiency, offering a scalable solution\nfor category-agnostic 3D animation. Project page:\nhttps://anima-x.github.io/{https://anima-x.github.io/}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a96a375a69e2ca889abdff/ttfXuQOJQmOqnO-_-F1op.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19851.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a96a375a69e2ca889abdff",
      "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
      "fullname": "fanhongxing",
      "name": "fanhongxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16141",
      "authors": [
        {
          "_id": "6858b1fac0c8e29df8ea3c18",
          "name": "Yi Chen",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c19",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1a",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1b",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1c",
          "name": "Junhao Cheng",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1d",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1e",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T08:49:13.000Z",
      "submittedOnDailyAt": "2025-06-25T01:50:33.428Z",
      "title": "GRPO-CARE: Aprendizaje por Reforzamiento Orientado a la Consistencia para la Inferencia Multi-Modular",
      "submittedOnDailyBy": {
        "_id": "60d045c4778bafd0fbcfa3f5",
        "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
        "isPro": false,
        "fullname": "Yi Chen",
        "user": "ChenYi99",
        "type": "user"
      },
      "summary": "Recientemente, el enfoque de aprendizaje reforzado llamado Resultado Observación Política Global (GRPO) ha promovido el desarrollo de la razón de hecho en la procesamiento de lenguaje de modelos de lenguaje grandes (LLMs), pero esta aproximación no se ha aplicado a los modelos de lenguaje multimodales (MLLMs). Para resolver la falta de evaluaciones estrictas de los métodos de entrenamiento de MLLMs, se introdujo el benchmark SEED-Bench-R1. Este benchmark requiere una reconocimiento equilibrado en imágenes realistas complejas y una serie lógica de razones, y proporciona un conjunto de datos de entrenamiento grande para evaluar la generalización en tres niveles: distribución interna, entre entornos y entre tareas. Al utilizar SEED-Bench-R1, el GRPO estándar mejoró la precisión de las respuestas pero disminuyó la coincidencia entre las razones lógicas y las respuestas, con un porcentaje de coincidencia de 57.9%. Esto se debe a que el GRPO se centra en la respuesta final, promueve caminos cortos y limita la exploración con penalizaciones estrictas de KL. En respuesta a esto, se propone el framework de aprendizaje reforzado GRPO-CARE, que optimiza tanto la precisión de las respuestas como la coincidencia de las razones lógicas, sin necesidad de supervisión explícita. GRPO-CARE introduce dos tipos de recompensas: (1) una recompensa básica para la precisión de la respuesta y (2) un bonus de mejora adaptativo para la coincidencia de las razones lógicas. Este bonus se calcula utilizando un modelo de referencia que se desarrolla gradualmente y compara pares de modelos. Esta estructura doble amplifica la recompensa para rutas lógicas precisas. Reemplazando la penalización de KL con este bonus adaptativo, GRPO-CARE supera al GRPO estándar en SEED-Bench-R1, logrando un aumento de 6.7% en el rendimiento y un aumento de 24.5% en la coincidencia en los niveles más difíciles de evaluación. Además, muestra una fuerte transferencia y mejora el rendimiento del modelo en diferentes benchmarks de comprensión de imágenes. Nuestro estudio proporciona un benchmark diseñado sistemáticamente y un framework de entrenamiento posterior que puede generalizarse, contribuyendo a la desarrollo de MLLMs analíticos y robustos.",
      "upvotes": 22,
      "discussionId": "6858b1fac0c8e29df8ea3c1f",
      "githubRepo": "https://github.com/TencentARC/GRPO-CARE",
      "ai_summary": "GRPO-CARE, a reinforcement learning framework optimizing for consistency and correctness, outperforms standard GRPO on a new video understanding benchmark, SEED-Bench-R1, improving both performance and logical coherence in multimodal large language models.",
      "ai_keywords": [
        "reinforcement learning",
        "outcome-supervised GRPO",
        "Chain-of-Thought reasoning",
        "large language models",
        "multimodal large language models",
        "SEED-Bench-R1",
        "in-distribution",
        "cross-environment",
        "cross-environment-task",
        "logical coherence",
        "reasoning steps",
        "answer accuracy",
        "reward signals",
        "shortcuts",
        "KL penalties",
        "exploration",
        "consistency-aware RL framework",
        "two-tiered reward",
        "reasoning-to-answer likelihood",
        "adaptive consistency bonus",
        "video understanding benchmarks",
        "transferability",
        "interpretable models",
        "robust models"
      ],
      "githubStars": 19
    },
    "publishedAt": "2025-06-19T04:49:13.000Z",
    "title": "GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal\n  Reasoning",
    "summary": "Recent reinforcement learning approaches, such as outcome-supervised GRPO,\nhave advanced Chain-of-Thought reasoning in large language models (LLMs), yet\ntheir adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack\nof rigorous evaluation for MLLM post-training methods, we introduce\nSEED-Bench-R1, a benchmark with complex real-world videos requiring balanced\nperception and reasoning. It offers a large training set and evaluates\ngeneralization across three escalating challenges: in-distribution,\ncross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,\nwe find that standard GRPO, while improving answer accuracy, often reduces\nlogical coherence between reasoning steps and answers, with only a 57.9%\nconsistency rate. This stems from reward signals focusing solely on final\nanswers, encouraging shortcuts, and strict KL penalties limiting exploration.To\naddress this, we propose GRPO-CARE, a consistency-aware RL framework optimizing\nboth answer correctness and reasoning coherence without explicit supervision.\nGRPO-CARE introduces a two-tiered reward: (1) a base reward for answer\ncorrectness, and (2) an adaptive consistency bonus, computed by comparing the\nmodel's reasoning-to-answer likelihood (via a slowly-evolving reference model)\nagainst group peers.This dual mechanism amplifies rewards for reasoning paths\nthat are both correct and logically consistent. Replacing KL penalties with\nthis adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,\nachieving a 6.7% performance gain on the hardest evaluation level and a 24.5%\nimprovement in consistency. It also shows strong transferability, improving\nmodel performance across diverse video understanding benchmarks. Our work\ncontributes a systematically designed benchmark and a generalizable\npost-training framework, advancing the development of more interpretable and\nrobust MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16141.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d045c4778bafd0fbcfa3f5",
      "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
      "fullname": "Yi Chen",
      "name": "ChenYi99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19848",
      "authors": [
        {
          "_id": "685b7cc2d2ee4fac76521e83",
          "name": "Long Xing",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e84",
          "user": {
            "_id": "656f1b21b075b63c90ba02ee",
            "avatarUrl": "/avatars/d6856815ef06261394178161e4d511b4.svg",
            "isPro": false,
            "fullname": "Huang Qidong",
            "user": "shikiw",
            "type": "user"
          },
          "name": "Qidong Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:23.757Z",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e85",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e86",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e87",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:21.449Z",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e88",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e89",
          "name": "Jinsong Li",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8a",
          "name": "Shuangrui Ding",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8b",
          "name": "Weiming Zhang",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8c",
          "name": "Nenghai Yu",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8d",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8e",
          "name": "Feng Wu",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8f",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-25T03:07:04.508Z",
      "title": "Escalado Captcha: Corrección de la bias en un módulo doble para captcha de imagenes escalables en el inferencia",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "En este artículo, se presenta ScaleCap, una etapa de captura de imágenes inferible y escalable, y se explica cómo se generan capturas detalladas de imágenes. Uno de los principales problemas de capturas de imágenes de alta calidad es la inclinación propia de los LVLM (Modelos de Visión y Lenguaje Grandes): la inclinación de la diversidad puede causar un desbalance en la calidad de las descripciones, ya que algunos elementos reciben una descripción detallada mientras que otros no; la inclinación lingüística puede generar descripciones distorsionadas para objetos que no existen en el lenguaje. Para resolver estos problemas, ScaleCap propone una etapa de captura de imágenes que no requiere un dispositivo escalable y que puede continuamente enriquecer y ajustar las capturas, aunque esto aumente los costos de inferencia. Específicamente, propone dos nuevos componentes: respuestas a preguntas heurísticas y evaluación contextual de frases. La primera genera y responde preguntas relacionadas con el contenido de la imagen, lo que permite introducir información en las capturas de imágenes de manera gradual. La segunda utiliza una evaluación contextual offline de frases para identificar y eliminar las distorsiones causadas por la inclinación lingüística. Aunque los costos de inferencia aumentan, ScaleCap detecta gradualmente más detalles visuales, genera capturas más precisas, equilibradas y ricas en información. Los experimentos amplios mostraron los efectos de ScaleCap. Se explicaron 450K imágenes con ScaleCap, y después de su uso en la entrenamiento previo de LVLM, se observó un mejoramiento continuo en 11 marcos de referencia ampliamente utilizados. Además, ScaleCap realiza dos tareas adicionales: reemplazar las imágenes con capturas y reconstruir las imágenes a partir de las capturas para evaluar la cobertura significativa, mostrando así la riqueza y fidelidad de las capturas generadas. El código está disponible en https://github.com/Cooperx521/ScaleCap.",
      "upvotes": 19,
      "discussionId": "685b7cc2d2ee4fac76521e90",
      "githubRepo": "https://github.com/Cooperx521/ScaleCap",
      "ai_summary": "ScaleCap enhances image captioning by iteratively enriching and calibrating captions using heuristic question answering and contrastive sentence rating, addressing multimodal and linguistic biases to improve accuracy, balance, and informativeness.",
      "ai_keywords": [
        "LVLMs",
        "multimodal bias",
        "linguistic bias",
        "heuristic question answering",
        "contrastive sentence rating",
        "VQA task"
      ],
      "githubStars": 22
    },
    "publishedAt": "2025-06-24T13:59:55.000Z",
    "title": "ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality\n  Debiasing",
    "summary": "This paper presents ScaleCap, an inference-time scalable image captioning\nstrategy that generates comprehensive and detailed image captions. The key\nchallenges of high-quality image captioning lie in the inherent biases of\nLVLMs: multimodal bias resulting in imbalanced descriptive granularity,\noffering detailed accounts of some elements while merely skimming over others;\nlinguistic bias leading to hallucinated descriptions of non-existent objects.\nTo address these issues, we propose a scalable debiased captioning strategy,\nwhich continuously enriches and calibrates the caption with increased inference\nbudget. Specifically, we propose two novel components: heuristic question\nanswering and contrastive sentence rating. The former generates\ncontent-specific questions based on the image and answers them to progressively\ninject relevant information into the caption. The latter employs sentence-level\noffline contrastive decoding to effectively identify and eliminate\nhallucinations caused by linguistic biases. With increased inference cost, more\nheuristic questions are raised by ScaleCap to progressively capture additional\nvisual details, generating captions that are more accurate, balanced, and\ninformative. Extensive modality alignment experiments demonstrate the\neffectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them\nfor LVLM pretraining leads to consistent performance gains across 11 widely\nused benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity\nof generated captions with two additional tasks: replacing images with captions\nin VQA task, and reconstructing images from captions to assess semantic\ncoverage. Code is available at https://github.com/Cooperx521/ScaleCap.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19848.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19290",
      "authors": [
        {
          "_id": "685b6640d2ee4fac76521e42",
          "user": {
            "_id": "6621efe1a6eec3ad03e38759",
            "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
            "isPro": false,
            "fullname": "Liang Zeng",
            "user": "zengliangcs",
            "type": "user"
          },
          "name": "Liang Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:30.529Z",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e43",
          "user": {
            "_id": "612cfc6e1f69b222aacf831b",
            "avatarUrl": "/avatars/b6c7d15ebc7b5dd4b56620bfab324c77.svg",
            "isPro": false,
            "fullname": "lycfight",
            "user": "lycfight",
            "type": "user"
          },
          "name": "Yongcong Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:28.275Z",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e44",
          "name": "Yuzhen Xiao",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e45",
          "name": "Changshi Li",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e46",
          "user": {
            "_id": "658229ef5f6d83438257fce5",
            "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
            "isPro": false,
            "fullname": "Chris (Yuhao) Liu",
            "user": "chrisliu298",
            "type": "user"
          },
          "name": "Chris Yuhao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:22.815Z",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e47",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e48",
          "name": "Tianwen Wei",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e49",
          "name": "Jujie He",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e4a",
          "name": "Xuchen Song",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e4b",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e4c",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T03:53:36.000Z",
      "submittedOnDailyAt": "2025-06-25T01:35:02.603Z",
      "title": "Skywork-SWE: Explica el método de escalado de datos en el desarrollo de software utilizando modelos de lenguaje de alto nivel (LLMs).",
      "submittedOnDailyBy": {
        "_id": "6621efe1a6eec3ad03e38759",
        "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
        "isPro": false,
        "fullname": "Liang Zeng",
        "user": "zengliangcs",
        "type": "user"
      },
      "summary": "El Software Engineering (SWE) ha adquirido una importancia creciente como un terreno de prueba para los próximos generaciones de agentes de LLM, con dos habilidades inherentes que son requeridas: la resolución de problemas complejos en secuencia (por ejemplo, interacciones de más de 50 turnos) y la resolución de relaciones de contexto a largo plazo (por ejemplo, más de 32,000 tokens). Sin embargo, el proceso de preparación de datos en SWE puede requerir mucho tiempo, ya que depende de la filtración de archivos de código y la configuración de entornos de ejecución específicos, lo que puede parecer más sencillo de entender que es. Como resultado, los conjuntos de datos actuales están casi exclusivamente limitados a miles de instancias de GitHub.\n\nEn estas condiciones, proponemos un proceso de automatización gradual para ampliar sistemáticamente la dimensión y la diversidad de los conjuntos de datos de SWE. Nuestro conjunto de datos incluye 10,169 instancias de tareas de Python en la realidad, provenientes de 2,531 repositorios diferentes de GitHub, cada una especificada en naturaleza y utilizada para la validación de pruebas unitarias automáticamente mediante imágenes de entornos de ejecución. Hemos preparado con más precaución un conjunto de datos de validación de ejecución exitosa de más de 8,000 instancias en nuestro conjunto de datos de SWE propuesto. Al ajustar el modelo Skywork-SWE con este conjunto de datos, el rendimiento en la capacidad de desarrollo de software de un LLM se mejora continuamente con el aumento de la cantidad de datos, sin señalar huellas de sobreajuste. En particular, nuestro modelo Skywork-SWE alcanzó una precisión de 38.0% en el benchmark SWE-bench Verified, estableciendo un nuevo nivel de excelencia (SOTA) en el marco de OpenHands para el agente Qwen2.5-Coder-32B. Además, la introducción de la tecnología de escalado temporal de pruebas ha mejorado la eficiencia, superando los resultados SOTA de modelos de 32B de parámetros y alcanzando una precisión de 47.0%. Publicamos el checkpoint del modelo Skywork-SWE-32B y buscamos acelerar futuros estudios.",
      "upvotes": 18,
      "discussionId": "685b6641d2ee4fac76521e4d",
      "projectPage": "https://quixotic-sting-239.notion.site/eb17f379610040ceb54da5d5d24065bd",
      "ai_summary": "An automated data-curation pipeline for software engineering improves large language model performance on SWE tasks, achieving state-of-the-art results with and without test-time scaling techniques.",
      "ai_keywords": [
        "LLM agents",
        "iterative problem-solving",
        "long-context dependency resolution",
        "code file filtering",
        "unit tests",
        "runtime environments",
        "data-curation pipeline",
        "software engineering capabilities",
        "Skywork-SWE model",
        "SWE-bench Verified",
        "pass@1 accuracy",
        "OpenHands agent framework",
        "test-time scaling techniques",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-06-23T23:53:36.000Z",
    "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in\n  LLMs",
    "summary": "Software engineering (SWE) has recently emerged as a crucial testbed for\nnext-generation LLM agents, demanding inherent capabilities in two critical\ndimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)\nand long-context dependency resolution (e.g., >32k tokens). However, the data\ncuration process in SWE remains notoriously time-consuming, as it heavily\nrelies on manual annotation for code file filtering and the setup of dedicated\nruntime environments to execute and validate unit tests. Consequently, most\nexisting datasets are limited to only a few thousand GitHub-sourced instances.\nTo this end, we propose an incremental, automated data-curation pipeline that\nsystematically scales both the volume and diversity of SWE datasets. Our\ndataset comprises 10,169 real-world Python task instances from 2,531 distinct\nGitHub repositories, each accompanied by a task specified in natural language\nand a dedicated runtime-environment image for automated unit-test validation.\nWe have carefully curated over 8,000 successfully runtime-validated training\ntrajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE\nmodel on these trajectories, we uncover a striking data scaling phenomenon: the\ntrained model's performance for software engineering capabilities in LLMs\ncontinues to improve as the data size increases, showing no signs of\nsaturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on\nthe SWE-bench Verified benchmark without using verifiers or multiple rollouts,\nestablishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based\nLLMs built on the OpenHands agent framework. Furthermore, with the\nincorporation of test-time scaling techniques, the performance further improves\nto 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter\nmodels. We release the Skywork-SWE-32B model checkpoint to accelerate future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19290.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6621efe1a6eec3ad03e38759",
      "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
      "fullname": "Liang Zeng",
      "name": "zengliangcs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18701",
      "authors": [
        {
          "_id": "685a14da0e4ad7e21975854d",
          "user": {
            "_id": "63aed0e7f873109b112dbb1b",
            "avatarUrl": "/avatars/0c07beb1ef7287128528b165d18371f6.svg",
            "isPro": false,
            "fullname": "Yifan Zhang",
            "user": "Vanint",
            "type": "user"
          },
          "name": "Yifan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:09:21.431Z",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e21975854e",
          "name": "Chunli Peng",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e21975854f",
          "name": "Boyang Wang",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758550",
          "name": "Puyi Wang",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758551",
          "name": "Qingcheng Zhu",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758552",
          "name": "Fei Kang",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758553",
          "name": "Biao Jiang",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758554",
          "name": "Zedong Gao",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758555",
          "name": "Eric Li",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758556",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758557",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63aed0e7f873109b112dbb1b/2AbnZSPpuvFOad20XamiG.mp4"
      ],
      "publishedAt": "2025-06-23T14:40:49.000Z",
      "submittedOnDailyAt": "2025-06-25T07:50:07.299Z",
      "title": "Matrix-Game: Modelo de Mundo Interactivo Fundamental",
      "submittedOnDailyBy": {
        "_id": "63aed0e7f873109b112dbb1b",
        "avatarUrl": "/avatars/0c07beb1ef7287128528b165d18371f6.svg",
        "isPro": false,
        "fullname": "Yifan Zhang",
        "user": "Vanint",
        "type": "user"
      },
      "summary": "Matrix-Game es un modelo basado en mundos interactivos para la generación de mundos de juego controlables. Matrix-Game realiza una grande escala de entrenamiento sin etiquetas para comprender la entorno y, posteriormente, entrena con etiquetas de acciones para la generación de videos interactivos. Para ello, se ha seleccionado el conjunto de datos detallado Matrix-Game-MC. Este conjunto de datos incluye más de 2,700 horas de videos de juegos sin etiquetas y más de 1,000 horas de clips etiquetados de alta calidad, así como anotaciones de acciones de teclado y mouse. Nuestro modelo adopta el patrón de generación de mundo a partir de imágenes controlables, acciones del usuario y contexto de acción. Con más de 1,700 millones de parámetros, Matrix-Game permite un control preciso de las acciones de los personajes y la movida de la cámara, manteniendo alta calidad visual y coherencia temporal. Para evaluar el rendimiento, se desarrolló un marco de referencia integrado llamado GameWorld Score, que evalúa la calidad visual, la calidad temporal, la posibilidad de control de acciones y la comprensión de leyes físicas en la generación de videos. Mediante experimentos distribuidos, Matrix-Game coincide y supera en todos los métricas a los modelos de juegos abierto source del pasado semana, como Oasis y MineWorld. En particular, se observó un efecto muy fuerte en la posibilidad de control y la coherencia física. Las evaluaciones de personas no informadas confirmaron la excelencia de Matrix-Game y destacó su capacidad para generar videos visualmente realistas y precisamente controlables en diferentes escenarios de juego. Se publican los pesos del modelo Matrix-Game y el marco de referencia GameWorld Score para futuras investigaciones en la generación de mundos a partir de imágenes interactivas.",
      "upvotes": 18,
      "discussionId": "685a14da0e4ad7e219758558",
      "projectPage": "https://matrix-game-homepage.github.io",
      "githubRepo": "https://github.com/SkyworkAI/Matrix-Game",
      "ai_summary": "Matrix-Game, a controllable game world generation model trained in a two-stage process, outperforms existing models by producing high-quality, action-controllable, and physically consistent Minecraft world videos.",
      "ai_keywords": [
        "Matrix-Game",
        "interactive world foundation model",
        "large-scale unlabeled pretraining",
        "action-labeled training",
        "contrrollable image-to-world generation",
        "Matrix-Game-MC",
        "motion context",
        "character actions",
        "camera movements",
        "visual quality",
        "temporal coherence",
        "GameWorld Score",
        "double-blind human evaluations",
        "interactive image-to-world generation",
        "Oasis",
        "MineWorld",
        "perceptually realistic"
      ],
      "githubStars": 744
    },
    "publishedAt": "2025-06-23T10:40:49.000Z",
    "title": "Matrix-Game: Interactive World Foundation Model",
    "summary": "We introduce Matrix-Game, an interactive world foundation model for\ncontrollable game world generation. Matrix-Game is trained using a two-stage\npipeline that first performs large-scale unlabeled pretraining for environment\nunderstanding, followed by action-labeled training for interactive video\ngeneration. To support this, we curate Matrix-Game-MC, a comprehensive\nMinecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips\nand over 1,000 hours of high-quality labeled clips with fine-grained keyboard\nand mouse action annotations. Our model adopts a controllable image-to-world\ngeneration paradigm, conditioned on a reference image, motion context, and user\nactions. With over 17 billion parameters, Matrix-Game enables precise control\nover character actions and camera movements, while maintaining high visual\nquality and temporal coherence. To evaluate performance, we develop GameWorld\nScore, a unified benchmark measuring visual quality, temporal quality, action\ncontrollability, and physical rule understanding for Minecraft world\ngeneration. Extensive experiments show that Matrix-Game consistently\noutperforms prior open-source Minecraft world models (including Oasis and\nMineWorld) across all metrics, with particularly strong gains in\ncontrollability and physical consistency. Double-blind human evaluations\nfurther confirm the superiority of Matrix-Game, highlighting its ability to\ngenerate perceptually realistic and precisely controllable videos across\ndiverse game scenarios. To facilitate future research on interactive\nimage-to-world generation, we will open-source the Matrix-Game model weights\nand the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63aed0e7f873109b112dbb1b/2AbnZSPpuvFOad20XamiG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18701.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63aed0e7f873109b112dbb1b",
      "avatarUrl": "/avatars/0c07beb1ef7287128528b165d18371f6.svg",
      "fullname": "Yifan Zhang",
      "name": "Vanint",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18951",
      "authors": [
        {
          "_id": "685ba757d2ee4fac76521f47",
          "name": "Jinyang Li",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f48",
          "user": {
            "_id": "653693cb8ee17cfd44eed8ce",
            "avatarUrl": "/avatars/82be2428bec4e06c0a15a27647b9b8aa.svg",
            "isPro": false,
            "fullname": "Xiaolong Li",
            "user": "xia01ongLi",
            "type": "user"
          },
          "name": "Xiaolong Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:05.354Z",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f49",
          "name": "Ge Qu",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4a",
          "name": "Per Jacobsson",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4b",
          "name": "Bowen Qin",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4c",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4d",
          "name": "Shuzheng Si",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4e",
          "name": "Nan Huo",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4f",
          "user": {
            "_id": "63a3eb8af460e4379b5991e7",
            "avatarUrl": "/avatars/7564a048d8496cac38d689178d90a8f9.svg",
            "isPro": false,
            "fullname": "Xiaohan Xu",
            "user": "Tebmer",
            "type": "user"
          },
          "name": "Xiaohan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T09:19:04.596Z",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f50",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f51",
          "name": "Ziwei Tang",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f52",
          "name": "Yuanshuai Li",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f53",
          "name": "Florensia Widjaja",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f54",
          "name": "Xintong Zhu",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f55",
          "name": "Feige Zhou",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f56",
          "name": "Yongfeng Huang",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f57",
          "name": "Yannis Papakonstantinou",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f58",
          "name": "Fatma Ozcan",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f59",
          "name": "Chenhao Ma",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f5a",
          "name": "Reynold Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T09:41:37.000Z",
      "submittedOnDailyAt": "2025-06-25T06:09:19.766Z",
      "title": "SWE-SQL: Cómo encontrar el paso a paso para resolver problemas de SQL de usuarios en proyectos reales",
      "submittedOnDailyBy": {
        "_id": "6419435385030eca6ac94701",
        "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
        "isPro": false,
        "fullname": "Ge Qu",
        "user": "gq2138",
        "type": "user"
      },
      "summary": "La resolución de problemas complejos de SQL sigue siendo un gran desafío en aplicaciones de bases de datos reales. Los grandes modelos de lenguaje actuales (LLMs) superan en la traducción de texto a SQL, pero no han sido evaluados con rigurosidad en el trabajo de depuración de problemas SQL. Para remediar esto, hemos extraído problemas reales y hemos construido un nuevo marco de referencia para la depuración de problemas SQL llamado BIRD-CRITIC, que consiste en 530 tareas críticas SQL recreativas (BIRD-CRITIC-PG) y 570 tareas multidirectorio (BIRD-CRITIC-Multi). El evaluación básica destaca la complejidad de estas tareas, y el modelo líder O3-Mini ha alcanzado un éxito del 38.87% en BIRD-CRITIC-PG y del 33.33% en BIRD-CRITIC-Multi. Además, es crucial el desarrollo de modelos abierto-source para tareas de bases de datos, respetando la privacidad de los datos y apoyando el desarrollo local. Por lo tanto, presentamos Sql-fIX-Gym, un entorno de entrenamiento para mejorar la capacidad de modelos abierto-source para la depuración de problemas SQL. Este entorno genera automáticamente conjuntos de datos de problemas a partir de SQL precisos usando la estrategia SQL-Rewind. Sin embargo, no se ha investigado el método final de entrenamiento de modelos correctos en grandes canales de audiencia. Además, proponemos el f-Plan Boosting para extraer planes de depuración de alto nivel a partir de soluciones SQL, lo que aumenta el éxito en entrenamiento en un 73.7%. Estos componentes se integraron para construir un agente abierto-source llamado Bird-Fixer, basado en Qwen-2.5-Coder-14B. Bird-Fixer ha alcanzado un éxito del 38.11% en BIRD-CRITIC-PG y del 29.65% en BIRD-CRITIC-Multi, abriendo significativamente la democratización de la capacidad de depuración de SQL compleja, superando a Claude-3.7-Sonnet y GPT-4.1. El liderboard y los códigos fuente están disponibles en https://bird-critic.github.io/.",
      "upvotes": 10,
      "discussionId": "685ba758d2ee4fac76521f5b",
      "ai_summary": "A new benchmark and training environment for debugging SQL issues using advanced open-source models significantly improves their performance compared to proprietary solutions.",
      "ai_keywords": [
        "BIRD-CRITIC",
        "BIRD-CRITIC-PG",
        "BIRD-CRITIC-Multi",
        "PostgreSQL",
        "Six-Gym (Sql-fIX-Gym)",
        "SQL-Rewind",
        "f-Plan Boosting",
        "Bird-Fixer",
        "Qwen-2.5-Coder-14B",
        "Claude-3.7-Sonnet",
        "GPT-4.1"
      ]
    },
    "publishedAt": "2025-06-23T05:41:37.000Z",
    "title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in\n  Real-World Applications",
    "summary": "Resolution of complex SQL issues persists as a significant bottleneck in\nreal-world database applications. Current Large Language Models (LLMs), while\nadept at text-to-SQL translation, have not been rigorously evaluated on the\nmore challenging task of debugging SQL issues. To address this gap, we\nintroduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530\nPostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks\n(BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within\nnew environments to facilitate rigorous evaluation. Baseline evaluations\nunderscore the task's complexity, with the leading reasoning model O3-Mini\nachieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on\nBIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks\nis crucial for empowering local development while safeguarding data privacy.\nTherefore, we present Six-Gym (Sql-fIX-Gym), a training environment for\nelevating open-source model capabilities for SQL issue debugging. This\nenvironment leverages SQL-Rewind strategy, which automatically generates\nexecutable issue-solution datasets by reverse-engineering issues from verified\nSQLs. However, popular trajectory-based fine-tuning methods do not explore\nsubstantial supervisory signals. We further propose f-Plan Boosting, which\nextracts high-level debugging plans from SQL solutions, enabling teacher LLMs\nto produce 73.7% more successful trajectories for training. We integrate these\ncomponents into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B,\nBird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on\nBIRD-CRITIC-Multi, surpassing leading proprietary models such as\nClaude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing\nsophisticated SQL-debugging capabilities. The leaderboard and source code are\navailable: https://bird-critic.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6419435385030eca6ac94701",
      "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
      "fullname": "Ge Qu",
      "name": "gq2138",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19767",
      "authors": [
        {
          "_id": "685b5791d2ee4fac76521dc2",
          "user": {
            "_id": "670aa09d35918e99fe7ff6b1",
            "avatarUrl": "/avatars/5cbea2284165191e96544bacf2bfb50f.svg",
            "isPro": false,
            "fullname": "Yuqian Fu",
            "user": "Yuqian-Fu",
            "type": "user"
          },
          "name": "Yuqian Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:54.115Z",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc3",
          "name": "Tinghong Chen",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc4",
          "name": "Jiajun Chai",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc5",
          "name": "Xihuai Wang",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc6",
          "user": {
            "_id": "66e14f4142ceed655c731966",
            "avatarUrl": "/avatars/d708562349913b89c8c4cf384628f82a.svg",
            "isPro": false,
            "fullname": "SONGJUN TU",
            "user": "SONGJUNTU",
            "type": "user"
          },
          "name": "Songjun Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:52.066Z",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc7",
          "name": "Guojun Yin",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc8",
          "name": "Wei Lin",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc9",
          "name": "Qichao Zhang",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dca",
          "name": "Yuanheng Zhu",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dcb",
          "name": "Dongbin Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T16:31:37.000Z",
      "submittedOnDailyAt": "2025-06-25T01:33:02.160Z",
      "title": "SRFT: Subjeto y simulación de retroalimentación incluidos en un método de una etapa",
      "submittedOnDailyBy": {
        "_id": "66e14f4142ceed655c731966",
        "avatarUrl": "/avatars/d708562349913b89c8c4cf384628f82a.svg",
        "isPro": false,
        "fullname": "SONGJUN TU",
        "user": "SONGJUNTU",
        "type": "user"
      },
      "summary": "Los grandes modelos de lenguaje (LLMs) han logrado un avance sorprendente en tareas, pero la integración óptima de ajustes de fines supervisados (SFT) y aprendizaje por refuerzo (RL) es un problema fundamental. Se requiere un análisis detallado de la distribución de tokens, dinámicas de aprendizaje, estructuras de integración, desde una perspectiva histórica, para revelar las relaciones entre estos paradigmas y su impacto en la eficiencia y robustez de los modelos. Este análisis debe abordar tanto la optimización de la estructura de entrenamiento como la mejora de la eficiencia del aprendizaje, así como la integración de conocimientos históricos y la influencia de la estructura de entrenamiento en la capacidad del modelo para abordar nuevas tareas. Además, se debe explorar cómo la combinación de estos enfoques puede mejorar la capacidad de generalización y la precisión de los modelos en contextos nuevos y complejos.",
      "upvotes": 7,
      "discussionId": "685b5792d2ee4fac76521dcc",
      "projectPage": "https://anonymous.4open.science/w/SRFT2025",
      "ai_summary": " Supervised Reinforcement Fine-Tuning (SRFT) integrates Supervised Fine-Tuning and Reinforcement Learning through entropy-aware weighting to achieve high accuracy in language model optimization.",
      "ai_keywords": [
        "Large language models",
        "Supervised Fine-Tuning",
        "Reinforcement Learning",
        "token distributions",
        "learning dynamics",
        "entropy",
        "Supervised Reinforcement Fine-Tuning"
      ]
    },
    "publishedAt": "2025-06-24T12:31:37.000Z",
    "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning",
    "summary": "Large language models (LLMs) have achieved remarkable progress in reasoning\ntasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) remains a fundamental challenge. Through\ncomprehensive analysis of token distributions, learning dynamics, and\nintegration mechanisms from entropy-based perspectives, we reveal key\ndifferences between these paradigms: SFT induces coarse-grained global changes\nto LLM policy distributions, while RL performs fine-grained selective\noptimizations, with entropy serving as a critical indicator of training\neffectiveness. Building on these observations, we propose Supervised\nReinforcement Fine-Tuning (SRFT), a single-stage method that unifies both\nfine-tuning paradigms through entropy-aware weighting mechanisms. Our approach\nsimultaneously applies SFT and RL to directly optimize the LLM using\ndemonstrations and self-exploration rollouts rather than through two-stage\nsequential methods. Extensive experiments show that SRFT achieves 59.1% average\naccuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning\nbenchmarks and 10.9% on three out-of-distribution benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19767.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e14f4142ceed655c731966",
      "avatarUrl": "/avatars/d708562349913b89c8c4cf384628f82a.svg",
      "fullname": "SONGJUN TU",
      "name": "SONGJUNTU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.19838",
      "authors": [
        {
          "_id": "685b5e05d2ee4fac76521ddd",
          "name": "Liangbin Xie",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521dde",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521ddf",
          "name": "Shian Du",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de0",
          "name": "Menghan Xia",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de1",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de2",
          "name": "Fanghua Yu",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de3",
          "name": "Ziyan Chen",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de4",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de5",
          "name": "Jiantao Zhou",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de6",
          "name": "Chao Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:57:26.000Z",
      "submittedOnDailyAt": "2025-06-25T00:55:41.694Z",
      "title": "SimpleGVR: Video Super Resolution de Base Line Básico de la Conexión de Secuencias Potenciales",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Los modelos de difusión han aparecido como un avance avanzado en la generación de vídeos eficientes. Sin embargo, a medida que las expectativas de los usuarios se centran en la salida de alta resolución, depender solo en la difusión ha mostrado su limitación. Una aproximación esperada divide el proceso en dos etapas: la generación de contenido semántico y la síntesis de detalles. La primera etapa utiliza un modelo básico rico en cálculos pero a baja resolución, mientras que la segunda etapa implementa una alta resolución de salida utilizando un modelo VSR (Video Super Resolution) ligero. En este estudio, se investigan las principios clave de diseño de modelos VSR continuos, que hasta ahora no han sido estudiados. Primero, se proponen dos estrategias de malicia para generar pares de entrenamiento que mejoren la representación de las características de la salida del modelo básico, ayudando a alinear el VSR con el generador superior. Luego, se realiza un análisis sistemático de la estrategia de muestreo en las etapas temporales y el impacto de la adición de ruido a los datos de baja resolución (LR), proporcionando importantes insights sobre el comportamiento del VSR. Estos hallazgos tienen un impacto directo en la arquitectura y el entrenamiento. Finalmente, se realizan entrenamientos eficientes y inferencias utilizando unidades de tiempo cortas y atención local esparcida, reduciendo significativamente el sobrecarga computacional. Los experimentos de difusión muestran que nuestro framework supera los métodos existentes, y los estudios de desvanecimiento confirman el efecto de cada selección de diseño. Nuestro estudio proporciona una base sencilla y efectiva, ofreciendo prácticas reales para el desarrollo de futuros sistemas de síntesis continuos eficientes.",
      "upvotes": 6,
      "discussionId": "685b5e05d2ee4fac76521de7",
      "ai_summary": "Researchers propose design principles for cascaded video super-resolution models to improve high-resolution video generation by introduces degradation strategies, timestep sampling, noise augmentation, and interleaving temporal units with sparse local attention.",
      "ai_keywords": [
        "latent diffusion models",
        "video generation",
        "cascaded video super-resolution",
        "VSR",
        "degradation strategies",
        "timestep sampling",
        "noise augmentation",
        "interleaving temporal unit",
        "sparse local attention"
      ]
    },
    "publishedAt": "2025-06-24T13:57:26.000Z",
    "title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution",
    "summary": "Latent diffusion models have emerged as a leading paradigm for efficient\nvideo generation. However, as user expectations shift toward higher-resolution\noutputs, relying solely on latent computation becomes inadequate. A promising\napproach involves decoupling the process into two stages: semantic content\ngeneration and detail synthesis. The former employs a computationally intensive\nbase model at lower resolutions, while the latter leverages a lightweight\ncascaded video super-resolution (VSR) model to achieve high-resolution output.\nIn this work, we focus on studying key design principles for latter cascaded\nVSR models, which are underexplored currently. First, we propose two\ndegradation strategies to generate training pairs that better mimic the output\ncharacteristics of the base model, ensuring alignment between the VSR model and\nits upstream generator. Second, we provide critical insights into VSR model\nbehavior through systematic analysis of (1) timestep sampling strategies, (2)\nnoise augmentation effects on low-resolution (LR) inputs. These findings\ndirectly inform our architectural and training innovations. Finally, we\nintroduce interleaving temporal unit and sparse local attention to achieve\nefficient training and inference, drastically reducing computational overhead.\nExtensive experiments demonstrate the superiority of our framework over\nexisting methods, with ablation studies confirming the efficacy of each design\nchoice. Our work establishes a simple yet effective baseline for cascaded video\nsuper-resolution generation, offering practical insights to guide future\nadvancements in efficient cascaded synthesis systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19838.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7183
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19794",
      "authors": [
        {
          "_id": "685b75d0d2ee4fac76521e70",
          "name": "Yuqi Zhu",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e71",
          "name": "Yi Zhong",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e72",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e73",
          "name": "Ziheng Zhang",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e74",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e75",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e76",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e77",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e78",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e79",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:04:23.000Z",
      "submittedOnDailyAt": "2025-06-25T02:37:00.536Z",
      "title": "¿Por qué los LLMs abiertos se enfrentan a problemas en el análisis de datos? Investigación experimental sistemática",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Los lenguajes de programación grandes (LLMs) tienen muchas posibilidades para la automatización de tareas de análisis de datos, sin embargo, en estos escaneos razonables, los modelos de código abierto tienen significativas limitaciones. En este estudio, se investigan estrategias para mejorar la capacidad de análisis de datos de los modelos de código abierto. Se construyen diferentes conjuntos de datos de prueba prácticos para evaluar los modelos de tres maneras: comprensión de datos, generación de código y planificación estratégica. En el análisis se encontraron tres principales descubrimientos: (1) la calidad de la planificación estratégica es un factor decisivo para el rendimiento del modelo; (2) el diseño de interacción y la complejidad de la tarea afectan significativamente la capacidad razonable; (3) la calidad de los datos tiene un mayor impacto que la diversidad para lograr un rendimiento óptimo. Utilizando estos hallazgos, se desarrollan métodos de síntesis de datos y se mejoran significativamente las capacidades analíticas razonables de los modelos de código abierto.",
      "upvotes": 6,
      "discussionId": "685b75d1d2ee4fac76521e7a",
      "ai_summary": "Enhancements to open-source large language models' data analysis capabilities through strategic planning, interaction design, and data quality improvements were identified and applied.",
      "ai_keywords": [
        "Large Language Models",
        "data analysis",
        "data understanding",
        "code generation",
        "strategic planning",
        "interaction design",
        "task complexity",
        "data quality",
        "data synthesis methodology"
      ]
    },
    "publishedAt": "2025-06-24T13:04:23.000Z",
    "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study",
    "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19794.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19713",
      "authors": [
        {
          "_id": "685b9a5dd2ee4fac76521ecc",
          "user": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "isPro": false,
            "fullname": "Seyedmorteza Sadat",
            "user": "msadat97",
            "type": "user"
          },
          "name": "Seyedmorteza Sadat",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:14.606Z",
          "hidden": false
        },
        {
          "_id": "685b9a5dd2ee4fac76521ecd",
          "name": "Tobias Vontobel",
          "hidden": false
        },
        {
          "_id": "685b9a5dd2ee4fac76521ece",
          "name": "Farnood Salehi",
          "hidden": false
        },
        {
          "_id": "685b9a5dd2ee4fac76521ecf",
          "name": "Romann M. Weber",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T15:19:42.000Z",
      "submittedOnDailyAt": "2025-06-25T05:16:23.771Z",
      "title": "Los guías de frecuencia en este ámbito permiten una muestra de alta calidad a escala baja.",
      "submittedOnDailyBy": {
        "_id": "63b4b02a103617b0a5b0ee2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
        "isPro": false,
        "fullname": "Seyedmorteza Sadat",
        "user": "msadat97",
        "type": "user"
      },
      "summary": "La Clasificación de Fréquences (CFG) funciona como una componente importante de los modelos de diferenciación condicional modernos. Es conocido que es muy efectivo en práctica, pero la estructura que mejora la calidad, el detalle y la atención al prompt de CFG no ha sido completamente comprendida. Hemos analizado el impacto de CFG en el dominio de las frecuencias, demostrando que las frecuencias bajas y altas tienen diferentes influencias. Específicamente, las guías de frecuencias bajas dominan la estructura global y la atención al prompt, mientras que las guías de frecuencias altas mejoran principalmente la fidelidad visual. Sin embargo, la CFG estándar aplica la misma escala a todas las frecuencias, lo que provoca una saturación y una disminución de la diversidad en altas escalas, y una pérdida de calidad visual en bajas escalas. Basándonos en esta perspectiva, proponemos la Guía de Fréquences Separadas (FDG). La FDG divide a CFG en componentes de frecuencias bajas y altas, y aplica una guía de frecuencias separadas a cada componente. La FDG tiene como objetivo mejorar la calidad de las imágenes con una escala de guía baja y evitar los defectos de la escala de guía alta. A través de una amplia gama de experimentos con diferentes conjuntos de datos y modelos, hemos demostrado que la FDG mejora la fidelidad de las muestras, mantiene la diversidad y mejora los índices de fidelidad (FID) y record (recall) más que la CFG. Hemos establecido nuestro método como un plug-in y plataforma para la CFG estándar.",
      "upvotes": 6,
      "discussionId": "685b9a5ed2ee4fac76521ed0",
      "ai_summary": "Frequency-decoupled guidance (FDG) enhances image quality and diversity by separately controlling low- and high-frequency guidance components in diffusion models, outperforming standard classifier-free guidance.",
      "ai_keywords": [
        "classifier-free guidance",
        "conditional diffusion models",
        "frequency domain",
        "low-frequency guidance",
        "high-frequency guidance",
        "frequency-decoupled guidance",
        "FID",
        "recall"
      ]
    },
    "publishedAt": "2025-06-24T11:19:42.000Z",
    "title": "Guidance in the Frequency Domain Enables High-Fidelity Sampling at Low\n  CFG Scales",
    "summary": "Classifier-free guidance (CFG) has become an essential component of modern\nconditional diffusion models. Although highly effective in practice, the\nunderlying mechanisms by which CFG enhances quality, detail, and prompt\nalignment are not fully understood. We present a novel perspective on CFG by\nanalyzing its effects in the frequency domain, showing that low and high\nfrequencies have distinct impacts on generation quality. Specifically,\nlow-frequency guidance governs global structure and condition alignment, while\nhigh-frequency guidance mainly enhances visual fidelity. However, applying a\nuniform scale across all frequencies -- as is done in standard CFG -- leads to\noversaturation and reduced diversity at high scales and degraded visual quality\nat low scales. Based on these insights, we propose frequency-decoupled guidance\n(FDG), an effective approach that decomposes CFG into low- and high-frequency\ncomponents and applies separate guidance strengths to each component. FDG\nimproves image quality at low guidance scales and avoids the drawbacks of high\nCFG scales by design. Through extensive experiments across multiple datasets\nand models, we demonstrate that FDG consistently enhances sample fidelity while\npreserving diversity, leading to improved FID and recall compared to CFG,\nestablishing our method as a plug-and-play alternative to standard\nclassifier-free guidance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19713.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b4b02a103617b0a5b0ee2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
      "fullname": "Seyedmorteza Sadat",
      "name": "msadat97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18843",
      "authors": [
        {
          "_id": "685a06460e4ad7e2197584c0",
          "user": {
            "_id": "6179f36a2a4e9edab3a95798",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
            "isPro": false,
            "fullname": "Heng-Jui Chang",
            "user": "vectominist",
            "type": "user"
          },
          "name": "Heng-Jui Chang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:49.104Z",
          "hidden": false
        },
        {
          "_id": "685a06460e4ad7e2197584c1",
          "user": {
            "_id": "67d301cbba86f5d66eb73d7c",
            "avatarUrl": "/avatars/8546bbd2145c16d4be5675624516b649.svg",
            "isPro": false,
            "fullname": "Saurabhchand Bhati",
            "user": "saurabhati",
            "type": "user"
          },
          "name": "Saurabhchand Bhati",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:09:27.586Z",
          "hidden": false
        },
        {
          "_id": "685a06460e4ad7e2197584c2",
          "name": "James Glass",
          "hidden": false
        },
        {
          "_id": "685a06460e4ad7e2197584c3",
          "name": "Alexander H. Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6179f36a2a4e9edab3a95798/QPPxw2hyPglQF_q3uWSyk.png"
      ],
      "publishedAt": "2025-06-23T17:02:00.000Z",
      "submittedOnDailyAt": "2025-06-25T02:21:09.630Z",
      "title": "USAD: Recopilación de experiencias mediante lenguaje general y expresiones vocales",
      "submittedOnDailyBy": {
        "_id": "6179f36a2a4e9edab3a95798",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
        "isPro": false,
        "fullname": "Heng-Jui Chang",
        "user": "vectominist",
        "type": "user"
      },
      "summary": "El aprendizaje autosupervisado (SSL) tiene un impacto innovador en las representaciones de voz, pero los modelos son generalmente especializados en tareas de lenguaje o no verbal, y se centran en cada tarea. En este artículo, se presenta una metodología para el aprendizaje de representaciones de voz que integra de manera unificada diferentes tipos de voz lingüística, acústica y musical, denominada Universal Speech and Audio Distillation (USAD). USAD utiliza un descenso eficiente por capas a partir de modelos SSL especializados para entrenar modelos estudiantes con conjuntos de datos de voz prácticos. USAD muestra un desempeño fuerte en diferentes marcos de referencia y conjuntos de datos, obteniendo resultados excelentes en tareas de procesamiento de lenguaje, etiquetado de voz, clasificación acústica, entre otras, y alcanzando los límites superiores en los marcos de referencia SUPERB y HEAR.",
      "upvotes": 6,
      "discussionId": "685a06470e4ad7e2197584c4",
      "ai_summary": "USAD integrates diverse audio types using efficient layer-to-layer distillation from domain-specific models, achieving competitive performance across various benchmarks with a single encoder.",
      "ai_keywords": [
        "self-supervised learning",
        "universal speech and audio distillation",
        "domain-specific models",
        "layer-to-layer distillation",
        "frame and instance-level speech processing",
        "audio tagging",
        "sound classification",
        "encoder",
        "SUPERB benchmarks",
        "HEAR benchmarks"
      ]
    },
    "publishedAt": "2025-06-23T13:02:00.000Z",
    "title": "USAD: Universal Speech and Audio Representation via Distillation",
    "summary": "Self-supervised learning (SSL) has revolutionized audio representations, yet\nmodels often remain domain-specific, focusing on either speech or non-speech\ntasks. In this work, we present Universal Speech and Audio Distillation (USAD),\na unified approach to audio representation learning that integrates diverse\naudio types - speech, sound, and music - into a single model. USAD employs\nefficient layer-to-layer distillation from domain-specific SSL models to train\na student on a comprehensive audio dataset. USAD offers competitive performance\nacross various benchmarks and datasets, including frame and instance-level\nspeech processing tasks, audio tagging, and sound classification, achieving\nnear state-of-the-art results with a single encoder on SUPERB and HEAR\nbenchmarks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6179f36a2a4e9edab3a95798/QPPxw2hyPglQF_q3uWSyk.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18843.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6179f36a2a4e9edab3a95798",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
      "fullname": "Heng-Jui Chang",
      "name": "vectominist",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.19807",
      "authors": [
        {
          "_id": "685b75edd2ee4fac76521e7c",
          "name": "Baochang Ren",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e7d",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e7e",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e7f",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e80",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:17:17.000Z",
      "submittedOnDailyAt": "2025-06-25T02:37:40.331Z",
      "title": "KnowRL: Aprendizaje de Reinforcement con Conocimiento para la Exploración de Verdad",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje general (LLMs), en particular los modelos lento de pensamiento, no pueden reconocer exactamente los límites de la conocida cuando explican razones, lo que provoca una confusión estricta y muchos casos de salida de contenido incorrecto. El aprendizaje por refuerzo (RL) puede mejorar la capacidad de razonamiento complejo, pero la estructura de recompensa relacionada con los objetivos carece de suficiente sobrevivencia real y promueve la confusión. Para resolver los altos niveles de confusión de los modelos lento de pensamiento, proponemos el conocido aprendizaje (KnowRL). KnowRL integra la recompensa de realidad en el proceso de aprendizaje RL, lo que conduce al modelo a pensar lentamente basado en la realidad y ayuda a reconocer los límites de la conocida. Durante el proceso de aprendizaje RL, el modelo aprende y internaliza estrategias de razonamiento basadas en la realidad, debido a entradas de entrada factuales objetivos. Al proporcionar una recompensa directa para actuar según la realidad dentro del proceso de razonamiento, KnowRL fomenta un proceso de pensamiento confiable. Los resultados de los experimentos con 3 datasets de evaluación de confusión y 2 datasets de evaluación de razonamiento muestran que KnowRL efectivamente mitiga la confusión de los modelos lento de pensamiento y mantiene su fuerte capacidad de razonamiento original. El código está disponible en: https://github.com/zjunlp/KnowRL.",
      "upvotes": 4,
      "discussionId": "685b75edd2ee4fac76521e81",
      "ai_summary": "KnowRL, a knowledge-enhanced reinforcement learning approach, reduces hallucinations in slow-thinking large language models by incorporating factuality rewards based on knowledge verification during training.",
      "ai_keywords": [
        "Large Language Models",
        "slow-thinking models",
        "hallucination",
        "Reinforcement Learning",
        "KnowRL",
        "factuality reward",
        "knowledge verification",
        "reasoning",
        "reasoning capabilities"
      ]
    },
    "publishedAt": "2025-06-24T13:17:17.000Z",
    "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
    "summary": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17612",
      "authors": [
        {
          "_id": "685b7538d2ee4fac76521e63",
          "user": {
            "_id": "64ecb174f22081b4ac7ca397",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ecb174f22081b4ac7ca397/PiAPtD_rbuhGOqfE6ZSIu.jpeg",
            "isPro": true,
            "fullname": "Yunlong Lin",
            "user": "LYL1015",
            "type": "user"
          },
          "name": "Yunlong Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:25.557Z",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e64",
          "name": "Zixu Lin",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e65",
          "name": "Kunjie Lin",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e66",
          "name": "Jinbin Bai",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e67",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e68",
          "name": "Chenxin Li",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e69",
          "name": "Haoyu Chen",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6a",
          "name": "Zhongdao Wang",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6b",
          "name": "Xinghao Ding",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6c",
          "name": "Wenbo Li",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6d",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ecb174f22081b4ac7ca397/PCTU9EbA4XUz8eKeLbxVC.gif"
      ],
      "publishedAt": "2025-06-21T06:36:00.000Z",
      "submittedOnDailyAt": "2025-06-25T02:43:05.885Z",
      "title": "Jabize Art: Liberación de la creación artística humana mediante el agente de edición de fotos inteligentes",
      "submittedOnDailyBy": {
        "_id": "64ecb174f22081b4ac7ca397",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ecb174f22081b4ac7ca397/PiAPtD_rbuhGOqfE6ZSIu.jpeg",
        "isPro": true,
        "fullname": "Yunlong Lin",
        "user": "LYL1015",
        "type": "user"
      },
      "summary": "La edición de fotos ha adquirido un papel importante como componente de las narrativas visuales modernas. Los usuarios pueden expresar su sentido estético y creatividad. Se ofrecen herramientas profesionales como Adobe Lightroom, que tienen potentes funciones, pero requieren conocimientos profundos y esfuerzo manual. Por otro lado, las soluciones basadas en IA actuales ofrecen automatización, pero con limitaciones en la ajuste y generalidad, lo que no es adecuada para la edición de diferentes usuarios. Para mitigar estas diferencias, se presenta JarvisArt, un agente que ejecuta un modelo de lenguaje multimodal (MLLM) diseñado para colaborar de manera estratégica con más de 200 herramientas de edición dentro de Lightroom. JarvisArt crece a través de dos etapas de entrenamiento: en la primera, se ajusta la lógica y el uso de las herramientas mediante un proceso de feedback de China de la Teoría de la Cadena de Pensamiento (Chain-of-Thought), y en la segunda, se mejora la decisión de edición y la perfección de las herramientas mediante la Política de Optimización Relativa para Retroque (GRPO-R). Además, propone un protocolo Agent-to-Lightroom para promover una integración sin restricciones entre el agente y Lightroom. En la evaluación de rendimiento, se desarrolló un nuevo benchmark llamado MMArt-Bench, en el que JarvisArt demostró interfaz amigable para el usuario, alta generalidad y control micro de ajustes globales e regionales, abriéndo nuevas perspectivas en la edición de fotos. En particular, mejoró los resultados de MMArt-Bench y GPT-4o en más de 60%, mientras que mantuvo la capacidad de seguir las mismas instrucciones. El sitio web del proyecto está disponible en https://jarvisart.vercel.app/.",
      "upvotes": 4,
      "discussionId": "685b7539d2ee4fac76521e6e",
      "projectPage": "https://jarvisart.vercel.app/",
      "githubRepo": "https://github.com/LYL1015/JarvisArt",
      "ai_summary": "JarvisArt, an MLLM-driven agent, achieves superior photo retouching by understanding user intent and coordinating multiple retouching tools in Lightroom, outperforming GPT-4o on a novel benchmark.",
      "ai_keywords": [
        "multi-modal large language model",
        "Chain-of-Thought supervised fine-tuning",
        "Group Relative Policy Optimization",
        "Agent-to-Lightroom Protocol",
        "MMArt-Bench",
        "global adjustments",
        "local adjustments",
        "content fidelity",
        "instruction-following capabilities"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-06-21T02:36:00.000Z",
    "title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo\n  Retouching Agent",
    "summary": "Photo retouching has become integral to contemporary visual storytelling,\nenabling users to capture aesthetics and express creativity. While professional\ntools such as Adobe Lightroom offer powerful capabilities, they demand\nsubstantial expertise and manual effort. In contrast, existing AI-based\nsolutions provide automation but often suffer from limited adjustability and\npoor generalization, failing to meet diverse and personalized editing needs. To\nbridge this gap, we introduce JarvisArt, a multi-modal large language model\n(MLLM)-driven agent that understands user intent, mimics the reasoning process\nof professional artists, and intelligently coordinates over 200 retouching\ntools within Lightroom. JarvisArt undergoes a two-stage training process: an\ninitial Chain-of-Thought supervised fine-tuning to establish basic reasoning\nand tool-use skills, followed by Group Relative Policy Optimization for\nRetouching (GRPO-R) to further enhance its decision-making and tool\nproficiency. We also propose the Agent-to-Lightroom Protocol to facilitate\nseamless integration with Lightroom. To evaluate performance, we develop\nMMArt-Bench, a novel benchmark constructed from real-world user edits.\nJarvisArt demonstrates user-friendly interaction, superior generalization, and\nfine-grained control over both global and local adjustments, paving a new\navenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a\n60% improvement in average pixel-level metrics on MMArt-Bench for content\nfidelity, while maintaining comparable instruction-following capabilities.\nProject Page: https://jarvisart.vercel.app/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ecb174f22081b4ac7ca397/PCTU9EbA4XUz8eKeLbxVC.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17612.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64ecb174f22081b4ac7ca397",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ecb174f22081b4ac7ca397/PiAPtD_rbuhGOqfE6ZSIu.jpeg",
      "fullname": "Yunlong Lin",
      "name": "LYL1015",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.19850",
      "authors": [
        {
          "_id": "685b63c2d2ee4fac76521dee",
          "name": "Yuqi Wang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521def",
          "name": "Xinghang Li",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df0",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df1",
          "name": "Junbo Zhang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df2",
          "name": "Yingyan Li",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df3",
          "name": "Yuntao Chen",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df4",
          "name": "Xinlong Wang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df5",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:59:57.000Z",
      "submittedOnDailyAt": "2025-06-25T06:01:30.093Z",
      "title": "Modelo de Lenguaje de Acción de Visión Integrada",
      "submittedOnDailyBy": {
        "_id": "649fe21d59c1ae90dbfacf91",
        "avatarUrl": "/avatars/7f77fa77113e80cb45406927e2386387.svg",
        "isPro": false,
        "fullname": "Wang Yuqi",
        "user": "Yuqi1997",
        "type": "user"
      },
      "summary": "Modelos de visión-lenguaje-acción (VLAs) se centran en la posibilidad de impulsar las acciones de manos mecánicas. Sin embargo, los métodos previos han principalmente utilizado la capacidad general de comprensión de modelos de lenguaje y visión (VLMs) para generar señales de acción, dejando de lado la estructura temporal y causal rica que se incluye en las observaciones visuales. En este artículo, proponemos el modelo VLA unificado y continuo, denominado UniVLA. Este modelo modela las señales visuales, lingüísticas y de acción como secuencias de tokens dispersos de manera automática y recursiva. Esta configuración facilita especialmente el aprendizaje de tareas multitiplas flexibles en grandes conjuntos de datos de vídeo. Después de entrenarse, UniVLA aplica modelización del mundo y promueve una eficiente transición de políticas para entender la dinámica causal en los vídeos. Nuestro enfoque ha obtenido nuevos resultados de distancia mínima en marcos de evaluación simulados ampliamente utilizados como CALVIN, LIBERO y Simplenv-Bridge, superando significativamente los métodos anteriores. Por ejemplo, UniVLA alcanzó un porcentaje de éxito promedio del 95.5% en el benchmark LIBERO, superando a pi0-FAST con un 85.5%. Además, hemos demostrado una amplia gama de aplicaciones en la acción real y el autonomo en el mundo real.",
      "upvotes": 3,
      "discussionId": "685b63c3d2ee4fac76521df6",
      "ai_summary": "UniVLA is a multimodal VLA model that autoregressively processes vision, language, and action as token sequences, incorporating world modeling for effective long-horizon policy learning and achieving state-of-the-art results across simulation and real-world benchmarks.",
      "ai_keywords": [
        "vision-language-action models",
        "VLAs",
        "vision-language models",
        "VLMs",
        "autoregressive models",
        "discrete token sequences",
        "multimodal tasks learning",
        "world modeling",
        "causal dynamics",
        "policy learning",
        "simulation benchmarks",
        "CALVIN",
        "LIBERO",
        "Simplenv-Bridge",
        "ALOHA manipulation",
        "autonomous driving"
      ]
    },
    "publishedAt": "2025-06-24T13:59:57.000Z",
    "title": "Unified Vision-Language-Action Model",
    "summary": "Vision-language-action models (VLAs) have garnered significant attention for\ntheir potential in advancing robotic manipulation. However, previous approaches\npredominantly rely on the general comprehension capabilities of vision-language\nmodels (VLMs) to generate action signals, often overlooking the rich temporal\nand causal structure embedded in visual observations. In this paper, we present\nUniVLA, a unified and native multimodal VLA model that autoregressively models\nvision, language, and action signals as discrete token sequences. This\nformulation enables flexible multimodal tasks learning, particularly from\nlarge-scale video data. By incorporating world modeling during post-training,\nUniVLA captures causal dynamics from videos, facilitating effective transfer to\ndownstream policy learning--especially for long-horizon tasks. Our approach\nsets new state-of-the-art results across several widely used simulation\nbenchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly\nsurpassing previous methods. For example, UniVLA achieves 95.5% average success\nrate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate\nits broad applicability on real-world ALOHA manipulation and autonomous\ndriving.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19850.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649fe21d59c1ae90dbfacf91",
      "avatarUrl": "/avatars/7f77fa77113e80cb45406927e2386387.svg",
      "fullname": "Wang Yuqi",
      "name": "Yuqi1997",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14012",
      "authors": [
        {
          "_id": "685b863bd2ee4fac76521e92",
          "user": {
            "_id": "655efd24afee0e00788bb589",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
            "isPro": false,
            "fullname": "Amr Mohamed",
            "user": "amr-mohamed",
            "type": "user"
          },
          "name": "Amr Mohamed",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:19.266Z",
          "hidden": false
        },
        {
          "_id": "685b863bd2ee4fac76521e93",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "685b863bd2ee4fac76521e94",
          "name": "Michalis Vazirgiannis",
          "hidden": false
        },
        {
          "_id": "685b863bd2ee4fac76521e95",
          "user": {
            "_id": "6087e598e2b7cc3a117b0dc5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png",
            "isPro": false,
            "fullname": "Guokan Shang",
            "user": "guokan-shang",
            "type": "user"
          },
          "name": "Guokan Shang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:16.772Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-16T21:19:27.000Z",
      "submittedOnDailyAt": "2025-06-25T03:51:26.828Z",
      "title": "Micks está confuso: Evaluación del entendimiento del texto de código switch.",
      "submittedOnDailyBy": {
        "_id": "655efd24afee0e00788bb589",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
        "isPro": false,
        "fullname": "Amr Mohamed",
        "user": "amr-mohamed",
        "type": "user"
      },
      "summary": "El código de cambio (CSW) es un comportamiento en el interior de un decoder que permite intercambiar dos o más lenguas. Este fenómeno se observa ampliamente en sociedades multilingües y ha evolucionado naturalmente en la comunicación diaria, donde se utilizan varios idiomas en contenidos en línea. Como resultado, los modelos de lenguaje grande escala (LLMs) han adquirido un papel central en el procesamiento y generación de contenido, pero frecuentemente reciben entradas que han sido CSW. Por lo tanto, es crucial comprender cómo los LLMs procesan y interpretan estos textos con una mezcla de lenguas. En este artículo, se genera una versión CSW de los marcos de referencia de razonamiento y comprensión, y se evalúa sistemáticamente la comprensión de CSW en los LLMs. Cuando tokens extranjeros afectan al texto en inglés, se puede observar daño, aunque el reescribir inglés en otro idioma mejora la comprensión. Aunque el input puede devolver resultados mezclados, la fine-tuning muestra un camino más estable para la inhibición del daño, permitiendo así un manejo efectivo de CSW.",
      "upvotes": 3,
      "discussionId": "685b863bd2ee4fac76521e96",
      "ai_summary": "LLMs' comprehension and reasoning skills are evaluated under code-switching conditions, revealing that embedding English into other languages can improve understanding, while prompts and fine-tuning affect degradation mitigation differently.",
      "ai_keywords": [
        "Large Language Models",
        "code-switching",
        "CSW",
        "reasoning benchmarks",
        "comprehension benchmarks",
        "foreign tokens",
        "embedding",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-06-16T17:19:27.000Z",
    "title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text",
    "summary": "Code-switching (CSW) is the act of alternating between two or more languages\nwithin a single discourse. This phenomenon is widespread in multilingual\ncommunities, and increasingly prevalent in online content, where users\nnaturally mix languages in everyday communication. As a result, Large Language\nModels (LLMs), now central to content processing and generation, are frequently\nexposed to code-switched inputs. Given their widespread use, it is crucial to\nunderstand how LLMs process and reason about such mixed-language text. This\npaper presents a systematic evaluation of LLM comprehension under\ncode-switching by generating CSW variants of established reasoning and\ncomprehension benchmarks. While degradation is evident when foreign tokens\ndisrupt English textx2013even under linguistic\nconstraintsx2013embedding English into other languages often\nimproves comprehension. Though prompting yields mixed results, fine-tuning\noffers a more stable path to degradation mitigation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655efd24afee0e00788bb589",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
      "fullname": "Amr Mohamed",
      "name": "amr-mohamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  }
]