[
  {
    "paper": {
      "id": "2507.01951",
      "authors": [
        {
          "_id": "6868daac213f123a1f88b9c8",
          "name": "Zixiao Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9c9",
          "name": "Yuxin Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9ca",
          "name": "Xiaorui Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cb",
          "name": "Mengting Xing",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cc",
          "name": "Jie Gao",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cd",
          "name": "Jianjun Xu",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9ce",
          "name": "Guangcan Liu",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cf",
          "name": "Chenhui Jin",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9d0",
          "name": "Zhuo Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9d1",
          "name": "Shengzhuo Zhang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9d2",
          "name": "Hongtao Xie",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638700c723da90491eb72722/mRIN68aSkxejrT4UqCaKz.jpeg"
      ],
      "publishedAt": "2025-07-02T17:58:01.000Z",
      "submittedOnDailyAt": "2025-07-14T00:42:27.924Z",
      "title": "Modelo de Generación de Reflexiones por Escalado de Tiempo de Prueba",
      "submittedOnDailyBy": {
        "_id": "638700c723da90491eb72722",
        "avatarUrl": "/avatars/6dcddca5c31121b60e45aab0816e11be.svg",
        "isPro": false,
        "fullname": "Yuxin Wang",
        "user": "wangyuxin87",
        "type": "user"
      },
      "summary": "Introducing MetaStone-S1, the first reflective generative model for OpenAI o3, which achieves its performance using the Self-Supporting Process Reward Model (SPRM). SPRM shares a backbone network, employs task-specific heads for next token prediction and process score evaluation, and integrates policy and process reward models (PRM) into a single unified feedback loop, reducing PRM parameters by over 99%, making it an efficient model suitable for inference. MetaStone-S1, equipped with SPRM, is natural and well-suited for test-time scaling (TTS). We provide efforts for three reasons: short, medium, and long controllable thought lengths. Additionally, to clarify the relationship between overall thought computation and TTS performance, we experimentally built a scaler. The results show that our MetaStone-S1 achieves performance comparable to the OpenAI-o3-mini series with only 32B parameters. For the support of the research community, we make MetaStone-S1 available as open source on GitHub (https://github.com/MetaStone-AI/MetaStone-S1).",
      "upvotes": 58,
      "discussionId": "6868daac213f123a1f88b9d3",
      "githubRepo": "https://github.com/MetaStone-AI/MetaStone-S1",
      "ai_summary": "MetaStone-S1, a reflective generative model using a self-supervised process reward model, achieves efficient reasoning and scalable performance with fewer parameters compared to existing models.",
      "ai_keywords": [
        "reflective generative model",
        "self-supervised process reward model",
        "backbone network",
        "task-specific heads",
        "policy model",
        "process reward model",
        "test time scaling",
        "controllable thinking length",
        "scaling law",
        "total thinking computation"
      ],
      "githubStars": 41
    },
    "publishedAt": "2025-07-02T13:58:01.000Z",
    "title": "Test-Time Scaling with Reflective Generative Model",
    "summary": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638700c723da90491eb72722/mRIN68aSkxejrT4UqCaKz.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01951.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "638700c723da90491eb72722",
      "avatarUrl": "/avatars/6dcddca5c31121b60e45aab0816e11be.svg",
      "fullname": "Yuxin Wang",
      "name": "wangyuxin87",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08776",
      "authors": [
        {
          "_id": "68745e0f257d4f0435370288",
          "name": "Zhengqing Wang",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f0435370289",
          "name": "Yuefan Wu",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f043537028a",
          "name": "Jiacheng Chen",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f043537028b",
          "name": "Fuyang Zhang",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f043537028c",
          "name": "Yasutaka Furukawa",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64d97c5bfd0b55d501ba00cf/xQVrjtR_Sb4SOz-GghYdp.mp4"
      ],
      "publishedAt": "2025-07-11T17:38:52.000Z",
      "submittedOnDailyAt": "2025-07-14T00:07:38.676Z",
      "title": "CLiFT: Explora la eficiencia computacional y adaptabilidad de los tokens de campo de luz ligero.",
      "submittedOnDailyBy": {
        "_id": "64d97c5bfd0b55d501ba00cf",
        "avatarUrl": "/avatars/47505f2a573acea7176a96f538226ecb.svg",
        "isPro": false,
        "fullname": "Zhengqing Wang",
        "user": "EricW123456",
        "type": "user"
      },
      "summary": "Este artículo propone la técnica de \"Tokenes de Compresión de Pantalla (CLiFTs)\" y utiliza esta para mantener la rica información de apariencia y geometría de la pantalla. Los CLiFTs permiten un rendimiento computacional eficiente a través de la compresión de los tokenes y, utilizando la misma red de entrenamiento, se pueden cambiar la cantidad de tokenes de la pantalla para renderizar nuevas vistas. Concretamente, se proporciona un conjunto de imágenes, se tokenizan las imágenes desde la perspectiva de múltiples cámaras. Se utiliza el método K-means en el espacio potencial para seleccionar como centros de cluster los tokenes que representan conjuntos de colores reducidos. Los \"compresores\" de múltiples puntos comprimen toda la información de los tokenes en los centros de cluster, construyendo así los CLiFTs. Durante el test, se proporcionan la visión objetivo y el bucket de cálculo (número de CLiFTs), se agrupan los tokenes más cercanos a la visión objetivo y se synthetizan nuevas vistas mediante un proceso de rendimiento computacional eficiente. Los experimentos con los conjuntos de datos RealEstate10K y DL3DV prueban cuantitativa y cualitativamente nuestro enfoque, logrando una mejora en la calidad del rendimiento, la cantidad de datos, la calidad del rendimiento y la velocidad de rendimiento.",
      "upvotes": 38,
      "discussionId": "68745e10257d4f043537028d",
      "projectPage": "https://clift-nvs.github.io/",
      "githubRepo": "https://github.com/eric-zqwang/CLiFT",
      "ai_summary": "A neural rendering method uses compressed light-field tokens to efficiently represent scenes and render novel views with varying compute budgets.",
      "ai_keywords": [
        "neural rendering",
        "compressed light-field tokens",
        "CLiFTs",
        "multi-view encoder",
        "latent-space K-means",
        "condenser",
        "compute-adaptive renderer",
        "RealEstate10K",
        "DL3DV datasets"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-07-11T13:38:52.000Z",
    "title": "CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive\n  Neural Rendering",
    "summary": "This paper proposes a neural rendering approach that represents a scene as\n\"compressed light-field tokens (CLiFTs)\", retaining rich appearance and\ngeometric information of a scene. CLiFT enables compute-efficient rendering by\ncompressed tokens, while being capable of changing the number of tokens to\nrepresent a scene or render a novel view with one trained network. Concretely,\ngiven a set of images, multi-view encoder tokenizes the images with the camera\nposes. Latent-space K-means selects a reduced set of rays as cluster centroids\nusing the tokens. The multi-view ``condenser'' compresses the information of\nall the tokens into the centroid tokens to construct CLiFTs. At test time,\ngiven a target view and a compute budget (i.e., the number of CLiFTs), the\nsystem collects the specified number of nearby tokens and synthesizes a novel\nview using a compute-adaptive renderer. Extensive experiments on RealEstate10K\nand DL3DV datasets quantitatively and qualitatively validate our approach,\nachieving significant data reduction with comparable rendering quality and the\nhighest overall rendering score, while providing trade-offs of data size,\nrendering quality, and rendering speed.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64d97c5bfd0b55d501ba00cf/xQVrjtR_Sb4SOz-GghYdp.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d97c5bfd0b55d501ba00cf",
      "avatarUrl": "/avatars/47505f2a573acea7176a96f538226ecb.svg",
      "fullname": "Zhengqing Wang",
      "name": "EricW123456",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08800",
      "authors": [
        {
          "_id": "6874615e257d4f043537028f",
          "name": "Luke Rivard",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370290",
          "name": "Sun Sun",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370291",
          "name": "Hongyu Guo",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370292",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370293",
          "name": "Yuntian Deng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/8bwvqBSzlNYlfcdn-pVWu.mp4"
      ],
      "publishedAt": "2025-07-11T17:59:40.000Z",
      "submittedOnDailyAt": "2025-07-14T01:10:17.755Z",
      "title": "NeuralOS: Investigación sobre la Simulación de Sistemas Operativos utilizando el Modelo de Generador Neuronal",
      "submittedOnDailyBy": {
        "_id": "63081e15a670ed10f9d44229",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
        "isPro": true,
        "fullname": "Yuntian Deng",
        "user": "yuntian-deng",
        "type": "user"
      },
      "summary": "Nueralos, un marco de trabajo de red neuronal se presenta. Este framework utiliza directamente la interfaz gráfica de usuario (GUI) del sistema operativo para predecir movimientos del ratón, clicks y eventos del teclado, entre otros, en respuesta a entradas del usuario. Nueralos combina una red neuronal recurrente (RNN) que rastrea el estado de la computadora con un modelo de renderización basado en ramas. El modelo se entrena con conjuntos de datos de interfaz de usuario, que incluyen tanto interacciones reales generadas por IA como interacciones generadas de manera aleatoria. Los experimentos muestran que Nueralos puede renderizar secuencias de GUI reales con éxito, reconocer precisamente las interacciones con el ratón y predecir con confianza transiciones de estado como el inicio de aplicaciones. Sin embargo, modelar con precisión la estructura micro de las interacciones del teclado es un desafío, pero Nueralos se está desarrollando como una interfaz neuronal generable adaptable a la interfaz humano-computadora.",
      "upvotes": 23,
      "discussionId": "6874615e257d4f0435370294",
      "projectPage": "https://neural-os.com/",
      "githubRepo": "https://github.com/yuntian-group/neural-os",
      "ai_summary": "NeuralOS uses a combination of RNNs and diffusion-based rendering to simulate OS GUIs by predicting screen frames from user inputs, demonstrating realistic GUI rendering and state transitions.",
      "ai_keywords": [
        "recurrent neural network",
        "RNN",
        "diffusion-based neural renderer",
        "GUI",
        "user inputs",
        "mouse interactions",
        "keyboard events",
        "state transitions",
        "application launches"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-11T13:59:40.000Z",
    "title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative\n  Models",
    "summary": "We introduce NeuralOS, a neural framework that simulates graphical user\ninterfaces (GUIs) of operating systems by directly predicting screen frames in\nresponse to user inputs such as mouse movements, clicks, and keyboard events.\nNeuralOS combines a recurrent neural network (RNN), which tracks computer\nstate, with a diffusion-based neural renderer that generates screen images. The\nmodel is trained on a large-scale dataset of Ubuntu XFCE recordings, which\ninclude both randomly generated interactions and realistic interactions\nproduced by AI agents. Experiments show that NeuralOS successfully renders\nrealistic GUI sequences, accurately captures mouse interactions, and reliably\npredicts state transitions like application launches. Although modeling\nfine-grained keyboard interactions precisely remains challenging, NeuralOS\noffers a step toward creating fully adaptive, generative neural interfaces for\nfuture human-computer interaction systems.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/8bwvqBSzlNYlfcdn-pVWu.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08800.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63081e15a670ed10f9d44229",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
      "fullname": "Yuntian Deng",
      "name": "yuntian-deng",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 245
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05397",
      "authors": [
        {
          "_id": "6874a1ff257d4f0435370344",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370345",
          "name": "Jie Xia",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370346",
          "name": "Xiaopeng Peng",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370347",
          "name": "Wangbo Zhao",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370348",
          "name": "Zilong Ye",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370349",
          "name": "Zekai Li",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034a",
          "name": "Suorong Yang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034b",
          "name": "Jiadong Pan",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034c",
          "name": "Yuanxiang Chen",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034d",
          "name": "Ziqiao Wang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034e",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034f",
          "name": "Qian Zheng",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370350",
          "name": "Xiaojun Chang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370351",
          "name": "Gang Pan",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370352",
          "name": "Shurong Dong",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370353",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370354",
          "name": "Yang You",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/KhRuhMNBeK8P8MfGydldo.png"
      ],
      "publishedAt": "2025-07-07T18:31:50.000Z",
      "submittedOnDailyAt": "2025-07-14T04:53:28.591Z",
      "title": "\"Nueral Network Drove Image Editing\"",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "El editado de imágenes tradicionalmente se basa en la dependencia de pantallas táctiles, lo que implica altos costos laborales y restringe su acceso a personas con limitaciones en la capacidad de control táctil o en el lenguaje. Recientemente, el desarrollo de Interfaces Cerebro-Computadora (BCI) y modelos generativos ha permitido proponer LoongX, un método de acceso al editado de imágenes sin manos. LoongX utiliza un modelo difusión actualizado con 23,928 pares de editados de imágenes para comprender la intención del usuario a través de señales como grafos eléctricos cerebrales (EEG), espectrografía de infrarrojos funcionales (fNIRS), potenciogramas (PPG) y señales de movimiento craneal. Para manejar efectivamente la diversidad de estas señales, LoongX combina dos módulos clave: el módulo de escalabilidad de estados espaciales (CS3) codifica características específicas de los modelos de información, mientras que el módulo de fusión de puente dinámico (DGF) integra estas características en un espacio latino. Este espacio latino se edita mediante un ajuste micro de un transformador difusión (DiT) para que la semántica del editado de imágenes coincida con la intención del usuario. Además, se utiliza entrenamiento de contractación para entrenar el encoder con datos previos y asegurar que el estado cognitivo y la semántica de la naturaleza del lenguaje coincidan con la intención del usuario. Los experimentos expandidos muestran que LoongX logra performances comparables a los métodos de texto dirigido, como CLIP-T (0.2588 vs. 0.2549). Estos resultados demuestran que los modelos generativos neuronales permiten un editado de imágenes intuitivo accesible y abren nuevas direcciones en la tecnología de contenedores cognitivos. Los conjuntos de datos y el código se publicarán para fomentar futuras investigaciones y el desarrollo de este nuevo campo.",
      "upvotes": 16,
      "discussionId": "6874a200257d4f0435370355",
      "projectPage": "https://loongx1.github.io/",
      "githubRepo": "https://github.com/LanceZPF/loongx",
      "ai_summary": "LoongX uses multimodal neurophysiological signals and diffusion models for hands-free image editing, achieving performance comparable to text-driven methods and outperforming them when combined with speech.",
      "ai_keywords": [
        "diffusion models",
        "electroencephalography (EEG)",
        "functional near-infrared spectroscopy (fNIRS)",
        "photoplethysmography (PPG)",
        "head motion signals",
        "cross-scale state space (CS3) module",
        "dynamic gated fusion (DGF) module",
        "diffusion transformer (DiT)",
        "contrastive learning"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-07-07T14:31:50.000Z",
    "title": "Neural-Driven Image Editing",
    "summary": "Traditional image editing typically relies on manual prompting, making it\nlabor-intensive and inaccessible to individuals with limited motor control or\nlanguage abilities. Leveraging recent advances in brain-computer interfaces\n(BCIs) and generative models, we propose LoongX, a hands-free image editing\napproach driven by multimodal neurophysiological signals. LoongX utilizes\nstate-of-the-art diffusion models trained on a comprehensive dataset of 23,928\nimage editing pairs, each paired with synchronized electroencephalography\n(EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography\n(PPG), and head motion signals that capture user intent. To effectively address\nthe heterogeneity of these signals, LoongX integrates two key modules. The\ncross-scale state space (CS3) module encodes informative modality-specific\nfeatures. The dynamic gated fusion (DGF) module further aggregates these\nfeatures into a unified latent space, which is then aligned with edit semantics\nvia fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train\nthe encoders using contrastive learning to align cognitive states with semantic\nintentions from embedded natural language. Extensive experiments demonstrate\nthat LoongX achieves performance comparable to text-driven methods (CLIP-I:\n0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural\nsignals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results\nhighlight the promise of neural-driven generative models in enabling\naccessible, intuitive image editing and open new directions for\ncognitive-driven creative technologies. Datasets and code will be released to\nsupport future work and foster progress in this emerging area.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/KhRuhMNBeK8P8MfGydldo.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05397.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08799",
      "authors": [
        {
          "_id": "6874ac1e257d4f0435370389",
          "name": "Max Belitsky",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038a",
          "name": "Dawid J. Kopiczko",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038b",
          "name": "Michael Dorkenwald",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038c",
          "name": "M. Jehanzeb Mirza",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038d",
          "name": "Cees G. M. Snoek",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038e",
          "name": "Yuki M. Asano",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/637d21239a5217b88b7549c3/w5UPBRgGM_-9_ELxgOBlL.png"
      ],
      "publishedAt": "2025-07-11T17:59:36.000Z",
      "submittedOnDailyAt": "2025-07-14T05:40:13.268Z",
      "title": "KV Cache Steering para Inducir Razonamiento en Pequeños Modelos de Lenguaje",
      "submittedOnDailyBy": {
        "_id": "637d21239a5217b88b7549c3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d21239a5217b88b7549c3/LrIGPiva5VGVZG87rTAJz.jpeg",
        "isPro": false,
        "fullname": "Yuki Asano",
        "user": "yukimasano",
        "type": "user"
      },
      "summary": "Proponemos la orientación de caché. Es un método ligero para orientar modelos de lenguaje, que aplica una interrupción directa en un caché de clave-valor para orientar el modelo de manera no invasiva. Para probar la eficacia de la orientación de caché, aplicamos esta técnica a un pequeño modelo de lenguaje para inducir pensamientos en cadenas. Nuestro enfoque utiliza las trazas de inferencia generadas por GPT-4o para construir vectores de orientación, lo que permite orientar el comportamiento del modelo de manera más clara y multi-nivel, sin necesidad de ajustes micro o modificaciones de prompts. Los resultados experimentales en diferentes marcos de evaluación de inferencia muestran que la orientación de caché mejora tanto la estructura cualitativa como el rendimiento cuantitativo del modelo de inferencia. En comparación con las tecnologías de orientación de activación que requieren una interrupción continua, una sola orientación de caché ofrece significativas ventajas en términos de estabilidad de hiperparámetros, eficiencia en tiempos de inferencia y facilidad de integración. Por lo tanto, se puede considerar una solución más robusta y práctica para la generación controlada.",
      "upvotes": 15,
      "discussionId": "6874ac1e257d4f043537038f",
      "ai_summary": "Cache steering improves reasoning in language models through a single intervention in the key-value cache, enhancing both reasoning structure and task performance.",
      "ai_keywords": [
        "cache steering",
        "key-value cache",
        "chain-of-thought reasoning",
        "GPT-4o",
        "steering vectors",
        "multi-step reasoning",
        "activation steering",
        "hyperparameter stability",
        "inference-time efficiency",
        "ease of integration",
        "controlled generation"
      ]
    },
    "publishedAt": "2025-07-11T13:59:36.000Z",
    "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
    "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/637d21239a5217b88b7549c3/w5UPBRgGM_-9_ELxgOBlL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08799.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "637d21239a5217b88b7549c3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d21239a5217b88b7549c3/LrIGPiva5VGVZG87rTAJz.jpeg",
      "fullname": "Yuki Asano",
      "name": "yukimasano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08801",
      "authors": [
        {
          "_id": "68746fc3257d4f04353702ce",
          "name": "Hangjie Yuan",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702cf",
          "name": "Weihua Chen",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d0",
          "name": "Jun Cen",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d1",
          "name": "Hu Yu",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d2",
          "name": "Jingyun Liang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d3",
          "name": "Shuning Chang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d4",
          "name": "Zhihui Lin",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d5",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d6",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d7",
          "name": "Jiazheng Xing",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d8",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d9",
          "name": "Jiasheng Tang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702da",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702db",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:59:42.000Z",
      "submittedOnDailyAt": "2025-07-14T01:18:10.056Z",
      "title": "Lumos-1: Perspectiva de la generación automática de videos de recuperación en modelos de unificación",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "Automáticamente, los modelos de lenguaje grandes (LLMs) han unificado varios trabajos de lenguaje y han impulsado inicios tempranos en la generación automática de vídeos. Aunque los actuales generadores automáticos de vídeos se alejan de la arquitectura estándar de LLMs, dependen de un gran encoder de texto externo y su interpretación de tokens sigue siendo un problema. En este artículo, presentamos Lumos-1, un generador automático de vídeos que mantiene la arquitectura de LLMs con un mínimo de cambios. Para introducir la correlación espacio-temporal en los LLMs, reconocimos el efecto de RoPE 3D y diagnosticamos el rango de espectro de frecuencias no equilibrado. Con esto, proponemos MM-RoPE. MM-RoPE mantiene el RoPE original del texto mientras proporciona una espectro de frecuencias expandido y información de posición 3D para múltiples modelizaciones. Además, Lumos-1 adopta la relación de dependencia de tokens que respeta la causalidad temporal. Basándonos en esta relación, identificamos el problema de la desigualdad en la pérdida de frames debido a la repetitividad de la información espacial y proponemos AR-DF. AR-DF utiliza un tapiz temporal durante el entrenamiento y evita la pérdida de calidad al asegurar que la política de tapiz sea compatible en la inferencia. Usando técnicas de entrenamiento eficientes en memoria, Lumos-1 logró rendimientos comparables a EMU3 en GenEval, a COSMOS-Video2World en VBench-I2V y a OpenSoraPlan en VBench-T2V. El código y el modelo están disponibles en https://github.com/alibaba-damo-academy/Lumos.",
      "upvotes": 13,
      "discussionId": "68746fc3257d4f04353702dc",
      "githubRepo": "https://github.com/alibaba-damo-academy/Lumos",
      "ai_summary": "Lumos-1 is an autoregressive video generator that uses a modified LLM architecture with MM-RoPE and AR-DF to address spatiotemporal correlation and frame-wise loss imbalance, achieving competitive performance with fewer resources.",
      "ai_keywords": [
        "autoregressive large language models",
        "LLMs",
        "autoregressive video generation",
        "3D RoPE",
        "MM-RoPE",
        "token dependency strategy",
        "intra-frame bidirectionality",
        "inter-frame temporal causality",
        "frame-wise loss imbalance",
        "Autoregressive Discrete Diffusion Forcing",
        "AR-DF",
        "temporal tube masking",
        "GenEval",
        "COSMOS-Video2World",
        "VBench-I2V",
        "OpenSoraPlan",
        "VBench-T2V"
      ],
      "githubStars": 26
    },
    "publishedAt": "2025-07-11T13:59:42.000Z",
    "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective",
    "summary": "Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08801.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08772",
      "authors": [
        {
          "_id": "68746c95257d4f04353702b7",
          "name": "Shaocong Dong",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702b8",
          "name": "Lihe Ding",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702b9",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702ba",
          "name": "Yaokun Li",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bb",
          "name": "Yuxin Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bc",
          "name": "Yucheng Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bd",
          "name": "Qi Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702be",
          "name": "Jaehyeok Kim",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bf",
          "name": "Chenjian Gao",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c0",
          "name": "Zhanpeng Huang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c1",
          "name": "Zibin Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c2",
          "name": "Tianfan Xue",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c3",
          "name": "Dan Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:33:18.000Z",
      "submittedOnDailyAt": "2025-07-14T01:05:21.741Z",
      "title": "Onneot Mood: Context-Aware Partial 3D Generation",
      "submittedOnDailyBy": {
        "_id": "63ae91af2314b93f9e6dde42",
        "avatarUrl": "/avatars/792ce138cbee85b8754fdcec7fb1ff52.svg",
        "isPro": false,
        "fullname": "Shaocong Dong",
        "user": "dscdyc",
        "type": "user"
      },
      "summary": "La tecnología reciente de generación 3D ha convertidose en un marco de difusión de la potencial específica de 3D, basándose en información geométrica inferida de datos reales. A pesar de estas avances, permanecen tres límites principales en la generación 3D: 1) una representación única de potencial no captura la complejidad de múltiples geometrías, lo que lleva a pérdida de detalles; 2) el codificado global pierde la independencia parcial y la relación entre partes necesarias para diseño; 3) la estructura de condicionamiento global no posee micro-conexividad. Proponemos un marco de difusión para la reconocimiento parcial que modela el flujo de trabajo de diseño 3D, decompone los objetos 3D en representaciones potenciales parciales con respecto al contexto, y permite la generación multi-parte distribuida. Este paradigma ofrece tres ventajas: i) reduce la complejidad mediante la codificación parcial; ii) permite modelar relaciones parciales explícitas; iii) soporta la asignación de condiciones a nivel parcial. Además, utilizando redes neuronales relacionadas, desarrollamos una estrategia de guía intercambiante para optimizar un modelo diferencial pre-entrenado con respecto a ruido potencial común en partes. Construimos un nuevo conjunto de datos 3D parcial, permitiendo entrenamiento a gran escala mediante división automática y anotaciones humanas confirmadas en Partverse-Objaverse. En experimentos extensos, CoPart ha demostrado altos rendimientos en edición parcial, generación de objetos arquitectónicos y configuración de escenarios sin precedentes con micro-conexividad.",
      "upvotes": 9,
      "discussionId": "68746c96257d4f04353702c4",
      "projectPage": "https://hkdsc.github.io/project/copart/",
      "githubRepo": "https://github.com/hkdsc/copart",
      "ai_summary": "A part-aware diffusion framework, CoPart, enhances 3D generation by decomposing objects into contextual parts, improving complexity handling, relationship modeling, and part-level conditioning.",
      "ai_keywords": [
        "latent diffusion frameworks",
        "geometric priors",
        "single-latent representations",
        "holistic latent coding",
        "part independence",
        "interrelationships",
        "compositional design",
        "global conditioning mechanisms",
        "fine-grained controllability",
        "human 3D design workflows",
        "part-aware diffusion framework",
        "contextual part latents",
        "coherent multi-part generation",
        "encoding complexity",
        "part relationship modeling",
        "part-level conditioning",
        "mutual guidance strategy",
        "joint part latent denoising",
        "geometric coherence",
        "foundation model priors",
        "Partverse",
        "Objaverse",
        "automated mesh segmentation",
        "human-verified annotations",
        "part-level editing",
        "articulated object generation",
        "scene composition"
      ],
      "githubStars": 38
    },
    "publishedAt": "2025-07-11T13:33:18.000Z",
    "title": "From One to More: Contextual Part Latents for 3D Generation",
    "summary": "Recent advances in 3D generation have transitioned from multi-view 2D\nrendering approaches to 3D-native latent diffusion frameworks that exploit\ngeometric priors in ground truth data. Despite progress, three key limitations\npersist: (1) Single-latent representations fail to capture complex multi-part\ngeometries, causing detail degradation; (2) Holistic latent coding neglects\npart independence and interrelationships critical for compositional design; (3)\nGlobal conditioning mechanisms lack fine-grained controllability. Inspired by\nhuman 3D design workflows, we propose CoPart - a part-aware diffusion framework\nthat decomposes 3D objects into contextual part latents for coherent multi-part\ngeneration. This paradigm offers three advantages: i) Reduces encoding\ncomplexity through part decomposition; ii) Enables explicit part relationship\nmodeling; iii) Supports part-level conditioning. We further develop a mutual\nguidance strategy to fine-tune pre-trained diffusion models for joint part\nlatent denoising, ensuring both geometric coherence and foundation model\npriors. To enable large-scale training, we construct Partverse - a novel 3D\npart dataset derived from Objaverse through automated mesh segmentation and\nhuman-verified annotations. Extensive experiments demonstrate CoPart's superior\ncapabilities in part-level editing, articulated object generation, and scene\ncomposition with unprecedented controllability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08772.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63ae91af2314b93f9e6dde42",
      "avatarUrl": "/avatars/792ce138cbee85b8754fdcec7fb1ff52.svg",
      "fullname": "Shaocong Dong",
      "name": "dscdyc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08794",
      "authors": [
        {
          "_id": "68747a58257d4f04353702f9",
          "name": "Yulai Zhao",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fa",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fb",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fc",
          "name": "S. Y. Kung",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fd",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fe",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:55:22.000Z",
      "submittedOnDailyAt": "2025-07-14T02:04:25.605Z",
      "title": "1 Token para engañar a LLM-as-a-Judge",
      "submittedOnDailyBy": {
        "_id": "62d58fd53bf5e059f7cc3245",
        "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
        "isPro": false,
        "fullname": "Dian Yu",
        "user": "yudian",
        "type": "user"
      },
      "summary": "El modelo de recompensa generado (o LLMs-as-judges) es un modelo que utiliza grandes modelos de lenguaje (LLMs) para evaluar la calidad de las respuestas, y se ha incrementado con la aplicación de un aprendizaje reforzado (RLVR) que incluye recompensas seguras. Se prefiere más que los métricas basadas en reglas, especialmente en tareas complejas de teoría de razonamiento que incluyen salidas libres. En este paradigma, generalmente, el LLM compara las respuestas candidatas con referencias factuales para asignar una recompensa binaria que demuestra la precisión. A diferencia de lo que parece una tarea sencilla, el modelo de recompensa generado es vulnerable a acciones superficiales: símbolos no palabrales (por ejemplo: «:» o «.») y procesos de razonamiento como «el proceso de pensamiento:» o «resolveré este problema de manera secuencial:» a menudo llevan a recompensas incorrectas. Esta vulnerabilidad se expande en una amplia gama de modelos de LLMs, datasets y formatos de exportación, y representa una amenaza severa para los paradigmas de algoritmos clave basados en modelos de recompensa (por ejemplo: sampling de rechazo, optimización de preferencias, RLVR). Para mitigar este problema, hemos introducido estrategias sencillas y efectivas de expansión de datos y hemos entrenado un nuevo modelo de recompensa generado para crear un modelo con una gran robustez. Nuestro descubrimiento subraya la urgente necesidad de métodos de evaluación basados en LLMs más confiables. Publicamos en https://huggingface.co/sarosavo/Master-RM y https://huggingface.co/datasets/sarosavo/Master-RM un modelo de recompensa robusto y generalizable y sus datos de expansión.",
      "upvotes": 7,
      "discussionId": "68747a58257d4f04353702ff",
      "ai_summary": "Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.",
      "ai_keywords": [
        "generative reward models",
        "LLMs-as-judges",
        "large language models",
        "reinforcement learning with verifiable rewards",
        "RLVR",
        "binary reward",
        "rejection sampling",
        "preference optimization"
      ]
    },
    "publishedAt": "2025-07-11T13:55:22.000Z",
    "title": "One Token to Fool LLM-as-a-Judge",
    "summary": "Generative reward models (also known as LLMs-as-judges), which use large\nlanguage models (LLMs) to evaluate answer quality, are increasingly adopted in\nreinforcement learning with verifiable rewards (RLVR). They are often preferred\nover rigid rule-based metrics, especially for complex reasoning tasks involving\nfree-form outputs. In this paradigm, an LLM is typically prompted to compare a\ncandidate answer against a ground-truth reference and assign a binary reward\nindicating correctness. Despite the seeming simplicity of this comparison task,\nwe find that generative reward models exhibit surprising vulnerabilities to\nsuperficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning\nopeners like \"Thought process:\" and \"Let's solve this problem step by step.\"\ncan often lead to false positive rewards. We demonstrate that this weakness is\nwidespread across LLMs, datasets, and prompt formats, posing a serious threat\nfor core algorithmic paradigms that rely on generative reward models, such as\nrejection sampling, preference optimization, and RLVR. To mitigate this issue,\nwe introduce a simple yet effective data augmentation strategy and train a new\ngenerative reward model with substantially improved robustness. Our findings\nhighlight the urgent need for more reliable LLM-based evaluation methods. We\nrelease our robust, general-domain reward model and its synthetic training data\nat https://huggingface.co/sarosavo/Master-RM and\nhttps://huggingface.co/datasets/sarosavo/Master-RM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08794.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d58fd53bf5e059f7cc3245",
      "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
      "fullname": "Dian Yu",
      "name": "yudian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08441",
      "authors": [
        {
          "_id": "6874b80f257d4f04353703a8",
          "name": "Anlin Zheng",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703a9",
          "name": "Xin Wen",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703aa",
          "name": "Xuanyang Zhang",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ab",
          "name": "Chuofan Ma",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ac",
          "name": "Tiancai Wang",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ad",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ae",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703af",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T09:32:45.000Z",
      "submittedOnDailyAt": "2025-07-14T06:26:25.095Z",
      "title": "El punto de vista sobre el tratamiento de la Vision Fund Model como un efectivo tokenizador visual para la generación automática de imágenes",
      "submittedOnDailyBy": {
        "_id": "63483629ac5172169929da0e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665676793089-noauth.jpeg",
        "isPro": false,
        "fullname": "Xin Wen",
        "user": "xwen99",
        "type": "user"
      },
      "summary": "Revisamos una nueva dirección que utiliza la fuerte expresión de un modelo de fundamentación de visión y texto para construir un tokenizador directamente sobre el modelo. En particular, esta área ha sido poco explorada. Concretamente, usamos un modelo de fundamentación de visión y texto libre como el encoder de un tokenizador. Para mejorar el efecto de este modelo, introducimos dos componentes principales: 1. Un marco de reducción por áreas para reducir la redundancia de las características predecidas en una malla 2D. 2. Un objeto de reconstrucción contextual que ajusta el output del tokenizador a la representación del modelo de fundamentación y mantiene la dependencia contextual. Basándonos en esta arquitectura, el tokenizador de imágenes propuesto, VFMTok, mejora significativamente la calidad de la reconstrucción y generación de imágenes, aumenta la eficiencia del tokenizado y promueve la generación automática de reconstrucción. Además, alcanza un gFID de 2.07 en el benchmark de ImageNet, acelera la convergencia del modelo en un tercio y permite la síntesis de alta calidad con condiciones de clase. El código está publicamente releasado y ofrece beneficios a la comunidad.",
      "upvotes": 4,
      "discussionId": "6874b80f257d4f04353703b0",
      "ai_summary": "A novel image tokenizer built on pre-trained vision foundation models improves image reconstruction, generation quality, and token efficiency, enhancing autoregressive generation and class-conditional synthesis.",
      "ai_keywords": [
        "pre-trained vision foundation models",
        "image tokenizer",
        "region-adaptive quantization framework",
        "semantic reconstruction objective",
        "VFMTok",
        "gFID",
        "autoregressive generation",
        "class-conditional synthesis",
        "classifier-free guidance"
      ]
    },
    "publishedAt": "2025-07-11T05:32:45.000Z",
    "title": "Vision Foundation Models as Effective Visual Tokenizers for\n  Autoregressive Image Generation",
    "summary": "Leveraging the powerful representations of pre-trained vision foundation\nmodels -- traditionally used for visual comprehension -- we explore a novel\ndirection: building an image tokenizer directly atop such models, a largely\nunderexplored area. Specifically, we employ a frozen vision foundation model as\nthe encoder of our tokenizer. To enhance its effectiveness, we introduce two\nkey components: (1) a region-adaptive quantization framework that reduces\nredundancy in the pre-trained features on regular 2D grids, and (2) a semantic\nreconstruction objective that aligns the tokenizer's outputs with the\nfoundation model's representations to preserve semantic fidelity. Based on\nthese designs, our proposed image tokenizer, VFMTok, achieves substantial\nimprovements in image reconstruction and generation quality, while also\nenhancing token efficiency. It further boosts autoregressive (AR) generation --\nachieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model\nconvergence by three times, and enabling high-fidelity class-conditional\nsynthesis without the need for classifier-free guidance (CFG). The code will be\nreleased publicly to benefit the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08441.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63483629ac5172169929da0e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665676793089-noauth.jpeg",
      "fullname": "Xin Wen",
      "name": "xwen99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06952",
      "authors": [
        {
          "_id": "68749c03257d4f043537033e",
          "name": "Keyon Vafa",
          "hidden": false
        },
        {
          "_id": "68749c03257d4f043537033f",
          "name": "Peter G. Chang",
          "hidden": false
        },
        {
          "_id": "68749c03257d4f0435370340",
          "name": "Ashesh Rambachan",
          "hidden": false
        },
        {
          "_id": "68749c03257d4f0435370341",
          "name": "Sendhil Mullainathan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T15:36:15.000Z",
      "submittedOnDailyAt": "2025-07-14T04:27:13.989Z",
      "title": "¿Qué se enfocó el modelo básico? Exploramos el modelo mundial utilizando la inclinación inferencial.",
      "submittedOnDailyBy": {
        "_id": "64d98ef7a4839890b25eb78b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
        "isPro": true,
        "fullname": "Fangyuan Yu",
        "user": "Ksgk-fy",
        "type": "user"
      },
      "summary": "El modelo básico se basa en la comprensión profunda del dominio de la predicción de secuencias. Esto es similar a cómo la predicción de la movimiento de planetas en el universo solar se conecta con la descubrimiento de la mecánica de Newton. Sin embargo, evaluar si el modelo comprende la estructura esencial exactamente es un desafío. Hemos desarrollado técnicas para evaluar el modelo básico, que investigan cómo el modelo se adapta a conjuntos de datos sintéticos generados a partir del modelo mundial establecido. Estas técnicas evalúan si el modelo básico tiene un índice de brazo que coincide con el modelo mundial, lo cual se hace mediante la prueba de índice de brazo. Aunque el modelo básico ha demostrado excelentes resultados en tareas de entrenamiento en varios dominios, se ha descubierto que no ha desarrollado pruebas de índice de brazo para el modelo mundial cuando se adapta a nuevas tareas. En particular, modelos entrenados en proyectos de rutas han mostrado un comportamiento consistente al no aplicar la mecánica de Newton en nuevas tareas físicas. Además, estos modelos han desarrollado heurísticas para las tareas específicas, pero han mostrado que no pueden generalizar sus comportamientos.",
      "upvotes": 4,
      "discussionId": "68749c03257d4f0435370342",
      "ai_summary": "Foundation models, despite excelling in training tasks, often fail to generalize to new tasks due to task-specific heuristics rather than capturing underlying world models.",
      "ai_keywords": [
        "sequence prediction",
        "foundation models",
        "inductive bias",
        "synthetic datasets",
        "world model",
        "inductive bias probe",
        "Newtonian mechanics",
        "task-specific heuristics",
        "generalization"
      ]
    },
    "publishedAt": "2025-07-09T11:36:15.000Z",
    "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for\n  World Models",
    "summary": "Foundation models are premised on the idea that sequence prediction can\nuncover deeper domain understanding, much like how Kepler's predictions of\nplanetary motion later led to the discovery of Newtonian mechanics. However,\nevaluating whether these models truly capture deeper structure remains a\nchallenge. We develop a technique for evaluating foundation models that\nexamines how they adapt to synthetic datasets generated from some postulated\nworld model. Our technique measures whether the foundation model's inductive\nbias aligns with the world model, and so we refer to it as an inductive bias\nprobe. Across multiple domains, we find that foundation models can excel at\ntheir training tasks yet fail to develop inductive biases towards the\nunderlying world model when adapted to new tasks. We particularly find that\nfoundation models trained on orbital trajectories consistently fail to apply\nNewtonian mechanics when adapted to new physics tasks. Further analysis reveals\nthat these models behave as if they develop task-specific heuristics that fail\nto generalize.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d98ef7a4839890b25eb78b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
      "fullname": "Fangyuan Yu",
      "name": "Ksgk-fy",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08771",
      "authors": [
        {
          "_id": "68747a17257d4f04353702ef",
          "name": "Chenyang Song",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f0",
          "name": "Weilin Zhao",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f1",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f2",
          "name": "Chaojun Xiao",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f3",
          "name": "Yingfa Chen",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f4",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f5",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f6",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:28:56.000Z",
      "submittedOnDailyAt": "2025-07-14T03:24:41.458Z",
      "title": "BlockFFN: Función de Activación Espacial con Especialidades Mixtas Orientadas hacia el Mixture of Experts adecuado para la Gain en la Sección de la Terminación a través del Chunk-Level Activation Sparsity",
      "submittedOnDailyBy": {
        "_id": "64c09684e56520a63d35ec87",
        "avatarUrl": "/avatars/86a338e8d122a66a94143bbb9bf3ebf8.svg",
        "isPro": false,
        "fullname": "Chenyang Song",
        "user": "Raincleared",
        "type": "user"
      },
      "summary": "Para reducir el carga computacional de los modelos de lenguaje de la naturaleza (LLMs), se han destacado arquitecturas con la rareza de las activaciones, especialmente la Mix of Experts (MoE). Sin embargo, la falta de invarianza diferencial y flexibilidad en la MoE de los modelos base ha afectado el rendimiento. Además, se ha demostrado que cada token solo necesita activar los pocos parámetros, lo que revela la baja rareza de activación en la arquitectura. Esto indica una alta proporción de parámetros activados en múltiples tokens continuos. Este patrón de rareza no es adecuado para aceleraciones bajo recursos limitados (por ejemplo: dispositivos de punto final) ni para las principales tecnologías de aceleración (por ejemplo: interpretación de inferencia). Para resolver estos problemas, introducimos una nueva arquitectura MoE, BlockFFN, y proponemos métodos eficientes de entrenamiento y batch. En particular, mediante la combinación de la activación ReLU y RMSNorm, logramos realizar una ruta diferenciable y flexible. Además, diseñamos objetivos de entrenamiento para promover la rareza de activación en el nivel de token (TLS) y en el nivel de reloj de tokens (CLS), y adaptamos BlockFFN para ello. Finalmente, implementamos el primer acelerador válido que combina la rareza de activación y la interpretación de inferencia. Los resultados experimentales muestran que BlockFFN supera a otras líneas basadas en MoE en rendimiento, alcanzando más del 80% de TLS y más del 70% de CLS para 8 tokens. Nuestro acelerador logró un aumento de velocidad de 3.67 veces en dispositivos de punto final. Todo el código y los checkpoints están disponibles (https://github.com/thunlp/BlockFFN).",
      "upvotes": 2,
      "discussionId": "68747a17257d4f04353702f7",
      "githubRepo": "https://github.com/thunlp/BlockFFN",
      "githubStars": 3
    },
    "publishedAt": "2025-07-11T13:28:56.000Z",
    "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity",
    "summary": "To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67times speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08771.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c09684e56520a63d35ec87",
      "avatarUrl": "/avatars/86a338e8d122a66a94143bbb9bf3ebf8.svg",
      "fullname": "Chenyang Song",
      "name": "Raincleared",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07151",
      "authors": [
        {
          "_id": "68707d75c8391850d6097823",
          "user": {
            "_id": "66b34647a29e5c00011d34c3",
            "avatarUrl": "/avatars/ae39f9b84f9379423fa3a8509fbcc94e.svg",
            "isPro": false,
            "fullname": "Zongmeng Zhang",
            "user": "ustc-zhangzm",
            "type": "user"
          },
          "name": "Zongmeng Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:24.241Z",
          "hidden": false
        },
        {
          "_id": "68707d75c8391850d6097824",
          "name": "Wengang Zhou",
          "hidden": false
        },
        {
          "_id": "68707d75c8391850d6097825",
          "name": "Jie Zhao",
          "hidden": false
        },
        {
          "_id": "68707d75c8391850d6097826",
          "name": "Houqiang Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b34647a29e5c00011d34c3/Zj-2S7r3SlJnKipepoQxm.png"
      ],
      "publishedAt": "2025-07-09T11:18:38.000Z",
      "submittedOnDailyAt": "2025-07-14T01:45:01.699Z",
      "title": "Modelo fuerte de lenguaje de doble lengua para el estudio de la red roja del modelo de flujo",
      "submittedOnDailyBy": {
        "_id": "66b34647a29e5c00011d34c3",
        "avatarUrl": "/avatars/ae39f9b84f9379423fa3a8509fbcc94e.svg",
        "isPro": false,
        "fullname": "Zongmeng Zhang",
        "user": "ustc-zhangzm",
        "type": "user"
      },
      "summary": "El modelo de lenguaje multimodal de DeepMood (MLLM) muestra excelentes capacidades en tareas de lenguaje visual pero suele producir hallucinaciones fácilmente en situaciones reales. En este artículo, se centra en el conflicto entre la respuesta del modelo y la entrada, investigando el fenómeno de hallucinación en MLLM. Aunque los estudios previos han enfocado el conflicto entre la respuesta y la entrada, nosotros investigamos el conflicto específico de entradas que provocan directamente la hallucinación. Definimos formalmente el conflicto del modelo y construimos el conjunto de datos de Modalidad Multimodal de Conflicto (MMMC) para simular este fenómeno en tareas de lenguaje visual. Proponemos tres métodos: ingeniería de prompts, ajustes normativos y aprendizaje por refuerzo, con el objetivo de mitigar la hallucinación causada por el conflicto del modelo. Extendimos el conjunto de datos MMMC para realizar experimentos y analizamos las ventajas y desventajas de estos métodos. Nuestros resultados muestran que el aprendizaje por refuerzo es el que mejor mitiga la hallucinación bajo conflictos del modelo, mientras que el ajuste normativo muestra un rendimiento deseable. Nuestra investigación clarifica el conflicto del modelo que provoca hallucinaciones y proporciona una comprensión más profunda de la robustez de MLLM.",
      "upvotes": 2,
      "discussionId": "68707d76c8391850d6097827",
      "projectPage": "https://github.com/zmzhang2000/MMMC",
      "githubRepo": "https://github.com/zmzhang2000/MMMC",
      "ai_summary": "Investigation of modality conflict in multimodal large language models reveals its role in causing hallucinations, with reinforcement learning emerging as the most effective mitigation strategy.",
      "ai_keywords": [
        "multimodal large language models",
        "vision-language tasks",
        "hallucinations",
        "modality conflict",
        "prompt engineering",
        "supervised fine-tuning",
        "reinforcement learning",
        "Multimodal Modality Conflict (MMMC)"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-09T07:18:38.000Z",
    "title": "Robust Multimodal Large Language Models Against Modality Conflict",
    "summary": "Despite the impressive capabilities of multimodal large language models\n(MLLMs) in vision-language tasks, they are prone to hallucinations in\nreal-world scenarios. This paper investigates the hallucination phenomenon in\nMLLMs from the perspective of modality conflict. Unlike existing works focusing\non the conflicts between model responses and inputs, we study the inherent\nconflicts in inputs from different modalities that place MLLMs in a dilemma and\ndirectly lead to hallucinations. We formally define the modality conflict and\nconstruct a dataset named Multimodal Modality Conflict (MMMC) to simulate this\nphenomenon in vision-language tasks. Three methods based on prompt engineering,\nsupervised fine-tuning, and reinforcement learning are proposed to alleviate\nthe hallucination caused by modality conflict. Extensive experiments are\nconducted on the MMMC dataset to analyze the merits and demerits of these\nmethods. Our results show that the reinforcement learning method achieves the\nbest performance in mitigating the hallucination under modality conflict, while\nthe supervised fine-tuning method shows promising and stable performance. Our\nwork sheds light on the unnoticed modality conflict that leads to\nhallucinations and provides more insights into the robustness of MLLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b34647a29e5c00011d34c3/Zj-2S7r3SlJnKipepoQxm.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07151.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b34647a29e5c00011d34c3",
      "avatarUrl": "/avatars/ae39f9b84f9379423fa3a8509fbcc94e.svg",
      "fullname": "Zongmeng Zhang",
      "name": "ustc-zhangzm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]