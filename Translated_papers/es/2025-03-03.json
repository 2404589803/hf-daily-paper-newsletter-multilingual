[
  {
    "paper": {
      "id": "2502.20730",
      "authors": [
        {
          "_id": "67c514aba3d873e41624a082",
          "user": {
            "_id": "63664c8fa2abcdf2fd6425ed",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
            "isPro": false,
            "fullname": "Li Zhuoqun",
            "user": "lzq2021",
            "type": "user"
          },
          "name": "Zhuoqun Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-03T08:07:26.218Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a083",
          "user": {
            "_id": "64a4ceda9a90f701134189b7",
            "avatarUrl": "/avatars/859a189c5d2ae2fcb9aa2d79104fbfe7.svg",
            "isPro": false,
            "fullname": "Haiyang Yu",
            "user": "yhycai",
            "type": "user"
          },
          "name": "Haiyang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-03T09:31:12.493Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a084",
          "user": {
            "_id": "63ef664304b0e373992a2633",
            "avatarUrl": "/avatars/cba554ff88bd8b68ae51bea8ee991d13.svg",
            "isPro": false,
            "fullname": "Xuanang Chen",
            "user": "xuanang",
            "type": "user"
          },
          "name": "Xuanang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:31.384Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a085",
          "user": {
            "_id": "6711c702f858a456b4b9f3a4",
            "avatarUrl": "/avatars/178e9567c3111ab22717c3c0dd003a6a.svg",
            "isPro": false,
            "fullname": "Hongyu  Lin",
            "user": "sanmusunrise",
            "type": "user"
          },
          "name": "Hongyu Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:28:09.791Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a086",
          "user": {
            "_id": "6216496a9b34d2fb49144599",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg",
            "isPro": false,
            "fullname": "Yaojie Lu",
            "user": "luyaojie",
            "type": "user"
          },
          "name": "Yaojie Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:38.957Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a087",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a088",
          "user": {
            "_id": "65e99a77e71555ed193609cf",
            "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
            "isPro": false,
            "fullname": "Xianpei Han",
            "user": "xphan",
            "type": "user"
          },
          "name": "Xianpei Han",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:51.007Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a089",
          "user": {
            "_id": "66641b2fd8e1e34bc621e688",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66641b2fd8e1e34bc621e688/csPETwnx2zCIHSWi9uAi-.png",
            "isPro": false,
            "fullname": "Yongbin Li",
            "user": "Yongbin-Li",
            "type": "user"
          },
          "name": "Yongbin Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:29:57.561Z",
          "hidden": false
        },
        {
          "_id": "67c514aba3d873e41624a08a",
          "name": "Le Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-28T05:23:10.000Z",
      "title": "DeepSolution: Mejora de diseño de soluciones técnicas de ingeniería complejas mediante el uso de exploración arborícola y dos formas de pensamiento",
      "summary": "La diseño de soluciones para problemas complejos de ingeniería es crucial en las actividades productivas humanas. Sin embargo, la investigación en el campo de la generación con revisión (Review-Augmented Generation, RAG) pasada no ha abordado suficientemente las tareas relacionadas con el diseño de soluciones complejas de ingeniería. Para llenar este vacío, presentamos SolutionBench, un nuevo marco de referencia para evaluar la capacidad de generar soluciones completas posibles para problemas de ingeniería. Además, para fomentar el desarrollo del diseño de soluciones complejas de ingeniería, proponemos un nuevo sistema llamado SolutionRAG, que utiliza exploración de árboles y estructuras de pensamiento bidimensionales. A través de resultados experimentales ampliados, SolutionRAG logra alcanzar el rendimiento más reciente (SOTA) en SolutionBench y muestra claramente su potencial para automatizar y mejorar la confianza en el diseño de soluciones complejas de ingeniería en aplicaciones globales.",
      "upvotes": 11,
      "discussionId": "67c514aca3d873e41624a10b"
    },
    "publishedAt": "2025-03-02T21:35:24.437Z",
    "title": "DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/y_kT4GP3xgm-5RdguMNV7.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/wDAS_USsxsVHbin1I5CEe.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/4lJgWp9V8pm4vDBUH4I5n.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20730.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63664c8fa2abcdf2fd6425ed",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
      "fullname": "Li Zhuoqun",
      "name": "lzq2021",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.18600",
      "authors": [
        {
          "_id": "67c0a8058589d8ecb79d472b",
          "user": {
            "_id": "6594b1bb57a556fbe162915e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594b1bb57a556fbe162915e/WuYxqbbvaJaT-xsk5KhoT.jpeg",
            "isPro": false,
            "fullname": "Silei Xu",
            "user": "sileixu",
            "type": "user"
          },
          "name": "Silei Xu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-27T18:01:14.543Z",
          "hidden": false
        },
        {
          "_id": "67c0a8058589d8ecb79d472c",
          "name": "Wenhao Xie",
          "hidden": false
        },
        {
          "_id": "67c0a8058589d8ecb79d472d",
          "name": "Lingxiao Zhao",
          "hidden": false
        },
        {
          "_id": "67c0a8058589d8ecb79d472e",
          "user": {
            "_id": "5efd09cf49ed724c8a135868",
            "avatarUrl": "/avatars/af12bc94657979677a9f26183f0c9727.svg",
            "isPro": false,
            "fullname": "Pengcheng He",
            "user": "DeBERTa",
            "type": "user"
          },
          "name": "Pengcheng He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:30:43.479Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T19:36:06.000Z",
      "title": "Una guía rápida para pensar rápido con un plan de borrador corto\n\n**Translation:**\nUna guía rápida para pensar rápido con un plan de borrador corto",
      "summary": "Los modelos de lenguaje grande (LLMs) muestran excelentes resultados en la resolución de problemas de razonamiento complejo a través de estructuras como el entrenamiento de la cadena de pensamiento (CoT). Sin embargo, la humanidad a menudo opta por estrategias más eficientes: escribir pensamientos intermedios resumidos y extraer de ellos solo la información básica. En este artículo, se propone un nuevo paradigma llamado \"Cadena de Borrado (CoD)\" que inspira en el proceso cognitivo humano, permitiendo que los LLMs emitan una mínima información de razonamiento intermedio mientras resuelven tareas. Con esto, se reduce la burocracia y se centran en los puntos de vista importantes, logrando resultados precisos comparables a los de CoT, y además se reducen significativamente los costos y la latencia en tareas de razonamiento compleja, usando solo el 7.6% de los tokens.",
      "upvotes": 6,
      "discussionId": "67c0a8078589d8ecb79d47ed"
    },
    "publishedAt": "2025-03-03T02:35:09.967Z",
    "title": "Chain of Draft: Thinking Faster by Writing Less",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18600.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63da3d7ae697e5898cb86854",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675246771355-noauth.jpeg",
      "fullname": "Talha Rüzgar Akkuş",
      "name": "Q-bert",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 85
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18017",
      "authors": [
        {
          "_id": "67bef5a6070ec160042d99f4",
          "user": {
            "_id": "657429d833e5a4bf5b278615",
            "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
            "isPro": false,
            "fullname": "QiuchenWang",
            "user": "autumncc",
            "type": "user"
          },
          "name": "Qiuchen Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-28T12:15:57.850Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f5",
          "name": "Ruixue Ding",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f6",
          "user": {
            "_id": "64892d31cbda0d1cdb956897",
            "avatarUrl": "/avatars/3cdafe03a8295124636347d15a099aaf.svg",
            "isPro": false,
            "fullname": "Zehui Chen",
            "user": "lovesnowbest",
            "type": "user"
          },
          "name": "Zehui Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:32:18.129Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f7",
          "user": {
            "_id": "65351cbe6141b3927afaed17",
            "avatarUrl": "/avatars/5abf5f2c4ab329e63a7f45c15c9dfb93.svg",
            "isPro": false,
            "fullname": "weiqi wu",
            "user": "vickywu",
            "type": "user"
          },
          "name": "Weiqi Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:32:12.075Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f8",
          "user": {
            "_id": "62e8efb14210d3fe69eacb42",
            "avatarUrl": "/avatars/2feadd75274bf353b910f4679ef72b39.svg",
            "isPro": false,
            "fullname": "Shihang Wang",
            "user": "shihang",
            "type": "user"
          },
          "name": "Shihang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:32:05.679Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99f9",
          "user": {
            "_id": "63a091e42fabbbb89991f5ce",
            "avatarUrl": "/avatars/d55485b06461764c36c9edf9d6e8892c.svg",
            "isPro": false,
            "fullname": "pengjun xie",
            "user": "xpjandy",
            "type": "user"
          },
          "name": "Pengjun Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:31:59.813Z",
          "hidden": false
        },
        {
          "_id": "67bef5a6070ec160042d99fa",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T09:26:12.000Z",
      "title": "ViDoRAG: Agente de razonamiento iterativo dinámico para la expansión y generación de búsqueda de documentos visuales",
      "summary": "Entender información en documentos visualmente ricos es un problema importante en el enfoque tradicional de Retrieval-Augmented Generation (RAG). Los actuales benchmarks se centran principalmente en la respuesta a preguntas basadas en imágenes, ignorando los problemas básicos de búsqueda eficiente, comprensión y búsqueda de razones en documentos visualmente complejos. Para complementar estas deficiencias, se introdujo el nuevo dataset ViDoSeek, proporcionando métodos de evaluación adecuados para documentos visualmente ricos que requieren razones complejas. Con esto, se identificaron las principales limitaciones del enfoque actual de RAG: (i) los métodos de búsqueda visual simples tienen dificultades en integrar efectivamente características visuales y textuales, y (ii) los enfoques actuales no suelen asignar suficientes tokens de razones, lo que limita su eficacia. Para resolver estos problemas, se propone ViDoRAG, un nuevo marco de RAG multi-agente. ViDoRAG requiere razones complejas para documentos visualmente ricos y utiliza un modelo híbrido basado en Gaussian Mixture Model (GMM) para procesar eficientemente búsquedas multi-modelo. Para mejorar la capacidad de razonamiento del modelo, se introduce un flujo de trabajo iterativo de agentes que incluye exploración, resumen y reflexión, y se proporciona un marco para escalar el tiempo de prueba en el campo de RAG. Los experimentos extendidos en ViDoSeek demuestran la eficacia y la capacidad de generalización de nuestro enfoque. En particular, ViDoRAG demostró un efecto competitivo con los métodos actuales en el benchmark ViDoSeek, mejorando los resultados en un 10% o más.",
      "upvotes": 4,
      "discussionId": "67bef5a7070ec160042d9a65"
    },
    "publishedAt": "2025-03-02T22:22:01.895Z",
    "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18017.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657429d833e5a4bf5b278615",
      "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
      "fullname": "QiuchenWang",
      "name": "autumncc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20545",
      "authors": [
        {
          "_id": "67c51b459d5807d6674b3d3c",
          "name": "Kechen Li",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d3d",
          "name": "Wenqi Zhu",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d3e",
          "name": "Coralia Cartis",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d3f",
          "user": {
            "_id": "64bb61e876a6e2efcc728e22",
            "avatarUrl": "/avatars/b0ed1c9f13fd1f2c99d202155001e39b.svg",
            "isPro": false,
            "fullname": "Tianbo Ji",
            "user": "jitianbo",
            "type": "user"
          },
          "name": "Tianbo Ji",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:35:49.782Z",
          "hidden": false
        },
        {
          "_id": "67c51b459d5807d6674b3d40",
          "name": "Shiwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T21:41:43.000Z",
      "title": "SoS1: O1 y R1-Like Reasoning LLMs son los encargados de resolver problemas de suma de cuadrados.",
      "summary": "Los modelos de lenguaje grande (LLMs) logran alcanzar una excelente capacidad cognitiva humana en diversas tareas, pero su capacidad para resolver problemas matemáticos de forma estricta es un desafío abierto. En este estudio, se abordan problemas básicamente complejos de cálculo, incluyendo la determinación de si un polinomio multivariable es positivo o no. Este problema tiene una relación estrecha con la 17ta pregunta de Hilbert y juega un papel crucial en la optimización global de polinomios, siendo aplicado en diversas áreas.\n\nPrimero, se presenta el dataset SoS-1K, un conjunto de datos refinado y estructurado. Este dataset incluye guías lógicas diseñadas por expertos basadas en cinco criterios evaluativos de dificultad progresiva. Usando este dataset, se evaluaron los modelos de LLMs más recientes. Sin guías lógicas estructuradas, todos los modelos superan la base aleatoria de 50% solo un poco más. Sin embargo, guías lógicas de alta calidad mejoran significativamente la precisión, aumentando el rendimiento en un 81%. Además, a través de un ajuste de 4 horas en SoS-1K, el modelo SoS-7B, con 7B parámetros, supera la precisión de DeepSeek-V3 y GPT-4o-mini, necesitando solo 1.8% y 5% del tiempo computacional respectivamente.\n\nLos resultados de este estudio demuestran que los modelos de LLMs pueden superar los límites de las guías lógicas matemáticas y mostrar la posibilidad de resolver problemas NP-hard.",
      "upvotes": 2,
      "discussionId": "67c51b469d5807d6674b3d88"
    },
    "publishedAt": "2025-03-02T22:00:31.796Z",
    "title": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20545.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6260
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20396",
      "authors": [
        {
          "_id": "67c51d36c830dcb76bbb5994",
          "name": "Toru Lin",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5995",
          "name": "Kartik Sachdev",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5996",
          "name": "Linxi Fan",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5997",
          "user": {
            "_id": "65369a95605a07338de78ab0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sGFjOjLT2akN-sn5beVWL.jpeg",
            "isPro": false,
            "fullname": "Jitendra Malik ",
            "user": "jitendra1995",
            "type": "user"
          },
          "name": "Jitendra Malik",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:36:34.177Z",
          "hidden": false
        },
        {
          "_id": "67c51d36c830dcb76bbb5998",
          "name": "Yuke Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T18:59:52.000Z",
      "title": "Simulación del detector visual basado en modelos de muñecas utilizando aprendizaje por refuerzo en un robot móvil real",
      "summary": "El aprendizaje por refuerzo es capaz de lograr resultados deseados en diversos dominios de problemas, pero el éxito en la manipulación de robótica dedicada es limitado. En esta investigación, se exploran los principales desafíos en la aplicación del aprendizaje por refuerzo para resolver tareas de trabajo que involucran un contacto abundante con la estructura corporal de un juguete. Presentamos una nueva metodología para realizar complementaciones cognitivas y probamos su efectividad. Nuestra contribución principal incluye un módulo de ajuste automático que se ajusta a entornos de simulación, un diseño de recompensas generalizado para simplificar la obtención de recompensas a largo plazo en tareas de trabajo con contacto abundante, un proceso de entrenamiento de ataques en diversos pasos que mejora la eficacia de la exploración y mantiene el rendimiento en el mundo real a partir de la simulación, y una representación mixta de objetos raros y densos para concretar errores cognitivos desde la simulación. Demostramos los resultados deseados en tres tareas de trabajo dedicadas en juguetes y realizamos un estudio de los métodos utilizados. Nuestro trabajo proporciona una aproximación exitosa para el aprendizaje por refuerzo desde la simulación hasta el mundo real, logrando una fortaleza generalizada y altos rendimientos sin la necesidad de guía humana.",
      "upvotes": 1,
      "discussionId": "67c51d39c830dcb76bbb5a1f"
    },
    "publishedAt": "2025-03-02T22:08:44.891Z",
    "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20396.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6260
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20811",
      "authors": [
        {
          "_id": "67c51c198d02783fa3a6249d",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a6249e",
          "name": "Jingyun Hua",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a6249f",
          "user": {
            "_id": "675a69699e086bd6250a36ef",
            "avatarUrl": "/avatars/95c72e3975d1a37f8655a2fe629746ec.svg",
            "isPro": false,
            "fullname": "Weihong Lin",
            "user": "lwher1996",
            "type": "user"
          },
          "name": "Weihong Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:42:30.547Z",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a0",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a1",
          "name": "Fuzheng Zhang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a2",
          "name": "Jianlong Wu",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a3",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "67c51c198d02783fa3a624a4",
          "name": "Liqiang Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-28T07:53:40.000Z",
      "title": "HAIC: Uso de capturas mejoradas para mejorar la comprensión y generación de acciones humanas en un modelo de lenguaje multimodal",
      "summary": "Los modelos de lenguaje multimodal (MLLMs) han logrado un gran avance en la comprensión de películas. Sin embargo, el rendimiento de los modelos en comprender el comportamiento humano está limitado por la escasez de datos de alta calidad. Para abordar este problema, hemos introducido un proceso de descripción de datos en dos etapas. Primero, diseñamos una estrategia para recopilar películas que incluyan claramente el comportamiento humano desde la web. Luego, utilizamos características humanas para diferenciar a los individuos y describir sus acciones y interacciones con detalles temporales en un formato de captiones estándar. Este proceso nos ha permitido crear dos conjuntos de datos: HAICTrain y HAICBench. HAICTrain se ha generado con Gemini-Pro y está compuesto por 126K pares de películas-capciones para fines de entrenamiento. Por otro lado, HAICBench incluye 500 pares de películas-capciones automáticamente generados y 1,400 pares de preguntas y respuestas, proporcionando una evaluación detallada de la comprensión del comportamiento humano. Los resultados de las pruebas muestran que el entrenamiento con HAICTrain mejora significativamente la capacidad de comprensión humana en cuatro marcos de referencia y mejora los resultados de generación de películas. HAICTrain y HAICBench están disponibles en https://huggingface.co/datasets/KuaishouHAIC/HAIC.",
      "upvotes": 1,
      "discussionId": "67c51c1b8d02783fa3a62543"
    },
    "publishedAt": "2025-03-02T22:04:15.087Z",
    "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20811.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6260
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20583",
      "authors": [
        {
          "_id": "67c516998d02783fa3a52dc8",
          "user": {
            "_id": "6304ac1a412a1b9d381ca378",
            "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
            "isPro": false,
            "fullname": "Keisuke Kamahori",
            "user": "kamahori",
            "type": "user"
          },
          "name": "Keisuke Kamahori",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-03T08:07:02.986Z",
          "hidden": false
        },
        {
          "_id": "67c516998d02783fa3a52dc9",
          "user": {
            "_id": "62908273c740ebb981a6dba4",
            "avatarUrl": "/avatars/465f50369c367b07670f5209c83d65f2.svg",
            "isPro": false,
            "fullname": "Jungo Kasai",
            "user": "jungok",
            "type": "user"
          },
          "name": "Jungo Kasai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:43:49.097Z",
          "hidden": false
        },
        {
          "_id": "67c516998d02783fa3a52dca",
          "user": {
            "_id": "628c26a8b80bb09700d6af86",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653352051245-noauth.jpeg",
            "isPro": false,
            "fullname": "Noriyuki Kojima",
            "user": "kojimano",
            "type": "user"
          },
          "name": "Noriyuki Kojima",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:43:56.698Z",
          "hidden": false
        },
        {
          "_id": "67c516998d02783fa3a52dcb",
          "user": {
            "_id": "654132fe5a9a913c6c870e79",
            "avatarUrl": "/avatars/2f6807eddef1929c571977e9af35f952.svg",
            "isPro": false,
            "fullname": "Baris Kasikci",
            "user": "kasikci",
            "type": "user"
          },
          "name": "Baris Kasikci",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-03T09:44:04.084Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T22:52:21.000Z",
      "title": "LiteASR: Uso de aproximaciones de baja calidad para un reconocimiento de voz automático eficiente",
      "summary": "En los modelos de reconocimiento automático de lenguaje modernos (ASR), como Whisper de OpenAI, se utilizan arquitecturas profundas encoder-decodor. Estos encoders desempeñan un papel crucial para la procesamiento eficiente, operando con altas cantidades de cálculo. Presentamos LiteASR, una técnica de compresión de bajo rendimiento para el encoder de ASR. Esta metodología permite reducir los costos de inferencia mientras mantiene la precisión de lectura. Nuestro enfoque se basa en la fuerte propiedad de bajo rendimiento de las activaciones intermedias: aplicamos el análisis de componentes principales (PCA) en pequeños conjuntos de datos de ajuste, y aproximamos la transformación lineal como una sucesión de multiplicaciones de matrices de bajo rendimiento, lo que permite optimizar la atención de símbolos. Los resultados de evaluación muestran que nuestro método puede reducir el tamaño del encoder de Whisper large-v3 en más del 50%, manteniendo o incluso mejorando la precisión de lectura, lo que establece una nueva frontera de Pareto entre eficiencia y rendimiento. El código de LiteASR está disponible en GitHub: https://github.com/efeslab/LiteASR.",
      "upvotes": 1,
      "discussionId": "67c516998d02783fa3a52dfd"
    },
    "publishedAt": "2025-03-02T21:48:46.577Z",
    "title": "LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20583.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6304ac1a412a1b9d381ca378",
      "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
      "fullname": "Keisuke Kamahori",
      "name": "kamahori",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.19577",
      "authors": [
        {
          "_id": "67c42356054ae6d1c760b643",
          "user": {
            "_id": "66588b6fd22637bfab498709",
            "avatarUrl": "/avatars/9007f0d3b078bd6193912a5359107f24.svg",
            "isPro": false,
            "fullname": "Hugues Turbé",
            "user": "hturbe",
            "type": "user"
          },
          "name": "Hugues Turbé",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-02T20:15:04.391Z",
          "hidden": false
        },
        {
          "_id": "67c42356054ae6d1c760b644",
          "name": "Mina Bjelogrlic",
          "hidden": false
        },
        {
          "_id": "67c42356054ae6d1c760b645",
          "name": "Gianmarco Mengaldo",
          "hidden": false
        },
        {
          "_id": "67c42356054ae6d1c760b646",
          "name": "Christian Lovis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T21:40:30.000Z",
      "title": "¡Hola! Aquí está la traducción al español:\n\n\"¡Hola! A continuación, se presenta la traducción al español del documento en inglés especificado.\"",
      "summary": "Los modelos de la Fundación Visual (VFMs) celebran altos rendimientos con la última tecnología y se están popularizando. Sin embargo, la explicabilidad es crucial. En este sentido, los modelos autoexplicativos (SEM) tienen como objetivo decompor la predicción en un conjunto de pesos de conceptos interpretables y proporcionar un clasificador de clases explicables. Se ha presentado evidencia de la falta de explicabilidad en estudios recientes. En este trabajo, se propone un enfoque llamado ProtoFM, que combina VFMs con una nueva arquitectura y objetivos de entrenamiento específicos. Con este enfoque, se entrena un pequeño conjunto de pesos (aproximadamente 1M parámetros) sobre VFMs, lo que ofrece una solución eficiente y explicable. Los resultados de evaluación muestran que nuestro enfoque supera los modelos actuales en una amplia gama de indicadores de explicabilidad, al mismo tiempo que logra un rendimiento relativo en la clasificación de clases. El código está disponible en https://github.com/hturbe/proto-fm.",
      "upvotes": 0,
      "discussionId": "67c4235c054ae6d1c760b806"
    },
    "publishedAt": "2025-03-03T04:21:42.563Z",
    "title": "Tell me why: Visual foundation models as self-explainable classifiers",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66588b6fd22637bfab498709/4VG_eDtZKZ4kj1AdG_P14.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19577.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66588b6fd22637bfab498709",
      "avatarUrl": "/avatars/9007f0d3b078bd6193912a5359107f24.svg",
      "fullname": "Hugues Turbé",
      "name": "hturbe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]