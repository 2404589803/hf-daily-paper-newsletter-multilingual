[
  {
    "paper": {
      "id": "2503.23307",
      "authors": [
        {
          "_id": "67eb4bd0eca57c4eebbb343a",
          "user": {
            "_id": "64f8e358766ff9f3d2b0de84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
            "isPro": true,
            "fullname": "Cong Wei",
            "user": "lim142857",
            "type": "user"
          },
          "name": "Cong Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:21.554Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343b",
          "name": "Bo Sun",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343c",
          "user": {
            "_id": "650a8979c19e5b4c8a6ff062",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650a8979c19e5b4c8a6ff062/64_JuECX_k_-uK7m7nlua.jpeg",
            "isPro": false,
            "fullname": "Haoyu Ma",
            "user": "haoyum1997",
            "type": "user"
          },
          "name": "Haoyu Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:54:47.847Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343d",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343e",
          "user": {
            "_id": "6444e8911cfc9ae6bb3ad216",
            "avatarUrl": "/avatars/8c06e064cf24789e4131f7af06dac86b.svg",
            "isPro": false,
            "fullname": "Xu",
            "user": "FelixXu",
            "type": "user"
          },
          "name": "Felix Juefei-Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:55:03.326Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343f",
          "name": "Zecheng He",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3440",
          "user": {
            "_id": "6549417b3ce45eb764faf993",
            "avatarUrl": "/avatars/d310f475d0697f5f13b3d4141ea0ccaf.svg",
            "isPro": false,
            "fullname": "Xiaoliang Dai",
            "user": "daixl1992",
            "type": "user"
          },
          "name": "Xiaoliang Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:54:01.490Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3441",
          "user": {
            "_id": "65a4fa7d2548c41ad9d9b710",
            "avatarUrl": "/avatars/3cace2d2f11f7194d8eca4b95b0b57cc.svg",
            "isPro": false,
            "fullname": "Luxin Zhang",
            "user": "Luczzz",
            "type": "user"
          },
          "name": "Luxin Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:53:55.152Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3442",
          "name": "Kunpeng Li",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3443",
          "user": {
            "_id": "655846d7ed8df83128f5826a",
            "avatarUrl": "/avatars/d7ce174d7d1b8614d5f6f071225c0057.svg",
            "isPro": false,
            "fullname": "Hou",
            "user": "Tingbo",
            "type": "user"
          },
          "name": "Tingbo Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:53:10.214Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3444",
          "name": "Animesh Sinha",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3445",
          "name": "Peter Vajda",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3446",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:52:50.248Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T04:22:09.000Z",
      "submittedOnDailyAt": "2025-04-01T00:46:45.446Z",
      "title": "Mojirad: Desafío a la síntesis de personajes de diálogo de nivel de película",
      "submittedOnDailyBy": {
        "_id": "64f8e358766ff9f3d2b0de84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
        "isPro": true,
        "fullname": "Cong Wei",
        "user": "lim142857",
        "type": "user"
      },
      "summary": "El reciente avance en la generación de imágenes ha logrado impresionantes movimientos, pero no se centra en los modos de transmisión de historias centradas en personajes. Esto es un problema importante para la generación automática de películas y animaciones. Presentamos 'Talking Characters', un trabajo más realista que genera animaciones interactivas de personajes a través de voz directa y frases. A diferencia de Talking Head, Talking Characters se centra en la generación de imágenes completas de personajes. En este artículo, proponemos MoCha, el primer modelo que genera animaciones interactivas de personajes. Para garantizar la precisa sincronización de imágenes y sonidos, proponemos una estructura de ventana de atención visuo-sonora efectiva en la correspondencia entre tokens de sonido y tokens de imágen. Para abordar la escasez de grandes conjuntos de datos de imágenes con etiquetas de sonido, proponemos una estrategia de entrenamiento bidireccional que utiliza tanto imágenes con etiquetas de sonido como con etiquetas de frases. Esto mejora significativamente la generalización para diferentes acciones de personajes. Además, diseñamos un template de prompt estructurado con etiquetas de personajes, lo que permite a AI interactuar en contextos cinematográficos y adaptarse a diversos personajes. La evaluación muy detallada, tanto cualitativa como cuantitativa, incluye estudios de preferencias humanas y comparaciones de benchmark. Esto establece nuevos estándares en la transmisión de historias cinematográficas generadas por AI, logrando realismo, expresividad, control y generalización de alto nivel.",
      "upvotes": 31,
      "discussionId": "67eb4bd3eca57c4eebbb34c7",
      "projectPage": "https://congwei1230.github.io/MoCha/",
      "ai_keywords": [
        "speech-video window attention mechanism",
        "speech-labeled video datasets",
        "text-labeled video data",
        "structured prompt templates",
        "character tags",
        "multi-character conversation",
        "turn-based dialogue",
        "context-aware conversations",
        "cinematic coherence"
      ]
    },
    "publishedAt": "2025-03-30T00:22:09.000Z",
    "title": "MoCha: Towards Movie-Grade Talking Character Synthesis",
    "summary": "Recent advancements in video generation have achieved impressive motion\nrealism, yet they often overlook character-driven storytelling, a crucial task\nfor automated film, animation generation. We introduce Talking Characters, a\nmore realistic task to generate talking character animations directly from\nspeech and text. Unlike talking head, Talking Characters aims at generating the\nfull portrait of one or more characters beyond the facial region. In this\npaper, we propose MoCha, the first of its kind to generate talking characters.\nTo ensure precise synchronization between video and speech, we propose a\nspeech-video window attention mechanism that effectively aligns speech and\nvideo tokens. To address the scarcity of large-scale speech-labeled video\ndatasets, we introduce a joint training strategy that leverages both\nspeech-labeled and text-labeled video data, significantly improving\ngeneralization across diverse character actions. We also design structured\nprompt templates with character tags, enabling, for the first time,\nmulti-character conversation with turn-based dialogue-allowing AI-generated\ncharacters to engage in context-aware conversations with cinematic coherence.\nExtensive qualitative and quantitative evaluations, including human preference\nstudies and benchmark comparisons, demonstrate that MoCha sets a new standard\nfor AI-generated cinematic storytelling, achieving superior realism,\nexpressiveness, controllability and generalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23307.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64f8e358766ff9f3d2b0de84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
      "fullname": "Cong Wei",
      "name": "lim142857",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23461",
      "authors": [
        {
          "_id": "67eb594988a08fae617242f1",
          "name": "Nikai Du",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f2",
          "user": {
            "_id": "66449e619ff401732687f013",
            "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
            "isPro": false,
            "fullname": "chen",
            "user": "zhen-nan",
            "type": "user"
          },
          "name": "Zhennan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:46.364Z",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f3",
          "user": {
            "_id": "637c22183d8e2e9c40c09fcf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669079538761-noauth.jpeg",
            "isPro": false,
            "fullname": "Zhizhou Chen",
            "user": "Chenzzzzzz",
            "type": "user"
          },
          "name": "Zhizhou Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:57:44.605Z",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f4",
          "name": "Shan Gao",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f5",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f6",
          "user": {
            "_id": "67593dd0f522f4409e614ba0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67593dd0f522f4409e614ba0/cvb9w_8seu3Kbjg_XAnNj.jpeg",
            "isPro": false,
            "fullname": "Jiang Zhengkai",
            "user": "jzzzzk",
            "type": "user"
          },
          "name": "Zhengkai Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:58:12.614Z",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f7",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f8",
          "user": {
            "_id": "65734004769f3ee9bde1af10",
            "avatarUrl": "/avatars/d6310ed861972fd691687d8f47413f33.svg",
            "isPro": false,
            "fullname": "Ying Tai",
            "user": "yingtai",
            "type": "user"
          },
          "name": "Ying Tai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:44.350Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T14:36:55.000Z",
      "submittedOnDailyAt": "2025-04-01T01:44:26.275Z",
      "title": "Texto Clasificador: Clasificación de patrones visuales complejos para graficar textos con precisión",
      "submittedOnDailyBy": {
        "_id": "66449e619ff401732687f013",
        "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
        "isPro": false,
        "fullname": "chen",
        "user": "zhen-nan",
        "type": "user"
      },
      "summary": "En este artículo, se revisa el desafío de la generación visual de texto complejo (CVTG). Esta se centra en la generación de contenidos de texto complejos distribuidos en diferentes áreas de una imagen visual. En el CVTG, se puede que el modelo de generación de imágenes cree textos visuales distorsionados o omita partes del texto visual. Para resolver estos problemas, proponemos una nueva técnica de graficado multidimensional de texto visual llamada \"TextCrafter\". TextCrafter descompone textos visuales complejos mediante stereografía progresiva y asegura una fuerte correspondencia entre el contenido del texto y la medida visual. Además, introduce una función de enfoque de tokens para fortalecer la importancia del texto visual. TextCrafter efectivamente resuelve principales problemas como confusión de texto, omisión y borrado en el CVTG. Además, presentamos un nuevo conjunto de datos de referencia \"CVTG-2K\" para evaluar rigurosamente el rendimiento de los modelos de generación en el CVTG. Los experimentos extendidos demuestran que nuestro enfoque supera los más avanzados en el campo.",
      "upvotes": 26,
      "discussionId": "67eb594b88a08fae617243ac",
      "projectPage": "https://dnknju.github.io/textcrafter-vue/",
      "githubRepo": "https://github.com/NJU-PCALab/TextCrafter",
      "ai_keywords": [
        "complex visual text",
        "TextCrafter",
        "multi-visual text rendering",
        "progressive strategy",
        "token focus enhancement",
        "CVTG-2K",
        "generative models",
        "CVTG tasks",
        "state-of-the-art approaches"
      ]
    },
    "publishedAt": "2025-03-30T10:36:55.000Z",
    "title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes",
    "summary": "This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23461.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66449e619ff401732687f013",
      "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
      "fullname": "chen",
      "name": "zhen-nan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24235",
      "authors": [
        {
          "_id": "67eb57023475e7b135788500",
          "user": {
            "_id": "62a42f22c683d02f5b63320c",
            "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
            "isPro": false,
            "fullname": "Qiyuan Zhang",
            "user": "DonJoey",
            "type": "user"
          },
          "name": "Qiyuan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:57:25.339Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788501",
          "user": {
            "_id": "65d2bb5c6130ef7be012d235",
            "avatarUrl": "/avatars/1c1e3bbb2c683a5c9d1f792a2c13fc4a.svg",
            "isPro": false,
            "fullname": "Fuyuan Lyu",
            "user": "silentspring2",
            "type": "user"
          },
          "name": "Fuyuan Lyu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:56:19.295Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788502",
          "user": {
            "_id": "65d1b42f3da87ce21e33261a",
            "avatarUrl": "/avatars/041cb441fa3871acde4ba565632056bf.svg",
            "isPro": false,
            "fullname": "RubinSun",
            "user": "RubinSun",
            "type": "user"
          },
          "name": "Zexu Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:51.229Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788503",
          "user": {
            "_id": "646def60df618b303b419323",
            "avatarUrl": "/avatars/97aa761d5255abf230304cfeade87835.svg",
            "isPro": false,
            "fullname": "Lei Wang",
            "user": "demolei",
            "type": "user"
          },
          "name": "Lei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T08:01:21.676Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788504",
          "user": {
            "_id": "63414659c5565a4b8d41bc42",
            "avatarUrl": "/avatars/25b4ca3002edf4c35cded0902c26632a.svg",
            "isPro": false,
            "fullname": "Weixu Zhang",
            "user": "nancy-zwx",
            "type": "user"
          },
          "name": "Weixu Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:56:26.532Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788505",
          "user": {
            "_id": "63c0c2497f52541dfc7d7567",
            "avatarUrl": "/avatars/16c174e2803ef86d09815b36a666ee0e.svg",
            "isPro": false,
            "fullname": "ZhihanGUO",
            "user": "ZhihanGUO",
            "type": "user"
          },
          "name": "Zhihan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:56:31.979Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788506",
          "name": "Yufei Wang",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788507",
          "name": "Irwin King",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788508",
          "name": "Xue Liu",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788509",
          "name": "Chen Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T15:46:15.000Z",
      "submittedOnDailyAt": "2025-04-01T01:37:27.268Z",
      "title": "¿Cómo, ¿cómo, ¿dónde, ¿cuánto mejor son? Investigación sobre la escalabilidad en el testeo de modelos de lenguaje de gran escala",
      "submittedOnDailyBy": {
        "_id": "62a42f22c683d02f5b63320c",
        "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
        "isPro": false,
        "fullname": "Qiyuan Zhang",
        "user": "DonJoey",
        "type": "user"
      },
      "summary": "La escalabilidad en empresas incluye la escalabilidad de datos y parámetros en etapa preparatoria, y la escalabilidad en tiempo de prueba (TTS) se ha centrado en \"calculo en tiempo de prueba\" en investigaciones rigurosas. Recientes estudios han demostrado que TTS afecta la capacidad de resolución de problemas en modelos de lenguaje de gran escala (LLMs), no solo en tareas de lógica especializada como matemáticas o programación, sino también en tareas generales como Q&A abierta. Sin embargo, con el rápido aumento de esfuerzos recientes en esta área, una investigación integral que proporcione un entendimiento sistemático es urgente. Para ello, proponemos un marco integrado y variado basado en cuatro aspectos clave: \"¿Qué escalar?\", \"¿Cómo escalar?\", \"¿Dónde escalar?\", \"¿Cómo escalar bien?\". Con este marco, realizamos una revisión ampliada sobre métodos, escenarios de aplicación y evaluación, analizando y presentando de manera sistemática las funciones específicas de cada tecnología en diferentes escalas de TTS. En esta análisis, extraemos los principales desarrollos históricos de TTS y proporcionamos guías para su introducción práctica. Además, desvelamos varios problemas públicos y proyectan las posibilidades de futuro, proponiendo también la expansión a mayores escalas, la clarificación de la realidad funcional de la tecnología, la generalización a más tareas y la explicación de su contribución.",
      "upvotes": 24,
      "discussionId": "67eb57053475e7b135788624",
      "ai_keywords": [
        "test-time scaling",
        "test-time computing",
        "large language models",
        "specialized reasoning tasks",
        "open-ended Q&A",
        "multidimensional framework",
        "what to scale",
        "how to scale",
        "where to scale",
        "how well to scale",
        "assessment aspects",
        "functional roles",
        "developmental trajectories",
        "practical deployment",
        "open challenges",
        "attributions"
      ]
    },
    "publishedAt": "2025-03-31T11:46:15.000Z",
    "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models",
    "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24235.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a42f22c683d02f5b63320c",
      "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
      "fullname": "Qiyuan Zhang",
      "name": "DonJoey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24388",
      "authors": [
        {
          "_id": "67eb544113ca8dcb9ccb991b",
          "name": "Zhonghan Zhao",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991c",
          "user": {
            "_id": "64e8505321540e1da3226b54",
            "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
            "isPro": false,
            "fullname": "Wenwei Zhang",
            "user": "ZwwWayne",
            "type": "user"
          },
          "name": "Wenwei Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:59:04.743Z",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991d",
          "name": "Haian Huang",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991e",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991f",
          "user": {
            "_id": "64070c5c4dc5f2846c925e93",
            "avatarUrl": "/avatars/ac2d7c1cd4ecccd6a88b85767c963ec7.svg",
            "isPro": false,
            "fullname": "Gao Jianfei",
            "user": "pppppM",
            "type": "user"
          },
          "name": "Jianfei Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:59:34.182Z",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb9920",
          "user": {
            "_id": "64c9033c5381684d3eaac7f1",
            "avatarUrl": "/avatars/07d36ca193826044b0df04e3602b9ef8.svg",
            "isPro": false,
            "fullname": "Gaoang Wang",
            "user": "GaoangWang",
            "type": "user"
          },
          "name": "Gaoang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:59:40.156Z",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb9921",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:59:52.000Z",
      "submittedOnDailyAt": "2025-04-01T01:27:12.837Z",
      "title": "RIG: Desde el fin del mundo hasta el fin del mundo, razones comunes de políticas y armonía de la imaginación",
      "submittedOnDailyBy": {
        "_id": "6601196cc91ba4c08ad6e270",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
        "isPro": false,
        "fullname": "yuzhe gu",
        "user": "vanilla1116",
        "type": "user"
      },
      "summary": "Inferencia y imaginación antes de la acción son esenciales para un agente corporizado que opera en un entorno complejo y abierto. Sin embargo, anteriores estudios han limitado la integración de estos dos habilidades en el agente, ya sea al incorporarlas de manera unilateral o al combinar varios modelos especializados en el aprendizaje de políticas, lo que restringía la eficiencia y la generalización. Por lo tanto, este artículo propone, por primera vez, la integración de inferencia y imaginación en una política generalizada llamada RIG (Reasoning and Imagination Generalist). Para entrenar a RIG de manera integrada, se construyó una pipeline de datos que gradualmente integraba y enriquecía los contenidos de imaginación y inferencia recopilados en las rutas de los agentes existentes. El aprendizaje conjunto de inferencia y generación de imágenes futuras mejoró la eficiencia de muestras y la generalización en un 17 veces más que los estudios previos, modelando de manera clara la correlación implícita entre inferencia, acción y dinámica del entorno. En la etapa de inferencia, RIG infería la siguiente acción, generaba acciones potenciales y, posteriormente, predecibía los resultados de las acciones, proporcionando al agente la oportunidad de revisar y auto-corregir sus acciones antes de que se ejecuten, basándose en la imaginación. Los resultados de los experimentos demuestran que la armonía entre inferencia y imaginación mejora la robustez, la generalización y la interacción de la política generalizada, y permite mejorar el rendimiento en tiempo de prueba a través de la expansión del tiempo de prueba.",
      "upvotes": 20,
      "discussionId": "67eb544213ca8dcb9ccb9963"
    },
    "publishedAt": "2025-03-31T13:59:52.000Z",
    "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy",
    "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than 17times sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24388.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.24370",
      "authors": [
        {
          "_id": "67eb4fff13ca8dcb9cca5f9b",
          "user": {
            "_id": "62fae9328e137d7c4b896498",
            "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
            "isPro": false,
            "fullname": "Tong Wu",
            "user": "tongwu2020",
            "type": "user"
          },
          "name": "Tong Wu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:31:28.689Z",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9c",
          "user": {
            "_id": "653319c135ad8b9e58a6b874",
            "avatarUrl": "/avatars/755efb5829f3b6d3cef886fee26e1ba9.svg",
            "isPro": false,
            "fullname": "Chong Xiang",
            "user": "cxiang",
            "type": "user"
          },
          "name": "Chong Xiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:00:19.547Z",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9d",
          "name": "Jiachen T. Wang",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9e",
          "name": "Prateek Mittal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:50:13.000Z",
      "submittedOnDailyAt": "2025-04-01T01:02:34.304Z",
      "title": "Control de modelos de inferencia efectivos mediante enfoque conceptual",
      "submittedOnDailyBy": {
        "_id": "62fae9328e137d7c4b896498",
        "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
        "isPro": false,
        "fullname": "Tong Wu",
        "user": "tongwu2020",
        "type": "user"
      },
      "summary": "Teorías asignadas a grandes modelos de lenguaje (LLMs) logran mejorar su rendimiento al generar una secuencia de lógica teórica explícita antes de producir la respuesta final, resolviendo de manera efectiva problemas complejos. En este artículo, se muestra que este nuevo marco de generación proporciona una oportunidad especial para controlar más precisamente el comportamiento del modelo. Proponemos un nuevo paradigma llamado \"Intervención en el Pensamiento\", que consiste en insertar o modificar tokenes de pensamiento estratégicamente para explicitar y guiar el proceso teórico-lógico interno de los LLMs. Se realizaron evaluaciones detalladas en tareas como IFEval, la estructura directiva de SEP, la consistencia de XSTest y la seguridad de SORRY-Bench. Resulta que la Intervención en el Pensamiento supera significativamente la aproximación de prompts convencional, mejorando la precisión en 6.7% utilizando el modelo DeepSeek R1 abierto, mejorando la lógica teórica en 15.4% y aumentando en 40.0% la rechazo de prompts inseguros. Nuestro estudio abre posibilidades para explorar nuevas investigaciones en el control de lógicas teóricas en LLMs.",
      "upvotes": 11,
      "discussionId": "67eb500013ca8dcb9cca5fe0",
      "ai_keywords": [
        "Reasoning-enhanced large language models (LLMs)",
        "intermediate reasoning steps",
        "Thinking Intervention",
        "thinking tokens",
        "instruction following",
        "IFEval",
        "instruction hierarchy",
        "SEP",
        "safety alignment",
        "XSTest",
        "SORRY-Bench",
        "open-source DeepSeek R1 models"
      ]
    },
    "publishedAt": "2025-03-31T13:50:13.000Z",
    "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
    "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fae9328e137d7c4b896498",
      "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
      "fullname": "Tong Wu",
      "name": "tongwu2020",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24364",
      "authors": [
        {
          "_id": "67eb6e6088a08fae617860f3",
          "user": {
            "_id": "600b381d3cc3b87db94bc0ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
            "isPro": false,
            "fullname": "Łukasz Borchmann",
            "user": "Borchmann",
            "type": "user"
          },
          "name": "Łukasz Borchmann",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-01T05:01:23.216Z",
          "hidden": false
        },
        {
          "_id": "67eb6e6088a08fae617860f4",
          "user": {
            "_id": "66c5e93e8f14c260be9d9f63",
            "avatarUrl": "/avatars/f4bf15e23923ef3256d3f01a3278d8bc.svg",
            "isPro": false,
            "fullname": "Marek Wydmuch",
            "user": "sfc-mwydmuch",
            "type": "user"
          },
          "name": "Marek Wydmuch",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:01:57.075Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/600b381d3cc3b87db94bc0ce/ucxS0pF0YXK8TDRhoezTE.qt"
      ],
      "publishedAt": "2025-03-31T17:43:36.000Z",
      "submittedOnDailyAt": "2025-04-01T03:14:34.239Z",
      "title": "Query and Conquer: Execution-Guided SQL Generation\n\nConsulta y Vencer: Generación de SQL orientada a la ejecución",
      "submittedOnDailyBy": {
        "_id": "600b381d3cc3b87db94bc0ce",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
        "isPro": false,
        "fullname": "Łukasz Borchmann",
        "user": "Borchmann",
        "type": "user"
      },
      "summary": "Proponemos un nuevo enfoque para aumentar significativamente la precisión de tareas SQL a partir de texto. Nuestro método selecciona varias candidatas para elegir la consulta que más se ajusta gramaticalmente a los resultados de ejecución, permitiendo a modelos pequeños y coste eficientes superar métodos de inferencia de alto costo (como o1, o3-mini, DeepSeek R1). Además, reducimos los costos de inferencia en un tercio. Este enfoque se integra fácilmente con modelos existentes y proporciona una clave para la creación de SQL práctica y escalable.",
      "upvotes": 10,
      "discussionId": "67eb6e6188a08fae6178613f",
      "ai_keywords": [
        "text-to-SQL",
        "execution results",
        "semantically consistent",
        "query",
        "candidates",
        "models",
        "reasoning methods",
        "o1",
        "o3-mini",
        "DeepSeek R1",
        "inference cost",
        "SQL generation"
      ]
    },
    "publishedAt": "2025-03-31T13:43:36.000Z",
    "title": "Query and Conquer: Execution-Guided SQL Generation",
    "summary": "We propose a novel approach for generating complex outputs that significantly\nimproves accuracy in text-to-SQL tasks. Our method leverages execution results\nto select the most semantically consistent query from multiple candidates,\nenabling smaller, cost-effective models to surpass computationally intensive\nreasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference\ncost by as much as 30 times. It integrates effortlessly with existing models,\noffering a practical and scalable pathway to state-of-the-art SQL generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/600b381d3cc3b87db94bc0ce/ucxS0pF0YXK8TDRhoezTE.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24364.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "600b381d3cc3b87db94bc0ce",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
      "fullname": "Łukasz Borchmann",
      "name": "Borchmann",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23284",
      "authors": [
        {
          "_id": "67eb5280aeab4ce97de07134",
          "user": {
            "_id": "6424538b9f9e65b42389920e",
            "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
            "isPro": false,
            "fullname": "Feng-Lin Liu",
            "user": "Okrin",
            "type": "user"
          },
          "name": "Feng-Lin Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:05.907Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07135",
          "user": {
            "_id": "662cd8b9322afcbae53fb06e",
            "avatarUrl": "/avatars/9847f5c2282d49e61e76a0a303e0b2b1.svg",
            "isPro": false,
            "fullname": "fuhongbo",
            "user": "fuhongbo",
            "type": "user"
          },
          "name": "Hongbo Fu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:01.616Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07136",
          "user": {
            "_id": "60e272ca6c78a8c122b12127",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
            "isPro": false,
            "fullname": "Xintao Wang",
            "user": "Xintao",
            "type": "user"
          },
          "name": "Xintao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:09.910Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07137",
          "user": {
            "_id": "6360d9f0472131c3bc4f61df",
            "avatarUrl": "/avatars/c5d884e5ef19b781e3405aba6dd68ca8.svg",
            "isPro": false,
            "fullname": "WeicaiYe",
            "user": "WeicaiYe",
            "type": "user"
          },
          "name": "Weicai Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:16.323Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07138",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07139",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:30.236Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de0713a",
          "name": "Lin Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T02:44:09.000Z",
      "submittedOnDailyAt": "2025-04-01T02:19:10.110Z",
      "title": "Sketch Video: Generación y edición de vídeos basados en dibujos",
      "submittedOnDailyBy": {
        "_id": "6424538b9f9e65b42389920e",
        "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
        "isPro": false,
        "fullname": "Feng-Lin Liu",
        "user": "Okrin",
        "type": "user"
      },
      "summary": "En la generación y edición de videos basados en gráficos de texto o imágenes, se ha avanzado significativamente, pero sigue existiendo un problema: controlar con precisión la disposición global y los detalles de la generalización solo con texto, así como realizar control de acciones y ediciones locales a través de imágenes, es una tarea compleja. En este artículo, se propone realizar un control espacial y de acciones basado en dibujos para la generación de videos, y proporcionar ediciones de gran detalle a videos reales o sintetizados. Basándonos en el modelo DiT para la generación de videos, se propone una estructura de control eficiente en memoria que incluye bloques de control basados en dibujos. Los dibujos se realizan en uno o dos marcos clave (en cualquier punto de tiempo), facilitando una interacción sencilla. Para propagar estas escenas temporalmente poco densas a todos los frames, se propone una estructura de atención entre marcos clave y frames de video. Para mantener los nuevos contenidos editados en la edición basada en dibujos, se diseña un módulo de inserción de videos. Durante la inferencia, se utiliza la fusión potencial para el almacenamiento preciso del midi. Los experimentos extendidos muestran que nuestros dibujos de videos permiten alcanzar un rendimiento superior en la generación y edición de videos con control.",
      "upvotes": 9,
      "discussionId": "67eb5286aeab4ce97de07320",
      "githubRepo": "https://github.com/IGLICT/SketchVideo",
      "ai_keywords": [
        "DiT video generation model",
        "memory-efficient control structure",
        "sketch control blocks",
        "residual features",
        "skipped DiT blocks",
        "temporally sparse sketch conditions",
        "inter-frame attention mechanism",
        "keyframes",
        "video insertion module",
        "spatial feature",
        "dynamic motion",
        "latent fusion",
        "SketchVideo"
      ]
    },
    "publishedAt": "2025-03-29T22:44:09.000Z",
    "title": "SketchVideo: Sketch-based Video Generation and Editing",
    "summary": "Video generation and editing conditioned on text prompts or images have\nundergone significant advancements. However, challenges remain in accurately\ncontrolling global layout and geometry details solely by texts, and supporting\nmotion control and local modification through images. In this paper, we aim to\nachieve sketch-based spatial and motion control for video generation and\nsupport fine-grained editing of real or synthetic videos. Based on the DiT\nvideo generation model, we propose a memory-efficient control structure with\nsketch control blocks that predict residual features of skipped DiT blocks.\nSketches are drawn on one or two keyframes (at arbitrary time points) for easy\ninteraction. To propagate such temporally sparse sketch conditions across all\nframes, we propose an inter-frame attention mechanism to analyze the\nrelationship between the keyframes and each video frame. For sketch-based video\nediting, we design an additional video insertion module that maintains\nconsistency between the newly edited content and the original video's spatial\nfeature and dynamic motion. During inference, we use latent fusion for the\naccurate preservation of unedited regions. Extensive experiments demonstrate\nthat our SketchVideo achieves superior performance in controllable video\ngeneration and editing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23284.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6424538b9f9e65b42389920e",
      "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
      "fullname": "Feng-Lin Liu",
      "name": "Okrin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18809",
      "authors": [
        {
          "_id": "67eaa0f83ace6eb46745a9fe",
          "user": {
            "_id": "674f43d6df6fa102409f6d1a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
            "isPro": false,
            "fullname": "Augusto B. Corrêa",
            "user": "abcorrea",
            "type": "user"
          },
          "name": "Augusto B. Corrêa",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:12.565Z",
          "hidden": false
        },
        {
          "_id": "67eaa0f83ace6eb46745a9ff",
          "user": {
            "_id": "662fb9c891587703a677856e",
            "avatarUrl": "/avatars/9cb7f035a513279532fc205ce9c5902c.svg",
            "isPro": false,
            "fullname": "Andre Grahl Pereira",
            "user": "andregrahl",
            "type": "user"
          },
          "name": "André G. Pereira",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:13.942Z",
          "hidden": false
        },
        {
          "_id": "67eaa0f83ace6eb46745aa00",
          "user": {
            "_id": "66f3dfd4b8703dde248f6d26",
            "avatarUrl": "/avatars/c199c91d422500cc7c7556569291644d.svg",
            "isPro": false,
            "fullname": "Jendrik Seipp",
            "user": "jendrikseipp",
            "type": "user"
          },
          "name": "Jendrik Seipp",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:39.862Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T15:50:20.000Z",
      "submittedOnDailyAt": "2025-04-01T00:45:14.321Z",
      "title": "Aplicar un generador de modelos de lenguaje llamado LLM con una heurística en un plan clásico: desafiando el estado de la arte con código Python",
      "submittedOnDailyBy": {
        "_id": "674f43d6df6fa102409f6d1a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
        "isPro": false,
        "fullname": "Augusto B. Corrêa",
        "user": "abcorrea",
        "type": "user"
      },
      "summary": "Recientemente, los modelos de lenguaje de gran tamaño (LLMs) han demostrado capacidades sorprendentes en diversas problemáticas de inteligencia artificial. Sin embargo, aún no se puede realizar planes confiables a pesar de presentar una definición detallada de los planes, y los esfuerzos para mejorar la capacidad de planificación de los LLMs, como por ejemplo, la prompting de la serie de pensamiento, el fine-tuning y la configuración explícita de 'razones', a menudo no logran generar planes correctos ni generalizarlos a tareas más grandes. En este artículo, se presenta un método para generar planes correctos en tareas de distribución externa de mayor tamaño utilizando LLMs. Se utilizan LLMs para crear funciones heurísticas en formato de código Python que dependen del dominio específico, se entrenan mediante búsqueda de mejores estrategias y se seleccionan las más potentes. Como resultado, las funciones heurísticas generadas por LLMs resuelven más tareas no vistas que las funciones heurísticas dominios independientes clásicas y muestran una fuerza comparable a las algoritmos de aprendizaje más fuertes dependientes del dominio. Estos hallazgos son sorprendentes, ya que las funciones heurísticas generadas por LLMs, aunque basadas en un implementación conceptual no optimizada de un programador de planes en Python, superan las funciones heurísticas avanzadas en tareas de dominio independiente, y su rendimiento es comparable a los algoritmos de aprendizaje más fuertes dependientes del dominio. En un dominio específico, las funciones heurísticas generadas por LLMs expanden menos estados que las funciones heurísticas basadas en un dominio independiente, y no solo expanden los estados de manera eficiente, sino que también muestran un nivel de información superior. En resumen, nuestros resultados demuestran que la generación de funciones heurísticas de planes a partir de LLMs puede significativamente mejorar la capacidad de planificación de estos modelos.",
      "upvotes": 8,
      "discussionId": "67eaa0f93ace6eb46745aa3e",
      "ai_keywords": [
        "large language models (LLMs)",
        "chain-of-thought prompting",
        "fine-tuning",
        "reasoning",
        "planning domain",
        "domain-dependent heuristic functions",
        "Python code",
        "greedy best-first search",
        "state-of-the-art domain-independent heuristics",
        "domain-dependent planning",
        "unoptimized Python planner",
        "highly optimized C++ code",
        "planning heuristic function programs"
      ]
    },
    "publishedAt": "2025-03-24T11:50:20.000Z",
    "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code",
    "summary": "In recent years, large language models (LLMs) have shown remarkable\ncapabilities in various artificial intelligence problems. However, they fail to\nplan reliably, even when prompted with a detailed definition of the planning\ntask. Attempts to improve their planning capabilities, such as chain-of-thought\nprompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans\nand usually fail to generalize to larger tasks. In this paper, we show how to\nuse LLMs to generate correct plans, even for out-of-distribution tasks of\nincreasing size. For a given planning domain, we ask an LLM to generate several\ndomain-dependent heuristic functions in the form of Python code, evaluate them\non a set of training tasks within a greedy best-first search, and choose the\nstrongest one. The resulting LLM-generated heuristics solve many more unseen\ntest tasks than state-of-the-art domain-independent heuristics for classical\nplanning. They are even competitive with the strongest learning algorithm for\ndomain-dependent planning. These findings are especially remarkable given that\nour proof-of-concept implementation is based on an unoptimized Python planner\nand the baselines all build upon highly optimized C++ code. In some domains,\nthe LLM-generated heuristics expand fewer states than the baselines, revealing\nthat they are not only efficiently computable, but sometimes even more\ninformative than the state-of-the-art heuristics. Overall, our results show\nthat sampling a set of planning heuristic function programs can significantly\nimprove the planning capabilities of LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18809.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674f43d6df6fa102409f6d1a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
      "fullname": "Augusto B. Corrêa",
      "name": "abcorrea",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24115",
      "authors": [
        {
          "_id": "67eb5116d3a707c0a5b02bd1",
          "user": {
            "_id": "64a0ed5ed5374ca472cfb0ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
            "isPro": false,
            "fullname": "ZhimingMa",
            "user": "JimmyMa99",
            "type": "user"
          },
          "name": "Zhiming Ma",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:36:07.612Z",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd2",
          "user": {
            "_id": "6385f7b969634850f8ddd541",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669723465271-noauth.png",
            "isPro": false,
            "fullname": "Peidong Wang",
            "user": "WDong",
            "type": "user"
          },
          "name": "Peidong Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:36:07.612Z",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd3",
          "user": {
            "_id": "6466d57bf3e78d1d6be0505c",
            "avatarUrl": "/avatars/9659b7d0f6fa51efc127afb7a1ba14b1.svg",
            "isPro": false,
            "fullname": "HuangMinhua",
            "user": "HuangMinhua",
            "type": "user"
          },
          "name": "Minhua Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:02:15.224Z",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd4",
          "name": "Jingpeng Wang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd5",
          "name": "Kai Wu",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd6",
          "name": "Xiangzhao Lv",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd7",
          "name": "Yachun Pang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd8",
          "name": "Yin Yang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd9",
          "name": "Wenjie Tang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bda",
          "name": "Yuchen Kang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/PsAJTW9JTyrHqtjQP0lLJ.png"
      ],
      "publishedAt": "2025-03-31T14:06:17.000Z",
      "submittedOnDailyAt": "2025-04-01T01:08:09.201Z",
      "title": "TeleAntiFraud-28k: Dataset de voz-texto para prevenir violaciones de llamadas",
      "submittedOnDailyBy": {
        "_id": "64a0ed5ed5374ca472cfb0ac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
        "isPro": false,
        "fullname": "ZhimingMa",
        "user": "JimmyMa99",
        "type": "user"
      },
      "summary": "El sentido de la fraude telefónica se encuentra en un gran problema debido a la escasez de datos de entrenamiento de alta calidad para modelos multimodelo. Para llenar este vacío, presentamos el primer conjunto de datos de audio-texto corto de tiempo abierto y adaptado para el análisis automático de fraudes telefónicas, llamado \"TeleAntiFraud-28k\". Este conjunto de datos ha sido construido siguiendo tres estrategias: 1) la generación de muestras de texto protegido de privacidad utilizando las grabaciones de voz telefónicas (incluyendo datos anónimos) con el reconocimiento automático de voz (ASR), asegurando la consistencia con el texto original mediante el modelo de texto de recreación; 2) la expansión del rango de escenarios a través de la auto-sampling basado en grandes modelos de lenguaje (LLM) con las salidas reales de ASR; 3) la simulación de las técnicas de fraude actuales utilizando un mix de agentes. El conjunto de datos generado incluye 28,511 pares de textos de habla estrictamente procesados, con notas detalladas sobre las razones de la fraude. Este conjunto de datos se divide en tres tareas: clasificación de escenarios, detección de fraude y clasificación de tipos de fraude. Además, hemos construido un marco de evaluación estándar llamado \"TeleAntiFraud-Bench\", que incluye instancias muestradas proporcionalmente, para promover la prueba sistemática del rendimiento de modelos en la tarea de detección de fraude telefónica. También proporcionamos modelos de entrenamiento optimizados para la industria basados en datos mixtos y abrimos el código de nuestro marco de procesamiento de datos para facilitar la expansión del conjunto de datos dirigida por la comunidad. Esta investigación tiene como objetivo abordar los problemas importantes de privacidad de datos y la diversidad de escenarios, al mismo tiempo que establece una base para la investigación multimodelo de anti-fraude. Este proyecto se lanza en GitHub en https://github.com/JimmyMa99/TeleAntiFraud.",
      "upvotes": 7,
      "discussionId": "67eb5117d3a707c0a5b02c4c",
      "ai_keywords": [
        "automatically speech recognition (ASR)",
        "text-to-speech (TTS)",
        "large language model (LLM)",
        "self-instruction sampling",
        "multi-agent adversarial synthesis",
        "supervised fine-tuning (SFT)",
        "hybrid real/synthetic data"
      ]
    },
    "publishedAt": "2025-03-31T10:06:17.000Z",
    "title": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
    "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/PsAJTW9JTyrHqtjQP0lLJ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24115.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a0ed5ed5374ca472cfb0ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
      "fullname": "ZhimingMa",
      "name": "JimmyMa99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23077",
      "authors": [
        {
          "_id": "67eb58c71e23a7499b683cce",
          "user": {
            "_id": "6650c77a74664a42ddfb9187",
            "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
            "isPro": false,
            "fullname": "yueliu1999",
            "user": "yueliu1999",
            "type": "user"
          },
          "name": "Yue Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:11:01.812Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683ccf",
          "name": "Jiaying Wu",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd0",
          "name": "Yufei He",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd1",
          "user": {
            "_id": "62728f4f6253fe2068da1021",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
            "isPro": false,
            "fullname": "Hongcheng Gao",
            "user": "HongchengGao",
            "type": "user"
          },
          "name": "Hongcheng Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:11:54.944Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd2",
          "user": {
            "_id": "67c7a9fb27c2e81cf8660375",
            "avatarUrl": "/avatars/a5129cca93a31d4b730af4c543051d8e.svg",
            "isPro": false,
            "fullname": "Hongyu Chen",
            "user": "HongyuChen",
            "type": "user"
          },
          "name": "Hongyu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:05.159Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd3",
          "user": {
            "_id": "642577e06d0f0f5f1dc68904",
            "avatarUrl": "/avatars/3df7cfcf9ca6cd9c9e9b10a73f4efc35.svg",
            "isPro": false,
            "fullname": "Bibaolong",
            "user": "Bibaolong",
            "type": "user"
          },
          "name": "Baolong Bi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:18.251Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd4",
          "user": {
            "_id": "669e19e5dac1eb34c0f5f505",
            "avatarUrl": "/avatars/bec7d1d1dac2ad6570844d1f00e7df0a.svg",
            "isPro": false,
            "fullname": "Jiaheng Zhang",
            "user": "jiaheng233",
            "type": "user"
          },
          "name": "Jiaheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:23.776Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd5",
          "user": {
            "_id": "66221f1a90f3fd333c4ec52e",
            "avatarUrl": "/avatars/a3173d9603a69020ec24170831c97c2f.svg",
            "isPro": false,
            "fullname": "Zhiqi Huang",
            "user": "Angelalilyer",
            "type": "user"
          },
          "name": "Zhiqi Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:36.398Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd6",
          "user": {
            "_id": "651d8032c50012d33e914f2f",
            "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
            "isPro": false,
            "fullname": "Bryan Hooi",
            "user": "bhooi",
            "type": "user"
          },
          "name": "Bryan Hooi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:42.880Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-29T13:27:46.000Z",
      "submittedOnDailyAt": "2025-04-01T01:39:12.154Z",
      "title": "Evaluación de grandes modelos de lógica que logran una eficiencia en la inferencia: resumen",
      "submittedOnDailyBy": {
        "_id": "6650c77a74664a42ddfb9187",
        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
        "isPro": false,
        "fullname": "yueliu1999",
        "user": "yueliu1999",
        "type": "user"
      },
      "summary": "Los modelos de Lógica de Grandes Tamaños (LRMs) han experimentado un claro aumento en sus capacidades lógicas y han demostrado un rendimiento esperado en tareas complejas. Sin embargo, presentan características desfavorables en el uso de tokens, la consumo de memoria y el tiempo de inferencia debido a procesos de cálculo lógicos. Por lo tanto, esta investigación se centra en revisar métodos eficientes de inferencia específicos para los LRMs, con el objetivo de mitigar las desventajas de los tokens mientras mantiene la calidad lógica. Primero, se presentan tecnologías para clasificar recientes métodos en dos categorías principales: (a) el Estropejamiento Cognitivo Explícito (CoT) reduce los tokens manteniendo una estructura lógica explícita. (b) el Estropejamiento Cognitivo Oculto (CoT) codifica los estados lógicos en representaciones ocultas. Se discuten sus ventajas y desventajas. Luego, se analizan experimentalmente el rendimiento y eficiencia de los métodos existentes. Se abordan también problemas abiertos en esta área. Se discuten temas como el control lógico centrado en el ser humano, la explicabilidad lógica y el equilibrio entre eficiencia y calidad lógica, la seguridad de la lógica eficiente, y la amplia aplicación de la lógica eficiente. Además, se destacan los principales elementos para mejorar la eficiencia de inferencia en LRMs, como la integración de modelos, nuevas arquitecturas y el uso de técnicas como el agente rotador. Este estudio desempeña un papel guíador para los investigadores en la resolución de problemas en esta área rica.",
      "upvotes": 7,
      "discussionId": "67eb58c81e23a7499b683d12",
      "ai_keywords": [
        "Chain-of-Thought (CoT)",
        "explicit compact Chain-of-Thought (CoT)",
        "implicit latent CoT",
        "model merging",
        "agent routers"
      ]
    },
    "publishedAt": "2025-03-29T09:27:46.000Z",
    "title": "Efficient Inference for Large Reasoning Models: A Survey",
    "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23077.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6650c77a74664a42ddfb9187",
      "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
      "fullname": "yueliu1999",
      "name": "yueliu1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24290",
      "authors": [
        {
          "_id": "67eb762381e530baa56dc830",
          "user": {
            "_id": "625026b7d2d191ac43320c5e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg",
            "isPro": false,
            "fullname": "Jingcheng Hu",
            "user": "reign12",
            "type": "user"
          },
          "name": "Jingcheng Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:09:02.123Z",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc831",
          "user": {
            "_id": "664ae39ab5e5f95dc6209365",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg",
            "isPro": false,
            "fullname": "Yinmin Zhang",
            "user": "YinminZhang",
            "type": "user"
          },
          "name": "Yinmin Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:09:09.884Z",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc832",
          "name": "Qi Han",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc833",
          "user": {
            "_id": "60d4440fe648443279aaffd8",
            "avatarUrl": "/avatars/bf7209c1f14ae120f5bfda5fda1301b7.svg",
            "isPro": false,
            "fullname": "Daxin Jiang",
            "user": "djiang",
            "type": "user"
          },
          "name": "Daxin Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:09:22.914Z",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc834",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc835",
          "name": "Heung-Yeung Shum",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T16:36:05.000Z",
      "submittedOnDailyAt": "2025-04-01T03:44:53.609Z",
      "title": "Open-Reasoner-Zero: Expansión de la aprendizaje por refuerzo basada en modelos básicos con un enfoque abierto-source",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Open-Radion-Zero introduce the first open-source implementation of large-scale theoretical RL training. This implementation is designed with scalability, simplicity, and accessibility as its core principles. Through scalability experiments, it demonstrates the ability to achieve scalable performance in response length and benchmark scores without including any KL normalization, using minimalistic approaches, Bayesian PPO (GAE(lambda=1, gamma=1)), and intuitive rule-based rewards. Utilizing foundational models like DeepSeek-R1-Zero, it achieves top performance on benchmarks such as AIME2024, MATH500, and GPQA Diamond, while requiring an efficient and sufficient training phase compared to the DeepSeek-R1-Zero pipeline. Based on an open-source spirit, it publicly releases the source code, parameter settings, training data, and model weights for each size.",
      "upvotes": 6,
      "discussionId": "67eb762481e530baa56dc872",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "vanilla PPO",
        "GAE ($\\lambda=1$, $\\gamma=1$)",
        "rule-based rewards",
        "KL regularization",
        "response length",
        "benchmark performance",
        "AIME2024",
        "MATH500",
        "GPQA Diamond benchmark",
        "training steps"
      ]
    },
    "publishedAt": "2025-03-31T12:36:05.000Z",
    "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model",
    "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of\nlarge-scale reasoning-oriented RL training focusing on scalability, simplicity\nand accessibility. Through extensive experiments, we demonstrate that a\nminimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and\nstraightforward rule-based rewards, without any KL regularization, is\nsufficient to scale up both response length and benchmark performance, similar\nto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as\nDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on\nAIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating\nremarkable efficiency -- requiring only a tenth of the training steps, compared\nto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our\nsource code, parameter settings, training data, and model weights across\nvarious sizes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24290.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6543
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23829",
      "authors": [
        {
          "_id": "67eb759cb9fa8908e1934f21",
          "name": "Yi Su",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f22",
          "user": {
            "_id": "62d58fd53bf5e059f7cc3245",
            "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
            "isPro": false,
            "fullname": "Dian Yu",
            "user": "yudian",
            "type": "user"
          },
          "name": "Dian Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:13:47.119Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f23",
          "user": {
            "_id": "64c94eddcb2f1bf0e7db5a4d",
            "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
            "isPro": false,
            "fullname": "Linfeng Song",
            "user": "freesunshine0316",
            "type": "user"
          },
          "name": "Linfeng Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:13:54.065Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f24",
          "user": {
            "_id": "6670e285b0c03c4e9d6e0985",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uCZHm4gKSHZ2b0hpHWgZv.jpeg",
            "isPro": false,
            "fullname": "Juntao Li",
            "user": "douvleplus",
            "type": "user"
          },
          "name": "Juntao Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:03.120Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f25",
          "user": {
            "_id": "65147a1426fbd558dbd08f1b",
            "avatarUrl": "/avatars/86574ee2d5c22e940be1c4e50be88675.svg",
            "isPro": false,
            "fullname": "Haitao Mi",
            "user": "haitaominlp",
            "type": "user"
          },
          "name": "Haitao Mi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:10.594Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f26",
          "user": {
            "_id": "67485743561b1e6f9579389f",
            "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
            "isPro": false,
            "fullname": "Zhaopeng Tu",
            "user": "zptu",
            "type": "user"
          },
          "name": "Zhaopeng Tu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:16.978Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f27",
          "name": "Min Zhang",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f28",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T08:22:49.000Z",
      "submittedOnDailyAt": "2025-04-01T03:42:19.595Z",
      "title": "Extensión de la RL con recompensas verificables en diversas áreas",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "La introducción de recompensas en aprendizaje por refuerzo (RL) es posible debido a razones matemáticas y porque muestra los resultados esperados en tareas de codificación con respuestas estructuradas. Sin embargo, la posibilidad de aplicación amplia de este método ha sido poco investigada. En este artículo, se estudia la extensión de RLVR en más diversas áreas como la medicina, química, psicología y economía. Cuando hay respuestas estructuradas, se observa que modelos de lenguaje de grandes escalas (LLMs) muestran alta concordancia en las dos evaluaciones, y se duda sobre la necesidad de grandes notas de dominio. Para resolver las limitaciones de las dos valoraciones en el tratamiento de respuestas no estructuradas, se agrega un score basado en modelos al RLVR y se mejora su flexibilidad. Los resultados de los experimentos muestran que los modelos generativos de recompensas no necesitan grandes notas de dominio y proporcionan señales de recompensa confiables para el aprendizaje por refuerzo. Optimizando modelos basales con otros algoritmos de RL y utilizando como ejemplo los mejores LLMs abiertos como Qwen2.5-72B-Instruct y DeepSeek-R1-Distill-Qwen-32B, se obtienen políticas que superan significativamente a los límites de estos modelos. Esto muestra excelente rendimiento en respuestas libres, fortalece la robustez y la escalabilidad del RLVR, y demuestra su utilidad incluso en aplicaciones reales que incluyen ruido y etiquetas débiles.",
      "upvotes": 5,
      "discussionId": "67eb759db9fa8908e1934f62",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "verifiable rewards (RLVR)",
        "mathematical reasoning",
        "coding tasks",
        "well-structured reference answers",
        "diverse domains",
        "medicine",
        "chemistry",
        "psychology",
        "economics",
        "large language models (LLMs)",
        "binary judgments",
        "domain-specific reward models",
        "model-based soft scoring",
        "distilled generative reward model",
        "effective cross-domain verifier",
        "reward signals",
        "fine-tuning",
        "base 7B model",
        "RL algorithms",
        "state-of-the-art open-source aligned LLMs",
        "Qwen2.5-72B-Instruct",
        "DeepSeek-R1-Distill-Qwen-32B",
        "free-form answer settings",
        "robustness",
        "scalability",
        "real-world applications",
        "noisy labels",
        "weak labels"
      ]
    },
    "publishedAt": "2025-03-31T04:22:49.000Z",
    "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
    "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement in binary judgments across different large\nlanguage models (LLMs) when objective reference answers exist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporate model-based soft scoring into RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nBy fine-tuning a base 7B model using various RL algorithms against our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23829.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6543
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21694",
      "authors": [
        {
          "_id": "67eb92defa85fe030e2db9e2",
          "user": {
            "_id": "64295d1f4e073875f6a605ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
            "isPro": true,
            "fullname": "Zhiyuan Ma",
            "user": "ZhiyuanthePony",
            "type": "user"
          },
          "name": "Zhiyuan Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:51.486Z",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e3",
          "user": {
            "_id": "672111333ced358bdac2925d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qHNDI3zYRkJZmhCHkSZwK.png",
            "isPro": false,
            "fullname": "Xinyue Liang",
            "user": "DarklordLeto",
            "type": "user"
          },
          "name": "Xinyue Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:58.992Z",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e4",
          "name": "Rongyuan Wu",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e5",
          "name": "Xiangyu Zhu",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e6",
          "name": "Zhen Lei",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e7",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64295d1f4e073875f6a605ac/F5lBcbDKq1_K31O-vxWBg.mp4"
      ],
      "publishedAt": "2025-03-27T16:59:15.000Z",
      "submittedOnDailyAt": "2025-04-01T06:05:21.846Z",
      "title": "Renderización avanzada de extracción: Tecnología de aprendizaje profundo para la generación de mashas 3D a partir de texto instintivo sin datos 3D, adaptada para la creación de mashas 3D.",
      "submittedOnDailyBy": {
        "_id": "64295d1f4e073875f6a605ac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
        "isPro": true,
        "fullname": "Zhiyuan Ma",
        "user": "ZhiyuanthePony",
        "type": "user"
      },
      "summary": "Tener altas expectativas es obtener un modelo que genere altas calidades de 3D meshes a través de puntos de corte en un texto plot. Los intentos recientes, como por ejemplo, aplicar modelos que expanden texto previamente entrenado a imágenes (como Stable Diffusion, SD) para generar modelos de representación 3D (como Triplane), pero su calidad puede disminuir debido a la escasez de datos de entrenamiento de alta calidad. Para superar esta limitación, proponemos un nuevo esquema de entrenamiento, llamado Progressive Rendering Distillation (PRD). Este método elimina la necesidad de datos reales 3D y entrena experiencialmente modelos de múltiples puntos de vista para aplicarlos a modelos de archivos 3D de SD. En cada etapa de entrenamiento, el PRD utiliza un U-Net para desnoiser paso a paso ruido aleatorio y interpreta los retenemos desde cada etapa en la salida 3D. Modelos de múltiples puntos de vista, como MVDream y RichDreamer, junto con SD, se centran en texturas y generalidad que coinciden con el texto a través de distillación de puntos de corte. El PRD facilita la expansión de datos de entrenamiento y la mejora de la calidad de generación para textos complejos, al eliminar la necesidad de datos reales 3D. Además, el PRD puede acelerar la velocidad de inferencia del modelo de generación en varias etapas. Usando el PRD, entrenamos el modelo Triplane y lo nombramos TriplaneTurbo. TriplaneTurbo aumenta los parámetros entrenables solo en un 2.5% para aplicar a la generación 3D de SD, sin embargo, es más eficiente y de mejor calidad que modelos anteriores de generación de 3D a partir de texto. En particular, TriplaneTurbo puede generar una mejora de calidad de 3D mesh en un segundo y se adapta bien a entradas de texto complejas. El código está disponible en https://github.com/theEricMa/TriplaneTurbo.",
      "upvotes": 5,
      "discussionId": "67eb92e2fa85fe030e2dbc04",
      "projectPage": "https://theericma.github.io/TriplaneTurbo/",
      "githubRepo": "https://github.com/theEricMa/TriplaneTurbo",
      "ai_keywords": [
        "diffusion models",
        "Stable Diffusion (SD)",
        "3D representations",
        "Progressive Rendering Distillation (PRD)",
        "U-Net",
        "latent from random noise",
        "denoise the latent",
        "3D output",
        "Multi-view diffusion models",
        "MVDream",
        "RichDreamer",
        "score distillation",
        "text-consistent textures",
        "geometries",
        "Triplane generator",
        "TriplaneTurbo",
        "high-quality 3D meshes"
      ]
    },
    "publishedAt": "2025-03-27T12:59:15.000Z",
    "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data",
    "summary": "It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only 2.5%\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64295d1f4e073875f6a605ac/F5lBcbDKq1_K31O-vxWBg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64295d1f4e073875f6a605ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
      "fullname": "Zhiyuan Ma",
      "name": "ZhiyuanthePony",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.19901",
      "authors": [
        {
          "_id": "67eac6433755a17e3cbff585",
          "user": {
            "_id": "6630cc7e9ee8861dd0b9bdbd",
            "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
            "isPro": false,
            "fullname": "Liang Pan",
            "user": "lianganimation",
            "type": "user"
          },
          "name": "Liang Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:34.754Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff586",
          "user": {
            "_id": "649c178950b4be74229d680f",
            "avatarUrl": "/avatars/941dec90fdfc46b9ae23378e3a3113f4.svg",
            "isPro": false,
            "fullname": "Zeshi Yang",
            "user": "Zeshi209",
            "type": "user"
          },
          "name": "Zeshi Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:03.851Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff587",
          "user": {
            "_id": "645223fb01d7bd9555ea399a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
            "isPro": false,
            "fullname": "Zhiyang Dou",
            "user": "frankzydou",
            "type": "user"
          },
          "name": "Zhiyang Dou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:09.328Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff588",
          "user": {
            "_id": "6437a813fac5ea753f1c72d2",
            "avatarUrl": "/avatars/69e60e60497e404149a1dad46649dad4.svg",
            "isPro": false,
            "fullname": "wenjia Wang",
            "user": "WenjiaWang",
            "type": "user"
          },
          "name": "Wenjia Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:22.428Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff589",
          "name": "Buzhen Huang",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff58a",
          "user": {
            "_id": "635f93577c05eb9f59966209",
            "avatarUrl": "/avatars/add48d7e3790a6b500d6c451ef8b0f75.svg",
            "isPro": false,
            "fullname": "Intelligent Digital Creation",
            "user": "BoDai",
            "type": "user"
          },
          "name": "Bo Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:59.939Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff58b",
          "name": "Taku Komura",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff58c",
          "user": {
            "_id": "669cb638666901f41dae51bf",
            "avatarUrl": "/avatars/a7cc19e3db84bd86bee3eb6fd4897959.svg",
            "isPro": false,
            "fullname": "Jingbo Wang",
            "user": "jingbocuhk",
            "type": "user"
          },
          "name": "Jingbo Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:43.455Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6630cc7e9ee8861dd0b9bdbd/2bErZDgOyv5xeo1wDfMz3.mp4"
      ],
      "publishedAt": "2025-03-25T17:57:46.000Z",
      "submittedOnDailyAt": "2025-04-01T06:24:17.315Z",
      "title": "TokenHSI: Integración de la Interacción Física Homo-Escena a través de un Síntesis Complementario\n  Tokenización de Tareas",
      "submittedOnDailyBy": {
        "_id": "6630cc7e9ee8861dd0b9bdbd",
        "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
        "isPro": false,
        "fullname": "Liang Pan",
        "user": "lianganimation",
        "type": "user"
      },
      "summary": "La síntesis de interacciones físicamente posibles entre humanos y escenas (HSI) es crucial tanto para la animación computacional como para la visión artificial. Mientras se desarrolla, los métodos actuales se centran principalmente en la creación de controladores especializados para tareas de interacción, lo que limita significativamente su capacidad para enfrentar diversas tareas de HSI complejas. Para resolver estos problemas, presentamos TokenHSI. TokenHSI utiliza una única política integrada basada en Transformer, que integra diversas tecnologías y permite una adaptación flexible. El punto clave es modelar la reconocimiento de familias humanas individualmente usando tokens compartidos y combinarlos con tokenes de tareas específicas a través de una estructura de mascara. Esta política integrada facilita el intercambio de conocimientos entre las tecnologías y promueve la entrenamiento de múltiples tareas. Además, nuestra arquitectura de política soporta entradas variable e impulsa la adaptabilidad de las tecnologías entrenadas. Además, se pueden entrenar más tokenes de tareas para cambiar la geometría de los objetivos de interacción o resolver tareas complejas mediante la colaboración de múltiples tecnologías. Nuestros experimentos muestran que nuestro enfoque mejora significativamente la diversidad, la adaptabilidad y la extensibilidad. Página web: https://liangpan99.github.io/TokenHSI/",
      "upvotes": 5,
      "discussionId": "67eac6443755a17e3cbff5cf",
      "projectPage": "https://liangpan99.github.io/TokenHSI/",
      "githubRepo": "https://github.com/liangpan99/TokenHSI",
      "ai_keywords": [
        "transformer-based policy",
        "proprioception",
        "shared token",
        "task tokens",
        "masking mechanism",
        "multi-task training",
        "variable length inputs",
        "task tokenizers"
      ]
    },
    "publishedAt": "2025-03-25T13:57:46.000Z",
    "title": "TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through\n  Task Tokenization",
    "summary": "Synthesizing diverse and physically plausible Human-Scene Interactions (HSI)\nis pivotal for both computer animation and embodied AI. Despite encouraging\nprogress, current methods mainly focus on developing separate controllers, each\nspecialized for a specific interaction task. This significantly hinders the\nability to tackle a wide variety of challenging HSI tasks that require the\nintegration of multiple skills, e.g., sitting down while carrying an object. To\naddress this issue, we present TokenHSI, a single, unified transformer-based\npolicy capable of multi-skill unification and flexible adaptation. The key\ninsight is to model the humanoid proprioception as a separate shared token and\ncombine it with distinct task tokens via a masking mechanism. Such a unified\npolicy enables effective knowledge sharing across skills, thereby facilitating\nthe multi-task training. Moreover, our policy architecture supports variable\nlength inputs, enabling flexible adaptation of learned skills to new scenarios.\nBy training additional task tokenizers, we can not only modify the geometries\nof interaction targets but also coordinate multiple skills to address complex\ntasks. The experiments demonstrate that our approach can significantly improve\nversatility, adaptability, and extensibility in various HSI tasks. Website:\nhttps://liangpan99.github.io/TokenHSI/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6630cc7e9ee8861dd0b9bdbd/2bErZDgOyv5xeo1wDfMz3.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19901.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6630cc7e9ee8861dd0b9bdbd",
      "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
      "fullname": "Liang Pan",
      "name": "lianganimation",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14941",
      "authors": [
        {
          "_id": "67eb932522a341478ae86cb6",
          "user": {
            "_id": "67a99d1fef1439e285c4cbec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
            "isPro": false,
            "fullname": "Qihui Zhang",
            "user": "77Hui",
            "type": "user"
          },
          "name": "Qihui Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:00.373Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb7",
          "user": {
            "_id": "65e14c28b1a6de8a71e70172",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e14c28b1a6de8a71e70172/D097SILGsqoufpp3sG8tV.jpeg",
            "isPro": false,
            "fullname": "Munan Ning",
            "user": "MunanNing",
            "type": "user"
          },
          "name": "Munan Ning",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:18:39.336Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb8",
          "name": "Zheyuan Liu",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb9",
          "name": "Yanbo Wang",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cba",
          "name": "Jiayi Ye",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbb",
          "user": {
            "_id": "6637443ecd9097ac3c996d3c",
            "avatarUrl": "/avatars/d1c38bf03c2517ba0a7004b2f9f9bc96.svg",
            "isPro": false,
            "fullname": "yue",
            "user": "yuehuang",
            "type": "user"
          },
          "name": "Yue Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:19:26.627Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbc",
          "name": "Shuo Yang",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbd",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbe",
          "user": {
            "_id": "62c51800cb7033fd49b8efb7",
            "avatarUrl": "/avatars/06c2be0015f8022f9912f2279f2b3597.svg",
            "isPro": false,
            "fullname": "Song",
            "user": "Yibing",
            "type": "user"
          },
          "name": "Yibing Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:19:07.616Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbf",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T07:15:41.000Z",
      "submittedOnDailyAt": "2025-04-01T05:48:16.581Z",
      "title": "UPME: Framework para Evaluación de Modelos de Lenguaje Multilingüe con Pair Reasoning y Sin Verificación",
      "submittedOnDailyBy": {
        "_id": "67a99d1fef1439e285c4cbec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
        "isPro": false,
        "fullname": "Qihui Zhang",
        "user": "77Hui",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje multilingüe multimodal (MLLMs) han surgido para abordar problemas de respuesta a preguntas visuales (VQA), y han llevado a un nuevo enfoque de investigación sobre la evaluación subjetiva de estos modelos. Los métodos de evaluación actuales están limitados por una gran carga de trabajo asociada con la diseño de pares de preguntas y respuestas para las imágenes, lo que reduce la escala y el rango de la evaluación. El enfoque automatizado de MLLM como jurado trata de reducir esta carga de trabajo, pero enfrenta dificultades al ser sesgado. Para resolver estos problemas, proponemos un marco de evaluación de MLLM basado en evaluación de pares sin supervisión. Este marco utiliza solo datos de imágenes, donde el modelo genera automáticamente las preguntas y evalúa las respuestas de otros modelos. De esta manera, se elimina la dependencia de la carga de trabajo. Además, presentamos un sistema de puntuación de visión lenguaje (Vision Language Score System) en tres aspectos: comprensión visual y lógica, relación entre imágenes y preguntas, y precisión de la respuesta, para mitigar los sesgos. Los resultados de los experimentos muestran que UPME obtiene una correlación de Pearson de 0.944 en el conjunto de datos MMstar y 0.814 en ScienceQA, que coinciden con la evaluación humana, demostrando que nuestro marco de trabajo muestra una similitud extremadamente alta con los benchmarks humanos y los preferencias humanos.",
      "upvotes": 3,
      "discussionId": "67eb932622a341478ae86d15",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Visual Question Answering (VQA)",
        "Q&A pairs",
        "MLLM-as-judge",
        "Unsupervised Peer review MLLM Evaluation (UPME)",
        "vision-language scoring system",
        "response correctness",
        "visual understanding and reasoning",
        "image-text correlation",
        "Pearson correlation",
        "MMstar dataset",
        "ScienceQA dataset"
      ]
    },
    "publishedAt": "2025-03-19T03:15:41.000Z",
    "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation",
    "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a99d1fef1439e285c4cbec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
      "fullname": "Qihui Zhang",
      "name": "77Hui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24391",
      "authors": [
        {
          "_id": "67eb72a2291b56e50b66a063",
          "user": {
            "_id": "66606a13fc6c0816442bd161",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
            "isPro": false,
            "fullname": "Xingyu Chen",
            "user": "rover-xingyu",
            "type": "user"
          },
          "name": "Xingyu Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:33.467Z",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a064",
          "user": {
            "_id": "66f80281d88dc2ad510663e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5HV3mTu-cxPCnWlCi_2wB.jpeg",
            "isPro": false,
            "fullname": "Yue Chen",
            "user": "faneggg",
            "type": "user"
          },
          "name": "Yue Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:31.158Z",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a065",
          "name": "Yuliang Xiu",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a066",
          "name": "Andreas Geiger",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a067",
          "name": "Anpei Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:59:58.000Z",
      "submittedOnDailyAt": "2025-04-01T07:01:19.970Z",
      "title": "Easi3R: Estimación de movimientos separados del DUSt3R sin necesidad de entrenamiento",
      "submittedOnDailyBy": {
        "_id": "66606a13fc6c0816442bd161",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
        "isPro": false,
        "fullname": "Xingyu Chen",
        "user": "rover-xingyu",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de DUSt3R ha permitido una estimación fuerte de puntos y parámetros de cámara en escenas estáticas de alta densidad. Esto se ha logrado mediante la estructura de red de Transformer y la normalización directa de conjuntos de datos 3D. Por otro lado, la escasez y la diversidad limitada de conjuntos de datos 4D actúan como grandes limitantes para alcanzar un alto rendimiento de generalización en modelos 4D. Esta restricción impide aplicar métodos existentes 4D (como, por ejemplo, ajustes de modelos 3D a datos de video dinámico escalable, flujo óptico, profundidad y otras informaciones geométricas adicionales). En este estudio, hemos tomado la ruta opuesta y introdujimos Easi3R. Easi3R es un método sencillo y eficiente para la reconstrucción 4D sin entrenamiento. Nuestro enfoque aplica un atributo de atención en la inferencia, sin necesidad de reiniciar desde las marcas de escurrimiento o ajustar la red. Hemos descubierto que las capas de atención de DUSt3R incluyen información rica sobre el movimiento de la cámara y los objetos. Al separar cuidadosamente estas mapas de atención, hemos logrado la segmentación dinámica de áreas, la estimación de la postura de la cámara y la reconstrucción de puntos de alta densidad en 4D. Los experimentos extendidos en videos dinámicos reales han demostrado que nuestro ligero atributo de atención supera significativamente los métodos más avanzados previos, incluso entrenados. Nuestro código está disponible en https://easi3r.github.io/ para la investigación.",
      "upvotes": 2,
      "discussionId": "67eb72a5291b56e50b66a152",
      "ai_keywords": [
        "Transformer network architectures",
        "dense point clouds",
        "camera parameters",
        "direct supervision",
        "3D datasets",
        "4D datasets",
        "4D model",
        "fine-tune",
        "dynamic video data",
        "geometric priors",
        "optical flow",
        "depths",
        "training-free method",
        "4D reconstruction",
        "attention adaptation",
        "inference",
        "attention layers",
        "dynamic region segmentation",
        "camera pose estimation",
        "4D dense point map reconstruction"
      ]
    },
    "publishedAt": "2025-03-31T13:59:58.000Z",
    "title": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training",
    "summary": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66606a13fc6c0816442bd161",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
      "fullname": "Xingyu Chen",
      "name": "rover-xingyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23730",
      "authors": [
        {
          "_id": "67eb567141abf40cd86e0e15",
          "user": {
            "_id": "67038a66eb760972bcb62c70",
            "avatarUrl": "/avatars/8cc82af8f11cae994bc83f4bd99b51bc.svg",
            "isPro": false,
            "fullname": "김윤식",
            "user": "yoonshik1205",
            "type": "user"
          },
          "name": "Yoonshik Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:53.729Z",
          "hidden": false
        },
        {
          "_id": "67eb567141abf40cd86e0e16",
          "user": {
            "_id": "646484cfb90150b2706df03b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
            "isPro": true,
            "fullname": "Jaeyoon Jung",
            "user": "lastdefiance20",
            "type": "user"
          },
          "name": "Jaeyoon Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:56.151Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T05:04:25.000Z",
      "submittedOnDailyAt": "2025-04-01T01:43:40.701Z",
      "title": "KOFFVQA: Benchmark de VQA de forma libre evaluado subjetivamente en el idioma coreano",
      "submittedOnDailyBy": {
        "_id": "646484cfb90150b2706df03b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
        "isPro": true,
        "fullname": "Jaeyoon Jung",
        "user": "lastdefiance20",
        "type": "user"
      },
      "summary": "Recientemente, se han surgido marcos de evaluación para los Modelos de Visión y Lenguaje Grandes (VLMs). Sin embargo, estos métodos de evaluación pueden ser subjetivos y no fiables cuando los modelos deben seleccionar respuestas predefinidas o cuando se utiliza un modelo de clasificación para evaluar. Además, los marcos de evaluación para VLMs en el idioma coreano necesitan un criterio independiente del inglés, pero no existen estos. Por lo tanto, puede haber grandes diferencias en el rendimiento de los modelos según el idioma. Por ello, se propone KOFFVQA, un marco de evaluación general para preguntas visuales en formato libre en coreano. Este marco incluye 275 problemas diseñados detalladamente, cada uno conectado a una imagen, y evalúa 10 diferentes aspectos de la performance de los VLMs. Los criterios de evaluación abordan la falta de fiabilidad mediante la evaluación de cada respuesta según reglas predefinidas por un modelo de clasificación. Al definir los criterios objetivamente, se permite que pequeños modelos abierto-código puedan ser evaluados confiablemente en este marco. Además, se ha confirmado experimentalmente que evaluar la mayoría de los VLMs existentes con este método resulta mucho más fiable que los métodos actuales. El código de evaluación está disponible en https://github.com/maum-ai/KOFFVQA.",
      "upvotes": 2,
      "discussionId": "67eb567341abf40cd86e0e63",
      "githubRepo": "https://github.com/maum-ai/KOFFVQA",
      "ai_keywords": [
        "Large Vision-Language Models (VLMs)",
        "visual question answering benchmark",
        "grading criteria"
      ]
    },
    "publishedAt": "2025-03-31T01:04:25.000Z",
    "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language",
    "summary": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23730.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646484cfb90150b2706df03b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
      "fullname": "Jaeyoon Jung",
      "name": "lastdefiance20",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23022",
      "authors": [
        {
          "_id": "67ebadecf9f9390b4cd1c6d9",
          "name": "Xianglong He",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6da",
          "name": "Junyi Chen",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6db",
          "name": "Di Huang",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6dc",
          "name": "Zexiang Liu",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6dd",
          "name": "Xiaoshui Huang",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6de",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6df",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6e0",
          "name": "Yangguang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-29T09:21:50.000Z",
      "submittedOnDailyAt": "2025-04-01T07:42:31.137Z",
      "title": "MeshCraft: Investigación sobre la generación eficiente y controlable de mallas utilizando DiTs flexibles",
      "submittedOnDailyBy": {
        "_id": "64d71083a787c9bc7b9f1238",
        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
        "isPro": false,
        "fullname": "Yangguang Li",
        "user": "Lp256",
        "type": "user"
      },
      "summary": "En el campo de la producción de contenido 3D, alcanzar la mejor topología de malla con modelos de IA ha sido un objetivo perseguido por los artistas 3D durante muchos años. Los métodos pasados intentaron crear objetos 3D preparados utilizando aprendizaje automático como el GPT de malla. Estos métodos lograron resultados visualmente impresionantes, pero dependían de predicciones por bloques de token en el proceso de recuperación automática, lo que implicaba una velocidad de generación muy lenta y limitaciones en el número de superficies de malla. En este artículo, se presenta un nuevo marco para la generación eficiente y controlable de mallas utilizando difusión continuo para crear superficies de triángulos continuas. En particular, Mesh Crafter está constituido por dos componentes clave: 1) El generador basado en transformers de VAE transforma estructuras hipernómicas en tokens de nivel de superficies continuas y los devuelve a la malla original. 2) La transformación de difusión basada en el número de superficies permite generar mallas de alta calidad con un número de superficies específico. Mesh Crafter utiliza modelos de difusión para generar la topología de la malla de la malla entera, logrando una generación de mallas de alta calidad mucho más rápida que los modelos de recuperación automática. En particular, Mesh Crafter puede generar una malla de 800 superficies en 3.2 segundos (35 veces más rápido que los métodos actuales). Los experimentos distribuidos en el conjunto de datos ShapeNet demostraron superar las técnicas más avanzadas a través de evaluaciones cualitativas y cuantitativas, y también mostraron excelentes resultados en el conjunto de datos Objaverse. Además, se puede integrar con estrategias de guía condicional y sin intervalos, liberando a los artistas de las largas tareas manuales que se consumeban en la generación de mallas.",
      "upvotes": 1,
      "discussionId": "67ebadeef9f9390b4cd1c7b3",
      "ai_keywords": [
        "mesh auto-regressive techniques",
        "continuous spatial diffusion",
        "transformer-based VAE",
        "flow-based diffusion transformer",
        "high-fidelity mesh generation",
        "ShapeNet dataset",
        "Objaverse dataset",
        "conditional guidance"
      ]
    },
    "publishedAt": "2025-03-29T05:21:50.000Z",
    "title": "MeshCraft: Exploring Efficient and Controllable Mesh Generation with\n  Flow-based DiTs",
    "summary": "In the domain of 3D content creation, achieving optimal mesh topology through\nAI models has long been a pursuit for 3D artists. Previous methods, such as\nMeshGPT, have explored the generation of ready-to-use 3D objects via mesh\nauto-regressive techniques. While these methods produce visually impressive\nresults, their reliance on token-by-token predictions in the auto-regressive\nprocess leads to several significant limitations. These include extremely slow\ngeneration speeds and an uncontrollable number of mesh faces. In this paper, we\nintroduce MeshCraft, a novel framework for efficient and controllable mesh\ngeneration, which leverages continuous spatial diffusion to generate discrete\ntriangle faces. Specifically, MeshCraft consists of two core components: 1) a\ntransformer-based VAE that encodes raw meshes into continuous face-level tokens\nand decodes them back to the original meshes, and 2) a flow-based diffusion\ntransformer conditioned on the number of faces, enabling the generation of\nhigh-quality 3D meshes with a predefined number of faces. By utilizing the\ndiffusion model for the simultaneous generation of the entire mesh topology,\nMeshCraft achieves high-fidelity mesh generation at significantly faster speeds\ncompared to auto-regressive methods. Specifically, MeshCraft can generate an\n800-face mesh in just 3.2 seconds (35times faster than existing baselines).\nExtensive experiments demonstrate that MeshCraft outperforms state-of-the-art\ntechniques in both qualitative and quantitative evaluations on ShapeNet dataset\nand demonstrates superior performance on Objaverse dataset. Moreover, it\nintegrates seamlessly with existing conditional guidance strategies, showcasing\nits potential to relieve artists from the time-consuming manual work involved\nin mesh creation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d71083a787c9bc7b9f1238",
      "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
      "fullname": "Yangguang Li",
      "name": "Lp256",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20286",
      "authors": [
        {
          "_id": "67eaa88c40bebc3127ade04c",
          "user": {
            "_id": "67e77099284080c98d8c9bfc",
            "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
            "isPro": false,
            "fullname": "Zhenyu Liang",
            "user": "ZhenyuLiang",
            "type": "user"
          },
          "name": "Zhenyu Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:16.646Z",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04d",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04e",
          "name": "Naiwei Yu",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04f",
          "name": "Kebin Sun",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade050",
          "name": "Ran Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T07:30:23.000Z",
      "submittedOnDailyAt": "2025-04-01T00:34:02.763Z",
      "title": "Evolución de la optimización multiobjetivo y aceleración con GPU utilizando TensorRT",
      "submittedOnDailyBy": {
        "_id": "67e77099284080c98d8c9bfc",
        "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
        "isPro": false,
        "fullname": "Zhenyu Liang",
        "user": "ZhenyuLiang",
        "type": "user"
      },
      "summary": "La evolución del algoritmo de optimización multiobjetivo (EMO) ha experimentado un desarrollo claro durante los últimos 20 años. Sin embargo, con el aumento de la granja y la complejidad de los problemas, los algoritmos tradicionales de EMO han mostrado limitaciones en su capacidad para escalar y paralelizar. Muchas investigaciones se han centrado en diseñar algoritmos que resuelvan estos problemas, pero ha habido una falta de atención a la arquitectura de hardware y un lago claro entre los algoritmos de EMO y dispositivos de cálculo avanzados como los GPU. Para cerrar esta brecha, proponemos la paralelización de algoritmos de EMO en GPU. Utilizando el método de tensores, convertimos la estructura de datos y las operaciones de los algoritmos de EMO en una representación tensorial clara, permitiendo el uso automático de cálculos en GPU. Demostramos el efecto de nuestro método aplicandolo a tres algoritmos de EMO representativos: NSGA-III, MOEA/D y HypE. Evaluamos nuestro método de manera global introduciendo un benchmark de control de robotes multiobjetivos utilizando un enfriador físico acelerado en GPU. Los resultados de los experimentos muestran que los algoritmos de EMO tensorizados logran un aumento de velocidad de 1113 veces en comparación con computadoras basadas en CPU, manteniendo la calidad de las soluciones mientras se puede ampliar la población de soluciones simplemente a través de la cantidad. Además, los algoritmos tensorizados de EMO resuelven eficientemente tareas de control de robotes multiobjetivos complejas, generando soluciones de alta calidad con una variedad de acciones. El código fuente está disponible en https://github.com/EMI-Group/evomo.",
      "upvotes": 1,
      "discussionId": "67eaa88e40bebc3127ade0eb",
      "githubRepo": "https://github.com/EMI-Group/evomo",
      "ai_keywords": [
        "evolutionary multiobjective optimization (EMO)",
        "parallelism",
        "scalability",
        "GPU",
        "tensorization",
        "tensor representations",
        "NSGA-III",
        "MOEA/D",
        "HypE",
        "GPU-accelerated physics engine",
        "multiobjective robot control benchmark",
        "population sizes",
        "high-quality solutions",
        "diverse behaviors"
      ]
    },
    "publishedAt": "2025-03-26T03:30:23.000Z",
    "title": "Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization",
    "summary": "Evolutionary multiobjective optimization (EMO) has made significant strides\nover the past two decades. However, as problem scales and complexities\nincrease, traditional EMO algorithms face substantial performance limitations\ndue to insufficient parallelism and scalability. While most work has focused on\nalgorithm design to address these challenges, little attention has been given\nto hardware acceleration, thereby leaving a clear gap between EMO algorithms\nand advanced computing devices, such as GPUs. To bridge the gap, we propose to\nparallelize EMO algorithms on GPUs via the tensorization methodology. By\nemploying tensorization, the data structures and operations of EMO algorithms\nare transformed into concise tensor representations, which seamlessly enables\nautomatic utilization of GPU computing. We demonstrate the effectiveness of our\napproach by applying it to three representative EMO algorithms: NSGA-III,\nMOEA/D, and HypE. To comprehensively assess our methodology, we introduce a\nmultiobjective robot control benchmark using a GPU-accelerated physics engine.\nOur experiments show that the tensorized EMO algorithms achieve speedups of up\nto 1113x compared to their CPU-based counterparts, while maintaining solution\nquality and effectively scaling population sizes to hundreds of thousands.\nFurthermore, the tensorized EMO algorithms efficiently tackle complex\nmultiobjective robot control tasks, producing high-quality solutions with\ndiverse behaviors. Source codes are available at\nhttps://github.com/EMI-Group/evomo.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20286.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67e77099284080c98d8c9bfc",
      "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
      "fullname": "Zhenyu Liang",
      "name": "ZhenyuLiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18225",
      "authors": [
        {
          "_id": "67ebabe3c545cab686735182",
          "name": "Massimo Bini",
          "hidden": false
        },
        {
          "_id": "67ebabe3c545cab686735183",
          "name": "Leander Girrbach",
          "hidden": false
        },
        {
          "_id": "67ebabe3c545cab686735184",
          "name": "Zeynep Akata",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T22:00:56.000Z",
      "submittedOnDailyAt": "2025-04-01T07:38:06.739Z",
      "title": "Reajuste de prioridades: independencia de ángulo y intensidad",
      "submittedOnDailyBy": {
        "_id": "63f62ee3b29015adc33aafa0",
        "avatarUrl": "/avatars/b7d7fe1b65333fb698402ed065fc5d13.svg",
        "isPro": false,
        "fullname": "Massimo Bini",
        "user": "mwbini",
        "type": "user"
      },
      "summary": "El método de ajuste parámetro eficiente (Parameter-Efficient FineTuning, PEFT) ha ganado popularidad recientemente debido a la expansión de los modelos de aprendizaje previo a gran escala. Estos métodos permiten adaptarse rápidamente a tareas de descarga en línea con un mínimo de costos computacionales. Sin embargo, métodos como LoRA, que son conocidos, tienen una robustez limitada ante la selección de hiperparámetros o planificaciones de aprendizaje a largo plazo, y no pueden demostrar una mejora en la performance de producto. En contraste, enfoques restringidos como ETHER pueden mejorar la robustez, pero están limitados a adaptaciones de muy bajo rango y transformaciones con flexibilidad fija, lo que reduce su capacidad de adaptación. En este artículo, se propone un nuevo método de ajuste, DeLoRA (Decoupled Low-rank Adaptation), que normaliza y escala matrices de bajo rango adaptables. DeLoRA limita la distancia de las transformaciones para separar el aprendizaje de los ángulos del rendimiento de la adaptación, mejorando la robustez sin aumentar los costos. Evalúado a través de generación de imágenes dirigidas por temas, comprensión de lenguaje natural e instrucciones de tuning, DeLoRA demostró ser tanto eficiente como otros métodos de PEFT, y en algunos casos, mejor, mostrando una robustez elevada. El código está disponible en: https://github.com/ExplainableML/DeLoRA.",
      "upvotes": 1,
      "discussionId": "67ebabe5c545cab68673521b",
      "githubRepo": "https://github.com/ExplainableML/DeLoRA",
      "ai_keywords": [
        "Parameter-Efficient FineTuning (PEFT)",
        "LoRA",
        "bounded approaches",
        "ETHER",
        "Decoupled Low-rank Adaptation (DeLoRA)",
        "learnable low-rank matrices",
        "angular learning",
        "adaptation strength",
        "subject-driven image generation",
        "natural language understanding",
        "instruction tuning"
      ]
    },
    "publishedAt": "2025-03-23T18:00:56.000Z",
    "title": "Decoupling Angles and Strength in Low-rank Adaptation",
    "summary": "Parameter-Efficient FineTuning (PEFT) methods have recently gained\nsignificant popularity thanks to the widespread availability of large-scale\npretrained models. These methods allow for quick adaptation to downstream tasks\nwith minimal computational cost. However, popular finetuning methods such as\nLoRA exhibit limited robustness when it comes to hyperparameter choices or\nextended training regimes, preventing optimal out-of-the-box performance. In\ncontrast, bounded approaches, such as ETHER, provide greater robustness but are\nlimited to extremely low-rank adaptations and fixed-strength transformations,\nreducing their adaptation expressive power. In this work, we propose Decoupled\nLow-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and\nscales learnable low-rank matrices. By bounding the distance of the\ntransformation, DeLoRA effectively decouples the angular learning from the\nadaptation strength, enhancing robustness without compromising performance.\nThrough evaluations on subject-driven image generation, natural language\nunderstanding, and instruction tuning, we show that DeLoRA matches or surpasses\nperformance of competing PEFT methods, while exhibiting stronger robustness.\nCode is available at https://github.com/ExplainableML/DeLoRA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f62ee3b29015adc33aafa0",
      "avatarUrl": "/avatars/b7d7fe1b65333fb698402ed065fc5d13.svg",
      "fullname": "Massimo Bini",
      "name": "mwbini",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23913",
      "authors": [
        {
          "_id": "67ebaea15baac6e5085afcb9",
          "name": "Xiaoxuan Wang",
          "hidden": false
        },
        {
          "_id": "67ebaea15baac6e5085afcba",
          "name": "Yihe Deng",
          "hidden": false
        },
        {
          "_id": "67ebaea15baac6e5085afcbb",
          "name": "Mingyu Derek Ma",
          "hidden": false
        },
        {
          "_id": "67ebaea15baac6e5085afcbc",
          "name": "Wei Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T10:04:35.000Z",
      "submittedOnDailyAt": "2025-04-01T07:47:10.184Z",
      "title": "Histología basada en ajuste automático de pesos para aprendizaje autodidacto",
      "submittedOnDailyBy": {
        "_id": "64ba5946c0f19c9025665a3c",
        "avatarUrl": "/avatars/bb148094ce52f1f385d30968dc22e0e6.svg",
        "isPro": false,
        "fullname": "Xiaoxuan Wang",
        "user": "xw27",
        "type": "user"
      },
      "summary": "La capacidad de los modelos de lenguaje para resolver problemas matemáticos es un tema central de la investigación, y se ha incrementado el interés en la posibilidad de mejorar los modelos utilizando rutas de generación automática de razones. Estas rutas permiten comprender el proceso lógico de pasos que conducen a la solución. Métodos de aprendizaje automático son efectivos para tareas de razones sin necesidad de modelos externos o manualidad, y pueden ser implementados de esta manera. Sin embargo, la optimización del entrenamiento de modelos con datos generados automáticamente es un problema abierto. En este artículo, se propone \"Entropy-Based Adaptive Weighting for Self-Training (EAST)\", una estrategia para implementar pesos adaptativos en el aprendizaje automático. EAST prioriza la incertidumbre de los datos en el entrenamiento automático, aplicando pesos adaptativos. Específicamente, EAST utiliza una función de mapeo con parámetros de ajuste para controlar los cortes de los pesos, asignando mayores pesos a datos más inciertos. Este enfoque ayuda a que el modelo se concentre más en ejemplos con mayor cantidad de información y mayor dificultad, mejorando así la capacidad de razonamiento. Los resultados de las pruebas en los benchmarks GSM8K y MATH muestran que el método virtual no presenta mejoras significativas en MATH, mientras que EAST mejora el modelo base en aproximadamente 1%. En GSM8K, EAST logra un aumento de rendimiento del 1-2% más que el método virtual.",
      "upvotes": 0,
      "discussionId": "67ebaea25baac6e5085afcfe",
      "ai_keywords": [
        "Entropy-Based Adaptive Weighting for Self-Training (EAST)",
        "mapping function",
        "tunable parameter",
        "weighting strategy",
        "uncertainty",
        "informative examples",
        "challenging examples"
      ]
    },
    "publishedAt": "2025-03-31T06:04:35.000Z",
    "title": "Entropy-Based Adaptive Weighting for Self-Training",
    "summary": "The mathematical problem-solving capabilities of large language models have\nbecome a focal point of research, with growing interests in leveraging\nself-generated reasoning paths as a promising way to refine and enhance these\nmodels. These paths capture step-by-step logical processes while requiring only\nthe correct answer for supervision. The self-training method has been shown to\nbe effective in reasoning tasks while eliminating the need for external models\nand manual annotations. However, optimizing the use of self-generated data for\nmodel training remains an open challenge. In this work, we propose\nEntropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive\nweighting strategy designed to prioritize uncertain data during self-training.\nSpecifically, EAST employs a mapping function with a tunable parameter that\ncontrols the sharpness of the weighting, assigning higher weights to data where\nthe model exhibits greater uncertainty. This approach guides the model to focus\non more informative and challenging examples, thereby enhancing its reasoning\nability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical\nresults show that, while the vanilla method yields virtually no improvement\n(0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K,\nEAST attains a further 1-2% performance boost compared to the vanilla method.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23913.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ba5946c0f19c9025665a3c",
      "avatarUrl": "/avatars/bb148094ce52f1f385d30968dc22e0e6.svg",
      "fullname": "Xiaoxuan Wang",
      "name": "xw27",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]