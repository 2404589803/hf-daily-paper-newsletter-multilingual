[
  {
    "paper": {
      "id": "2505.02567",
      "authors": [
        {
          "_id": "681c7895c7211b7efbc49f17",
          "name": "Xinjie Zhang",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f18",
          "name": "Jintao Guo",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f19",
          "user": {
            "_id": "66ab4c8a1703f12f49583c6d",
            "avatarUrl": "/avatars/59c77d4556edc049bb410e180813d5e3.svg",
            "isPro": false,
            "fullname": "zss",
            "user": "Suikong",
            "type": "user"
          },
          "name": "Shanshan Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T10:07:03.107Z",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1a",
          "name": "Minghao Fu",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1b",
          "name": "Lunhao Duan",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1c",
          "user": {
            "_id": "636f4c6b5d2050767e4a1491",
            "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
            "isPro": false,
            "fullname": "Guo-Hua Wang",
            "user": "Flourish",
            "type": "user"
          },
          "name": "Guo-Hua Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:04.757Z",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1d",
          "name": "Qing-Guo Chen",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1e",
          "name": "Zhao Xu",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1f",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f20",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658a8a837959448ef5500ce5/DwtDeTTjSrZoe2r4sL-a4.png"
      ],
      "publishedAt": "2025-05-05T11:18:03.000Z",
      "submittedOnDailyAt": "2025-05-08T07:57:47.854Z",
      "title": "La comprensión del desarrollo de la dimensión de la unidad y la generación de la dimensión de la creación, así como los desafíos y oportunidades.",
      "submittedOnDailyBy": {
        "_id": "658a8a837959448ef5500ce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
        "isPro": false,
        "fullname": "Shiyin Lu",
        "user": "runninglsy",
        "type": "user"
      },
      "summary": "Recientemente, se observa un extraordinario progreso en ambos campos: el modelo de comprensión multimodal y el modelo de generación de imágenes. Además del éxito en cada campo, ambos se han desarrollado de manera independiente y han formado arquitecturas paradigmáticas diferentes: la arquitectura basada en regresión automática se utiliza principalmente en la comprensión multimodal, mientras que la arquitectura basada en distribución es la base de la generación de imágenes. En los últimos tiempos, ha aumentado el interés en el desarrollo de marcos de trabajo que integren ambos campos. Las nuevas funciones de GPT-4o son un ejemplo de este tendimiento hacia la integración. Sin embargo, las diferencias arquitectónicas entre estos campos plantean grandes desafíos. Esta investigación presenta los esfuerzos actuales para mostrar claramente los problemas y guiar futuras investigaciones. Primero, se presentan los conceptos básicos y los recientes avances en la comprensión multimodal y en los modelos de generación de imágenes a partir de texto. Luego, se clasifican los modelos integrados en tres arquitecturas principales y se analizan las innovaciones en diseño estructural y investigación relacionada. Además, se recopilan conjuntos de datos y marcos de evaluación adecuados para los modelos integrados, proporcionando recursos para futuras discusiones. Finalmente, se discuten las principales cuestiones en este nuevo campo, incluyendo estrategias de tokenización, atención cruzada modal y datos. Este campo se encuentra en una etapa inicial y se espera un rápido progreso, con esta investigación actualizada regularmente. Nuestro objetivo es fomentar la investigación y proporcionar recursos efectivos a la comunidad. Las referencias relacionadas con esta investigación se pueden acceder en GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
      "upvotes": 33,
      "discussionId": "681c7896c7211b7efbc49f76",
      "githubRepo": "https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models",
      "ai_keywords": [
        "autoregressive-based architectures",
        "diffusion-based models",
        "unified frameworks",
        "GPT-4o",
        "multimodal understanding",
        "text-to-image generation models",
        "diffusion-based",
        "autoregressive-based",
        "hybrid approaches",
        "cross-modal attention"
      ]
    },
    "publishedAt": "2025-05-05T07:18:03.000Z",
    "title": "Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities",
    "summary": "Recent years have seen remarkable progress in both multimodal understanding\nmodels and image generation models. Despite their respective successes, these\ntwo domains have evolved independently, leading to distinct architectural\nparadigms: While autoregressive-based architectures have dominated multimodal\nunderstanding, diffusion-based models have become the cornerstone of image\ngeneration. Recently, there has been growing interest in developing unified\nframeworks that integrate these tasks. The emergence of GPT-4o's new\ncapabilities exemplifies this trend, highlighting the potential for\nunification. However, the architectural differences between the two domains\npose significant challenges. To provide a clear overview of current efforts\ntoward unification, we present a comprehensive survey aimed at guiding future\nresearch. First, we introduce the foundational concepts and recent advancements\nin multimodal understanding and text-to-image generation models. Next, we\nreview existing unified models, categorizing them into three main architectural\nparadigms: diffusion-based, autoregressive-based, and hybrid approaches that\nfuse autoregressive and diffusion mechanisms. For each category, we analyze the\nstructural designs and innovations introduced by related works. Additionally,\nwe compile datasets and benchmarks tailored for unified models, offering\nresources for future exploration. Finally, we discuss the key challenges facing\nthis nascent field, including tokenization strategy, cross-modal attention, and\ndata. As this area is still in its early stages, we anticipate rapid\nadvancements and will regularly update this survey. Our goal is to inspire\nfurther research and provide a valuable reference for the community. The\nreferences associated with this survey are available on GitHub\n(https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658a8a837959448ef5500ce5/DwtDeTTjSrZoe2r4sL-a4.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02567.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "658a8a837959448ef5500ce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
      "fullname": "Shiyin Lu",
      "name": "runninglsy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04588",
      "authors": [
        {
          "_id": "681c15ab84d0a008fcdb1ee8",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1ee9",
          "name": "Zile Qiao",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eea",
          "user": {
            "_id": "66224557c61c7fbd98099079",
            "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
            "isPro": false,
            "fullname": "GJ",
            "user": "SpaceProduct",
            "type": "user"
          },
          "name": "Jiayan Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:01.834Z",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eeb",
          "name": "Xuanbo Fan",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eec",
          "name": "Yingyan Hou",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eed",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eee",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eef",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1ef0",
          "name": "Yan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T17:30:22.000Z",
      "submittedOnDailyAt": "2025-05-08T00:54:07.103Z",
      "title": "ZeroSearch: No se necesita buscar para mejorar la capacidad de búsqueda",
      "submittedOnDailyBy": {
        "_id": "66224557c61c7fbd98099079",
        "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
        "isPro": false,
        "fullname": "GJ",
        "user": "SpaceProduct",
        "type": "user"
      },
      "summary": "La búsqueda de información efectiva es crucial para mejorar las capacidades de lógica y generación de los grandes modelos de lenguaje (LLMs). En los últimos estudios, se ha revisado el aprendizaje cognitivo reforzado (RL) para mejorar la capacidad de búsqueda de los LLMs en entornos reales interactuando con motores de búsqueda en tiempo real. Este enfoque muestra resultados esperados, pero presenta dos grandes problemas: (1) control de la calidad de los documentos es imposible: la calidad de los documentos devueltos por el motor de búsqueda es difícil de predecir y lleva a ruido y instabilidades en el proceso de entrenamiento; (2) aumento drástico de los costos de API: el entrenamiento con RL requiere frecuentes descargas y millones de solicitudes de búsqueda, lo que significa un aumento significativo de los costos de API y limita la escalabilidad. Para resolver estos problemas, se presenta el marco de aprendizaje cognitivo reforzado ZeroSearch. Este enfoque busca mejorar la capacidad de búsqueda de los LLMs sin interactuar con motores de búsqueda en entornos reales. Nuestro enfoque comienza con un ajuste ligero supervisado para transformar los LLMs en módulos de búsqueda. Posteriormente, durante el entrenamiento con RL, se utiliza una estrategia de descargas basada en colectores, y se deshace gradualmente la calidad de los documentos generados, exponiendo el modelo a escenarios de búsqueda que lo dificulten para desarrollar su capacidad de lógica. Extensive experiments show that ZeroSearch es efectivo en mejorar la capacidad de búsqueda de un LLM de 3B, alcanzando un rendimiento comparable a un motor de búsqueda real con un módulo de 7B y superandolo con un módulo de 14B. Además, se puede expandir tanto modelos básicos como modelos de entrenamiento de instancia con diferentes tamaños de parámetros, y tiene buena compatibilidad con una amplia gama de algoritmos de RL.",
      "upvotes": 23,
      "discussionId": "681c15ac84d0a008fcdb1f21",
      "projectPage": "https://alibaba-nlp.github.io/ZeroSearch/",
      "githubRepo": "https://github.com/Alibaba-nlp/ZeroSearch",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "large language models (LLMs)",
        "search capabilities",
        "live search engines",
        "real-world environments",
        "document quality",
        "noise",
        "instability",
        "training process",
        "API costs",
        "rollouts",
        "search requests",
        "ZeroSearch",
        "lightweight supervised fine-tuning",
        "retrieval module",
        "relevant documents",
        "noisy documents",
        "query",
        "curriculum-based rollout strategy",
        "reasoning ability",
        "retrieval scenarios",
        "base models",
        "instruction-tuned models",
        "parameter sizes",
        "RL algorithms"
      ]
    },
    "publishedAt": "2025-05-07T13:30:22.000Z",
    "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
    "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04588.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66224557c61c7fbd98099079",
      "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
      "fullname": "GJ",
      "name": "SpaceProduct",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04512",
      "authors": [
        {
          "_id": "681c546817fc8222efed5318",
          "name": "Teng Hu",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed5319",
          "name": "Zhentao Yu",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531a",
          "name": "Zhengguang Zhou",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531b",
          "name": "Sen Liang",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531c",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531d",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531e",
          "name": "Qinglin Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T15:33:18.000Z",
      "submittedOnDailyAt": "2025-05-08T05:21:39.978Z",
      "title": "Formas de Customización: Arquitectura de Dirección por la Diversidad de Vídeos Personalizados",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "La generación de videos personalizados tiene como objetivo crear videos que incluyen un tema específico bajo condiciones definidas de manera flexible por el usuario. Sin embargo, los métodos actuales enfrentan desafíos en la consistencia de identidad y las limitaciones del modelo de entrada. En este artículo, se propone el marco de trabajo HunyuanCustom, un sistema basado en múltiples modelos para la generación de videos personalizados. Este marco busca priorizar la consistencia del tema, apoyando condiciones de imagen, voz, video y texto. El modelo construido en HunyuanVideo introduce un módulo de fusión de texto-imagen basado en LLaVA y un módulo de fortalecimiento de ID de imágenes para abordar la tarea de generación condicionada por imagen y texto. Además, se propone una estructura de injección de condiciones basada en modelos para permitir la generación condicionada por voz y video. El módulo AudioNet implementa una atención espacial para realizar un enfoque de atención jerárquico, mientras que el módulo de generación condicionada por video utiliza una red de correspondencia de características basada en bloques para integrar videos condicionados de manera potencial. Los experimentos en casos de tema único y múltiples muestran que los métodos actuales de identidad, realismo y enfoque de texto-video son significativamente superados por los métodos abiertos y cerrados más avanzados. Además, se valida la asociación con tareas de descarga y se muestra una excelente performance en la generación personalizada de videos con voz y movimiento. Los resultados demuestran que la generación condicionada de múltiples modelos y la estrategia de mantener la identidad son cruciales para el desarrollo de la generación de videos controlables. Todo el código y los modelos están disponibles en https://hunyuancustom.github.io.",
      "upvotes": 8,
      "discussionId": "681c546e17fc8222efed54ce",
      "ai_keywords": [
        "LLaVA",
        "text-image fusion module",
        "image ID enhancement module",
        "temporal concatenation",
        "modality-specific condition injection mechanisms",
        "AudioNet module",
        "spatial cross-attention",
        "video-driven injection module",
        "latent-compressed conditional video",
        "patchify-based feature-alignment network",
        "ID consistency",
        "text-video alignment",
        "controllable video generation"
      ]
    },
    "publishedAt": "2025-05-07T11:33:18.000Z",
    "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation",
    "summary": "Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04512.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 50
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04622",
      "authors": [
        {
          "_id": "681c03418ff29a163ef5f370",
          "name": "Jingwen Ye",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f371",
          "user": {
            "_id": "64c903957b4d0d947ce86bc6",
            "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
            "isPro": false,
            "fullname": "Yuze He",
            "user": "hyz317",
            "type": "user"
          },
          "name": "Yuze He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:10.350Z",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f372",
          "name": "Yanning Zhou",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f373",
          "name": "Yiqin Zhu",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f374",
          "user": {
            "_id": "6441491c5d600fb0951cd872",
            "avatarUrl": "/avatars/d98892f3b52d87c2328201efa9897110.svg",
            "isPro": false,
            "fullname": "Kaiwen Xiao",
            "user": "loktarxiao",
            "type": "user"
          },
          "name": "Kaiwen Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:12.445Z",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f375",
          "name": "Yong-Jin Liu",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f376",
          "name": "Wei Yang",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f377",
          "name": "Xiao Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T17:59:46.000Z",
      "submittedOnDailyAt": "2025-05-08T05:41:14.360Z",
      "title": "Primitive Aniki: Generación de Asambleas 3D de Primitivas Humanizadas con Transformador Auto-regressivo",
      "submittedOnDailyBy": {
        "_id": "64c903957b4d0d947ce86bc6",
        "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
        "isPro": false,
        "fullname": "Yuze He",
        "user": "hyz317",
        "type": "user"
      },
      "summary": "La abstracción de elementos esenciales desempeña un papel crucial en la descomposición de complejos 3D modelos en elementos geométricos sencillos, afectando significativamente la percepción visual humana y presentando una amplia gama de aplicaciones en el campo de la visión computacional y grafica. Aunque el desarrollo reciente del contenido 3D ha demostrado una sorprendente evolución, los métodos actuales de abstracción de elementos dependen de optimizaciones geométricas y tienen una comprensión limitada de la significación, siendo entrenados en pequeños conjuntos de datos por clase, lo que dificulta la generalización entre diferentes clases de formas. Presentamos un nuevo marco de trabajo llamado PrimitiveAnything. Este marco redefine la abstracción de elementos como una tarea de generación de combinaciones de elementos, incluyendo también la transición de elementos que incluye condiciones de forma. PrimitiveAnything incluye un método de parametrización sin errores que representa de manera consistente diferentes tipos de elementos, así como un método de transición de elementos para generar combinaciones de elementos automáticamente. El marco propuesto permite que el sistema aprenda directamente las combinaciones de elementos a través de grandes abstracciones humanas, lo que permite entender cómo las personas decomponen complejas formas en elementos. A través de experimentos extensos, PrimitiveAnything ha demostrado su capacidad de mantener la precisión geométrica en diferentes clases de formas, generando combinaciones de elementos que se alinean con las observaciones humanas, y ha demostrado su potencial en diversas aplicaciones de proyectos 3D, así como en la posibilidad de contenido de usuario basado en elementos en el juego. Página del proyecto: https://primitiveanything.github.io",
      "upvotes": 7,
      "discussionId": "681c03468ff29a163ef5f4d7",
      "projectPage": "https://primitiveanything.github.io/",
      "githubRepo": "https://github.com/PrimitiveAnything/PrimitiveAnything",
      "ai_keywords": [
        "shape primitive abstraction",
        "geometric elements",
        "human visual cognition",
        "computer vision",
        "graphics",
        "3D content generation",
        "geometric optimization",
        "semantic understanding",
        "category-specific datasets",
        "primitive assembly generation task",
        "shape-conditioned primitive transformer",
        "auto-regressive generation",
        "ambiguity-free parameterization scheme",
        "human-crafted abstractions",
        "high-quality primitive assemblies",
        "human perception",
        "geometric fidelity",
        "3D applications",
        "user-generated content (UGC)"
      ]
    },
    "publishedAt": "2025-05-07T13:59:46.000Z",
    "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer",
    "summary": "Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04622.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c903957b4d0d947ce86bc6",
      "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
      "fullname": "Yuze He",
      "name": "hyz317",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.04364",
      "authors": [
        {
          "_id": "681c189c791c72783efe5a94",
          "user": {
            "_id": "6205fefd3f1dc8a642d70b10",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
            "isPro": false,
            "fullname": "Kai Ruan",
            "user": "6cf",
            "type": "user"
          },
          "name": "Kai Ruan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:58:58.134Z",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a95",
          "name": "Mowen Huang",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a96",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a97",
          "name": "Hao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T12:32:01.000Z",
      "submittedOnDailyAt": "2025-05-08T01:06:26.256Z",
      "title": "Benchmark del Conocimiento Colaborativo de LLM",
      "submittedOnDailyBy": {
        "_id": "6205fefd3f1dc8a642d70b10",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
        "isPro": false,
        "fullname": "Kai Ruan",
        "user": "6cf",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) demuestran la posibilidad de lógicas complejas, pero la función de la colaboración episódica en sistemas de agentes multi (MAS) bajo características específicas de grupos naturales (por ejemplo, cognición y comunicación localmente limitadas) ha sido poco investigada. Los actuales benchmarks no logran comprender completamente los problemas característicos de la colaboración distribuida cuando los agentes actúan basándose en información espacial-temporal incompleta. Para llenar esta brecha, se introduce SwarmBench, un nuevo benchmark que evalúa la capacidad de conocimiento colectivo de LLMs como agentes distribuidos. SwarmBench presenta 5 tareas de colaboración básicas en un 2D grid, diseñadas para que los agentes dependan principalmente de percepciones locales (un área visual de k×k) y de comunicación local. Se proponen indicadores para la eficacia de la colaboración y se analizan las dinámicas de la colectividad episódica. Se evaluan varios avanzados LLMs en 0 shot, observando diferencias significativas en cada tarea. Se destacan claramente los problemas derivados de la limitación de información local. La colaboración episódica aparece, pero se encuentra limitada en la formación de planes y estrategias robustas frente a la incertidumbre. Evaluar el rendimiento de LLMs en condiciones colectivas es crucial para la implementación de futuros sistemas distribuidos. SwarmBench se construye como un sistema físico escalable con características mecánicas predefinidas, proporcionando entornos, prompts, scripts de evaluación y conjuntos de datos experimentales detallados, contribuyendo a la investigación reproducible de colaboración basada en MAS y a la base teórica de MAS. El repositorio de código está disponible en https://github.com/x66ccff/swarmbench.",
      "upvotes": 7,
      "discussionId": "681c189e791c72783efe5b2d",
      "githubRepo": "https://github.com/x66ccff/swarmbench",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Multi-Agent Systems (MAS)",
        "swarm intelligence",
        "decentralized coordination",
        "spatio-temporal information",
        "SwarmBench",
        "foundational MAS coordination tasks",
        "2D grid environment",
        "local sensory input",
        "local communication",
        "coordination effectiveness",
        "emergent group dynamics",
        "zero-shot setting",
        "robust planning",
        "strategy formation",
        "uncertainty",
        "decentralized scenarios",
        "Embodied MAS"
      ]
    },
    "publishedAt": "2025-05-07T08:32:01.000Z",
    "title": "Benchmarking LLMs' Swarm intelligence",
    "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04364.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6205fefd3f1dc8a642d70b10",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
      "fullname": "Kai Ruan",
      "name": "6cf",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04528",
      "authors": [
        {
          "_id": "681c5152c7211b7efbba4b73",
          "user": {
            "_id": "641aef7b1911d3be67425338",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641aef7b1911d3be67425338/CmCbWWB6NxkAaus59q31w.jpeg",
            "isPro": false,
            "fullname": "Qi Liu",
            "user": "purewhite42",
            "type": "user"
          },
          "name": "Qi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:58:55.624Z",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b74",
          "name": "Xinhao Zheng",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b75",
          "name": "Renqiu Xia",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b76",
          "name": "Xingzhi Qi",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b77",
          "name": "Qinxiang Cao",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b78",
          "name": "Junchi Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T16:02:14.000Z",
      "submittedOnDailyAt": "2025-05-08T05:14:50.449Z",
      "title": "Identificación y resolución de problemas mediante formalización, marcos de trabajo y benchmarks oficiales",
      "submittedOnDailyBy": {
        "_id": "65b7ae76768464877cdb2e39",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
        "isPro": false,
        "fullname": "Renqiu Xia",
        "user": "renqiux0302",
        "type": "user"
      },
      "summary": "La resolución de problemas es una tarea clara en sí misma y una componente importante de la ciencia y la ingeniería. Sin embargo, falta una suficiente cantidad de reglas específicas para la resolución de problemas. Con el desarrollo reciente de agentes basados en IA para la resolución de problemas, ha surgido un creciente interés en la posibilidad de verificar el proceso de resolución de problemas a nivel de proceso, lo cual aún no se ha investigado suficientemente. Para complementar estas deficiencias, proponemos establecer principios básicos de resolución de problemas en términos de procesos deterministas Markovianos (MDP) y FPS (Formal Problem-Solving). FPS es un nuevo marco de trabajo para la resolución de problemas verificada basado en el entorno de FTP (Formal Theorem Proving). Además, en D-FPS (Deductive FPS), se separa la verificación de la solución de la solución misma, con el objetivo de hacer que la resolución sea más humana. La expresividad, precisión y completitud de estos marcos de trabajo han sido demostradas. Hemos construido tres marcos de referencia para la resolución de problemas: FormalMath500 es una parte formalizada del benchmark MATH500. MiniF2F-Solving y PutnamBench-Solving son versiones mejoradas de los benchmarks FTP MiniF2F y PutnamBench. Proponemos RPE (Restricted Propositional Equivalence), un enfoque sintáctico utilizando demostraciones formales, con el objetivo de evaluar con precisión, analíticamente y de manera humana. RPE es un método para determinar la exactitud de la respuesta mediante demostraciones formales. Con base en 4 modelos comunes de FTP y 2 métodos de programación avanzada, FPS puede resolver hasta el 23.77% máximo de FormalMath500, el 27.47% máximo de MiniF2F-Solving y el 0.31% máximo de PutnamBench-Solving.",
      "upvotes": 5,
      "discussionId": "681c5153c7211b7efbba4bb4",
      "githubRepo": "https://github.com/Purewhite2019/formal_problem_solving_main",
      "ai_keywords": [
        "Markov decision process",
        "FPS (Formal Problem-Solving)",
        "FTP (formal theorem proving)",
        "D-FPS (Deductive FPS)",
        "FormalMath500",
        "MiniF2F-Solving",
        "PutnamBench-Solving",
        "RPE (Restricted Propositional Equivalence)"
      ]
    },
    "publishedAt": "2025-05-07T12:02:14.000Z",
    "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving",
    "summary": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04528.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b7ae76768464877cdb2e39",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
      "fullname": "Renqiu Xia",
      "name": "renqiux0302",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03912",
      "authors": [
        {
          "_id": "681c549cb322a2fe864c8b0d",
          "name": "Can Cui",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b0e",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b0f",
          "name": "Wenxuan Song",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b10",
          "name": "Shuanghao Bai",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b11",
          "name": "Xinyang Tong",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b12",
          "name": "Zirui Ge",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b13",
          "name": "Runze Suo",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b14",
          "name": "Wanqi Zhou",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b15",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b16",
          "name": "Bofang Jia",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b17",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b18",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b19",
          "name": "Donglin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T18:35:07.000Z",
      "submittedOnDailyAt": "2025-05-08T05:23:33.004Z",
      "title": "OpenHelix: Servicio corto, análisis experimental, código abierto\nModelo de sistema doble VLA para la manipulación de robots",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "La arquitectura VLA (Visión-Lenguaje-Acción) de sistemas dobles ha convertidose en un tema de interés creciente en la investigación de inteligencia artificial, pero carece de suficientes trabajos abiertos de código para la análisis y optimización de su rendimiento avanzado. Para abordar este problema, en este trabajo se resume la diseño de la arquitectura existente de sistemas dobles y se realiza una evaluación experimental sistemática de sus elementos de diseño fundamentales. Finalmente, se propone proporcionar un modelo abierto de bajo costo para exploraciones avanzadas. Naturalmente, este proyecto continuará añadiendo resultados de experimentos y modelos de código abierto que mejoren el rendimiento, de modo que todos puedan utilizarlos como opciones disponibles. Página del proyecto: https://openhelix-robot.github.io/.",
      "upvotes": 3,
      "discussionId": "681c549eb322a2fe864c8b6e"
    },
    "publishedAt": "2025-05-06T14:35:07.000Z",
    "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation",
    "summary": "Dual-system VLA (Vision-Language-Action) architectures have become a hot\ntopic in embodied intelligence research, but there is a lack of sufficient\nopen-source work for further performance analysis and optimization. To address\nthis problem, this paper will summarize and compare the structural designs of\nexisting dual-system architectures, and conduct systematic empirical\nevaluations on the core design elements of existing dual-system architectures.\nUltimately, it will provide a low-cost open-source model for further\nexploration. Of course, this project will continue to update with more\nexperimental conclusions and open-source models with improved performance for\neveryone to choose from. Project page: https://openhelix-robot.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03912.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03418",
      "authors": [
        {
          "_id": "681c4d5b5971460af345032a",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032b",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032c",
          "name": "Junwei Su",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032d",
          "name": "Yuchen Tian",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032e",
          "name": "Yuqi Zhu",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032f",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450330",
          "name": "Lanning Wei",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450331",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450332",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T10:53:58.000Z",
      "submittedOnDailyAt": "2025-05-08T04:51:36.213Z",
      "title": "Investigación sobre la implementación de soluciones complejas para la resolución de problemas de cálculo de conocimiento mediante modelos de lenguaje de gran escala",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "La resolución de problemas ha sido reconocida como una motivación fundamental para el progreso humano en diversas áreas, lo cual ha sido comúnmente reconocido en la historia. Con el desarrollo de la inteligencia artificial, los modelos de lenguaje de gran escala (LLMs) han emergido como potentes herramientas para resolver problemas complejos en diversas áreas. A diferencia de los sistemas de cálculo tradicionales, los LLMs combinan un exceso de potencia de cálculo y una aproximación a la lógica humana para generar soluciones, realizar inferencias y utilizar herramientas externas de cálculo. Sin embargo, al aplicar los LLMs a problemas de gran escala en el mundo real, se encuentran grandes desafíos, como la necesidad de explicaciones lógicas paso a paso, la integración de conocimientos de diversas áreas y la verificación de resultados. Este estudio investiga las capacidades y limitaciones de los LLMs en la resolución de problemas complejos, revisando técnicas como la explicación lógica paso a paso (Chain-of-Thought, CoT), la expansión de conocimientos, y métodos de verificación basados en LLMs y herramientas. Además, se abordan problemas específicos en áreas como el desarrollo de software, la explicación y demostración lógica matemática, el análisis de datos y modelado, y la investigación científica. Este artículo discute las limitaciones básicas de las soluciones actuales basadas en LLMs y propone futuras direcciones para la resolución de problemas complejos basadas en LLMs, desde la perspectiva de explicaciones lógicas paso a paso, la integración de conocimientos de diversas áreas y la verificación de resultados.",
      "upvotes": 2,
      "discussionId": "681c4d5f5971460af3450465",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Chain-of-Thought (CoT) reasoning",
        "knowledge augmentation",
        "verification techniques",
        "software engineering",
        "mathematical reasoning and proving",
        "data analysis and modeling",
        "scientific research",
        "multi-step reasoning",
        "domain knowledge integration",
        "result verification"
      ]
    },
    "publishedAt": "2025-05-06T06:53:58.000Z",
    "title": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey",
    "summary": "Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03418.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03821",
      "authors": [
        {
          "_id": "681c7a3829ba66a745217db5",
          "user": {
            "_id": "63caf7ce9f78909f9f81eb72",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
            "isPro": true,
            "fullname": "Gracjan Goral",
            "user": "Gracjan",
            "type": "user"
          },
          "name": "Gracjan Góral",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:02.558Z",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db6",
          "name": "Alicja Ziarko",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db7",
          "name": "Piotr Miłoś",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db8",
          "name": "Michał Nauman",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db9",
          "name": "Maciej Wołczyk",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217dba",
          "name": "Michał Kosiński",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63caf7ce9f78909f9f81eb72/b8OMQONPXPyBtcitD1Yg7.jpeg"
      ],
      "publishedAt": "2025-05-03T00:10:41.000Z",
      "submittedOnDailyAt": "2025-05-08T08:19:59.040Z",
      "title": "Visión: Evaluación de la perspectiva visual del modelo de visión",
      "submittedOnDailyBy": {
        "_id": "63caf7ce9f78909f9f81eb72",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
        "isPro": true,
        "fullname": "Gracjan Goral",
        "user": "Gracjan",
        "type": "user"
      },
      "summary": "Visión de Languaje y Modelos de Visión (VLMs) se investigan para evaluar sus habilidades en representación visual. Se utilizan nuevas tareas visuales que más se parecen a las pruebas humanas. Nuestro enfoque combina un muñeco mini y un objeto para configurar escenarios ajustados. Se cambian sistemáticamente la posición del objeto y la dirección del muñeco mini, y se utilizan tanto el punto de vista de un pájaro como de una perspectiva superficial, creando 144 tareas visuales únicas. Cada tarea visual está compuesta por 7 series de preguntas diagnósticas diseñadas para evaluar la comprensión del escenario, la razón espacial y la representación visual. Se evaluaron varios modelos recientes como GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct y Claude Sonnet. Estos modelos demostraron excelencia en la comprensión del escenario, pero su rendimiento en la razón espacial fue significativamente menor y su representación visual se deterioró aún más. Nuestro análisis muestra la diferencia entre la reconocimiento de objetos a nivel superficial y la necesidad de una representación profunda espacial y visual, argumentando que la integración de una representación geométrica clara y protocolos de entrenamiento es esencial para el futuro de los VLMs.",
      "upvotes": 2,
      "discussionId": "681c7a3e29ba66a745217f0c"
    },
    "publishedAt": "2025-05-02T20:10:41.000Z",
    "title": "Beyond Recognition: Evaluating Visual Perspective Taking in Vision\n  Language Models",
    "summary": "We investigate the ability of Vision Language Models (VLMs) to perform visual\nperspective taking using a novel set of visual tasks inspired by established\nhuman tests. Our approach leverages carefully controlled scenes, in which a\nsingle humanoid minifigure is paired with a single object. By systematically\nvarying spatial configurations - such as object position relative to the\nhumanoid minifigure and the humanoid minifigure's orientation - and using both\nbird's-eye and surface-level views, we created 144 unique visual tasks. Each\nvisual task is paired with a series of 7 diagnostic questions designed to\nassess three levels of visual cognition: scene understanding, spatial\nreasoning, and visual perspective taking. Our evaluation of several\nstate-of-the-art models, including GPT-4-Turbo, GPT-4o,\nLlama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that\nwhile they excel in scene understanding, the performance declines significantly\non spatial reasoning and further deteriorates on perspective-taking. Our\nanalysis suggests a gap between surface-level object recognition and the deeper\nspatial and perspective reasoning required for complex visual tasks, pointing\nto the need for integrating explicit geometric representations and tailored\ntraining protocols in future VLM development.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63caf7ce9f78909f9f81eb72/b8OMQONPXPyBtcitD1Yg7.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63caf7ce9f78909f9f81eb72",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
      "fullname": "Gracjan Goral",
      "name": "Gracjan",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.00358",
      "authors": [
        {
          "_id": "68154d77c8ab88a66b8d81a7",
          "name": "Albert Ge",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81a8",
          "name": "Tzu-Heng Huang",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81a9",
          "name": "John Cooper",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81aa",
          "name": "Avi Trost",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ab",
          "name": "Ziyi Chu",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ac",
          "name": "Satya Sai Srinath Namburi GNVV",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ad",
          "name": "Ziyang Cai",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ae",
          "name": "Kendall Park",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81af",
          "name": "Nicholas Roberts",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81b0",
          "name": "Frederic Sala",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T07:08:19.000Z",
      "submittedOnDailyAt": "2025-05-08T05:38:42.650Z",
      "title": "R&B: Reconfiguración del dominio y equilibrio de la confusión de datos para la entrenamiento eficiente de modelos básicos",
      "submittedOnDailyBy": {
        "_id": "650263c89a612aa33a018383",
        "avatarUrl": "/avatars/a1415a4696da98ffebc1ac798cbab052.svg",
        "isPro": false,
        "fullname": "Albert Ge",
        "user": "albertge",
        "type": "user"
      },
      "summary": "El sistema de mezcla de datos reduce los costos en la entrenamiento de modelos de lenguaje. Estos métodos tienen dos desventajas. Primero, el dominio de datos (por ejemplo, fuentes de datos, tipos de tareas) se determina antes, lo que impide comprender nuances lingüísticas importantes y que no se conecten con la mejora de la calidad. Segundo, estos métodos pueden ser computacionalmente complejos. Con el aumento del número de dominios, la cantidad de cálculos aumenta. Para enfrentar estas desafíos, se utiliza el marco de trabajo R&B para repartir de nuevo los datos de entrenamiento basados en la similitud lingüística, crear dominios más pequeños y utilizar la experiencia del dominio obtenida en el proceso de entrenamiento para optimizar la estructura de datos con la matriz Gram. A diferencia de los estudios previos, no se necesita calcular información de evaluación o pérdidas/gradientes adicionales. Se realiza un análisis de esta técnica bajo condiciones estándar y se compara el eficacia del R&B con un enfoque de mezcla no adecuado, proporcionando una conformidad teórica. Experimentalmente, se muestra el efecto del R&B en cinco conjuntos de datos diferentes de procesamiento de lenguaje, inferencia y tareas de datos multiformes. El R&B puede superar o igualar al mejor rendimiento de un sistema de mezcla de datos de datos superior, solo con un sobrecargo computacional adicional de 0.01%.",
      "upvotes": 2,
      "discussionId": "68154d78c8ab88a66b8d820c",
      "ai_keywords": [
        "semantic similarity",
        "Gram matrix",
        "domain gradients"
      ]
    },
    "publishedAt": "2025-05-01T03:08:19.000Z",
    "title": "R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training",
    "summary": "Data mixing strategies have successfully reduced the costs involved in\ntraining language models. While promising, such methods suffer from two flaws.\nFirst, they rely on predetermined data domains (e.g., data sources, task\ntypes), which may fail to capture critical semantic nuances, leaving\nperformance on the table. Second, these methods scale with the number of\ndomains in a computationally prohibitive way. We address these challenges via\nR&B, a framework that re-partitions training data based on semantic similarity\n(Regroup) to create finer-grained domains, and efficiently optimizes the data\ncomposition (Balance) by leveraging a Gram matrix induced by domain gradients\nobtained throughout training. Unlike prior works, it removes the need for\nadditional compute to obtain evaluation information such as losses or\ngradients. We analyze this technique under standard regularity conditions and\nprovide theoretical insights that justify R&B's effectiveness compared to\nnon-adaptive mixing approaches. Empirically, we demonstrate the effectiveness\nof R&B on five diverse datasets ranging from natural language to reasoning and\nmultimodal tasks. With as little as 0.01% additional compute overhead, R&B\nmatches or exceeds the performance of state-of-the-art data mixing strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00358.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650263c89a612aa33a018383",
      "avatarUrl": "/avatars/a1415a4696da98ffebc1ac798cbab052.svg",
      "fullname": "Albert Ge",
      "name": "albertge",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03570",
      "authors": [
        {
          "_id": "681b518bf497fd5e45b55eeb",
          "user": {
            "_id": "667ed2bf12e48bee0e972ccc",
            "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
            "isPro": false,
            "fullname": "Mariya Davydova",
            "user": "mariya-davydova",
            "type": "user"
          },
          "name": "Mariya Davydova",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:24.254Z",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eec",
          "name": "Daniel Jeffries",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eed",
          "name": "Patrick Barker",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eee",
          "name": "Arturo Márquez Flores",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eef",
          "name": "Sinéad Ryan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/fe49DABHt5h8Uw-_GCzaB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/yfxA_jM6W5khwa_hHL_mF.png"
      ],
      "publishedAt": "2025-05-06T14:29:47.000Z",
      "submittedOnDailyAt": "2025-05-08T07:36:52.978Z",
      "title": "OSUniverse: Benchmark de la GUI de Navegación AI de DamoDal",
      "submittedOnDailyBy": {
        "_id": "667ed2bf12e48bee0e972ccc",
        "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
        "isPro": false,
        "fullname": "Mariya Davydova",
        "user": "mariya-davydova",
        "type": "user"
      },
      "summary": "En este artículo se presenta un marco de prueba para OSUniverse:desktop que evalúa una amplia gama de tareas complejas y diversas. Este marco de prueba se centra en la usuario amigable, la extensibilidad, la cobertura completa de casos de prueba y la verificación automática, exige la capacidad, precisión y claridad de pensamiento de un agente IA que debe realizar desde la pincelada de precisión básica hasta las pruebas de múltiples aplicaciones. Los contenidos presentados en la versión 1 del marco de prueba se incluyen aquí. Se ajusta la complejidad de los casos de prueba de manera que los agentes IA del estado de la arte al momento de su publicación no logren obtener más del 50% de los resultados, y se verifica si el promedio de los empleados pueden realizar estas tareas con precisión total. El marco de prueba permite calificar puntualmente manualmente y presenta una estructura de verificación automática con un error promedio inferior al 2%. Esta herramienta proporciona una base sólida para medir el desarrollo de la automatización completa, la capacidad y la eficiencia de los agentes IA de navegación de GUI tanto a corto como a largo plazo. El código fuente del marco de prueba está disponible en https://github.com/agentsea/osuniverse.",
      "upvotes": 1,
      "discussionId": "681b518cf497fd5e45b55f0f",
      "projectPage": "https://agentsea.github.io/osuniverse/",
      "githubRepo": "https://github.com/agentsea/osuniverse"
    },
    "publishedAt": "2025-05-06T10:29:47.000Z",
    "title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents",
    "summary": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\nease of use, extensibility, comprehensive coverage of test cases, and automated\nvalidation. We divide the tasks in increasing levels of complexity, from basic\nprecision clicking to multistep, multiapplication tests requiring dexterity,\nprecision, and clear thinking from the agent. In version one of the benchmark,\npresented here, we have calibrated the complexity of the benchmark test cases\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\ndo not achieve results higher than 50%, while the average white collar worker\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\nmanually, but we also introduce an automated validation mechanism that has an\naverage error rate less than 2%. Therefore, this benchmark presents solid\nground for fully automated measuring of progress, capabilities and the\neffectiveness of GUI-navigation AI agents over the short and medium-term\nhorizon. The source code of the benchmark is available at\nhttps://github.com/agentsea/osuniverse.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/fe49DABHt5h8Uw-_GCzaB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/yfxA_jM6W5khwa_hHL_mF.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03570.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667ed2bf12e48bee0e972ccc",
      "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
      "fullname": "Mariya Davydova",
      "name": "mariya-davydova",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02393",
      "authors": [
        {
          "_id": "681c423f198e1dea5c26f2f4",
          "user": {
            "_id": "6445e9bd1cfc9ae6bb40985c",
            "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
            "isPro": false,
            "fullname": "Evan Jeong",
            "user": "Eavn",
            "type": "user"
          },
          "name": "Sungheon Jeong",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-08T09:38:37.142Z",
          "hidden": false
        },
        {
          "_id": "681c423f198e1dea5c26f2f5",
          "user": {
            "_id": "646b57c6e5abcbf6709fabf6",
            "avatarUrl": "/avatars/e9749acf7866eeaf017f0a43351794fc.svg",
            "isPro": false,
            "fullname": "Jihong Park",
            "user": "Paper9795",
            "type": "user"
          },
          "name": "Jihong Park",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-08T05:36:09.797Z",
          "hidden": false
        },
        {
          "_id": "681c423f198e1dea5c26f2f6",
          "name": "Mohsen Imani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:33:20.000Z",
      "submittedOnDailyAt": "2025-05-08T04:12:55.976Z",
      "title": "Detección de la excepcionalidad en vídeos utilizando la fusión de imágenes y eventos con énfasis en la incertidumbre",
      "submittedOnDailyBy": {
        "_id": "6445e9bd1cfc9ae6bb40985c",
        "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
        "isPro": false,
        "fullname": "Evan Jeong",
        "user": "Eavn",
        "type": "user"
      },
      "summary": "Varios sistemas de detección de vídeos actuales dependen únicamente de los marcos RGB, pero esto les hace carecer de la capacidad analítica temporal necesaria para detectar movimientos rápidos o instantáneos, lo que los convierte en indicadores de eventos extraños. Para resolver esta limitación, proponemos un marco de trabajo para la detección de eventos en vídeos \"Fusión de Eventos y Imágenes para la Detección de Vídeos de Anomalías (IEF-VAD)\" que combina la representación de eventos y las características de imágenes en vídeos RGB. Este marco básicamente realiza la fusión directamente de los marcos de imágenes y las características de imágenes en el proceso de tratamiento de incertidumbre. Este sistema: (i) modela el ruido sensorial mediante la adecuación de la distribución de Student's-t y obtiene pesos de desvío inverso a nivel de valor mediante la aproximación de Laplace; (ii) aplica actualizaciones de tipo Kalman para ajustar el equilibrio del modelo temporal; (iii) refina repetidamente el estado potencial y elimina el ruido cruzado de los otros modalidades. IEF-VAD no requiere sensores de eventos profesionales ni etiquetas a nivel de marco, y establece el estado de la arte en varios benchmarks de detección de anomalías en el mundo real. Estos hallazgos demuestran la utilidad de la representación de eventos sintéticas en RGB para destacar códigos de movimiento que generalmente no se representan, y permiten alcanzar una comprensión precisa y robusta de vídeos en diversas aplicaciones. El código y el modelo están disponibles en https://github.com/EavnJeong/IEF-VAD.",
      "upvotes": 1,
      "discussionId": "681c4243198e1dea5c26f3cd",
      "githubRepo": "https://github.com/EavnJeong/IEF-VAD",
      "ai_keywords": [
        "Image-Event Fusion",
        "Video Anomaly Detection",
        "event representations",
        "Student`s-t likelihood",
        "Laplace approximation",
        "Kalman-style frame-wise updates",
        "fused latent state",
        "cross-modal noise"
      ]
    },
    "publishedAt": "2025-05-05T02:33:20.000Z",
    "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection",
    "summary": "Most existing video anomaly detectors rely solely on RGB frames, which lack\nthe temporal resolution needed to capture abrupt or transient motion cues, key\nindicators of anomalous events. To address this limitation, we propose\nImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that\nsynthesizes event representations directly from RGB videos and fuses them with\nimage features through a principled, uncertainty-aware process. The system (i)\nmodels heavy-tailed sensor noise with a Student`s-t likelihood, deriving\nvalue-level inverse-variance weights via a Laplace approximation; (ii) applies\nKalman-style frame-wise updates to balance modalities over time; and (iii)\niteratively refines the fused latent state to erase residual cross-modal noise.\nWithout any dedicated event sensor or frame-level labels, IEF-VAD sets a new\nstate of the art across multiple real-world anomaly detection benchmarks. These\nfindings highlight the utility of synthetic event representations in\nemphasizing motion cues that are often underrepresented in RGB frames, enabling\naccurate and robust video understanding across diverse applications without\nrequiring dedicated event sensors. Code and models are available at\nhttps://github.com/EavnJeong/IEF-VAD.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02393.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6445e9bd1cfc9ae6bb40985c",
      "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
      "fullname": "Evan Jeong",
      "name": "Eavn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]