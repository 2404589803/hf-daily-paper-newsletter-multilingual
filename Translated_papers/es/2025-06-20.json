[
  {
    "paper": {
      "id": "2506.14965",
      "authors": [
        {
          "_id": "68538be099bf39f9665c79b9",
          "name": "Zhoujun Cheng",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79ba",
          "name": "Shibo Hao",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bb",
          "name": "Tianyang Liu",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bc",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bd",
          "name": "Yutao Xie",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79be",
          "name": "Feng Yao",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bf",
          "name": "Yuexin Bian",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c0",
          "name": "Yonghao Zhuang",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c1",
          "name": "Nilabjo Dey",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c2",
          "name": "Yuheng Zha",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c3",
          "name": "Yi Gu",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c4",
          "name": "Kun Zhou",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c5",
          "name": "Yuqi Wang",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c6",
          "name": "Yuan Li",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c7",
          "name": "Richard Fan",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c8",
          "name": "Jianshu She",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c9",
          "name": "Chengqian Gao",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79ca",
          "name": "Abulhair Saparov",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cb",
          "name": "Haonan Li",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cc",
          "name": "Taylor W. Killian",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cd",
          "name": "Mikhail Yurochkin",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79ce",
          "name": "Zhengzhong Liu",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cf",
          "name": "Eric P. Xing",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79d0",
          "name": "Zhiting Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T20:24:00.000Z",
      "submittedOnDailyAt": "2025-06-20T06:25:47.447Z",
      "title": "¡Hola! Una revisión en el contexto de la aprendizaje por refuerzo para la restauración de la inferencia de un LLM",
      "submittedOnDailyBy": {
        "_id": "6083902e1e36b13a64497d91",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6083902e1e36b13a64497d91/h4rGHMn2c6z5GesF0F6VU.png",
        "isPro": false,
        "fullname": "cheng",
        "user": "zhoujun",
        "type": "user"
      },
      "summary": "El aprendizaje por refuerzo (RL) aparece como un potencial enfoque para mejorar la lógica de grandes modelos de lenguaje (LLM), pero muchos esfuerzos abiertos se centran en matemáticas y código, limitando la comprensión de su aplicación en lógica general. El problema radica en la falta de señales de recompensa scalables de RL confiables en diversas áreas de lógica. Presentamos Guru, un corpus recopilado de RL para lógica. Este corpus extiende seis áreas de lógica: matemáticas, código, ciencia, lógica, simulación y tablas, construido con diseño, eliminación y filtrado de recompensas específicos para garantizar confiabilidad y eficiencia. Basándonos en Guru, revisamos sistemáticamente los descubrimientos en la lógica de LLM y observamos claros cambios entre áreas. Por ejemplo, aunque previos estudios muestran que RL extrae conocimiento de modelos preentrenados, nuestros resultados muestran patrones más complejos: áreas que se ven frecuentemente en el entrenamiento previo (matemáticas, código, ciencia) pueden beneficiarse fácilmente en el aprendizaje por refuerzo cruzado de dominios, mientras que áreas limitadas en el entrenamiento previo (lógica, simulación, tablas) requieren aprendizaje específico para lograr mejoras significativas. Finalmente, presentamos dos modelos: Guru-7B y Guru-32B. Estos modelos tienen los mejores rendimientos entre los modelos abiertos entrenados con RL en datos públicos, superando en 17 tareas de 6 áreas de lógica un 7.9% y un 6.7%. Además, nuestros modelos mejoran eficazmente el rendimiento Pass@k del modelo base, especialmente en tareas complejas que no aparecen en el entrenamiento previo. Liberamos código de datos, modelos, entrenamiento y evaluación, así como un marco para la realización de lógica general. https://github.com/LLM360/Reasoning360",
      "upvotes": 6,
      "discussionId": "68538be099bf39f9665c79d1",
      "ai_summary": "Guru, a diverse RL reasoning corpus, highlights domain-specific training needs and demonstrates improved performance in complex tasks for RL-enhanced LLMs.",
      "ai_keywords": [
        "reinforcement learning",
        "large language model",
        "RL reasoning",
        "curated RL reasoning corpus",
        "domain-specific reward design",
        "dereplication",
        "filtering",
        "cross-domain RL training",
        "in-domain training",
        "Guru-7B",
        "Guru-32B",
        "Pass@k performance"
      ]
    },
    "publishedAt": "2025-06-17T16:24:00.000Z",
    "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective",
    "summary": "Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14965.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6083902e1e36b13a64497d91",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6083902e1e36b13a64497d91/h4rGHMn2c6z5GesF0F6VU.png",
      "fullname": "cheng",
      "name": "zhoujun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15154",
      "authors": [
        {
          "_id": "685393f499bf39f9665c79db",
          "name": "Anuradha Chopra",
          "hidden": false
        },
        {
          "_id": "685393f499bf39f9665c79dc",
          "name": "Abhinaba Roy",
          "hidden": false
        },
        {
          "_id": "685393f499bf39f9665c79dd",
          "name": "Dorien Herremans",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655431b2997379e9b0999d23/zAwOQ2zY8-8Eu7bUhFURQ.png"
      ],
      "publishedAt": "2025-06-18T05:51:36.000Z",
      "submittedOnDailyAt": "2025-06-20T04:32:59.096Z",
      "title": "SonicVerse: Características musicales basadas en aprendizaje multitarea para captiones",
      "submittedOnDailyBy": {
        "_id": "655431b2997379e9b0999d23",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
        "isPro": false,
        "fullname": "Dorien Herremans",
        "user": "dorienh",
        "type": "user"
      },
      "summary": "Detalles de capturación reflejan exactamente las características de la música y pueden enriquecer los bases de datos de música, así como promover el desarrollo de investigaciones en IA de música. En este artículo, se presenta un modelo de capturación musical integrado de tareas de generación de capturación y detección de características musicales (por ejemplo, detección de tono, detección de arpa, etc.), llamado SonicVerse, que permite la comprensión directa de detalles sonoros bajos y propiedades musicales altos. La principal contribución es la arquitectura basada en proyección que transforma entradas de voz en tokens de lenguaje, y utiliza un auxiliar especial para identificar características musicales. También, el output de este auxiliar se convierte en tokens de lenguaje para fortalecer el entrada de capturación. Este marco de trabajo puede generar breves fragmentos musicales con abundantes explicaciones, lo que permite crear detalladas explicaciones incluyendo información temporal de largos trabajos musicales. El entrenamiento del modelo se realizó utilizando el módulo de extracción de características musicales MIRFLEX, que comenta las características musicales en el conjunto de datos MusicBench, generando pares de capturación y datos de características musicales. Los resultados de los experimentos demuestran que la inserción de estas características mejora la calidad y los detalles de la capturación generada.",
      "upvotes": 2,
      "discussionId": "685393f599bf39f9665c79de",
      "ai_summary": "SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.",
      "ai_keywords": [
        "multi-task music captioning",
        "SonicVerse",
        "caption generation",
        "key detection",
        "vocals detection",
        "projection-based architecture",
        "language tokens",
        "auxiliary heads",
        "time-informed descriptions",
        "large-language model",
        "MusicBench dataset",
        "MIRFLEX",
        "music feature extractor"
      ]
    },
    "publishedAt": "2025-06-18T01:51:36.000Z",
    "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
    "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655431b2997379e9b0999d23/zAwOQ2zY8-8Eu7bUhFURQ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15154.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "655431b2997379e9b0999d23",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
      "fullname": "Dorien Herremans",
      "name": "dorienh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09827",
      "authors": [
        {
          "_id": "685519bb4f1add9d4c5c5cbd",
          "user": {
            "_id": "61a24fc72101184cfb29c965",
            "avatarUrl": "/avatars/e32aa61016caef50de28c16b30196799.svg",
            "isPro": false,
            "fullname": "Christoph Schuhmann",
            "user": "ChristophSchuhmann",
            "type": "user"
          },
          "name": "Christoph Schuhmann",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-20T08:20:12.243Z",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cbe",
          "name": "Robert Kaczmarczyk",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cbf",
          "name": "Gollam Rabby",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc0",
          "user": {
            "_id": "62e7dd4036a8e8a82700041c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
            "isPro": false,
            "fullname": "Felix Friedrich",
            "user": "felfri",
            "type": "user"
          },
          "name": "Felix Friedrich",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-20T08:57:36.090Z",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc1",
          "name": "Maurice Kraus",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc2",
          "name": "Kourosh Nadi",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc3",
          "name": "Huu Nguyen",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc4",
          "name": "Kristian Kersting",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc5",
          "name": "Sören Auer",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62e7dd4036a8e8a82700041c/tvkMYrIKiGhbIVAuaxvlt.png"
      ],
      "publishedAt": "2025-06-11T15:06:59.000Z",
      "submittedOnDailyAt": "2025-06-20T06:53:47.262Z",
      "title": "EmoNet-Voice: Nivel de detección de emociones en conversaciones: puntos de referencia verificados por expertos",
      "submittedOnDailyBy": {
        "_id": "62e7dd4036a8e8a82700041c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
        "isPro": false,
        "fullname": "Felix Friedrich",
        "user": "felfri",
        "type": "user"
      },
      "summary": "El progreso en modelos de conversión de texto a voz es necesario para evaluar la capacidad de entendimiento emocional de sistemas AI. Los conjuntos de datos actuales de reconocimiento de emociones en voz (SER) son frecuentemente limitados por la diferenciación de emociones, preocupaciones de privacidad o expresiones actuadas. En este trabajo, se presenta un nuevo recurso para la detección de emociones en voz, llamado EmoNet-Voice. EmoNet-Voice Big es un conjunto de datos de aprendizaje previo a gran escala (11 vozes, 40 emociones, 4 idiomas, más de 4,500 horas de voz). EmoNet-Voice Bench es un nuevo conjunto de datos de evaluación que incluye comentarios de expertos en el campo. EmoNet-Voice evalúa modelos de SER utilizando una espectro detallado de 40 emociones. Se ha utilizado la generación de voz de alto nivel para crear muestras de voz que imitan escenas en las que actores provocan ciertas emociones. Se realizó una validación rigurosa por parte de expertos en psicología y se asignaron etiquetas de intensidad reconocidas. Este enfoque de aproximación sintética y protección de privacidad permite incluir estados emocionales sensibles que faltaban en los conjuntos de datos actuales. Finalmente, se presenta el modelo de voz Empathic Insight. Estos modelos establecen nuevos estándares en la reconocimiento de emociones en voz, demostrando altos niveles de acuerdo con expertos en humanos. Se observan descubrimientos valiosos, como que emociones de alta arousal (por ejemplo, enojo) son más fáciles de detectar que estados de baja arousal (por ejemplo, concentración).",
      "upvotes": 1,
      "discussionId": "685519bb4f1add9d4c5c5cc6",
      "ai_summary": "EmoNet-Voice, a new resource with large pre-training and benchmark datasets, advances speech emotion recognition by offering fine-grained emotion evaluation with synthetic, privacy-preserving audio.",
      "ai_keywords": [
        "speech emotion recognition",
        "SER",
        "EmoNet-Voice",
        "EmoNet-Voice Big",
        "EmoNet-Voice Bench",
        "human expert annotations",
        "synthetic audio snippets",
        "psychology experts",
        "high-arousal emotions",
        "low-arousal states",
        "Empathic Insight Voice models"
      ]
    },
    "publishedAt": "2025-06-11T11:06:59.000Z",
    "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech\n  Emotion Detection",
    "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62e7dd4036a8e8a82700041c/tvkMYrIKiGhbIVAuaxvlt.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09827.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e7dd4036a8e8a82700041c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
      "fullname": "Felix Friedrich",
      "name": "felfri",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.14837",
      "authors": [
        {
          "_id": "6854ea7a7bc8d012d4ca998d",
          "name": "Chengzhi Xu",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca998e",
          "name": "Yuyang Wang",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca998f",
          "user": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "WaltonFuture",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-20T08:57:40.628Z",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca9990",
          "name": "Lichao Sun",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca9991",
          "name": "Weiran Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-15T14:10:16.000Z",
      "submittedOnDailyAt": "2025-06-20T03:28:58.032Z",
      "title": "Improving Code Generation through Structured Instructions in Charts",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "Recientemente, los modelos de lenguaje multimodal de diferencias múltiples (MLLMs) han ganado atención por su poderosa capacidad de comprensión visual. Aunque han logrado impresionantes resultados en diversas tareas visuales, no logran alcanzar el mejor rendimiento en la generación de códigos en gráficos. Este trabajo requiere la generación de código ejecutable para reproducir un gráfico, lo que implica una precisión en la comprensión visual y la traducción exacta de los elementos visuales a código estructurado. Cuando los MLLMs reciben directamente un prompt para estas tareas complejas, a menudo producen resultados insatisfactorios. Para enfrentar estas desafíos, proponemos {ChartIR}, que es un método iterativo de refactoring basado en comandos estructurados. Primero, distinguimos dos tareas: comprensión visual y traducción de código. Para la comprensión visual, diseñamos dos comandos estructurados: explicación y diferencia. El comando explicación captura los elementos visuales de la referencia gráfica, mientras que el comando diferencia caracteriza las diferencias entre la referencia gráfica y la gráfica generada. Estos comandos transforman las características visuales en expresiones lingüísticas y apoyan el proceso de traducción de código. Luego, dividimos la pipeline completa de generación de gráficos en dos etapas: la generación inicial de código y el refactoring iterativo, lo que permite mejorar gradualmente el salida final. A través de los resultados de los experimentos, demostramos que nuestro método supera otros métodos, alcanzando mejores rendimientos tanto en el modelo abierto Qwen2-VL como en el modelo cerrado GPT-4o.",
      "upvotes": 0,
      "discussionId": "6854ea7a7bc8d012d4ca9992",
      "ai_summary": "ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "visual understanding",
        "code translation",
        "structured instruction",
        "description instruction",
        "difference instruction",
        "language representations",
        "initial code generation",
        "iterative refinement",
        "Qwen2-VL",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-06-15T10:10:16.000Z",
    "title": "Improved Iterative Refinement for Chart-to-Code Generation via\n  Structured Instruction",
    "summary": "Recently, multimodal large language models (MLLMs) have attracted increasing\nresearch attention due to their powerful visual understanding capabilities.\nWhile they have achieved impressive results on various vision tasks, their\nperformance on chart-to-code generation remains suboptimal. This task requires\nMLLMs to generate executable code that can reproduce a given chart, demanding\nnot only precise visual understanding but also accurate translation of visual\nelements into structured code. Directly prompting MLLMs to perform this complex\ntask often yields unsatisfactory results. To address this challenge, we propose\n{ChartIR}, an iterative refinement method based on structured instruction.\nFirst, we distinguish two tasks: visual understanding and code translation. To\naccomplish the visual understanding component, we design two types of\nstructured instructions: description and difference. The description\ninstruction captures the visual elements of the reference chart, while the\ndifference instruction characterizes the discrepancies between the reference\nchart and the generated chart. These instructions effectively transform visual\nfeatures into language representations, thereby facilitating the subsequent\ncode translation process. Second, we decompose the overall chart generation\npipeline into two stages: initial code generation and iterative refinement,\nenabling progressive enhancement of the final output. Experimental results show\nthat, compared to other method, our method achieves superior performance on\nboth the open-source model Qwen2-VL and the closed-source model GPT-4o.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14837.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  }
]