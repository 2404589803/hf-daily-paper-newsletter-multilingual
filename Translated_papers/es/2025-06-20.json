[
  {
    "paper": {
      "id": "2506.14965",
      "authors": [
        {
          "_id": "68538be099bf39f9665c79b9",
          "name": "Zhoujun Cheng",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79ba",
          "name": "Shibo Hao",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bb",
          "name": "Tianyang Liu",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bc",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bd",
          "name": "Yutao Xie",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79be",
          "name": "Feng Yao",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79bf",
          "name": "Yuexin Bian",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c0",
          "name": "Yonghao Zhuang",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c1",
          "name": "Nilabjo Dey",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c2",
          "name": "Yuheng Zha",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c3",
          "name": "Yi Gu",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c4",
          "name": "Kun Zhou",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c5",
          "name": "Yuqi Wang",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c6",
          "name": "Yuan Li",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c7",
          "name": "Richard Fan",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c8",
          "name": "Jianshu She",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79c9",
          "name": "Chengqian Gao",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79ca",
          "name": "Abulhair Saparov",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cb",
          "name": "Haonan Li",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cc",
          "name": "Taylor W. Killian",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cd",
          "name": "Mikhail Yurochkin",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79ce",
          "name": "Zhengzhong Liu",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79cf",
          "name": "Eric P. Xing",
          "hidden": false
        },
        {
          "_id": "68538be099bf39f9665c79d0",
          "name": "Zhiting Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T20:24:00.000Z",
      "submittedOnDailyAt": "2025-06-20T06:25:47.447Z",
      "title": "Adiós a la revisión de la renovación del aprendizaje de la lógica de los LLM - Visión en el ámbito de los áreas personalizadas",
      "submittedOnDailyBy": {
        "_id": "6083902e1e36b13a64497d91",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6083902e1e36b13a64497d91/h4rGHMn2c6z5GesF0F6VU.png",
        "isPro": false,
        "fullname": "cheng",
        "user": "zhoujun",
        "type": "user"
      },
      "summary": "El aprendizaje por refuerzo (RL) ha aparecido como un potencial enfoque para mejorar la lógica de los modelos de lenguaje grandes (LLM), pero muchos esfuerzos abiertos se han centrado en matemáticas y código, limitando la comprensión de su aplicación a lógicas generales. La falta de señales de recompensa escalables de RL confiables en campos de lógica no relacionados sigue siendo un problema principal. Presentamos 'Guru', un corpus de RL de lógica que reúne 92K evidencias que cruzan 6 áreas de lógica: matemáticas, código, ciencia, lógica, simulación y tapas. Estos se aseguran de confiabilidad y eficacia a través de diseño, eliminación y filtrado de recompensas por área. Basándonos en 'Guru', revisamos sistemáticamente el RL en lógica de los LLM y observamos cambios significativos entre áreas. Por ejemplo, investigaciones anteriores afirman que RL extrae principalmente conocimientos ya existentes en modelos pre-entrenados, pero nuestros resultados muestran patrones más complejos: áreas que se ven frecuentemente durante el pre-entrenamiento (matemáticas, código, ciencia) pueden beneficiarse fácilmente en el aprendizaje por refuerzo cruzado, mientras que áreas con experiencia limitada durante el pre-entrenamiento (lógica, simulación, tapas) requieren aprendizaje dentro del mismo dominio para lograr mejoras significativas. Finalmente, presentamos 'Guru-7B' y 'Guru-32B', dos modelos que alcanzan los mejores rendimientos entre los modelos abiertos entrenados con RL en datos públicos, superando en 17 tareas de 6 áreas de lógica un 7.9% y 6.7%, respectivamente. Además, nuestros modelos demostraron mejorar eficazmente la capacidad de paso @k de los modelos base en tareas complejas no vistas en los datos de pre-entrenamiento. Los datos, modelos, códigos de entrenamiento y evaluación están disponibles en https://github.com/LLM360/Reasoning360, y fomentamos la realización de lógicas generales.",
      "upvotes": 6,
      "discussionId": "68538be099bf39f9665c79d1",
      "ai_summary": "Guru, a diverse RL reasoning corpus, highlights domain-specific training needs and demonstrates improved performance in complex tasks for RL-enhanced LLMs.",
      "ai_keywords": [
        "reinforcement learning",
        "large language model",
        "RL reasoning",
        "curated RL reasoning corpus",
        "domain-specific reward design",
        "dereplication",
        "filtering",
        "cross-domain RL training",
        "in-domain training",
        "Guru-7B",
        "Guru-32B",
        "Pass@k performance"
      ]
    },
    "publishedAt": "2025-06-17T16:24:00.000Z",
    "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective",
    "summary": "Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14965.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6083902e1e36b13a64497d91",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6083902e1e36b13a64497d91/h4rGHMn2c6z5GesF0F6VU.png",
      "fullname": "cheng",
      "name": "zhoujun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15154",
      "authors": [
        {
          "_id": "685393f499bf39f9665c79db",
          "name": "Anuradha Chopra",
          "hidden": false
        },
        {
          "_id": "685393f499bf39f9665c79dc",
          "name": "Abhinaba Roy",
          "hidden": false
        },
        {
          "_id": "685393f499bf39f9665c79dd",
          "name": "Dorien Herremans",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655431b2997379e9b0999d23/zAwOQ2zY8-8Eu7bUhFURQ.png"
      ],
      "publishedAt": "2025-06-18T05:51:36.000Z",
      "submittedOnDailyAt": "2025-06-20T04:32:59.096Z",
      "title": "Sonic Base: Captions Based on Dimensional Learning from Music Features",
      "submittedOnDailyBy": {
        "_id": "655431b2997379e9b0999d23",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
        "isPro": false,
        "fullname": "Dorien Herremans",
        "user": "dorienh",
        "type": "user"
      },
      "summary": "Detallado capturando es un aspecto de la música que refleja precisamente sus características, lo cual es una forma efectiva de enriquecer bases de datos de música y promover la investigación en inteligencia artificial de la música. En este artículo, se presenta un modelo de capturando de música multi-tarea llamado SonicVerse, que integra la generación de capturando con tareas de detección de características musicales (por ejemplo, detección de tono, detección de artistas). La principal contribución de este modelo es su arquitectura basada en proyecciones que transforman entradas sonoras en tokens de lenguaje. Además, utiliza auxiliares específicos para identificar características musicales, y los resultados de estos auxiliares también se convierten en tokens de lenguaje para fortalecer el entrada de capturando. Este marco de trabajo permite la generación de capturando detallado y rico para pequeñas secuencias musicales, así como la creación de descripciones detalladas que incluyen información temporal para obras de música más largas. El entrenamiento del modelo se realizó utilizando MIRFLEX (Extracción Modular de Características Musicales) y el conjunto de datos MusicBench, para anular las características musicales y generar pares de datos de sonido, capturando y características musicales. Los resultados de los experimentos muestran que la calidad y la detallade de los capturando generados se mejoran debido a estas características.",
      "upvotes": 2,
      "discussionId": "685393f599bf39f9665c79de",
      "ai_summary": "SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.",
      "ai_keywords": [
        "multi-task music captioning",
        "SonicVerse",
        "caption generation",
        "key detection",
        "vocals detection",
        "projection-based architecture",
        "language tokens",
        "auxiliary heads",
        "time-informed descriptions",
        "large-language model",
        "MusicBench dataset",
        "MIRFLEX",
        "music feature extractor"
      ]
    },
    "publishedAt": "2025-06-18T01:51:36.000Z",
    "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
    "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655431b2997379e9b0999d23/zAwOQ2zY8-8Eu7bUhFURQ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15154.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "655431b2997379e9b0999d23",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
      "fullname": "Dorien Herremans",
      "name": "dorienh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09827",
      "authors": [
        {
          "_id": "685519bb4f1add9d4c5c5cbd",
          "user": {
            "_id": "61a24fc72101184cfb29c965",
            "avatarUrl": "/avatars/e32aa61016caef50de28c16b30196799.svg",
            "isPro": false,
            "fullname": "Christoph Schuhmann",
            "user": "ChristophSchuhmann",
            "type": "user"
          },
          "name": "Christoph Schuhmann",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-20T08:20:12.243Z",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cbe",
          "name": "Robert Kaczmarczyk",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cbf",
          "name": "Gollam Rabby",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc0",
          "user": {
            "_id": "62e7dd4036a8e8a82700041c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
            "isPro": false,
            "fullname": "Felix Friedrich",
            "user": "felfri",
            "type": "user"
          },
          "name": "Felix Friedrich",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-20T08:57:36.090Z",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc1",
          "name": "Maurice Kraus",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc2",
          "name": "Kourosh Nadi",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc3",
          "name": "Huu Nguyen",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc4",
          "name": "Kristian Kersting",
          "hidden": false
        },
        {
          "_id": "685519bb4f1add9d4c5c5cc5",
          "name": "Sören Auer",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62e7dd4036a8e8a82700041c/tvkMYrIKiGhbIVAuaxvlt.png"
      ],
      "publishedAt": "2025-06-11T15:06:59.000Z",
      "submittedOnDailyAt": "2025-06-20T06:53:47.262Z",
      "title": "EmoNet-Voice: Niveles de detección de emociones en conversaciones: puntos de referencia verificados por expertos",
      "submittedOnDailyBy": {
        "_id": "62e7dd4036a8e8a82700041c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
        "isPro": false,
        "fullname": "Felix Friedrich",
        "user": "felfri",
        "type": "user"
      },
      "summary": "El desarrollo de los modos de texto y de generación de voz ha llevado a que se necesite un fuerte marco de referencia para evaluar la capacidad de comprensión emocional de los sistemas AI. Actualmente, los conjuntos de datos de reconocimiento de emociones de voz son frecuentemente basados en la microdiferencia de emociones, preocupaciones de privacidad o expresiones actuadas. En este artículo, se presenta un nuevo recurso para la detección de emociones en voz, el EmoNet-Voice. El EmoNet-Voice incluye un gran conjunto de datos previamente entrenado llamado EmoNet-Voice Big (11 voces, 40 tipos de emociones, 4 idiomas, más de 4,500 horas de voz) y un nuevo conjunto de datos de referencia que incluye anotaciones de expertos humanos llamado EmoNet-Voice Bench. El EmoNet-Voice tiene como objetivo evaluar el modo SER en un espectro muy detallado de 40 tipos de emociones. Se utilizaron las últimas tecnologías de generación de voz para preparar muestras de voz sintéticas que mimetizan escenarios en los que un actor lleva a cabo una emoción específica. Un punto importante es que estas muestras sintéticas fueron rigurosamente validadas por expertos en psicología y etiquetadas con etiquetas de intensidad de reconocimiento. Este enfoque sintético y protegido de la privacidad puede incluir estados emocionales sensibles que faltaban en los conjuntos de datos existentes. Finalmente, se presenta el nuevo modo Empathic Insight Voice, que establece un nuevo estándar de reconocimiento de emociones al realizar una alta colaboración con expertos humanos. De la evaluación desde el punto de vista actual de los modos, se han obtenido descubrimientos beneficiosos, como que emociones alto-arousantes (por ejemplo, enojo) pueden ser más fácilmente detectadas que estados bajo-arousantes (por ejemplo, concentración).",
      "upvotes": 1,
      "discussionId": "685519bb4f1add9d4c5c5cc6",
      "ai_summary": "EmoNet-Voice, a new resource with large pre-training and benchmark datasets, advances speech emotion recognition by offering fine-grained emotion evaluation with synthetic, privacy-preserving audio.",
      "ai_keywords": [
        "speech emotion recognition",
        "SER",
        "EmoNet-Voice",
        "EmoNet-Voice Big",
        "EmoNet-Voice Bench",
        "human expert annotations",
        "synthetic audio snippets",
        "psychology experts",
        "high-arousal emotions",
        "low-arousal states",
        "Empathic Insight Voice models"
      ]
    },
    "publishedAt": "2025-06-11T11:06:59.000Z",
    "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech\n  Emotion Detection",
    "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62e7dd4036a8e8a82700041c/tvkMYrIKiGhbIVAuaxvlt.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09827.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e7dd4036a8e8a82700041c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
      "fullname": "Felix Friedrich",
      "name": "felfri",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.14837",
      "authors": [
        {
          "_id": "6854ea7a7bc8d012d4ca998d",
          "name": "Chengzhi Xu",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca998e",
          "name": "Yuyang Wang",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca998f",
          "user": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "WaltonFuture",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-20T08:57:40.628Z",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca9990",
          "name": "Lichao Sun",
          "hidden": false
        },
        {
          "_id": "6854ea7a7bc8d012d4ca9991",
          "name": "Weiran Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-15T14:10:16.000Z",
      "submittedOnDailyAt": "2025-06-20T03:28:58.032Z",
      "title": "Construcción de Instrucciones Estructuradas para Mejorar la Generación de Código en Entrenamiento Iterativo de Gráficos",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "Recientemente, los modelos de lenguaje multimodal de difusión (MLLM) han sido objeto de mucha atención debido a su capacidad potente para entender visualmente. Sin embargo, mientras logran impresionantes resultados en muchas tareas visuales, no alcanzan los mejores rendimientos en la generación de código a partir de gráficos. Este trabajo plantea la necesidad de que los MLLM puedan recrear códigos implementables a partir de gráficos que proporcionan, y que se traduzcan precisamente los elementos visuales estructurados en código. Los MLLM no pueden recibir directamente un prompt para estas tareas complejas. Para resolver este desafío, proponemos {ChartIR}, un método iterativo de respuesta basado en comandos estructurados. Primero, distinguimos dos tareas: la comprensión visual y la traducción de código. Para la comprensión visual, diseñamos dos comandos estructurados: explicar y diferenciar. El comando explicar captura los elementos visuales de la referencia gráfico, mientras que el comando diferenciar destaca las diferencias entre la referencia gráfico y el gráfico generado. Estos comandos convierten las características visuales en expresiones lingüísticas y promueven eficientemente el proceso de traducción de código. Luego, dividimos la cadena de trabajo completa de creación de gráficos en dos etapas: la generación inicial de código y el rinferenciación iterativa, así como la mejora evolutiva del final. Los resultados de los experimentos muestran que, en comparación con otros métodos, nuestro enfoque logra mejores resultados tanto en el modelo abierto Qwen2-VL como en el modelo cerrado GPT-4o.",
      "upvotes": 0,
      "discussionId": "6854ea7a7bc8d012d4ca9992",
      "ai_summary": "ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "visual understanding",
        "code translation",
        "structured instruction",
        "description instruction",
        "difference instruction",
        "language representations",
        "initial code generation",
        "iterative refinement",
        "Qwen2-VL",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-06-15T10:10:16.000Z",
    "title": "Improved Iterative Refinement for Chart-to-Code Generation via\n  Structured Instruction",
    "summary": "Recently, multimodal large language models (MLLMs) have attracted increasing\nresearch attention due to their powerful visual understanding capabilities.\nWhile they have achieved impressive results on various vision tasks, their\nperformance on chart-to-code generation remains suboptimal. This task requires\nMLLMs to generate executable code that can reproduce a given chart, demanding\nnot only precise visual understanding but also accurate translation of visual\nelements into structured code. Directly prompting MLLMs to perform this complex\ntask often yields unsatisfactory results. To address this challenge, we propose\n{ChartIR}, an iterative refinement method based on structured instruction.\nFirst, we distinguish two tasks: visual understanding and code translation. To\naccomplish the visual understanding component, we design two types of\nstructured instructions: description and difference. The description\ninstruction captures the visual elements of the reference chart, while the\ndifference instruction characterizes the discrepancies between the reference\nchart and the generated chart. These instructions effectively transform visual\nfeatures into language representations, thereby facilitating the subsequent\ncode translation process. Second, we decompose the overall chart generation\npipeline into two stages: initial code generation and iterative refinement,\nenabling progressive enhancement of the final output. Experimental results show\nthat, compared to other method, our method achieves superior performance on\nboth the open-source model Qwen2-VL and the closed-source model GPT-4o.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14837.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  }
]