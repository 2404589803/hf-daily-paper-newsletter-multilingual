[
  {
    "paper": {
      "id": "2502.12900",
      "authors": [
        {
          "_id": "67b54851b986e35c41e063da",
          "user": {
            "_id": "66975b9f8031bf92b428e138",
            "avatarUrl": "/avatars/3254281a7bac1c8ddde1d6bc7e518b2f.svg",
            "isPro": false,
            "fullname": "Yuhao Zhang",
            "user": "Yoohao",
            "type": "user"
          },
          "name": "Yuhao Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-19T02:56:18.848Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063db",
          "user": {
            "_id": "66597f2cf769c3c443b7cf41",
            "avatarUrl": "/avatars/735cc8aa430748d20ca7312c72b1eaf1.svg",
            "isPro": false,
            "fullname": "Chihang Lau",
            "user": "puccho",
            "type": "user"
          },
          "name": "Zhiheng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:05.678Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063dc",
          "user": {
            "_id": "668e7f46c243a12604035758",
            "avatarUrl": "/avatars/35bd20032fafb7d7603266cf9a72d1e0.svg",
            "isPro": false,
            "fullname": "Fan Bu",
            "user": "FanBuCUHK",
            "type": "user"
          },
          "name": "Fan Bu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:42:08.544Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063dd",
          "user": {
            "_id": "67b587c8882e49771f610b51",
            "avatarUrl": "/avatars/aecfb38b44141b8284416fc261692909.svg",
            "isPro": false,
            "fullname": "Ruiyu Zhang",
            "user": "PhoenixAxis",
            "type": "user"
          },
          "name": "Ruiyu Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:42:14.866Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063de",
          "user": {
            "_id": "637c6703ca8542a0ba900ccb",
            "avatarUrl": "/avatars/288ed63a1efa566c3f01e850c6ba5dd5.svg",
            "isPro": false,
            "fullname": "Wang",
            "user": "Benyou",
            "type": "user"
          },
          "name": "Benyou Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:42:23.845Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063df",
          "name": "Haizhou Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T14:36:39.000Z",
      "title": "Soundwave: El menor es el mayor.",
      "summary": "En el actual endpoint, los modelos de lenguaje de lenguaje a lenguaje (LLMs) hasta el endpoint se entrenan generalmente con grandes cantidades de datos etiquetados, y la eficiencia de la entrenamiento de datos no se discute profundamente. Nosotros abordamos enfocadamente dos problemas básicos entre sonido y texto: el espacio de representación y la incoherencia de la longitud de secuencia. Proponemos una nueva arquitectura llamada Soundwave y estrategias de entrenamiento eficientes para resolver estos problemas. Como resultado, Soundwave muestra un desempeño superior a Qwen2-Audio, la líder en traducción de sonido y tareas de sonido en AIR-Bench. Además, utiliza solo un 1/50 de los datos de entrenamiento. La análisis realizado muestra que Soundwave mantiene su inteligencia durante el proceso de transformación. Este proyecto puede acceder a https://github.com/FreedomIntelligence/Soundwave.",
      "upvotes": 47,
      "discussionId": "67b54852b986e35c41e06426"
    },
    "publishedAt": "2025-02-19T00:22:36.628Z",
    "title": "Soundwave: Less is More for Speech-Text Alignment in LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66975b9f8031bf92b428e138",
      "avatarUrl": "/avatars/3254281a7bac1c8ddde1d6bc7e518b2f.svg",
      "fullname": "Yuhao Zhang",
      "name": "Yoohao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11564",
      "authors": [
        {
          "_id": "67b40f93aba9e111862052ab",
          "user": {
            "_id": "65e5bd4568234ef5d6decadc",
            "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
            "isPro": false,
            "fullname": "Jaehyeong Jo",
            "user": "harryjo97",
            "type": "user"
          },
          "name": "Jaehyeong Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:27.544Z",
          "hidden": false
        },
        {
          "_id": "67b40f93aba9e111862052ac",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T08:54:29.000Z",
      "title": "Modelado de lenguaje en modelos de expansión continua",
      "summary": "Los modelos de difusión han aparecido como prometedores alternativas a los modelos de regresión automática en la modelación de datos discretos. Sin embargo, los modelos de difusión que funcionan directamente en el espacio de datos discretos pierden señales entre estados discretos y no pueden aprovechar completamente la fuerza de mejoras continuas. Los modelos de difusión continuos actuales para datos discretos están limitados en su rendimiento comparado con los métodos de acceso discreto, y las conexiones inciertas entre ellos impiden el desarrollo de modelos de difusión para datos discretos. En este estudio, proponemos un modelo de difusión continuo que integra la estructura de la distribución de clasificación discreta. Establecemos la conexión entre la difusión discreta y los flujos continuos en la variedad estadística, y proponemos un diseño sencillo de proceso de difusión para generalizar los modelos de difusión discretos anteriores basado en esta similitud. Además, proponemos un marco de entrenamiento sin limitaciones de simulación basado en la simetría radial y un método sencillo para resolver problemas de variedades de alta dimensión. Los experimentos detallados en los benchmarks de modelos de lenguaje muestran que nuestro método supera a los modelos de difusión discretos actuales y se acerca al rendimiento de los modelos de regresión automática. El código está disponible en https://github.com/harryjo97/RDLM.",
      "upvotes": 30,
      "discussionId": "67b40f94aba9e111862052d5"
    },
    "publishedAt": "2025-02-18T22:43:02.567Z",
    "title": "Continuous Diffusion Model for Language Modeling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11564.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65e5bd4568234ef5d6decadc",
      "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
      "fullname": "Jaehyeong Jo",
      "name": "harryjo97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11079",
      "authors": [
        {
          "_id": "67b40141ad717fe02e188c1a",
          "user": {
            "_id": "63a950ac3453852ef5394178",
            "avatarUrl": "/avatars/48a5e537b10e2247a17e63502e3201a6.svg",
            "isPro": false,
            "fullname": "Lijie Liu",
            "user": "liulj13",
            "type": "user"
          },
          "name": "Lijie Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:42.570Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1b",
          "user": {
            "_id": "657ab4705e1c941f4c2f7877",
            "avatarUrl": "/avatars/c450f81f83dd0436ae120ab15616c4f7.svg",
            "isPro": false,
            "fullname": "Tianxiang Ma",
            "user": "Grayson111",
            "type": "user"
          },
          "name": "Tianxiang Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:45:00.117Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1c",
          "user": {
            "_id": "63b415037af2e415f2599c18",
            "avatarUrl": "/avatars/4afbe7d6d05a702f1beeed9c53e78153.svg",
            "isPro": false,
            "fullname": "Bingchuan Li",
            "user": "lbc402",
            "type": "user"
          },
          "name": "Bingchuan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:47:57.441Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1d",
          "user": {
            "_id": "6304e2dabad6ce7fc0287d57",
            "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
            "isPro": false,
            "fullname": "Zhuowei_Chen",
            "user": "ZhuoweiChen",
            "type": "user"
          },
          "name": "Zhuowei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:47:50.995Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1e",
          "name": "Jiawei Liu",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1f",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c20",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T11:02:50.000Z",
      "title": "Fantom: Creación de vídeos de acuerdo con temas de arrayenamiento de modalidades cruzadas",
      "summary": "El modelo básico de generación de vídeos continuos ha evolucionado para diversas aplicaciones, pero la generación de vídeos con un tema coincidente sigue en un estado de exploración. Esto se denomina \"Subject-to-Video\", que implica extraer elementos temáticos de una imagen de referencia y generar vídeos que coincidan con el tema según indicaciones de contexto. Creemos que el esencia de Subject-to-Video consiste en equilibrar el balance entre el contexto y el programación de dos modalidades, así como en responder a ambos contexto y contenido visual en un nivel profundo. Por ello, proponemos el framework integrado de generación de vídeos llamado Phantom. Basado en la arquitectura de vídeos desde el contexto existente, re-diseñamos el modelo de injección de imagen de contexto y realizamos una entrenamiento más profundo sobre los tripletes de vídeos de imagen de contexto, para lograr una correspondencia entre el contexto y la imagen en diferentes modalidades. En particular, enfatizamos la coincidencia temática en la generación humana, incluyendo la mejora de la generación de vídeos que mantienen la identidad existente. El sitio web del proyecto está disponible en https://phantom-video.github.io/Phantom/.",
      "upvotes": 27,
      "discussionId": "67b40144ad717fe02e188cb2"
    },
    "publishedAt": "2025-02-18T21:56:39.407Z",
    "title": "Phantom: Subject-consistent video generation via cross-modal alignment",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63a950ac3453852ef5394178/HuVZ5d9xTlI4R1onRv_F5.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a950ac3453852ef5394178",
      "avatarUrl": "/avatars/48a5e537b10e2247a17e63502e3201a6.svg",
      "fullname": "Lijie Liu",
      "name": "liulj13",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12464",
      "authors": [
        {
          "_id": "67b55b2cc92c4aa82c13562d",
          "user": {
            "_id": "64ad5f59b7e4b2c1ce47eb43",
            "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg",
            "isPro": false,
            "fullname": "Seanie Lee",
            "user": "Seanie-lee",
            "type": "user"
          },
          "name": "Seanie Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:53.341Z",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c13562e",
          "name": "Dong Bok Lee",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c13562f",
          "name": "Dominik Wagner",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135630",
          "name": "Minki Kang",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135631",
          "user": {
            "_id": "63a9379e2e05ca32e352d93b",
            "avatarUrl": "/avatars/6cda37befc873a92ed6d5dcba507954a.svg",
            "isPro": false,
            "fullname": "Haebin Seong",
            "user": "hbseong",
            "type": "user"
          },
          "name": "Haebin Seong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:51:37.783Z",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135632",
          "name": "Tobias Bocklet",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135633",
          "name": "Juho Lee",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135634",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T02:51:17.000Z",
      "title": "SafeRoute: Selección de modelos adaptativos para eficiencia y precisión de seguridad\nGuía de estándares de seguridad para modelos de lenguaje de gran escala",
      "summary": "En la realidad de dispositivos de visualización, la introducción de modelos de lenguaje grandes (LLMs) requiere la implementación de fuertes modelos de control de seguridad. Estos modelos detectan y bloquean consultas de usuarios peligrosas. Sin embargo, los grandes modelos de control de seguridad, aunque efectivos, tienen un costo computacional elevado. Para mitigar este problema, se utilizan pequeños modelos de control de seguridad. Sin embargo, en casos \"difíciles\" de ejemplo, el rendimiento de estos pequeños modelos es inferior al de los grandes modelos utilizados para predicciones precisas. Hemos encontrado que muchos datos pueden ser procesados confiablemente con pequeños modelos, y que solo se necesita el cálculo de los grandes modelos para un pequeño porcentaje de los datos. Basándonos en esto, proponemos SafeRoute, un rotor binario. Este método aplica selectivamente los grandes modelos de control de seguridad solo en casos de datos difíciles, optimizando así el rendimiento y manteniendo la precisión. Los resultados de experimentos en varios conjuntos de datos de prueba muestran que nuestra selección adaptativa de modelos mejora significativamente el equilibrio entre costo computacional y rendimiento de seguridad, superando a los referencias basales relacionados.",
      "upvotes": 24,
      "discussionId": "67b55b2dc92c4aa82c13568b"
    },
    "publishedAt": "2025-02-18T23:23:34.214Z",
    "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ad5f59b7e4b2c1ce47eb43/ZEq_vSLjsXuPX3O-TWIpE.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12464.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ad5f59b7e4b2c1ce47eb43",
      "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg",
      "fullname": "Seanie Lee",
      "name": "Seanie-lee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13131",
      "authors": [
        {
          "_id": "67b5461d29cc269e5a4eb823",
          "name": "Feng Luo",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb824",
          "user": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "isPro": false,
            "fullname": "Rui Yang",
            "user": "Ray2333",
            "type": "user"
          },
          "name": "Rui Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:23.095Z",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb825",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb826",
          "user": {
            "_id": "634b9914dcf125e4da02498b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634b9914dcf125e4da02498b/crRgFroWq5U6XWtvlTXSZ.jpeg",
            "isPro": false,
            "fullname": "Chunyuan Deng",
            "user": "CharlesDDDD",
            "type": "user"
          },
          "name": "Chunyuan Deng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:56:33.053Z",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb827",
          "name": "Jiarui Yao",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb828",
          "name": "Jingyan Shen",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb829",
          "user": {
            "_id": "6719d581a6cad13741b8bc7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719d581a6cad13741b8bc7f/w4EttqfXRgWZJc6HpYOS9.jpeg",
            "isPro": false,
            "fullname": "Huan Zhang",
            "user": "huanzhang12",
            "type": "user"
          },
          "name": "Huan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:52:47.329Z",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb82a",
          "name": "Hanjie Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:55:26.000Z",
      "title": "Reevaluación de la aprendizaje de diferentes preferencias humanas mediante el análisis de componentes principales",
      "summary": "Entender las ociosidades humanas es crucial para el desarrollo de modelos básicos y la construcción de sistemas AI con características. Sin embargo, las ociosidades son fundamentalmente diversas y complejas, y los modelos de recompensa tradicionales tienen dificultades para comprender su gama total. Además, los datos específicos de ociosidades son útiles pero su recopilación es costosa y su expansión es difícil. En este artículo, utilizamos una nueva aproximación para extraer diferentes ociosidades humanas: los modelos de recompensa desglosadas (DRMs). Nuestro enfoque principal es representar las ociosidades humanas como vectores y analizarlas mediante Análisis de Componentes Principales (PCA). Construyendo un conjunto de datos con la diferencia de vectores ocultos de respuestas aceptadas y rechazadas, los DRMs identifican vectores ortogonales que detectan diferentes aspectos de las ociosidades. Estos modelos de recompensa desglosadas pueden combinarse flexiblemente según las necesidades de los usuarios y son una alternativa a los modelos de recompensa tradicionales, que son explicables y expandibles. Los DRMs extraen eficazmente dimensiones significativas de las ociosidades (por ejemplo, eficacia, seguridad, excelencia) y pueden adaptarse a nuevos usuarios sin necesidad de entrenamiento adicional. Estos resultados revelan que los DRMs juegan un papel crucial como un fuerte marco para la alineación explicable de LLMs con características.",
      "upvotes": 23,
      "discussionId": "67b5461f29cc269e5a4eb8bc"
    },
    "publishedAt": "2025-02-18T21:59:45.466Z",
    "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13131.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d45451c34a346181b130dd",
      "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
      "fullname": "Rui Yang",
      "name": "Ray2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13143",
      "authors": [
        {
          "_id": "67b546c0d8a1eac02c605f6a",
          "user": {
            "_id": "63c3e8abc7d7f4c63a515a02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
            "isPro": false,
            "fullname": "Zekun Qi",
            "user": "qizekun",
            "type": "user"
          },
          "name": "Zekun Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:21.001Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6b",
          "user": {
            "_id": "65f9533b136fb8ddbd14e1fa",
            "avatarUrl": "/avatars/d88f75da0448093ccd1babba2a37d73f.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "WenyaoZhang",
            "type": "user"
          },
          "name": "Wenyao Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T10:08:31.789Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6c",
          "user": {
            "_id": "66bde456198f9d79f2be2d17",
            "avatarUrl": "/avatars/8c349aecb8a3a7cd7ef9d69e94eca8bd.svg",
            "isPro": false,
            "fullname": "Yufei Ding",
            "user": "YufeiD",
            "type": "user"
          },
          "name": "Yufei Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T10:08:57.294Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6d",
          "user": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "isPro": false,
            "fullname": "Runpei Dong",
            "user": "RunpeiDong",
            "type": "user"
          },
          "name": "Runpei Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:18.622Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6e",
          "name": "Xinqiang Yu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6f",
          "name": "Jingwen Li",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f70",
          "name": "Lingyun Xu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f71",
          "name": "Baoyu Li",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f72",
          "name": "Xialin He",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f73",
          "name": "Guofan Fan",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f74",
          "name": "Jiazhao Zhang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f75",
          "name": "Jiawei He",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f76",
          "name": "Jiayuan Gu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f77",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f78",
          "name": "Kaisheng Ma",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f79",
          "name": "Zhizheng Zhang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f7a",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f7b",
          "name": "Li Yi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:02.000Z",
      "title": "Hasta ahora: las puentes de dirección basadas en lenguaje conectan la teoría espacial y la manipulación del objeto.",
      "summary": "La capacidad de entender el espacio es uno de los componentes importantes de la inteligencia artificial concreta, ayudando a los robots a entender y interactuar con su entorno. Los recientes avances han mejorado la capacidad de los VLM para reconocer las posiciones de los objetos y comprender las relaciones de posición, pero aún no tienen la capacidad de determinar la dirección de los objetos. Para resolver esto, es necesario una inferencia geométrica y una representación de la dirección con intuición y expresividad. En este contexto, la naturaleza humana proporciona un espacio de representación más flexible que los marcos estándar, y se propone que sea particularmente adecuada para sistemas de robots que escuchan comandos. En este artículo, se presenta una metodología para definir la dirección de los objetos en naturaleza humana (por ejemplo, la dirección 'PLUG IN' de un USB o la de 'HANDLE' de una cuchara). Para apoyar esto, se construye un conjunto de datos grande de modelos 3D llamado OrientText300K, cuyo objetivo es combinar el semántico funcional y la comprensión geométrica. Incluyendo la dirección contextual en los sistemas VLM, se logra que los robots puedan generar acciones con restricciones de posición y dirección. Los experimentos de expansión en simulación y en el mundo real, como en Open6DOR (mostrando una precisión del 48.7%) y SIMPLER (mostrando una precisión del 74.9%), demuestran una significativa mejora en la capacidad de acción de los robots.",
      "upvotes": 22,
      "discussionId": "67b546c5d8a1eac02c606090"
    },
    "publishedAt": "2025-02-18T21:51:33.957Z",
    "title": "SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c3e8abc7d7f4c63a515a02",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
      "fullname": "Zekun Qi",
      "name": "qizekun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13145",
      "authors": [
        {
          "_id": "67b54b04bd51b4e46e39d287",
          "user": {
            "_id": "6577073fc2bf55b1f6bafb49",
            "avatarUrl": "/avatars/58803398b1a918b7570db17893e65122.svg",
            "isPro": false,
            "fullname": "liao",
            "user": "LegendBC",
            "type": "user"
          },
          "name": "Bencheng Liao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:00.934Z",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d288",
          "name": "Hongyuan Tao",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d289",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28a",
          "user": {
            "_id": "646b3db131968a60a01e4cf5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
            "isPro": false,
            "fullname": "Tianheng Cheng",
            "user": "wondervictor",
            "type": "user"
          },
          "name": "Tianheng Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:58.351Z",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28b",
          "name": "Yingyue Li",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28c",
          "name": "Haoran Yin",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28d",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28e",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:57.000Z",
      "title": "Marcimoderal Maba: Espectro de Estados Marcimoderal para Modelos Decodificadores en Procesos de Calor en 2D Lineales",
      "summary": "Recientemente, los modelos de lenguaje multimodal de alto nivel (MLLM) han logrado un desempeño sorprendente, aunque se han visto incrementos en la complejidad computacional y en la demanda de caché de clave-valor, además de problemas de implementación debido a la dependencia de los encoderes visuales. Proponemos un marco de desarrollo llamado mmMamba, que permite la creación de modelos de espacios de estados multimodal con una complejidad de cálculo intermedia, para mejorar los MLLM actuales. Este enfoque permite transformar directamente a los MLLM que solo tienen un decodificador entrenado en una arquitectura basada en RNN o que no requiere un encoder visual, con una complejidad de cálculo lineal. Además, proponemos una estrategia de punto final para crear Mamba a partir de Transformer y un receta de tres pasos para transmitir eficientemente el conocimiento desde Transformer a Mamba. Esta metodología permite soportar arquitecturas híbridas flexibles a través de la combinación de capas de Transformer y Mamba, así como ajustar la continuidad y el rendimiento. En HoVLE, el mmMamba-linear, después de ser optimizado, presenta un rendimiento competitivo con los VLMs de complejidad lineal y ordenada, mientras que el mmMamba-hybrid alcanza un rendimiento más alto y se acerca a los capacidades de HoVLE. En un conjunto de 103K tokens, el mmMamba-linear ofrece un aumento de velocidad del 20.6 veces y una reducción del 75.8% en la memoria de GPU en comparación con HoVLE, mientras que el mmMamba-hybrid logra un aumento de velocidad del 13.5 veces y una reducción del 60.2% en el uso de memoria. Los códigos y modelos están disponibles en https://github.com/hustvl/mmMamba.",
      "upvotes": 18,
      "discussionId": "67b54b05bd51b4e46e39d2bb"
    },
    "publishedAt": "2025-02-18T22:08:27.750Z",
    "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13145.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6577073fc2bf55b1f6bafb49",
      "avatarUrl": "/avatars/58803398b1a918b7570db17893e65122.svg",
      "fullname": "liao",
      "name": "LegendBC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11433",
      "authors": [
        {
          "_id": "67b54a644508bd0617598c21",
          "name": "Guojun Xiong",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c22",
          "name": "Zhiyang Deng",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c23",
          "name": "Keyi Wang",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c24",
          "name": "Yupeng Cao",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c25",
          "name": "Haohang Li",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c26",
          "name": "Yangyang Yu",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c27",
          "name": "Xueqing Peng",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c28",
          "name": "Mingquan Lin",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c29",
          "name": "Kaleb E Smith",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2a",
          "name": "Xiao-Yang Liu",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2b",
          "user": {
            "_id": "63b58ed5889aa6707f0bb0f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
            "isPro": true,
            "fullname": "Jimin Huang",
            "user": "jiminHuang",
            "type": "user"
          },
          "name": "Jimin Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:03.181Z",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2c",
          "name": "Sophia Ananiadou",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2d",
          "name": "Qianqian Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T04:45:53.000Z",
      "title": "FLAG-Trader: Aprendizaje por Reforzamiento con Base de Gradiente para el Financiamiento de Inversiones con Fusion LLM-Agent",
      "summary": "El ajuste a múltiples tipos de datos financieros de un modelo de lenguaje grande (LLMs) muestra un desempeño impresionante en diversas operaciones financieras. Sin embargo, para escenarios graduales y objetivos en un mercado financiero interactivo, se requiere un enfoque complejo para el output, lo que puede ser difícil para mejorar las decisiones. Para enfrentar esto, se propone FLAG-Trader. FLAG-Trader es una serie de arquitecturas que integran el procesamiento de lenguaje (a través de LLMs) y la optimización de políticas basada en gradientes de aprendizaje por refuerzo (RL). Los LLMs ajustados utilizan conocimientos existentes y funcionan como redes de políticas de manera eficiente con parámetros ajustados en datos financieros. Mediante la optimización de políticas basada en la compensación del trading, nuestro marco mejora el desempeño de LLMs en el trading y puede mejorar los resultados de otras tareas con bases de datos financieras. Estos mejoramientos se demuestran con datos experimentales ampliados.",
      "upvotes": 18,
      "discussionId": "67b54a654508bd0617598c7e"
    },
    "publishedAt": "2025-02-18T22:06:19.200Z",
    "title": "FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/2C9mhT-1Qz14hik7sxjf2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11433.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b58ed5889aa6707f0bb0f4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
      "fullname": "Jimin Huang",
      "name": "jiminHuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.09245",
      "authors": [
        {
          "_id": "67b57a993d4f319f1fa9424b",
          "name": "Gleb Gerasimov",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424c",
          "user": {
            "_id": "63ed5676684767daecac6f8a",
            "avatarUrl": "/avatars/d0e4a715f9c3fb6d74c183bab751ec35.svg",
            "isPro": false,
            "fullname": "Yaroslav Aksenov",
            "user": "yaraksen",
            "type": "user"
          },
          "name": "Yaroslav Aksenov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:41.123Z",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424d",
          "user": {
            "_id": "60b364e7f88532cd79eaff7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
            "isPro": false,
            "fullname": "Nikita Balagansky",
            "user": "elephantmipt",
            "type": "user"
          },
          "name": "Nikita Balagansky",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:33:26.858Z",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424e",
          "name": "Viacheslav Sinii",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424f",
          "user": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "isPro": false,
            "fullname": "Daniil Gavrilov",
            "user": "kefirski",
            "type": "user"
          },
          "name": "Daniil Gavrilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:43.143Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-13T12:00:50.000Z",
      "title": "No estás utilizando completamente la expresividad de Transformer.",
      "summary": "A diferencia de los RNN, los Transformers pueden procesar directamente todos los tokens anteriores. Sin embargo, los Transformers estándar solo utilizan la representación de la capa anterior. Este artículo muestra que estas decisiones de diseño pueden causar la destrucción de la representación y limitar el rendimiento óptimo. Para resolver este problema, se presenta Layer-Integrated Memory (LIMe). LIMe expande la representación al permitir acceso a los estados ocultos de las capas anteriores mientras mantiene el estado de memoria global del modelo. A través de experimentos ampliados que combinan diferentes arquitecturas y estructuras de búsqueda, LIMe muestra un mejoramiento uniforme en una variedad de tareas. Además, el análisis de la dinámica de las representaciones aprendidas y la exploración de circuitos en profundidad demuestran cómo LIMe integra información entre capas, proporcionando una dirección guía para futuras investigaciones.",
      "upvotes": 12,
      "discussionId": "67b57a9a3d4f319f1fa94274"
    },
    "publishedAt": "2025-02-19T03:03:51.930Z",
    "title": "You Do Not Fully Utilize Transformer's Representation Capacity",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63ed5676684767daecac6f8a/tZDsnW0gjHoYCpbZ-wwJi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09245.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ed5676684767daecac6f8a",
      "avatarUrl": "/avatars/d0e4a715f9c3fb6d74c183bab751ec35.svg",
      "fullname": "Yaroslav Aksenov",
      "name": "yaraksen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13130",
      "authors": [
        {
          "_id": "67b5625fb27eb6046b2ceec5",
          "name": "Jianwei Yang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec6",
          "name": "Reuben Tan",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec7",
          "name": "Qianhui Wu",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec8",
          "name": "Ruijie Zheng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec9",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceeca",
          "name": "Yongyuan Liang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecb",
          "name": "Yu Gu",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecc",
          "name": "Mu Cai",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecd",
          "name": "Seonghyeon Ye",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceece",
          "name": "Joel Jang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecf",
          "name": "Yuquan Deng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceed0",
          "name": "Lars Liden",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceed1",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:55:21.000Z",
      "title": "Magma: Modelo básico de agente AI de DamoPai",
      "summary": "El magma es una extensión importante del modelo de lenguaje visual (VL), capaz de mantener su capacidad de comprensión visual (capacidad cognitiva de la visión) mientras adquiere habilidades de planificación y acción en el espacio visual (capacidad cognitiva del tiempo espacial). Además, puede procesar tareas de navegación de interfaces de usuario (UI) hasta la manipulación de robots. Para adquirir estas habilidades de out-of-the-box, el magma entrena a gran escala con conjuntos de datos variados, desde imágenes y videos hasta datos de robots. En las imágenes, etiqueta objetos visuales que pueden ser accionados (por ejemplo, botones clicables en una interfaz gráfica) con un Set-of-Mark (SoM), y en los videos, etiqueta el movimiento de objetos (por ejemplo, las trazas de las manos de una persona o las de las brazos de un robot) con un Trace-of-Mark (ToM). Los experimentos extendidos muestran que el SoM y el ToM se integran bien, ayudando a la adquisición de la capacidad cognitiva del tiempo espacial del modelo de magma. Esto forma la base para una amplia gama de tareas, como se muestra en la figura 1. En particular, el magma supera a modelos anteriores específicos para estas tareas y obtiene nuevos resultados líder en la navegación de UI y la manipulación de robots. En diferentes tareas relacionadas con imágenes y videos, el magma también es popular comparado con modelos de gran escala, y es adecuado para entrenamiento con conjuntos de datos similares. Nuestro modelo y código están disponibles y se pueden reproducir de manera reproducible siguiendo el siguiente URL: https://microsoft.github.io/Magma",
      "upvotes": 11,
      "discussionId": "67b56265b27eb6046b2cf08f"
    },
    "publishedAt": "2025-02-18T23:51:36.910Z",
    "title": "Magma: A Foundation Model for Multimodal AI Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13130.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6142
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12513",
      "authors": [
        {
          "_id": "67b545fd88527668fa8bcc14",
          "name": "Tiancheng Gu",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc15",
          "name": "Kaicheng Yang",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc16",
          "name": "Chaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc17",
          "name": "Yin Xie",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc18",
          "name": "Xiang An",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc19",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1a",
          "name": "Dongnan Liu",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1b",
          "name": "Weidong Cai",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1c",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T03:58:38.000Z",
      "title": "RealSyn: Eficaz y expandible paradigma de documento transformador multimodal",
      "summary": "Tras preentrenamiento en amplias parejas de imagen y texto, CLIP (Contrastive Language-Image Pre-training) muestra un rendimiento esperado en diferentes marcos de referencia, pero no se aprovecha plenamente la gran cantidad de datos no pareadas para el aprendizaje de representaciones de lenguaje visual. Para maximizar la utilización de estos datos no pareados, primero se construye un pipeline de extracción de datos de la realidad y se extraen imágenes de alta calidad y textos. Luego, se diseña un método de búsqueda jerárquico para asociar eficientemente cada imagen con textos textualmente asociados. Además, se propone un módulo de expansión sintáctica para generar textos sintéticos que fomenten la mejora de información visual. También se introduce una estrategia de muestreo de equilibrio sintáctico para aumentar la diversidad del conjunto de datos y mejorar el aprendizaje de conceptos de cola larga. Basándose en estas innovaciones, se construye el conjunto de datos RealSyn, que combina textos reales y sintéticos, y se ofrece en tres escalas: 15M, 30M y 100M. Las pruebas de extensión demuestran claramente que el preentrenamiento en RealSyn promueve el aprendizaje de representaciones de lenguaje visual y muestra una fuerte escalabilidad. Los modelos preentrenados en RealSyn alcanzan los mejores rendimientos en varias tareas posteriores. Los pesos del conjunto de datos RealSyn y los modelos preentrenados están disponibles en https://github.com/deepglint/RealSyn para fomentar futuras investigaciones.",
      "upvotes": 10,
      "discussionId": "67b545fe88527668fa8bcc65"
    },
    "publishedAt": "2025-02-18T21:52:22.326Z",
    "title": "RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12513.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12859",
      "authors": [
        {
          "_id": "67b576aa489d68b981e086ad",
          "name": "Chenxing Wei",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086ae",
          "name": "Yao Shu",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086af",
          "name": "Mingwen Ou",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086b0",
          "name": "Ying Tiffany He",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086b1",
          "name": "Fei Richard Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T13:46:47.000Z",
      "title": "PAFT: Ajuste micro de prompts irrelevantes",
      "summary": "Los modelos de lenguaje de alto nivel (LLMs) se adaptan a tareas posteriores después de un ajuste fino, pero este ajuste puede ser afectado significativamente por pequeñas variaciones en los prompts, lo que puede destruir la robustez de los prompts. Para abordar este problema, se propone el Ajuste Fino Sin Saber del Prompt (PAFT). El PAFT es un enfoque sencillo y efectivo que ajusta dinamicamente el prompt durante el ajuste fino. Así, el modelo aprende los principios que fundamentan el problema y evita la sobreajuste a la representación específica de un prompt. El PAFT funciona en dos etapas: primero, se construyen diferentes conjuntos de candidatos de prompts sintéticos y significativos. En segundo lugar, se seleccionan aleatoriamente los prompts para generar entradas de entrenamiento dinámicas. En experimentos amplios con diferentes conjuntos de datos y modelos de LLMs, se demostró que los modelos entrenados con PAFT muestran una fuerte robustez y generalización hacia prompts no vistos. Este aumento de robustez mejora tanto el rendimiento del modelo como la velocidad de inferencia, mientras mantiene la eficiencia del entrenamiento. Los experimentos de desaparición del prompt confirman la efectividad del PAFT.",
      "upvotes": 8,
      "discussionId": "67b576aa489d68b981e08708"
    },
    "publishedAt": "2025-02-19T01:21:54.836Z",
    "title": "PAFT: Prompt-Agnostic Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12859.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ed3051492a7f35db21fea2",
      "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
      "fullname": "Chenxing Wei",
      "name": "kittttttt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12170",
      "authors": [
        {
          "_id": "67b5434f2b2ec6908fffe75e",
          "name": "Da Xiao",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe75f",
          "name": "Qingye Meng",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe760",
          "name": "Shengping Li",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe761",
          "name": "Xingyuan Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-13T10:26:27.000Z",
      "title": "MUDDFormer: Transformer's Residual Boltneck Destruction via Multi-Directional Dynamic Tight Coupling",
      "summary": "Proponemos la conexión MUltiway Dynamic Dense (MUDD) para resolver los límites de las conexiones residuales y mejorar el flujo de información entre capas en Transformers de una manera sencilla y efectiva. A diferencia de los métodos de conexión densa existentes, MUDD genera pesos de conexión dinámicamente para cada flujo de entrada separado (consulta, clave, valor o residual) en cada posición de la secuencia de cada bloque de Transformer. La conexión MUDD puede integrarse fácilmente en cualquier parte de la arquitectura de Transformer y permite la creación de MUDDFormer. Extensos experimentos muestran que MUDDFormer supera significativamente a los Transformers en modelos de diferentes arquitecturas y escalas de modelado de lenguaje, logrando resultados que los Transformers entrenados con 1.8X-2.4X más cálculos no lograrían. En particular, MUDDPythia-2.8B coincide en preentrenamiento y tareas inferiores con Pythia-6.9B, compite en la configuración de 5-shot con Pythia-12B, y añade solo 0.23% de parámetros y 0.4% de cálculos. Los códigos y modelos preentrenados en JAX y PyTorch están disponibles en https://github.com/Caiyun-AI/MUDDFormer.",
      "upvotes": 6,
      "discussionId": "67b543502b2ec6908fffe788"
    },
    "publishedAt": "2025-02-18T22:59:16.530Z",
    "title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12170.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d77440bad37ef354028365",
      "avatarUrl": "/avatars/df0dea879e06fa814867e9aad03d1e68.svg",
      "fullname": "Da Xiao",
      "name": "xiaoda99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12215",
      "authors": [
        {
          "_id": "67b56007fa141a55e51d9d78",
          "name": "Zhiyuan Zeng",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d79",
          "name": "Qinyuan Cheng",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7a",
          "name": "Zhangyue Yin",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7b",
          "name": "Yunhua Zhou",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7c",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T07:21:11.000Z",
      "title": "Nuevamente, se examina el escalado de tiempos de prueba del modelo O1 de Rashiin: ¿tienen realmente la capacidad de escalar los tiempos de prueba?",
      "summary": "En los modelos de lenguaje grande (LLMs), como se muestra con la serie o1 de OpenAI, se puede mejorar la capacidad de computación de recursos de cómputo en la inferencia para desarrollar habilidades lógicas. Al examinar modelos posteriores como Deepseek-R1 (R1) y LIMIT, se puede observar que estos modelos recapitulan estas mejoras, pero aún no se han investigado su capacidad de escalabilidad en el test. En este estudio, se muestra que la longitud de los CoTs (compuertas lógicas) en modelos como o1 no mejora la precisión consistentemente. En realidad, a veces la respuesta correcta aparece más rápidamente que la incorrecta. Este fenómeno está estrechamente relacionado con la capacidad de autocorrección del modelo. Los CoTs más largos contienen más autocorrecciones, lo que generalmente reduce su rendimiento. Comparando permutaciones y escalas paralelas en R1 y LIMIT, se concluye que la escalabilidad paralela logra un mejor cobertura y menos fallos en el escalador. Basándose en esta observación, se propone un método que combina la escalabilidad paralela y las características de la longitud de los CoTs, llamado \"votación por la mayoría mínima\", y se muestra que este método mejora significativamente la capacidad de escalabilidad en el test comparado con el método tradicional de votación por la mayoría.",
      "upvotes": 5,
      "discussionId": "67b56007fa141a55e51d9da7"
    },
    "publishedAt": "2025-02-18T23:37:46.756Z",
    "title": "Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12215.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6142
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12501",
      "authors": [
        {
          "_id": "67b547ffc9071a3e97139532",
          "user": {
            "_id": "62a42f22c683d02f5b63320c",
            "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
            "isPro": false,
            "fullname": "Qiyuan Zhang",
            "user": "DonJoey",
            "type": "user"
          },
          "name": "Qiyuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:10.215Z",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139533",
          "name": "Yufei Wang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139534",
          "user": {
            "_id": "63c20105726f62e411fbe882",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c20105726f62e411fbe882/2UsU9O2psbDjJzz-sAmGH.jpeg",
            "isPro": false,
            "fullname": "Yuxin Jiang",
            "user": "YuxinJiang",
            "type": "user"
          },
          "name": "Yuxin Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:08.101Z",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139535",
          "name": "Liangyou Li",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139536",
          "name": "Chuhan Wu",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139537",
          "name": "Yasheng Wang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139538",
          "name": "Xin Jiang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139539",
          "name": "Lifeng Shang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953a",
          "name": "Ruiming Tang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953b",
          "name": "Fuyuan Lyu",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953c",
          "name": "Chen Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T03:31:06.000Z",
      "title": "Razones de la comparación de clusters: Comprender la evaluación integral de LLM-as-a-Judge",
      "summary": "LLM-as-a-Judge es un método de evaluación automática ampliamente aplicado que genera una serie de razonamientos de tipo \"chain-of-thought\" (CoT). Sin embargo, la confianza en este método se ve comprometida porque los CoT no pueden entender completamente los detalles, lo que puede llevar a la generación de resultados incompletos. Actualmente, los métodos principales se basan en la votación de mayoría o en la extensión de los criterios de evaluación, pero no resolven las limitaciones de los CoT. Proponemos la Evaluación Comparativa Basada en Crowd. Este método revela los detalles de las respuestas candidatas comparándolas con respuestas adicionales generadas por código. Este proceso impulsa a LLM-as-a-Judge a proporcionar una evaluación de CoT detallada. A través de una amplia gama de experimentos, nuestro enfoque ha mejorado la confianza en la evaluación y aumentado la precisión en un promedio de 6.7% en 5 marcos de referencia. Además, nuestro método utiliza la experiencia de la evaluación para generar CoT de alta calidad y mejorar la convergencia de entrenamiento (SFT), lo que se conoce como \"convergencia de muestras de código\", facilitando la ejecución eficiente de la SFT. La análisis muestra que los CoT generados por nosotros son detallados y de alta calidad, y que la precisión de la evaluación se mejora con el escalado de la complejidad de la inferencia.",
      "upvotes": 5,
      "discussionId": "67b54800c9071a3e9713956c"
    },
    "publishedAt": "2025-02-18T21:55:26.822Z",
    "title": "Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12501.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a42f22c683d02f5b63320c",
      "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
      "fullname": "Qiyuan Zhang",
      "name": "DonJoey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11271",
      "authors": [
        {
          "_id": "67b4322c217ec18a40587bec",
          "user": {
            "_id": "60f5f68fa7fd83d025749234",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
            "isPro": false,
            "fullname": "Pan Lu",
            "user": "lupantech",
            "type": "user"
          },
          "name": "Pan Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:04:43.677Z",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bed",
          "name": "Bowen Chen",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bee",
          "name": "Sheng Liu",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bef",
          "name": "Rahul Thapa",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bf0",
          "name": "Joseph Boen",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bf1",
          "name": "James Zou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T21:18:47.000Z",
      "title": "OctoTools: Marco ágil con herramientas expandibles para abordar complejos problemas de lógica y razonamiento.",
      "summary": "Para resolver tareas lógicas complejas se incluyen elementos como comprensión visual, búsqueda de conocimientos en áreas específicas, cálculos numéricos y lógica multinivel. Los métodos actuales fortalecen los modelos de lenguaje grande escala (LLMs) utilizando herramientas externas, pero presentan limitaciones en áreas específicas y necesidad de datos de entrenamiento adicionales. En este artículo se presenta OctoTools, un marco de agente abierto, necesidad de entrenamiento, amigable con el usuario y fácilmente expandible. Este marco está diseñado para resolver tareas lógicas complejas en diferentes áreas. OctoTools incluye funciones de herramientas mediante tarjetas de herramientas estándar, un planificador para planificación alto y bajo nivel, y una función de ejecución para usar las herramientas. OctoTools ha demostrado su generalidad en 16 tareas diferentes, como MathVista, MMLU-Pro, MedQA y GAIA-Text, con un aumento significativo en la precisión promedio (9.3%) en comparación con GPT-4o. Además, utilizando la misma colección de herramientas, OctoTools supera a AutoGen, GPT-Functions y LangChain con un incremento máximo de 10.6%. A través de análisis detallado y pruebas de eliminación, OctoTools muestra excelentes resultados en la planificación de tareas, el uso de herramientas válidas y la resolución de problemas multiniveles.",
      "upvotes": 4,
      "discussionId": "67b4322d217ec18a40587c27"
    },
    "publishedAt": "2025-02-19T02:27:36.940Z",
    "title": "OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11271.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f5f68fa7fd83d025749234",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
      "fullname": "Pan Lu",
      "name": "lupantech",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.09838",
      "authors": [
        {
          "_id": "67b55078a64445f58c771d84",
          "name": "Tianwei Lin",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d85",
          "name": "Wenqiao Zhang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d86",
          "name": "Sijing Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d87",
          "name": "Yuqian Yuan",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d88",
          "name": "Binhe Yu",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d89",
          "name": "Haoyuan Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8a",
          "name": "Wanggui He",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8b",
          "name": "Hao Jiang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8c",
          "name": "Mengze Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8d",
          "name": "Xiaohui Song",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8e",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8f",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d90",
          "name": "Hui Lin",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d91",
          "name": "Yueting Zhuang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d92",
          "name": "Beng Chin Ooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-14T00:42:36.000Z",
      "title": "Salud GPT: Modelo de lenguaje visuolingüístico de gran escala para usos médicos\nIntegración a través de comprensión y generación, adaptando diferentes conocimientos entre sí",
      "summary": "HolaGPT, una potente versión de un grande modelo de lenguaje visuolingüístico médico (Med-LVLM) se presenta. Esta capacidad se integra en un paradigma de recuperación automática unificado, permitiendo la comprensión y generación de visión médica. Nuestra filosofía inicial es la aplicación gradual de entendimientos y conocimientos en el dominio de la computación y la generación, aplicados a modelos de lenguaje grandes previamente entrenados (LLMs). Esto se ha logrado mediante la nueva técnica H-LoRA, complementado por un enfoque de reconocimiento visual gradual y una estrategia de entrenamiento en tres etapas. Para el aprendizaje efectivo de HolaGPT, se diseñó VL-Health, un conjunto de datos específico y detallado en el campo médico, que combina entendimiento y generación. Los resultados de los experimentos muestran que HolaGPT demostra una excelente rendimiento y escalabilidad en tareas de integración visual médica. Este proyecto está disponible en https://github.com/DCDmllm/HealthGPT.",
      "upvotes": 4,
      "discussionId": "67b5507aa64445f58c771df9"
    },
    "publishedAt": "2025-02-18T22:35:23.066Z",
    "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09838.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fc18edfb66882aba4d548e",
      "avatarUrl": "/avatars/f70d47fe4aba98b5a5cd64f7e002dfd2.svg",
      "fullname": "wenqiao",
      "name": "wannature",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12574",
      "authors": [
        {
          "_id": "67b547f555d0424a31b9c384",
          "user": {
            "_id": "64cb48f7667f4f808535107e",
            "avatarUrl": "/avatars/8f77f378ad665b246e1ea3aaba2153ae.svg",
            "isPro": false,
            "fullname": "chengluo",
            "user": "wdlctc",
            "type": "user"
          },
          "name": "Cheng Luo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:40:25.130Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c385",
          "user": {
            "_id": "64b15284372d4340772a3dca",
            "avatarUrl": "/avatars/417d5f1bc1bcb5e4d5de6169673c2cf7.svg",
            "isPro": false,
            "fullname": "Zefan Cai",
            "user": "ZefanCai",
            "type": "user"
          },
          "name": "Zefan Cai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:40:47.077Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c386",
          "name": "Hanshi Sun",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c387",
          "user": {
            "_id": "64c15c5bea792b1950e302e4",
            "avatarUrl": "/avatars/51f84365cc08a1dcd5da70968389aed2.svg",
            "isPro": false,
            "fullname": "Jinqi Xiao",
            "user": "jinqixiao",
            "type": "user"
          },
          "name": "Jinqi Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:41:01.931Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c388",
          "name": "Bo Yuan",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c389",
          "name": "Wen Xiao",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38a",
          "user": {
            "_id": "675f8271a63fff7b5bcbc478",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/9tJn7NyzLMreCJVH4wRho.png",
            "isPro": false,
            "fullname": "Junjie Hu",
            "user": "junjiehu",
            "type": "user"
          },
          "name": "Junjie Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:41:18.304Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38b",
          "name": "Jiawei Zhao",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38c",
          "user": {
            "_id": "64b732f832403871593e082c",
            "avatarUrl": "/avatars/dd21932b0c167131ee7545a622c46c3c.svg",
            "isPro": false,
            "fullname": "Beidi Chen",
            "user": "beidic",
            "type": "user"
          },
          "name": "Beidi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:39:20.563Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38d",
          "user": {
            "_id": "6532920b3e385cfc6002938d",
            "avatarUrl": "/avatars/cb9cc6d2733031582c83f56dc6cd1dd5.svg",
            "isPro": false,
            "fullname": "Anima Anandkumar",
            "user": "animakumar",
            "type": "user"
          },
          "name": "Anima Anandkumar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:39:15.091Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T06:26:05.000Z",
      "title": "HeadInfer: Método de inferencia eficiente en memoria para modelos LLM que divide y ejecuta cada capa del modelo de red neuronal de manera separada",
      "summary": "Los modelos de lenguaje grandes basados en Transformer (LLMs) muestran un excelente rendimiento en la generación de contextos largos. La expansión de la longitud del contexto ha desbalanceado la calidad de memoria de las LLMs en la cache de palabras clave (KV cache). En este artículo, se propone HEADINFER. HEADINFER evita almacenar completamente en la RAM del CPU la cache de palabras clave de todas las capas de Transformer. HEADINFER deja en la GPU el cache de palabras clave de los cabezas de atención selectivas, calculando las salidas de atención dinámicamente. A través de un análisis de loprín, se demuestra que HEADINFER puede reducir significativamente la calidad de memoria mientras mantiene la eficiencia computacional. Se evaluó HEADINFER con una secuencia de 1,000,000 tokens en el modelo Llama-3-8B. La calidad de memoria de la cache de palabras clave en la GPU se redujo de 128GB a 1GB, y el uso total de memoria de la GPU se redujo de 207GB a 17GB, logrando un descenso del 92% en la base de inferencia lineal BF16. En particular, HEADINFER permite realizar una inferencia de 4,000,000 tokens en un GPU de consumo de 24GB (por ejemplo, NVIDIA RTX 4090) para un modelo de 8B. Además, evita el uso de métodos aproximados.",
      "upvotes": 3,
      "discussionId": "67b547f755d0424a31b9c3e5"
    },
    "publishedAt": "2025-02-18T21:57:00.289Z",
    "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12574.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb48f7667f4f808535107e",
      "avatarUrl": "/avatars/8f77f378ad665b246e1ea3aaba2153ae.svg",
      "fullname": "chengluo",
      "name": "wdlctc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13063",
      "authors": [
        {
          "_id": "67b5a7896f72266cb765e744",
          "user": {
            "_id": "618b9540682ec1c38327e586",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618b9540682ec1c38327e586/v_ZBkfh8O9Zh6C2YQpuBX.jpeg",
            "isPro": false,
            "fullname": "Yury Kuratov",
            "user": "yurakuratov",
            "type": "user"
          },
          "name": "Yuri Kuratov",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-19T09:42:34.422Z",
          "hidden": false
        },
        {
          "_id": "67b5a7896f72266cb765e745",
          "name": "Mikhail Arkhipov",
          "hidden": false
        },
        {
          "_id": "67b5a7896f72266cb765e746",
          "name": "Aydar Bulatov",
          "hidden": false
        },
        {
          "_id": "67b5a7896f72266cb765e747",
          "user": {
            "_id": "639c6e978a34ed9a404c6a7b",
            "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
            "isPro": false,
            "fullname": "MIKHAIL BURTSEV",
            "user": "mbur",
            "type": "user"
          },
          "name": "Mikhail Burtsev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:56:59.080Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T17:08:45.000Z",
      "title": "Incluimos el token CLAMING 1568 en un solo vector y lo vuelvemos a revisar: esto investiga los límites del espacio de embarazados.",
      "summary": "Recientes estudios diversos buscan hacer que la permutación de tokens se compresa en una permutación de un vector de valores reales cortos, reemplazando el token embedding o caché de clave valor. Esta aproximación puede reducir la cantidad de cálculos de los modelos de lenguaje actuales. Sin embargo, cuando se usa un modelo fuerte como encoder, el ratio de compresión no supera generalmente el x10. Este hecho es muy interesante. Teóricamente, la capacidad de información máxima de un grande vector de valores reales puede ser mucho más alta, incluso con precisión de 16 bits y un tamaño de vector ligero, que la velocidad actual. En este artículo, se reemplazan los encoders por un proceso de optimización por bloques para investigar las limitaciones de la compresión y mostrar vectores que se pueden compresar hasta x1500. Además, se define claramente la diferencia entre las soluciones existentes y las prácticas útiles. Experimentalmente, se demuestra que la limitación de la compresión depende de la cantidad de incertidumbre reducida, no de la longitud del input. Es decir, la pérdida de entropía cruzada de estas permutaciones. Esta límite obtenida muestra una gran diferencia entre el capacidad teórica del codificado de input y su uso práctico, mostrando que hay un gran potencial de optimización en el diseño de modelos.",
      "upvotes": 1,
      "discussionId": "67b5a78a6f72266cb765e779"
    },
    "publishedAt": "2025-02-19T04:43:42.973Z",
    "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13063.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "639c6e978a34ed9a404c6a7b",
      "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
      "fullname": "MIKHAIL BURTSEV",
      "name": "mbur",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.10708",
      "authors": [
        {
          "_id": "67b58e32e972a2806a9a0451",
          "user": {
            "_id": "65407ba7a38390065750233f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
            "isPro": false,
            "fullname": "Zirui Song",
            "user": "Ziruibest",
            "type": "user"
          },
          "name": "Zirui Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:38.943Z",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0452",
          "name": "Bin Yan",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0453",
          "name": "Yuhan Liu",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0454",
          "name": "Miao Fang",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0455",
          "name": "Mingzhe Li",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0456",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0457",
          "name": "Xiuying Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-15T07:43:43.000Z",
      "title": "Metodología de Inyección de Conocimientos Específicos en Modelos de Lenguaje de Gran Escala: Investigación Coherente",
      "summary": "Los modelos de lenguaje grande (LLMs) han demostrado un extraordinario éxito en tareas como la comprensión del lenguaje natural, la resumen de texto y la traducción automática. Sin embargo, sus características generales están limitadas en su efectividad en áreas específicas que requieren conocimientos particularizados, como la atención médica, la química y el análisis de leyes. Por lo tanto, los investigadores están buscando métodos para integrar conocimientos específicos en los LLMs y fortalecerlos. En esta revisión, se clasifican estos métodos en cuatro enfoques principales: injección de conocimiento dinámico, inserción de conocimiento estático, modularización de decodificadores y optimización de profundidad. Se explica cómo cada enfoque proporciona conocimientos específicos a los LLMs, equilibrando el trade-off entre flexibilidad, escalabilidad y eficiencia. Estos métodos permiten que los LLMs se adapten a tareas específicas, comparando sus ventajas y desventajas, y comparando los LLMs específicos con los generales, clarificando los problemas y oportunidades en esta nueva área. Se recomienda a quienes estén interesados, pero para aquellos que deseen un interés más profundo, se proporciona una organización de los datasets y marcos de evaluación generalmente utilizados en: https://github.com/abilliyb/Knowledge_Injection_Survey_Papers, donde se registran estudios de investigación especializados en LLMs.",
      "upvotes": 1,
      "discussionId": "67b58e33e972a2806a9a04b8"
    },
    "publishedAt": "2025-02-19T02:56:09.510Z",
    "title": "Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10708.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65407ba7a38390065750233f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
      "fullname": "Zirui Song",
      "name": "Ziruibest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12669",
      "authors": [
        {
          "_id": "67b58c806e53744c2a373351",
          "user": {
            "_id": "63024676056ec3a2a8714b24",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
            "isPro": false,
            "fullname": "Xiang Liu",
            "user": "Dominic789654",
            "type": "user"
          },
          "name": "Xiang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:34:03.429Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373352",
          "user": {
            "_id": "64eded5fdfe0a679d840bc98",
            "avatarUrl": "/avatars/4d4c67c13e547a4d296a301e8694e79e.svg",
            "isPro": false,
            "fullname": "sunpenglei",
            "user": "sunpenglei",
            "type": "user"
          },
          "name": "Penglei Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:34:15.889Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373353",
          "name": "Shuyan Chen",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373354",
          "name": "Longhan Zhang",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373355",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373356",
          "name": "Huajie You",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373357",
          "user": {
            "_id": "64473221dcbe1333b64b2db2",
            "avatarUrl": "/avatars/5e4495d3581ad3e6ea3c47650f20b993.svg",
            "isPro": false,
            "fullname": "yongqi zhang",
            "user": "yongqi2023",
            "type": "user"
          },
          "name": "Yongqi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:35:12.059Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373358",
          "name": "Chang Yan",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373359",
          "user": {
            "_id": "6676935fcd0b89a0115174b0",
            "avatarUrl": "/avatars/4caca1b672d29e787814f9a30bf20bcc.svg",
            "isPro": false,
            "fullname": "Xiaowen Chu",
            "user": "wenxinsiju",
            "type": "user"
          },
          "name": "Xiaowen Chu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:35:20.611Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a37335a",
          "name": "Tong-yi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T09:19:24.000Z",
      "title": "Perovskite-LLM: Modelo de lenguaje de inteligencia artificial de tipo LLM especializado en perovskitas",
      "summary": "Los PSCs de Polovita Solar Cells (PSCs) han experimentado un rápido crecimiento, lo que ha llevado a un aumento exponencial en la cantidad de artículos científicos publicados. Esto ha despertado la necesidad urgente de sistemas eficientes para la gestión del conocimiento y la análisis lógico en este campo. Presentamos un sistema de ampliación de conocimiento diseñado para grafos de conocimiento específicos en PSCs. Este sistema integra tres componentes principales:\n\n1. **Desarrollo de un grafo de conocimiento especializado**: Se ha construido un grafo de conocimiento basado en 1,517 artículos de investigación, que incluye 23,789 entidades y 22,272 relaciones.\n\n2. **Generación de dos conjuntos de datos de interpolación**:\n   - **PSC샹**: Incluye 55,101 pares de preguntas y respuestas de alta calidad generados por un nuevo marco de agentes, aportando una amplia gama de respuestas.\n   - **PSC랩핑**: Contiene 2,217 problemas de ciencia de materiales cuidadosamente seleccionados, ofreciendo una base sólida para la investigación.\n\n3. **Uso de dos modelos de lenguaje a escala de conocimiento especializado**:\n   - **PSC샹 LLM**: Proporciona soporte para el conocimiento especializado.\n   - **PSC랩핑 LLM**: Trata tareas de análisis lógico en la ciencia.\n\nLos resultados de los experimentos muestran que este sistema presenta significativamente mejores rendimientos en la búsqueda de conocimiento especializado y en las tareas de análisis lógico científico, comparados con los modelos actuales. Además, proporciona a los investigadores herramientas efectivas para la búsqueda de artículos, la diseño de experimentos y la resolución de problemas complejos.",
      "upvotes": 1,
      "discussionId": "67b58c826e53744c2a3733c2"
    },
    "publishedAt": "2025-02-19T02:47:33.654Z",
    "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.10990",
      "authors": [
        {
          "_id": "67b3ee6c1e80a69e79c3155a",
          "user": {
            "_id": "647d834618274bce03013cc2",
            "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
            "isPro": true,
            "fullname": "yixuan",
            "user": "yixuantt",
            "type": "user"
          },
          "name": "Yixuan Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:04:50.969Z",
          "hidden": false
        },
        {
          "_id": "67b3ee6c1e80a69e79c3155b",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T04:23:52.000Z",
      "title": "FinMTEB: Master de Texto Financiero de End-to-End Benchmark",
      "summary": "Los modelos de embedding desempeñan un papel crucial en la representación de información y en la búsqueda en diversas aplicaciones de NLP. El desarrollo reciente de grandes modelos de lenguaje (LLMs) ha mejorado el rendimiento de estos modelos de embedding. Generalmente, estos modelos se evalúan en conjuntos de datos comunes, pero las aplicaciones reales en el mundo actual requieren una evaluación en contextos de dominio. En este artículo, se presenta FinMTEB (Benchmark de Embedding de Texto de Finanzas Masivo), un contenedor especializado basado en MTEB, que se adapta a la industria financiera. FinMTEB consiste en 7 tareas y cubre diferentes tipos de oraciones en chino y inglés, como artículos de noticias financieras, informes anuales de empresas, informes de ESG, solicitudes de regulación y discursos de reuniones contables. Además, se ha desarrollado FinPersona-E5, un modelo adaptativo financiero que utiliza una metodología de síntesis de datos basada en personas, y cubre una variedad de tareas de embedding financieras. A través de evaluaciones extendidas de 15 modelos de embedding, se presentan tres principales conclusiones: (1) el rendimiento en los benchmarks generales tiene una relación limitada con las tareas de dominio financiero, (2) los modelos adaptativos para el dominio financiero superan claramente a los modelos generales, y (3) una simple aproximación de \"bag of words\" (BoW) supera a los técnicas densas de embedding en tareas de semántica textual similaridad (STS) financiera, revelando limitaciones actuales de las tecnologías de embedding densas. Este artículo contribuye a la construcción de un fuerte marco de evaluación para aplicaciones de NLP en el sector financiero y destaca la importancia de desarrollar modelos de embedding adaptados al dominio financiero.",
      "upvotes": 0,
      "discussionId": "67b3ee6d1e80a69e79c3158f"
    },
    "publishedAt": "2025-02-19T04:54:27.788Z",
    "title": "FinMTEB: Finance Massive Text Embedding Benchmark",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d834618274bce03013cc2",
      "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
      "fullname": "yixuan",
      "name": "yixuantt",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13142",
      "authors": [
        {
          "_id": "67b5790132be608036ee94e5",
          "user": {
            "_id": "65c3fdf79d062be813813e45",
            "avatarUrl": "/avatars/52528a61abe5bbbef4a4a431944973cd.svg",
            "isPro": false,
            "fullname": "Dantong Niu",
            "user": "NdtSoCool",
            "type": "user"
          },
          "name": "Dantong Niu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:12:28.457Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e6",
          "user": {
            "_id": "65406e82deee4716f1c29271",
            "avatarUrl": "/avatars/25331a773f8125f9ad1c3d6ac3375586.svg",
            "isPro": false,
            "fullname": "Yuvan Sharma",
            "user": "yuvansharma",
            "type": "user"
          },
          "name": "Yuvan Sharma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:12:35.531Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e7",
          "name": "Haoru Xue",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e8",
          "user": {
            "_id": "650bd36a7c99ca283e58e973",
            "avatarUrl": "/avatars/606d24b2dac190ebcbb4b2a2e4671380.svg",
            "isPro": false,
            "fullname": "Giscard Biamby",
            "user": "gbiamby",
            "type": "user"
          },
          "name": "Giscard Biamby",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:12:49.219Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e9",
          "name": "Junyi Zhang",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94ea",
          "user": {
            "_id": "66a09aec369dd38cf2113070",
            "avatarUrl": "/avatars/cc13bdd3dc1271d33b083b61e12f1a05.svg",
            "isPro": false,
            "fullname": "Ziteng Ji",
            "user": "zitengj0618",
            "type": "user"
          },
          "name": "Ziteng Ji",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:13:13.907Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94eb",
          "user": {
            "_id": "64cbdf02f103036e23d1c7f3",
            "avatarUrl": "/avatars/496069463900dea20929b57381182d39.svg",
            "isPro": false,
            "fullname": "Trevor Darrell",
            "user": "trevordarrell",
            "type": "user"
          },
          "name": "Trevor Darrell",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:13:20.379Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94ec",
          "user": {
            "_id": "667c5764186b27ef806636d3",
            "avatarUrl": "/avatars/5c08f0109bc0e350624112c0aff544f6.svg",
            "isPro": false,
            "fullname": "Roei Herzig",
            "user": "roeiherz",
            "type": "user"
          },
          "name": "Roei Herzig",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:13:26.134Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:01.000Z",
      "title": "Entrenamiento de un modelo de robot automático de regresión de retroceso utilizando aprendizaje previo con representaciones 4D",
      "summary": "Fundamentāl mōdelu, ŏi jūi naru label dā'itā sutītōn de sakain sūrīn sareta mono de, kon'esutā suto ni tōkon'i hōyō ni kaikaku-teki eiyō o age, kyōi-teki ichigai kōka o shijishin suru, sakain sūrīn no jūyō-sei o shōmei shite imāsu. shika, rōbutikuku ni oite, kono yōna seikō o tachiru koto wa kōnan de, tōgō no tōgō no dā'itā no rekō to wa, fysik-u kai-sen o yōgo ni arau hyōgen no kōraku ni yotte seigen sarete imasu. kono ronbun de, jinjin no bi-deo-dā'itā kara sūrīn sareta rō-ru-bīn rē-bī-rō-mō-dēl o kōyō shite, rōbuto no mō-dēl o kaihatsu suru tame no jidō-hō-eki-teki rōbuto mō-dēl o tōji shite, ARM4R to iu mono o shōryū shimasu. tokubetsu, jikan ni wa tachite mono-karamu depusutō-seido tōkei o yōgo suru 2D hyōgen o 3D kō-sen ni hiku-ageta 3D pīn tō-chō hyōgen o riyū shimasu. konra no 4D hyōgen o, pīn to rōbuto no jōtai hyōgen no kanjō o kōgyō suru, senritsu henkan ni yotte kōnan naru yōni kaizen sarete iru tame, jinnin no bi-deo-dā'itā kara rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sūrīn ni yōgo ni tōgō no tōgō kōka o kōyō shite, rō-ru-bīn sū",
      "upvotes": 0,
      "discussionId": "67b5790832be608036ee9638"
    },
    "publishedAt": "2025-02-19T01:24:26.365Z",
    "title": "Pre-training Auto-regressive Robotic Models with 4D Representations",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667c5764186b27ef806636d3",
      "avatarUrl": "/avatars/5c08f0109bc0e350624112c0aff544f6.svg",
      "fullname": "Roei Herzig",
      "name": "roeiherz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.10852",
      "authors": [
        {
          "_id": "67b55321f703732d151de666",
          "name": "Zeli Su",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de667",
          "name": "Ziyin Zhang",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de668",
          "name": "Guixian Xu",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de669",
          "name": "Jianing Liu",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66a",
          "name": "XU Han",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66b",
          "name": "Ting Zhang",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66c",
          "name": "Yushuang Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-15T16:53:10.000Z",
      "title": "El multilingual encoder cree que sabe lo que realmente no pensó: la compartida de pesos\nEl aprendizaje previo con muy pocos recursos en lenguajes de bajo recurso",
      "summary": "El modelo de multilingüismo XLM-R promueve la multilingüización en el NLP, mientras que su rendimiento en lenguas con pocos recursos lingüísticos es insatisfactorio. Esta situación se ve agravada porque los modernos LLMs (como LLaMA y Qwen) proporcionan menos lenguas que XLM-R. Para enfrentar estas desafíos, proponemos un nuevo marco de trabajo para la generación de oraciones en lenguas con muy pocos recursos lingüísticos. Este marco permite reutilizar los pesos del encoder para aprovechar el espacio de significado entrenado, lo que ayuda a la aprendizaje eficiente y la generalización efectiva en lenguas con pocos recursos lingüísticos. Aplicamos este marco a cuatro lenguas minoritarias de China, presentando XLM-SWCM y demostrando su excelente rendimiento.",
      "upvotes": 0,
      "discussionId": "67b55322f703732d151de69d"
    },
    "publishedAt": "2025-02-18T22:46:16.586Z",
    "title": "Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10852.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6430bdd8cd31d174a9f900fb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
      "fullname": "Ziyin Zhang",
      "name": "Geralt-Targaryen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]