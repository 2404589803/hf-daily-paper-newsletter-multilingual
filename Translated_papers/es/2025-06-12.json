[
  {
    "paper": {
      "id": "2506.09113",
      "authors": [
        {
          "_id": "684a3b0a9b38e1e5a33a683f",
          "name": "Yu Gao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6840",
          "name": "Haoyuan Guo",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6841",
          "name": "Tuyen Hoang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6842",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6843",
          "name": "Lu Jiang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6844",
          "name": "Fangyuan Kong",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6845",
          "name": "Huixia Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6846",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6847",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6848",
          "name": "Xiaojie Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6849",
          "name": "Xunsong Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684a",
          "name": "Yifu Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684b",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684c",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684d",
          "name": "Jiawei Liu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684e",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684f",
          "name": "Xiaonan Nie",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6850",
          "name": "Zhiwu Qing",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6851",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6852",
          "name": "Li Sun",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6853",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6854",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6855",
          "name": "Sen Wang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6856",
          "name": "Guoqiang Wei",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6857",
          "name": "Guohong Wu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6858",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6859",
          "name": "Ruiqi Xia",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685a",
          "name": "Fei Xiao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685b",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685c",
          "name": "Jiangqiao Yan",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685d",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685e",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685f",
          "name": "Runkai Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6860",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6861",
          "name": "Yihang Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6862",
          "name": "Zilyu Ye",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6863",
          "name": "Xuejiao Zeng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6864",
          "name": "Yan Zeng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6865",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6866",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6867",
          "name": "Xiaozheng Zheng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6868",
          "name": "Peihao Zhu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6869",
          "name": "Jiaxin Zou",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a686a",
          "name": "Feilong Zuo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:56:11.000Z",
      "submittedOnDailyAt": "2025-06-12T01:08:56.090Z",
      "title": "Seedance 1.0: Desafíando los límites de los modelos de generación de imágenes",
      "submittedOnDailyBy": {
        "_id": "6381c5d63680a7cf34e08ca9",
        "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
        "isPro": false,
        "fullname": "wujie10558@gmail.com",
        "user": "wujie10",
        "type": "user"
      },
      "summary": "Actualmente, modelos similares permiten una aprendizaje panorámico en diferentes escenarios al agregar la precisión de datos pequeños de suma y capturas de vídeo significativas, lo que facilita la adición de estas características para mejorar la precisión y la capacidad de aprendizaje en escenarios variados. Mediante una diseño arquitectónico eficiente y un paradigma de entrenamiento propuesto, se logra el aprendizaje común para dos tareas: la generación de vídeo a partir de una imagen y la generación de vídeo a partir de un texto. Utilizando una estructura de recompensas multidimensional y un proceso de ajuste con retroalimentación de usuarios, se logra un mejoramiento generalizado del rendimiento. Además, se realiza una aceleración del modelo con alto efecto dirigido, lo que permite un aumento en la velocidad de inferencia de aproximadamente 10 veces a través de una estrategia de diseño multinivel y optimización del sistema. Esto resulta en un modelo de alta eficiencia y rendimiento basado en vídeo.\n\nSeedance 1.0 puede generar un vídeo de 5 segundos a 1080p en solo 41.4 segundos en NVIDIA-L20. Comparado con los modelos de generación de vídeo más avanzados, es un modelo especializado en la representación coherente de temas, con alta calidad y velocidad de generación de vídeo, alta fidelidad espectral y estabilidad estructural, precisión en contextos complejos y respeto a las instrucciones claras, y una división nativa de vídeo a partir de una imagen que mantiene una representación coherente del tema.",
      "upvotes": 33,
      "discussionId": "684a3b0b9b38e1e5a33a686b",
      "projectPage": "https://seed.bytedance.com/seedance",
      "ai_summary": "Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.",
      "ai_keywords": [
        "diffusion modeling",
        "multi-source data curation",
        "precision and meaningful video captioning",
        "efficient architecture",
        "training paradigm",
        "multi-shot generation",
        "text-to-video",
        "image-to-video",
        "fine-grained supervised fine-tuning",
        "video-specific RLHF",
        "multi-dimensional reward mechanisms",
        "multi-stage distillation strategies",
        "model acceleration",
        "spatiotemporal fluidity",
        "structural stability",
        "instruction adherence",
        "multi-shot narrative coherence",
        "consistent subject representation"
      ]
    },
    "publishedAt": "2025-06-10T13:56:11.000Z",
    "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
    "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09113.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6381c5d63680a7cf34e08ca9",
      "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
      "fullname": "wujie10558@gmail.com",
      "name": "wujie10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06395",
      "authors": [
        {
          "_id": "68492dcf42e4f9106973f437",
          "user": {
            "_id": "6734e315c1aadce903f73aea",
            "avatarUrl": "/avatars/95d95c49419372debc201cb63c354b86.svg",
            "isPro": false,
            "fullname": "Li Pengyi",
            "user": "LiPengyi29",
            "type": "user"
          },
          "name": "Pengyi Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-11T07:18:40.287Z",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f438",
          "name": "Matvey Skripkin",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f439",
          "name": "Alexander Zubrey",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f43a",
          "name": "Andrey Kuznetsov",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f43b",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643984dceb7c5616ef3f5d54/5kHQrpj1ivFhnzHL36xhr.jpeg"
      ],
      "publishedAt": "2025-06-05T19:55:15.000Z",
      "submittedOnDailyAt": "2025-06-12T07:02:06.762Z",
      "title": "Autoestima puede ser todo: Microajustes de RL cortos para modelos de lenguaje",
      "submittedOnDailyBy": {
        "_id": "643984dceb7c5616ef3f5d54",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
        "isPro": false,
        "fullname": "Andrey Kuznetsov",
        "user": "kuznetsoffandrey",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) superan la lógica, pero la entrenamiento posterior a los tokens es crucial para ajustar el comportamiento del modelo a los objetivos de la tarea. Los métodos actuales de aprendizaje por refuerzo (RL) dependen de anotaciones humanas costosas o de modelos de recompensa externos. Proponemos un aprendizaje por refuerzo basado en confianza del modelo (RLSC), que utiliza señales de confianza para recompensar el modelo. Este método elimina la necesidad de etiquetas, modelos de preferencia o de aprendizaje de recompensa. Para Qwen2.5-Math-7B, al entrenar 16 problemas por grupo con 10 o 20 etapas, el RLSC mejora la precisión en AIME2024 (+13.4%), MATH500 (+21.2%), Minerva Math (+21.7%), Olympiadbench (+20.8%) y AMC23 (+9.7%). RLSC proporciona una simple y escalable forma de entrenamiento posterior a los tokens, sin necesidad de muestras con decimales o etiquetas, y que requiere un supervígen de superinteligencia.",
      "upvotes": 27,
      "discussionId": "68492dd042e4f9106973f43c",
      "ai_summary": "Reinforcement Learning via Self-Confidence (RLSC) improves large language model accuracy using the model's confidence as a reward signal, eliminating the need for human labels or reward engineering.",
      "ai_keywords": [
        "Reinforcement Learning",
        "Large language models",
        "self-confidence",
        "RLSC"
      ]
    },
    "publishedAt": "2025-06-05T15:55:15.000Z",
    "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models",
    "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,\nRLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on\nMinerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a\nsimple, scalable post-training method for inference models, requiring only a\nsmall number of samples and unlabelled supervision.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643984dceb7c5616ef3f5d54/5kHQrpj1ivFhnzHL36xhr.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06395.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "643984dceb7c5616ef3f5d54",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
      "fullname": "Andrey Kuznetsov",
      "name": "kuznetsoffandrey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09995",
      "authors": [
        {
          "_id": "684a39639b38e1e5a33a6837",
          "name": "Yuanpeng Tu",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a6838",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a6839",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683a",
          "name": "Xiang Bai",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683b",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683c",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:59:53.000Z",
      "submittedOnDailyAt": "2025-06-12T00:50:19.796Z",
      "title": "PlayerOne: Mundo Simulador Centrado en el Yo",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "PlayerVerse, se introduce como el primer simulador central de mundos realistas. Esto impulsa la exploración sin límites en un entorno vibrante. Acepta una imagen esencial de esquema proporcionada por el usuario, construye con precisión un mundo correspondiente y genera vídeos centrales. Estos vídeos coinciden con los movimientos humanos reales capturados con una cámara central por el usuario. PlayerVerse aprende de manera flotante, desde la generalidad hasta los detalles como las hojas de un árbol. Primero, realiza un aprendizaje previo centrado en pares de texto-vídeo de gran escala, logrando una comprensión central. Luego, realiza un aprendizaje de ajuste basado en datos de movimiento-vídeo motivados extraídos de un conjunto de datos de vídeo central. Además, diseña secuencias de injección de movimientos en niveles parciales, considerando la importancia de los componentes, y puede controlar movimientos en niveles parciales con precisión. Además, diseña dos marcos de reconstrucción de pesos complejos que modelan de manera evolutiva tanto el esquema 4D como los frames de vídeo, asegurando la consistencia del esquema en la generación de largos vídeos. Los resultados de los experimentos muestran la capacidad de modelar mundos y controlar decisivomente diferentes esquemas, representando una primera tentativa de simulación realista del mundo central y abrir caminos para la comunidad que desafía los nuevos límites en la modelación del mundo.",
      "upvotes": 22,
      "discussionId": "684a39639b38e1e5a33a683d",
      "projectPage": "https://playerone-hku.github.io/",
      "ai_summary": "PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.",
      "ai_keywords": [
        "egocentric realistic world simulator",
        "coarse-to-fine pipeline",
        "pretraining",
        "finetuning",
        "synchronous motion-video data",
        "automatic construction pipeline",
        "part-disentangled motion injection",
        "joint reconstruction framework",
        "4D scene",
        "video frames",
        "scene consistency",
        "long-form video generation",
        "worldconsistent modeling"
      ]
    },
    "publishedAt": "2025-06-11T13:59:53.000Z",
    "title": "PlayerOne: Egocentric World Simulator",
    "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09995.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09350",
      "authors": [
        {
          "_id": "684a79ca9b38e1e5a33a68bf",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c0",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c1",
          "name": "Hao He",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c2",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c3",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c4",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c5",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c6",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c7",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T03:04:23.000Z",
      "submittedOnDailyAt": "2025-06-12T05:25:53.654Z",
      "title": "Generación de videos de diálogo por unidades de tiempo utilizando entrenamiento posterior de reconocimiento relativo a cualquier persona",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Actualmente, los grandes modelos de generación de vídeo requieren una gran cantidad de cálculo y no pueden ejecutarse en tiempo real, lo que dificulta su introducción en aplicaciones interactivas o en tiempo real. En este estudio, proponemos la transformación de un modelo de difusión latina preentrenado en un generador de vídeos temporal y interactivo utilizando el aprendizaje adversario automático de retroalimentación (AAPT). Nuestro modelo utiliza 1NFE (1 Neural Fibonacci) para generar una frame latina automáticamente en un solo tiempo. El modelo puede streamear los resultados temporales a los usuarios y responder interactivamente para generar la siguiente frame latina. A diferencia de los métodos tradicionales, nuestro enfoque revisa el aprendizaje adversario efectivo para la generación automática. Esto permite diseñar una arquitectura eficiente utilizando caching de KV y reducir la acumulación de errores durante la generación de vídeos largos. A través de los resultados de las pruebas, nuestro modelo de 8B utiliza 1NFE para generar una frame latina en tiempo real, y se ha diseñado para generar una arquitectura eficiente utilizando caching de KV y reducir la acumulación de errores durante la generación de vídeos largos.",
      "upvotes": 22,
      "discussionId": "684a79ca9b38e1e5a33a68c8",
      "projectPage": "https://seaweed-apt.com/2",
      "ai_summary": "Autoregressive adversarial post-training transforms pre-trained latent video diffusion models into real-time, interactive video generators with reduced computational requirements.",
      "ai_keywords": [
        "autoregressive adversarial post-training",
        "latent video diffusion model",
        "autoregressive generation",
        "neural function evaluation",
        "KV cache",
        "student-forcing",
        "real-time video generation",
        "24fps",
        "736x416 resolution",
        "1280x720 resolution",
        "H100"
      ]
    },
    "publishedAt": "2025-06-10T23:04:23.000Z",
    "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
    "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09350.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7093
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08889",
      "authors": [
        {
          "_id": "684a39599b38e1e5a33a6822",
          "name": "Yizhao Gao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6823",
          "name": "Shuming Guo",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6824",
          "name": "Shijie Cao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6825",
          "name": "Yuqing Xia",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6826",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6827",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6828",
          "name": "Lingxiao Ma",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6829",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682a",
          "name": "Tianzhu Ye",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682b",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682c",
          "name": "Hayden Kwok-Hay So",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682d",
          "name": "Yu Hua",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682e",
          "name": "Ting Cao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682f",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6830",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T15:17:26.000Z",
      "submittedOnDailyAt": "2025-06-12T00:54:43.454Z",
      "title": "SeerAttention-R: Attend-R para Attención Esparsa a Largo Plazo",
      "submittedOnDailyBy": {
        "_id": "661c96f48921f03a9dae04c3",
        "avatarUrl": "/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg",
        "isPro": false,
        "fullname": "Yizhao Gao",
        "user": "LongMountain",
        "type": "user"
      },
      "summary": "Introducimos SeerAttention-R, un marco de atención esparsa diseñado específicamente para la decodifica larga de modelos de inferencia. Extendiendo a partir de SeerAttention, SeerAttention-R mantiene el diseño de aprendizaje de la esparsidad de la atención mediante un mecanismo de auto-distilación de puertas, mientras que elimina la pooleadora de consultas para adaptarse a la decodificación autoregresiva. Mediante un mecanismo de puertas de bajo peso y modular, SeerAttention-R es flexible y puede integrarse fácilmente en modelos preentrenados existentes sin modificar los parámetros originales. Demostramos que, con solo 0.4B de etiquetas de entrenamiento, SeerAttention-R mantiene una precisión de inferencia casi sin pérdida en el benchmark AIME, con bloques de atención esparsos grandes (64/128), y un presupuesto de 4K de etiquetas. Utilizando TileLang, desarrollamos un núcleo de decodifica esparsa altamente optimizado, que alcanza una aceleración cercana al límite teórico de 9 veces con un 90% de esparsidad en una GPU H100, superando a FlashAttention-3. El código está disponible en la siguiente enlace: https://github.com/microsoft/SeerAttention.",
      "upvotes": 15,
      "discussionId": "684a39599b38e1e5a33a6833",
      "githubRepo": "https://github.com/microsoft/SeerAttention",
      "ai_summary": "SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.",
      "ai_keywords": [
        "sparse attention",
        "reasoning models",
        "self-distilled gating mechanism",
        "query pooling",
        "lightweight plug-in gating",
        "AIME benchmark",
        "TileLang",
        "sparse decoding kernel",
        "FlashAttention-3",
        "H100 GPU"
      ]
    },
    "publishedAt": "2025-06-10T11:17:26.000Z",
    "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
    "summary": "We introduce SeerAttention-R, a sparse attention framework specifically\ntailored for the long decoding of reasoning models. Extended from\nSeerAttention, SeerAttention-R retains the design of learning attention\nsparsity through a self-distilled gating mechanism, while removing query\npooling to accommodate auto-regressive decoding. With a lightweight plug-in\ngating, SeerAttention-R is flexible and can be easily integrated into existing\npretrained model without modifying the original parameters. We demonstrate that\nSeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning\naccuracy with 4K token budget in AIME benchmark under large sparse attention\nblock sizes (64/128). Using TileLang, we develop a highly optimized sparse\ndecoding kernel that achieves near-theoretical speedups of up to 9x over\nFlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:\nhttps://github.com/microsoft/SeerAttention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08889.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661c96f48921f03a9dae04c3",
      "avatarUrl": "/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg",
      "fullname": "Yizhao Gao",
      "name": "LongMountain",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09790",
      "authors": [
        {
          "_id": "684a33989b38e1e5a33a6804",
          "name": "Zhenran Xu",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6805",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6806",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6807",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6808",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6809",
          "name": "Kaifu Zhang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a680a",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a680b",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T14:35:15.000Z",
      "submittedOnDailyAt": "2025-06-12T00:38:07.422Z",
      "title": "ComfyUI-R1: Revisión del modelo de inferencia para la generación de flujos de trabajo",
      "submittedOnDailyBy": {
        "_id": "639c379cdb7c5f35004066cb",
        "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
        "isPro": false,
        "fullname": "Zhenran Xu",
        "user": "imryanxu",
        "type": "user"
      },
      "summary": "El contenido generado por IA ha evolucionado desde procesos de trabajo modulares en un solo modelo a la posibilidad de personalizar procesos creativos en plataformas como ComfyUI. Sin embargo, para crear procesos de trabajo efectivos, es necesario una gran conocidación profesional para ajustar muchos componentes especializados. Esto puede imponer una curva de aprendizaje rápido a los usuarios. Para enfrentar estas desafíos, presentamos ComfyUI-R1, el primer modelo de inferencia grande utilizado para automatizar procesos de trabajo. Hemos construido un conjunto de datos de procesos de trabajo de 4K, que incluye datos de CoT (reasoning chain) para selección de nodos, planificación de procesos de trabajo y representación de procesos de trabajo a nivel de código. ComfyUI-R1 se entrena a través de un marco de dos etapas: (1) ajuste de CoT para adaptar el modelo al área de ComfyUI; (2) luego de fine-tuning, se realiza un entrenamiento de reinforcement con una combinación de reglas y dominio para estimular la capacidad de inferencia. Los resultados experimentales muestran que nuestro modelo de 7000 millones de parámetros alcanza un rendimiento de 97% de formación, altos rendimientos, y puntuaciones de F1 a nivel de nodo y de grafo, superando significativamente los métodos anteriores y los modelos líderes como GPT-4o y Claude series. Un análisis adicional destaca la importancia central del proceso de inferencia y la ventaja de convertir procesos de trabajo en código. La comparación cualitativa revela las ventajas de nuestro método para procesos complejos de generación y la potencial de CoT en la creación artística con IA.",
      "upvotes": 14,
      "discussionId": "684a33989b38e1e5a33a680c",
      "projectPage": "https://github.com/AIDC-AI/ComfyUI-Copilot",
      "githubRepo": "https://github.com/AIDC-AI/ComfyUI-Copilot",
      "ai_summary": "ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.",
      "ai_keywords": [
        "modular workflows",
        "ComfyUI",
        "large reasoning model",
        "automated workflow generation",
        "chain-of-thought (CoT) reasoning",
        "node selection",
        "workflow planning",
        "code-level workflow representation",
        "CoT fine-tuning",
        "reinforcement learning",
        "fine-grained rule-metric hybrid reward",
        "format validity",
        "structural integrity",
        "node-level fidelity",
        "GPT-4o",
        "Claude series",
        "pass rate",
        "node-level F1 scores",
        "graph-level F1 scores",
        "intricate workflows",
        "diverse nodes",
        "qualitative comparison",
        "AI art creation"
      ]
    },
    "publishedAt": "2025-06-11T10:35:15.000Z",
    "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation",
    "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09790.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639c379cdb7c5f35004066cb",
      "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
      "fullname": "Zhenran Xu",
      "name": "imryanxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09003",
      "authors": [
        {
          "_id": "6848eed742e4f9106973f2cf",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d0",
          "name": "Jiaxi Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d1",
          "name": "Min Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d2",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d3",
          "name": "Mouxiang Chen",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d4",
          "name": "Jiajun Zhang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d5",
          "name": "Zeyu Cui",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d6",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d7",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:23:33.000Z",
      "submittedOnDailyAt": "2025-06-12T00:17:18.390Z",
      "title": "SWE-Flow: Síntesis de datos de desarrollo de software dirigida por pruebas",
      "submittedOnDailyBy": {
        "_id": "64c38871f9cd765462fa1a17",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg",
        "isPro": false,
        "fullname": "Lei Zhang",
        "user": "Lemoncoke",
        "type": "user"
      },
      "summary": "Hemos introducido **SWE-Flow**. Es un nuevo marco de trabajo para la síntesis de datos basado en el desarrollo dirigido por pruebas (TDD). Los datos de software ingeniería existentes se basan en problemas humanos presentados, pero **SWE-Flow** infere automáticamente los pasos de desarrollo incremental directamente desde los pruebas unitarias. Las pruebas unitarias contienen altos niveles de requisitos, por lo que el centro de **SWE-Flow** es la construcción del grafo de dependencias de tiempo de ejecución (RDG). Este captura exactamente las interacciones entre funciones para generar un *plan de desarrollo estructurado por etapas*. En cada etapa, **SWE-Flow** genera un código parcial, las pruebas unitarias correspondientes y las modificaciones de código necesarias para que el trabajo sea completamente verificable bajo TDD. Con este enfoque, hemos generado en proyectos reales de GitHub 16,061 datos de entrenamiento y 2,020 datos de prueba para crear el marco de referencia **SWE-Flow-Eval**. Nuestros experimentos muestran que la mejora significativa en el rendimiento de codificación basado en TDD se logra al ajustar modelos abiertos en este conjunto de datos. Para más investigación, publicamos en [Github](https://github.com/Hambaobao/SWE-Flow) todos los códigos, conjuntos de datos, modelos y imágenes de Docker.",
      "upvotes": 13,
      "discussionId": "6848eed842e4f9106973f2d8",
      "githubRepo": "https://github.com/Hambaobao/SWE-Flow",
      "ai_summary": "A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.",
      "ai_keywords": [
        "Test-Driven Development (TDD)",
        "Runtime Dependency Graph (RDG)",
        "SWE-Flow",
        "unit tests",
        "development schedule",
        "SWE-Flow-Eval",
        "fine-tuning",
        "open model"
      ]
    },
    "publishedAt": "2025-06-10T13:23:33.000Z",
    "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner",
    "summary": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in\nTest-Driven Development (TDD). Unlike existing software engineering data that\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\ndevelopment steps directly from unit tests, which inherently encapsulate\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\nenabling the generation of a structured, step-by-step *development schedule*.\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\ntests, and the necessary code modifications, resulting in fully verifiable TDD\ntasks. With this approach, we generated 16,061 training instances and 2,020\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\nbenchmark. Our experiments show that fine-tuning open model on this dataset\nsignificantly improves performance in TDD-based coding. To facilitate further\nresearch, we release all code, datasets, models, and Docker images at\n[Github](https://github.com/Hambaobao/SWE-Flow).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09003.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64c38871f9cd765462fa1a17",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg",
      "fullname": "Lei Zhang",
      "name": "Lemoncoke",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09984",
      "authors": [
        {
          "_id": "684a49fa9b38e1e5a33a6884",
          "name": "Zhenzhi Wang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6885",
          "name": "Jiaqi Yang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6886",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6887",
          "name": "Chao Liang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6888",
          "name": "Gaojie Lin",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6889",
          "name": "Zerong Zheng",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a688a",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a688b",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:57:09.000Z",
      "submittedOnDailyAt": "2025-06-12T02:02:32.983Z",
      "title": "InterActHuman: Animación humana multi-concepto y voz que coinciden en orden con la animación",
      "submittedOnDailyBy": {
        "_id": "6519346a186bc3b6997c1aaf",
        "avatarUrl": "/avatars/8981bd278962da50f9bbfb92c2abe2bf.svg",
        "isPro": false,
        "fullname": "Zhenzhi Wang",
        "user": "zhenzhiwang",
        "type": "user"
      },
      "summary": "Desde el final hasta el final, la human animation ha experimentado un desarrollo notable recientemente gracias a la utilización de condiciones ricas de multimodalidad (por ejemplo, texto, imágenes, sonido). Sin embargo, muchos de los métodos actuales realizan animaciones de entidades únicas, inyectando las condiciones de manera global y, cuando en un mismo video se presentan múltiples conceptos, ignoran la interacción rica entre personas y el entre personas y objetos. Esta asunción global impide el control preciso y en el campo de trabajo global de múltiples conceptos, incluyendo a personas o objetos, y también impide a las aplicaciones. En este artículo, se renuncia a la asunción de entidades únicas y se introduce un nuevo marco de trabajo para forzar una fuerte y propia unión en las huellas espacio-temporales de cada identidad a partir de las condiciones de modalidad. Al proporcionar referencias de múltiples conceptos, nuestro método utiliza un productor de máscaras para estimar automáticamente la información de redacción de lámina, alineando el código de color facial de las imágenes de ruido con las referencias. Además, se inyecta la condición de sonido local en los áreas correspondientes para realizar una correspondencia de modalidad que coincida con las láminas de múltiples conceptos. Este diseño permite la generación de videos de alta calidad centrados en la persona con múltiples conceptos. A través de los resultados experimentales, demostramos la efectividad de nuestro control explícito de redacción de lámina y prueba la efectividad relativa de las condiciones ricas de multimodalidad en comparación con los métodos existentes.",
      "upvotes": 9,
      "discussionId": "684a49fa9b38e1e5a33a688c",
      "ai_summary": "A new framework enables precise, per-identity control of multiple concepts in end-to-end human animation by enforcing region-specific binding of multi-modal conditions.",
      "ai_keywords": [
        "human animation",
        "multi-modal conditions",
        "mask predictor",
        "denoised video",
        "layout information",
        "local audio condition",
        "controllable multi-concept videos",
        "explicit layout control"
      ]
    },
    "publishedAt": "2025-06-11T13:57:09.000Z",
    "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions",
    "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6519346a186bc3b6997c1aaf",
      "avatarUrl": "/avatars/8981bd278962da50f9bbfb92c2abe2bf.svg",
      "fullname": "Zhenzhi Wang",
      "name": "zhenzhiwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09937",
      "authors": [
        {
          "_id": "684a3d639b38e1e5a33a686d",
          "name": "Qiao Gu",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a686e",
          "name": "Yuanliang Ju",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a686f",
          "name": "Shengxiang Sun",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6870",
          "name": "Igor Gilitschenski",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6871",
          "name": "Haruki Nishimura",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6872",
          "name": "Masha Itkina",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6873",
          "name": "Florian Shkurti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T16:59:13.000Z",
      "submittedOnDailyAt": "2025-06-12T01:08:05.216Z",
      "title": "SAFE: Modelo de Visión, Lenguaje y Acción para Detección de Fallas en Tareas Múltiples",
      "submittedOnDailyBy": {
        "_id": "63d1df92f7f31a66a2d7292c",
        "avatarUrl": "/avatars/eb3339c1f2c82742b518b8a7f142e99a.svg",
        "isPro": false,
        "fullname": "Qiao Gu",
        "user": "guqiao",
        "type": "user"
      },
      "summary": "VISION-LANGUAGE-ACTION MODEL (VLAs) tiene el potencial de mostrar acciones de robot en diversas tareas pero en la implementación en nuevas tareas su éxito está limitado. Estas políticas necesitan un detector de fallos que permita que el robot se detenga, retroceda o solicite ayuda para funcionar de manera segura. Sin embargo, actualmente los detectores de fallos no son generalizables para detectar fallos en nuevas tareas o en nuevos entornos. Este artículo introduce el problema de detectar fallos en múltiples tareas y propone un detector de fallos SAFE adecuado para políticas generales como VLAs. SAFE se basa en el análisis de las características específicas de VLAs y descubre que VLAs tienen alto nivel de conocimiento sobre el éxito y el fracaso de las tareas que se pueden aplicar a otras tareas. Desde esta perspectiva, SAFE se diseña para predecir una única escala que representa la probabilidad de fracaso de la tarea a partir de las características internas de VLAs. SAFE se entrena tanto con éxitos como con fracasos y se evalúa en nuevas tareas. SAFE es compatible con diferentes arquitecturas de políticas. Se ha testado rigurosamente en OpenVLA, pi_0 y pi_0-FAST en entornos literarios y reales. SAFE logró alcanzar la mejor performance de detección de fallos, equilibrando precisión y tiempo de detección frente a diferentes límites de referencia. Para obtener resultados detallados, consulte https://vla-safe.github.io/.",
      "upvotes": 3,
      "discussionId": "684a3d639b38e1e5a33a6874",
      "ai_summary": "SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.",
      "ai_keywords": [
        "vision-language-action models",
        "VLAs",
        "multitask failure detection",
        "failure detector",
        "feature space",
        "high-level knowledge",
        "scalar prediction",
        "rollout",
        "conformal prediction"
      ]
    },
    "publishedAt": "2025-06-11T12:59:13.000Z",
    "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
    "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, pi_0,\nand pi_0-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09937.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d1df92f7f31a66a2d7292c",
      "avatarUrl": "/avatars/eb3339c1f2c82742b518b8a7f142e99a.svg",
      "fullname": "Qiao Gu",
      "name": "guqiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08001",
      "authors": [
        {
          "_id": "684a649f9b38e1e5a33a6895",
          "name": "Zeju Qiu",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6896",
          "name": "Simon Buchholz",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6897",
          "name": "Tim Z. Xiao",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6898",
          "name": "Maximilian Dax",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6899",
          "name": "Bernhard Schölkopf",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a689a",
          "name": "Weiyang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:59:34.000Z",
      "submittedOnDailyAt": "2025-06-12T03:55:47.829Z",
      "title": "La transformación ortogonal igualdad para la reformulación de entrenamiento de LLM",
      "submittedOnDailyBy": {
        "_id": "648905d1a15c43c791d4381f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
        "isPro": false,
        "fullname": "Weiyang Liu",
        "user": "wy1iu",
        "type": "user"
      },
      "summary": "La raíz es un modelo de lenguaje de la inteligencia artificial (LLMs) que lidera el rápido desarrollo de la IA, y su efectivo y confiable entrenamiento es uno de los mayores problemas de la disciplina. Para abordar este problema, proponemos un nuevo algoritmo de repararametrización: POET. En particular, POET utiliza dos matrices ortogonales aprendibles y una matriz de pesos de números aleatorios fijos para repararmentrar cada neurona. Con esta diseño, POET puede mantener las características espectrales de la matriz de pesos de manera demostrativamente, lo que permite una optimización estable del funcional objetivo y mejora en la capacidad de generalización. Además, hemos desarrollado métodos eficientes para fortalecer la flexibilidad y escalabilidad de POET. Los experimentos dispersos demuestran la eficacia y escalabilidad de POET en el entrenamiento de LLMs.",
      "upvotes": 2,
      "discussionId": "684a649f9b38e1e5a33a689b",
      "projectPage": "https://spherelab.ai/poet/",
      "githubRepo": "https://github.com/Sphere-AI-Lab/poet",
      "ai_summary": "POET is a reParameterized training algorithm using Orthogonal Equivalence Transformation to optimize neurons in large language models, ensuring stable training and improved generalization.",
      "ai_keywords": [
        "POET",
        "reParameterized training algorithm",
        "Orthogonal Equivalence Transformation",
        "orthogonal matrices",
        "spectral properties",
        "weight matrices",
        "large language models",
        "generalization",
        "efficient approximations",
        "training",
        "scalability"
      ]
    },
    "publishedAt": "2025-06-09T13:59:34.000Z",
    "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
    "summary": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648905d1a15c43c791d4381f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
      "fullname": "Weiyang Liu",
      "name": "wy1iu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08900",
      "authors": [
        {
          "_id": "684a96ab9aebf043cf7bdb2e",
          "name": "José Morano",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb2f",
          "name": "Botond Fazekas",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb30",
          "name": "Emese Sükei",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb31",
          "name": "Ronald Fecso",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb32",
          "name": "Taha Emre",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb33",
          "name": "Markus Gumpinger",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb34",
          "name": "Georg Faustmann",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb35",
          "name": "Marzieh Oghbaie",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb36",
          "name": "Ursula Schmidt-Erfurth",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb37",
          "name": "Hrvoje Bogunović",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655b3383ed8df831286969f0/FuwagzdL2fBNhZyfwg22J.png"
      ],
      "publishedAt": "2025-06-10T15:25:55.000Z",
      "submittedOnDailyAt": "2025-06-12T07:31:36.181Z",
      "title": "MIRAGE: Modelo básico de DAMO y benchmark de análisis de imágenes OCT con capas de cabello y misterioso",
      "submittedOnDailyBy": {
        "_id": "655b3383ed8df831286969f0",
        "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
        "isPro": false,
        "fullname": "José Morano",
        "user": "j-morano",
        "type": "user"
      },
      "summary": "El lenguaje inglés proporcionado ha sido traducido al español de manera profesional y precisa. Aquí está el resultado de la traducción:\n\n\"El Inteligencia Artificial (IA) funciona como una herramienta básica para el análisis de imágenes relacionadas con la vista que los médicos clínicos utilizan. En particular, se utiliza para el análisis de imágenes como la Topología Óptica Cohérente (OCT). Sin embargo, el desarrollo de modelos IA requiere una anotación extensa y tiende a presentar una pérdida de rendimiento cuando se aplican a datos independientes y no vistos antes. El Modelo Básico (FM) es un gran modelo IA, entrenado con grandes conjuntos de datos sin etiquetas, y muestra un excelente rendimiento frente a estos desafíos. Sin embargo, los FM actuales relacionados con la vista carecen particularmente de una validación muy detallada para tareas de segmentación y se centran únicamente en un modelo de imágenes. En este contexto, proponemos MIRAGE, un nuevo FM multimodelo para el análisis de imágenes OCT y de óptica de láser de escaneo (SLO). Además, proponemos también un nuevo marco de evaluación para tareas de clasificación y segmentación OCT/SLO. La comparación con modelos generales, FMs profesionales y métodos de segmentación demuestra que MIRAGE muestra un excelente rendimiento en ambas tareas y constituye la base para el desarrollo de un potente sistema IA para el análisis de imágenes OCT de óptica de láser de escaneo. MIRAGE y el marco de evaluación están disponibles para uso público: https://github.com/j-morano/MIRAGE.\"",
      "upvotes": 1,
      "discussionId": "684a96ab9aebf043cf7bdb38",
      "githubRepo": "https://github.com/j-morano/MIRAGE",
      "ai_summary": "MIRAGE, a multimodal foundation model, outperforms existing models in the classification and segmentation of OCT and SLO images, demonstrating its potential for robust AI in ophthalmologic image analysis.",
      "ai_keywords": [
        "foundational models",
        "multimodal models",
        "optical coherence tomography",
        "scanning laser ophthalmoscopy",
        "image segmentation",
        "evaluation benchmark"
      ]
    },
    "publishedAt": "2025-06-10T11:25:55.000Z",
    "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive\n  retinal OCT image analysis",
    "summary": "Artificial intelligence (AI) has become a fundamental tool for assisting\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\n(OCT). However, developing AI models often requires extensive annotation, and\nexisting models tend to underperform on independent, unseen data. Foundation\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\npromise in overcoming these challenges. Nonetheless, available FMs for\nophthalmology lack extensive validation, especially for segmentation tasks, and\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\nclassification and segmentation tasks. The comparison with general and\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\nboth types of tasks, highlighting its suitability as a basis for the\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\nand the evaluation benchmark are publicly available:\nhttps://github.com/j-morano/MIRAGE.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655b3383ed8df831286969f0/FuwagzdL2fBNhZyfwg22J.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655b3383ed8df831286969f0",
      "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
      "fullname": "José Morano",
      "name": "j-morano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09007",
      "authors": [
        {
          "_id": "6849516c42e4f9106973f4d1",
          "name": "Sophia Tang",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d2",
          "name": "Yinuo Zhang",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d3",
          "name": "Alexander Tong",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d4",
          "name": "Pranam Chatterjee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:29:48.000Z",
      "submittedOnDailyAt": "2025-06-12T01:32:31.298Z",
      "title": "Punto Shuringger Bridge Matching",
      "submittedOnDailyBy": {
        "_id": "64cd5b3f0494187a9e8b7c69",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
        "isPro": false,
        "fullname": "Pranam Chatterjee",
        "user": "pranamanam",
        "type": "user"
      },
      "summary": "Predecir la trayectoria intermedia entre la distribución inicial y la distribución final es un problema esencial en la modelación generativa. En los métodos actuales, la modelación de un paso probabilístico es efectiva para aprender la mapeo entre ambas distribuciones, y se han propuesto técnicas como Flow Matching y Sinkhorn Bridging Matching. Sin embargo, estos métodos están limitados a la movilidad de una sola modalidad y no permiten comprender la evolución divergente desde un solo punto común hacia diversos resultados. Para abordar este problema, se presenta BranchSBM (Branching Sinkhorn Bridging Matching). BranchSBM introduce un nuevo marco de trabajo para entrenar un Sinkhorn Bridging de tipo ramificado, donde los procesos de tiempo dependientes de los campos de velocidad y la evolución se modelan con múltiples parámetros, permitiendo la integración de ramificaciones a nivel poblacional en diversas distribuciones finales. BranchSBM permite modelar la navegación de superficies múltiples, la modelación de ramificaciones celulares bajo condiciones uniformes de ancestros, y la simulación de ramificaciones en la respuesta celular, lo que hace de él un modelo más expresivo y esencial para estas tareas.",
      "upvotes": 0,
      "discussionId": "6849516d42e4f9106973f4d5",
      "ai_summary": "BranchSBM, a novel generative modeling framework, extends Schr\\\"odinger Bridge Matching to model branched stochastic paths and multi-path evolution from a single initial distribution to multiple outcomes.",
      "ai_keywords": [
        "flow matching",
        "Schr\\\"odinger Bridge Matching",
        "Branched Schr\\\"odinger Bridge Matching",
        "BranchSBM",
        "time-dependent velocity fields",
        "growth processes",
        "multi-path surface navigation",
        "cell fate bifurcations",
        "cellular responses to perturbations"
      ]
    },
    "publishedAt": "2025-06-10T13:29:48.000Z",
    "title": "Branched Schrödinger Bridge Matching",
    "summary": "Predicting the intermediate trajectories between an initial and target\ndistribution is a central problem in generative modeling. Existing approaches,\nsuch as flow matching and Schr\\\"odinger Bridge Matching, effectively learn\nmappings between two distributions by modeling a single stochastic path.\nHowever, these methods are inherently limited to unimodal transitions and\ncannot capture branched or divergent evolution from a common origin to multiple\ndistinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge\nMatching (BranchSBM), a novel framework that learns branched Schr\\\"odinger\nbridges. BranchSBM parameterizes multiple time-dependent velocity fields and\ngrowth processes, enabling the representation of population-level divergence\ninto multiple terminal distributions. We show that BranchSBM is not only more\nexpressive but also essential for tasks involving multi-path surface\nnavigation, modeling cell fate bifurcations from homogeneous progenitor states,\nand simulating diverging cellular responses to perturbations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09007.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cd5b3f0494187a9e8b7c69",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
      "fullname": "Pranam Chatterjee",
      "name": "pranamanam",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]