[
  {
    "paper": {
      "id": "2504.20734",
      "authors": [
        {
          "_id": "6811966ae20ba7d0683b8adc",
          "user": {
            "_id": "66d30f5fad293ffc4b7672bc",
            "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
            "isPro": false,
            "fullname": "Woongyeong Yeo",
            "user": "wgcyeo",
            "type": "user"
          },
          "name": "Woongyeong Yeo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:56:03.853Z",
          "hidden": false
        },
        {
          "_id": "6811966ae20ba7d0683b8add",
          "user": {
            "_id": "66ed7737f2f27a5dfd81ef09",
            "avatarUrl": "/avatars/f45eea356e92ac7b3db23c2c92dec9fa.svg",
            "isPro": false,
            "fullname": "Kangsan Kim",
            "user": "KangsanKim71",
            "type": "user"
          },
          "name": "Kangsan Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:56:00.948Z",
          "hidden": false
        },
        {
          "_id": "6811966ae20ba7d0683b8ade",
          "name": "Soyeong Jeong",
          "hidden": false
        },
        {
          "_id": "6811966ae20ba7d0683b8adf",
          "user": {
            "_id": "63036b6c5c70c21d0ea79d48",
            "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
            "isPro": false,
            "fullname": "Jinheon Baek",
            "user": "jinheon",
            "type": "user"
          },
          "name": "Jinheon Baek",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:56:07.351Z",
          "hidden": false
        },
        {
          "_id": "6811966ae20ba7d0683b8ae0",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T13:18:58.000Z",
      "submittedOnDailyAt": "2025-04-30T01:50:28.624Z",
      "title": "UniversalRAG: Búsqueda de una fusión entre múltiples documentos, modelos diversos y granularidades, generación de razones de la visión universal",
      "submittedOnDailyBy": {
        "_id": "66d30f5fad293ffc4b7672bc",
        "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
        "isPro": false,
        "fullname": "Woongyeong Yeo",
        "user": "wgcyeo",
        "type": "user"
      },
      "summary": "La RAG (Retrieval-Augmented Generation) muestra la posibilidad de significativamente mejorar la precisión factual de las respuestas del modelo basándose en conocimientos externos relacionados con la pregunta. Sin embargo, el enfoque actual de la RAG está principalmente limitado a bases de texto, y los esfuerzos recientes han tratado de expandir la RAG para imágenes o vídeos, pero generalmente funcionan en un corpus propio del modelo. Las preguntas reales plantean un gran rango de diversas fuentes de conocimiento que no se pueden investigar en un solo origen. En este contexto, presentamos un nuevo marco de trabajo de RAG llamado \"UniversalRAG\", que permite la búsqueda y integración de conocimientos de diferentes modelos y modalidades. En particular, UniversalRAG forza a todos los modelos a obtener una representación unificada desde un solo espacio de conocimiento integrado, y luego, después de observar los intercambios entre modelos, propone la selección dinámica de un corpus óptimo para cada modelo y la ejecución de búsquedas específicas. Además, UniversalRAG permite a los modelos en un rango más amplio de aplicaciones, asemejando cada modelo a diferentes niveles de entrada y facilitando investigaciones adecuadas en función de la complejidad y el rango de la pregunta. UniversalRAG ha sido validado en 8 marcos de referencia y muestra un rendimiento excelente, superando tanto los límites de base del modelo como la línea base unificada.",
      "upvotes": 37,
      "discussionId": "6811966ae20ba7d0683b8b0e",
      "projectPage": "https://universalrag.github.io",
      "githubRepo": "https://github.com/wgcyeo/UniversalRAG",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "factual accuracy",
        "external knowledge",
        "text-only corpus",
        "modality-specific corpus",
        "heterogenous sources",
        "diverse modalities",
        "granularities",
        "modality gap",
        "modality-aware routing mechanism",
        "targeted retrieval",
        "granularity levels",
        "fine-tuned retrieval",
        "multi-modal benchmarks",
        "modality-specific baselines",
        "unified baselines"
      ]
    },
    "publishedAt": "2025-04-29T09:18:58.000Z",
    "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
    "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20734.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d30f5fad293ffc4b7672bc",
      "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
      "fullname": "Woongyeong Yeo",
      "name": "wgcyeo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20571",
      "authors": [
        {
          "_id": "681187ddda5ce4cbd7556714",
          "user": {
            "_id": "653586fae778506c5b38a3f1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653586fae778506c5b38a3f1/GL_RShZhAkEZmIinA5_8E.jpeg",
            "isPro": false,
            "fullname": "Yiping Wang",
            "user": "ypwang61",
            "type": "user"
          },
          "name": "Yiping Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T09:58:59.486Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556715",
          "user": {
            "_id": "673a83b99e6f1c0d81a771fc",
            "avatarUrl": "/avatars/f3d8e1bf7d4c36b21adee632ea12ffe0.svg",
            "isPro": false,
            "fullname": "Qing Yang",
            "user": "hushqyang",
            "type": "user"
          },
          "name": "Qing Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:56:17.953Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556716",
          "user": {
            "_id": "64a85e23b6512b8328f9d9e2",
            "avatarUrl": "/avatars/4a6b35752d3f76cb03278f52b3b43426.svg",
            "isPro": false,
            "fullname": "Zhiyuan Zeng",
            "user": "ZhiyuanZeng",
            "type": "user"
          },
          "name": "Zhiyuan Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T09:59:12.620Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556717",
          "user": {
            "_id": "63815eff4761ddfa00903762",
            "avatarUrl": "/avatars/3419b239d42e091586f1c51b526d88e5.svg",
            "isPro": false,
            "fullname": "Liliang Ren",
            "user": "renll",
            "type": "user"
          },
          "name": "Liliang Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T09:59:18.310Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556718",
          "name": "Lucas Liu",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556719",
          "user": {
            "_id": "61942296d5c2ba6daa290357",
            "avatarUrl": "/avatars/594021cc183c4922d48b46f43772a062.svg",
            "isPro": false,
            "fullname": "Baolin Peng",
            "user": "Baolin",
            "type": "user"
          },
          "name": "Baolin Peng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T09:59:45.735Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671a",
          "name": "Hao Cheng",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671b",
          "user": {
            "_id": "6310493158d83e8f64dc8c55",
            "avatarUrl": "/avatars/5f91ac4dfec0d6a5bf7bad6094f0fd0f.svg",
            "isPro": false,
            "fullname": "Xuehai He",
            "user": "Xuehai",
            "type": "user"
          },
          "name": "Xuehai He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T09:59:52.344Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671c",
          "user": {
            "_id": "633523b131a2be3938ca1016",
            "avatarUrl": "/avatars/06a18f80927289bb949d9f19ffdc4bda.svg",
            "isPro": false,
            "fullname": "Kuan Wang",
            "user": "Keynes",
            "type": "user"
          },
          "name": "Kuan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T09:59:58.392Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671d",
          "user": {
            "_id": "641904caf9d6f1d772ec7af7",
            "avatarUrl": "/avatars/4a63eac71eb30f70b1a0e9d4708f26c1.svg",
            "isPro": false,
            "fullname": "Jianfeng Gao",
            "user": "wyngjf",
            "type": "user"
          },
          "name": "Jianfeng Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:00:04.685Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671e",
          "user": {
            "_id": "64da876370446182be5b608d",
            "avatarUrl": "/avatars/e412fdc71404ecdf638e416846e3ebfb.svg",
            "isPro": false,
            "fullname": "Weizhu Chen",
            "user": "chenweizhu",
            "type": "user"
          },
          "name": "Weizhu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:00:10.823Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671f",
          "user": {
            "_id": "6463b2247572c66a8e625a57",
            "avatarUrl": "/avatars/7722fb5649d42d966ce1e478946d5f8f.svg",
            "isPro": false,
            "fullname": "Wang",
            "user": "Shuohang",
            "type": "user"
          },
          "name": "Shuohang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:00:19.855Z",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556720",
          "name": "Simon Shaolei Du",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556721",
          "user": {
            "_id": "6454c337a13edf669cd5d8ea",
            "avatarUrl": "/avatars/a383a0dda7c2ef6a0d6c3c64651f42ff.svg",
            "isPro": false,
            "fullname": "Yelong Shen",
            "user": "uuu6",
            "type": "user"
          },
          "name": "Yelong Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:00:33.179Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T09:24:30.000Z",
      "submittedOnDailyAt": "2025-04-30T00:46:23.617Z",
      "title": "1-punto aprendizaje ejemplo del aprendizaje lógico en modelos de lenguaje de gran escala",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Hemos demostrado que el aprendizaje por refuerzo con recompensas verificables (RLVR) funciona eficazmente como incentivo para mejorar la capacidad de inferencia matemática de grandes modelos de lenguaje (LLMs) utilizando un solo ejemplo de entrenamiento (1-shot RLVR). Al aplicar RLVR a un modelo basado en Qwen2.5-Math-1.5B, hemos aumentado la performance del modelo en MATH500 de 36.0% a 73.6%, y la media de 6 de los benchmarks matemáticos generales se ha mejorado de 17.6% a 35.7%. Estos resultados coinciden con los obtenidos en el subconjunto de 1.2k DeepScaleR (MATH500: 73.6%, media: 35.9%), incluyendo también los ejemplos mencionados anteriormente. Se observó una mejora significativa en diversos modelos (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), algoritmos de aprendizaje por refuerzo (GRPO y PPO), y otros ejemplos matemáticos (muchos mejoraron más del 30% en MATH500). Además, durante el proceso de entrenamiento con RLVR 1-shot, se observaron fenómenos interesantes como la generalización cruzada, la aumento de la frecuencia de auto-reflexión y la mejora de la performance en el test después de la consolidación del entrenamiento. Estos fenómenos se denominaron \"generalización después de la consolidación\". Además, se verificó que la efectividad de RLVR se debe principalmente a la pérdida de gradiente de la política. También se demostró la importancia de ajustar el coeficiente adecuado de la pérdida de entropía para promover la exploración en el entrenamiento con RLVR 1-shot. Además, se observó que aplicar solo la pérdida de entropía y no tener recompensas resultó en un aumento de 27.4% en la performance de Qwen2.5-Math-1.5B en MATH500. Estos hallazgos impulsan la investigación futura sobre la eficiencia de datos de RLVR y invitan a una revisión de los avances recientes y mecanismos fundamentales de RLVR. Nuestro código, modelos y datos están disponibles en open source en https://github.com/ypwang61/One-Shot-RLVR.",
      "upvotes": 30,
      "discussionId": "681187ddda5ce4cbd7556754",
      "ai_keywords": [
        "reinforcement learning with verifiable reward (RLVR)",
        "1-shot RLVR",
        "large language models (LLMs)",
        "Qwen2.5-Math-1.5B",
        "MATH500",
        "mathematical reasoning benchmarks",
        "Qwen2.5-Math-7B",
        "Llama3.2-3B-Instruct",
        "DeepSeek-R1-Distill-Qwen-1.5B",
        "GRPO",
        "PPO",
        "cross-domain generalization",
        "self-reflection",
        "post-saturation generalization",
        "policy gradient loss",
        "entropic exploration",
        "entropy loss"
      ]
    },
    "publishedAt": "2025-04-29T05:24:30.000Z",
    "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
    "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20571.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6748
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20595",
      "authors": [
        {
          "_id": "68118a9f4570c2ba44bf4418",
          "user": {
            "_id": "6334a0bd31a2be3938c59537",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6334a0bd31a2be3938c59537/kSetFUWAmJbPQ1KSlNKBr.jpeg",
            "isPro": false,
            "fullname": "Rulin Shao",
            "user": "rulins",
            "type": "user"
          },
          "name": "Rulin Shao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:02:20.688Z",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf4419",
          "user": {
            "_id": "64ff618c35ec9717626d1431",
            "avatarUrl": "/avatars/941befd75925d6b691133f84cce525f9.svg",
            "isPro": false,
            "fullname": "Rui Qiao",
            "user": "volpato30",
            "type": "user"
          },
          "name": "Rui Qiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:02:05.204Z",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441a",
          "name": "Varsha Kishore",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441b",
          "user": {
            "_id": "5f1eb362eec0ad2a071ad6e2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
            "isPro": false,
            "fullname": "Niklas Muennighoff",
            "user": "Muennighoff",
            "type": "user"
          },
          "name": "Niklas Muennighoff",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:02:27.811Z",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441c",
          "name": "Xi Victoria Lin",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441d",
          "name": "Daniela Rus",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441e",
          "name": "Bryan Kian Hsiang Low",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441f",
          "user": {
            "_id": "63a76d0de27a6dbd485fe863",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a76d0de27a6dbd485fe863/qJJwHOuvyQGq1o0KscOF_.jpeg",
            "isPro": false,
            "fullname": "Sewon Min",
            "user": "sewon",
            "type": "user"
          },
          "name": "Sewon Min",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:01:46.233Z",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf4420",
          "name": "Wen-tau Yih",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf4421",
          "user": {
            "_id": "641b4263abfce26bcf7b27de",
            "avatarUrl": "/avatars/e91b4205e4f74b0dd8c333c23203a924.svg",
            "isPro": false,
            "fullname": "Pang Wei Koh",
            "user": "pangwei",
            "type": "user"
          },
          "name": "Pang Wei Koh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:01:57.373Z",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf4422",
          "name": "Luke Zettlemoyer",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T09:49:28.000Z",
      "submittedOnDailyAt": "2025-04-30T00:58:16.950Z",
      "title": "Desarrollo de herramientas de búsqueda para apoyar el entrenamiento de IR",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "ReasonIR-8B es el primer modelo entrenado especialmente para tareas lógicas generales. Actualmente, este modelo se aplica a una variedad de tareas, pero muestra un efecto limitado en tareas lógicas. El conjunto de datos de entrenamiento actual se centra en preguntas de hecho simples sobre documentos. Hemos desarrollado un pipeline para generar preguntas complejas negativas que son relevantes pero no útiles, incluyendo ejemplos negativos. Usando este pipeline, el modelo ReasonIR-8B, entrenado con datos sintéticos y datos públicos, alcanzó nuevos niveles de excelencia en el benchmark lógico BRIGHT, con un nDCG@10 de 29.9 sin reranker y de 36.9 con reranker. En tareas de RAG, ReasonIR-8B aumentó el rendimiento en los conjuntos de datos MMLU y GPQA en 6.4% y 22.6%, respectivamente, y puede competir con otros modelos y búsquedas. Además, ReasonIR-8B utiliza la computación de prueba de manera más eficiente: mejora consistentemente el rendimiento con preguntas de información rica y largas en BRIGHT, y se puede combinar con un reranker de LLM para competir con otros modelos. Nuestro diseño de entrenamiento es general y puede fácilmente ser extendido a futuros modelos de LLM. Por lo tanto, abrimos nuestra código, datos y modelo bajo una licencia de código abierto.",
      "upvotes": 22,
      "discussionId": "68118aa44570c2ba44bf457b",
      "ai_keywords": [
        "retriever",
        "ReasonIR-8B",
        "general reasoning tasks",
        "synthetic data generation pipeline",
        "hard negative",
        "nDCG@10",
        "BRIGHT",
        "information retrieval (IR) benchmark",
        "RAG tasks",
        "MMLU",
        "GPQA",
        "closed-book baseline",
        "LLM reranker",
        "test-time compute",
        "rewritten queries",
        "LLM"
      ]
    },
    "publishedAt": "2025-04-29T05:49:28.000Z",
    "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
    "summary": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20595.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6748
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20157",
      "authors": [
        {
          "_id": "68119750ff0764f3840a7f93",
          "user": {
            "_id": "61e0c5053a1781f66b4e9aed",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642120523097-61e0c5053a1781f66b4e9aed.jpeg",
            "isPro": false,
            "fullname": "Zae Myung Kim",
            "user": "zaemyung",
            "type": "user"
          },
          "name": "Zae Myung Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:55:58.262Z",
          "hidden": false
        },
        {
          "_id": "68119750ff0764f3840a7f94",
          "name": "Chanwoo Park",
          "hidden": false
        },
        {
          "_id": "68119750ff0764f3840a7f95",
          "user": {
            "_id": "60985a0547dc3dbf8a976607",
            "avatarUrl": "/avatars/3c37bf4b7c9db83a46af7c473ee4eb86.svg",
            "isPro": false,
            "fullname": "Vipul Raheja",
            "user": "machineteacher",
            "type": "user"
          },
          "name": "Vipul Raheja",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:03:14.631Z",
          "hidden": false
        },
        {
          "_id": "68119750ff0764f3840a7f96",
          "user": {
            "_id": "64356b40a4bd75c62cbc5926",
            "avatarUrl": "/avatars/5f4c603464e9c8ad613a3a25fa4cacbf.svg",
            "isPro": false,
            "fullname": "Dongyeop Kang",
            "user": "dykang",
            "type": "user"
          },
          "name": "Dongyeop Kang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:03:26.612Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/tHS8gWUK0ptmNTs6lZck6.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/uMD9av8pogPwYTW-KNFJ2.png"
      ],
      "publishedAt": "2025-04-28T18:02:35.000Z",
      "submittedOnDailyAt": "2025-04-30T02:04:16.540Z",
      "title": "「Dirección de pensamiento evaluativo: optimización de meta-políticas para la optimización de modelos de recompensa」",
      "submittedOnDailyBy": {
        "_id": "6434b6619bd5a84b5dcfa4de",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434b6619bd5a84b5dcfa4de/h8Q6kPNjFNc03wmdboHzq.jpeg",
        "isPro": true,
        "fullname": "Young-Jun Lee",
        "user": "passing2961",
        "type": "user"
      },
      "summary": "Los dos principales límites en el método de arraymanción de criterios de recompensa para modelos de lenguaje grandes (LLMs) son: la vulnerabilidad de la captura de puntos de mala calidad del señal de recompensa para arraymancar y la dependencia de la ingeniería de prompts débil y costosa cuando se utilizan LLMs como modelos de recompensa. Presentamos un meta modelo de recompensa que ajusta los prompts del modelo de recompensa dinámicamente durante el proceso de entrenamiento, junto con un marco de optimización de políticas meta (MPO) para abordar estos desafíos. En MPO, el meta modelo de recompensa detecta cambios en el contexto de entrenamiento, ajusta continuamente los prompts del modelo de recompensa, mantiene altos niveles de arraymanción y proporciona recompensas adaptativas para evitar que la política adopte acciones no deseadas. Este enfoque de aprendizaje meta promueve la optimización de políticas estables y disminuye significativamente la necesidad de diseñar prompts de recompensa handcrafted. Comparado con modelos guiados por prompts handcrafted, este método muestra el mismo o mejor rendimiento. Además, MPO es efectivo en varias tareas y muestra que, en algunos casos, la disección de recompensas no es necesaria. La configuración de MPO para aprendizaje meta se puede fácilmente expandir a un marco de alto nivel de arraymanción, en comparación con RLAIF estándar. En resumen, este método resuelve los desafíos teóricos y prácticos en la arraymanción de criterios de recompensa para LLMs, introduciendo estrategias de arraymanción más robustas y adaptativas. Los códigos y modelos están disponibles públicamente.",
      "upvotes": 17,
      "discussionId": "68119751ff0764f3840a7fc5",
      "ai_keywords": [
        "Meta Policy Optimization (MPO)",
        "meta-reward model",
        "reward hacking",
        "prompt engineering",
        "policy optimization",
        "adaptive reward signal",
        "meta-learning approach",
        "prompt design",
        "reward-based RL alignment",
        "question answering",
        "mathematical reasoning"
      ]
    },
    "publishedAt": "2025-04-28T14:02:35.000Z",
    "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving\n  Reward Models",
    "summary": "Reward-based alignment methods for large language models (LLMs) face two key\nlimitations: vulnerability to reward hacking, where models exploit flaws in the\nreward signal; and reliance on brittle, labor-intensive prompt engineering when\nLLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a\nframework that addresses these challenges by integrating a meta-reward model\nthat dynamically refines the reward model's prompt throughout training. In MPO,\nthe meta-reward model monitors the evolving training context and continuously\nadjusts the reward model's prompt to maintain high alignment, providing an\nadaptive reward signal that resists exploitation by the policy. This\nmeta-learning approach promotes a more stable policy optimization, and greatly\nreduces the need for manual reward prompt design. It yields performance on par\nwith or better than models guided by extensively hand-crafted reward prompts.\nFurthermore, we show that MPO maintains its effectiveness across diverse tasks,\nsuch as question answering and mathematical reasoning, without requiring\nspecialized reward designs. Beyond standard RLAIF, MPO's meta-learning\nformulation is readily extensible to higher-level alignment frameworks.\nOverall, this method addresses theoretical and practical challenges in\nreward-based RL alignment for LLMs, paving the way for more robust and\nadaptable alignment strategies. The code and models will be publicly shared.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/tHS8gWUK0ptmNTs6lZck6.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/uMD9av8pogPwYTW-KNFJ2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20157.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "6434b6619bd5a84b5dcfa4de",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434b6619bd5a84b5dcfa4de/h8Q6kPNjFNc03wmdboHzq.jpeg",
      "fullname": "Young-Jun Lee",
      "name": "passing2961",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20995",
      "authors": [
        {
          "_id": "68118c049c2765c9323de70b",
          "user": {
            "_id": "6437c7dae282b4a48eaf065e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6437c7dae282b4a48eaf065e/AxodKQXyrviTFQRyjnL01.jpeg",
            "isPro": false,
            "fullname": "Haoyu Zhen",
            "user": "anyeZHY",
            "type": "user"
          },
          "name": "Haoyu Zhen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:04:19.238Z",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de70c",
          "name": "Qiao Sun",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de70d",
          "name": "Hongxin Zhang",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de70e",
          "name": "Junyan Li",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de70f",
          "name": "Siyuan Zhou",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de710",
          "user": {
            "_id": "63c9bd445fdc575773c732fe",
            "avatarUrl": "/avatars/def472d1ab3fbf751225357c0932ae7e.svg",
            "isPro": false,
            "fullname": "Yilun Du",
            "user": "yilundu",
            "type": "user"
          },
          "name": "Yilun Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-30T10:03:42.041Z",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de711",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T17:59:30.000Z",
      "submittedOnDailyAt": "2025-04-30T01:05:28.658Z",
      "title": "TesserAct: Aprendizaje de un modelo de cuerpo en cuatro dimensiones",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "En este artículo, se propone un nuevo enfoque efectivo para el aprendizaje de un modelo mundial de visualización en 4 dimensiones. Este enfoque permite predecir la evolución dinámica en un espacio tridimensional de las acciones de los agentes de visualización, asegurando también la coherencia espacial y temporal. Proponemos el uso de videos RGB-DN (RGB, profundidad, normal) para el aprendizaje de un modelo mundial en 4 dimensiones. Este enfoque incluye detalles de forma, estructura y cambios temporales en las predicciones del modelo, permitiendo además el aprendizaje preciso de su modelo mecánico. En particular, utilizamos modelos de profundidad y normal para agregar información a los datos de videos de manipulación de robots existentes. Luego, ajustamos un modelo de generación de videos para predecir simultáneamente RGB-DN (RGB, profundidad, normal) utilizando estos datos. Finalmente, proponemos un algoritmo para transformar directamente los videos RGB, profundidad y normal en un mundo 4D de alta calidad. Nuestro método garantiza la coherencia temporal y espacial en la predicción de espacios 4D a partir de escenarios de visualización, permitiendo la síntesis de nuevas imágenes en el entorno de visualización y fomentando la aprendizaje de estrategias que superan significativamente los modelos mundiales basados en videos existentes.",
      "upvotes": 9,
      "discussionId": "68118c089c2765c9323de81d",
      "ai_keywords": [
        "embodied world models",
        "4D world models",
        "RGB-DN (RGB, Depth, and Normal) videos",
        "video generation model",
        "inverse dynamic models",
        "robotic manipulation video datasets",
        "temporal coherence",
        "spatial coherence",
        "novel view synthesis",
        "policy learning"
      ]
    },
    "publishedAt": "2025-04-29T13:59:30.000Z",
    "title": "TesserAct: Learning 4D Embodied World Models",
    "summary": "This paper presents an effective approach for learning novel 4D embodied\nworld models, which predict the dynamic evolution of 3D scenes over time in\nresponse to an embodied agent's actions, providing both spatial and temporal\nconsistency. We propose to learn a 4D world model by training on RGB-DN (RGB,\nDepth, and Normal) videos. This not only surpasses traditional 2D models by\nincorporating detailed shape, configuration, and temporal changes into their\npredictions, but also allows us to effectively learn accurate inverse dynamic\nmodels for an embodied agent. Specifically, we first extend existing robotic\nmanipulation video datasets with depth and normal information leveraging\noff-the-shelf models. Next, we fine-tune a video generation model on this\nannotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for\neach frame. We then present an algorithm to directly convert generated RGB,\nDepth, and Normal videos into a high-quality 4D scene of the world. Our method\nensures temporal and spatial coherence in 4D scene predictions from embodied\nscenarios, enables novel view synthesis for embodied environments, and\nfacilitates policy learning that significantly outperforms those derived from\nprior video-based world models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20995.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6748
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16046",
      "authors": [
        {
          "_id": "68119c70e8a3493171fadce2",
          "user": {
            "_id": "62fb40b59af1d16bc0ac60f4",
            "avatarUrl": "/avatars/03ff66a419db8f2bc8e89a3b47aaaeac.svg",
            "isPro": false,
            "fullname": "Jack Zhang",
            "user": "jackzhang",
            "type": "user"
          },
          "name": "Jingyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:55:55.481Z",
          "hidden": false
        },
        {
          "_id": "68119c70e8a3493171fadce3",
          "name": "Jiacan Yu",
          "hidden": false
        },
        {
          "_id": "68119c70e8a3493171fadce4",
          "name": "Marc Marone",
          "hidden": false
        },
        {
          "_id": "68119c70e8a3493171fadce5",
          "name": "Benjamin Van Durme",
          "hidden": false
        },
        {
          "_id": "68119c70e8a3493171fadce6",
          "name": "Daniel Khashabi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:16:53.000Z",
      "submittedOnDailyAt": "2025-04-30T02:15:12.625Z",
      "title": "Certificado Reducción de Infringimiento de Derechos de Autor en Casos Peor Caso de LLM",
      "submittedOnDailyBy": {
        "_id": "62fb40b59af1d16bc0ac60f4",
        "avatarUrl": "/avatars/03ff66a419db8f2bc8e89a3b47aaaeac.svg",
        "isPro": false,
        "fullname": "Jack Zhang",
        "user": "jackzhang",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) en español presentan preocupaciones sobre la exposición de materiales de memoria de derechos de autor durante el período de entrenamiento reservado, y sobre la invasión de derechos de autor inconsciente después del entrenamiento. Esto ha llevado a la desarrollo del método \"borrado de derechos\" y a la creación de una aproximación de entrenamiento posterior para evitar que el modelo genere contenido similar a materiales de derechos de autor. El enfoque actual de compensación es muy efectivo frente a riesgos generales, pero falla en proteger contra los peores riesgos de derechos de autor, como la inclusión de citas largas. En este contexto, se propone un enfoque de inferencia eficiente y potente. Este método itera entre técnicas de detección de citas y sustitución para transformar secciones potencialmente invadidas. Utilizando filtros de captura de datos eficientes (Bloom filtros), este enfoque permite la escaneo de grandes corpus de la realidad para derechos de autor. Si la longitud de la cita supera un umbral, el sistema puede rechazar la respuesta, reduciendo así el riesgo de invasión. Los resultados de las pruebas muestran que BloomScrub reduce el riesgo de invasión mientras mantiene la utilidad del modelo, y puede adaptarse para rechazar solo aquellas citas que presentan un riesgo significativo. Estos resultados demuestran que un enfoque ligero de inferencia es capaz de demostrar un impacto notable en la prevención de derechos de autor.",
      "upvotes": 7,
      "discussionId": "68119c70e8a3493171fadd11",
      "ai_keywords": [
        "BloomScrub",
        "Bloom filters",
        "quotation detection",
        "rewriting techniques",
        "copyright screening",
        "adaptive abstention"
      ]
    },
    "publishedAt": "2025-04-22T13:16:53.000Z",
    "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
    "summary": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16046.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fb40b59af1d16bc0ac60f4",
      "avatarUrl": "/avatars/03ff66a419db8f2bc8e89a3b47aaaeac.svg",
      "fullname": "Jack Zhang",
      "name": "jackzhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20998",
      "authors": [
        {
          "_id": "6811899ba6198824c5589ed7",
          "name": "Thao Nguyen",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589ed8",
          "name": "Krishna Kumar Singh",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589ed9",
          "name": "Jing Shi",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589eda",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589edb",
          "name": "Yong Jae Lee",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589edc",
          "name": "Yuheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T17:59:57.000Z",
      "submittedOnDailyAt": "2025-04-30T00:54:06.806Z",
      "title": "YoChameleon: Visión personalizada y generación de lenguaje",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Los modelos de grandes redes multimodal (por ejemplo: GPT-4, Gemini, Chameleon) están desarrollandose como herramientas potentes conocidas por cientos de miles de usuarios. Sin embargo, son modelos graduales y carecen de conocimientos personalizados sobre conceptos específicos de usuarios. Los estudios previos han investigado la personalización en la generación de texto, pero no está claro cómo aplicar estas técnicas en nuevas tareas de modelado (por ejemplo: generación de imágenes). En este artículo, se presenta una investigación sobre la personalización de modelos de grandes redes multimodal. Al proporcionar imágenes de 3 a 5 páginas sobre un concepto específico, Yo'Chameleon utiliza la técnica de tuning de soft prompt para recopilar información relacionada con el tema, responder preguntas sobre él y generar imágenes del tema en nuevos contextos. Yo'Chameleon mantiene el rendimiento del modelo equilibrado y funciona efectivamente en diversas tareas de modelado, utilizando una estructura automática de optimización de prompts y un enfoque de generación de imágenes \"soft positive\", además de ser entrenada con pocos ejemplos para mejorar la calidad de las imágenes.",
      "upvotes": 6,
      "discussionId": "6811899ca6198824c5589f45",
      "ai_keywords": [
        "soft-prompt tuning",
        "subject-specific information",
        "self-prompting optimization mechanism",
        "soft-positive image generation approach"
      ]
    },
    "publishedAt": "2025-04-29T13:59:57.000Z",
    "title": "YoChameleon: Personalized Vision and Language Generation",
    "summary": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into\npowerful tools with millions of users. However, they remain generic models and\nlack personalized knowledge of specific user concepts. Previous work has\nexplored personalization for text generation, yet it remains unclear how these\nmethods can be adapted to new modalities, such as image generation. In this\npaper, we introduce Yo'Chameleon, the first attempt to study personalization\nfor large multimodal models. Given 3-5 images of a particular concept,\nYo'Chameleon leverages soft-prompt tuning to embed subject-specific information\nto (i) answer questions about the subject and (ii) recreate pixel-level details\nto produce images of the subject in new contexts. Yo'Chameleon is trained with\n(i) a self-prompting optimization mechanism to balance performance across\nmultiple modalities, and (ii) a ``soft-positive\" image generation approach to\nenhance image quality in a few-shot setting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20998.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6748
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20879",
      "authors": [
        {
          "_id": "6811ae6b7f4f553788e905b8",
          "name": "Shivalika Singh",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905b9",
          "name": "Yiyang Nan",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905ba",
          "name": "Alex Wang",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905bb",
          "name": "Daniel D'Souza",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905bc",
          "name": "Sayash Kapoor",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905bd",
          "name": "Ahmet Üstün",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905be",
          "name": "Sanmi Koyejo",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905bf",
          "user": {
            "_id": "63081e15a670ed10f9d44229",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
            "isPro": true,
            "fullname": "Yuntian Deng",
            "user": "yuntian-deng",
            "type": "user"
          },
          "name": "Yuntian Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T09:56:42.033Z",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c0",
          "name": "Shayne Longpre",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c1",
          "name": "Noah Smith",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c2",
          "name": "Beyza Ermis",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c3",
          "name": "Marzieh Fadaee",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c4",
          "name": "Sara Hooker",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T15:48:49.000Z",
      "submittedOnDailyAt": "2025-04-30T03:36:53.331Z",
      "title": "La Ilusión del Ranking\n\n(Note: La traducción directa de \"The Leaderboard Illusion\" al español es \"La Ilusión del Ranking\". Esta frase metafóricamente refiere a la percepción de ser en una posición de liderazgo o el mito de ser un líder, lo que no tiene una traducción literal directa en español.)",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "La evaluación progresista es la base para el desarrollo de todos los campos científicos. Cuando los benchmarks desempeñan un papel central, pueden verse deformados y vulnerables. Chatbot Arena ha ocupado una posición principal como el principal leaderboard para clasificar los sistemas AI con mayor capacidad. Sin embargo, en esta investigación se han descubierto problemas sistemáticos que son la causa de la deformación. Hemos comprobado que los proveedores reciben procesos de prueba no públicos, lo que permite testear varias versiones antes de la publicación y ajustar puntuaciones según sea necesario. Hemos demostrado que los proveedores tienen la capacidad selectiva de elegir las mejores puntuaciones, lo que genera un sesgo en las puntuaciones de Arena debido a la publicidad. En casos extremos, Meta ha probado 27 versiones no públicas de Llama-4 antes de su lanzamiento. Además, los modelos no públicos tienen más interacciones que los modelos públicos o abiertos, y los modelos abiertos de pesos y código abierto son menos probables de ser eliminados de Arena. Estas políticas generan grandes desigualdades en la acceso a grandes cantidades de datos a medida que el tiempo pasa. Proveedores como Google y OpenAI han obtenido aproximadamente el 19.2% y 20.4% de los datos totales de Arena, mientras que 83 modelos públicos han obtenido aproximadamente el 29.7%. Mostramos que la acceso a los datos de Chatbot Arena confiere grandes ventajas y, basándonos en nuestra estimación conservadora, se ha observado un aumento en la rendimiento relativo del 112%. Estas tendencias indican que Arena se ha adaptado demasiado a sus propios patrones, lo que reduce su efecto en la calidad general de los modelos. Chatbot Arena se ha construido como un plataforma de evaluación valiosa mantenida por organizaciones y comunidades personales. Proporcionamos una reforma del marco de evaluación de Chatbot Arena y un plan concreto e implementable para promover un benchmark más justo y transparente.",
      "upvotes": 6,
      "discussionId": "6811ae6c7f4f553788e905fc"
    },
    "publishedAt": "2025-04-29T11:48:49.000Z",
    "title": "The Leaderboard Illusion",
    "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 77
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20630",
      "authors": [
        {
          "_id": "6811ea47f8ca0d9acb45374b",
          "user": {
            "_id": "66569729ea21cfae5f5797c4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66569729ea21cfae5f5797c4/IguwJzljFN3QiEd1bn5BP.jpeg",
            "isPro": false,
            "fullname": "Yu Zhang",
            "user": "AaronZ345",
            "type": "user"
          },
          "name": "Yu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T09:56:39.194Z",
          "hidden": false
        },
        {
          "_id": "6811ea47f8ca0d9acb45374c",
          "name": "Wenxiang Guo",
          "hidden": false
        },
        {
          "_id": "6811ea47f8ca0d9acb45374d",
          "name": "Changhao Pan",
          "hidden": false
        },
        {
          "_id": "6811ea47f8ca0d9acb45374e",
          "name": "Zhiyuan Zhu",
          "hidden": false
        },
        {
          "_id": "6811ea47f8ca0d9acb45374f",
          "name": "Tao Jin",
          "hidden": false
        },
        {
          "_id": "6811ea47f8ca0d9acb453750",
          "name": "Zhou Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T10:56:44.000Z",
      "submittedOnDailyAt": "2025-04-30T07:54:10.120Z",
      "title": "ISDrama: La generación de espectral drama a través de un enfoque de programación multimodal",
      "submittedOnDailyBy": {
        "_id": "66569729ea21cfae5f5797c4",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66569729ea21cfae5f5797c4/IguwJzljFN3QiEd1bn5BP.jpeg",
        "isPro": false,
        "fullname": "Yu Zhang",
        "user": "AaronZ345",
        "type": "user"
      },
      "summary": "Varios sonidos repetitivos.",
      "upvotes": 3,
      "discussionId": "6811ea48f8ca0d9acb4537cf",
      "projectPage": "https://aaronz345.github.io/ISDramaDemo/",
      "ai_keywords": [
        "multimodal inputs",
        "binaural speech",
        "dramatic prosody",
        "multimodal prompts",
        "multimodal recorded dataset",
        "contrastive learning",
        "Doppler effect",
        "Multimodal Pose Encoder",
        "flow-based model",
        "mamba-transformer",
        "Drama-MOE",
        "classifier-free guidance",
        "context-consistent guidance"
      ]
    },
    "publishedAt": "2025-04-29T06:56:44.000Z",
    "title": "ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting",
    "summary": "Multimodal immersive spatial drama generation focuses on creating continuous\nmulti-speaker binaural speech with dramatic prosody based on multimodal\nprompts, with potential applications in AR, VR, and others. This task requires\nsimultaneous modeling of spatial information and dramatic prosody based on\nmultimodal inputs, with high data collection costs. To the best of our\nknowledge, our work is the first attempt to address these challenges. We\nconstruct MRSDrama, the first multimodal recorded spatial drama dataset,\ncontaining binaural drama audios, scripts, videos, geometric poses, and textual\nprompts. Then, we propose ISDrama, the first immersive spatial drama generation\nmodel through multimodal prompting. ISDrama comprises these primary components:\n1) Multimodal Pose Encoder, based on contrastive learning, considering the\nDoppler effect caused by moving speakers to extract unified pose information\nfrom multimodal prompts. 2) Immersive Drama Transformer, a flow-based\nmamba-transformer model that generates high-quality drama, incorporating\nDrama-MOE to select proper experts for enhanced prosody and pose control. We\nalso design a context-consistent classifier-free guidance strategy to\ncoherently generate complete drama. Experimental results show that ISDrama\noutperforms baseline models on objective and subjective metrics. The demos and\ndataset are available at https://aaronz345.github.io/ISDramaDemo.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20630.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66569729ea21cfae5f5797c4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66569729ea21cfae5f5797c4/IguwJzljFN3QiEd1bn5BP.jpeg",
      "fullname": "Yu Zhang",
      "name": "AaronZ345",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20996",
      "authors": [
        {
          "_id": "6811c55384adfa26b82abd76",
          "name": "Sicheng Mo",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd77",
          "name": "Thao Nguyen",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd78",
          "name": "Xun Huang",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd79",
          "name": "Siddharth Srinivasan Iyer",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7a",
          "name": "Yijun Li",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7b",
          "name": "Yuchen Liu",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7c",
          "name": "Abhishek Tandon",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7d",
          "name": "Eli Shechtman",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7e",
          "name": "Krishna Kumar Singh",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7f",
          "name": "Yong Jae Lee",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd80",
          "name": "Bolei Zhou",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd81",
          "name": "Yuheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T17:59:45.000Z",
      "submittedOnDailyAt": "2025-04-30T05:08:54.031Z",
      "title": "X-Fusion: Introducimos un nuevo modelo de teoría en un grande modelo de lenguaje fijado en hielo.",
      "submittedOnDailyBy": {
        "_id": "637c94d3f219c71f93eda9ad",
        "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
        "isPro": false,
        "fullname": "Sicheng Mo",
        "user": "Sichengmo",
        "type": "user"
      },
      "summary": "X-Fusion es un marco de modelo de aprendizaje profundo diseñado para modelos de lenguaje expansados (LLMs) que pueden utilizarse en tareas que involucran múltiples modelos, manteniendo su capacidad lingüística. Utiliza un diseño de dos torres asociados a los tipos de modelo, integrando información específica de imágenes necesaria para entender y generar, mientras que los parámetros del LLM se mantienen fijos. Según los experimentos, X-Fusion supera a arquitecturas equivalentes en dos tareas: entendimiento de imágenes y generación de texto a partir de imágenes. La inserción de datos enfocados en entendimiento mejora la calidad de la generación y la reducción de ruido en los datos de imágenes mejora el rendimiento general, mientras que la matriz de características acelera la convergencia de pequeños modelos, con un impacto mínimo en grandes modelos. Estos hallazgos proporcionan una valiosa retroalimentación para la construcción de modelos multimodelos integrados eficientes.",
      "upvotes": 2,
      "discussionId": "6811c55584adfa26b82abdfe",
      "projectPage": "https://sichengmo.github.io/XFusion/",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "multimodal tasks",
        "dual-tower design",
        "modality-specific weights",
        "vision-specific information",
        "image-to-text",
        "text-to-image",
        "understanding-focused data",
        "feature alignment",
        "unified multimodal models"
      ]
    },
    "publishedAt": "2025-04-29T13:59:45.000Z",
    "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
    "summary": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c94d3f219c71f93eda9ad",
      "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
      "fullname": "Sicheng Mo",
      "name": "Sichengmo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.18087",
      "authors": [
        {
          "_id": "6810746e4be021d4dcd8d4de",
          "name": "Weipeng Tan",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4df",
          "name": "Chuming Lin",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4e0",
          "user": {
            "_id": "652fab9d04a34a9282bf29d6",
            "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
            "isPro": false,
            "fullname": "Chengming Xu",
            "user": "ChengmingX",
            "type": "user"
          },
          "name": "Chengming Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T09:56:46.843Z",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4e1",
          "name": "FeiFan Xu",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4e2",
          "name": "Xiaobin Hu",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4e3",
          "name": "Xiaozhong Ji",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4e4",
          "name": "Junwei Zhu",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4e5",
          "name": "Chengjie Wang",
          "hidden": false
        },
        {
          "_id": "6810746e4be021d4dcd8d4e6",
          "name": "Yanwei Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-25T05:28:21.000Z",
      "submittedOnDailyAt": "2025-04-30T07:13:30.231Z",
      "title": "Identity Separación, Cooperación Emocional: Generación de Personajes Dialógicos Emocionales para la Recognición de Correlaciones Emocionales",
      "submittedOnDailyBy": {
        "_id": "652fab9d04a34a9282bf29d6",
        "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
        "isPro": false,
        "fullname": "Chengming Xu",
        "user": "ChengmingX",
        "type": "user"
      },
      "summary": "El reciente desarrollo de la Generación de Cabezas Hablantes (THG) ha permitido alcanzar movimientos de boca impresionantes y una calidad visual alta a través de modelos de difusión, aunque los métodos existentes no han logrado crear personajes con expresiones emocionales ni mantener la identidad del orador. En la generación de THG con emociones actual, se han identificado tres límites importantes: la falta de uso adecuado de códigos emocionales únicos del sonido, la pérdida de la identidad de la expresión emocional y el aprendizaje independiente de las relaciones emocionales. Para enfrentar estos desafíos, proponemos un nuevo marco que separa la identidad y la emoción, y luego que colabore emociones con características similares. Este marco se llama DICE-Talk. Primero, desarrollamos un emocional de separación para modelar el código emocional del video de sonido, permitiendo expresar emociones sin relación a la identidad. Luego, introducimos un módulo de condición emocional fortalecido que maneja bancos de emociones aprendibles, utilizando vectores cuadratónicos y atención para clarificar las relaciones entre emociones. Finalmente, diseñamos un objetivo de discriminación emocional para imponer la consistencia emocional durante el proceso de difusión. Los experimentos de difusión en los conjuntos de datos MEAD y HDTF superan el acercamiento más avanzado en precisión emocional, manteniendo el rendimiento del movimiento de boca, y confirman la excelente performance de nuestro método. Los resultados cualitativos y la experiencia del usuario generan expresiones emocionales ricas y relacionadas con una identidad naturalmente adaptada, confirmando la capacidad de nuestro método.",
      "upvotes": 2,
      "discussionId": "681074704be021d4dcd8d57c",
      "projectPage": "https://toto222.github.io/DICE-Talk/",
      "githubRepo": "https://github.com/toto222/DICE-Talk"
    },
    "publishedAt": "2025-04-25T01:28:21.000Z",
    "title": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional\n  Talking Portrait Generation",
    "summary": "Recent advances in Talking Head Generation (THG) have achieved impressive lip\nsynchronization and visual quality through diffusion models; yet existing\nmethods struggle to generate emotionally expressive portraits while preserving\nspeaker identity. We identify three critical limitations in current emotional\ntalking head generation: insufficient utilization of audio's inherent emotional\ncues, identity leakage in emotion representations, and isolated learning of\nemotion correlations. To address these challenges, we propose a novel framework\ndubbed as DICE-Talk, following the idea of disentangling identity with emotion,\nand then cooperating emotions with similar characteristics. First, we develop a\ndisentangled emotion embedder that jointly models audio-visual emotional cues\nthrough cross-modal attention, representing emotions as identity-agnostic\nGaussian distributions. Second, we introduce a correlation-enhanced emotion\nconditioning module with learnable Emotion Banks that explicitly capture\ninter-emotion relationships through vector quantization and attention-based\nfeature aggregation. Third, we design an emotion discrimination objective that\nenforces affective consistency during the diffusion process through\nlatent-space classification. Extensive experiments on MEAD and HDTF datasets\ndemonstrate our method's superiority, outperforming state-of-the-art approaches\nin emotion accuracy while maintaining competitive lip-sync performance.\nQualitative results and user studies further confirm our method's ability to\ngenerate identity-preserving portraits with rich, correlated emotional\nexpressions that naturally adapt to unseen identities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18087.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652fab9d04a34a9282bf29d6",
      "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
      "fullname": "Chengming Xu",
      "name": "ChengmingX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]