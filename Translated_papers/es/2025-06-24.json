[
  {
    "paper": {
      "id": "2506.18882",
      "authors": [
        {
          "_id": "685a163c0e4ad7e219758569",
          "name": "Hong Li",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856a",
          "name": "Houyuan Chen",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856b",
          "name": "Chongjie Ye",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856c",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856d",
          "name": "Bohan Li",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856e",
          "name": "Shaocong Xu",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e21975856f",
          "name": "Xianda Guo",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758570",
          "name": "Xuhui Liu",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758571",
          "name": "Yikai Wang",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758572",
          "name": "Baochang Zhang",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758573",
          "name": "Satoshi Ikehata",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758574",
          "name": "Boxin Shi",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758575",
          "name": "Anyi Rao",
          "hidden": false
        },
        {
          "_id": "685a163c0e4ad7e219758576",
          "name": "Hao Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643a1f5b58cb07c2a3745116/4JHMhL80xxrkPBOIt9-Cg.mp4"
      ],
      "publishedAt": "2025-06-23T17:53:11.000Z",
      "submittedOnDailyAt": "2025-06-24T02:59:45.489Z",
      "title": "Normal illumination under light: general lighting for studio photography to represent consistent features.",
      "submittedOnDailyBy": {
        "_id": "643a1f5b58cb07c2a3745116",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a1f5b58cb07c2a3745116/OiSDfgfcCUWu0X4-FiNm0.jpeg",
        "isPro": false,
        "fullname": "Hugo",
        "user": "chongjie",
        "type": "user"
      },
      "summary": "La técnica general de tomar fotografías (PS) tiene como objetivo recuperar las altas calidades de las normales de superficie (normal vector) de objetos bajo cualquier condición de iluminación. Sin embargo, a pesar de los avances recientes, como SDM-UniPS y Uni MS-PS, se mantienen dos problemas básicos. 1) La profunda correlación entre la iluminación cambiante y las características de las normales de superficie, lo que dificulta la interpretación de las incertidumbres en la intensidad observada en función de cambios en la iluminación o la dirección de la superficie. 2) La conservación de la geometría de alta frecuencia en superficies complejas, donde estas geometrías complejas pueden causar cambios en las imágenes propias, reflexión mutua y cambios en las normales de superficie, lo que dificulta su reconocimiento con precisión por operaciones tradicionales de procesamiento de características.",
      "upvotes": 62,
      "discussionId": "685a163c0e4ad7e219758577",
      "githubRepo": "https://github.com/houyuanchen111/LINO_UniPS",
      "ai_summary": "Photometric stereo aims to recover high-quality surface normals under arbitrary lighting conditions, addressing challenges related to illumination-surface normal coupling and high-frequency geometric detail preservation.",
      "ai_keywords": [
        "photometric stereo",
        "deep coupling",
        "surface normals",
        "illumination conditions",
        "intensity variations",
        "self-shadowing",
        "inter-reflections",
        "subtle normal variations"
      ]
    },
    "publishedAt": "2025-06-23T13:53:11.000Z",
    "title": "Light of Normals: Unified Feature Representation for Universal\n  Photometric Stereo",
    "summary": "Universal photometric stereo (PS) aims to recover high-quality surface\nnormals from objects under arbitrary lighting conditions without relying on\nspecific illumination models. Despite recent advances such as SDM-UniPS and Uni\nMS-PS, two fundamental challenges persist: 1) the deep coupling between varying\nillumination and surface normal features, where ambiguity in observed intensity\nmakes it difficult to determine whether brightness variations stem from\nlighting changes or surface orientation; and 2) the preservation of\nhigh-frequency geometric details in complex surfaces, where intricate\ngeometries create self-shadowing, inter-reflections, and subtle normal\nvariations that conventional feature processing operations struggle to capture\naccurately.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643a1f5b58cb07c2a3745116/4JHMhL80xxrkPBOIt9-Cg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18882.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643a1f5b58cb07c2a3745116",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a1f5b58cb07c2a3745116/OiSDfgfcCUWu0X4-FiNm0.jpeg",
      "fullname": "Hugo",
      "name": "chongjie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18871",
      "authors": [
        {
          "_id": "685a0be90e4ad7e2197584f4",
          "user": {
            "_id": "65f19fa7f591e4538b65dea5",
            "avatarUrl": "/avatars/a38e65701e1d2eb3eb93335d6d0b937c.svg",
            "isPro": false,
            "fullname": "Chenyuan Wu",
            "user": "wcyno23",
            "type": "user"
          },
          "name": "Chenyuan Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:41:49.563Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f5",
          "name": "Pengfei Zheng",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f6",
          "user": {
            "_id": "661ac5b53d7248a6f20080c1",
            "avatarUrl": "/avatars/26aef5944759c2e4366a71eb8c7fc50a.svg",
            "isPro": false,
            "fullname": "Ruiran Yan",
            "user": "Ruiran",
            "type": "user"
          },
          "name": "Ruiran Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:41:58.820Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f7",
          "user": {
            "_id": "62612679bbcbd1c34f1638af",
            "avatarUrl": "/avatars/c0675d05a52192ee14e9ab1633353956.svg",
            "isPro": false,
            "fullname": "Xiao",
            "user": "Shitao",
            "type": "user"
          },
          "name": "Shitao Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:42:07.997Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f8",
          "user": {
            "_id": "641bd1737c21ab946bf69aff",
            "avatarUrl": "/avatars/83759075ad893a69a0c2cf5493d7e988.svg",
            "isPro": false,
            "fullname": "xin luo",
            "user": "sienna223",
            "type": "user"
          },
          "name": "Xin Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:42.720Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584f9",
          "user": {
            "_id": "6458b59c7a7e192202df8fa0",
            "avatarUrl": "/avatars/33ee716477e5686da8723d01e199cd27.svg",
            "isPro": false,
            "fullname": "Yueze Wang",
            "user": "yzwang",
            "type": "user"
          },
          "name": "Yueze Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T09:33:03.979Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fa",
          "user": {
            "_id": "675bcb9ce16de4a95aac9950",
            "avatarUrl": "/avatars/a1d0a2fd96ddee9cdea4f97819233fe5.svg",
            "isPro": false,
            "fullname": "Wanli Li",
            "user": "liwanli",
            "type": "user"
          },
          "name": "Wanli Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:42:15.514Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fb",
          "user": {
            "_id": "674972973dc92067bd606877",
            "avatarUrl": "/avatars/8366448b45baf7d7f3d3d2b8793479ed.svg",
            "isPro": false,
            "fullname": "Jiang Xiyan",
            "user": "Emilia515",
            "type": "user"
          },
          "name": "Xiyan Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:42:29.622Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fc",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fd",
          "user": {
            "_id": "6564a2ceedae9c33b7654a1f",
            "avatarUrl": "/avatars/42f09356a1282896573ccb44830cd327.svg",
            "isPro": false,
            "fullname": "JUNJIE ZHOU",
            "user": "JUNJIE99",
            "type": "user"
          },
          "name": "Junjie Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:40.699Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584fe",
          "user": {
            "_id": "66164f6245336ca774679611",
            "avatarUrl": "/avatars/9baf0ab475bc8d5997abda9ffe8cfa28.svg",
            "isPro": false,
            "fullname": "Ze Liu",
            "user": "marsh123",
            "type": "user"
          },
          "name": "Ze Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:38.616Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e2197584ff",
          "user": {
            "_id": "6540617c7cadb2d1b42007c5",
            "avatarUrl": "/avatars/b1877fd0564c362a0d4a064d4ec43a73.svg",
            "isPro": false,
            "fullname": "Ziyi Xia",
            "user": "ZiyiXia",
            "type": "user"
          },
          "name": "Ziyi Xia",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:42:47.445Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758500",
          "name": "Chaofan Li",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758501",
          "name": "Haoge Deng",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758502",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758503",
          "name": "Kun Luo",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758504",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758505",
          "user": {
            "_id": "66ed026076a8038cb4ae6053",
            "avatarUrl": "/avatars/99b6527da6b66c6b5df3fc8261587322.svg",
            "isPro": false,
            "fullname": "Defu Lian",
            "user": "dove-ustc",
            "type": "user"
          },
          "name": "Defu Lian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:43:23.976Z",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758506",
          "name": "Xinlong Wang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758507",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758508",
          "name": "Tiejun Huang",
          "hidden": false
        },
        {
          "_id": "685a0be90e4ad7e219758509",
          "name": "Zheng Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:38:54.000Z",
      "submittedOnDailyAt": "2025-06-24T01:06:04.763Z",
      "title": "OmniGen2: Exploración de Generación Monoral Avanzada",
      "submittedOnDailyBy": {
        "_id": "6564a2ceedae9c33b7654a1f",
        "avatarUrl": "/avatars/42f09356a1282896573ccb44830cd327.svg",
        "isPro": false,
        "fullname": "JUNJIE ZHOU",
        "user": "JUNJIE99",
        "type": "user"
      },
      "summary": "En este estudio se presenta OmniGen2, un modelo generativo amplio y abierto-código. Este modelo tiene como objetivo proporcionar soluciones para diferentes tareas generativas, como generar imágenes a partir de texto, editar imágenes, o generar texto dentro de un marco de texto, en un período de un mes. A diferencia de OmniGen v1, OmniGen2 utiliza dos pasos seguros diferentes para el modelo de texto y imágenes, así como parámetros compartidos y un conjunto de tokens de imagen combinado. Esta arquitectura permite que OmniGen2 se basique en modelos de entendimiento multimodal existentes, reduciendo la necesidad de reemplazar los entradas de VAE y manteniendo su capacidad original de generación de texto. Para apoyar el entrenamiento de OmniGen2, se desarrolló una pipeline de construcción de datos detallada que incluye datos de edición de imágenes y generación de texto dentro de un marco de texto. Además, se introdujo una estructura de reflejo adecuada para tareas de generación de imágenes y se construyó un conjunto de datos de reflejo específico basado en OmniGen2. OmniGen2 muestra resultados competitivos en varios marcos de evaluación, a pesar de tener un tamaño de parámetros relativamente pequeño en tareas de generación de texto a imágenes y edición de imágenes. Para evaluar la generación de texto dentro de un marco de texto, se introdujo un nuevo marco de evaluación llamado OmniContext. OmniGen2 es el modelo de código abierto con la mejor rendimiento. Para apoyar futuras investigaciones, se publican el modelo, el código de entrenamiento, los conjuntos de datos y la pipeline de construcción de datos. Página del proyecto: https://vectorspacelab.github.io/OmniGen2; Link de GitHub: https://github.com/VectorSpaceLab/OmniGen2",
      "upvotes": 33,
      "discussionId": "685a0be90e4ad7e21975850a",
      "projectPage": "https://vectorspacelab.github.io/OmniGen2/",
      "githubRepo": "https://github.com/VectorSpaceLab/OmniGen2",
      "ai_summary": "OmniGen2, a versatile generative model, introduces dual decoding pathways for text and images, preserves original text generation, and achieves competitive results with a new subject-driven benchmark.",
      "ai_keywords": [
        "decoding pathways",
        "unshared parameters",
        "decoupled image tokenizer",
        "multimodal understanding models",
        "reflection mechanism",
        "reflection dataset",
        "OmniContext",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-06-23T13:38:54.000Z",
    "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
    "summary": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18871.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6564a2ceedae9c33b7654a1f",
      "avatarUrl": "/avatars/42f09356a1282896573ccb44830cd327.svg",
      "fullname": "JUNJIE ZHOU",
      "name": "JUNJIE99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18841",
      "authors": [
        {
          "_id": "685a0f330e4ad7e219758514",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758515",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758516",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:35.954Z",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758517",
          "name": "Roy Ka-Wei Lee",
          "hidden": false
        },
        {
          "_id": "685a0f330e4ad7e219758518",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T16:59:02.000Z",
      "submittedOnDailyAt": "2025-06-24T01:08:07.123Z",
      "title": "LongWriter-Zero: Se logra la optimización de la generación de largas oraciones mediante el método de entrenamiento de reinfocación.",
      "submittedOnDailyBy": {
        "_id": "63369da91ba5d5ece24118a4",
        "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
        "isPro": false,
        "fullname": "wuyuhao",
        "user": "mozhu",
        "type": "user"
      },
      "summary": "Superlongitudinalidad en grandes modelos de lenguaje (LLMs) es uno de los escenarios que requieren una amplia demanda, pero se encuentra limitada por la longitud máxima de generación y el descenso de calidad cuando la longitud aumenta, lo que convierte este problema en una cuestión de importancia real. Un ejemplo de una forma de abordar este problema es LongWriter, que basa su enfoque en la \"regla\". Esto implica entrenar un ajuste de regulación de sentencias largas (SFT) para la generación de largas oraciones, pero esta estrategia depende de datos de SFT de sintetización, lo que puede ser costoso y complejo en su construcción, y también puede resultar artificial y estructurado en exceso, lo que reduce la colaboración y la coherencia. En este estudio, proponemos un enfoque desde cero, sin depender de cualquier nota o datos de sintetización, utilizando una aproximación de recompensas. Este enfoque tiene como objetivo fomentar la capacidad de generación de alta calidad y larga longitud de texto mediante modelos de LLMs utilizando aprendizaje por refuerzo (RL). El aprendizaje RL comienza con un modelo básico y guia el modelo durante el proceso de escritura para promover planificación y mejora. Para lograr esto, se utiliza un modelo de recompensas especial, y se ajusta continuamente el modelo de LLM para controlar la longitud del texto, la calidad de las oraciones y la estructura de la composición. Según los resultados experimentales, nuestro modelo LongWriter-Zero (entrenado en Qwen2.5-32B) obtiene resultados consistentemente mejores que los métodos tradicionales de SFT. Al respecto, LongWriter-Zero realiza los mejores resultados en todos los métricas de WritingBench y Arena-Write, y supera modelos como DeepSeek R1 y Qwen3-235B, que tienen más de 100B de parámetros. Los datos y los puntos de control del modelo se publican en https://huggingface.co/THU-KEG/LongWriter-Zero-32B.",
      "upvotes": 30,
      "discussionId": "685a0f340e4ad7e219758519",
      "ai_summary": "An incentivization-based reinforcement learning approach is used to develop a large language model capable of generating ultra-long, high-quality text without the need for synthetic data or supervised fine-tuning.",
      "ai_keywords": [
        "reinforcement learning",
        "reward models",
        "long-form text generation",
        "ultra-long generation",
        "large language models",
        "synthetic fine-tuning",
        "length control",
        "writing quality",
        "structural formatting",
        "WritingBench",
        "Arena-Write"
      ]
    },
    "publishedAt": "2025-06-23T12:59:02.000Z",
    "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
    "summary": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18841.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63369da91ba5d5ece24118a4",
      "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
      "fullname": "wuyuhao",
      "name": "mozhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18851",
      "authors": [
        {
          "_id": "685a0fb40e4ad7e219758528",
          "user": {
            "_id": "6304e2dabad6ce7fc0287d57",
            "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
            "isPro": false,
            "fullname": "Zhuowei_Chen",
            "user": "ZhuoweiChen",
            "type": "user"
          },
          "name": "Zhuowei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:44:49.590Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758529",
          "user": {
            "_id": "63b415037af2e415f2599c18",
            "avatarUrl": "/avatars/4afbe7d6d05a702f1beeed9c53e78153.svg",
            "isPro": false,
            "fullname": "Bingchuan Li",
            "user": "lbc402",
            "type": "user"
          },
          "name": "Bingchuan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:44:57.735Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852a",
          "user": {
            "_id": "6804ce31d205d72ddbeec8a0",
            "avatarUrl": "/avatars/772d20a653649063158cba166298801a.svg",
            "isPro": false,
            "fullname": "Tianxiang Ma",
            "user": "TianxiangMa",
            "type": "user"
          },
          "name": "Tianxiang Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:45:10.156Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852b",
          "name": "Lijie Liu",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852c",
          "user": {
            "_id": "619b404bab4c7b7f16a7d57d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619b404bab4c7b7f16a7d57d/coT_UGRfBOUAeSxjyhdlG.jpeg",
            "isPro": false,
            "fullname": "Mingcong Liu",
            "user": "onion-liu",
            "type": "user"
          },
          "name": "Mingcong Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:23.740Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852d",
          "name": "Yi Zhang",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852e",
          "name": "Gen Li",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e21975852f",
          "user": {
            "_id": "6752cd83ffaeeb979db974ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
            "isPro": false,
            "fullname": "Xinghui Li",
            "user": "Crayon-Shinchan",
            "type": "user"
          },
          "name": "Xinghui Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:45:40.054Z",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758530",
          "name": "Siyu Zhou",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758531",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758532",
          "user": {
            "_id": "67bc6b515d9470ec64bdcc33",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dttL8Fb3bKCVG5zjg_02q.png",
            "isPro": false,
            "fullname": "Xinglong Wu",
            "user": "Xingzhe-xlwu",
            "type": "user"
          },
          "name": "Xinglong Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-24T09:45:50.807Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:11:56.000Z",
      "submittedOnDailyAt": "2025-06-24T01:12:12.451Z",
      "title": "Fantom Data: Tema General de Generación de Video para la Consistencia de Entidades\nDataset",
      "submittedOnDailyBy": {
        "_id": "6304e2dabad6ce7fc0287d57",
        "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
        "isPro": false,
        "fullname": "Zhuowei_Chen",
        "user": "ZhuoweiChen",
        "type": "user"
      },
      "summary": "La generación de vídeos temáticos ha experimentado un gran avance recientemente. Sin embargo, los modelos actuales encuentran un desafío en seguir adicionalmente a las instrucciones de texto contextual. Este límite, conocido como problema de copia y pega, es causado por el uso amplio de un par de enfoques de entrenamiento. Este enfoque intenta conectar la reconocción del tema con su contexto y atributos mediante la sampling de imágenes de referencia en el mismo escenario que el vídeo objetivo. Para resolver este problema, presentamos Phantom-Data. Phantom-Data es un conjunto de datos destinado a garantizar la consistencia del tema en los vídeos generales. Este conjunto de datos incluye aproximadamente 1 millón de pares de identificadores que se distribuyen en diferentes categorías. Nuestro conjunto de datos se ha construido a través de una pipeline de tres etapas: (1) un módulo de exploración temática para entradas generales, (2) una búsqueda de temas en una gran escala de vídeos y imágenes, con más de 53 millones de vídeos y 3.000 millones de imágenes, y (3) una confirmación de identificación a través de un líder de la fila que asegura la consistencia visual con los cambios de contexto. Los experimentos detallados muestran que, mientras mantiene un nivel similar de consistencia de identificación comparado con las bases de pares dentro del conjunto de datos, el Phantom-Data mejora significativamente el rendimiento de ProNuPAMI y la calidad visual.",
      "upvotes": 21,
      "discussionId": "685a0fb40e4ad7e219758535",
      "projectPage": "https://phantom-video.github.io/Phantom-Data/",
      "githubRepo": "https://github.com/Phantom-video/Phantom-Data",
      "ai_summary": "A cross-pair dataset called Phantom-Data improves subject-to-video generation by enhancing prompt alignment and visual quality while maintaining identity consistency.",
      "ai_keywords": [
        "Phantom-Data",
        "subject-to-video generation",
        "copy-paste problem",
        "in-pair training paradigm",
        "subject detection",
        "cross-context subject retrieval",
        "prior-guided identity verification"
      ]
    },
    "publishedAt": "2025-06-23T13:11:56.000Z",
    "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation\n  Dataset",
    "summary": "Subject-to-video generation has witnessed substantial progress in recent\nyears. However, existing models still face significant challenges in faithfully\nfollowing textual instructions. This limitation, commonly known as the\ncopy-paste problem, arises from the widely used in-pair training paradigm. This\napproach inherently entangles subject identity with background and contextual\nattributes by sampling reference images from the same scene as the target\nvideo. To address this issue, we introduce Phantom-Data, the first\ngeneral-purpose cross-pair subject-to-video consistency dataset, containing\napproximately one million identity-consistent pairs across diverse categories.\nOur dataset is constructed via a three-stage pipeline: (1) a general and\ninput-aligned subject detection module, (2) large-scale cross-context subject\nretrieval from more than 53 million videos and 3 billion images, and (3)\nprior-guided identity verification to ensure visual consistency under\ncontextual variation. Comprehensive experiments show that training with\nPhantom-Data significantly improves prompt alignment and visual quality while\npreserving identity consistency on par with in-pair baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18851.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6304e2dabad6ce7fc0287d57",
      "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
      "fullname": "Zhuowei_Chen",
      "name": "ZhuoweiChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18896",
      "authors": [
        {
          "_id": "685a02790e4ad7e2197584b2",
          "name": "Jiaru Zou",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b3",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b4",
          "name": "Jingwen Gu",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b5",
          "name": "Jiahao Qiu",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b6",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b7",
          "name": "Jingrui He",
          "hidden": false
        },
        {
          "_id": "685a02790e4ad7e2197584b8",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:59:02.000Z",
      "submittedOnDailyAt": "2025-06-24T01:03:32.146Z",
      "title": "ReasonFlux-PRM: En los PRMs interesados en la trayectoria, una sucesión de pensamientos largos y continuos de inferencia.",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Los Modelos de Recompensa de Procesos (PRMs) han evolucionado desde un potente marco de trabajo para revisar la razón intermedia de los grandes modelos de lenguaje (LLMs). Los PRMs de la semana pasada se habían entrenado principalmente con datos sobre el output final del modelo, lo que dificultaba la evaluación de los caminos intermedios, especialmente en el nuevo contexto de los modelos de razón como Deepseek-R1. En este estudio, se presenta ReasonFlux-PRM. ReasonFlux-PRM es un nuevo PRM que se centra en los caminos y está diseñado para evaluar las trazas de razón de camino-output. ReasonFlux-PRM utiliza sub-objetivos a nivel de etapa y de camino, permitiendo una distribución de recompensas más precisa para datos de razonamiento estructurado. ReasonFlux-PRM soporta la sub-objetivo de recompensa en dos configuraciones: offline y online. Específicamente, (i) selección de datos para garantizar modelos de alta calidad en entrenamiento, (ii) proporcionación de recompensas a nivel de proceso que requieren una alta densidad para la optimización de la política en aprendizaje por refuerzo, y (iii) escalabilidad de tiempo de prueba guiada por la recompensa. A través de experimentos en difíciles benchmarks de entrenamiento descendente como AIME, MATH500 y GPQA-Diamond, ReasonFlux-PRM-7B demostró una selección de datos de alta calidad, mejorando los PRMs fuertes (como Qwen2.5-Math-PRM-72B) o líneas de modelos basadas en humanos, con un rendimiento promedio del 12.1% en entrenamiento, 4.5% en aprendizaje por refuerzo y 6.3% en escalabilidad de tiempo de prueba. Además, se lanza una versión eficiente de ReasonFlux-PRM-1.5B adecuada para aplicaciones con limitaciones de recursos o deployment en bordes. Proyecto: https://github.com/Gen-Verse/ReasonFlux",
      "upvotes": 20,
      "discussionId": "685a027a0e4ad7e2197584b9",
      "projectPage": "https://huggingface.co/collections/Gen-Verse/reasonflux-prm-68463c73cf1c6a0ec6fafeb5",
      "githubRepo": "https://github.com/Gen-Verse/ReasonFlux",
      "ai_summary": "ReasonFlux-PRM, a novel trajectory-aware Process Reward Model, evaluates reasoning traces with step-level and trajectory-level supervision, enhancing performance in model distillation, reinforcement learning, and test-time scaling.",
      "ai_keywords": [
        "Process Reward Models",
        "trajectory-aware PRM",
        "trajectory-response outputs",
        "step-level supervision",
        "trajectory-level supervision",
        "chain-of-thought data",
        "model distillation",
        "policy optimization",
        "reinforcement learning",
        "Best-of-N test-time scaling",
        "AIME",
        "MATH500",
        "GPQA-Diamond"
      ]
    },
    "publishedAt": "2025-06-23T13:59:02.000Z",
    "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
    "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18896.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18898",
      "authors": [
        {
          "_id": "685a07510e4ad7e2197584c6",
          "user": {
            "_id": "62318c0386753f5f41d0e261",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
            "isPro": false,
            "fullname": "Jiaming Han",
            "user": "csuhan",
            "type": "user"
          },
          "name": "Jiaming Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:47.088Z",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584c7",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584c8",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584c9",
          "user": {
            "_id": "6365a174ad2c9e8b6731dd0f",
            "avatarUrl": "/avatars/2f4dd0eda92bca5a7464129fe7d961f9.svg",
            "isPro": false,
            "fullname": "Hanyu Wang",
            "user": "hywang66",
            "type": "user"
          },
          "name": "Hanyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:44.987Z",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584ca",
          "name": "Qi Zhao",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584cb",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584cc",
          "name": "Hao He",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584cd",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "685a07510e4ad7e2197584ce",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:59:14.000Z",
      "submittedOnDailyAt": "2025-06-24T01:02:20.046Z",
      "title": "Visión en lengua local: representación correspondiente a texto que integra la percepción y la generación visual.",
      "submittedOnDailyBy": {
        "_id": "62318c0386753f5f41d0e261",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
        "isPro": false,
        "fullname": "Jiaming Han",
        "user": "csuhan",
        "type": "user"
      },
      "summary": "En este artículo se proponen diferentes marcos que integran la comprensión e generación de imágenes utilizando expresiones discretas de significado común. Uno de los puntos clave es el TokenNerf (TA-Tok), que utiliza cajas de código de texto correspondiente, impulsada por la distribución de grandes modelos de lenguaje (LLM), para convertir imágenes en tokens discretos. Este marco integra visión y texto en una distribución amplia, permitiendo que los modelos de cualquier tipo utilicen una interfaz común para entradas y salidas cruzadas. Además, se propone un encoding y decoding adaptativos a la escala para mantener un equilibrio entre eficacia y detalles visuales, y se implementa un detector generativo para producir salidas visuales de alta calidad. Para satisfacer las diferentes necesidades de decodificación, se utilizan dos detectores interpolativos: un modelo automático de regresión rápido y un modelo basado en branches. Para mejorar la fusión del modelo, se revisan tareas de pre-entrenamiento avanzadas y se muestran mejoras tanto en la comprensión visual como en la generación. Los experimentos en los benchmarks superan los métodos actuales de diferentes LLM, alcanzando una convergencia más rápida y mayor eficacia de entrenamiento. El código, el modelo y los datos están disponibles en la siguiente URL: https://tar.csuhan.com",
      "upvotes": 18,
      "discussionId": "685a07520e4ad7e2197584cf",
      "projectPage": "https://tar.csuhan.com",
      "githubRepo": "https://github.com/csuhan/Tar",
      "ai_summary": "A multimodal framework uses a Text-Aligned Tokenizer (TA-Tok) to integrate vision and text into a unified space, employing a generative de-tokenizer with autoregressive and diffusion-based models for efficient and high-fidelity visual outputs.",
      "ai_keywords": [
        "Text-Aligned Tokenizer (TA-Tok)",
        "multimodal LLM",
        "Tar",
        "scale-adaptive encoding",
        "diffusion-based model",
        "autoregressive model",
        "modality fusion",
        "pre-training tasks"
      ]
    },
    "publishedAt": "2025-06-23T13:59:14.000Z",
    "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
    "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18898.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62318c0386753f5f41d0e261",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
      "fullname": "Jiaming Han",
      "name": "csuhan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18254",
      "authors": [
        {
          "_id": "685a39d60e4ad7e219758622",
          "name": "Tianyu Yu",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758623",
          "name": "Bo Ji",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758624",
          "name": "Shouli Wang",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758625",
          "name": "Shu Yao",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758626",
          "name": "Zefan Wang",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758627",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758628",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e219758629",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862a",
          "name": "Yuan Yao",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862b",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862c",
          "name": "Maosong Sun",
          "hidden": false
        },
        {
          "_id": "685a39d60e4ad7e21975862d",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T02:56:36.000Z",
      "submittedOnDailyAt": "2025-06-24T04:14:54.081Z",
      "title": "RLPR: Inferencia de RLVR en el ámbito general a partir de datos no válidos",
      "submittedOnDailyBy": {
        "_id": "64abc4aa6cadc7aca585dddf",
        "avatarUrl": "/avatars/736afea979cd0021c7a37f68731524ea.svg",
        "isPro": false,
        "fullname": "Tianyu Yu",
        "user": "Yirany",
        "type": "user"
      },
      "summary": "La aprendizaje por refuerzo con recompensas confiables (RLVR) demuestra una prometedora posibilidad para el desarrollo de habilidades lógicas en los modelos de lenguaje grande (LLM). Sin embargo, su éxito se limita principalmente a las áreas de matemáticas y código. Esta limitación principal se debe a que el enfoque se centra en bases de datos específicas, lo que hace que la complejidad sea infinita y que la escalabilidad sea limitada. Para enfrentar estos desafíos, nuestra principal perspectiva es que los LLM tienen un propio probabilismo inherente para generar respuestas precisas, lo que les permite evaluar las recompensas lógicas (cómo se conducen los pasos teóricos a una respuesta correcta). Basándonos en esta observación, proponemos un simple marco sin base de datos llamado RLPR (Reinforcement Learning con Recompensas Probabilísticas). RLPR utiliza los puntajes de probabilidad de los tokens propios de los LLM como señales de recompensa, maximizando la esperanza de la recompensa durante el entrenamiento. Reconocemos la importancia de reducir la alta variación de la recompensa probabilística, así como la necesidad de garantizar una recompensa estable y precisa a partir de la probabilidad propia del LLM, proponiendo métodos como prob-to-reward y estabilización. Los experimentos detallados en cuatro marcos de referencia generales y tres benchmarks de matemáticas muestran que la capacidad lógica se mejora de manera consistente en modelos basados en Gemma, Llama y Qwen. En particular, en TheoremQA superan a VeriFree con 7.6 puntos y a Minerva con 7.5 puntos, y en los 7 benchmarks generales, el promedio de puntajes del General-Reasoner se incrementa en más de 1.6 puntos.",
      "upvotes": 18,
      "discussionId": "685a39d80e4ad7e21975862e",
      "projectPage": "https://github.com/OpenBMB/RLPR",
      "githubRepo": "https://github.com/OpenBMB/RLPR",
      "ai_summary": "RLPR, a verifier-free framework using LLM's token probability scores as reward signals, enhances reasoning capabilities across both general and mathematical domains, outperforming other methods in various benchmarks.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "reasoning capabilities",
        "LLM",
        "RLPR",
        "token probability scores",
        "prob-to-reward",
        "stabilizing methods",
        "TheoremQA",
        "Minerva",
        "General-Reasoner"
      ]
    },
    "publishedAt": "2025-06-22T22:56:36.000Z",
    "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising\npotential in advancing the reasoning capabilities of LLMs. However, its success\nremains largely confined to mathematical and code domains. This primary\nlimitation stems from the heavy reliance on domain-specific verifiers, which\nresults in prohibitive complexity and limited scalability. To address the\nchallenge, our key observation is that LLM's intrinsic probability of\ngenerating a correct free-form answer directly indicates its own evaluation of\nthe reasoning reward (i.e., how well the reasoning process leads to the correct\nanswer). Building on this insight, we propose RLPR, a simple verifier-free\nframework that extrapolates RLVR to broader general domains. RLPR uses the\nLLM's own token probability scores for reference answers as the reward signal\nand maximizes the expected reward during training. We find that addressing the\nhigh variance of this noisy probability reward is crucial to make it work, and\npropose prob-to-reward and stabilizing methods to ensure a precise and stable\nreward from LLM intrinsic probabilities. Comprehensive experiments in four\ngeneral-domain benchmarks and three mathematical benchmarks show that RLPR\nconsistently improves reasoning capabilities in both areas for Gemma, Llama,\nand Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6\npoints on TheoremQA and 7.5 points on Minerva, and even surpasses strong\nverifier-model-dependent approaches General-Reasoner by 1.6 average points\nacross seven benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18254.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64abc4aa6cadc7aca585dddf",
      "avatarUrl": "/avatars/736afea979cd0021c7a37f68731524ea.svg",
      "fullname": "Tianyu Yu",
      "name": "Yirany",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15741",
      "authors": [
        {
          "_id": "685a48970e4ad7e219758662",
          "name": "He Zhu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758663",
          "user": {
            "_id": "64301abe450c0de9a1d3d18e",
            "avatarUrl": "/avatars/01b284874dadc7d21d656c53dcb77e42.svg",
            "isPro": false,
            "fullname": "tianrui",
            "user": "tianyue818",
            "type": "user"
          },
          "name": "Tianrui Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T07:14:35.823Z",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758664",
          "user": {
            "_id": "6578265ddea7e2122d02f6ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578265ddea7e2122d02f6ba/Bh6JjoVF5ceLSjV7Z7nTk.jpeg",
            "isPro": false,
            "fullname": "kang zhu",
            "user": "kangz",
            "type": "user"
          },
          "name": "King Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T09:33:02.082Z",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758665",
          "name": "Heyuan Huang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758666",
          "name": "Yeyi Guan",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758667",
          "name": "Jinxiang Xia",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758668",
          "name": "Yi Yao",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758669",
          "name": "Hanhao Li",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866a",
          "name": "Ningning Wang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866b",
          "name": "Pai Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866c",
          "name": "Tianhao Peng",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866d",
          "name": "Xin Gui",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866e",
          "name": "Xiaowan Li",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e21975866f",
          "name": "Yuhui Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758670",
          "name": "Yuchen Eleanor Jiang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758671",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758672",
          "name": "Changwang Zhang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758673",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758674",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758675",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758676",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758677",
          "name": "Xitong Gao",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758678",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "685a48970e4ad7e219758679",
          "name": "Wangchunshu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T17:59:02.000Z",
      "submittedOnDailyAt": "2025-06-24T05:17:03.413Z",
      "title": "OAgents: Investigación Empírica sobre la Construcción de Agnets Válidos",
      "submittedOnDailyBy": {
        "_id": "628c8598ef14f971b698107f",
        "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
        "isPro": false,
        "fullname": "Zhou",
        "user": "Wangchunshu",
        "type": "user"
      },
      "summary": "Recientemente, el campo de investigación de la IA Agentic ha aumentado. Sin embargo, la práctica actual de la investigación en Agentes presenta problemas de estandarización y falta de ciencia, lo que dificulta la comparación equitativa entre métodos. Como consecuencia, no es posible determinar qué efecto tienen las diferentes diseños de marco de agentes, ni tampoco es fácil evaluar su desarrollo. En este estudio, se investigarán sistemáticamente el GAIA benchmark y el BrowseComp, examinando los impactos de los diseños populares de manera equitativa y rigurosa. Hemos encontrado defectos en los protocolos de evaluación estándar de estudios anteriores, incluyendo códigos abiertos, y observamos falta de reproducibilidad y grandes variaciones extremos entre experimentos aleatorios. Por ello, hemos introducido un protocolo de evaluación más robusto para mejorar la estabilidad. Nuestro estudio muestra que ciertas composiciones y diseños son esenciales para un Agente efectivo, mientras que otros son ineficientes. Basándonos en nuestros hallazgos, hemos construido y proporcionado de manera abierta un nuevo marco de agentes de base, OAgents. OAgents ofrece un diseño modular que integra múltiples componentes de agentes, lo que fomenta la investigación futura de la IA Agentic.",
      "upvotes": 18,
      "discussionId": "685a48980e4ad7e21975867a"
    },
    "publishedAt": "2025-06-17T13:59:02.000Z",
    "title": "OAgents: An Empirical Study of Building Effective Agents",
    "summary": "Recently, Agentic AI has become an increasingly popular research field.\nHowever, we argue that current agent research practices lack standardization\nand scientific rigor, making it hard to conduct fair comparisons among methods.\nAs a result, it is still unclear how different design choices in agent\nframeworks affect effectiveness, and measuring their progress remains\nchallenging. In this work, we conduct a systematic empirical study on GAIA\nbenchmark and BrowseComp to examine the impact of popular design choices in key\nagent components in a fair and rigorous manner. We find that the lack of a\nstandard evaluation protocol makes previous works, even open-sourced ones,\nnon-reproducible, with significant variance between random runs. Therefore, we\nintroduce a more robust evaluation protocol to stabilize comparisons. Our study\nreveals which components and designs are crucial for effective agents, while\nothers are redundant, despite seeming logical. Based on our findings, we build\nand open-source OAgents, a new foundation agent framework that achieves\nstate-of-the-art performance among open-source projects. OAgents offers a\nmodular design for various agent components, promoting future research in\nAgentic AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15741.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "628c8598ef14f971b698107f",
      "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
      "fullname": "Zhou",
      "name": "Wangchunshu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18903",
      "authors": [
        {
          "_id": "685a1ce80e4ad7e2197585b7",
          "name": "Runjia Li",
          "hidden": false
        },
        {
          "_id": "685a1ce80e4ad7e2197585b8",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "685a1ce80e4ad7e2197585b9",
          "name": "Andrea Vedaldi",
          "hidden": false
        },
        {
          "_id": "685a1ce80e4ad7e2197585ba",
          "name": "Tomas Jakab",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638e29cf319f9c746b87ad4b/Gt6zbaM3ILEQIG4Cl1W2J.mp4"
      ],
      "publishedAt": "2025-06-23T17:59:56.000Z",
      "submittedOnDailyAt": "2025-06-24T02:12:37.233Z",
      "title": "VMem: Memoria de vista para el uso de índices de serpiente para la generación de escenarios interactivos de video coherentes",
      "submittedOnDailyBy": {
        "_id": "638e29cf319f9c746b87ad4b",
        "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
        "isPro": true,
        "fullname": "Runjia Li",
        "user": "liguang0115",
        "type": "user"
      },
      "summary": "Proponemos una nueva estructura de memoria. Esta estructura permite construir un generador de videos que pueden explorar el ambiente interactivamente. Anteriormente, se lograban resultados similares al expandir la visión 2D hacia el exterior mediante un método que reconstruía de manera gradual la geometría 3D, pero este método acumulaba errores rápidamente. Además, se lograban los mismos resultados en generadores de videos que mantenían una consistencia a largo plazo. Para resolver estas limitaciones, introducimos un mecanismo de índice geométrico basado en superficies de elementos (surfels) para memorizar visiones pasadas, llamado Memoria de Visiones Indexadas por Surfels (VMem). VMem permite obtener visiones pasadas relevantes de manera eficiente cuando se crea una nueva visión. De esta manera, se puede realizar una exploración coherente del entorno imaginado con un menor costo computacional, comparado con el uso de partes del pasado como contexto. Nuestro método se evaluó en benchmarks de síntesis de visión a largo plazo y muestra un excelente rendimiento en la coherencia de la visión y el control de la cámara, comparado con los métodos existentes.",
      "upvotes": 8,
      "discussionId": "685a1ce80e4ad7e2197585bb",
      "projectPage": "https://v-mem.github.io/",
      "githubRepo": "https://github.com/runjiali-rl/vmem",
      "ai_summary": "A novel memory mechanism called Surfel-Indexed View Memory enhances video generation by efficiently remembering and retrieving relevant past views, improving long-term scene coherence and reducing computational cost.",
      "ai_keywords": [
        "memory mechanism",
        "video generators",
        "out-painting",
        "3D geometry",
        "context window",
        "Surfel-Indexed View Memory",
        "surfels",
        "scene coherence",
        "camera control",
        "long-term scene synthesis benchmarks"
      ]
    },
    "publishedAt": "2025-06-23T13:59:56.000Z",
    "title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed\n  View Memory",
    "summary": "We propose a novel memory mechanism to build video generators that can\nexplore environments interactively. Similar results have previously been\nachieved by out-painting 2D views of the scene while incrementally\nreconstructing its 3D geometry, which quickly accumulates errors, or by video\ngenerators with a short context window, which struggle to maintain scene\ncoherence over the long term. To address these limitations, we introduce\nSurfel-Indexed View Memory (VMem), a mechanism that remembers past views by\nindexing them geometrically based on the 3D surface elements (surfels) they\nhave observed. VMem enables the efficient retrieval of the most relevant past\nviews when generating new ones. By focusing only on these relevant views, our\nmethod produces consistent explorations of imagined environments at a fraction\nof the computational cost of using all past views as context. We evaluate our\napproach on challenging long-term scene synthesis benchmarks and demonstrate\nsuperior performance compared to existing methods in maintaining scene\ncoherence and camera control.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638e29cf319f9c746b87ad4b/Gt6zbaM3ILEQIG4Cl1W2J.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18903.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638e29cf319f9c746b87ad4b",
      "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
      "fullname": "Runjia Li",
      "name": "liguang0115",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18309",
      "authors": [
        {
          "_id": "685a2df10e4ad7e219758604",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758605",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758606",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758607",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758608",
          "name": "Jianfeng Liu",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e219758609",
          "name": "Yuefeng Zhan",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860a",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860b",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860c",
          "name": "Weiwei Deng",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860d",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860e",
          "name": "Feng Sun",
          "hidden": false
        },
        {
          "_id": "685a2df10e4ad7e21975860f",
          "name": "Qi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T05:51:52.000Z",
      "submittedOnDailyAt": "2025-06-24T03:21:15.103Z",
      "title": "RITENGO: Exploración del sistema de recomendaciones para la generación de perfiles de usuarios",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "El profilado de usuarios es crucial en los sistemas de recomendación, ya que permite convertir datos de interacción de usuarios en una representación estructurada y sencilla, lo que permite generar recomendaciones personalizadas. El profilado tradicional basado en embeddinges presenta limitaciones en la interpretabilidad y adaptabilidad, pero el desarrollo de grandes modelos de lenguaje (LLMs) recientes ha permitido la creación de un profilado basado en texto significativo y transparente. Sin embargo, los métodos actuales dependen de formatos fijos y tienen limitaciones para comprender la diversidad de acciones de los usuarios. En este artículo, se presenta LettinGo, un nuevo marco de trabajo que utiliza la capacidad de los LLMs para generar un profilado de usuarios con diversidad y adaptabilidad, y que recibe retroalimentación directa de las tareas de recomendación. Este enfoque evita los restricciones causadas por el ajuste de micro (SFT) y utiliza la Optimización de Preferencias Directas (DPO) para garantizar que el generador de profilados sea adaptable y efectivo en función de la rendimiento específico de la tarea. LettinGo funciona en tres etapas: 1. exploración de diferentes profilados de usuarios, 2. evaluación de la calidad del profilado basada en su impacto en el sistema de recomendación, y 3. ajuste del generador de profilados basado en datos de preferencias obtenidas del rendimiento de la tarea. Los resultados de los experimentos muestran que nuestro marco de trabajo mejora significativamente la precisión de la recomendación, la flexibilidad y la comprensión del contexto. Esta investigación representa una innovación importante en la generación de profilados de usuarios para los siguientes sistemas de recomendación.",
      "upvotes": 6,
      "discussionId": "685a2df20e4ad7e219758610",
      "ai_summary": "LettinGo enhances user profiling via diverse, adaptive profiles generated using LLMs and Direct Preference Optimization, improving recommendation accuracy and flexibility.",
      "ai_keywords": [
        "large language models (LLMs)",
        "semantically richer",
        "more transparent",
        "supervised fine-tuning (SFT)",
        "Direct Preference Optimization (DPO)",
        "profile generator",
        "pairwise preference data",
        "recommendation systems",
        "recommendation accuracy",
        "flexibility",
        "contextual awareness"
      ]
    },
    "publishedAt": "2025-06-23T01:51:52.000Z",
    "title": "LettinGo: Explore User Profile Generation for Recommendation System",
    "summary": "User profiling is pivotal for recommendation systems, as it transforms raw\nuser interaction data into concise and structured representations that drive\npersonalized recommendations. While traditional embedding-based profiles lack\ninterpretability and adaptability, recent advances with large language models\n(LLMs) enable text-based profiles that are semantically richer and more\ntransparent. However, existing methods often adhere to fixed formats that limit\ntheir ability to capture the full diversity of user behaviors. In this paper,\nwe introduce LettinGo, a novel framework for generating diverse and adaptive\nuser profiles. By leveraging the expressive power of LLMs and incorporating\ndirect feedback from downstream recommendation tasks, our approach avoids the\nrigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ\nDirect Preference Optimization (DPO) to align the profile generator with\ntask-specific performance, ensuring that the profiles remain adaptive and\neffective. LettinGo operates in three stages: (1) exploring diverse user\nprofiles via multiple LLMs, (2) evaluating profile quality based on their\nimpact in recommendation systems, and (3) aligning the profile generation\nthrough pairwise preference data derived from task performance. Experimental\nresults demonstrate that our framework significantly enhances recommendation\naccuracy, flexibility, and contextual awareness. This work enhances profile\ngeneration as a key innovation for next-generation recommendation systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18309.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18631",
      "authors": [
        {
          "_id": "685a1b390e4ad7e2197585a9",
          "name": "Chenxing Wei",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585aa",
          "name": "Jiarui Yu",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ab",
          "name": "Ying Tiffany He",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ac",
          "name": "Hande Dong",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ad",
          "name": "Yao Shu",
          "hidden": false
        },
        {
          "_id": "685a1b390e4ad7e2197585ae",
          "name": "Fei Yu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65ed3051492a7f35db21fea2/Oc7EWF5wDYqu1FEvfuAlq.png"
      ],
      "publishedAt": "2025-06-23T13:36:24.000Z",
      "submittedOnDailyAt": "2025-06-24T02:05:49.825Z",
      "title": "Redit: Mejora de la Optimización de Políticas de LLM con Diseño de Recompensas",
      "submittedOnDailyBy": {
        "_id": "65ed3051492a7f35db21fea2",
        "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
        "isPro": false,
        "fullname": "Chenxing Wei",
        "user": "kittttttt",
        "type": "user"
      },
      "summary": "DeepSeek-R1 mejoró exitosamente la capacidad de inferencia de un modelo de lenguaje grande (LLM) utilizando un sistema de recompensas basado en reglas (Reward System). Este sistema de recompensas demuestra \"completudad\" y efectivamente inhibe el hacking de recompensas, pero su función de recompensa generalmente es discreta (discrete). De nuestros observaciones experimentales, esta discretidad de recompensa conduce a problemas como la inestabilidad del gradiente, la optimización no estable y la convergencia gradual. Para resolver estos problemas, proponemos ReDit (Reward Design). ReDit es un método para diseñar señales de recompensa discretas añadiendo simplemente una aleatoriedad. Con esta recompensa desordenada, se mantiene un gradiente exploratorio continuo durante todo el proceso de aprendizaje, permitiendo actualizaciones de gradiente suaves y acelerando la convergencia. La aleatoriedad injetada llama la atención a la particularidad en los dominios suaves de recompensa y incentiva al modelo a encontrar nuevas políticas para escapar de los óptimos locales. Los experimentos en diferentes tareas demuestran la eficacia y eficiencia de ReDit. En promedio, ReDit alcanza un rendimiento equivalente a GRPO en aproximadamente 10% menos etapas de aprendizaje y muestra un aumento de rendimiento del 4% en un período de aprendizaje de la misma duración. Se confirma un gran efecto en la inhibición de problemas de gradiente en ReDit. Además, se proporcionan análisis teóricos que proban aún más esta excelencia.",
      "upvotes": 5,
      "discussionId": "685a1b390e4ad7e2197585af",
      "githubRepo": "https://github.com/kithib/ReDit",
      "ai_summary": "ReDit, a reward dithering method, addresses issues in discrete reward systems by introducing noise, leading to smoother optimization and faster convergence compared to standard methods.",
      "ai_keywords": [
        "rule-based reward system",
        "reward hacking",
        "discrete rewards",
        "gradient anomaly",
        "unstable optimization",
        "slow convergence",
        "random noise",
        "exploratory gradients",
        "flat reward regions",
        "local optima",
        "vanilla GRPO",
        "performance improvement",
        "gradient issues"
      ]
    },
    "publishedAt": "2025-06-23T09:36:24.000Z",
    "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
    "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65ed3051492a7f35db21fea2/Oc7EWF5wDYqu1FEvfuAlq.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18631.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ed3051492a7f35db21fea2",
      "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
      "fullname": "Chenxing Wei",
      "name": "kittttttt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18527",
      "authors": [
        {
          "_id": "685a0cba0e4ad7e21975850c",
          "name": "JiaKui Hu",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e21975850d",
          "name": "Yuxiao Yang",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e21975850e",
          "name": "Jialun Liu",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e21975850f",
          "name": "Jinbo Wu",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e219758510",
          "name": "Chen Zhao",
          "hidden": false
        },
        {
          "_id": "685a0cba0e4ad7e219758511",
          "name": "Yanye Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T11:28:37.000Z",
      "submittedOnDailyAt": "2025-06-24T03:17:08.044Z",
      "title": "Automáticamente generamos imágenes de coincidencia en múltiples puntos a través de la inversión automática.",
      "submittedOnDailyBy": {
        "_id": "64ccd5cc4726a3f833831087",
        "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
        "isPro": false,
        "fullname": "Hu",
        "user": "Jiakui",
        "type": "user"
      },
      "summary": "Los imágenes múltiples de punto de vista son un elemento importante en la generación de contenido 3D. Uno de los principales problemas es la coincidencia en múltiples puntos de vista y la síntesis válida de formas y texturas en diferentes condiciones. En este artículo, se propone el método de regresión automática múltiple de puntos de vista (MV-AR) y se objetiva utilizar modelos de regresión automática para generar imágenes múltiples de punto de vista en secuencia a partir de cualquier prompt. Primero, el poder predictivo de los siguientes tokens del modelo de regresión automática mejora significativamente la eficiencia de la síntesis secuencial de imágenes múltiples de punto de vista. Cuando los puntos de vista están muy separados, MV-AR puede extraer información de referencia válida utilizando todos los puntos de vista anteriores. Además, se propone un modelo integrado que combina diseño estructural y etapa de entrenamiento para abordar diversos prompts. Para procesar diferentes condiciones, se introduce un módulo de entrada de condiciones que toma en cuenta contexto, posición de la cámara, imágenes y formas. Para manejar múltiples condiciones simultáneamente, se utiliza un entrenamiento secuencial. Esta etapa permite implementar el modelo MV-AR (X2mv) basado en el modelo t2mv, donde se generan y combinan condiciones aleatoriamente a partir de texto. Finalmente, para resolver el problema de sobreajuste debido a datos de alta calidad limitados, se propone la técnica \"Shuffle View\" para ampliar los datos de entrenamiento. Los experimentos demostraron la eficacia y diversidad del MV-AR, la generación de imágenes múltiples de punto de vista en condiciones variadas, y un rendimiento comparable con modelos avanzados de generación de imágenes múltiples de punto de vista basados en difusión. Los códigos y modelos están previstas para ser publicados en https://github.com/MILab-PKU/MVAR.",
      "upvotes": 4,
      "discussionId": "685a0cbb0e4ad7e219758512",
      "ai_summary": "The Multi-View Auto-Regressive (MV-AR) method uses an auto-regressive model to generate consistent multi-view images from prompts, addressing challenges in shape and texture synthesis across diverse conditions.",
      "ai_keywords": [
        "Multi-View Auto-Regressive",
        "MV-AR",
        "auto-regressive model",
        "next-token-prediction",
        "condition injection modules",
        "text-to-multi-view",
        "X-to-multi-view",
        "progressive training strategy",
        "Shuffle View",
        "data augmentation"
      ]
    },
    "publishedAt": "2025-06-23T07:28:37.000Z",
    "title": "Auto-Regressively Generating Multi-View Consistent Images",
    "summary": "Generating multi-view images from human instructions is crucial for 3D\ncontent creation. The primary challenges involve maintaining consistency across\nmultiple views and effectively synthesizing shapes and textures under diverse\nconditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR)\nmethod, which leverages an auto-regressive model to progressively generate\nconsistent multi-view images from arbitrary prompts. Firstly, the\nnext-token-prediction capability of the AR model significantly enhances its\neffectiveness in facilitating progressive multi-view synthesis. When generating\nwidely-separated views, MV-AR can utilize all its preceding views to extract\neffective reference information. Subsequently, we propose a unified model that\naccommodates various prompts via architecture designing and training\nstrategies. To address multiple conditions, we introduce condition injection\nmodules for text, camera pose, image, and shape. To manage multi-modal\nconditions simultaneously, a progressive training strategy is employed. This\nstrategy initially adopts the text-to-multi-view (t2mv) model as a baseline to\nenhance the development of a comprehensive X-to-multi-view (X2mv) model through\nthe randomly dropping and combining conditions. Finally, to alleviate the\noverfitting problem caused by limited high-quality data, we propose the\n\"Shuffle View\" data augmentation technique, thus significantly expanding the\ntraining data by several magnitudes. Experiments demonstrate the performance\nand versatility of our MV-AR, which consistently generates consistent\nmulti-view images across a range of conditions and performs on par with leading\ndiffusion-based multi-view image generation models. Code and models will be\nreleased at https://github.com/MILab-PKU/MVAR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ccd5cc4726a3f833831087",
      "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
      "fullname": "Hu",
      "name": "Jiakui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18349",
      "authors": [
        {
          "_id": "685a02460e4ad7e2197584a9",
          "user": {
            "_id": "659c6a50615d5e661222fe16",
            "avatarUrl": "/avatars/8a946482e49a821dbe397dc3898f22c5.svg",
            "isPro": false,
            "fullname": "Zichong Li",
            "user": "Pearush",
            "type": "user"
          },
          "name": "Zichong Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:56.270Z",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584aa",
          "name": "Chen Liang",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ab",
          "name": "Zixuan Zhang",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ac",
          "name": "Ilgee Hong",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ad",
          "name": "Young Jin Kim",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584ae",
          "name": "Weizhu Chen",
          "hidden": false
        },
        {
          "_id": "685a02460e4ad7e2197584af",
          "name": "Tuo Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T07:15:59.000Z",
      "submittedOnDailyAt": "2025-06-24T02:19:52.155Z",
      "title": "SlimMoE: Estructuración de compresión de MoE modelos grandes y slicing de expertos con calentamiento",
      "submittedOnDailyBy": {
        "_id": "63e6b5e22d2c508de9001afd",
        "avatarUrl": "/avatars/43cec9e8b8d490bd259e383954846a1e.svg",
        "isPro": false,
        "fullname": "Chen Liang",
        "user": "cliang1453",
        "type": "user"
      },
      "summary": "La arquitectura de Mixture of Experts (MoE) ha surgido como un potente paradigma para expandir grandes modelos de lenguaje (LLMs) mientras mantiene la eficiencia de inferencia. Sin embargo, su alta demanda en memoria impone desafíos para fine-tuning o deploying en entornos con limitaciones de recursos. Para abordar estos desafíos, presentamos SlimMoE. SlimMoE es un marco de trabajo de compresión multi-nivel que transforma modelos MoE expansidos en versiones eficientes al reducirlos significativamente, diseñado para evitar los costos prohibitivos de entrenamiento desde el principio. Nuestro enfoque es simplificar el modelo de expertos y transmitir conocimientos en etapas intermedias, lo que permite reducir sistemáticamente el número de parámetros y mitigar la pérdida de rendimiento debido a la acceso de bloques. Este marco de trabajo permite que el modelo Phi 3.5-MoE (con un total de 41.9B/activación de 16.6B parámetros) se comprese en Phi-mini-MoE (con un total de 7.6B/activación de 2.4B parámetros) y Phi-tiny-MoE (con un total de 3.8B/activación de 1.1B parámetros). Estos modelos compresados pueden ser fine-tuned en un solo GPU (A100 para Phi-mini-MoE, A6000 para Phi-tiny-MoE) y ofrecen una alta calidad en aplicaciones académicas o en entornos con limitaciones de recursos. Los resultados de los experimentos muestran que estos modelos compresados superan a modelos de la misma tamaño y compiten con modelos más grandes. Por ejemplo, Phi-mini-MoE utiliza el 2/3 de los parámetros activos y alcanza o supera el rendimiento de Phi-3-mini, presentando una puntuación MMLU comparable a Llama 3.1 8B con un importante ahorro de costos. Nuestro hallazgo demuestra que la combinación de planteamientos estructurales y desgaste de etapas permite crear modelos pequeños de alta calidad, fomentando la amplia adopción de la arquitectura MoE. Nuestros modelos se pueden encontrar en los siguientes URLs:\n\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct",
      "upvotes": 4,
      "discussionId": "685a02460e4ad7e2197584b0",
      "ai_summary": "SlimMoE compresses large MoE models into smaller, efficient variants using multi-stage compression without full retraining, maintaining competitive performance with significantly fewer resources.",
      "ai_keywords": [
        "Mixture of Experts (MoE)",
        "large language models (LLMs)",
        "parameter counts",
        "knowledge transfer",
        "one-shot pruning",
        "Phi 3.5-MoE",
        "Phi-mini-MoE",
        "Phi-tiny-MoE",
        "structured pruning",
        "staged distillation",
        "MMLU scores",
        "latency"
      ]
    },
    "publishedAt": "2025-06-23T03:15:59.000Z",
    "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming\n  and Distillation",
    "summary": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm\nfor scaling large language models (LLMs) while maintaining inference\nefficiency. However, their enormous memory requirements make them prohibitively\nexpensive to fine-tune or deploy in resource-constrained environments. To\naddress this challenge, we introduce SlimMoE, a multi-stage compression\nframework for transforming large MoE models into much smaller, efficient\nvariants without incurring the prohibitive costs of training from scratch. Our\nmethod systematically reduces parameter counts by slimming experts and\ntransferring knowledge through intermediate stages, effectively mitigating the\nperformance degradation common in one-shot pruning approaches. Using this\nframework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to\ncreate Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE\n(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of\nthe original model's training data. These compressed models can be fine-tuned\non a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them\nhighly suitable for academic and resource-limited settings. Our experiments\ndemonstrate that these compressed models outperform others of similar size and\nremain competitive with larger models. For instance, Phi-mini-MoE achieves\nsimilar or better performance to Phi-3-mini using only 2/3 of the activated\nparameters and yields comparable MMLU scores to Llama 3.1 8B despite having\nsignificantly lower latency. Our findings demonstrate that structured pruning\ncombined with staged distillation offers an effective path to creating\nhigh-quality, compact MoE models, paving the way for broader adoption of MoE\narchitectures. We make our models publicly available at\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct and\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18349.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e6b5e22d2c508de9001afd",
      "avatarUrl": "/avatars/43cec9e8b8d490bd259e383954846a1e.svg",
      "fullname": "Chen Liang",
      "name": "cliang1453",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16962",
      "authors": [
        {
          "_id": "6858d907c0c8e29df8ea3ce2",
          "user": {
            "_id": "67547707f168984215451697",
            "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
            "isPro": false,
            "fullname": "manglu",
            "user": "manglu3935",
            "type": "user"
          },
          "name": "Haoran Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-23T15:52:05.938Z",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce3",
          "name": "Yankai Jiang",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce4",
          "name": "Wenjie Lou",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce5",
          "name": "Yujie Zhang",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce6",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce7",
          "name": "Lilong Wang",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce8",
          "name": "Mianxin Liu",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3ce9",
          "name": "Lei Liu",
          "hidden": false
        },
        {
          "_id": "6858d907c0c8e29df8ea3cea",
          "name": "Xiaosong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T12:51:19.000Z",
      "submittedOnDailyAt": "2025-06-24T02:20:40.889Z",
      "title": "Mejora de la posibilidad de verificación lógica médica de modelos microlibrerías por etapa",
      "submittedOnDailyBy": {
        "_id": "67547707f168984215451697",
        "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
        "isPro": false,
        "fullname": "manglu",
        "user": "manglu3935",
        "type": "user"
      },
      "summary": "Los módulos de lenguaje de la Universidad de Damodar (MLLMs) comienzan mostrando una fuerte capacidad lógica en tareas generales, pero su aplicación en el campo médico está en las primeras etapas. Para fortalecer la capacidad lógica de los MLLMs médicos, es crucial la construcción de datos de entrenamiento de \"Chain-of-Thought\" (CoT). Sin embargo, la metodología actual no proporciona un marco para buscar y evaluar pasos lógicos válidos para diagnósticos importantes. Para resolver estas desafíos, proponemos la Mentor-Intern Collaboration Search (MICS). MICS es un nuevo esquema de búsqueda de pasos lógicos para generar datos CoT estrictos y válidos en el campo médico. MICS empieza con el Modelo Mayoría para iniciar el proceso lógico, después activa cada modelo de proyecto según el paso inicial, y finalmente selecciona el mejor paso lógico basándose en el rendimiento lógico de múltiples modelos de proyecto. El rendimiento lógico se evalúa mediante el MICS-Score, que mide la calidad de los pasos lógicos generados. Finalmente, hemos construido el nuevo MLLM médico Chiron-o1 utilizando el conjunto de datos médicos complejos MMRP y estrategias de enseñanza de materias. Chiron-o1 se entrenó con los datos CoT generados por MICS y ha logrado los mejores resultados en el campo de la visualización médica y en los marcos de referencia lógicos. El código está disponible en GitHub - manglu097/Chiron-o1: Código para mejorar la lógica y validación médica en MLLMs de manera iterativa.",
      "upvotes": 4,
      "discussionId": "6858d907c0c8e29df8ea3ceb",
      "githubRepo": "https://github.com/manglu097/Chiron-o1",
      "ai_summary": "MICS, a novel reasoning-path searching scheme, enhances medical MLLMs like Chiron-o1 with robust generalizable reasoning and visual question-answering capabilities through comprehensive chain-of-thought data generation.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "chain-of-thought",
        "Mentor-Intern Collaborative Search",
        "MICS",
        "mentor models",
        "intern models",
        "MICS-Score",
        "multi-task medical reasoning dataset",
        "MMRP",
        "curriculum learning",
        "medical visual question answering",
        "reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-06-20T08:51:19.000Z",
    "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
    "summary": "Multimodal large language models (MLLMs) have begun to demonstrate robust\nreasoning capabilities on general tasks, yet their application in the medical\ndomain remains in its early stages. Constructing chain-of-thought (CoT)\ntraining data is essential for bolstering the reasoning abilities of medical\nMLLMs. However, existing approaches exhibit a deficiency in offering a\ncomprehensive framework for searching and evaluating effective reasoning paths\ntowards critical diagnosis. To address this challenge, we propose Mentor-Intern\nCollaborative Search (MICS), a novel reasoning-path searching scheme to\ngenerate rigorous and effective medical CoT data. MICS first leverages mentor\nmodels to initialize the reasoning, one step at a time, then prompts each\nintern model to continue the thinking along those initiated paths, and finally\nselects the optimal reasoning path according to the overall reasoning\nperformance of multiple intern models. The reasoning performance is determined\nby an MICS-Score, which assesses the quality of generated reasoning paths.\nEventually, we construct MMRP, a multi-task medical reasoning dataset with\nranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum\nlearning strategy, with robust visual question-answering and generalizable\nreasoning capabilities. Extensive experiments demonstrate that Chiron-o1,\ntrained on our CoT dataset constructed using MICS, achieves state-of-the-art\nperformance across a list of medical visual question answering and reasoning\nbenchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing\nStep-by-Step and Verifiable Medical Reasoning in MLLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16962.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67547707f168984215451697",
      "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
      "fullname": "manglu",
      "name": "manglu3935",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18792",
      "authors": [
        {
          "_id": "685a72a00e4ad7e219758702",
          "name": "Michal Nazarczuk",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758703",
          "name": "Sibi Catley-Chandar",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758704",
          "name": "Thomas Tanay",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758705",
          "name": "Zhensong Zhang",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758706",
          "name": "Gregory Slabaugh",
          "hidden": false
        },
        {
          "_id": "685a72a00e4ad7e219758707",
          "name": "Eduardo Pérez-Pellitero",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T16:01:15.000Z",
      "submittedOnDailyAt": "2025-06-24T08:15:41.084Z",
      "title": "ViDAR: Reconstrucción 4D para cámaras de interés - diferenciación de vídeo",
      "submittedOnDailyBy": {
        "_id": "66f44a3df9252e0f50b59fdb",
        "avatarUrl": "/avatars/4234203c77a0e6f594f3de26bfe8c649.svg",
        "isPro": false,
        "fullname": "Michal Nazarczuk",
        "user": "michaal94",
        "type": "user"
      },
      "summary": "La generación de imágenes de movimiento dinámico de nuevos puntos de vista tiene como objetivo crear imágenes de cuerpos en movimiento desde cualquier punto de vista. Este trabajo es especialmente difícil cuando se depende de vídeos de única perspectiva, ya que es complicado separar la estructura y generar subimágenes insuficientes. Utilizamos modelos de difusión personalizados para presentar el marco de reconstrucción 4D Video Diffusion-Aware Reconstruction (ViDAR), que utiliza señales de subimágenes de múltiples vistas de Pizemá. ViDAR mantiene la calidad visual e integridad estructural, reduciendo la incertidumbre causada por una única perspectiva. Para abordar la incertidumbre espacio-temporal de la subimágenes difusivas, proponemos una función de difusión Wavelet y la optimización del ángulo de la cámara Stereo. Los experimentos en el benchmark difícil DyCheck muestran que ViDAR supera a todos los límites de base de la calidad visual e integridad estructural. Además, ViDAR muestra significativas mejoras en el área dinámica y proporciona un nuevo benchmark para comparar la reconstrucción de movimientos ricos. El sitio web del proyecto está disponible en https://vidar-4d.github.io.",
      "upvotes": 3,
      "discussionId": "685a72a10e4ad7e219758708",
      "ai_summary": "ViDAR uses diffusion-aware reconstruction to generate high-quality novel views of dynamic scenes from monocular video, outperforming existing methods in visual quality and geometric consistency.",
      "ai_keywords": [
        "Video Diffusion-Aware Reconstruction",
        "ViDAR",
        "Gaussian splatting",
        "diffusion models",
        "spatio-temporal inconsistency",
        "diffusion-aware loss function",
        "camera pose optimisation",
        "DyCheck benchmark"
      ]
    },
    "publishedAt": "2025-06-23T12:01:15.000Z",
    "title": "ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs",
    "summary": "Dynamic Novel View Synthesis aims to generate photorealistic views of moving\nsubjects from arbitrary viewpoints. This task is particularly challenging when\nrelying on monocular video, where disentangling structure from motion is\nill-posed and supervision is scarce. We introduce Video Diffusion-Aware\nReconstruction (ViDAR), a novel 4D reconstruction framework that leverages\npersonalised diffusion models to synthesise a pseudo multi-view supervision\nsignal for training a Gaussian splatting representation. By conditioning on\nscene-specific features, ViDAR recovers fine-grained appearance details while\nmitigating artefacts introduced by monocular ambiguity. To address the\nspatio-temporal inconsistency of diffusion-based supervision, we propose a\ndiffusion-aware loss function and a camera pose optimisation strategy that\naligns synthetic views with the underlying scene geometry. Experiments on\nDyCheck, a challenging benchmark with extreme viewpoint variation, show that\nViDAR outperforms all state-of-the-art baselines in visual quality and\ngeometric consistency. We further highlight ViDAR's strong improvement over\nbaselines on dynamic regions and provide a new benchmark to compare performance\nin reconstructing motion-rich parts of the scene. Project page:\nhttps://vidar-4d.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18792.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66f44a3df9252e0f50b59fdb",
      "avatarUrl": "/avatars/4234203c77a0e6f594f3de26bfe8c649.svg",
      "fullname": "Michal Nazarczuk",
      "name": "michaal94",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17538",
      "authors": [
        {
          "_id": "685a33c50e4ad7e219758612",
          "name": "Yile Gu",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758613",
          "name": "Rohan Kadekodi",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758614",
          "name": "Hoang Nguyen",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758615",
          "user": {
            "_id": "6304ac1a412a1b9d381ca378",
            "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
            "isPro": false,
            "fullname": "Keisuke Kamahori",
            "user": "kamahori",
            "type": "user"
          },
          "name": "Keisuke Kamahori",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:07:52.905Z",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758616",
          "name": "Yiyu Liu",
          "hidden": false
        },
        {
          "_id": "685a33c50e4ad7e219758617",
          "name": "Baris Kasikci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-21T01:32:22.000Z",
      "submittedOnDailyAt": "2025-06-24T03:49:17.409Z",
      "title": "ConsumerBench: Benchmark de Consumidores Marca de prueba para aplicaciones de IA generativa en dispositivos de consumidores",
      "submittedOnDailyBy": {
        "_id": "6304ac1a412a1b9d381ca378",
        "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
        "isPro": false,
        "fullname": "Keisuke Kamahori",
        "user": "kamahori",
        "type": "user"
      },
      "summary": "Recientemente, las aplicaciones de la Inteligencia Artificial Generativa (GenAI) se están moviendo hacia los dispositivos de usuario en entornos de nube, planteando nuevos desafíos en la gestión de recursos, la ingeniería del sistema y la experiencia del usuario. En este artículo, se presenta un marco de referencia de pruebas extendido \"ConsumerBench\" para la ingeniería del sistema y la evaluación del tiempo de respuesta de modelos GenAI que se ejecutan en los dispositivos de usuario. A diferencia de los marcos de referencia actuales, ConsumerBench no asume que los modelos tengan acceso único a un GPU específico. ConsumerBench simula escenarios de múltiples aplicaciones que se ejecutan en hardware limitado. Además, ConsumerBench apoya flujos de trabajo variables que simulan la colaboración de múltiples aplicaciones para tareas complejas. ConsumerBench considera tanto métricas a nivel de aplicación (puntos de entrada, metas de nivel de servicio (SLO)) como métricas a nivel de sistema (uso de CPU/GPU, ancho de banda de memoria). A través de experimentos extendidos, ConsumerBench revela la adecuación de la comparación de recursos, la desigualdad de la distribución extrema y los puntos de máxima eficiencia de los servidores estáticos de modelo. Además, proporciona ideas prácticas para los desarrolladores de modelos y diseñadores de sistemas, revelando los beneficios de kernels personalizados ajustados a la arquitectura de GPU de consumo y el valor de estrategias de programación para los SLO.",
      "upvotes": 3,
      "discussionId": "685a33c70e4ad7e219758618",
      "githubRepo": "https://github.com/efeslab/ConsumerBench",
      "ai_summary": "ConsumerBench evaluates GenAI system efficiency and response time on end-user devices through a comprehensive benchmarking framework, emphasizing realistic multi-application scenarios and customizable workflows.",
      "ai_keywords": [
        "Generative AI",
        "ConsumerBench",
        "system efficiency",
        "response time",
        "benchmarking framework",
        "multi-application scenarios",
        "application-level metrics",
        "latency",
        "Service Level Objective",
        "SLO",
        "system-level metrics",
        "CPU utilization",
        "GPU utilization",
        "memory bandwidth",
        "greedy allocation",
        "static model server configurations",
        "custom kernels",
        "SLO-aware scheduling strategies"
      ]
    },
    "publishedAt": "2025-06-20T21:32:22.000Z",
    "title": "ConsumerBench: Benchmarking Generative AI Applications on End-User\n  Devices",
    "summary": "The recent shift in Generative AI (GenAI) applications from cloud-only\nenvironments to end-user devices introduces new challenges in resource\nmanagement, system efficiency, and user experience. This paper presents\nConsumerBench, a comprehensive benchmarking framework designed to evaluate the\nsystem efficiency and response time of GenAI models running on end-user\ndevices. Unlike existing benchmarks that assume exclusive model access on\ndedicated GPUs, ConsumerBench simulates realistic multi-application scenarios\nexecuting concurrently on constrained hardware. Furthermore, ConsumerBench\nsupports customizable workflows that simulate complex tasks requiring\ncoordination among multiple applications. ConsumerBench captures both\napplication-level metrics, including latency and Service Level Objective (SLO)\nattainment, and system-level metrics like CPU/GPU utilization and memory\nbandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies\nin resource sharing, unfair scheduling under greedy allocation, and performance\npitfalls of static model server configurations. The paper also provides\npractical insights for model developers and system designers, highlighting the\nbenefits of custom kernels tailored to consumer-grade GPU architectures and the\nvalue of implementing SLO-aware scheduling strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6304ac1a412a1b9d381ca378",
      "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
      "fullname": "Keisuke Kamahori",
      "name": "kamahori",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18879",
      "authors": [
        {
          "_id": "685a1a090e4ad7e21975859c",
          "name": "Junyan Li",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e21975859d",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e21975859e",
          "name": "Muhammad Yusuf Hassan",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e21975859f",
          "name": "Talha Chafekar",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a0",
          "name": "Tianle Cai",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a1",
          "name": "Zhile Ren",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a2",
          "name": "Pengsheng Guo",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a3",
          "name": "Foroozan Karimzadeh",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a4",
          "name": "Colorado Reed",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a5",
          "name": "Chong Wang",
          "hidden": false
        },
        {
          "_id": "685a1a090e4ad7e2197585a6",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T17:50:11.000Z",
      "submittedOnDailyAt": "2025-06-24T01:53:55.727Z",
      "title": "CommVQ: Compresión de caché KV utilizando resumenes vectoriales intercambiables",
      "submittedOnDailyBy": {
        "_id": "62d09eb86a61a88ea0d83918",
        "avatarUrl": "/avatars/81b511d94cced304ffca058caff662d4.svg",
        "isPro": false,
        "fullname": "Junyan Li",
        "user": "senfu",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) están aumentando su uso en aplicaciones que requieren largas longitudes de contexto, pero cuando el contexto se expande en la GPU, el caché de clave-valor (KV) es obligado a romperse el memoria. Para enfrentar esto, se propone la técnica de Vectores de Cómo VQ (CommVQ) para reducir significativamente el uso de memoria en la inferencia de LLMs con largo contexto. Primero, se utilizan un encoder ligero y un códigobook para compresionar el caché de KV mediante técnicas de caché adicional, y se transforman en una forma que permita decodificar mediante multiplicación de matrices sencilla. Además, para reducir los costos de cálculo durante la decodificación, se utilizan Embeddings de Posición Rotativa (RoPE) y el algoritmo de Expectation-Maximization (EM) diseñado para el contexto de la comunidad. Así, se puede integrar la decodificación de manera eficiente en la estructura de atención automática. Nuestro enfoque utiliza RoPE-CommVQ para mantener una alta precisión y mostrar un bajo sobrecargado, al reducir significativamente el tamaño del caché de KV FP16 a 87.5% con 2 bits de caché adicional. En los benchmarks de largo contexto y en GSM8K, se ha demostrado un excelente desempeño comparado con otros métodos de caché de KV. En particular, se ha logrado reducir el error de precisión a un mínimo, permitiendo ejecutar el modelo LLaMA-3.1 8B con una longitud de contexto de 128K en un solo GPU RTX 4090. El código fuente está disponible en la siguiente URL: https://github.com/UMass-Embodied-AGI/CommVQ.",
      "upvotes": 2,
      "discussionId": "685a1a090e4ad7e2197585a7",
      "ai_summary": "Commutative Vector Quantization (CommVQ) reduces memory usage in long-context LLM inference by compressing the KV cache with additive quantization and integration of Rotary Position Embedding (RoPE).",
      "ai_keywords": [
        "Commutative Vector Quantization",
        "CommVQ",
        "additive quantization",
        "codebook",
        "Rotary Position Embedding",
        "RoPE",
        "Expectation-Maximization",
        "self-attention",
        "FP16",
        "KV cache quantization",
        "GSM8K",
        "LLaMA-3.1 8B model",
        "RTX 4090 GPU"
      ]
    },
    "publishedAt": "2025-06-23T13:50:11.000Z",
    "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
    "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d09eb86a61a88ea0d83918",
      "avatarUrl": "/avatars/81b511d94cced304ffca058caff662d4.svg",
      "fullname": "Junyan Li",
      "name": "senfu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17673",
      "authors": [
        {
          "_id": "685a5c8e0e4ad7e2197586c7",
          "user": {
            "_id": "64105805928400b416439f10",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
            "isPro": false,
            "fullname": "Seonglae Cho",
            "user": "seonglae",
            "type": "user"
          },
          "name": "Seonglae Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:09:45.757Z",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586c8",
          "name": "Harryn Oh",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586c9",
          "name": "Donghyun Lee",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586ca",
          "name": "Luis Eduardo Rodrigues Vieira",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586cb",
          "name": "Andrew Bermingham",
          "hidden": false
        },
        {
          "_id": "685a5c8e0e4ad7e2197586cc",
          "name": "Ziad El Sayed",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-21T10:18:25.000Z",
      "submittedOnDailyAt": "2025-06-24T06:38:36.202Z",
      "title": "FaithfulSAE: Utilizando un autoencoder autónomo para eliminar la dependencia de un conjunto de datos externo, se extraen características fijas.",
      "submittedOnDailyBy": {
        "_id": "64105805928400b416439f10",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
        "isPro": false,
        "fullname": "Seonglae Cho",
        "user": "seonglae",
        "type": "user"
      },
      "summary": "Los Autoencoders Esparsos (AEs) han aparecido como un potencial solución para decomponer las características de la representación de grandes modelos de lenguaje. Sin embargo, Paulo y Belrose (2025) han notado una inestabilidad dependiente de la inicialización, y Heap et al. (2025) han planteado la posibilidad de que los AEs no puedan capturar las características internas del modelo. Estos problemas podrían deberse a la entrenamiento de los AEs con conjuntos de datos externos. Estos conjuntos de datos pueden ser recopilados de la web o generados por otros modelos, y pueden incluir datos Offensive Data (OOD) que exceden la capacidad de generalización del modelo, lo que puede generar características \"falsas\" que representan inaccuradamente la actividad interna del modelo. Para enfrentar estos problemas, proponemos el método FaithfulSAE. Este método utiliza conjuntos de datos de datos sintéticos generados por el modelo mismo para entrenar los AEs. Al usar FaithfulSAE, demostramos que entrenar AEs con conjuntos de datos OOD puede reducir la inestabilidad de los AEs en diferentes escenarios. Específicamente, FaithfulSAE ha reducido la proporción de características falsas en 5 de los 7 modelos entrenados con conjuntos de datos basados en la web. En general, nuestro enfoque elimina la dependencia de conjuntos de datos externos, mejora la comprensión de las características internas del modelo, mejora la interpretabilidad y subraya la importancia de los conjuntos de datos de entrenamiento de los AEs.",
      "upvotes": 1,
      "discussionId": "685a5c8e0e4ad7e2197586cd",
      "ai_summary": "FaithfulSAE improves Sparse Autoencoder stability and interpretability by training on synthetic datasets generated by the model itself, reducing the occurrence of fake features and out-of-distribution data issues.",
      "ai_keywords": [
        "Sparse Autoencoders",
        "SAEs",
        "interpretability",
        "instability",
        "initialization seeds",
        "model-internal features",
        "out-of-distribution",
        "OOD",
        "Fake Features",
        "SAE probing task",
        "synthetic dataset"
      ]
    },
    "publishedAt": "2025-06-21T06:18:25.000Z",
    "title": "FaithfulSAE: Towards Capturing Faithful Features with Sparse\n  Autoencoders without External Dataset Dependencies",
    "summary": "Sparse Autoencoders (SAEs) have emerged as a promising solution for\ndecomposing large language model representations into interpretable features.\nHowever, Paulo and Belrose (2025) have highlighted instability across different\ninitialization seeds, and Heap et al. (2025) have pointed out that SAEs may not\ncapture model-internal features. These problems likely stem from training SAEs\non external datasets - either collected from the Web or generated by another\nmodel - which may contain out-of-distribution (OOD) data beyond the model's\ngeneralisation capabilities. This can result in hallucinated SAE features,\nwhich we term \"Fake Features\", that misrepresent the model's internal\nactivations. To address these issues, we propose FaithfulSAE, a method that\ntrains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we\ndemonstrate that training SAEs on less-OOD instruction datasets results in SAEs\nbeing more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained\non web-based datasets in the SAE probing task and exhibit a lower Fake Feature\nRatio in 5 out of 7 models. Overall, our approach eliminates the dependency on\nexternal datasets, advancing interpretability by better capturing\nmodel-internal features while highlighting the often neglected importance of\nSAE training datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17673.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64105805928400b416439f10",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
      "fullname": "Seonglae Cho",
      "name": "seonglae",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16929",
      "authors": [
        {
          "_id": "6858b7c0c0c8e29df8ea3c29",
          "name": "Mohon Raihan",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2a",
          "name": "Plabon Kumar Saha",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2b",
          "user": {
            "_id": "67a3002c637d195f3c4bf371",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
            "isPro": false,
            "fullname": "Rajan Das Gupta",
            "user": "rajandasgupta",
            "type": "user"
          },
          "name": "Rajan Das Gupta",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-23T08:15:03.417Z",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2c",
          "name": "A Z M Tahmidul Kabir",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2d",
          "name": "Afia Anjum Tamanna",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2e",
          "name": "Md. Harun-Ur-Rashid",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c2f",
          "name": "Adnan Bin Abdus Salam",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c30",
          "name": "Md Tanvir Anjum",
          "hidden": false
        },
        {
          "_id": "6858b7c0c0c8e29df8ea3c31",
          "name": "A Z M Ahteshamul Kabir",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Kx6P69oj873BKEjH3euNc.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/0S6fvcxnAdj4DiU-NEhtz.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/EBuiK2wVbSXnLVWmxDiZ5.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Id__vCzRydC1Bmxv-j1W0.png"
      ],
      "publishedAt": "2025-06-20T11:44:48.000Z",
      "submittedOnDailyAt": "2025-06-24T08:29:18.461Z",
      "title": "Approach to Deep Learning and Machine Learning for Neonatal Infant Mortality Prediction",
      "submittedOnDailyBy": {
        "_id": "67a3002c637d195f3c4bf371",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
        "isPro": false,
        "fullname": "Rajan Das Gupta",
        "user": "rajandasgupta",
        "type": "user"
      },
      "summary": "El fallecimiento de recién nacidos sigue siendo una realidad preocupante en países en desarrollo y incluso en algunos países desarrollados. Según los datos globales de Macro Trades, 26.693 de cada 1,000 recién nacidos mueren. Para reducir esta tasa, es crucial predecir a los niños peligrosos temprano. Esta predicción proporciona a los niños y las madres la oportunidad de ser revisados suficientemente para evitar la muerte prematura. En este contexto, se utiliza aprendizaje automático para determinar si un recién nacido está en riesgo. Se entrenó un modelo de predicción utilizando historiales de 1.4 millones de recién nacidos. Se identificaron modelos que utilizan técnicas de aprendizaje automático y aprendizaje profundo, como regresión logística, vecinos cercanos K, clasificador de árboles aleatorios, XGBoost (Extreme Gradient Boosting), red neuronal convolucional (CNN) y memoria temporal larga (LSTM). Los algoritmos de aprendizaje automático, XGBoost y el clasificador de árboles aleatorios alcanzaron una precisión del 94%, mientras que el modelo de aprendizaje profundo, LSTM, alcanzó una precisión del 99%. Por lo tanto, se puede afirmar que el LSTM es la mejor opción para predecir si un niño necesita medidas preventivas.",
      "upvotes": 1,
      "discussionId": "6858b7c1c0c8e29df8ea3c32",
      "ai_summary": "Deep learning, specifically LSTM, outperforms other machine learning techniques in predicting neonatal mortality using historical data.",
      "ai_keywords": [
        "logical regression",
        "K-nearest neighbor",
        "random forest classifier",
        "extreme gradient boosting (XGBoost)",
        "convolutional neural network",
        "long short-term memory (LSTM)"
      ]
    },
    "publishedAt": "2025-06-20T07:44:48.000Z",
    "title": "A deep learning and machine learning approach to predict neonatal death\n  in the context of São Paulo",
    "summary": "Neonatal death is still a concerning reality for underdeveloped and even some\ndeveloped countries. Worldwide data indicate that 26.693 babies out of 1,000\nbirths die, according to Macro Trades. To reduce this number, early prediction\nof endangered babies is crucial. Such prediction enables the opportunity to\ntake ample care of the child and mother so that early child death can be\navoided. In this context, machine learning was used to determine whether a\nnewborn baby is at risk. To train the predictive model, historical data of 1.4\nmillion newborns was used. Machine learning and deep learning techniques such\nas logical regression, K-nearest neighbor, random forest classifier, extreme\ngradient boosting (XGBoost), convolutional neural network, and long short-term\nmemory (LSTM) were implemented using the dataset to identify the most accurate\nmodel for predicting neonatal mortality. Among the machine learning algorithms,\nXGBoost and random forest classifier achieved the best accuracy with 94%, while\namong the deep learning models, LSTM delivered the highest accuracy with 99%.\nTherefore, using LSTM appears to be the most suitable approach to predict\nwhether precautionary measures for a child are necessary.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Kx6P69oj873BKEjH3euNc.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/0S6fvcxnAdj4DiU-NEhtz.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/EBuiK2wVbSXnLVWmxDiZ5.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Id__vCzRydC1Bmxv-j1W0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16929.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a3002c637d195f3c4bf371",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
      "fullname": "Rajan Das Gupta",
      "name": "rajandasgupta",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.17323",
      "authors": [
        {
          "_id": "685a6c720e4ad7e2197586f2",
          "name": "Tamas Bisztray",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f3",
          "user": {
            "_id": "64d3db80aea0ccb1b4975d95",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
            "isPro": false,
            "fullname": "Bilel Cherif",
            "user": "Neo111x",
            "type": "user"
          },
          "name": "Bilel Cherif",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T09:32:57.540Z",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f4",
          "name": "Richard A. Dubniczky",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f5",
          "name": "Nils Gruschka",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f6",
          "name": "Bertalan Borsos",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f7",
          "name": "Mohamed Amine Ferrag",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f8",
          "name": "Attila Kovacs",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586f9",
          "name": "Vasileios Mavroeidis",
          "hidden": false
        },
        {
          "_id": "685a6c720e4ad7e2197586fa",
          "name": "Norbert Tihanyi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T19:49:41.000Z",
      "submittedOnDailyAt": "2025-06-24T07:45:13.848Z",
      "title": "Esta entrada mantiene el vocabulario profesional y la precisión, y devuelve el resultado de la traducción directa:\n\n「Soy consciente de que un LLM conocido a alguien el último día de verano: Código generador de LLM y identificación de atributos de los generadores de LLM mediante estructuras de identificación de estilo」",
      "submittedOnDailyBy": {
        "_id": "64d3db80aea0ccb1b4975d95",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
        "isPro": false,
        "fullname": "Bilel Cherif",
        "user": "Neo111x",
        "type": "user"
      },
      "summary": "La detección de código AI, deepfakes y otros contenidos sintéticos es un nuevo campo de investigación emergente. Cuando el código generado por modelos de lenguaje grande (LLM) se vuelve más generalizado, se ha demostrado que es crucial identificar el modelo específico de cada muestra. En este artículo, se presenta una investigación sistemática sobre la identificación de autoría en programación en lenguaje C utilizando modelos de LLM. Se publica un nuevo modelo llamado CodeT5-Authorship. Este modelo utiliza solo la unidad de encoder de la arquitectura encoder-decoder de CodeT5, eliminando el decoder y centrandose en la clasificación. El output del encoder del modelo (el primer token) es procesado por un cabezal de clasificación de dos capas que incluyen la función de activación GELU y dropout, generando una distribución de probabilidades para los posibles autores. Para evaluar el modelo, se utilizan 32,000 programas generados por los 8 LLM más cortos. Se compara el modelo con 7 clasificadores de ML tradicionales y 8 modelos transformadores fine-tunados (BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3, Longformer, LoRA-fine-tuned Qwen2-1.5B). En la clasificación binaria, se alcanzó una precisión del 97.56% para distinguir entre códigos de programación C generados por modelos similares (GPT-4.1 y GPT-4o). En la identificación multi-clase, se alcanzó una precisión del 95.40% para los 5 LLM más cortos (Gemini 2.5 Flash, Claude 3.5 Haiku, GPT-4.1, Llama 3.3, DeepSeek-V3). Para apoyar la ciencia abierta, se publican en GitHub todos los scripts de Google Colab relacionados, incluyendo la arquitectura de CodeT5-Authorship, el benchmark LLM-AuthorBench, y todos los scripts de Google Colab relacionados.",
      "upvotes": 1,
      "discussionId": "685a6c720e4ad7e2197586fb",
      "projectPage": "https://github.com/LLMauthorbench",
      "githubRepo": "https://github.com/LLMauthorbench/LLMauthorbench",
      "ai_summary": "A novel model, CodeT5-Authorship, is introduced to classify the authorship of C programs generated by Large Language Models, achieving high accuracy compared to traditional and transformer-based classifiers.",
      "ai_keywords": [
        "Large Language Models",
        "LLM authorship attribution",
        "CodeT5-Authorship",
        "encoder-decoder architecture",
        "GELU activation",
        "dropout",
        "LLM-AuthorBench",
        "traditional ML classifiers",
        "BERT",
        "RoBERTa",
        "CodeBERT",
        "ModernBERT",
        "DistilBERT",
        "DeBERTa-V3",
        "Longformer",
        "LoRA",
        "Qwen2-1.5B",
        "binary classification",
        "multi-class attribution"
      ]
    },
    "publishedAt": "2025-06-18T15:49:41.000Z",
    "title": "I Know Which LLM Wrote Your Code Last Summer: LLM generated Code\n  Stylometry for Authorship Attribution",
    "summary": "Detecting AI-generated code, deepfakes, and other synthetic content is an\nemerging research challenge. As code generated by Large Language Models (LLMs)\nbecomes more common, identifying the specific model behind each sample is\nincreasingly important. This paper presents the first systematic study of LLM\nauthorship attribution for C programs. We released CodeT5-Authorship, a novel\nmodel that uses only the encoder layers from the original CodeT5\nencoder-decoder architecture, discarding the decoder to focus on\nclassification. Our model's encoder output (first token) is passed through a\ntwo-layer classification head with GELU activation and dropout, producing a\nprobability distribution over possible authors. To evaluate our approach, we\nintroduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs\ngenerated by eight state-of-the-art LLMs across diverse tasks. We compare our\nmodel to seven traditional ML classifiers and eight fine-tuned transformer\nmodels, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,\nLongformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model\nachieves 97.56% accuracy in distinguishing C programs generated by closely\nrelated models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class\nattribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,\nGPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the\nCodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant\nGoogle Colab scripts on GitHub: https://github.com/LLMauthorbench/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17323.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d3db80aea0ccb1b4975d95",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
      "fullname": "Bilel Cherif",
      "name": "Neo111x",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16507",
      "authors": [
        {
          "_id": "685a752f0e4ad7e21975870a",
          "name": "Pragya Srivastava",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870b",
          "name": "Harman Singh",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870c",
          "name": "Rahul Madhavan",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870d",
          "name": "Gandharv Patil",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870e",
          "name": "Sravanti Addepalli",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e21975870f",
          "name": "Arun Suggala",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758710",
          "name": "Rengarajan Aravamudhan",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758711",
          "name": "Soumya Sharma",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758712",
          "name": "Anirban Laha",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758713",
          "name": "Aravindan Raghuveer",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758714",
          "name": "Karthikeyan Shanmugam",
          "hidden": false
        },
        {
          "_id": "685a752f0e4ad7e219758715",
          "name": "Doina Precup",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T17:59:47.000Z",
      "submittedOnDailyAt": "2025-06-24T08:24:29.234Z",
      "title": "Aplicamos las reglas de caucus para modelar un compensación sólida.",
      "submittedOnDailyBy": {
        "_id": "639ccab166106be1436e1640",
        "avatarUrl": "/avatars/1e3806e18ac427be20e93e5400f153d4.svg",
        "isPro": false,
        "fullname": "Pragya Srivastava",
        "user": "pragsri8",
        "type": "user"
      },
      "summary": "Los modelos de recompensa (RMs) son elementos esenciales para ajustar los modelos de lenguaje de gran escala (LLMs) basándose en las reacciones humanas, pero suelen enfrentar dificultades debido a la hacking de recompensas. Estos modelos dependen, por ejemplo, de la longitud o el formato de la respuesta, y a veces se basan en características superficiales o inciertas, lo que les hace mal interpretar la verdadera causa de la consulta (como la verdad o la relevancia). Esto hace que los funciones de entrenamiento estándar no puedan diferenciar estas características, lo que resulta en RMs vulnerables y políticas inadecuadas. Presentamos un nuevo marco de trabajo llamado Crome (Modelado de Recompensas Efectivos) para mitigar el hacking de recompensas. Crome se basa en modelos causales explícitos y utiliza en el entrenamiento una amplia gama de aumentaciones sintéticas: 1) Aumentaciones causales crean pares basados en características causales específicas, forzando la sensibilidad individual de cada característica causal. 2) Aumentaciones neutras crean pares principalmente conectando resultados a través de características inciertas, forzando variaciones en estas características. En particular, nuestras aumentaciones generan respuestas en un intervalo basado en reglas causales, sin necesidad de conocer el conocimiento sobre causas inciertas. Experimentalmente, Crome supera significativamente a los límites de base de los benchmarks de RewardBench, aumentando la precisión promedio en un 5.4%, y alcanzando efectos de 13.2% y 7.2% en categorías específicas. La robustez de Crome se mantiene consistente en diferentes benchmarks, incluyendo RewardBench (comentarios, comentarios-hard, seguridad, razonamiento), WildGuardTest (enfocada en seguridad) y GSM8k (enfocado en razonamiento), incluso en configuraciones de inferencia \"Best-of-N\" donde se incrementa el número de N.",
      "upvotes": 0,
      "discussionId": "685a752f0e4ad7e219758716",
      "ai_summary": "Crome, a novel reward modeling framework using causal and neutral augmentations, significantly improves the robustness and accuracy of reward models against reward hacking.",
      "ai_keywords": [
        "Reward models",
        "Large Language Models",
        "reward hacking",
        "causal model",
        "causal augmentations",
        "neutral augmentations",
        "answer interventions",
        "oracle LLM",
        "RewardBench",
        "WildGuardTest",
        "GSM8k",
        "Best-of-N inference"
      ]
    },
    "publishedAt": "2025-06-19T13:59:47.000Z",
    "title": "Robust Reward Modeling via Causal Rubrics",
    "summary": "Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)\nvia human feedback, yet they often suffer from reward hacking. They tend to\nlatch on to superficial or spurious attributes, such as response length or\nformatting, mistaking these cues learned from correlations in training data for\nthe true causal drivers of quality (e.g., factuality, relevance). This occurs\nbecause standard training objectives struggle to disentangle these factors,\nleading to brittle RMs and misaligned policies. We introduce Crome (Causally\nRobust Reward Modeling), a novel framework grounded in an explicit causal model\ndesigned to mitigate reward hacking. Crome employs the following synthetic\ntargeted augmentations during training: (1) Causal Augmentations, which are\npairs that differ along specific causal attributes, to enforce sensitivity\nalong each causal attribute individually, and (2) Neutral Augmentations, which\nare tie-label pairs varying primarily in spurious attributes, to enforce\ninvariance along spurious attributes. Notably, our augmentations are produced\nwithout any knowledge of spurious factors, via answer interventions only along\ncausal rubrics, that are identified by querying an oracle LLM. Empirically,\nCrome significantly outperforms standard baselines on RewardBench, improving\naverage accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in\nspecific categories. The robustness of Crome is further testified by the\nconsistent gains obtained in a Best-of-N inference setting across increasing N,\nacross various benchmarks, including the popular RewardBench (covering chat,\nchat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and\nthe reasoning-specific GSM8k.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639ccab166106be1436e1640",
      "avatarUrl": "/avatars/1e3806e18ac427be20e93e5400f153d4.svg",
      "fullname": "Pragya Srivastava",
      "name": "pragsri8",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10597",
      "authors": [
        {
          "_id": "685a0fb40e4ad7e219758522",
          "name": "Xunguang Wang",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758523",
          "name": "Zhenlan Ji",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758524",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758525",
          "name": "Zongjie Li",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758526",
          "name": "Daoyuan Wu",
          "hidden": false
        },
        {
          "_id": "685a0fb40e4ad7e219758527",
          "name": "Shuai Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T11:42:40.000Z",
      "submittedOnDailyAt": "2025-06-24T01:16:03.239Z",
      "title": "SoK: Evaluación de la Guarda de Brazos de Modelos de Lenguaje de Gran Escala",
      "submittedOnDailyBy": {
        "_id": "6601853162471e0981261241",
        "avatarUrl": "/avatars/ccd1c5ce9d2f6fe7c2aff80fd9c39270.svg",
        "isPro": false,
        "fullname": "XunguangWang",
        "user": "xunguangwang",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) han logrado una sorprendente transición, pero su introducción ha revelado importantes vulnerabilidades en las instituciones seguras, especialmente en el ataque con palanca de freno. En respuesta a este ataque, se ha introducido la estructura de defensa externa llamada \"Guardrail\". Sin embargo, el estado actual de los Guardrail es desarticulado y carece de una taxonomía uniforme y un marco de evaluación completo. Este artículo de sistematización de conocimientos (SoK) realiza el primer análisis de seguridad hardware sobre los ataques con palanca de freno en los LLMs. Proponemos una nueva y diversa taxonomía, y objetivamos evaluar eficacias prácticas mediante marcos de evaluación de seguridad, eficiencia y utilidad. Mediante un análisis riguroso y experimentos, revelamos las fortalezas y limitaciones de los métodos de acceso actuales a los Guardrail, investigamos la generalización en una amplia gama de tipos de ataques y proporcionamos retroalimentación para la optimización de combinaciones de defensa. Nuestro estudio proporciona una base estructurada para futuras investigaciones y desarrollos, y se centra en guiar el avance teórico y la introducción de fuertes Guardrail de LLMs. El código está disponible en https://github.com/xunguangwang/SoK4JailbreakGuardrails.",
      "upvotes": 0,
      "discussionId": "685a0fb40e4ad7e219758533",
      "ai_summary": "A systematic analysis and evaluation framework for jailbreak guardrails in Large Language Models is presented, categorizing and assessing their effectiveness and optimization potential.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "jailbreak attacks",
        "guardrails",
        "Security-Efficiency-Utility framework",
        "multi-dimensional taxonomy"
      ]
    },
    "publishedAt": "2025-06-12T07:42:40.000Z",
    "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models",
    "summary": "Large Language Models (LLMs) have achieved remarkable progress, but their\ndeployment has exposed critical vulnerabilities, particularly to jailbreak\nattacks that circumvent safety mechanisms. Guardrails--external defense\nmechanisms that monitor and control LLM interaction--have emerged as a\npromising solution. However, the current landscape of LLM guardrails is\nfragmented, lacking a unified taxonomy and comprehensive evaluation framework.\nIn this Systematization of Knowledge (SoK) paper, we present the first holistic\nanalysis of jailbreak guardrails for LLMs. We propose a novel,\nmulti-dimensional taxonomy that categorizes guardrails along six key\ndimensions, and introduce a Security-Efficiency-Utility evaluation framework to\nassess their practical effectiveness. Through extensive analysis and\nexperiments, we identify the strengths and limitations of existing guardrail\napproaches, explore their universality across attack types, and provide\ninsights into optimizing defense combinations. Our work offers a structured\nfoundation for future research and development, aiming to guide the principled\nadvancement and deployment of robust LLM guardrails. The code is available at\nhttps://github.com/xunguangwang/SoK4JailbreakGuardrails.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10597.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601853162471e0981261241",
      "avatarUrl": "/avatars/ccd1c5ce9d2f6fe7c2aff80fd9c39270.svg",
      "fullname": "XunguangWang",
      "name": "xunguangwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]