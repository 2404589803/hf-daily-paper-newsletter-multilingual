[
  {
    "paper": {
      "id": "2503.21776",
      "authors": [
        {
          "_id": "67e6090248742d6df75853ae",
          "user": {
            "_id": "67079840a9bcb7459b8d2a46",
            "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
            "isPro": false,
            "fullname": "Kaituo Feng",
            "user": "KaituoFeng",
            "type": "user"
          },
          "name": "Kaituo Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:40:54.494Z",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853af",
          "user": {
            "_id": "642e427f6748dd4f8eeb2f38",
            "avatarUrl": "/avatars/07158ff6aa1803c846403594c5d55a34.svg",
            "isPro": false,
            "fullname": "Kaixiong Gong",
            "user": "kxgong",
            "type": "user"
          },
          "name": "Kaixiong Gong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:41:00.671Z",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b0",
          "user": {
            "_id": "6310b7e70a43f97f6c56191e",
            "avatarUrl": "/avatars/4a24c76e34d12c3d6230a4a081115f72.svg",
            "isPro": false,
            "fullname": "Bohao Li",
            "user": "BreakLee",
            "type": "user"
          },
          "name": "Bohao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:22.901Z",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b1",
          "user": {
            "_id": "6491af36c1741666238f3bff",
            "avatarUrl": "/avatars/0ee7d2ec1566e2cc5e8f144140e17f00.svg",
            "isPro": false,
            "fullname": "Zonghao Guo",
            "user": "guozonghao96",
            "type": "user"
          },
          "name": "Zonghao Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:41:07.167Z",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b2",
          "name": "Yibing Wang",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b3",
          "user": {
            "_id": "6538dd471ad9b3ba7c2df861",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6538dd471ad9b3ba7c2df861/MbEa7KHAK6u7PRb7WiPUC.jpeg",
            "isPro": false,
            "fullname": "Tianshuo Peng",
            "user": "Potentialts",
            "type": "user"
          },
          "name": "Tianshuo Peng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:41:29.644Z",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b4",
          "user": {
            "_id": "637c6703ca8542a0ba900ccb",
            "avatarUrl": "/avatars/288ed63a1efa566c3f01e850c6ba5dd5.svg",
            "isPro": false,
            "fullname": "Wang",
            "user": "Benyou",
            "type": "user"
          },
          "name": "Benyou Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:41:41.619Z",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b5",
          "user": {
            "_id": "666a8f24e2990b0cb16b7bf9",
            "avatarUrl": "/avatars/fcbaf8f1e3e53a2a4a819b7cb2c53aa4.svg",
            "isPro": false,
            "fullname": "Xiangyu Yue",
            "user": "xyyue",
            "type": "user"
          },
          "name": "Xiangyu Yue",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:41:48.267Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:59:51.000Z",
      "submittedOnDailyAt": "2025-03-28T01:02:30.945Z",
      "title": "Video-R1: Estudio de fortalecer la teoría de la razón en la reconocimiento de videos a través de MLLM",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "DeepSeek-R1 ha demostrado éxito al desarrollar capacidades de inferencia a través del aprendizaje por refuerzo basado en reglas (RL). En este contexto, presentamos Video-R1, un esfuerzo para impulsar la inferencia en modelos de lenguaje grandes multimodales (MLLMs) mediante el uso sistemático de patrones en R1. Sin embargo, al aplicar el algoritmo GRPO de aprendizaje por refuerzo directamente a la inferencia de videos, se encuentran dos desafíos principales: (i) la insuficiencia en la modelización temporal de la inferencia de videos, y (ii) la raridad de datos de inferencia de video de alta calidad. Para resolver estos problemas, proponemos primero el algoritmo T-GRPO, lo que conduce al modelo a utilizar la información temporal del video para realizar inferencias. Además, evitamos depender simplemente de datos de video, incluyendo en el proceso de entrenamiento datos de inferencia de imagen de alta calidad. Hemos construido dos datasets: Video-R1-COT-165k (para inicio de entrenamiento de SFT) y Video-R1-260k (para entrenamiento de RL). Ambos datasets incluyen datos de imagen y video. Los resultados de los experimentos muestran que Video-R1 ha mejorado significativamente en los estándares de inferencia de video como VideoMMMU y VSI-Bench, así como en los estándares generales de video MVBench y TempCompass. En particular, Video-R1-7B ha alcanzado una precisión del 35.8% en los estándares de inferencia espacial de video de VSI-Bench, superando al modelo comercial GPT-4o. Todo el código, modelos y datos están disponibles.",
      "upvotes": 42,
      "discussionId": "67e6090348742d6df75853de",
      "ai_keywords": [
        "rule-based reinforcement learning (RL)",
        "GRPO algorithm",
        "temporal modeling",
        "Video-R1-COT-165k",
        "Video-R1-260k",
        "SFT cold start",
        "VideoMMMU",
        "VSI-Bench",
        "MVBench",
        "TempCompass",
        "video spatial reasoning",
        "GPT-4o",
        "T-GRPO algorithm"
      ]
    },
    "publishedAt": "2025-03-27T13:59:51.000Z",
    "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
    "summary": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for eliciting video reasoning\nwithin multimodal large language models (MLLMs). However, directly applying RL\ntraining with the GRPO algorithm to video reasoning presents two primary\nchallenges: (i) a lack of temporal modeling for video reasoning, and (ii) the\nscarcity of high-quality video-reasoning data. To address these issues, we\nfirst propose the T-GRPO algorithm, which encourages models to utilize temporal\ninformation in videos for reasoning. Additionally, instead of relying solely on\nvideo data, we incorporate high-quality image-reasoning data into the training\nprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold start\nand Video-R1-260k for RL training, both comprising image and video data.\nExperimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncodes, models, data are released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21776.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21620",
      "authors": [
        {
          "_id": "67e606fb6c44ab0376a498a1",
          "user": {
            "_id": "676127cf11b19ea602bb202a",
            "avatarUrl": "/avatars/dfd802a24bd63e509728159ebb1769f6.svg",
            "isPro": false,
            "fullname": "Zhengxi Lu",
            "user": "LZXzju",
            "type": "user"
          },
          "name": "Zhengxi Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:42:01.155Z",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a2",
          "user": {
            "_id": "6458ce236fa580137af5aa95",
            "avatarUrl": "/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg",
            "isPro": false,
            "fullname": "Yuxiang Chai",
            "user": "Yuxiang007",
            "type": "user"
          },
          "name": "Yuxiang Chai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:39.852Z",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a3",
          "user": {
            "_id": "65c237220c57a7141888363e",
            "avatarUrl": "/avatars/ce43c52f47d524c5b747523058946325.svg",
            "isPro": false,
            "fullname": "guoyaxuan",
            "user": "guoyaxuan0106",
            "type": "user"
          },
          "name": "Yaxuan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:42:12.269Z",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a4",
          "name": "Xi Yin",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a5",
          "name": "Liang Liu",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a6",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a7",
          "name": "Guanjing Xiong",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a8",
          "user": {
            "_id": "65c04e9c27a5fdca81abcbd9",
            "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
            "isPro": false,
            "fullname": "Hongsheng LI",
            "user": "hsli-cuhk",
            "type": "user"
          },
          "name": "Hongsheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:43:30.298Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T15:39:30.000Z",
      "submittedOnDailyAt": "2025-03-28T00:48:51.950Z",
      "title": "Predicción del comportamiento de agentes gráficos mediante aprendizaje por refuerzo",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Recientemente, DeepSeek-R1 ha desarrollado habilidades lógicas utilizando aprendizaje reforzado (RL) basado en reglas. Según esta idea, investigamos si el aprendizaje reforzado basado en reglas puede mejorar las capacidades lógicas de grandes modelos de lenguaje multimodal (MLLMs) para la predicción de acciones en interfaces gráficas (GUI). Para esto, construimos un pequeño y altamente calificado dataset de 136 tareas difíciles que incluyen 5 tipos de acciones comunes. Además, introdujimos una serie de recompensas basadas en reglas para modelos basados en políticas, como GRPO, para optimizar los modelos. Los resultados de nuestros experimentos muestran que nuestro modelo eficiente en datos, UI-R1-3B, mejoró significativamente tanto dentro como fuera del dataset. En particular, la precisión de tipos de acciones en el benchmark ID AndroidControl aumentó en más de 15%. Comparado con el modelo básico (Qwen2.5-VL-3B), nuestro modelo mejoró significativamente tanto dentro como fuera del dataset. En resumen, DeepSeek-R1 ha demostrado un avance significativo en la mejora de habilidades lógicas y la eficiencia en datos para MLLMs en tareas de acciones en GUI.",
      "upvotes": 27,
      "discussionId": "67e606fe6c44ab0376a49962",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "multimodal large language models (MLLMs)",
        "graphical user interface (GUI) action prediction tasks",
        "rule-based action reward",
        "Group Relative Policy Optimization (GRPO)",
        "in-domain (ID) tasks",
        "out-of-domain (OOD) tasks",
        "action type accuracy",
        "grounding accuracy",
        "supervised fine-tuning (SFT)",
        "GUI grounding benchmark ScreenSpot-Pro",
        "OS-Atlas-7B"
      ]
    },
    "publishedAt": "2025-03-27T11:39:30.000Z",
    "title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning",
    "summary": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Building\non this idea, we are the first to explore how rule-based RL can enhance the\nreasoning capabilities of multimodal large language models (MLLMs) for graphic\nuser interface (GUI) action prediction tasks. To this end, we curate a small\nyet high-quality dataset of 136 challenging tasks, encompassing five common\naction types on mobile devices. We also introduce a unified rule-based action\nreward, enabling model optimization via policy-based algorithms such as Group\nRelative Policy Optimization (GRPO). Experimental results demonstrate that our\nproposed data-efficient model, UI-R1-3B, achieves substantial improvements on\nboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID\nbenchmark AndroidControl, the action type accuracy improves by 15%, while\ngrounding accuracy increases by 10.3%, compared with the base model (i.e.\nQwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model\nsurpasses the base model by 6.0% and achieves competitive performance with\nlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning\n(SFT) on 76K data. These results underscore the potential of rule-based\nreinforcement learning to advance GUI understanding and control, paving the way\nfor future research in this domain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21620.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21380",
      "authors": [
        {
          "_id": "67e5f4ad147ee85622ad0df1",
          "user": {
            "_id": "65df408822d66a997b4d5f6e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65df408822d66a997b4d5f6e/poROuCSvB39NZSiLzxLZf.jpeg",
            "isPro": false,
            "fullname": "Haoxiang Sun",
            "user": "CoderBak",
            "type": "user"
          },
          "name": "Haoxiang Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:46:46.418Z",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df2",
          "user": {
            "_id": "6703ac76ea890f0ca5b225eb",
            "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
            "isPro": false,
            "fullname": "Yingqian Min",
            "user": "EliverQ",
            "type": "user"
          },
          "name": "Yingqian Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:38:03.334Z",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df3",
          "user": {
            "_id": "629b765ce1af194c641fcbc6",
            "avatarUrl": "/avatars/7c53a4c2a1e528c19641a2b601731754.svg",
            "isPro": false,
            "fullname": "Zhipeng Chen",
            "user": "TimothyCzp",
            "type": "user"
          },
          "name": "Zhipeng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:38:01.177Z",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df4",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df5",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df6",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df7",
          "name": "Lei Fang",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df8",
          "user": {
            "_id": "64b8c89052b7353d8c6a1013",
            "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
            "isPro": false,
            "fullname": "Ji-Rong Wen",
            "user": "jrwen",
            "type": "user"
          },
          "name": "Ji-Rong Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:47:20.525Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T11:20:17.000Z",
      "submittedOnDailyAt": "2025-03-28T01:15:16.769Z",
      "title": "El motivo de enfrentar los límites: el desafío hacia un marco de referencia de matemáticas de nivel olímpico.",
      "submittedOnDailyBy": {
        "_id": "648e6a4567aa8ab0e0e4c30f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648e6a4567aa8ab0e0e4c30f/b9OsE6C0a5iJE1yAlFG-_.jpeg",
        "isPro": false,
        "fullname": "Beichen Zhang",
        "user": "ToheartZhang",
        "type": "user"
      },
      "summary": "Recientemente, el rápido desarrollo de grandes modelos de inferencia matemática ha llevado a que los actuales marcos de referencia para evaluar estas capacidades se han agotado y han aumentado la necesidad de un marco de evaluación más estricto. En respuesta a este vacío, se presenta el Olimpiada Matemática, un marco de referencia de nivel olímpico para evaluar la capacidad de inferencia matemática de los modelos de lenguaje. Este marco de referencia ha sido diseñado para evaluar de manera estricta la capacidad de inferencia compleja de los modelos de lenguaje. El Olimpiada Matemática se caracteriza por 200 problemas seleccionados con rigor, cada uno de los cuales ha sido revisado manualmente y está disponible en inglés y chino. Los problemas se dividen en dos niveles de dificultad: (1) problemas de nivel AIME (fáciles) y (2) problemas más difíciles (difíciles). El primer nivel es diseñado para establecer los estándares para evaluar la inferencia matemática, mientras que el segundo nivel es diseñado para superar los límites de los modelos de máxima calidad actuales. Este marco de referencia aborda 4 áreas matemáticas clave y cada problema incluye una solución numérica provable, permitiendo una evaluación basada en reglas y evitando la influencia subjetiva. Los resultados de los experimentos han revelado importantes problemas en el Olimpiada Matemática, y los modelos más avanzados, como DeepSeek-R1 y o3-mini de OpenAI, han demostrado una alta precisión en las pruebas difíciles. Además, el marco de referencia permite una evaluación detallada de la capacidad de inferencia matemática en aspectos importantes que no son suficientemente abordados en los marcos de referencia matemáticos generales. El marco de referencia Olimpiada Matemática se ha publicado en el proyecto \"STILL\": https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
      "upvotes": 24,
      "discussionId": "67e5f4ae147ee85622ad0e27",
      "ai_keywords": [
        "DeepSeek-R1",
        "o3-mini"
      ]
    },
    "publishedAt": "2025-03-27T07:20:17.000Z",
    "title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math\n  Benchmark for Large Language Models",
    "summary": "In recent years, the rapid development of large reasoning models has resulted\nin the saturation of existing benchmarks for evaluating mathematical reasoning,\nhighlighting the urgent need for more challenging and rigorous evaluation\nframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level\nmathematical benchmark, designed to rigorously test the complex reasoning\ncapabilities of LLMs. OlymMATH features 200 meticulously curated problems, each\nmanually verified and available in parallel English and Chinese versions. The\nproblems are systematically organized into two distinct difficulty tiers: (1)\nAIME-level problems (easy) that establish a baseline for mathematical reasoning\nassessment, and (2) significantly more challenging problems (hard) designed to\npush the boundaries of current state-of-the-art models. In our benchmark, these\nproblems span four core mathematical fields, each including a verifiable\nnumerical solution to enable objective, rule-based evaluation. Empirical\nresults underscore the significant challenge presented by OlymMATH, with\nstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini\ndemonstrating notably limited accuracy on the hard subset. Furthermore, the\nbenchmark facilitates comprehensive bilingual assessment of mathematical\nreasoning abilities-a critical dimension that remains largely unaddressed in\nmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark\nat the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21380.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "648e6a4567aa8ab0e0e4c30f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648e6a4567aa8ab0e0e4c30f/b9OsE6C0a5iJE1yAlFG-_.jpeg",
      "fullname": "Beichen Zhang",
      "name": "ToheartZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21755",
      "authors": [
        {
          "_id": "67e60823284844fd3014f62b",
          "name": "Dian Zheng",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f62c",
          "user": {
            "_id": "60efe7fa0d920bc7805cada5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png",
            "isPro": false,
            "fullname": "Ziqi Huang",
            "user": "Ziqi",
            "type": "user"
          },
          "name": "Ziqi Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:48:07.968Z",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f62d",
          "user": {
            "_id": "6690dfd73bbfdee5f43ffc4d",
            "avatarUrl": "/avatars/88ff9b61663299d7751037696a75f1d7.svg",
            "isPro": false,
            "fullname": "Hongbo Liu",
            "user": "HongboLiu",
            "type": "user"
          },
          "name": "Hongbo Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:48:14.875Z",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f62e",
          "user": {
            "_id": "647993d9f966f086918da59e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647993d9f966f086918da59e/NDxz3PEpo3srZQNhwT7Qf.jpeg",
            "isPro": false,
            "fullname": "kzou",
            "user": "jackyhate",
            "type": "user"
          },
          "name": "Kai Zou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:25.760Z",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f62f",
          "user": {
            "_id": "65b9d9961fe588f824fde191",
            "avatarUrl": "/avatars/a9245958cc998a4b4b870bf2490fdaee.svg",
            "isPro": false,
            "fullname": "Yinan He",
            "user": "yinanhe",
            "type": "user"
          },
          "name": "Yinan He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:48:21.118Z",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f630",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f631",
          "name": "Yuanhan Zhang",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f632",
          "user": {
            "_id": "670749a9d827da9f37508209",
            "avatarUrl": "/avatars/f14fc05ad405f3967b9af0bcc73d4207.svg",
            "isPro": false,
            "fullname": "he jingwen",
            "user": "mimihe",
            "type": "user"
          },
          "name": "Jingwen He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:48:30.978Z",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f633",
          "name": "Wei-Shi Zheng",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f634",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f635",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:48:49.191Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:57:01.000Z",
      "submittedOnDailyAt": "2025-03-28T00:53:44.602Z",
      "title": "VBench-2.0: Desarrollo de un conjunto de marcos de referencia de video generación para la fidelidad intrínseca",
      "submittedOnDailyBy": {
        "_id": "60efe7fa0d920bc7805cada5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png",
        "isPro": false,
        "fullname": "Ziqi Huang",
        "user": "Ziqi",
        "type": "user"
      },
      "summary": "El desarrollo de la generación de vídeo ha experimentado un gran avance, transformandose en un método para crear vídeos visualmente confiables desde salidas no-realistas. Para evaluar estos modelos de generación de vídeo, se ha desarrollado marcos de referencia como VBench, que miden la precisión y evalúan aspectos como la artesanalidad de una sola frame, la coherencia secuencial y la adecuación a los prompts básicos. Sin embargo, estos marcos de referencia principalmente se centran en la precisión superficial de una sola frame, y si el vídeo es visualmente confiable, sin embargo, no se centran suficientemente en si el vídeo cumple con los principios de la realidad. Los modelos recientes han mostrado un buen desempeño en estos métricas, pero el vídeo solo sea visualmente confiable no es suficiente; existe una dificultad para sentir que el vídeo sea fundamentalmente real. Para lograr la generación de vídeos con un modelo \"mundo real\", es necesario abordar la segunda frontera: la obtención de precisión implícita. Es crucial que el vídeo generado cumpla con las leyes físicas, la lógica común, la exactitud anatómica y la coherencia estructural. Llegar a este nivel de realidad es crucial para aplicaciones como la producción de películas con IA o la modelación de mundos reales. Para cubrir este gap, se presenta VBench-2.0, la próxima generación de marcos de referencia. VBench-2.0 está diseñado para evaluar de manera automática la precisión implícita de modelos de generación de vídeo. Evalúa 5 principales indicadores: Human Fidelity, Controllability, Creativity, Physics, Commonsense, y mejora la capacidad de cada uno. El marco de evaluación diseñado para cada indicador combina a los expertos en VLMs y LLMs, especialistas en géneros y métodos de detección de anomalías en vídeos. Para asegurar la consistencia con la percepción humana, se realizan notas ampliadas. VBench-2.0, que evoluciona la precisión superficial hacia la precisión implícita, busca establecer nuevas normas para el desarrollo de los modelos de generación de vídeo de las próximas generaciones.",
      "upvotes": 21,
      "discussionId": "67e60824284844fd3014f68e",
      "ai_keywords": [
        "VBench",
        "VBench-2.0",
        "visual generation",
        "per-frame aesthetics",
        "temporal consistency",
        "prompt adherence",
        "intrinsic faithfulness",
        "physical laws",
        "commonsense reasoning",
        "anatomical correctness",
        "compositional integrity",
        "AI-assisted filmmaking",
        "simulated world modeling",
        "VLMs",
        "LLMs",
        "anomaly detection"
      ]
    },
    "publishedAt": "2025-03-27T13:57:01.000Z",
    "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness",
    "summary": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21755.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60efe7fa0d920bc7805cada5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png",
      "fullname": "Ziqi Huang",
      "name": "Ziqi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21749",
      "authors": [
        {
          "_id": "67e6041d9a97e46f3102f7cc",
          "user": {
            "_id": "62c66504031996c36c86976a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
            "isPro": true,
            "fullname": "steve z",
            "user": "stzhao",
            "type": "user"
          },
          "name": "Shitian Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:47.519Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7cd",
          "user": {
            "_id": "64379d79fac5ea753f1c10f3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png",
            "isPro": false,
            "fullname": "Jerry Wu",
            "user": "QJerry",
            "type": "user"
          },
          "name": "Qilong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:55.109Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7ce",
          "user": {
            "_id": "66aba287b0f0b7411f511a47",
            "avatarUrl": "/avatars/1450f182c38e80066ae5ea5df4fa218f.svg",
            "isPro": false,
            "fullname": "Xinyue Li",
            "user": "Xxxy13",
            "type": "user"
          },
          "name": "Xinyue Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:52.434Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7cf",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d0",
          "user": {
            "_id": "6794cd79b72b1721ea69f4f2",
            "avatarUrl": "/avatars/4e4fb9e9e127a0c031131ace705687cd.svg",
            "isPro": false,
            "fullname": "Ming Li",
            "user": "afdsafas",
            "type": "user"
          },
          "name": "Ming Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:49.525Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d1",
          "user": {
            "_id": "66bb136002fd8eb58bc84ffb",
            "avatarUrl": "/avatars/122cb8f59c502392768099b3c2afe043.svg",
            "isPro": false,
            "fullname": "qinqi",
            "user": "Dakerqi",
            "type": "user"
          },
          "name": "Qi Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:58:27.757Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d2",
          "user": {
            "_id": "646f1bef075e11ca78da3bb7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f1bef075e11ca78da3bb7/gNS-ikyZXYeMrf4a7HTQE.jpeg",
            "isPro": false,
            "fullname": "Dongyang Liu (Chris Liu)",
            "user": "Cxxs",
            "type": "user"
          },
          "name": "Dongyang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:49:23.924Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d3",
          "user": {
            "_id": "63527f4e7d071f23d085ad45",
            "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
            "isPro": false,
            "fullname": "KAIPENG ZHANG",
            "user": "kpzhang",
            "type": "user"
          },
          "name": "Kaipeng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:49:30.453Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d4",
          "user": {
            "_id": "65c04e9c27a5fdca81abcbd9",
            "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
            "isPro": false,
            "fullname": "Hongsheng LI",
            "user": "hsli-cuhk",
            "type": "user"
          },
          "name": "Hongsheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:49:37.542Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d5",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d6",
          "user": {
            "_id": "67b299cc6f6dc4376d9e6c76",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UniMpmfOUlyiSOrf47wuT.png",
            "isPro": false,
            "fullname": "Peng Gao",
            "user": "cosumosu25",
            "type": "user"
          },
          "name": "Peng Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:49:44.411Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d7",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d8",
          "user": {
            "_id": "6285a9133ab6642179158944",
            "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
            "isPro": false,
            "fullname": "Zhen Li",
            "user": "Paper99",
            "type": "user"
          },
          "name": "Zhen Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:58:29.550Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:56:15.000Z",
      "submittedOnDailyAt": "2025-03-28T00:54:19.590Z",
      "title": "LeX-Art: Sintesis de Datos de Alta Calidad Escalable para Reflexionar sobre la Generación de Texto",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "LeX-Art es uno de los sistemas de síntesis de texto-imagen de alta calidad diseñados para resolver de manera sistemática la desacercación entre la expresividad de los prompts y la precisión de la dibujación de caracteres. Nuestro enfoque se basa en un paradigma centrado en datos, construyendo un proceso de síntesis de datos de alta calidad basado en Deepseek-R1, lo que nos permite editar un conjunto de datos de 10,000 imágenes de alta resolución y estilo artístico (1024×1024). Más allá de la construcción del conjunto de datos, hemos desarrollado un potente modelo de fortalecimiento de prompts llamado LeX-Enhancer y entrenamos dos modelos de texto-imagen: LeX-FLUX y LeX-Lumina, con el objetivo de alcanzar los mejores rendimientos en la dibujación de texto. Introducimos LeX-Bench, un nuevo marco de referencia para evaluar sistemáticamente la generación visual de texto, utilizando el nuevo índice Pairwise Normalized Edit Distance (PNED) para evaluar precisión, artesanalidad e coherencia. Los resultados de nuestros experimentos muestran que LeX-Lumina logró un ganancia del 79.81% en PNED en CreateBench, y LeX-FLUX superó los estándares en colores (+3.18%), posición (+4.45%) y precisión de fuente (+3.81%). Nuestro código, modelos, conjunto de datos y demos están disponibles para uso público.",
      "upvotes": 15,
      "discussionId": "67e6041f9a97e46f3102f89b",
      "projectPage": "https://zhaoshitian.github.io/lexart/",
      "githubRepo": "https://github.com/zhaoshitian/LeX-Art/",
      "ai_keywords": [
        "Deepseek-R1",
        "LeX-Enhancer",
        "LeX-FLUX",
        "LeX-Lumina",
        "LeX-Bench",
        "Pairwise Normalized Edit Distance (PNED)"
      ]
    },
    "publishedAt": "2025-03-27T13:56:15.000Z",
    "title": "LeX-Art: Rethinking Text Generation via Scalable High-Quality Data\n  Synthesis",
    "summary": "We introduce LeX-Art, a comprehensive suite for high-quality text-image\nsynthesis that systematically bridges the gap between prompt expressiveness and\ntext rendering fidelity. Our approach follows a data-centric paradigm,\nconstructing a high-quality data synthesis pipeline based on Deepseek-R1 to\ncurate LeX-10K, a dataset of 10K high-resolution, aesthetically refined\n1024times1024 images. Beyond dataset construction, we develop LeX-Enhancer,\na robust prompt enrichment model, and train two text-to-image models, LeX-FLUX\nand LeX-Lumina, achieving state-of-the-art text rendering performance. To\nsystematically evaluate visual text generation, we introduce LeX-Bench, a\nbenchmark that assesses fidelity, aesthetics, and alignment, complemented by\nPairwise Normalized Edit Distance (PNED), a novel metric for robust text\naccuracy evaluation. Experiments demonstrate significant improvements, with\nLeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX\noutperforming baselines in color (+3.18%), positional (+4.45%), and font\naccuracy (+3.81%). Our codes, models, datasets, and demo are publicly\navailable.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21749.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21460",
      "authors": [
        {
          "_id": "67e609ee389245233f0d316f",
          "user": {
            "_id": "642da1cd99f3110ac27caca5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
            "isPro": false,
            "fullname": "junyu",
            "user": "luojunyu",
            "type": "user"
          },
          "name": "Junyu Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:10.158Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3170",
          "name": "Weizhi Zhang",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3171",
          "name": "Ye Yuan",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3172",
          "user": {
            "_id": "668388cb549c1b932c9fe699",
            "avatarUrl": "/avatars/aa7523fbde4c2a8508cff13c74291e6a.svg",
            "isPro": false,
            "fullname": "Yusheng Zhao",
            "user": "yszhao",
            "type": "user"
          },
          "name": "Yusheng Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:54:38.409Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3173",
          "name": "Junwei Yang",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3174",
          "user": {
            "_id": "6329a8ff688ad82b783b0e54",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1663674611122-noauth.png",
            "isPro": false,
            "fullname": "Yiyang Gu",
            "user": "evan-gyy",
            "type": "user"
          },
          "name": "Yiyang Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:55:04.907Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3175",
          "name": "Bohan Wu",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3176",
          "name": "Binqi Chen",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3177",
          "user": {
            "_id": "67d3e9f53c8b9f6c843aacaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/LTPxPALNDWWTGnP_K30hH.png",
            "isPro": false,
            "fullname": "Ziyue Qiao",
            "user": "joeyleo",
            "type": "user"
          },
          "name": "Ziyue Qiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:55:52.770Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3178",
          "user": {
            "_id": "648c0620c2e1388f44e2eddc",
            "avatarUrl": "/avatars/ab093add13d5eb6032e47aea356ca9f2.svg",
            "isPro": false,
            "fullname": "Qingqing Long",
            "user": "qqlong",
            "type": "user"
          },
          "name": "Qingqing Long",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:56:00.392Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3179",
          "name": "Rongcheng Tu",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317a",
          "name": "Xiao Luo",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317b",
          "name": "Wei Ju",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317c",
          "user": {
            "_id": "66ab566e30c55e83b02aa050",
            "avatarUrl": "/avatars/62692be88b9ad34ad3f474fb0359ae20.svg",
            "isPro": false,
            "fullname": "Zhiping Xiao",
            "user": "Shockzipper",
            "type": "user"
          },
          "name": "Zhiping Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:56:14.710Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317d",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317e",
          "name": "Meng Xiao",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317f",
          "name": "Chenwu Liu",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3180",
          "name": "Jingyang Yuan",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3181",
          "user": {
            "_id": "64b6d98861dc301f1326341a",
            "avatarUrl": "/avatars/14df7497a1a982894f5889903793773f.svg",
            "isPro": false,
            "fullname": "Shichang Zhang",
            "user": "shichangzh",
            "type": "user"
          },
          "name": "Shichang Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:56:59.067Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3182",
          "user": {
            "_id": "60d596784cf0297c143fcd33",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d596784cf0297c143fcd33/phknQ4Z2VuUj3akhcoxLC.png",
            "isPro": false,
            "fullname": "Yiqiao Jin",
            "user": "Ahren09",
            "type": "user"
          },
          "name": "Yiqiao Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:57:05.559Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3183",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3184",
          "name": "Xian Wu",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3185",
          "name": "Hanqing Zhao",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3186",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3187",
          "user": {
            "_id": "67a088c531bab0a2a39665d4",
            "avatarUrl": "/avatars/7188815ff8b5e4a475e7ebc09687e10d.svg",
            "isPro": false,
            "fullname": "Philip Yu",
            "user": "philipyu",
            "type": "user"
          },
          "name": "Philip S. Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:57:54.103Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3188",
          "name": "Ming Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/hXVaNIVxOX8326Ri2Px11.png",
        "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/UuH6cO5owx0fzF8WFnGq5.png"
      ],
      "publishedAt": "2025-03-27T12:50:17.000Z",
      "submittedOnDailyAt": "2025-03-28T01:08:58.527Z",
      "title": "Modelo de lenguaje de lenguaje general: métodos, aplicaciones y problemas investigados",
      "submittedOnDailyBy": {
        "_id": "642da1cd99f3110ac27caca5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
        "isPro": false,
        "fullname": "junyu",
        "user": "luojunyu",
        "type": "user"
      },
      "summary": "La era de los inteligentes agentes se acerca gracias a la innovadora evolución de los modelos de lenguaje grande (LLM). Los agentes de LLM tienen la capacidad de actuar según objetivos y adaptarse dinámicamente, lo que les hace un paso crucial hacia la inteligencia general de la inteligencia artificial. Este estudio se enfoca en la agente sistema mediante una metodología de diseño, desglosando el sistema y conectando la arquitectura básica, la estructura de colaboración y el torreón de episodios. Revelamos las relaciones fundamentales entre los principios de diseño de los agentes y los comportamientos de episodios en entornos complejos, uniendo los líneas de investigación. Nuestro trabajo proporciona una serie de visiones arquitectónicas sobre cómo se construyen los agentes, cómo colaboran y cómo evolucionan con el tiempo, abordando también el método de evaluación, la aplicación de herramientas, problemas prácticos y diversas áreas de aplicación. Esta colección investiga las últimas avances en este campo en constante expansión, proporcionando a los investigadores una metodología estructurada para los agentes de LLM y estableciendo las posibles direcciones de investigación futura. Este conjunto de trabajos está disponible en https://github.com/luo-junyu/Awesome-Agent-Papers.",
      "upvotes": 12,
      "discussionId": "67e609ef389245233f0d31c0",
      "projectPage": "https://huggingface.co/spaces/luojunyu/Agent-Papers",
      "githubRepo": "https://github.com/luo-junyu/Awesome-Agent-Papers",
      "ai_keywords": [
        "Large Language Model (LLM) agents",
        "goal-driven behaviors",
        "dynamic adaptation capabilities",
        "artificial general intelligence",
        "methodology-centered taxonomy",
        "architectural foundations",
        "collaboration mechanisms",
        "evolutionary pathways",
        "agent design principles",
        "emergent behaviors",
        "complex environments",
        "unified architectural perspective",
        "evaluation methodologies",
        "practical challenges",
        "diverse application domains"
      ]
    },
    "publishedAt": "2025-03-27T08:50:17.000Z",
    "title": "Large Language Model Agent: A Survey on Methodology, Applications and\n  Challenges",
    "summary": "The era of intelligent agents is upon us, driven by revolutionary\nadvancements in large language models. Large Language Model (LLM) agents, with\ngoal-driven behaviors and dynamic adaptation capabilities, potentially\nrepresent a critical pathway toward artificial general intelligence. This\nsurvey systematically deconstructs LLM agent systems through a\nmethodology-centered taxonomy, linking architectural foundations, collaboration\nmechanisms, and evolutionary pathways. We unify fragmented research threads by\nrevealing fundamental connections between agent design principles and their\nemergent behaviors in complex environments. Our work provides a unified\narchitectural perspective, examining how agents are constructed, how they\ncollaborate, and how they evolve over time, while also addressing evaluation\nmethodologies, tool applications, practical challenges, and diverse application\ndomains. By surveying the latest developments in this rapidly evolving field,\nwe offer researchers a structured taxonomy for understanding LLM agents and\nidentify promising directions for future research. The collection is available\nat https://github.com/luo-junyu/Awesome-Agent-Papers.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/hXVaNIVxOX8326Ri2Px11.png",
      "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/UuH6cO5owx0fzF8WFnGq5.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21460.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642da1cd99f3110ac27caca5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
      "fullname": "junyu",
      "name": "luojunyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21758",
      "authors": [
        {
          "_id": "67e6092a40fb111ac9342c39",
          "user": {
            "_id": "66bb136002fd8eb58bc84ffb",
            "avatarUrl": "/avatars/122cb8f59c502392768099b3c2afe043.svg",
            "isPro": false,
            "fullname": "qinqi",
            "user": "Dakerqi",
            "type": "user"
          },
          "name": "Qi Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:18.440Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3a",
          "user": {
            "_id": "6358a167f56b03ec9147074d",
            "avatarUrl": "/avatars/e54ea7bf0c240cf76d538296efb3976c.svg",
            "isPro": false,
            "fullname": "Le Zhuo",
            "user": "JackyZhuo",
            "type": "user"
          },
          "name": "Le Zhuo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:50:03.385Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3b",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3c",
          "user": {
            "_id": "64a54586c0f13de8e7093314",
            "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg",
            "isPro": false,
            "fullname": "Ruoyi Du",
            "user": "RuoyiDu",
            "type": "user"
          },
          "name": "Ruoyi Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:50:09.220Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3d",
          "user": {
            "_id": "6285a9133ab6642179158944",
            "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
            "isPro": false,
            "fullname": "Zhen Li",
            "user": "Paper99",
            "type": "user"
          },
          "name": "Zhen Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:58:25.783Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3e",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3f",
          "user": {
            "_id": "6467ec2f374fe5728d4216e0",
            "avatarUrl": "/avatars/4d14f64572c9c0707edb54993e331a49.svg",
            "isPro": false,
            "fullname": "Yiting Lu",
            "user": "luyiting",
            "type": "user"
          },
          "name": "Yiting Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:50:17.491Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c40",
          "user": {
            "_id": "64a3d1ddb3239f3e3892b24b",
            "avatarUrl": "/avatars/7ce585f5fc1d077fb1d70cc18c4da2c1.svg",
            "isPro": false,
            "fullname": "Jiakang Yuan",
            "user": "JiakangYuan",
            "type": "user"
          },
          "name": "Jiakang Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:50:23.000Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c41",
          "user": {
            "_id": "66aba287b0f0b7411f511a47",
            "avatarUrl": "/avatars/1450f182c38e80066ae5ea5df4fa218f.svg",
            "isPro": false,
            "fullname": "Xinyue Li",
            "user": "Xxxy13",
            "type": "user"
          },
          "name": "Xinyue Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:20.809Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c42",
          "user": {
            "_id": "646f1bef075e11ca78da3bb7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f1bef075e11ca78da3bb7/gNS-ikyZXYeMrf4a7HTQE.jpeg",
            "isPro": false,
            "fullname": "Dongyang Liu (Chris Liu)",
            "user": "Cxxs",
            "type": "user"
          },
          "name": "Dongyang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:50:37.419Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c43",
          "name": "Xiangyang Zhu",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c44",
          "name": "Manyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c45",
          "user": {
            "_id": "629aed605ab4232a3fe266f7",
            "avatarUrl": "/avatars/53e1f4a9fc2bad17b05a80b14118442e.svg",
            "isPro": false,
            "fullname": "Will Beddow",
            "user": "willbeddow",
            "type": "user"
          },
          "name": "Will Beddow",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:51:18.703Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c46",
          "user": {
            "_id": "62a2712903bf94c3ac3ae004",
            "avatarUrl": "/avatars/2f11b73ecd7a4cb561b42c676b70b7f8.svg",
            "isPro": false,
            "fullname": "Erwann Millon",
            "user": "erwann",
            "type": "user"
          },
          "name": "Erwann Millon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:51:24.290Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c47",
          "name": "Victor Perez",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c48",
          "user": {
            "_id": "64d1c560c0c627dfa71bdbe0",
            "avatarUrl": "/avatars/f42794fe25bffcd870a1bcee69b95298.svg",
            "isPro": false,
            "fullname": "wenhai.wang",
            "user": "wangwhcore",
            "type": "user"
          },
          "name": "Wenhai Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:51:42.390Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c49",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:51:48.715Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4a",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4b",
          "name": "Xiaohong Liu",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4c",
          "user": {
            "_id": "65c04e9c27a5fdca81abcbd9",
            "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
            "isPro": false,
            "fullname": "Hongsheng LI",
            "user": "hsli-cuhk",
            "type": "user"
          },
          "name": "Hongsheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:52:25.823Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4d",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4e",
          "user": {
            "_id": "6391fa34e110d51320389b06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670510944260-noauth.jpeg",
            "isPro": false,
            "fullname": "chang xu",
            "user": "changxu-2022",
            "type": "user"
          },
          "name": "Chang Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:52:36.782Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4f",
          "name": "Peng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:57:07.000Z",
      "submittedOnDailyAt": "2025-03-28T00:59:52.114Z",
      "title": "Lumina-Image 2.0: Marco de generación de imágenes integrado y eficiente",
      "submittedOnDailyBy": {
        "_id": "6285a9133ab6642179158944",
        "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
        "isPro": false,
        "fullname": "Zhen Li",
        "user": "Paper99",
        "type": "user"
      },
      "summary": "Introduzco Lumina-Image 2.0. Este ha avanzado significativamente en comparación con Lumina-Next, y es un marco de trabajo para la generación de imágenes desde textos avanzados. Lumina-Image 2.0 se basa en dos principios clave.\n\n1. **Unificación** - Utiliza una arquitectura unificada (Unified Next-DiT), trata los tokens de texto y imagen como secuencias comunes, facilitando un intercambio natural entre modalidades y permitiendo una expansión de tareas fácil y extensible. Además, puede proporcionar pares de entrenamiento de texto-imagen significativos, lo que justifica la introducción del sistema de captura integrado (Unified Captioner, UniCap) específico para la generación de imágenes a partir de texto. UniCap genera capturas detalladas y precisas, acelera la convergencia y mejora la respuesta a los prompts.\n\n2. **Eficiencia** - Para mejorar la eficiencia del modelo, se desarrolló una estrategia de entrenamiento progresiva y se introdujo tecnología de aceleración de inferencia que no daña la calidad de las imágenes. Los resultados de benchmark académicos y el desarrollo desde texto a imagen publicados muestran que Lumina-Image 2.0 muestra un excelente rendimiento incluso con 2,6 billones de parámetros, destacando su escalabilidad y eficiencia de diseño. Los detalles de entrenamiento, código y modelo están disponibles en la siguiente URL.\n\nhttps://github.com/Alpha-VLLM/Lumina-Image-2.0",
      "upvotes": 11,
      "discussionId": "67e6092e40fb111ac9342d4e",
      "projectPage": "https://github.com/Alpha-VLLM/Lumina-Image-2.0",
      "githubRepo": "https://github.com/Alpha-VLLM/Lumina-Image-2.0",
      "ai_keywords": [
        "Unified Next-DiT",
        "text and image tokens",
        "cross-modal interactions",
        "Unified Captioner (UniCap)",
        "text-to-image (T2I)",
        "multi-stage progressive training",
        "inference acceleration techniques",
        "academic benchmarks",
        "public text-to-image arenas"
      ]
    },
    "publishedAt": "2025-03-27T13:57:07.000Z",
    "title": "Lumina-Image 2.0: A Unified and Efficient Image Generative Framework",
    "summary": "We introduce Lumina-Image 2.0, an advanced text-to-image generation framework\nthat achieves significant progress compared to previous work, Lumina-Next.\nLumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts\na unified architecture (Unified Next-DiT) that treats text and image tokens as\na joint sequence, enabling natural cross-modal interactions and allowing\nseamless task expansion. Besides, since high-quality captioners can provide\nsemantically well-aligned text-image training pairs, we introduce a unified\ncaptioning system, Unified Captioner (UniCap), specifically designed for T2I\ngeneration tasks. UniCap excels at generating comprehensive and accurate\ncaptions, accelerating convergence and enhancing prompt adherence. (2)\nEfficiency - to improve the efficiency of our proposed model, we develop\nmulti-stage progressive training strategies and introduce inference\nacceleration techniques without compromising image quality. Extensive\nevaluations on academic benchmarks and public text-to-image arenas show that\nLumina-Image 2.0 delivers strong performances even with only 2.6B parameters,\nhighlighting its scalability and design efficiency. We have released our\ntraining details, code, and models at\nhttps://github.com/Alpha-VLLM/Lumina-Image-2.0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21758.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6285a9133ab6642179158944",
      "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
      "fullname": "Zhen Li",
      "name": "Paper99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21729",
      "authors": [
        {
          "_id": "67e610ee6e73232cf0903b73",
          "user": {
            "_id": "652542861e9db26e407aa1fc",
            "avatarUrl": "/avatars/4c47ef4564f498a7f34b4a17a1e209a8.svg",
            "isPro": false,
            "fullname": "Lee Zhicheng",
            "user": "ZhiCheng0326",
            "type": "user"
          },
          "name": "Zhicheng Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:06.279Z",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b74",
          "user": {
            "_id": "62c924a6334a6ee11c2e8dfa",
            "avatarUrl": "/avatars/3dbc37af162b94d68cb83665ac4528c3.svg",
            "isPro": false,
            "fullname": "ShulinCao",
            "user": "caoshulin",
            "type": "user"
          },
          "name": "Shulin Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:52:53.008Z",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b75",
          "name": "Jinxin Liu",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b76",
          "user": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "name": "Jiajie Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:53:38.290Z",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b77",
          "user": {
            "_id": "61132a7ab75d3040e6e88a3a",
            "avatarUrl": "/avatars/faf9d96770251f31e7e4edbf1fee9798.svg",
            "isPro": false,
            "fullname": "liuweichuan",
            "user": "liuweichuan",
            "type": "user"
          },
          "name": "Weichuan Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:53:49.672Z",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b78",
          "user": {
            "_id": "65d66cb2b06abf924b07ff76",
            "avatarUrl": "/avatars/de94e2fe07040b7dc3053bcaafa64ffe.svg",
            "isPro": false,
            "fullname": "Xiaoyin Chen",
            "user": "chenyn66",
            "type": "user"
          },
          "name": "Xiaoyin Che",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:53:55.317Z",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b79",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b7a",
          "user": {
            "_id": "65df8cbc2705d9672f55d1aa",
            "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg",
            "isPro": false,
            "fullname": "Juanzi Li",
            "user": "juanli",
            "type": "user"
          },
          "name": "Juanzi Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:54:01.074Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/652542861e9db26e407aa1fc/OFvhHK9WLS_twsPoDxlpU.png"
      ],
      "publishedAt": "2025-03-27T17:44:18.000Z",
      "submittedOnDailyAt": "2025-03-28T01:51:24.016Z",
      "title": "ReaRAG: Mejora de la Teoría de la Reasoning del Modelo de Guia de Conocimientos mediante la Generación Iterativa y Recursiva de Realidades Certas",
      "submittedOnDailyBy": {
        "_id": "652542861e9db26e407aa1fc",
        "avatarUrl": "/avatars/4c47ef4564f498a7f34b4a17a1e209a8.svg",
        "isPro": false,
        "fullname": "Lee Zhicheng",
        "user": "ZhiCheng0326",
        "type": "user"
      },
      "summary": "Los Grandes Modelos de Reacción (GMRs) demuestran una capacidad de inferencia sorprendente, pero dependen principalmente de conocimientos de parámetros y limitan la precisión factual. En los últimos estudios, se han añadido funciones de búsqueda basadas en aprendizaje por refuerzo (RL) a GMRs, pero estos suelen presentar un exceso de pensamiento y una insuficiencia en la robustez de la inferencia, lo que afecta la eficiencia en tareas de respuesta a preguntas (QA). En este sentido, proponemos un modelo de inferencia orientado a mejorar la precisión factual, llamado ReaRAG, que evita el exceso de iteraciones para explorar diversas consultas. Nuestra solución incluye un nuevo marco de construcción de datos que establece un límite en la longitud de la cadena de razonamiento. Específicamente, utilizamos GMRs para generar un estilo de pensamiento profundo y seleccionamos acciones en un espacio de acciones predefinido (busqueda y completar). En el caso de la acción de búsqueda, se ejecuta una consulta al motor RAG, que devuelve observaciones que guian los siguientes pasos de inferencia. Este proceso se repite hasta que se elija la acción de completar. Al aprovechar la fuerte capacidad de inferencia de ReaRAG, nuestro enfoque supera a los baselines existentes en múltiples etapas de QA. Un análisis adicional revela la fuerte capacidad de reflexión en la reconocimiento de errores y en la precisión del proceso de inferencia. Nuestro estudio mejora la precisión factual de los GMRs y integra efectivamente una expansión de búsqueda con un fuerte potencial de inferencia (RAG).",
      "upvotes": 11,
      "discussionId": "67e610ee6e73232cf0903ba6",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "parametric knowledge",
        "retrieval capabilities",
        "overthinking",
        "robustness",
        "question answering (QA)",
        "ReaRAG",
        "factuality-enhanced reasoning model",
        "reasoning chain length",
        "LRM",
        "deliberate thinking",
        "predefined action space",
        "Search",
        "Finish",
        "RAG engine",
        "observation",
        "multi-hop QA",
        "reflective ability",
        "reasoning trajectory",
        "Retrieval-Augmented Generation (RAG)"
      ]
    },
    "publishedAt": "2025-03-27T13:44:18.000Z",
    "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large\n  Reasoning Models with Iterative Retrieval Augmented Generation",
    "summary": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/652542861e9db26e407aa1fc/OFvhHK9WLS_twsPoDxlpU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21729.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652542861e9db26e407aa1fc",
      "avatarUrl": "/avatars/4c47ef4564f498a7f34b4a17a1e209a8.svg",
      "fullname": "Lee Zhicheng",
      "name": "ZhiCheng0326",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21248",
      "authors": [
        {
          "_id": "67e63581e8a16f38741f4baa",
          "user": {
            "_id": "661980b46a43b2760d8d551f",
            "avatarUrl": "/avatars/e4743d55303fe3a88688c29dd4c67a69.svg",
            "isPro": false,
            "fullname": "Yujie Liu",
            "user": "yujieliu",
            "type": "user"
          },
          "name": "Yujie Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:58:09.374Z",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bab",
          "user": {
            "_id": "646a11791556443f24b582e9",
            "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
            "isPro": false,
            "fullname": "Zonglin Yang",
            "user": "ZonglinY",
            "type": "user"
          },
          "name": "Zonglin Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:58:40.651Z",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bac",
          "name": "Tong Xie",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bad",
          "user": {
            "_id": "62a7362fd1e7a011fd4e31a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a7362fd1e7a011fd4e31a7/ZY_mwH-sI0o05SHpLqwc7.png",
            "isPro": false,
            "fullname": "Jinjie Ni",
            "user": "jinjieni",
            "type": "user"
          },
          "name": "Jinjie Ni",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:58:47.888Z",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bae",
          "user": {
            "_id": "64fe3c6b4c8924c4fed6d97b",
            "avatarUrl": "/avatars/5e0a49372af19aae9ec5ee84b299d111.svg",
            "isPro": false,
            "fullname": "Ben Gao",
            "user": "bgao22182",
            "type": "user"
          },
          "name": "Ben Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:58:54.480Z",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4baf",
          "user": {
            "_id": "65cb61ba0390fce629de99d1",
            "avatarUrl": "/avatars/9b4b4556ffd8de215dc37b02366d781b.svg",
            "isPro": false,
            "fullname": "Yuqiang Li",
            "user": "yuqiangli",
            "type": "user"
          },
          "name": "Yuqiang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:59:07.268Z",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bb0",
          "user": {
            "_id": "6436403bf3b08e267d9f0329",
            "avatarUrl": "/avatars/a5d9c3d47073e71e4cea124d9c17356d.svg",
            "isPro": false,
            "fullname": "SHIXIANG TANG",
            "user": "tangshixiang",
            "type": "user"
          },
          "name": "Shixiang Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:59:12.700Z",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bb1",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bb2",
          "name": "Erik Cambria",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bb3",
          "user": {
            "_id": "6538b861613fe158bd581e35",
            "avatarUrl": "/avatars/6817dbfe903675721fd227058b0a91ac.svg",
            "isPro": false,
            "fullname": "Dongzhan Zhou",
            "user": "schrodingers-tiger",
            "type": "user"
          },
          "name": "Dongzhan Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:59:22.218Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T08:09:15.000Z",
      "submittedOnDailyAt": "2025-03-28T04:07:23.222Z",
      "title": "Benchmark de la investigación: se promueve la benchmarkización de los modelos de lenguaje basada en descubrimientos científicos mediante la decomposición de tareas basada en la instanciación.",
      "submittedOnDailyBy": {
        "_id": "646a11791556443f24b582e9",
        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
        "isPro": false,
        "fullname": "Zonglin Yang",
        "user": "ZonglinY",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) demuestran la posibilidad de ser útiles en la investigación científica, pero por falta de marcos de evaluación profesionales, su capacidad para descubrir hipótesis de alta calidad no ha sido evaluada. Para resolver esta limitación, presentamos el primer gran marco de evaluación para sub-tareas satisfactorias de la satisfacción de la búsqueda científica: búsqueda de información, generación de hipótesis y ranking de hipótesis. Hemos extraído los elementos clave para formular problemas de investigación, investigación de fondo, búsqueda de información y hipótesis en 12 áreas de ciencia a través de un marco automatizado, y hemos verificado su precisión con la evaluación de expertos. Para prevenir la contaminación de datos, nos concentramos en artículos publicados en 2024, minimizando así la posibilidad de duplicación con los datos de entrenamiento previo de los LLMs. Los resultados de la evaluación muestran que los LLMs funcionan bien en la búsqueda de información y muestran buenas capacidades en tareas fuera de su distribución, demostrando también su capacidad para generar conocimiento nuevo. De este modo, los LLMs se convierten en \"minerales de hipótesis de investigación\", ofreciendo ayuda humana casi necesaria y potenciando la generación de hipótesis innovadoras y la investigación científica automatizada.",
      "upvotes": 9,
      "discussionId": "67e63582e8a16f38741f4c16",
      "ai_keywords": [
        "large language models (LLMs)",
        "scientific discovery",
        "inspiration retrieval",
        "hypothesis composition",
        "hypothesis ranking",
        "automated framework",
        "research questions",
        "background surveys",
        "inspirations",
        "hypotheses",
        "out-of-distribution task",
        "automated scientific discovery",
        "innovative hypotheses"
      ]
    },
    "publishedAt": "2025-03-27T04:09:15.000Z",
    "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition",
    "summary": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21248.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646a11791556443f24b582e9",
      "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
      "fullname": "Zonglin Yang",
      "name": "ZonglinY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21696",
      "authors": [
        {
          "_id": "67e64ff17fe72aad5c26ab2f",
          "user": {
            "_id": "6485bd278d14bcd5cdbb7c8d",
            "avatarUrl": "/avatars/1427cf1a72b5db0cb263ad45885cf925.svg",
            "isPro": false,
            "fullname": "Wenqi Zhang",
            "user": "zwq2018",
            "type": "user"
          },
          "name": "Wenqi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:10:00.794Z",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab30",
          "name": "Mengna Wang",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab31",
          "user": {
            "_id": "64bb986ac733e8552fa4c5d1",
            "avatarUrl": "/avatars/78dc5f802def0bdebd890438fcb1f966.svg",
            "isPro": false,
            "fullname": "Gangao Liu",
            "user": "Gangao",
            "type": "user"
          },
          "name": "Gangao Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:09:24.111Z",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab32",
          "name": "Xu Huixin",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab33",
          "user": {
            "_id": "60abbe1fe3de7c7440abb84d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1621868056572-noauth.jpeg",
            "isPro": false,
            "fullname": "Yiwei Jiang",
            "user": "yijiang",
            "type": "user"
          },
          "name": "Yiwei Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:09:05.352Z",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab34",
          "user": {
            "_id": "5e1058e9fcf41d740b69966d",
            "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
            "isPro": false,
            "fullname": "Yongliang Shen",
            "user": "tricktreat",
            "type": "user"
          },
          "name": "Yongliang Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:08:58.629Z",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab35",
          "user": {
            "_id": "67c03110e8c7d56a8e135ac8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
            "isPro": false,
            "fullname": "Hou",
            "user": "Guiyang1001",
            "type": "user"
          },
          "name": "Guiyang Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:08:43.400Z",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab36",
          "name": "Zhe Zheng",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab37",
          "name": "Hang Zhang",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab38",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab39",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab3a",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab3b",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6485bd278d14bcd5cdbb7c8d/eTJU1eLh0tSYvjN5T6Tj4.mp4"
      ],
      "publishedAt": "2025-03-27T17:00:51.000Z",
      "submittedOnDailyAt": "2025-03-28T06:01:25.299Z",
      "title": "Nominal-Riron: Búsqueda visual, armonía entre la teoría de las causas y el comportamiento para lograr interacciones experimentales de tareas.",
      "submittedOnDailyBy": {
        "_id": "6485bd278d14bcd5cdbb7c8d",
        "avatarUrl": "/avatars/1427cf1a72b5db0cb263ad45885cf925.svg",
        "isPro": false,
        "fullname": "Wenqi Zhang",
        "user": "zwq2018",
        "type": "user"
      },
      "summary": "El reciente desarrollo de modelos profundos ha demostrado un asombroso potencial de inferencia en tareas de matemáticas y programación. Sin embargo, su exploración en el ámbito físico y su interacción continua con el entorno para desempeñar un papel efectivo ha sido limitada. Presentamos el modelo Embodied Reasoner, que extiende la inferencia de estilo o1 en el contexto de la tarea de búsqueda interactiva embodied. La inferencia matemática depende principalmente de la lógica, mientras que los escenarios físicos requieren una comprensión espacial, un pensamiento temporal y una reflexión continua basada en interacciones y registros. Para abordar estos desafíos, hemos construido un conjunto de 9.3k trazas interactivas de observación-pensamiento-acción, junto con 64k imágenes interactivas y 90k procesos de pensamiento diversos (análisis, pensamiento espacial, reflexión, planificación, verificación). Hemos desarrollado una pipeline de entrenamiento en tres etapas mediante el aprendizaje del modelo, la exploración autonómica mediante muestreo de muestras negativas y la tuning de reflexión mediante auto-ajuste. En las evaluaciones, nuestro modelo supera a OpenAI o1, o3-mini y Claude-3.7 en un porcentaje del 9%, 24% y 13% más. La análisis muestra que nuestro modelo mejora la exploración iterativa, reduce las inapropiaciones lógicas y se destaca en tareas de predicción a largo plazo complejas. En entornos reales y mundiales, la reducción de la exploración iterativa y las inapropiaciones lógicas contribuyen a la excelente performance de nuestro modelo.",
      "upvotes": 8,
      "discussionId": "67e64ff37fe72aad5c26ac06",
      "projectPage": "https://embodied-reasoner.github.io/",
      "githubRepo": "https://github.com/zwq2018/embodied_reasoner",
      "ai_keywords": [
        "Embodied Reasoner",
        "Observation-Thought-Action trajectories",
        "imitation learning",
        "rejection sampling",
        "reflection tuning",
        "visual reasoning models",
        "long-horizon tasks"
      ]
    },
    "publishedAt": "2025-03-27T13:00:51.000Z",
    "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks",
    "summary": "Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6485bd278d14bcd5cdbb7c8d/eTJU1eLh0tSYvjN5T6Tj4.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21696.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6485bd278d14bcd5cdbb7c8d",
      "avatarUrl": "/avatars/1427cf1a72b5db0cb263ad45885cf925.svg",
      "fullname": "Wenqi Zhang",
      "name": "zwq2018",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21774",
      "authors": [
        {
          "_id": "67e6094c48742d6df7586714",
          "name": "Jianning Pei",
          "hidden": false
        },
        {
          "_id": "67e6094c48742d6df7586715",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "67e6094c48742d6df7586716",
          "user": {
            "_id": "64c38fcf573c5a427e12cd37",
            "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
            "isPro": false,
            "fullname": "cientgu",
            "user": "cientgu",
            "type": "user"
          },
          "name": "Shuyang Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:01:17.103Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:59:46.000Z",
      "submittedOnDailyAt": "2025-03-28T00:58:59.244Z",
      "title": "Estimado, aquí tienes la traducción al español:\n\n\"Estimación de la Varianza óptima de Paso\"",
      "submittedOnDailyBy": {
        "_id": "64c38fcf573c5a427e12cd37",
        "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
        "isPro": false,
        "fullname": "cientgu",
        "user": "cientgu",
        "type": "user"
      },
      "summary": "Los modelos de difusión logran un alto calidad de generación, pero presentan problemas debido a la discretización en pasos inadecuados, lo que lleva a una gran cantidad de cálculos en la sampling. El actual desarrollo de investigación se centra en la optimización de la eliminación de ruido, pero nosotros estamos enfocando en el diseño fundamental de los esquemas de tamaño de paso. En este trabajo, proponemos un marco de planificación dinámica llamado \"Distilación de Tamaños de Paso Óptimos\" para extraer un esquema de tamaño de paso óptimo teóricamente adecuado basado en el conocimiento obtenido de la referencia trazadora. Reestructuramos la optimización del tamaño de paso y minimizamos la función para asegurar que nuestro método utilice estructuras óptimas y evite limitaciones de discretización global. Es importante destacar que el esquema extraído muestra una fuerte invarianza respecto a la estructura, el solver de ecuaciones diferenciales ordinarias y el esquema de ruido. En los experimentos, logramos acelerar la generación de imágenes desde contextos de texto en un factor de 10 mientras mantenemos la performance de GenEval en un 99.4%. El código está disponible en https://github.com/bebebe666/OptimalSteps.",
      "upvotes": 7,
      "discussionId": "67e6095248742d6df75868db",
      "ai_keywords": [
        "diffusion models",
        "generation quality",
        "computational intensive sampling",
        "step discretization",
        "denoising directions",
        "Optimal Stepsize Distillation",
        "dynamic programming framework",
        "theoretical optimal schedules",
        "knowledge distillation",
        "reference trajectories",
        "stepsize optimization",
        "recursive error minimization",
        "global discretization bounds",
        "optimal substructure exploitation",
        "robustness",
        "ODE solvers",
        "noise schedules",
        "text-to-image generation",
        "GenEval"
      ]
    },
    "publishedAt": "2025-03-27T13:59:46.000Z",
    "title": "Optimal Stepsize for Diffusion Sampling",
    "summary": "Diffusion models achieve remarkable generation quality but suffer from\ncomputational intensive sampling due to suboptimal step discretization. While\nexisting works focus on optimizing denoising directions, we address the\nprincipled design of stepsize schedules. This paper proposes Optimal Stepsize\nDistillation, a dynamic programming framework that extracts theoretically\noptimal schedules by distilling knowledge from reference trajectories. By\nreformulating stepsize optimization as recursive error minimization, our method\nguarantees global discretization bounds through optimal substructure\nexploitation. Crucially, the distilled schedules demonstrate strong robustness\nacross architectures, ODE solvers, and noise schedules. Experiments show 10x\naccelerated text-to-image generation while preserving 99.4% performance on\nGenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21774.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c38fcf573c5a427e12cd37",
      "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
      "fullname": "cientgu",
      "name": "cientgu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21765",
      "authors": [
        {
          "_id": "67e60da88decdc7da4bf69a9",
          "user": {
            "_id": "67e617ebd0fd66b1f393eedc",
            "avatarUrl": "/avatars/97a5ba0d0422f04e396399da1b74e8d4.svg",
            "isPro": false,
            "fullname": "Minghui Lin",
            "user": "minnielin",
            "type": "user"
          },
          "name": "Minghui Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:01:37.704Z",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69aa",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69ab",
          "user": {
            "_id": "65dcaf16287a93e081d9c2f0",
            "avatarUrl": "/avatars/2db4e25c6924461abb5634f8ffd1ee87.svg",
            "isPro": false,
            "fullname": "Yishanwang",
            "user": "yishanwang",
            "type": "user"
          },
          "name": "Yishan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:01:51.882Z",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69ac",
          "user": {
            "_id": "65fd82762bf2cd20ddaa193f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
            "isPro": false,
            "fullname": "Siteng Huang",
            "user": "huangsiteng",
            "type": "user"
          },
          "name": "Shu Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-28T02:47:05.703Z",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69ad",
          "name": "Fengqi Dai",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69ae",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69af",
          "user": {
            "_id": "65eaf755ab0a6a90da55ab58",
            "avatarUrl": "/avatars/a46890a9d067a913513edf3759f12c85.svg",
            "isPro": false,
            "fullname": "Cunxiang Wang",
            "user": "wangcunxiang",
            "type": "user"
          },
          "name": "Cunxiang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:02:15.357Z",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69b0",
          "name": "Zhengrong Zuo",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69b1",
          "name": "Nong Sang",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69b2",
          "user": {
            "_id": "65fd82762bf2cd20ddaa193f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
            "isPro": false,
            "fullname": "Siteng Huang",
            "user": "huangsiteng",
            "type": "user"
          },
          "name": "Siteng Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:02:43.147Z",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69b3",
          "user": {
            "_id": "67597be2f3cd6492d4162ef8",
            "avatarUrl": "/avatars/ba580c04b7057927d4a22dcb44c52400.svg",
            "isPro": false,
            "fullname": "DONGLIN",
            "user": "wangdonglin130",
            "type": "user"
          },
          "name": "Donglin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:02:37.839Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:58:33.000Z",
      "submittedOnDailyAt": "2025-03-28T01:19:58.442Z",
      "title": "El desarrollo de conocimientos en física en la investigación sobre generación de imágenes: lista",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "Recientemente, el avance en la generación de imágenes ha destacado especialmente el rápido progreso de los modelos de difusión. Sin embargo, estas limitaciones físicas han llevado a casos donde el contenido generado puede violar las leyes fundamentales de la física y se enfrentan a dos desafíos: la realidad visual y algo físico. Los investigadores han subrayado la importancia de la verdad física en la generación de imágenes, incorporando percepciones físicas inducidas por la inferencia, como representaciones dinámicas y conocimientos físicos, y han intentado simular escenarios dinámicos reales. Considerando que este campo carece de una visión sistemática, este estudio tiene como objetivo proporcionar un resumen detallado de la estructura y sus aplicaciones para remediar estas deficiencias. En particular, desde la perspectiva de la ciencia cognitiva, se discuten los procesos de evolución del conocimiento físico, proponiendo tres etapas tecnológicas: el reconocimiento básico de la escena, el reconocimiento pasivo del conocimiento físico y el reconocimiento activo de la simulación del mundo. Este estudio también destaca los problemas esenciales propios de este campo y muestra patrones para futuras investigaciones, con el objetivo de liderar la discusión entre la comunidad académica e industrial. A través de una revisión estructurada y un análisis multidisciplinario, este estudio proporciona una orientación para el desarrollo de un paradigma de generación de imágenes que sea interpretable, controlable y físicamente consistente, con el objetivo de avanzar los modelos de generación de imágenes desde la etapa de \"simulación visual\" hacia una nueva etapa de \"reconocimiento físico humano\".",
      "upvotes": 7,
      "discussionId": "67e60da98decdc7da4bf6a28",
      "githubRepo": "https://github.com/minnie-lin/Awesome-Physics-Cognition-based-Video-Generation",
      "ai_keywords": [
        "diffusion models",
        "physical cognition",
        "motion representations",
        "generative systems",
        "real-world dynamic scenarios",
        "cognitive science",
        "schema perception",
        "passive cognition",
        "active cognition",
        "state-of-the-art methods",
        "classical paradigms",
        "benchmarks",
        "key challenges",
        "interpretable",
        "controllable",
        "physically consistent",
        "human-like physical comprehension"
      ]
    },
    "publishedAt": "2025-03-27T13:58:33.000Z",
    "title": "Exploring the Evolution of Physics Cognition in Video Generation: A\n  Survey",
    "summary": "Recent advancements in video generation have witnessed significant progress,\nespecially with the rapid advancement of diffusion models. Despite this, their\ndeficiencies in physical cognition have gradually received widespread attention\n- generated content often violates the fundamental laws of physics, falling\ninto the dilemma of ''visual realism but physical absurdity\". Researchers began\nto increasingly recognize the importance of physical fidelity in video\ngeneration and attempted to integrate heuristic physical cognition such as\nmotion representations and physical knowledge into generative systems to\nsimulate real-world dynamic scenarios. Considering the lack of a systematic\noverview in this field, this survey aims to provide a comprehensive summary of\narchitecture designs and their applications to fill this gap. Specifically, we\ndiscuss and organize the evolutionary process of physical cognition in video\ngeneration from a cognitive science perspective, while proposing a three-tier\ntaxonomy: 1) basic schema perception for generation, 2) passive cognition of\nphysical knowledge for generation, and 3) active cognition for world\nsimulation, encompassing state-of-the-art methods, classical paradigms, and\nbenchmarks. Subsequently, we emphasize the inherent key challenges in this\ndomain and delineate potential pathways for future research, contributing to\nadvancing the frontiers of discussion in both academia and industry. Through\nstructured review and interdisciplinary analysis, this survey aims to provide\ndirectional guidance for developing interpretable, controllable, and physically\nconsistent video generation paradigms, thereby propelling generative models\nfrom the stage of ''visual mimicry'' towards a new phase of ''human-like\nphysical comprehension''.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21765.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21144",
      "authors": [
        {
          "_id": "67e60e315f20b94fcd0d1f9b",
          "name": "Jinwei Qi",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1f9c",
          "name": "Chaonan Ji",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1f9d",
          "user": {
            "_id": "672d72751474234855223935",
            "avatarUrl": "/avatars/dc8a79b7d5e1175725334b337702e1fd.svg",
            "isPro": false,
            "fullname": "Sheng Xu",
            "user": "shengxu97",
            "type": "user"
          },
          "name": "Sheng Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:10:43.747Z",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1f9e",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1f9f",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1fa0",
          "user": {
            "_id": "63d0cc736b985b0f25d0412c",
            "avatarUrl": "/avatars/3eb8c79f9a7c4c819038ea7b04e323dd.svg",
            "isPro": false,
            "fullname": "Bo",
            "user": "Liefeng",
            "type": "user"
          },
          "name": "Liefeng Bo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:11:02.629Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T04:18:53.000Z",
      "submittedOnDailyAt": "2025-03-28T01:20:25.562Z",
      "title": "Chat Anniversary: Modelo de Difusor de Momentos para la Creación de Videos de Portrait con Cores de Luz Stylizadas",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "El entorno de interacción vía video de chatbot PotterBot ha sido reconocido como una tendencia futura, especialmente debido al desarrollo impresionante de las tecnologías de chatbot de texto y voz. Sin embargo, los métodos actuales se centran principalmente en la generación en tiempo real de movimientos faciales, lo que es difícil generar movimientos corporales sincronizados con los movimientos faciales. Además, controlar la precisión y la expresividad facial durante la habla es un desafío. Para resolver estos limitaciones, presentamos un nuevo marco de trabajo para la generación en tiempo real de vídeos de chatbot PotterBot que permite controlar la expresividad y el estilo facial, así como la sincronización de movimientos corporales. Este marco de trabajo permite la implementación de vídeos de chatbot con alta expresividad y flexibilidad, desde el movimiento de la cabeza hasta la interacción entre las extremidades. Nuestro enfoque se compone de dos etapas: en la primera, utilizamos un modelo de desplazamiento de movimientos eficiente que considera expresiones faciales explícitas y ocultas, permitiendo la generación de expresiones faciales variadas y la sincronización de movimientos de cabeza y cuerpo. En la segunda etapa, se centra en la generación de vídeos de chatbot PotterBot que incluyen movimientos corporales. Inyectamos señales de control de manos explícitas para generar movimientos más detallados de las manos y realizamos un refinamiento facial para mejorar la realismo y la expresividad del vídeo. Además, nuestro enfoque permite la generación eficiente de vídeos de chatbot PotterBot con movimientos corporales en resoluciones máximas de 512 * 768 en una GPU RTX 4090, asegurando una interacción en tiempo real. Los resultados de los experimentos demuestran la capacidad de nuestro enfoque para generar vídeos de chatbot PotterBot con alta expresividad y movimientos corporales naturales.",
      "upvotes": 5,
      "discussionId": "67e60e325f20b94fcd0d1fff",
      "ai_keywords": [
        "hierarchical motion diffusion models",
        "explicit and implicit motion representations",
        "stylistic control",
        "synchronization between head and body movements",
        "face refinement",
        "continous generation",
        "upper-body portrait video",
        "interactive video-chat"
      ]
    },
    "publishedAt": "2025-03-27T00:18:53.000Z",
    "title": "ChatAnyone: Stylized Real-time Portrait Video Generation with\n  Hierarchical Motion Diffusion Model",
    "summary": "Real-time interactive video-chat portraits have been increasingly recognized\nas the future trend, particularly due to the remarkable progress made in text\nand voice chat technologies. However, existing methods primarily focus on\nreal-time generation of head movements, but struggle to produce synchronized\nbody motions that match these head actions. Additionally, achieving\nfine-grained control over the speaking style and nuances of facial expressions\nremains a challenge. To address these limitations, we introduce a novel\nframework for stylized real-time portrait video generation, enabling expressive\nand flexible video chat that extends from talking head to upper-body\ninteraction. Our approach consists of the following two stages. The first stage\ninvolves efficient hierarchical motion diffusion models, that take both\nexplicit and implicit motion representations into account based on audio\ninputs, which can generate a diverse range of facial expressions with stylistic\ncontrol and synchronization between head and body movements. The second stage\naims to generate portrait video featuring upper-body movements, including hand\ngestures. We inject explicit hand control signals into the generator to produce\nmore detailed hand movements, and further perform face refinement to enhance\nthe overall realism and expressiveness of the portrait video. Additionally, our\napproach supports efficient and continuous generation of upper-body portrait\nvideo in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supporting\ninteractive video-chat in real-time. Experimental results demonstrate the\ncapability of our approach to produce portrait videos with rich expressiveness\nand natural upper-body movements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21144.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21088",
      "authors": [
        {
          "_id": "67e602b6dabfd9d4bbf10849",
          "user": {
            "_id": "66f4bdbdc51768d9d4498d16",
            "avatarUrl": "/avatars/0f6ded5fd9cf4e6f0b180b5aa329ea33.svg",
            "isPro": false,
            "fullname": "Haoming Xu",
            "user": "HaomingXu",
            "type": "user"
          },
          "name": "Haoming Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-28T02:00:25.958Z",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084a",
          "user": {
            "_id": "66d270dc5ae47374c27c9e9a",
            "avatarUrl": "/avatars/556094d864e8e779b15bfc4360e91a44.svg",
            "isPro": false,
            "fullname": "Shuxun Wang",
            "user": "Saberlve",
            "type": "user"
          },
          "name": "Shuxun Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:57.087Z",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084b",
          "name": "Yanqiu Zhao",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084c",
          "name": "Yi Zhong",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084d",
          "name": "Ziyan Jiang",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084e",
          "name": "Ningyuan Zhao",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084f",
          "name": "Shumin Deng",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf10850",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf10851",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T02:03:25.000Z",
      "submittedOnDailyAt": "2025-03-28T00:32:00.822Z",
      "title": "ZJUKLAB at SemEval-2025 Task 4: Integración de Modelos para la Olla de Fuego",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Este artículo reporta la presentación de un trabajo realizado por el equipo ZJUKLAB en el TASK4 de SemEval 2025 sobre \"Olvido de contenidos sensibles en modelos de lenguaje grandes\". Este TASK tiene como objetivo eliminar contenidos sensibles de modelos de lenguaje grandes de manera selectiva, evitando problemas como el olvido excesivo o la pérdida incompleta. Proponemos un sistema de olvido utilizando Model Merging (específicamente TIES-Merging), integrando dos modelos especializados para crear un modelo equilibrado de olvido. Nuestro sistema obtuvo los segundos lugares en un total de 26 equipos, con un puntaje en línea de TASK AGREEGATE de 0.944 y un puntaje en línea de AGREEGATE total de 0.487. En este artículo, realizamos experimentos locales, analizamos el proceso de olvido en detalle, predicemos la trayectoria del rendimiento, la dinámica de la pérdida y desde la perspectiva de los pesos, y realizamos experimentos adicionales para comprender la eficacia de nuestro método. Además, analizamos las limitaciones de nuestro método y los criterios de evaluación, destacando que no es posible evaluar completamente el éxito del olvido solo con puntuaciones basadas en MIA y ROUGE. Finalmente, destacamos la necesidad de revisar los objetivos del olvido y de desarrollar mejores métodos de evaluación en futuras investigaciones. El código está disponible en https://github.com/zjunlp/unlearn/tree/main/semeval25.",
      "upvotes": 4,
      "discussionId": "67e602badabfd9d4bbf10973",
      "githubRepo": "https://github.com/zjunlp/unlearn",
      "ai_keywords": [
        "Model Merging",
        "TIES-Merging",
        "large language models",
        "unlearning",
        "over-forgetting",
        "under-forgetting",
        "unlearned model",
        "performance trajectories",
        "loss dynamics",
        "weight perspectives",
        "MIA scores",
        "ROUGE-based metrics"
      ]
    },
    "publishedAt": "2025-03-26T22:03:25.000Z",
    "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
    "summary": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20990",
      "authors": [
        {
          "_id": "67e626a9cb305c5a3ee11fac",
          "user": {
            "_id": "62dd8f328456396d4f8aa894",
            "avatarUrl": "/avatars/af8f5dc7ff937e3e849ecdfd9ca4750b.svg",
            "isPro": false,
            "fullname": "Yupeng Cao",
            "user": "YupengCao",
            "type": "user"
          },
          "name": "Yupeng Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:11:18.739Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fad",
          "user": {
            "_id": "634cabd104491d9f7111eea3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665969099097-noauth.jpeg",
            "isPro": false,
            "fullname": "Haohang Li",
            "user": "Acatsama",
            "type": "user"
          },
          "name": "Haohang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:11:24.664Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fae",
          "user": {
            "_id": "64f757c6016d60f3199ef5e6",
            "avatarUrl": "/avatars/2659ba698081265d0480b08161718013.svg",
            "isPro": false,
            "fullname": "Yangyang Yu",
            "user": "ShirleyY",
            "type": "user"
          },
          "name": "Yangyang Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:11:30.611Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11faf",
          "user": {
            "_id": "6265db3f637f6ec042b2c4d7",
            "avatarUrl": "/avatars/7bb20dce7c96059d2c4f06890344af86.svg",
            "isPro": false,
            "fullname": "Shashidhar Reddy Javaji",
            "user": "Shashidhar",
            "type": "user"
          },
          "name": "Shashidhar Reddy Javaji",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:11:36.871Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb0",
          "user": {
            "_id": "65bd14e8ce846f8aa94db1d1",
            "avatarUrl": "/avatars/76eaad15bf32eba75271f3dc315527c2.svg",
            "isPro": false,
            "fullname": "Yueru He",
            "user": "Yueru1",
            "type": "user"
          },
          "name": "Yueru He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:12:00.574Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb1",
          "user": {
            "_id": "63b58ed5889aa6707f0bb0f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/znl74_aMswlV8VtHrfj3G.jpeg",
            "isPro": true,
            "fullname": "Jimin Huang",
            "user": "jiminHuang",
            "type": "user"
          },
          "name": "Jimin Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:12:09.888Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb2",
          "user": {
            "_id": "62d63a9dc5ada8ef841b4787",
            "avatarUrl": "/avatars/a79a4ac07984d9a8623c99bdce9add54.svg",
            "isPro": false,
            "fullname": "Zining Zhu",
            "user": "ZiningZhu",
            "type": "user"
          },
          "name": "Zining Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:12:18.401Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb3",
          "user": {
            "_id": "6479f4317c18dca75e9a9324",
            "avatarUrl": "/avatars/9aa709230b057f57ee4415c04a622c63.svg",
            "isPro": false,
            "fullname": "Xie",
            "user": "QianqianXie1994",
            "type": "user"
          },
          "name": "Qianqian Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:12:33.144Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb4",
          "user": {
            "_id": "642de494b42737f9e1e0046b",
            "avatarUrl": "/avatars/01ebcb41b89b1da557abdb9cf867331f.svg",
            "isPro": false,
            "fullname": "Xiao-Yang Liu Yanglet",
            "user": "yanglet",
            "type": "user"
          },
          "name": "Xiao-yang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:12:40.590Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb5",
          "name": "Koduvayur Subbalakshmi",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb6",
          "name": "Meikang Qiu",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb7",
          "user": {
            "_id": "66f6cb352c5d4ef3578a9c3f",
            "avatarUrl": "/avatars/0a70c94072bc5e1d018cf12da0904ff0.svg",
            "isPro": false,
            "fullname": "Sophia Ananiadou",
            "user": "Effoula",
            "type": "user"
          },
          "name": "Sophia Ananiadou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:12:56.712Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb8",
          "name": "Jian-Yun Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T21:07:51.000Z",
      "submittedOnDailyAt": "2025-03-28T03:07:27.038Z",
      "title": "FinAudio: FinAudio es un marco de referencia para modelos de lenguaje de voz en aplicaciones financieras.",
      "submittedOnDailyBy": {
        "_id": "63a0c0803c8841cfe2cd1f15",
        "avatarUrl": "/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg",
        "isPro": false,
        "fullname": "Xueqing Peng",
        "user": "Xueqing",
        "type": "user"
      },
      "summary": "Los Modelos de Lenguaje de Largo Audio (AudioLLMs) han significativamente mejorado el rendimiento en tareas de voz, como la conversación, la comprensión del lenguaje de voz y la reconocimiento automático de voz (ASR). Durante este proceso de desarrollo, no existen marcos de referencia para evaluar los AudioLLMs en escenarios financieros. Los recursos importantes para análisis financiero y decisiones de inversión, incluyen datos de voz como conferencias de beneficios y discursos de CEO. En este artículo, se presenta por primera vez FinAudio, una evaluación de los AudioLLMs en el sector financiero. Se definen tres tareas basadas en las características propias del sector financiero: 1) la reconocimiento automático de voz en corto audio financiero, 2) la reconocimiento automático de voz en largo audio financiero, y 3) la resumen de largo audio financiero. Se combinan dos conjuntos de datos de voz cortos y dos de datos de voz largos para desarrollar un nuevo conjunto de datos de resumen de voz financiero, que se configura como el marco de referencia FinAudio. Se evaluan en FinAudio seis principales AudioLLMs. Los resultados de la evaluación claramente demuestran las limitaciones de los AudioLLMs en el sector financiero y proporcionan ideas para su mejora. Todos los conjuntos de datos y códigos están disponibles.",
      "upvotes": 4,
      "discussionId": "67e626aacb305c5a3ee12027",
      "ai_keywords": [
        "Audio Large Language Models (AudioLLMs)",
        "automatic speech recognition (ASR)",
        "financial scenarios",
        "benchmark",
        "financial analysis",
        "investment decisions",
        "summarization",
        "ASR for short financial audio",
        "ASR for long financial audio",
        "summarization of long financial audio",
        "\\textsc{FinAudio}"
      ]
    },
    "publishedAt": "2025-03-26T17:07:51.000Z",
    "title": "FinAudio: A Benchmark for Audio Large Language Models in Financial\n  Applications",
    "summary": "Audio Large Language Models (AudioLLMs) have received widespread attention\nand have significantly improved performance on audio tasks such as\nconversation, audio understanding, and automatic speech recognition (ASR).\nDespite these advancements, there is an absence of a benchmark for assessing\nAudioLLMs in financial scenarios, where audio data, such as earnings conference\ncalls and CEO speeches, are crucial resources for financial analysis and\ninvestment decisions. In this paper, we introduce FinAudio, the first\nbenchmark designed to evaluate the capacity of AudioLLMs in the financial\ndomain. We first define three tasks based on the unique characteristics of the\nfinancial domain: 1) ASR for short financial audio, 2) ASR for long financial\naudio, and 3) summarization of long financial audio. Then, we curate two short\nand two long audio datasets, respectively, and develop a novel dataset for\nfinancial audio summarization, comprising the FinAudio benchmark.\nThen, we evaluate seven prevalent AudioLLMs on FinAudio. Our\nevaluation reveals the limitations of existing AudioLLMs in the financial\ndomain and offers insights for improving AudioLLMs. All datasets and codes will\nbe released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a0c0803c8841cfe2cd1f15",
      "avatarUrl": "/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg",
      "fullname": "Xueqing Peng",
      "name": "Xueqing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20776",
      "authors": [
        {
          "_id": "67e6019af54a34f8a989d8eb",
          "user": {
            "_id": "642a276516d4d8293c9a47e8",
            "avatarUrl": "/avatars/80e6db8bc2544f3486b11b57858a8692.svg",
            "isPro": false,
            "fullname": "Shijie Zhou",
            "user": "shijiezhou",
            "type": "user"
          },
          "name": "Shijie Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:59.022Z",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8ec",
          "name": "Hui Ren",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8ed",
          "name": "Yijia Weng",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8ee",
          "user": {
            "_id": "67b3c26f3d0f54ab3805954d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/beyNv649ZZ5A29CH3m8mj.png",
            "isPro": false,
            "fullname": "Shuwang Zhang",
            "user": "ShuwangZhang00",
            "type": "user"
          },
          "name": "Shuwang Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:13:30.134Z",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8ef",
          "name": "Zhen Wang",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f0",
          "user": {
            "_id": "62f687a0c58915315c4ff75d",
            "avatarUrl": "/avatars/b657180c7666735062782edd4f6a69c9.svg",
            "isPro": false,
            "fullname": "Dejia Xu",
            "user": "ir1d",
            "type": "user"
          },
          "name": "Dejia Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:13:40.451Z",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f1",
          "user": {
            "_id": "6526386e1c6a09292d8d0a22",
            "avatarUrl": "/avatars/471de830de2d775d35368678c1579f87.svg",
            "isPro": false,
            "fullname": "fan",
            "user": "Fanzhiwen",
            "type": "user"
          },
          "name": "Zhiwen Fan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:16:41.739Z",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f2",
          "name": "Suya You",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f3",
          "name": "Zhangyang Wang",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f4",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f5",
          "name": "Achuta Kadambi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:56:16.000Z",
      "submittedOnDailyAt": "2025-03-28T00:26:28.356Z",
      "title": "Feature4X: Campos de características gaussianas diversos conectados con la inteligencia artificial 4D salida",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "El reciente desarrollo de módulos bidimensionales y multimodales ha logrado un éxito impresionante a través de entrenamientos a gran escala. Sin embargo, extender dichos éxitos a escenarios complejos de 3D/4D y a interacciones libres, así como a manipulaciones significativas, es un desafío. Este desafío se basa en la limitada disponibilidad de grandes conjuntos de datos 3D/4D o multidimensionales. Estos conjuntos de datos son cruciales para tareas comunes como la división de cajas abiertas, la división basada en prompts, edición guiada por lenguaje y respuestas a consultas visuales y lingüísticas (VQA). En este artículo, se presenta un marco general llamado Feature4X, que utiliza entradas de vídeo monocromático ampliamente utilizables en contenido generado por usuarios para expandir las funciones de un módulo visual fundamental bidimensional al mundo 4D. La 'X' en Feature4X representa la diversidad y indica que el modelo puede ser entrenado para realizar cualquier tarea mediante características 4D condicionadas por el modelo. El núcleo de este marco es una estrategia de optimización dinámica que integra las capacidades de múltiples modelos en una sola representación. Además, Feature4X utiliza la técnica de dispersión gaussiana para entrenar las características de un módulo fundamental de vídeo (por ejemplo, SAM2, InternVideo2) en características 4D explícitas, lo cual es un primer logro. En este artículo, se muestra cómo, mediante un ciclo de funciones basado en modelos de lenguaje, se pueden implementar nuevas tareas como nuevas divisiones visuales, edición de escala de estructuras y formas, y VQA libres de restricciones en todo el tiempo. Estos avances permiten interacciones dinámicas y 4D, proporcionan sistemas escalables y reconocibles en el tiempo y espacio, y amplían el rango de aplicaciones de la IA.",
      "upvotes": 4,
      "discussionId": "67e6019ff54a34f8a989d9d6",
      "ai_keywords": [
        "Feature4X",
        "4D feature field distillation",
        "Gaussian Splatting",
        "SAM2",
        "InternVideo2",
        "novel view segment anything",
        "geometric and appearance scene editing",
        "free-form VQA",
        "LLMs",
        "agentic AI applications",
        "scalable systems",
        "contextually aware",
        "spatiotemporally aware",
        "immersive dynamic 4D scene interaction"
      ]
    },
    "publishedAt": "2025-03-26T13:56:16.000Z",
    "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields",
    "summary": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20822",
      "authors": [
        {
          "_id": "67e61017b116b3c559188a0f",
          "name": "Qi Zhao",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a10",
          "user": {
            "_id": "67b576e39b7058fa21ab72a3",
            "avatarUrl": "/avatars/5393231dc585950d9579323521f41ff4.svg",
            "isPro": false,
            "fullname": "Xingyu Ni",
            "user": "Univstar",
            "type": "user"
          },
          "name": "Xingyu Ni",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:17:44.258Z",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a11",
          "name": "Ziyu Wang",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a12",
          "user": {
            "_id": "65c3dfb180497543ca257ffd",
            "avatarUrl": "/avatars/1eb9e114e2c4dc2dc2c3b2e3f387d214.svg",
            "isPro": false,
            "fullname": "Feng Cheng",
            "user": "fengcheng1",
            "type": "user"
          },
          "name": "Feng Cheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:18:07.030Z",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a13",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a14",
          "name": "Lu Jiang",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a15",
          "name": "Bohan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T00:45:07.000Z",
      "submittedOnDailyAt": "2025-03-28T01:28:16.803Z",
      "title": "El video sintético mejora la precisión física en la síntesis de vídeo.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Examinamos en el proceso de gráficos computacionales cómo se pueden mejorar la precisión física de modelos de video mediante la utilización de videos de composición. Estos videos de renderización respetan leyes físicas como la consistencia tridimensional de la realidad y se utilizan como recursos beneficiosos para mejorar los modelos de video. Para aprovechar esta posibilidad, proponemos métodos para seleccionar y integrar datos de composición, introducimos formas de transmitir características físicas a los modelos y establecemos como objetivo reducir significativamente las artefactos inadecuados. Demostramos los efectos de nuestro modelo a través de experimentos en tres tareas representativas que enfatizan la consistencia física. Sin embargo, nuestro modelo no tiene una profunda comprensión física, pero nuestro estudio muestra por primera vez experimentalmente cómo los videos de composición pueden mejorar la precisión física en el proceso de composición de videos. Página web: https://kevinz8866.github.io/simulation/",
      "upvotes": 4,
      "discussionId": "67e6101bb116b3c559188b67",
      "ai_keywords": [
        "video generation models",
        "synthetic videos",
        "computer graphics pipelines",
        "3D consistency",
        "transfering physical realism",
        "unwanted artifacts",
        "physical consistency",
        "physical fidelity",
        "video synthesis"
      ]
    },
    "publishedAt": "2025-03-25T20:45:07.000Z",
    "title": "Synthetic Video Enhances Physical Fidelity in Video Synthesis",
    "summary": "We investigate how to enhance the physical fidelity of video generation\nmodels by leveraging synthetic videos derived from computer graphics pipelines.\nThese rendered videos respect real-world physics, such as maintaining 3D\nconsistency, and serve as a valuable resource that can potentially improve\nvideo generation models. To harness this potential, we propose a solution that\ncurates and integrates synthetic data while introducing a method to transfer\nits physical realism to the model, significantly reducing unwanted artifacts.\nThrough experiments on three representative tasks emphasizing physical\nconsistency, we demonstrate its efficacy in enhancing physical fidelity. While\nour model still lacks a deep understanding of physics, our work offers one of\nthe first empirical demonstrations that synthetic video enhances physical\nfidelity in video synthesis. Website: https://kevinz8866.github.io/simulation/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20822.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20853",
      "authors": [
        {
          "_id": "67e5ff24cce3913200f29387",
          "user": {
            "_id": "62f6bd1dd278a8f3e7867392",
            "avatarUrl": "/avatars/7e5b6014d99909958eb0f95c486b2226.svg",
            "isPro": false,
            "fullname": "Alexander Swerdlow",
            "user": "aswerdlow",
            "type": "user"
          },
          "name": "Alexander Swerdlow",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:18:36.245Z",
          "hidden": false
        },
        {
          "_id": "67e5ff24cce3913200f29388",
          "user": {
            "_id": "6310ff7dd43c55e811f8772f",
            "avatarUrl": "/avatars/0dcab81771329d13cc62b08e55a71c9a.svg",
            "isPro": false,
            "fullname": "Mihir Prabhudesai",
            "user": "mihirpd",
            "type": "user"
          },
          "name": "Mihir Prabhudesai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:18:30.166Z",
          "hidden": false
        },
        {
          "_id": "67e5ff24cce3913200f29389",
          "user": {
            "_id": "67ad6a0924ad6fd76672ff2f",
            "avatarUrl": "/avatars/141280d9d1b97740f0e8acf9b681411d.svg",
            "isPro": false,
            "fullname": "Siddharth Gandhi",
            "user": "Sid1275",
            "type": "user"
          },
          "name": "Siddharth Gandhi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:18:42.006Z",
          "hidden": false
        },
        {
          "_id": "67e5ff24cce3913200f2938a",
          "user": {
            "_id": "653692e8ac570e90963cf2c5",
            "avatarUrl": "/avatars/e391ca21c2292e916ab0ab00f8ee2ba6.svg",
            "isPro": false,
            "fullname": "Deepak pathak",
            "user": "Deepak765",
            "type": "user"
          },
          "name": "Deepak Pathak",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:18:48.839Z",
          "hidden": false
        },
        {
          "_id": "67e5ff24cce3913200f2938b",
          "name": "Katerina Fragkiadaki",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:59:51.000Z",
      "submittedOnDailyAt": "2025-03-28T00:17:26.527Z",
      "title": "Integración de Modelos Multiescala y Difusión Discreta",
      "submittedOnDailyBy": {
        "_id": "6310ff7dd43c55e811f8772f",
        "avatarUrl": "/avatars/0dcab81771329d13cc62b08e55a71c9a.svg",
        "isPro": false,
        "fullname": "Mihir Prabhudesai",
        "user": "mihirpd",
        "type": "user"
      },
      "summary": "Los modelos de géneración multimodal son principalmente dominados por la forma de progresión automática secuencial (AR). Estos modelos procesan etiquetas secuencialmente desde la izquierda hacia la derecha o desde arriba hacia abajo. Pueden abordar diversas tareas como capturar imágenes, responder preguntas y generar imágenes. En este estudio, se revisa un modelo de difusión discreto que procesa texto y imágenes en un entorno de generación uniforme, basándose en el éxito reciente de la generación de texto. Este modelo de difusión discreto tiene varias ventajas sobre los modelos AR, incluyendo el control de la calidad y diversidad de los ejemplos generados, interpolación en ambos dominios (texto y imágenes) y un ancho de control más amplio durante el proceso de generación. Se propone, por primera vez, el primer modelo de difusión discreto uniforme de multimodalidad (UniDisc), que puede entender y generar texto y imágenes juntos en diversas tareas. Se compara UniDisc con modelos AR de multimodalidad y se realiza un análisis de escalabilidad para mostrar que UniDisc es más excelente en términos de rendimiento, cantidad de cálculos durante la inferencia, control, edición, interpolación, tiempo de inferencia y calidad de la generación, considerando un flexible trade-off. Los códigos y visualizaciones adicionales están disponibles en https://unidisc.github.io.",
      "upvotes": 2,
      "discussionId": "67e5ff28cce3913200f2951e",
      "projectPage": "https://unidisc.github.io/",
      "githubRepo": "https://github.com/alexanderswerdlow/unidisc",
      "ai_keywords": [
        "autoregressive (AR) approaches",
        "discrete diffusion models",
        "multimodal inpainting",
        "control over quality versus diversity",
        "Unity Multimodal Discrete Diffusion (UniDisc)",
        "scaling analysis",
        "generation quality",
        "inference-time compute",
        "controllability",
        "editability"
      ]
    },
    "publishedAt": "2025-03-26T13:59:51.000Z",
    "title": "Unified Multimodal Discrete Diffusion",
    "summary": "Multimodal generative models that can understand and generate across multiple\nmodalities are dominated by autoregressive (AR) approaches, which process\ntokens sequentially from left to right, or top to bottom. These models jointly\nhandle images, text, video, and audio for various tasks such as image\ncaptioning, question answering, and image generation. In this work, we explore\ndiscrete diffusion models as a unified generative formulation in the joint text\nand image domain, building upon their recent success in text generation.\nDiscrete diffusion models offer several advantages over AR models, including\nimproved control over quality versus diversity of generated samples, the\nability to perform joint multimodal inpainting (across both text and image\ndomains), and greater controllability in generation through guidance.\nLeveraging these benefits, we present the first Unified Multimodal Discrete\nDiffusion (UniDisc) model which is capable of jointly understanding and\ngenerating text and images for a variety of downstream tasks. We compare\nUniDisc to multimodal AR models, performing a scaling analysis and\ndemonstrating that UniDisc outperforms them in terms of both performance and\ninference-time compute, enhanced controllability, editability, inpainting, and\nflexible trade-off between inference time and generation quality. Code and\nadditional visualizations are available at https://unidisc.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310ff7dd43c55e811f8772f",
      "avatarUrl": "/avatars/0dcab81771329d13cc62b08e55a71c9a.svg",
      "fullname": "Mihir Prabhudesai",
      "name": "mihirpd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20578",
      "authors": [
        {
          "_id": "67e5fd806e73232cf08af3bb",
          "user": {
            "_id": "659430138fec845e50a27558",
            "avatarUrl": "/avatars/f5de806f55c90ae303cd94af7c15005c.svg",
            "isPro": false,
            "fullname": "Alif Al Hasan",
            "user": "alifalhasan",
            "type": "user"
          },
          "name": "Alif Al Hasan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-28T04:39:45.135Z",
          "hidden": false
        },
        {
          "_id": "67e5fd806e73232cf08af3bc",
          "user": {
            "_id": "6664836f3038d313d1ac7867",
            "avatarUrl": "/avatars/30de3b29cf06b5481f100ace55a85f29.svg",
            "isPro": false,
            "fullname": "Subarna Saha",
            "user": "Subarna10",
            "type": "user"
          },
          "name": "Subarna Saha",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:19:03.409Z",
          "hidden": false
        },
        {
          "_id": "67e5fd806e73232cf08af3bd",
          "user": {
            "_id": "6331c3f618711776b468e9ec",
            "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
            "isPro": false,
            "fullname": "Mia Mohammad Imran",
            "user": "imranraad",
            "type": "user"
          },
          "name": "Mia Mohammad Imran",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-28T01:38:09.589Z",
          "hidden": false
        },
        {
          "_id": "67e5fd806e73232cf08af3be",
          "name": "Tarannum Shaila Zaman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T14:25:01.000Z",
      "submittedOnDailyAt": "2025-03-28T00:08:31.084Z",
      "title": "Título: Verificación de Modelos de Lenguaje de Gran Escala - Investigación sobre la Generación de Entradas Basadas en Informes de Bugs\n\nTítulo en Español: Verificación de Modelos de Lenguaje de Gran Escala - Investigación sobre la Generación de Entradas Basadas en Informes de Bugs",
      "submittedOnDailyBy": {
        "_id": "6331c3f618711776b468e9ec",
        "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
        "isPro": false,
        "fullname": "Mia Mohammad Imran",
        "user": "imranraad",
        "type": "user"
      },
      "summary": "Los datos de líneas de código desempeñan un papel importante en el diagnóstico y análisis de errores en software. Los informes de errores generalmente incluyen estas informaciones, y los desarrolladores extraen estos datos para facilitar el depuración. Debido a que los informes de errores se escriben en lenguaje natural, los estudios previos han utilizado técnicas de procesamiento del lenguaje natural (NLP) para realizar la extracción automática de datos. Con el surgimiento de los modelos de lenguaje de gran tamaño (LLMs), se ha surgido una tarea de investigación importante: ¿Cómo pueden los LLMs generativos extraer de manera efectiva los datos de líneas de código de informes de errores? En este artículo, se propone la técnica LLPut y se establece el objetivo de evaluar de manera experimental el rendimiento de tres modelos de LLMs generativos abiertos: LLaMA, Qwen y Qwen-Coder. Se realizan evaluaciones experimentales sobre un conjunto de datos que incluye 206 informes de errores, y se evalúan la precisión y eficiencia de estos modelos. Nuestros hallazgos ofrecen una explicación de la capacidad automática de los LLMs para diagnosticar errores y sus limitaciones.",
      "upvotes": 1,
      "discussionId": "67e5fd816e73232cf08af3e2",
      "projectPage": "https://zenodo.org/records/15092886",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "generative LLMs",
        "LLPut",
        "LLaMA",
        "Qwen",
        "Qwen-Coder",
        "natural language",
        "empirical evaluation",
        "bug diagnosis"
      ]
    },
    "publishedAt": "2025-03-26T10:25:01.000Z",
    "title": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation",
    "summary": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20578.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6331c3f618711776b468e9ec",
      "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
      "fullname": "Mia Mohammad Imran",
      "name": "imranraad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]