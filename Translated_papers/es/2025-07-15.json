[
  {
    "paper": {
      "id": "2507.09862",
      "authors": [
        {
          "_id": "6875c14a257d4f043537056b",
          "name": "Youliang Zhang",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f043537056c",
          "name": "Zhaoyang Li",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f043537056d",
          "name": "Duomin Wang",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f043537056e",
          "name": "Jiahe Zhang",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f043537056f",
          "name": "Deyu Zhou",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f0435370570",
          "name": "Zixin Yin",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f0435370571",
          "name": "Xili Dai",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f0435370572",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "6875c14a257d4f0435370573",
          "name": "Xiu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-14T02:22:47.000Z",
      "submittedOnDailyAt": "2025-07-15T01:26:49.276Z",
      "title": "Generación Interactiva de Humanoides con Relación de Sonido-Visión en Datasets de Alta Calidad y Gran Tamaño",
      "submittedOnDailyBy": {
        "_id": "64ae9b88a22a179fc4d07992",
        "avatarUrl": "/avatars/c9065f04a1188ea3129e56a90328ffd3.svg",
        "isPro": false,
        "fullname": "wang",
        "user": "dorni",
        "type": "user"
      },
      "summary": "El rápido desarrollo de modelos grandes ha impulsado un gran cambio en el ámbito de los seres humanoides digitales. Estas avanzadas técnicas proporcionan soluciones de alta calidad en el ámbito de los Avatar-driven y Rendering, y la comunidad académica se está centrando en el siguiente gran desafío: la interacción bidireccional de voz y visión en seres humanoides virtuales. Para fomentar la investigación en este nuevo campo, proporcionamos el Dataset SpeakerVid-5M. Este es el primer dataset de gran escala y alta calidad diseñado para la interacción con seres humanoides virtuales de Alodia. Cuenta con más de 8,743 horas de video y más de 52 millones de clips de personas. Esta cantidad de material abarca diversas escalas y tipos de interacción, incluyendo conversaciones, escuchas y interacciones bidireccionales. Un punto clave es que el dataset está estructurado en dos dimensiones principales: el tipo de interacción y la calidad de los datos. Primero, se clasifican en cuatro tipos de interacción basados en los escenarios (área de diseño, área de escucha, área de interacción, área de múltiples tareas). Luego, se utilizan subconjuntos de edición previa a gran escala y subconjuntos de alta calidad en subcanales para la entrenamiento final en subcanales (SFT). Esta doble estructura permite a 2D seres humanoides realizar diversas tareas. Además, basándose en este dataset, proporcionamos líneas de chat de video basadas en la reconstrucción automática (AR) y ofrecemos un marco de referencia de video de chat (VidChatBench) con un conjunto especial de métricas y datos de prueba para apoyar futuras investigaciones. El código de procesamiento del dataset se ha publicado de manera abierta. Página del proyecto: https://dorniwang.github.io/SpeakerVid-5M/",
      "upvotes": 27,
      "discussionId": "6875c14a257d4f0435370574",
      "projectPage": "https://dorniwang.github.io/SpeakerVid-5M/",
      "ai_summary": "A large-scale dataset named SpeakerVid-5M is introduced for audio-visual dyadic interactive virtual human generation, featuring diverse interactions and high-quality data for various virtual human tasks.",
      "ai_keywords": [
        "audio-visual dyadic interactive virtual human",
        "SpeakerVid-5M",
        "video clips",
        "monadic talking",
        "listening",
        "dyadic conversations",
        "dialogue branch",
        "single branch",
        "listening branch",
        "multi-turn branch",
        "pre-training subset",
        "Supervised Fine-Tuning",
        "autoregressive",
        "video chat baseline",
        "VidChatBench"
      ]
    },
    "publishedAt": "2025-07-13T22:22:47.000Z",
    "title": "SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual\n  Dyadic Interactive Human Generation",
    "summary": "The rapid development of large-scale models has catalyzed significant\nbreakthroughs in the digital human domain. These advanced methodologies offer\nhigh-fidelity solutions for avatar driving and rendering, leading academia to\nfocus on the next major challenge: audio-visual dyadic interactive virtual\nhuman. To facilitate research in this emerging area, we present SpeakerVid-5M\ndataset, the first large-scale, high-quality dataset designed for audio-visual\ndyadic interactive virtual human generation. Totaling over 8,743 hours,\nSpeakerVid-5M contains more than 5.2 million video clips of human portraits. It\ncovers diverse scales and interaction types, including monadic talking,\nlistening, and dyadic conversations. Crucially, the dataset is structured along\ntwo key dimensions: interaction type and data quality. First, it is categorized\ninto four types (dialogue branch, single branch, listening branch and\nmulti-turn branch) based on the interaction scenario. Second, it is stratified\ninto a large-scale pre-training subset and a curated, high-quality subset for\nSupervised Fine-Tuning (SFT). This dual structure accommodates a wide array of\n2D virtual human tasks. In addition, we provide an autoregressive (AR)-based\nvideo chat baseline trained on this data, accompanied by a dedicated set of\nmetrics and test data to serve as a benchmark VidChatBench for future work.\nBoth the dataset and the corresponding data processing code will be publicly\nreleased. Project page: https://dorniwang.github.io/SpeakerVid-5M/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09862.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64ae9b88a22a179fc4d07992",
      "avatarUrl": "/avatars/c9065f04a1188ea3129e56a90328ffd3.svg",
      "fullname": "wang",
      "name": "dorni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.10548",
      "authors": [
        {
          "_id": "6875d6e7257d4f04353705b5",
          "name": "Mingxian Lin",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705b6",
          "name": "Wei Huang",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705b7",
          "name": "Yitang Li",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705b8",
          "name": "Chengjie Jiang",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705b9",
          "name": "Kui Wu",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705ba",
          "name": "Fangwei Zhong",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705bb",
          "name": "Shengju Qian",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705bc",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "6875d6e7257d4f04353705bd",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/sVJXSwN-mBG1ahHjfHx7V.gif"
      ],
      "publishedAt": "2025-07-14T17:59:46.000Z",
      "submittedOnDailyAt": "2025-07-15T03:13:21.491Z",
      "title": "EmbRACE-3K: Razones y acciones concretas en entornos complejos",
      "submittedOnDailyBy": {
        "_id": "656db3f53dc1d277e5a64410",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png",
        "isPro": false,
        "fullname": "Wei Huang",
        "user": "AaronHuangWei",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje visual recientemente desarrollados (VLMs) han demostrado un potente rendimiento en tareas de comprensión de imágenes y videos. Sin embargo, para desempeñar un funcionamiento efectivo en entornos reales, son necesarios interacciones en línea y la comprensión de escenarios dinámicos. En estos escenarios, las entidades inteligentes supervisan el entorno desde la perspectiva de una persona, y cada acción forma dinamicamente la siguiente visión. Los modelos más recientes, como GPT-4o, Claude 3.5 Sonnet y Gemini 2.5 Pro, han mostrado límites claros en la comprensión espacial y la planificación a largo plazo en interacciones abiertas. Para complementar estos límites, presentamos EmRACE-3K. EmRACE-3K es un conjunto de datos de más de 3,000 tareas guíadas por lenguaje construido utilizando Unreal Engine y el marco de trabajo UnrealCV-Zoo en diversos entornos realistas. Las tareas incluyen desafíos de diferentes entornos reales y abordan tareas de dirección, manipulación de objetos y ejecución de objetivos en etapas. Cada tarea se desarrolla como un proyecto en etapas, combinando la supervisión visual de una persona, altos niveles de instrucciones, acciones de referencia y razones expresadas en lenguaje natural para representar el objetivo de la entidad inteligente. Usando EmRACE-3K, hemos construido un benchmark para evaluar la capacidad de inferencia de VLMs en entornos reales. En este benchmark, la tasa de éxito de todos los modelos es menor del 20% en la configuración sin ejemplos. Esto claramente demuestra los desafíos de nuestro benchmark y los límites actuales de los VLMs. Para demostrar la utilidad de EmRACE-3K, hemos desarrollado y continuado a través de aprendizaje supervisado y aprendizaje por refuerzo a Qwen2.5-VL-7B. Este enfoque ha traído significativas mejoras en todos los categorías de desafíos y ha mostrado la eficiencia del conjunto de datos al relacionar el desarrollo de la capacidad de inferencia en entornos reales.",
      "upvotes": 21,
      "discussionId": "6875d6e7257d4f04353705be",
      "projectPage": "https://mxllc.github.io/EmbRACE-3K/",
      "githubRepo": "https://github.com/mxllc/EmbRACE-3K",
      "ai_summary": "A new dataset, EmRACE-3K, evaluates vision-language models in embodied settings, showing limitations in spatial reasoning and long-horizon planning, and demonstrates improvements through supervised and reinforcement learning fine-tuning.",
      "ai_keywords": [
        "vision-language models",
        "embodied settings",
        "first-person perspective",
        "dynamic spatial reasoning",
        "long-horizon planning",
        "EmRACE-3K",
        "Unreal Engine",
        "UnrealCV-Zoo",
        "navigation",
        "object manipulation",
        "multi-stage goal execution",
        "zero-shot settings",
        "supervised learning",
        "reinforcement learning"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-07-14T13:59:46.000Z",
    "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments",
    "summary": "Recent advanced vision-language models(VLMs) have demonstrated strong\nperformance on passive, offline image and video understanding tasks. However,\ntheir effectiveness in embodied settings, which require online interaction and\nactive scene understanding remains limited. In such scenarios, an agent\nperceives the environment from a first-person perspective, with each action\ndynamically shaping subsequent observations. Even state-of-the-art models such\nas GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment\ninteractions, exhibiting clear limitations in spatial reasoning and\nlong-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset\nof over 3,000 language-guided tasks situated in diverse, photorealistic\nenvironments constructed using Unreal Engine and the UnrealCV-Zoo framework.\nThe tasks encompass a wide range of embodied challenges, including navigation,\nobject manipulation, and multi-stage goal execution. Each task unfolds as a\nmulti-step trajectory, pairing first-person visual observations with high-level\ninstructions, grounded actions, and natural language rationales that express\nthe agent's intent at every step. Using EmRACE-3K, we establish a benchmark to\nevaluate the embodied reasoning capabilities of VLMs across three key\ndimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage\nGoal Execution. In zero-shot settings, all models achieve success rates below\n20%, underscoring the challenge posed by our benchmark and the current\nlimitations of VLMs in interactive environments. To demonstrate the utility of\nEmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning\nfollowed by reinforcement learning. This approach yields substantial\nimprovements across all three challenge categories, highlighting the dataset's\neffectiveness in enabling the development of embodied reasoning capabilities.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/sVJXSwN-mBG1ahHjfHx7V.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10548.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "656db3f53dc1d277e5a64410",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png",
      "fullname": "Wei Huang",
      "name": "AaronHuangWei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.10532",
      "authors": [
        {
          "_id": "6875f107257d4f0435370613",
          "name": "Mingqi Wu",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f0435370614",
          "name": "Zhihao Zhang",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f0435370615",
          "name": "Qiaole Dong",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f0435370616",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f0435370617",
          "name": "Jun Zhao",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f0435370618",
          "name": "Senjie Jin",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f0435370619",
          "name": "Xiaoran Fan",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f043537061a",
          "name": "Yuhao Zhou",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f043537061b",
          "name": "Yanwei Fu",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f043537061c",
          "name": "Qin Liu",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f043537061d",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "6875f107257d4f043537061e",
          "name": "Qi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-14T17:55:15.000Z",
      "submittedOnDailyAt": "2025-07-15T04:41:41.806Z",
      "title": "Teoría o memoria? El impacto de la contaminación de datos en el aprendizaje por refuerzo que produce resultados no normales",
      "submittedOnDailyBy": {
        "_id": "630716d11801ecc7d2595021",
        "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
        "isPro": false,
        "fullname": "Songyang Zhang",
        "user": "zsytony",
        "type": "user"
      },
      "summary": "El aporte de los modelos de lenguaje grande (LLMs) ha sido un foco central en la investigación a largo plazo. Recientes estudios han utilizado aprendizaje por refuerzo (RL) para evolucionar estas capacidades y mejorar su rendimiento sin incrementar significativamente los costos de subproducto externo, proponiendo nuevas estrategias. Sorprendentemente, algunos estudios indican que se puede mejorar la capacidad de los modelos con señales de recompensa aleatorias o imprecisas. Sin embargo, estas innovaciones se han reportado principalmente para la familia del modelo Qwen2.5, evaluados en marcos de referencia famosos como MATH-500, AMC y AIME. Sin embargo, no se han logrado los mismos resultados en modelos como Llama. Según nuestro análisis, Qwen2.5 puede mejorar su capacidad matemática, pero su aprendizaje previo en grandes bases de datos de crawling web, ha renderizado sus rendimientos en populares marcos de referencia vulnerables a la copia de datos. Como consecuencia, no podemos confiar en los resultados obtenidos en estos marcos de referencia. Para resolver este problema, presentamos un generador que crea problemas de cálculo de longitud y dificultad aleatorias. Este conjunto de datos permite mejorar el rendimiento de los modelos de forma consistente, sin copiar datos, y sin que el rendimiento sea afectado por ruido o señales imprecisas. Para asegurar que estos resultados sean confiables, proponemos que la evaluación de métodos de aprendizaje por refuerzo se haga sin copia de datos, tanto en marcos de referencia como en familias de modelos diversas.",
      "upvotes": 20,
      "discussionId": "6875f107257d4f043537061f",
      "ai_summary": "Research on enhancing LLM reasoning through RL reveals that accurate reward signals are crucial for performance improvement, and current benchmarks may be unreliable due to data contamination.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "reinforcement learning",
        "RL",
        "Qwen2.5",
        "MATH-500",
        "AMC",
        "AIME",
        "Llama",
        "pretraining",
        "large-scale web corpora",
        "data contamination",
        "synthetic arithmetic problems",
        "RandomCalculation",
        "leakage-free datasets",
        "reward signals"
      ]
    },
    "publishedAt": "2025-07-14T13:55:15.000Z",
    "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning\n  Due to Data Contamination",
    "summary": "The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10532.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630716d11801ecc7d2595021",
      "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
      "fullname": "Songyang Zhang",
      "name": "zsytony",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04404",
      "authors": [
        {
          "_id": "6875d1a3257d4f043537058e",
          "name": "Jingze Zhu",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f043537058f",
          "name": "Yongliang Wu",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370590",
          "name": "Wenbo Zhu",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370591",
          "name": "Jiawang Cao",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370592",
          "name": "Yanqiang Zheng",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370593",
          "name": "Jiawei Chen",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370594",
          "name": "Xu Yang",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370595",
          "name": "Bernt Schiele",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370596",
          "name": "Jonas Fischer",
          "hidden": false
        },
        {
          "_id": "6875d1a3257d4f0435370597",
          "name": "Xinting Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-06T14:35:43.000Z",
      "submittedOnDailyAt": "2025-07-15T02:28:43.418Z",
      "title": "Layercake: una decision relativamente determinista que se da al reconocer información de tokens dentro de un modelo de lenguaje grande",
      "submittedOnDailyBy": {
        "_id": "66f6bc97980d52c75c300511",
        "avatarUrl": "/avatars/f7c23c4b09701580b533212ec9b6e306.svg",
        "isPro": false,
        "fullname": "Yongliang",
        "user": "Liang0223",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) tienen un excelente rendimiento en la comprensión y generación de lenguaje natural, pero son vulnerables a errores factuales y limitan la confianza en tareas de densidad de conocimiento. Las estrategias de decodificación no requieren entrenamiento y proporcionan soluciones eficientes, pero actualmente, se analizan separadamente los mensajes y las capas de señales a nivel de token y capa, sin detectar los movimientos más delicados entre ellos. En este estudio, se presenta una metodología de decodificación relativamente eficiente que combina tipos de tokens específicos y capas transformadoras de alto impacto. A través de un análisis experimental de acciones, se identificaron dos patrones principales: los tokens de símbolo reciben la principal acción en las capas iniciales, mientras que los tokens conceptuales son dominados por razones lingüísticas en las capas intermedias. Al restringir la acción selectiva de estos tipos de tokens, se puede obtener una señal relativamente controlada que conduce a la introducción de una deterioración factual controlada y, finalmente, a un decodificado factual. Nuestro método no requiere adicionales entrenamientos o modificaciones del modelo, y los experimentos muestran que mejoran la consistencia factual en varios LLMs y diferentes marcos de referencia.",
      "upvotes": 16,
      "discussionId": "6875d1a3257d4f0435370598",
      "ai_summary": "A token-aware, layer-localized contrastive decoding method improves factual accuracy in large language models by selectively suppressing attention to specific token types at their respective depths.",
      "ai_keywords": [
        "large language models",
        "natural language understanding",
        "natural language generation",
        "factual errors",
        "decoding-time strategies",
        "token-level signals",
        "layer-level signals",
        "token-aware",
        "layer-localized",
        "contrastive decoding",
        "transformer layers",
        "punctuation tokens",
        "conceptual tokens",
        "semantic reasoning",
        "empirical attention analysis",
        "factual generation",
        "controlled factual degradation",
        "contrastive signals",
        "factual decoding"
      ]
    },
    "publishedAt": "2025-07-06T10:35:43.000Z",
    "title": "LayerCake: Token-Aware Contrastive Decoding within Large Language Model\n  Layers",
    "summary": "Large language models (LLMs) excel at natural language understanding and\ngeneration but remain vulnerable to factual errors, limiting their reliability\nin knowledge-intensive tasks. While decoding-time strategies provide a\npromising efficient solution without training, existing methods typically treat\ntoken-level and layer-level signals in isolation, overlooking the joint\ndynamics between them. In this work, we introduce a token-aware,\nlayer-localized contrastive decoding method that aligns specific token types\nwith their most influential transformer layers to improve factual generation.\nThrough empirical attention analysis, we identify two key patterns: punctuation\ntokens receive dominant attention in early layers, while conceptual tokens\ngovern semantic reasoning in intermediate layers. By selectively suppressing\nattention to these token types at their respective depths, we achieve the\ninduction of controlled factual degradation and derive contrastive signals to\nguide the final factual decoding. Our method requires no additional training or\nmodel modification, and experiments demonstrate that our method consistently\nimproves factuality across multiple LLMs and various benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04404.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66f6bc97980d52c75c300511",
      "avatarUrl": "/avatars/f7c23c4b09701580b533212ec9b6e306.svg",
      "fullname": "Yongliang",
      "name": "Liang0223",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.10541",
      "authors": [
        {
          "_id": "6875e5f0257d4f04353705de",
          "name": "Zhuoshi Pan",
          "hidden": false
        },
        {
          "_id": "6875e5f0257d4f04353705df",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "6875e5f0257d4f04353705e0",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "6875e5f0257d4f04353705e1",
          "name": "Qiyao Sun",
          "hidden": false
        },
        {
          "_id": "6875e5f0257d4f04353705e2",
          "name": "Zinan Tang",
          "hidden": false
        },
        {
          "_id": "6875e5f0257d4f04353705e3",
          "name": "H. Vicky Zhao",
          "hidden": false
        },
        {
          "_id": "6875e5f0257d4f04353705e4",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "6875e5f0257d4f04353705e5",
          "name": "Lijun Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-14T17:58:47.000Z",
      "submittedOnDailyAt": "2025-07-15T05:04:35.807Z",
      "title": "REST: Puede solicitar múltiples problemas al mismo tiempo, lo que permite realizar pruebas de estres de gran escala en modelos lógicos complejos de manera efectiva.",
      "submittedOnDailyBy": {
        "_id": "66580d3d80ee5b1e11a94e57",
        "avatarUrl": "/avatars/1a88e7337f9095c40c6d402fab797d83.svg",
        "isPro": false,
        "fullname": "Zinan Tang",
        "user": "Word2Li",
        "type": "user"
      },
      "summary": "Recientemente, los grandes modelos lógicos (LRMs) han logrado sorprendentes resultados en marcos de evaluación para ciertos tareas, pero están limitados por métodos de evaluación que siguen un paradigma de resolución de problemas aislados. Los actuales marcos de evaluación evalúan modelos lógicos de preguntas individuales mediante pruebas secuenciales, y presentan las siguientes limitaciones importantes: 1) son vulnerables a la contaminación de datos y se encuentran en estado de poca variedad (por ejemplo, DeepSeek-R1 alcanza el 97.0% en MATH500), lo que requiere un gran esfuerzo humano para generar nuevas preguntas, y 2) fallan en evaluar modelos en presión de contextos multi-contextuales que son necesarios para su introducción en el mundo real. Para mejorar esta situación, proponemos la prueba simultánea para la evaluación lógica (REST). REST evalúa a modelos lógicos más avanzados que los básicos, considerando la asignación de prioridades en el contexto, la resistencia a la interferencia entre problemas y la gestión de cargas cognitivas dinámicas. Los resultados de la evaluación muestran que, incluso el modelo más avanzado, DeepSeek-R1, experimenta un descenso significativo en su rendimiento bajo prueba de estrés. Es importante destacar que REST muestra una mayor capacidad de discriminación que los actuales marcos de evaluación y muestra claras diferencias en el rendimiento entre modelos que se acercan a los límites de rendimiento. De la análisis se derivan los siguientes puntos mecánicos: 1) \"El vacío de pensamiento excesivo\" es una causa importante de la pérdida de rendimiento; 2) modelos entrenados con \"long-short\" tecnologías mantienen mejor la precisión en el rendimiento de problemas individuales y superan los modelos entrenados de manera estándar en REST. Estos resultados indican que REST es un patrón de evaluación futuro que es costo-eficiente, refleja más precisamente las exigencias lógicas del mundo real y reduce la dependencia continua de anotaciones humanas.",
      "upvotes": 14,
      "discussionId": "6875e5f0257d4f04353705e6",
      "projectPage": "https://opendatalab.github.io/REST/",
      "githubRepo": "https://github.com/opendatalab/REST",
      "ai_summary": "REST evaluates large reasoning models under simultaneous multi-context pressure, revealing performance differences not apparent in single-question tests and highlighting the importance of contextual priority allocation and cognitive load management.",
      "ai_keywords": [
        "Large Reasoning Models",
        "REST",
        "stress-testing framework",
        "contextual priority allocation",
        "cross-problem interference resistance",
        "dynamic cognitive load management",
        "overthinking trap",
        "long2short technique"
      ],
      "githubStars": 14
    },
    "publishedAt": "2025-07-14T13:58:47.000Z",
    "title": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems\n  at Once",
    "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual\ncreation of new questions with large human efforts, (2) failure to evaluate\nmodels under multi-context pressure, a key requirement for real-world\ndeployment. To bridge this gap, we present REST (Reasoning Evaluation through\nSimultaneous Testing), a stress-testing framework that concurrently exposes\nLRMs to multiple problems simultaneously. Beyond basic reasoning, REST\nspecifically evaluates several under-tested capabilities: contextual priority\nallocation, cross-problem interference resistance, and dynamic cognitive load\nmanagement. Our evaluation reveals several striking findings: Even\nstate-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance\ndegradation under stress testing. Crucially, REST demonstrates stronger\ndiscriminative power than existing benchmarks, revealing pronounced performance\ndifferences among models that exhibit similar, near-ceiling performance under\nsingle-question evaluations. Some key mechanistic insights emerge from our\nanalysis: (1) the \"overthinking trap\" is a critical factor contributing to the\nperformance degradation; (2) the models trained with \"long2short\" technique\npreserve more accuracy of their single-problem performance under REST,\noutperforming standard-trained counterparts. These results establish REST as a\ncost-efficient, future-proof evaluation paradigm that better reflects\nreal-world reasoning demands while reducing reliance on continuous human\nannotation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10541.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66580d3d80ee5b1e11a94e57",
      "avatarUrl": "/avatars/1a88e7337f9095c40c6d402fab797d83.svg",
      "fullname": "Zinan Tang",
      "name": "Word2Li",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.10524",
      "authors": [
        {
          "_id": "6875e531257d4f04353705d1",
          "name": "Sangmin Bae",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d2",
          "name": "Yujin Kim",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d3",
          "name": "Reza Bayat",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d4",
          "name": "Sungnyun Kim",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d5",
          "name": "Jiyoun Ha",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d6",
          "name": "Tal Schuster",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d7",
          "name": "Adam Fisch",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d8",
          "name": "Hrayr Harutyunyan",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705d9",
          "name": "Ziwei Ji",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705da",
          "name": "Aaron Courville",
          "hidden": false
        },
        {
          "_id": "6875e531257d4f04353705db",
          "name": "Se-Young Yun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-14T17:49:00.000Z",
      "submittedOnDailyAt": "2025-07-15T03:52:43.642Z",
      "title": "Mixture-of-Recursions: Adaptación del cálculo de niveles de tokens dinámicos en función de la profundidad de la recursión",
      "submittedOnDailyBy": {
        "_id": "6602ca1e10a1441af41637be",
        "avatarUrl": "/avatars/5880e699def320beb352cbed77495b2f.svg",
        "isPro": false,
        "fullname": "Sangmin Bae",
        "user": "raymin0223",
        "type": "user"
      },
      "summary": "El modelo de escalabilidad de larga ruta desarrolla una excelente capacidad, pero los altos costos asociados con el cálculo y la memoria son un problema para el entrenamiento y la implementación. Los esfuerzos actuales para mejorar la eficiencia se centran generalmente en parámetros compartidos y cálculos adaptativos, pero no se han encontrado métodos para lograr ambos objetivos simultáneamente. Presentamos Mixture-of-Recursions (MoR), una estructura que integra dos ejes de eficiencia en el Transformer Recursivo. MoR reutiliza capas en una pila compartida en cada paso de recursión para optimizar los parámetros y utiliza un router ligero que asigna diferentes profundidades de recursión dinámicamente para cada token, permitiendo una acción de pensamiento adaptativa a nivel de token. De esta manera, MoR se centra en los cálculos de propietario solo entre los tokens activos en una profundidad de recursión específica y cache solo las parejas Key-Value necesarias, mejorando así la eficiencia de acceso a la memoria. Además, proponemos variantes de compartir Key-Value. Estas parejas se reutilizan desde la primera recursión, reduciendo especialmente la profundidad de profundidad y el peso de la memoria. Con un rango de tamaños del modelo de 135M a 1.7B parámetros, MoR crea una nueva parada de patrones: un FLOP de entrenamiento igual y un tamaño de modelo pequeño, reduciendo tanto el FLOP de entrenamiento como el tamaño del modelo.",
      "upvotes": 13,
      "discussionId": "6875e531257d4f04353705dc",
      "githubRepo": "https://github.com/raymin0223/mixture_of_recursions",
      "ai_summary": "Mixture-of-Recursions (MoR) achieves parameter and computational efficiency in large language models through shared layers and adaptive recursion depths, improving performance metrics and throughput.",
      "ai_keywords": [
        "Mixture-of-Recursions",
        "MoR",
        "Recursive Transformer",
        "parameter efficiency",
        "adaptive computation",
        "lightweight routers",
        "token-level thinking",
        "recursion depth",
        "quadratic attention computation",
        "key-value pairs",
        "KV sharing",
        "prefill latency",
        "memory footprint",
        "validation perplexity",
        "few-shot accuracy",
        "throughput"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-07-14T13:49:00.000Z",
    "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
    "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10524.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602ca1e10a1441af41637be",
      "avatarUrl": "/avatars/5880e699def320beb352cbed77495b2f.svg",
      "fullname": "Sangmin Bae",
      "name": "raymin0223",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.09104",
      "authors": [
        {
          "_id": "6875bfaa257d4f0435370564",
          "name": "Taolin Zhang",
          "hidden": false
        },
        {
          "_id": "6875bfaa257d4f0435370565",
          "name": "Maosong Cao",
          "hidden": false
        },
        {
          "_id": "6875bfaa257d4f0435370566",
          "name": "Alexander Lam",
          "hidden": false
        },
        {
          "_id": "6875bfaa257d4f0435370567",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "6875bfaa257d4f0435370568",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-12T01:34:24.000Z",
      "submittedOnDailyAt": "2025-07-15T02:47:17.884Z",
      "title": "CompassJudger-2: Verifica la dirección del modelo de juicio general basado en la posibilidad de recompensa.",
      "submittedOnDailyBy": {
        "_id": "630716d11801ecc7d2595021",
        "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
        "isPro": false,
        "fullname": "Songyang Zhang",
        "user": "zsytony",
        "type": "user"
      },
      "summary": "Recientemente, LLM-as-judge ha desempeñado un papel importante en la evaluación de modelos de lenguaje. Sin embargo, los modelos de juicio actuales tienen una especialización restringida y limitada robustez, y su capacidad de evaluación general está disminuida. En este artículo, presentamos un nuevo modelo de juicio general, CompassJudger-2, que supera estas limitaciones mediante la estrategia de cartera de datos multidisciplinarios que lideran las tareas. El enfoque clave de nuestro trabajo es centrar en tareas de juicio con recompensas verificables, continuando con la crítica crítica a través del sampling de muestras rechazadas, y fomentando la capacidad de juicio robusta y generalizable. Además, presentamos una mejora en el objetivo de entrenamiento utilizando la técnica de gradiente de margen de la política de aprendizaje. Experimentalmente, CompassJudger-2 obtiene resultados superiores en varios benchmarks de juicio, y nuestro modelo de 7B compite con modelos grandes como DeepSeek-V3 y Qwen3-235B-A22B en términos de precisión de juicio. Además, proponemos un benchmark detallado llamado JudgerBenchV2 para evaluar la consistencia de precisión y clasificación en diferentes dominios, con el objetivo de estandarizar la evaluación de modelos de juicio. Esta contribución aporta a la evaluación robusta y escalable de modelos de LLM, estableciendo nuevos estándares de rendimiento y evaluación.",
      "upvotes": 12,
      "discussionId": "6875bfaa257d4f0435370569",
      "githubRepo": "https://github.com/open-compass/CompassJudger",
      "ai_summary": "CompassJudger-2, a generalist judge model, achieves superior performance across multiple benchmarks through task-driven data curation, verifiable rewards, and a refined learning objective with margin policy gradient loss.",
      "ai_keywords": [
        "LLM-as-judge",
        "generalist judge model",
        "task-driven",
        "multi-domain data curation",
        "verifiable rewards",
        "rejection sampling",
        "margin policy gradient loss",
        "JudgerBenchV2",
        "cross-domain judgment accuracy",
        "rank consistency"
      ],
      "githubStars": 98
    },
    "publishedAt": "2025-07-11T21:34:24.000Z",
    "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards",
    "summary": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09104.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630716d11801ecc7d2595021",
      "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
      "fullname": "Songyang Zhang",
      "name": "zsytony",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.10065",
      "authors": [
        {
          "_id": "6875ed50257d4f04353705f1",
          "name": "Chenguo Lin",
          "hidden": false
        },
        {
          "_id": "6875ed50257d4f04353705f2",
          "name": "Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "6875ed50257d4f04353705f3",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "6875ed50257d4f04353705f4",
          "name": "Yifan Yu",
          "hidden": false
        },
        {
          "_id": "6875ed50257d4f04353705f5",
          "name": "Honglei Yan",
          "hidden": false
        },
        {
          "_id": "6875ed50257d4f04353705f6",
          "name": "Katerina Fragkiadaki",
          "hidden": false
        },
        {
          "_id": "6875ed50257d4f04353705f7",
          "name": "Yadong Mu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/SYUoEzWMnM6UPGTKHPqzn.mp4"
      ],
      "publishedAt": "2025-07-14T08:49:57.000Z",
      "submittedOnDailyAt": "2025-07-15T04:26:29.071Z",
      "title": "Movies: 4D Dynamic Visual Synthesis with Interest in Action in 1 Second",
      "submittedOnDailyBy": {
        "_id": "62e18206926f4892a4c782bd",
        "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
        "isPro": false,
        "fullname": "Chenguo Lin",
        "user": "chenguolin",
        "type": "user"
      },
      "summary": "MovieS es un nuevo modelo de orientación que synthetiza una nueva visión dinámica 4D cada segundo, desarrollado desde el video monocular. MovieS utiliza primitivas gaussianas para representar espacios 3D dinámicos, controlando explícitamente la evolución temporal del movimiento. Esto permite un modelado integral de apariencia, forma y movimiento, lo que permite realizar en un marco de trabajo basado en aprendizaje la síntesis de perspectivas, reconstrucción de formas y el rastreo de puntos 3D. Al conectar la síntesis de perspectivas con la reconstrucción dinámica de formas, MovieS minimiza la dependencia para controlar tareas complejas, proporcionando un soporte natural para aplicaciones de 0 shot en áreas amplias, como la predicción del flujo de perspectiva y la segmentación dinámica de objetos. Se ha demostrado su efectividad y eficiencia en varios desarrollos, ofreciendo mejoras en rendimiento y un aumento de velocidad en varias ocasiones.",
      "upvotes": 6,
      "discussionId": "6875ed51257d4f04353705f8",
      "projectPage": "https://chenguolin.github.io/projects/MoVieS",
      "githubRepo": "https://github.com/chenguolin/MoVieS",
      "ai_summary": "MoVieS synthesizes 4D dynamic novel views from monocular videos using Gaussian primitives, enabling unified modeling of appearance, geometry, and motion with minimal task-specific supervision.",
      "ai_keywords": [
        "feed-forward model",
        "Gaussian primitives",
        "time-varying motion",
        "view synthesis",
        "reconstruction",
        "3D point tracking",
        "scene flow estimation",
        "moving object segmentation"
      ],
      "githubStars": 35
    },
    "publishedAt": "2025-07-14T04:49:57.000Z",
    "title": "MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second",
    "summary": "We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic\nnovel views from monocular videos in one second. MoVieS represents dynamic 3D\nscenes using pixel-aligned grids of Gaussian primitives, explicitly supervising\ntheir time-varying motion. This allows, for the first time, the unified\nmodeling of appearance, geometry and motion, and enables view synthesis,\nreconstruction and 3D point tracking within a single learning-based framework.\nBy bridging novel view synthesis with dynamic geometry reconstruction, MoVieS\nenables large-scale training on diverse datasets with minimal dependence on\ntask-specific supervision. As a result, it also naturally supports a wide range\nof zero-shot applications, such as scene flow estimation and moving object\nsegmentation. Extensive experiments validate the effectiveness and efficiency\nof MoVieS across multiple tasks, achieving competitive performance while\noffering several orders of magnitude speedups.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/SYUoEzWMnM6UPGTKHPqzn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10065.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "62e18206926f4892a4c782bd",
      "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
      "fullname": "Chenguo Lin",
      "name": "chenguolin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08924",
      "authors": [
        {
          "_id": "6875aa3c257d4f043537052c",
          "name": "Seokhee Hong",
          "hidden": false
        },
        {
          "_id": "6875aa3c257d4f043537052d",
          "name": "Sunkyoung Kim",
          "hidden": false
        },
        {
          "_id": "6875aa3c257d4f043537052e",
          "name": "Guijin Son",
          "hidden": false
        },
        {
          "_id": "6875aa3c257d4f043537052f",
          "name": "Soyeon Kim",
          "hidden": false
        },
        {
          "_id": "6875aa3c257d4f0435370530",
          "name": "Yeonjung Hong",
          "hidden": false
        },
        {
          "_id": "6875aa3c257d4f0435370531",
          "name": "Jinsik Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:56:32.000Z",
      "submittedOnDailyAt": "2025-07-15T05:31:41.102Z",
      "title": "KMMLU-Redux a KMMLU-Pro: Sistema de referencia profesional para evaluar LLM en Coreano",
      "submittedOnDailyBy": {
        "_id": "60d3e619b8448e1785bbda2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
        "isPro": false,
        "fullname": "GUIJIN SON",
        "user": "amphora",
        "type": "user"
      },
      "summary": "El desarrollo de un LLM requiere fortalecerse tanto en la academia como en el ámbito industrial a través de fuertes marcos de referencia. En este artículo, se presentan dos marcos de referencia de nivel experto en coreano. KMMLU-Redux es una reconstrucción de KMMLU actual. Esto se ha realizado utilizando problemas que surgieron en los exámenes nacionales de tecnología, eliminando errores importantes y mejorando la confianza. KMMLU-Pro se basa en los exámenes nacionales de profesionales coreanos, reflejando con precisión la conocida en el ámbito profesional coreano. Los resultados de los experimentos indican que estos marcos de referencia representan ampliamente el conocimiento industrial coreano. Nuestro conjunto de datos está disponible para uso público.",
      "upvotes": 1,
      "discussionId": "6875aa3c257d4f0435370532",
      "ai_summary": "Korean expert-level benchmarks, KMMLU-Redux and KMMLU-Pro, are introduced to evaluate Large Language Models across academic and industrial domains in Korea.",
      "ai_keywords": [
        "Large Language Models",
        "LLMS",
        "benchmarks",
        "KMMLU-Redux",
        "KMMLU-Pro",
        "Korean National Technical Qualification exams",
        "Korean National Professional Licensure exams",
        "industrial knowledge"
      ]
    },
    "publishedAt": "2025-07-11T13:56:32.000Z",
    "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for\n  LLM Evaluation",
    "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08924.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d3e619b8448e1785bbda2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
      "fullname": "GUIJIN SON",
      "name": "amphora",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 57
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08267",
      "authors": [
        {
          "_id": "6875e45f257d4f04353705cc",
          "name": "Hiroshi Yoshihara",
          "hidden": false
        },
        {
          "_id": "6875e45f257d4f04353705cd",
          "name": "Taiki Yamaguchi",
          "hidden": false
        },
        {
          "_id": "6875e45f257d4f04353705ce",
          "name": "Yuichi Inoue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T02:26:01.000Z",
      "submittedOnDailyAt": "2025-07-15T06:15:48.021Z",
      "title": "Los algoritmos prácticos de 2 etapas para los LLMs de matemática: maximizar la precisión con SFT y optimizar la eficiencia con aprendizaje por refuerzo",
      "submittedOnDailyBy": {
        "_id": "63233b16462470712718c2a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1663253229258-noauth.png",
        "isPro": false,
        "fullname": "Inoue Yuichi",
        "user": "Inoichan",
        "type": "user"
      },
      "summary": "El desarrollo de grandes modelos de lenguaje (LLMs) para mejorar la inferencia matemática es un problema importante en la mejora de las capacidades de la inteligencia artificial. El paradigma de entrenamiento está dominado por el aprendizaje supervisado (SFT) y el aprendizaje por refuerzo (RL), pero los métodos sistemáticos que combinan estos para maximizar precisión y eficiencia se están explorando. En este artículo, se presenta un método efectivo y práctico de entrenamiento que combina de manera estratégica la extensión del SFT y el aprendizaje por refuerzo (GRPO). Este método complementa mutuamente, y el entrenamiento prolongado del SFT puede elevar a la precisión del modelo hasta sus límites, mientras que el GRPO en las etapas subsiguientes mejora significativamente la eficiencia de los tokens, manteniendo así un excelente rendimiento. Los experimentos confirman claramente que el entrenamiento prolongado del SFT es crucial para un gran avance en el rendimiento, y el GRPO desempeña un papel principal en la optimización de la longitud de las decisiones. El efecto de este método ha sido rigurosamente validado en difíciles benchmarks, y el modelo ha obtenido un alto rango en la AI Mathematical Olympiad (AIMO), entrenando a más de 2,200 equipos. Esta investigación ofrece una plan realista para el desarrollo de modelos de inferencia matemática líder en precisión y eficiencia práctica a la comunidad. Para garantizar la reproducibilidad y el desarrollo futuro de la investigación, se publican un marco abierto, todos los códigos, los puntos de control del modelo y las configuraciones de entrenamiento: https://github.com/analokmaus/kaggle-aimo2-fast-math-r1",
      "upvotes": 1,
      "discussionId": "6875e45f257d4f04353705cf",
      "ai_summary": "A combination of extended supervised fine-tuning and reinforcement learning from online inference enhances the mathematical reasoning capabilities of large language models, achieving top-tier performance on benchmarks like the AI Mathematical Olympiad.",
      "ai_keywords": [
        "Supervised Fine-Tuning",
        "Reinforcement Learning",
        "GRPO",
        "token efficiency",
        "solution length optimization",
        "AI Mathematical Olympiad"
      ]
    },
    "publishedAt": "2025-07-10T22:26:01.000Z",
    "title": "A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy\n  with SFT and Efficiency with Reinforcement Learning",
    "summary": "Enhancing the mathematical reasoning of Large Language Models (LLMs) is a\npivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning\n(SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a\nsystematic methodology for combining them to maximize both accuracy and\nefficiency remains largely unexplored. This paper introduces a practical and\neffective training recipe that strategically integrates extended SFT with RL\nfrom online inference (GRPO). We posit that these methods play complementary,\nnot competing, roles: a prolonged SFT phase first pushes the model's accuracy\nto its limits, after which a GRPO phase dramatically improves token efficiency\nwhile preserving this peak performance. Our experiments reveal that extending\nSFT for as many as 10 epochs is crucial for performance breakthroughs, and that\nthe primary role of GRPO in this framework is to optimize solution length. The\nefficacy of our recipe is rigorously validated through top-tier performance on\nchallenging benchmarks, including a high rank among over 2,200 teams in the\nstrictly leak-free AI Mathematical Olympiad (AIMO). This work provides the\ncommunity with a battle-tested blueprint for developing state-of-the-art\nmathematical reasoners that are both exceptionally accurate and practically\nefficient. To ensure full reproducibility and empower future research, we will\nopen-source our entire framework, including all code, model checkpoints, and\ntraining configurations at\nhttps://github.com/analokmaus/kaggle-aimo2-fast-math-r1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08267.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63233b16462470712718c2a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1663253229258-noauth.png",
      "fullname": "Inoue Yuichi",
      "name": "Inoichan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  }
]