[
  {
    "paper": {
      "id": "2502.01237",
      "authors": [
        {
          "_id": "67a1c1428747511e7b9a1965",
          "user": {
            "_id": "62897fce5d9e25c10e4f319d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg",
            "isPro": false,
            "fullname": "Alexey Gorbatovski",
            "user": "Myashka",
            "type": "user"
          },
          "name": "Alexey Gorbatovski",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:00.767Z",
          "hidden": false
        },
        {
          "_id": "67a1c1428747511e7b9a1966",
          "name": "Boris Shaposhnikov",
          "hidden": false
        },
        {
          "_id": "67a1c1428747511e7b9a1967",
          "user": {
            "_id": "6416272d986557e8cac64ece",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6416272d986557e8cac64ece/s3CLjNN_pGj-vJDcENFD2.jpeg",
            "isPro": false,
            "fullname": "Viacheslav",
            "user": "ummagumm-a",
            "type": "user"
          },
          "name": "Viacheslav Sinii",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:38:52.039Z",
          "hidden": false
        },
        {
          "_id": "67a1c1428747511e7b9a1968",
          "user": {
            "_id": "636e71b2b0ebc04888157b71",
            "avatarUrl": "/avatars/957ba705d470e3a01792741d7f0ff038.svg",
            "isPro": false,
            "fullname": "Alexey Malakhov",
            "user": "ZeL1k7",
            "type": "user"
          },
          "name": "Alexey Malakhov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:38:54.121Z",
          "hidden": false
        },
        {
          "_id": "67a1c1428747511e7b9a1969",
          "user": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "isPro": false,
            "fullname": "Daniil Gavrilov",
            "user": "kefirski",
            "type": "user"
          },
          "name": "Daniil Gavrilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:38:57.087Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T10:54:14.000Z",
      "title": "Los diferencias en el algoritmo de disposición en línea son similares a las del algoritmo de búsqueda.",
      "summary": "Los algoritmos de alineamiento directo (DAAs) sustituyen el aprendizaje de retroalimentación humana (RLHF) y la modelación de recompensas (RM) en aprendizaje de refuerzo (RL) directamente por la optimización de políticas para simplificar la calibración de modelos de lenguaje. Los DAAs se clasifican en función de la pérdida de clasificación (ranking, tanto de tipo pareja vs. puntual) y la recompensa utilizada para esta pérdida (por ejemplo, el razonamiento de probabilidades entre la política y la política de referencia) o si se necesita un paso de ajuste de aprendizaje supervisado (SFT, tanto en dos etapas vs. una etapa). Primero, se muestra que los métodos de una etapa tienen un rendimiento inferior a los métodos de dos etapas. Se aplican a los métodos ORPO y ASFT de una etapa, incluyendo un paso explícito de SFT y la introducción del parámetro beta para controlar la intensidad de la optimización de la preferencia de la política, lo que mejora los resultados en Alpaca Eval 2 con un +3.46 (ORPO) y un +8.27 (ASFT), demostrando un rendimiento comparable a los métodos de dos etapas como DPO. Además, el análisis muestra que el uso de objetivos de puntual o de clasificación de parejas puede no ser igualmente efectivo para todas las funciones de recompensa o pérdida, lo que subraya la importancia de una evaluación precisa para evitar exagerados aumentos en el rendimiento o la afirmación de la excelencia general de un algoritmo de calibración.",
      "upvotes": 33,
      "discussionId": "67a1c1438747511e7b9a19ae"
    },
    "publishedAt": "2025-02-04T03:10:49.348Z",
    "title": "The Differences Between Direct Alignment Algorithms are a Blur",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62897fce5d9e25c10e4f319d/ndKErkZSfT5LvqKfIrC7f.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01237.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "62897fce5d9e25c10e4f319d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg",
      "fullname": "Alexey Gorbatovski",
      "name": "Myashka",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01456",
      "authors": [
        {
          "_id": "67a19d705efa4fab15497775",
          "user": {
            "_id": "650eba9555dc1e841746f132",
            "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
            "isPro": false,
            "fullname": "Ganqu Cui",
            "user": "ganqu",
            "type": "user"
          },
          "name": "Ganqu Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:23.889Z",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497776",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497777",
          "name": "Zefan Wang",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497778",
          "user": {
            "_id": "6321152b8c0da827c72c7c16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678783813705-6321152b8c0da827c72c7c16.jpeg",
            "isPro": false,
            "fullname": "Hanbin Wang",
            "user": "hanbin",
            "type": "user"
          },
          "name": "Hanbin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:25.869Z",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497779",
          "name": "Wendi Li",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777a",
          "name": "Bingxiang He",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777b",
          "name": "Yuchen Fan",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777c",
          "name": "Tianyu Yu",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777d",
          "name": "Qixin Xu",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777e",
          "name": "Weize Chen",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549777f",
          "name": "Jiarui Yuan",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497780",
          "name": "Huayu Chen",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497781",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497782",
          "name": "Xingtai Lv",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497783",
          "name": "Shuo Wang",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497784",
          "name": "Yuan Yao",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497785",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497786",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497787",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497788",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab15497789",
          "name": "Maosong Sun",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549778a",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "67a19d705efa4fab1549778b",
          "name": "Ning Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T15:43:48.000Z",
      "title": "El entrenamiento por recompensas ocultas",
      "summary": "Los recompensas densas han demostrado ser efectivas como sustitutos de las recompensas escasas en la escalación de modelos de lenguaje grandes (LLMs) para tareas que requieren lógica multinivel compleja, en comparación con los niveles de resultado de las recompensas escasas. Sin embargo, también son atractivas en la aprendizaje por refuerzo (RL) de los LLMs, ya que resuelven problemas fundamentales de las recompensas de resultado. Sin embargo, esta posibilidad aún no se ha alcanzado de manera significativa. Esto se debe a que la recopilación de etiquetas de proceso de alta calidad en el entrenamiento en línea de modelos de recompensas de proceso (PRMs) es costosa y vulnerable al \"piling on\" de recompensas. Para resolver estos problemas, se propone PRIME (Aprendizaje por Refuerzo con Recompensas Implicitas). PRIME permite actualizar en línea los PRMs utilizando políticas de rollout y etiquetas de resultado. PRIME omite el paso de entrenamiento de modelos de recompensas especializadas necesarios en los métodos actuales, reduciendo significativamente el sobrecarga del dispositivo. Los efectos de PRIME se muestran en matemáticas y programación. A partir de Qwen2.5-Math-7B-Base, PRIME puede superar modelos de SFT. En particular, nuestro modelo Eurus-2-7B-PRIME supera a Qwen2.5-Math-7B-Instruct, utilizando solo el 10% de los datos de entrenamiento.",
      "upvotes": 26,
      "discussionId": "67a19d705efa4fab154977d0"
    },
    "publishedAt": "2025-02-04T00:02:39.922Z",
    "title": "Process Reinforcement through Implicit Rewards",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01456.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6321152b8c0da827c72c7c16",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678783813705-6321152b8c0da827c72c7c16.jpeg",
      "fullname": "Hanbin Wang",
      "name": "hanbin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01534",
      "authors": [
        {
          "_id": "67a1ad77d797fac51fa80770",
          "name": "Dawei Li",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80771",
          "user": {
            "_id": "653a195b0da86d726c9c580c",
            "avatarUrl": "/avatars/61649e1d600fdc1edc50ead0dfa99fdd.svg",
            "isPro": false,
            "fullname": "Renliang Sun",
            "user": "RLSNLP",
            "type": "user"
          },
          "name": "Renliang Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:11.035Z",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80772",
          "name": "Yue Huang",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80773",
          "name": "Ming Zhong",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80774",
          "name": "Bohan Jiang",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80775",
          "name": "Jiawei Han",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80776",
          "name": "Xiangliang Zhang",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80777",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67a1ad77d797fac51fa80778",
          "name": "Huan Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T17:13:03.000Z",
      "title": "Bias Leakage: The Problem of Contamination in LLM-as-a-Judge",
      "summary": "Los lenguajes grandes (LLMs) han adquirido un papel fundamental en el desarrollo de modelos, y la síntesis de datos basada en LLMs ha sido uno de los dos principales métodos de explicación de datos liderados por los LLMs. Esta combinación ha contribuido significativamente al mejorar la eficiencia en el entrenamiento y evaluación de los modelos, pero ha recibido poca atención en relación con los potenciales problemas de contaminación que surgen de este nuevo paradigma de desarrollo de modelos. En este artículo, se analizan los problemas de contaminación causados por la sesgo de los jurados como evaluadores y la relación entre los generadores de datos basados en LLMs y los evaluadores. Para estudiar estos problemas, se definen tres tipos de relaciones comunes entre los modelos de generación de datos y los modelos de evaluación: el mismo modelo, una relación de herencia o pertenecer al mismo conjunto de modelos. Mediante experimentos extendidos, se ha confirmado experimentalmente que el sesgo de la evaluación puede ser reconocido cuando el modelo de evaluación es sesgado por la preferencia del evaluador. El análisis realizado muestra que el sesgo de la preferencia es un problema amplio que es difícil de detectar comparado con el sesgo de los modelos evaluados por jurados anteriormente. Todos estos hallazgos indican que el sesgo de la preferencia es un problema que es difícil de resolver en el ámbito de los modelos evaluados por jurados. Todo el código y los datos están disponibles en la siguiente URL: https://github.com/David-Li0406/Preference-Leakage.",
      "upvotes": 11,
      "discussionId": "67a1ad78d797fac51fa807c1"
    },
    "publishedAt": "2025-02-04T01:04:33.630Z",
    "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01534.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6474e1afb68461d5cf7c41cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
      "fullname": "Dawei Li",
      "name": "wjldw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01061",
      "authors": [
        {
          "_id": "67a1a7a166a8a88726963ef4",
          "name": "Gaojie Lin",
          "hidden": false
        },
        {
          "_id": "67a1a7a166a8a88726963ef5",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "67a1a7a166a8a88726963ef6",
          "name": "Jiaqi Yang",
          "hidden": false
        },
        {
          "_id": "67a1a7a166a8a88726963ef7",
          "name": "Zerong Zheng",
          "hidden": false
        },
        {
          "_id": "67a1a7a166a8a88726963ef8",
          "name": "Chao Liang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T05:17:32.000Z",
      "title": "OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1: OmniHuman-1",
      "summary": "El desarrollo notable en la animación humana, como el caso de la generación de personas bajo el efecto de la embriagada en la tabla, ha mostrado progresos significativos en los últimos años. Sin embargo, los métodos actuales no se pueden expandir para modelos de video de gran escala, lo que limita su aplicabilidad en campos reales. En este artículo, se propone un marco escalable para extender la base de datos efectivamente, mezclando condiciones relacionadas con el movimiento en la etapa de aprendizaje. Esto permite introducir dos principios de aprendizaje relacionados con el movimiento y proponer una estructura de modelo y estrategias de inferencia correspondientes. Este diseño permite que OmniHuman maximice el uso de la generación de movimientos de datos para crear videos de alta calidad de personas. Lo más importante es que OmniHuman soporta contenidos visuales de diferentes tipos, como caras, fotos, half-body y completos, así como diálogos y canciones, y también procesa interacciones entre personas y objetos y estados corporales complejos, además de tratar diferentes estilos de imágenes. En comparación con los métodos actuales de embriagada, OmniHuman genera videos realistas y tiene mayor flexibilidad en los entradas. Además, soporta múltiples modos de embriagada (embriagada, video, señales combinadas). Los ejemplos de videos están disponibles en la página de proyecto del ttfamily (https://omnihuman-lab.github.io).",
      "upvotes": 11,
      "discussionId": "67a1a7a466a8a88726963f90"
    },
    "publishedAt": "2025-02-04T00:37:57.949Z",
    "title": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01061.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18636",
      "authors": [
        {
          "_id": "67a1bfc314cba2eba6da4b2b",
          "name": "Xun Liang",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b2c",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b2d",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b2e",
          "name": "Sensen Zhang",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b2f",
          "user": {
            "_id": "669e0b93c7cb0568dac6e92e",
            "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
            "isPro": false,
            "fullname": "hanyu Wang",
            "user": "UglyToilet",
            "type": "user"
          },
          "name": "Hanyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:04.452Z",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b30",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b31",
          "name": "Jason Zhaoxin Fan",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b32",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b33",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b34",
          "name": "Mengwei Wang",
          "hidden": false
        },
        {
          "_id": "67a1bfc314cba2eba6da4b35",
          "name": "Jiawei Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T17:01:31.000Z",
      "title": "SafeRAG: Seguridad RAG en el marco de un marco de referencia de seguridad",
      "summary": "El paradigma de generación con registros (RAG) en el dominio de la búsqueda y generación de registros ha tenido un gran éxito al integrar un gran modelo de lenguaje (LLMs) con un banco de datos externo para resolver tareas de alto contenido de conocimiento. Sin embargo, la integración con un banco de datos externo y un banco de datos no confirmado aumenta la fragilidad de los LLMs, permitiendo a los atacantes manipular el conocimiento para realizar tareas de ataque. En este trabajo, se utiliza el marco de referencia \"SafeRAG\" para evaluar la seguridad de RAG. Primero, se clasifican las tareas de ataque en ruido dorado, conflictos entre contextos, adware suave, y servicios blancos de denial. Luego, se construye manualmente el conjunto de datos de evaluación de seguridad de RAG (o SafeRAG) para cada tarea de ataque. Finalmente, se simulan los diversos escenarios de ataque que pueden enfrentarse por parte de RAG utilizando el conjunto de datos SafeRAG. Se realizan experimentos con 14 componentes representativos de RAG, demostrando que RAG es vulnerable a todas las tareas de ataque y que la tarea de ataque más clara es pasar fácilmente a través de un módulo de búsqueda de registros existente, un filtro o un LLM avanzado, lo que reduce la calidad de los servicios de RAG. El código está disponible en la siguiente URL: https://github.com/IAAR-Shanghai/SafeRAG.",
      "upvotes": 8,
      "discussionId": "67a1bfc414cba2eba6da4b63"
    },
    "publishedAt": "2025-02-04T03:22:06.520Z",
    "title": "SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18636.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62a155e615eeab266b2f2243",
      "avatarUrl": "/avatars/e89ef156e73af028e3ce3664e6cb4e62.svg",
      "fullname": "Zhiyu Li",
      "name": "jimi888",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01068",
      "authors": [
        {
          "_id": "67a1a75f6aa8429da4945eeb",
          "user": {
            "_id": "639ffbc6beb95d698de9640d",
            "avatarUrl": "/avatars/7ef1aaadd5b378d00e17dc548e42cb7e.svg",
            "isPro": false,
            "fullname": "Dongwon Jo",
            "user": "dongwonjo",
            "type": "user"
          },
          "name": "Dongwon Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:16.125Z",
          "hidden": false
        },
        {
          "_id": "67a1a75f6aa8429da4945eec",
          "user": {
            "_id": "662672eaebdfec5cfdf1d034",
            "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
            "isPro": false,
            "fullname": "Jiwon Song",
            "user": "jiwonsong",
            "type": "user"
          },
          "name": "Jiwon Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:14.253Z",
          "hidden": false
        },
        {
          "_id": "67a1a75f6aa8429da4945eed",
          "name": "Yulhwa Kim",
          "hidden": false
        },
        {
          "_id": "67a1a75f6aa8429da4945eee",
          "name": "Jae-Joon Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T05:25:09.000Z",
      "title": "FastKV: Procesamiento rápido de contextos largos mediante compresión de caché KV y propagación selectiva de tokens",
      "summary": "Los Modelos de Lenguaje Grandes (LLMs) tienen un excelente rendimiento para procesar secuencias de contexto largas, pero requieren una gran caché de valores clave (KV) para almacenar la información contextual, lo que representa una gran carga en términos de eficiencia computacional y uso de memoria. Los intentos previos de compresión de la caché se centraron únicamente en reducir la demanda de memoria, pero estuvieron limitados en mejorar el tiempo de respuesta. Para resolver estos problemas, se presenta FastKV, un método de compresión de caché diseñado para mejorar el tiempo de respuesta en secuencias de contexto largas. Para aumentar la velocidad de procesamiento mientras mantiene la precisión, FastKV mantiene toda la información contextual en las capas iniciales de los LLMs y propaga algunas de ellas de manera selectiva en las capas más profundas, adoptando una nueva aproximación denominada Token Selective Propagation (TSP). Además, FastKV adopta la compresión de la caché de valores clave relacionada con el Proceso de Consultas de Grupo (GQA) y aprovecha los beneficios de memoria y eficiencia computacional de GQA. A través de los resultados experimentales, FastKV logró mejoras significativas frente a los métodos de compresión de caché más avanzados como HeadKV, con un aumento del 2.00 en el TTFT y del 1.40 en el Transtorp. Además, FastKV logró mantener la precisión mientras mejoró el rendimiento en las pruebas de benchmark de contexto largo. El código está disponible en https://github.com/dongwonjo/FastKV.",
      "upvotes": 7,
      "discussionId": "67a1a7616aa8429da4945f95"
    },
    "publishedAt": "2025-02-04T00:45:45.545Z",
    "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01068.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639ffbc6beb95d698de9640d",
      "avatarUrl": "/avatars/7ef1aaadd5b378d00e17dc548e42cb7e.svg",
      "fullname": "Dongwon Jo",
      "name": "dongwonjo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.00094",
      "authors": [
        {
          "_id": "67a185ab908f4534beb94b8c",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:32.712Z",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b8d",
          "name": "Sara Ghaboura",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b8e",
          "name": "Omkar Thawkar",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b8f",
          "name": "Fahad Shahbaz Khan",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b90",
          "name": "Hisham Cholakkal",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b91",
          "name": "Rao Muhammad Anwer",
          "hidden": false
        },
        {
          "_id": "67a185ab908f4534beb94b92",
          "name": "Salman Khan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-31T18:58:20.000Z",
      "title": "AIN: Modelo de Inclusión Árabe para la Diversificación Masiva",
      "summary": "El rápido desarrollo de los LLMs y la evolución de los grandes modelos multimodal (LMMs) en el otro extremo, en lenguajes como el inglés o el chino, se han visto desarrollar notablemente. En contraste, los modelos de LLMs en árabe han avanzado notablemente, pero los LMMs aún no han sido investigados en muchos aspectos, especialmente en aspectos específicos de un idioma o en la comprensión visual. Para llenar este vacío, los modelos monomodales en árabe aún no han sido investigados. El modelo monomodal en árabe (AIN) aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AIN es un modelo monomodal en árabe que aún no ha sido investigado. AI",
      "upvotes": 7,
      "discussionId": "67a185b0908f4534beb94c49"
    },
    "publishedAt": "2025-02-03T22:22:44.375Z",
    "title": "AIN: The Arabic INclusive Large Multimodal Model",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mmf9V_8rdsi9hN-QdFZV8.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/uLq0E1qq75-P4P1KV4xWF.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/1eixiKjHGNVm6RaJpdWeq.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/XVJSPAgIQcQn8Zi4gUVwi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00094.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01081",
      "authors": [
        {
          "_id": "67a1a56d83c3565727d22f0c",
          "name": "Vernon Y. H. Toh",
          "hidden": false
        },
        {
          "_id": "67a1a56d83c3565727d22f0d",
          "name": "Yew Ken Chia",
          "hidden": false
        },
        {
          "_id": "67a1a56d83c3565727d22f0e",
          "name": "Deepanway Ghosal",
          "hidden": false
        },
        {
          "_id": "67a1a56d83c3565727d22f0f",
          "name": "Soujanya Poria",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T05:47:04.000Z",
      "title": "¡¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿",
      "summary": "La publicación de o1 y o3 de OpenAI marca un cambio de paradigma importante en la mejora de capacidades lógicas avanzadas en modelos de lenguaje grandes. En particular, o3 ha demostrado un rendimiento excepcional en la resolución de nuevos problemas y en la adquisición de nuevas tecnologías en el contexto de la generalización artificial (AGI) y el aprendizaje lógico desde un corpus de texto. Sin embargo, estos modelos están limitados a patrones simbólicos, mientras que la humanidad es capaz de reconocer y aplicar lógica en una variedad de escenarios que incluyen datos visuales y lingüísticos. Por lo tanto, es necesario investigar el desarrollo de diferentes capacidades lógicas en modelos como GPT-[n] y o-[n]. Esto se hace mediante el seguimiento de la evolución de estos modelos y mediante el desafío de resolver puzzles complejos que requieren reconocimiento visual y lógica abstracta o algoritmica. La mejora en la capacidad lógica de o1 es aproximadamente 750 veces más eficiente que GPT-4o, lo que genera dudas sobre su eficiencia. Nuestros resultados muestran un incremento claro en la capacidad lógica durante el proceso de entrenamiento de los modelos, con un salto claro en el rendimiento de GPT-4o a o1. Sin embargo, o1 presenta dificultades en situaciones que requieren lógica abstracta en puzzles con patrones diversos. Además, su rendimiento en puzzles algoritmicos es aún insatisfactorio. Planificamos seguir monitoreando nuevos modelos de esta serie y actualizar los resultados de acuerdo con este artículo. Todos los recursos utilizados en esta evaluación están disponibles para uso público (https://github.com/declare-lab/LLM-PuzzleTest).",
      "upvotes": 5,
      "discussionId": "67a1a57083c3565727d22fc6"
    },
    "publishedAt": "2025-02-04T00:28:35.436Z",
    "title": "The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01081.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01142",
      "authors": [
        {
          "_id": "67a1b4630e9634919de9bc52",
          "user": {
            "_id": "643407dd4b34368fdb0149e8",
            "avatarUrl": "/avatars/9477b9267d5692a4fe59e30590e9639d.svg",
            "isPro": false,
            "fullname": "Xinyan Guan",
            "user": "xinyan233333",
            "type": "user"
          },
          "name": "Xinyan Guan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:08.849Z",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc53",
          "name": "Jiali Zeng",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc54",
          "name": "Fandong Meng",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc55",
          "name": "Chunlei Xin",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc56",
          "name": "Yaojie Lu",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc57",
          "name": "Hongyu Lin",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc58",
          "name": "Xianpei Han",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc59",
          "name": "Le Sun",
          "hidden": false
        },
        {
          "_id": "67a1b4630e9634919de9bc5a",
          "name": "Jie Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T08:22:45.000Z",
      "title": "DeepRAG: Modelo de lenguaje de grandes escalas para considerar etapas de búsqueda de manera iterativa",
      "summary": "Los modelos de lenguaje grande (LLMs) enfrentan desafíos en conversaciones factuales según la temporalidad, precisión y eficiencia de cobertura, demostrando la posibilidad de la posibilidad de argumentación lógica. Por otro lado, la integración de argumentación lógica con generación de búsqueda asociada (RAG) sufre desafíos debido a la división efectiva de tareas y la introducción de ruido y la pérdida de calidad en respuestas debido a búsquedas largas y densas. En este artículo, se modela la lógica de generación de búsqueda como un proceso de decisión de Markov (MDP) y se propone el marco de trabajo DeepRAG para permitir búsquedas estratégicas e adaptativas. DeepRAG divide las preguntas de manera continua y decide de manera dinámica si se realizará búsqueda externa de conocimiento o dependiendo de una lógica paramétrica. Los resultados de los experimentos muestran que DeepRAG mejora la eficiencia de la búsqueda y puede aumentar la precisión de las respuestas en un 21.99%, demostrando su eficacia en la optimización de la lógica de generación de búsqueda asociada.",
      "upvotes": 3,
      "discussionId": "67a1b4640e9634919de9bc8b"
    },
    "publishedAt": "2025-02-04T04:35:57.149Z",
    "title": "DeepRAG: Thinking to Retrieval Step by Step for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643407dd4b34368fdb0149e8",
      "avatarUrl": "/avatars/9477b9267d5692a4fe59e30590e9639d.svg",
      "fullname": "Xinyan Guan",
      "name": "xinyan233333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01100",
      "authors": [
        {
          "_id": "67a1a649f4aecd0dfc96ebf4",
          "user": {
            "_id": "607f666a4ad99100d63ce35c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/607f666a4ad99100d63ce35c/QxhxnvfeV6efkxwUFHwjI.png",
            "isPro": false,
            "fullname": "Bill Yuchen Lin",
            "user": "yuchenlin",
            "type": "user"
          },
          "name": "Bill Yuchen Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:17.972Z",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf5",
          "user": {
            "_id": "635049104e753c9940fefd71",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635049104e753c9940fefd71/HgR43XIFw3dneY5ufrAE8.jpeg",
            "isPro": false,
            "fullname": "Ronan Le Bras",
            "user": "ronanlb",
            "type": "user"
          },
          "name": "Ronan Le Bras",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-04T05:31:56.722Z",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf6",
          "name": "Kyle Richardson",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf7",
          "name": "Ashish Sabharwal",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf8",
          "name": "Radha Poovendran",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebf9",
          "name": "Peter Clark",
          "hidden": false
        },
        {
          "_id": "67a1a649f4aecd0dfc96ebfa",
          "name": "Yejin Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T06:44:49.000Z",
      "title": "Jerrabo-Jia: Límites de la escalabilidad de la racionalidad lógica en modelos de lenguaje grandes (LLMs)",
      "summary": "Introducing ZebraLogic, a detailed evaluation framework, to assess the logical performance of LLMs in Logic Grid Puzzles, which translates Constraint Satisfaction Problems (CSPs) into more complex and quantitative puzzles. ZebraLogic enables the creation of structurally adjustable, quantitatively complex puzzles, thereby promoting systematic research on the scaling limitations of models like Llama, o1, and DeepSeek-R1. By constructing an evaluation environment that considers the complexity of the search space and various logical constraints, and providing a structured environment for evaluating logicality as the difficulty increases.\n\nOur results clearly show that as the complexity of the problem increases, accuracy significantly decreases, a phenomenon we call the \"complexity curse,\" highlighting the inherent limitations in the logical performance of current LLMs. Additionally, we review strategies for improving logical syntax and present methods such as Best-of-N sampling, backtracking structures, and automatic proof prompts. Our findings provide important insights into the scaling of LLM logical performance, reveal fundamental limitations, and offer possibilities for improvement.",
      "upvotes": 3,
      "discussionId": "67a1a64cf4aecd0dfc96ecb8"
    },
    "publishedAt": "2025-02-04T00:32:03.929Z",
    "title": "ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01100.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01441",
      "authors": [
        {
          "_id": "67a189e8fbbab3ce03462fb3",
          "user": {
            "_id": "63e083e6f351dc0745745d17",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e083e6f351dc0745745d17/N0GE4uLrkm14blAQMnm2E.jpeg",
            "isPro": false,
            "fullname": "Quan Dao",
            "user": "quandao10",
            "type": "user"
          },
          "name": "Quan Dao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:30.529Z",
          "hidden": false
        },
        {
          "_id": "67a189e8fbbab3ce03462fb4",
          "name": "Khanh Doan",
          "hidden": false
        },
        {
          "_id": "67a189e8fbbab3ce03462fb5",
          "name": "Di Liu",
          "hidden": false
        },
        {
          "_id": "67a189e8fbbab3ce03462fb6",
          "user": {
            "_id": "66db7db231e772c5ec4c5576",
            "avatarUrl": "/avatars/aa0eb054bd6c881054431a22daf1aea1.svg",
            "isPro": false,
            "fullname": "Trung Le",
            "user": "trungleuc",
            "type": "user"
          },
          "name": "Trung Le",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-04T03:30:50.175Z",
          "hidden": false
        },
        {
          "_id": "67a189e8fbbab3ce03462fb7",
          "name": "Dimitris Metaxas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T15:25:58.000Z",
      "title": "Metodo de entrenamiento de modelos potenciales de incertidumbre ampliados",
      "summary": "El modelo de consistencia es un nuevo miembro de la familia de modelos generativos, capaz de generar muestras de alta calidad en un solo paso o varios pasos. Recientemente, ha demostrado un desempeño sorprendente, logrando resultados en el espacio de píxeles y alcanzar resultados equivalentes a los modelos de variación. Sin embargo, el éxito en expandir el entrenamiento de consistencia a grandes conjuntos de datos ha sido especialmente decisivo en tareas de generación de imágenes y vídeos a partir de texto, dependiendo del rendimiento en el espacio potencial. En este artículo, se analizan las diferencias estadísticas entre el espacio de píxeles y el espacio potencial, y se encontró que los datos potenciales incluían excesos de entrada alto de los outliers, lo que significativamente afectaba el rendimiento del modelo de consistencia en el espacio potencial. Para resolver esto, se reemplazó el pérdida Pseudo-Huber por la pérdida Cauchy para reducir efectivamente la influencia de los outliers. Además, se introdujo una pérdida de variación en las etapas iniciales y se mejoró el rendimiento utilizando copias de transferencia óptima (OT). Finalmente, se introdujo un scheduler adaptativo y se incorporó una LayerNorm no escalable para mejorar la comprensión de la estadística de las características y reducir la influencia de los outliers. Estas estrategias permitieron generar muestras de alta calidad en un solo o varios pasos, reduciendo significativamente la diferencia entre el rendimiento de modelos de consistencia potencial y modelos de variación. La implementación se puede encontrar en la siguiente liga: https://github.com/quandao10/sLCT/",
      "upvotes": 2,
      "discussionId": "67a189eafbbab3ce0346300b"
    },
    "publishedAt": "2025-02-03T22:32:23.956Z",
    "title": "Improved Training Technique for Latent Consistency Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01441.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e083e6f351dc0745745d17",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e083e6f351dc0745745d17/N0GE4uLrkm14blAQMnm2E.jpeg",
      "fullname": "Quan Dao",
      "name": "quandao10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01636",
      "authors": [
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb1",
          "user": {
            "_id": "64e8f4a24f3f7b0b84834315",
            "avatarUrl": "/avatars/242bb68c7ccffe5061c2d1c229ea3b0b.svg",
            "isPro": false,
            "fullname": "Akshat Gupta",
            "user": "akshat57",
            "type": "user"
          },
          "name": "Akshat Gupta",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-04T05:53:11.213Z",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb2",
          "name": "Phudish Prateepamornkul",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb3",
          "name": "Maochuan Lu",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb4",
          "name": "Ahmed Alaa",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb5",
          "name": "Thomas Hartvigsen",
          "hidden": false
        },
        {
          "_id": "67a1aa5dc7fa0ccf0a32ceb6",
          "name": "Gopala Anumanchipalli",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:59:14.000Z",
      "title": "Lifelong Sequential Knowledge Editing without Model Degradation\n\n한국어로 번역하면:\n\n수명주기 순차적 지식 편집에 모델 저하 없이\n\n(수명주기 순차적 지식 편집에 모델 성능 저하 없이)",
      "summary": "En los estudios anteriores, se ha demostrado claramente que los cambios de parámetros en la edición de conocimientos pueden causar un impacto grave en el modelo. En este artículo, se investiga la causa de este fenómeno y se intenta realizar 10,000 ediciones secuenciales de conocimientos mientras se mantiene el rendimiento del modelo original. Primero, se muestra que el enfoque de edición de conocimientos \"editar en lugar específico\" puede causar sobreajuste a los hechos editados. Además, se demuestra claramente que utilizando este enfoque de manera continua puede llevar a la crecimiento desbalanceado de la norma de los nodos de la matriz editada. Luego, se proporciona una perspectiva importante y compleja sobre la función interna de este enfoque. El crecimiento de la norma es representado como una tecnología oculta que asigna gran importancia a las salidas activas generadas en las capas editadas, lo que contribuye significativamente al rendimiento del modelo. Esta \"importancia de calidad\" permite que las capas editadas contribuyan de manera significativa al rendimiento del modelo. Para mitigar estos problemas, se propone ENCORE (Edición de Conocimientos Robusta con Interrupción Inicial y Límites de Norma), que inhibe la sobreajuste y el crecimiento desbalanceado de la norma, permitiendo ediciones secuenciales a largo plazo y evitando la pérdida del rendimiento posterior, lo que permite realizar 10,000 ediciones secuenciales. Además, en Llama3-8B, funciona 61% más rápido que MEMIT y 64% más rápido que AlphaEdit, mostrando una eficiencia notable.",
      "upvotes": 1,
      "discussionId": "67a1aa5fc7fa0ccf0a32cf90"
    },
    "publishedAt": "2025-02-04T00:50:46.370Z",
    "title": "Lifelong Sequential Knowledge Editing without Model Degradation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01636.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e8f4a24f3f7b0b84834315",
      "avatarUrl": "/avatars/242bb68c7ccffe5061c2d1c229ea3b0b.svg",
      "fullname": "Akshat Gupta",
      "name": "akshat57",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.01637",
      "authors": [
        {
          "_id": "67a1a51e6aa8429da493d0b5",
          "name": "Da Yu",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0b6",
          "name": "Edith Cohen",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0b7",
          "name": "Badih Ghazi",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0b8",
          "name": "Yangsibo Huang",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0b9",
          "name": "Pritish Kamath",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0ba",
          "name": "Ravi Kumar",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0bb",
          "name": "Daogao Liu",
          "hidden": false
        },
        {
          "_id": "67a1a51e6aa8429da493d0bc",
          "name": "Chiyuan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:59:32.000Z",
      "title": "Escalado de capas internas en modelos de red",
      "summary": "Se propone SCONEN (escalable, context-aware, offline, N-gram embedding). SCONEN es un método para mejorar el rendimiento de modelos de lenguaje cuando se expande el tamaño de las capas. Se agregan embeddings de N-gramas generales mientras se mantiene el vector de palabras original. Estos embeddings proporcionan una representación contextual para cada token de entrada y se entrenan en otro modelo. Durante la inferencia, se calculan y almacenan en memoria off-line de manera anticipada para minimizar el impacto en la velocidad de inferencia. SCONEN permite dos nuevas estrategias de escalabilidad: aumentar el número de embeddings de N-gramas almacenados y escalar el modelo que los entrena, todo esto bajo un presupuesto fijo de FLOPS en tiempo de inferencia. Al aplicar ambas escalabilidades, SCONEN supera el límite de 1.9B parámetros y muestra un mejor rendimiento en tiempos de inferencia reducidos, incluso en corpus diversos.",
      "upvotes": 1,
      "discussionId": "67a1a51e6aa8429da493d0d5"
    },
    "publishedAt": "2025-02-04T00:27:13.960Z",
    "title": "Scaling Embedding Layers in Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01637.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01591",
      "authors": [
        {
          "_id": "67a1a4b72bf092a7612b36eb",
          "name": "Antoine Dedieu",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36ec",
          "name": "Joseph Ortiz",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36ed",
          "name": "Xinghua Lou",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36ee",
          "name": "Carter Wendelken",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36ef",
          "name": "Wolfgang Lehrach",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36f0",
          "name": "J Swaroop Guntupalli",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36f1",
          "name": "Miguel Lazaro-Gredilla",
          "hidden": false
        },
        {
          "_id": "67a1a4b72bf092a7612b36f2",
          "name": "Kevin Patrick Murphy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:25:17.000Z",
      "title": "Mejora de los Modelos Transformer Mundo para Optimizar la RL con Respecto a los Datos",
      "summary": "Proponemos un enfoque adecuado para el aprendizaje por refuerzo basado en modelos, lo que permite alcanzar nuevos rendimientos en el desafiante benchmark Craftax-classic. En este juego de supervivencia 2D en un mundo abierto, es necesario demostrar habilidades fuertes. Hemos ajustado los episodios de muestras para mejorar su calidad y, al aplicar el algoritmo MBRL de 1M pasos de entorno, alcanzamos un 67.4% de recompensa, superando significativamente al 53.2% de DreamerV3 y alcanzando por primera vez un rendimiento superior al de los humanos (65.0%). Nuestro método construye una línea de puntos de estado de la arte sin modelos y utiliza una nueva arquitectura de políticas. Posteriormente, añadimos tres mejoras a los entornos estándar de MBRL: (a) \"Dyna con calentamiento\" - aprendizaje de la política con datos reales y imaginarios, (b) \"tokenizador de vecinos cercanos\" - mejora de la estructura de entrada para el Modelo Mundo Transformer (TWM) para imágenes, y (c) \"forzamiento de bloques\" - explicación de los tokens futuros del próximo paso de tiempo por parte del TWM.",
      "upvotes": 1,
      "discussionId": "67a1a4b82bf092a7612b371b"
    },
    "publishedAt": "2025-02-04T00:25:52.071Z",
    "title": "Improving Transformer World Models for Data-Efficient RL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01591.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5927
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01584",
      "authors": [
        {
          "_id": "67a1e658a68ad21bcdffead6",
          "name": "Carolyn Jane Anderson",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffead7",
          "name": "Joydeep Biswas",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffead8",
          "name": "Aleksander Boruch-Gruszecki",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffead9",
          "name": "Federico Cassano",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffeada",
          "name": "Molly Q Feldman",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffeadb",
          "name": "Arjun Guha",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffeadc",
          "name": "Francesca Lucchetti",
          "hidden": false
        },
        {
          "_id": "67a1e658a68ad21bcdffeadd",
          "name": "Zixuan Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:10:38.000Z",
      "title": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models",
      "summary": "Los modelos líder actuales de benchmark miden la conocida \"nivel de pila de herramientas\" de conocimiento profesional, que es difícil de entender para expertos. En contraste, proporcionamos un benchmark basado en el desafío de perdidas del domingo de NPR, que requiere conocimientos generales. Nuestro benchmark es difícil para tanto humanos como modelos, pero permite fácilmente ver soluciones precisas y detectar errores en los modelos.\n\nNuestro estudio revela deficiencias en capacidades no detectadas en los benchmarks actuales: o1 de OpenAI tiene una ventaja significativa sobre otros modelos de conocimiento profesional. Además, nuestro análisis de salidas de razonamiento detectó fallos en nuevos campos. Por ejemplo, DeepSeek R1, que proporciona respuestas incorrectas antes de decir \"no más planes\", mostrando una \"insatisfacción\" excepcional. Por otro lado, la capacidad de concluir razonamientos de Cosa, que muestra que el modelo considera la necesidad de \"poner fin\" a la forma de razonamiento. Además, R1 y Gemini Thinking se han cuantificado en el efecto de la razonamiento a largo plazo, y se han tratado de establecer los límites en el benchmark para mejorar la precisión, especialmente cuando es necesario un razonamiento a largo plazo.",
      "upvotes": 0,
      "discussionId": "67a1e659a68ad21bcdffeb04"
    },
    "publishedAt": "2025-02-04T05:06:50.415Z",
    "title": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d8315bad693a1a962864b3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664332914111-62d8315bad693a1a962864b3.png",
      "fullname": "Arjun Guha",
      "name": "arjunguha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18055",
      "authors": [
        {
          "_id": "67a197099b2f48315e74dcde",
          "user": {
            "_id": "67225dd94201755d88e104c4",
            "avatarUrl": "/avatars/6da69788ce0cd41c86f9dd0bf8d092aa.svg",
            "isPro": false,
            "fullname": "Edwin D. de Jong",
            "user": "EdwinDdeJong",
            "type": "user"
          },
          "name": "Edwin D. de Jong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:28.120Z",
          "hidden": false
        },
        {
          "_id": "67a197099b2f48315e74dcdf",
          "name": "Eric Marcus",
          "hidden": false
        },
        {
          "_id": "67a197099b2f48315e74dce0",
          "name": "Jonas Teuwen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-29T23:38:14.000Z",
      "title": "Los modelos basados en patología actualmente son vulnerables a las diferencias entre instituciones médicas.",
      "summary": "La modelación basada en patología (FM) tiene un gran potencial en la medicina. Es esencial mantener fuertes las diferencias entre instituciones médicas antes de que se usen en la práctica clínica. Se evalúa si los FM se centran en las características biológicas de células o cáncer, o si se centran en las diferencias de señales causadas por la técnica de citología o otras técnicas. Se introduce el Índice de Robustez. Este nuevo métrico de robustez refleja en qué medida las características biológicas tienen prioridad sobre las características de las señales. Se evaluan actualmente 10 FM de patología públicas. Los FM actuales muestran fuertemente las características de las instituciones médicas. Se observa una diferencia significativa en el Índice de Robustez. Hasta ahora no habían sido tan robustos, pero no había modelos que reflejaran cierta prioridad de las características biológicas sobre las de las señales. Se describe un método para evaluar la influencia de las diferencias entre instituciones médicas en el rendimiento predictivo de los FM. Se analiza cómo la invariancia del Modelo de Robustez Fix (MRF) afecta el rendimiento de clasificación de los modelos inferiores, y se confirma que los errores de clasificación de tipo de cáncer no son aleatorios, sino que se corresponden especialmente con las señales de la misma institución médica. Se visualizan los espacios de mapeo de los FM, lo que muestra que las estructuras de la teoría biológica son más fuertes que aquellas impuestas por la institución médica. Como resultado, la fuente de origen de la institución médica es más precisa para predecir el tipo de tejido y el cáncer. El Índice de Rendimiento de Robustez Fix introducido promueve el desarrollo de FM de patología para aplicaciones clínicas confiables y potentes.",
      "upvotes": 0,
      "discussionId": "67a1970b9b2f48315e74dd5d"
    },
    "publishedAt": "2025-02-04T04:59:22.696Z",
    "title": "Current Pathology Foundation Models are unrobust to Medical Center Differences",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67225dd94201755d88e104c4/oD8gcxl4D9G3FPXWGVGiz.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67225dd94201755d88e104c4/_jrPyZDKwbr3K9-Q4_sCH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67225dd94201755d88e104c4",
      "avatarUrl": "/avatars/6da69788ce0cd41c86f9dd0bf8d092aa.svg",
      "fullname": "Edwin D. de Jong",
      "name": "EdwinDdeJong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.00314",
      "authors": [
        {
          "_id": "67a1d1ca167bea74d520eb59",
          "name": "Moein Heidari",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5a",
          "name": "Ehsan Khodapanah Aghdam",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5b",
          "name": "Alexander Manzella",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5c",
          "name": "Daniel Hsu",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5d",
          "name": "Rebecca Scalabrino",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5e",
          "name": "Wenjin Chen",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb5f",
          "name": "David J. Foran",
          "hidden": false
        },
        {
          "_id": "67a1d1ca167bea74d520eb60",
          "name": "Ilker Hacihaliloglu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-01T04:25:28.000Z",
      "title": "Investigación sobre la mejora de la performance de la versión U-Net - Estudio sobre la segmentación de cáncer de páncreas posterior",
      "summary": "Detrás del peritoneo existen diversas neoplasias, incluyendo raras benignas y malignas, lo que complica su diagnóstico y tratamiento debido a su raridad y su cercanía a estructuras importantes. La estimación del tamaño de estas neoplasias es complicada debido a la asimetría morfológica. La separación manual requiere tiempo. Aunque el uso de modelos como U-Net puede proporcionar resultados esperados, su aplicación implica un alto consumo computacional. Para resolver estos problemas, modelos como el Mamba State Space Model (SSM) y la Extended Long-Short Term Memory (xLSTM) ofrecen soluciones eficientes que reducen el consumo de recursos mientras manejan dependencias a larga distancia. En este estudio, se evalua la extensión de U-Net (CNN, ViT, Mamba, xLSTM) en un nuevo conjunto de datos CT interno y en un conjunto de datos de separación de instituciones públicos. El modelo propuesto, ViLU-Net, mejora la separación utilizando Vi-blocks. Finalmente, xLSTM muestra claramente su eficiencia en el marco de U-Net. El código está disponible en GitHub.",
      "upvotes": 0,
      "discussionId": "67a1d1cd167bea74d520ebf6"
    },
    "publishedAt": "2025-02-04T03:38:34.899Z",
    "title": "A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00314.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61ba19bf6122a4fd29049371",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1639586194527-noauth.jpeg",
      "fullname": "Moein Heidari",
      "name": "moein99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]