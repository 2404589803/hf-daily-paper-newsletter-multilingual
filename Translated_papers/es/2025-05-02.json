[
  {
    "paper": {
      "id": "2504.21853",
      "authors": [
        {
          "_id": "681441e64d6a681c7c840b1f",
          "name": "Jiwen Yu",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b20",
          "name": "Yiran Qin",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b21",
          "name": "Haoxuan Che",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b22",
          "name": "Quande Liu",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b23",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b24",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b25",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b26",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b27",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b28",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/Y9dvfNIIrOJ4rCzwIB6XI.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/5kPNXDj9LIsgShhFz5WSg.jpeg"
      ],
      "publishedAt": "2025-04-30T17:59:02.000Z",
      "submittedOnDailyAt": "2025-05-02T02:29:37.187Z",
      "title": "Interface Generation Video Investigation",
      "submittedOnDailyBy": {
        "_id": "64105a6d14215c0775dfdd14",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
        "isPro": false,
        "fullname": "Jiwen Yu",
        "user": "VictorYuki",
        "type": "user"
      },
      "summary": "El IGV (Interactive Generative Video) ha surgido como una tecnología importante con el aumento de la demanda de contenido de video interactivo de alta calidad. En este artículo, se define el IGV como una tecnología que, además de su capacidad generadora, proporciona funciones interactivas para crear diversos contenidos de video de alta calidad y fomentar la participación del usuario. Se examina el estado actual de los aplicaciones de IGV y se centra en tres áreas principales: juegos, AI de especificación y autonoma de conducción. En los juegos, el IGV permite explorar infinitamente el mundo virtual. En la AI de especificación, se realiza la síntesis de entornos dinámicos que facilitan diversas interacciones. En la conducción autónoma, se ofrece una función de simulación de procesamiento posterior cerrada para pruebas y validaciones cruciales para la seguridad. Para guiar el desarrollo futuro, se propone un marco detallado que divide un sistema ideal de IGV en cinco módulos básicos: generación, control, memoria, dinámica y inteligencia. Además, se analiza sistemáticamente los problemas técnicos y las direcciones futuras para cada componente de un sistema ideal de IGV. Esta análisis sistemático se espera fomentar la investigación y el desarrollo en el campo del IGV y ayudar a desarrollar aplicaciones más complejas y prácticas.",
      "upvotes": 24,
      "discussionId": "681441e84d6a681c7c840bae",
      "ai_keywords": [
        "generative capabilities",
        "interactive features",
        "control signals",
        "responsive feedback",
        "virtual worlds",
        "physics-aware environment synthesizer",
        "multimodal interaction",
        "dynamically evolving scenes",
        "closed-loop simulation",
        "safety-critical testing",
        "validation",
        "real-time generation",
        "open-domain control",
        "long-term coherence",
        "accurate physics",
        "causal reasoning"
      ]
    },
    "publishedAt": "2025-04-30T13:59:02.000Z",
    "title": "A Survey of Interactive Generative Video",
    "summary": "Interactive Generative Video (IGV) has emerged as a crucial technology in\nresponse to the growing demand for high-quality, interactive video content\nacross various domains. In this paper, we define IGV as a technology that\ncombines generative capabilities to produce diverse high-quality video content\nwith interactive features that enable user engagement through control signals\nand responsive feedback. We survey the current landscape of IGV applications,\nfocusing on three major domains: 1) gaming, where IGV enables infinite\nexploration in virtual worlds; 2) embodied AI, where IGV serves as a\nphysics-aware environment synthesizer for training agents in multimodal\ninteraction with dynamically evolving scenes; and 3) autonomous driving, where\nIGV provides closed-loop simulation capabilities for safety-critical testing\nand validation. To guide future development, we propose a comprehensive\nframework that decomposes an ideal IGV system into five essential modules:\nGeneration, Control, Memory, Dynamics, and Intelligence. Furthermore, we\nsystematically analyze the technical challenges and future directions in\nrealizing each component for an ideal IGV system, such as achieving real-time\ngeneration, enabling open-domain control, maintaining long-term coherence,\nsimulating accurate physics, and integrating causal reasoning. We believe that\nthis systematic analysis will facilitate future research and development in the\nfield of IGV, ultimately advancing the technology toward more sophisticated and\npractical applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/Y9dvfNIIrOJ4rCzwIB6XI.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/5kPNXDj9LIsgShhFz5WSg.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64105a6d14215c0775dfdd14",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
      "fullname": "Jiwen Yu",
      "name": "VictorYuki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.00662",
      "authors": [
        {
          "_id": "68142e4a551709da9244e8d1",
          "user": {
            "_id": "64b7df742f5a966b973e25f7",
            "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
            "isPro": false,
            "fullname": "Wenkai Yang",
            "user": "Keven16",
            "type": "user"
          },
          "name": "Wenkai Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-02T06:34:18.531Z",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d2",
          "name": "Jingwen Chen",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d3",
          "name": "Yankai Lin",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d4",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T17:03:17.000Z",
      "submittedOnDailyAt": "2025-05-02T01:12:33.949Z",
      "title": "DeepCritic: Criticice con prudencia contra el modelo de lenguaje.",
      "submittedOnDailyBy": {
        "_id": "64b7df742f5a966b973e25f7",
        "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
        "isPro": false,
        "fullname": "Wenkai Yang",
        "user": "Keven16",
        "type": "user"
      },
      "summary": "El rápido desarrollo de los LLM ha llevado a que la provisión de retroalimentación precisa y monitoreo escalable sea una cuestión urgente e importante. La realización de un modelo de evaluación para supervisar automatizadamente a los LLM es una solución deseable. En este estudio, se centra en la investigación y mejora de los capacidades de evaluación matemática de los LLM. Los evaluadores actuales de los LLM ofrecen evaluaciones superficiales y superficiales en cada etapa, con una precisión baja y no proporcionan suficientes retroalimentaciones para el generador de LLM. Para abordar estos problemas, proponemos un nuevo marco efectivo de 2 etapas para desarrollar un evaluador que evalúe intencionalmente cada etapa lógica de una respuesta matemática. El primer paso utiliza Qwen2.5-72B-Instruct para generar evaluaciones de 4.5K de largo y formato de texto extenso, proporcionando datos de ajuste micro con retroalimentación manual. Cada evaluación incluye una evaluación profunda inicial en diferentes verificaciones y etapas lógicas. A continuación, se utilizan datos de etiquetado humano desde PRM800K y datos de notas automáticas basadas en sampling Monte Carlo para estimar la precisión, y se realiza aprendizaje por refuerzo para mejorar el modelo de ajuste, fomentando la capacidad de evaluación. El modelo de evaluación basado en Qwen2.5-7B-Instruct es notablemente superior en el marco de pruebas de reconocimiento de errores en comparación con los evaluadores actuales de LLM (incluyendo modelos del mismo tamaño como DeepSeek-R1-distill y GPT-4o), y puede ayudar efectivamente a corregir los errores del generador de LLM con retroalimentaciones más detalladas.",
      "upvotes": 21,
      "discussionId": "68142e4b551709da9244e8f8",
      "ai_keywords": [
        "LLMs (Large Language Models)",
        "critique models",
        "automated supervision",
        "math critique ability",
        "supervised fine-tuning",
        "Qwen2.5-72B-Instruct",
        "seed data",
        "deliberate step-wise critiques",
        "multi-perspective verifications",
        "reinforcement learning",
        "PRM800K",
        "Monte Carlo sampling-based correctness estimation",
        "Qwen2.5-7B-Instruct",
        "DeepSeek-R1-distill models",
        "GPT-4o",
        "error identification benchmarks"
      ]
    },
    "publishedAt": "2025-05-01T13:03:17.000Z",
    "title": "DeepCritic: Deliberate Critique with Large Language Models",
    "summary": "As Large Language Models (LLMs) are rapidly evolving, providing accurate\nfeedback and scalable oversight on their outputs becomes an urgent and critical\nproblem. Leveraging LLMs as critique models to achieve automated supervision is\na promising solution. In this work, we focus on studying and enhancing the math\ncritique ability of LLMs. Current LLM critics provide critiques that are too\nshallow and superficial on each step, leading to low judgment accuracy and\nstruggling to offer sufficient feedback for the LLM generator to correct\nmistakes. To tackle this issue, we propose a novel and effective two-stage\nframework to develop LLM critics that are capable of deliberately critiquing on\neach reasoning step of math solutions. In the first stage, we utilize\nQwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for\nsupervised fine-tuning. Each seed critique consists of deliberate step-wise\ncritiques that includes multi-perspective verifications as well as in-depth\ncritiques of initial critiques for each reasoning step. Then, we perform\nreinforcement learning on the fine-tuned model with either existing\nhuman-labeled data from PRM800K or our automatically annotated data obtained\nvia Monte Carlo sampling-based correctness estimation, to further incentivize\nits critique ability. Our developed critique model built on Qwen2.5-7B-Instruct\nnot only significantly outperforms existing LLM critics (including the\nsame-sized DeepSeek-R1-distill models and GPT-4o) on various error\nidentification benchmarks, but also more effectively helps the LLM generator\nrefine erroneous steps through more detailed feedback.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00662.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "64b7df742f5a966b973e25f7",
      "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
      "fullname": "Wenkai Yang",
      "name": "Keven16",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.00703",
      "authors": [
        {
          "_id": "681428debcdf962d03da2797",
          "name": "Dongzhi Jiang",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da2798",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da2799",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279a",
          "name": "Zhuofan Zong",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279b",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279c",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279d",
          "name": "Shilin Yan",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279e",
          "name": "Pheng-Ann Heng",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279f",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T17:59:46.000Z",
      "submittedOnDailyAt": "2025-05-02T00:38:40.412Z",
      "title": "T2I-R1: Mejora de la generación de imágenes utilizando niveles de significado complejo y de token del CoT.",
      "submittedOnDailyBy": {
        "_id": "6349214f8146350b3a4c5cdf",
        "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
        "isPro": false,
        "fullname": "Dongzhi Jiang",
        "user": "CaraJ",
        "type": "user"
      },
      "summary": "El reciente desarrollo de grandes modelos de lenguaje ha demostrado que la estrategia de chain-of-thought (CoT) y el aprendizaje por refuerzo (RL) pueden mejorar significativamente el rendimiento. Sin embargo, la aplicación de estos enfoques lógicos a la generación visual es principalmente exploratoria. En este artículo, se presenta un nuevo modelo de texto-imagen (T2I-R1) que incluye RL y adopta un proceso bi-level CoT lógico. Específicamente, se identifican dos niveles de CoT para mejorar la generación en diferentes etapas: (1) un CoT de nivel de significado para planificar de manera alta el prompt y (2) un CoT de nivel de token para planificar de manera baja el tratamiento de cada patch de la imagen. Para mejorar la coordinación de estos dos niveles de CoT, se introduce BiCoT-GRPO y se utiliza un ensamble de compensaciones de generación para optimizar continuamente ambos CoT en el mismo proceso de entrenamiento. Aplicando estas estrategias lógicas a la base de modelo Janus-Pro, se logró mejorar en 13% en T2I-CompBench y en 19% en el WISE benchmark, superando el modelo líder FLUX. El código está disponible en la siguiente URL: https://github.com/CaraJ7/T2I-R1",
      "upvotes": 13,
      "discussionId": "681428dfbcdf962d03da281c",
      "githubRepo": "https://github.com/CaraJ7/T2I-R1",
      "ai_keywords": [
        "chain-of-thought (CoT)",
        "reinforcement learning (RL)",
        "text-to-image generation model",
        "bi-level CoT reasoning process",
        "semantic-level CoT",
        "token-level CoT",
        "BiCoT-GRPO",
        "generation rewards",
        "Janus-Pro",
        "T2I-CompBench",
        "WISE benchmark",
        "FLUX"
      ]
    },
    "publishedAt": "2025-05-01T13:59:46.000Z",
    "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level\n  and Token-level CoT",
    "summary": "Recent advancements in large language models have demonstrated how\nchain-of-thought (CoT) and reinforcement learning (RL) can improve performance.\nHowever, applying such reasoning strategies to the visual generation domain\nremains largely unexplored. In this paper, we present T2I-R1, a novel\nreasoning-enhanced text-to-image generation model, powered by RL with a\nbi-level CoT reasoning process. Specifically, we identify two levels of CoT\nthat can be utilized to enhance different stages of generation: (1) the\nsemantic-level CoT for high-level planning of the prompt and (2) the\ntoken-level CoT for low-level pixel processing during patch-by-patch\ngeneration. To better coordinate these two levels of CoT, we introduce\nBiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes\nboth generation CoTs within the same training step. By applying our reasoning\nstrategies to the baseline model, Janus-Pro, we achieve superior performance\nwith 13% improvement on T2I-CompBench and 19% improvement on the WISE\nbenchmark, even surpassing the state-of-the-art model FLUX.1. Code is available\nat: https://github.com/CaraJ7/T2I-R1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6349214f8146350b3a4c5cdf",
      "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
      "fullname": "Dongzhi Jiang",
      "name": "CaraJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21659",
      "authors": [
        {
          "_id": "68142de6111ccf18a993c890",
          "name": "Haotian Luo",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c891",
          "name": "Haiying He",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c892",
          "name": "Yibo Wang",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c893",
          "name": "Jinluan Yang",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c894",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c895",
          "name": "Naiqiang Tan",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c896",
          "name": "Xiaochun Cao",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c897",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c898",
          "name": "Li Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T14:01:45.000Z",
      "submittedOnDailyAt": "2025-05-02T01:01:49.479Z",
      "title": "Optimización lógica de niveles adaptativos en contextos híbridos a largo plazo",
      "submittedOnDailyBy": {
        "_id": "632ab8f5a968c34257da5c52",
        "avatarUrl": "/avatars/59df09e6c9e1e633170514d950ad7981.svg",
        "isPro": false,
        "fullname": "Haotian Luo",
        "user": "LordNoah",
        "type": "user"
      },
      "summary": "Recientemente, los modelos de teoría de razones basados en consideraciones a largo plazo han mostrado un potente desempeño en tareas complejas de teoría de razones, pero su eficiencia se ve afectada por un aumento significativo en el overhead de inferencia. Nuestro análisis experimental ha demostrado que los beneficios de la consideración a largo plazo pueden variar según la situación: en algunos casos, la necesidad de dar razones detalladas es crucial, mientras que en otros, la consideración a largo plazo no mejora o incluso reduce la precisión. Esto ha llevado a la necesidad de una estrategia adaptativa de teoría de razones que ajuste la estructura de razones a las características del input. Sin embargo, la mayoría de los estudios previos han estado limitados a reducir la redundancia en largos pasos de Teoría de Razones y a explorar estrategias más eficientes que superen el patrón de Teoría de Razones a Largo Plazo. En respuesta a esto, proponemos un nuevo marco de 2 etapas para una teoría de razones adaptativa e eficiente. Primero, integramos modelos de Teoría de Razones a largo plazo y a corto plazo para facilitar diferentes estilos de razonamiento. Luego, aplicamos un entrenamiento de niveles bidireccionales para guiar al modelo a elegir un estilo de razonamiento adecuado y priorizar razonamientos claros y precisos dentro de cada grupo de estilos. Nuestros experimentos han mostrado que este enfoque puede reducir significativamente los costos de inferencia mientras mantiene o mejora el desempeño, especialmente en cinco conjuntos de datos matemáticos, donde la longitud promedio de las razones se redujo en más del 50%. Este estudio también ha revelado la posibilidad de mejorar la eficiencia de la teoría de razones en modelos de lenguaje grandes a gran escala. Nuestro código será publicado pronto en: https://github.com/StarDewXXX/AdaR1",
      "upvotes": 3,
      "discussionId": "68142de7111ccf18a993c8ba",
      "ai_keywords": [
        "CoT models",
        "Long-CoT",
        "hybrid reasoning model",
        "bi-level preference training",
        "adaptive reasoning strategies"
      ]
    },
    "publishedAt": "2025-04-30T10:01:45.000Z",
    "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization",
    "summary": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21659.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632ab8f5a968c34257da5c52",
      "avatarUrl": "/avatars/59df09e6c9e1e633170514d950ad7981.svg",
      "fullname": "Haotian Luo",
      "name": "LordNoah",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.00497",
      "authors": [
        {
          "_id": "68147d4d687b82a9b6308cfd",
          "name": "Antoni Bigata",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308cfe",
          "name": "Rodrigo Mira",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308cff",
          "name": "Stella Bounareli",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d00",
          "name": "Michał Stypułkowski",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d01",
          "name": "Konstantinos Vougioukas",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d02",
          "name": "Stavros Petridis",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d03",
          "name": "Maja Pantic",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/640777812e309e65452491dd/4ao3SssNM8wCiTo5amAxn.mp4"
      ],
      "publishedAt": "2025-05-01T12:56:17.000Z",
      "submittedOnDailyAt": "2025-05-02T06:38:20.471Z",
      "title": "KeySync: potente método para lograr sincronización de labios alta resolución y suave.",
      "submittedOnDailyBy": {
        "_id": "640777812e309e65452491dd",
        "avatarUrl": "/avatars/01b927675e78f212408168522f65fe36.svg",
        "isPro": true,
        "fullname": "Antoni Bigata",
        "user": "toninio19",
        "type": "user"
      },
      "summary": "La sincronización de labios es un proceso que ajusta los movimientos de los labios en un video existente con un nuevo entrada de voz, generalmente tratado como una simple deformación del movimiento facial de voz. Sin embargo, la sincronización de labios presenta problemas adicionales como la pérdida de expresiones y la ocultación de la cara en el video de entrada, además de los problemas comunes de la generación de tokens. Para resolver estos problemas, proponemos un marco de trabajo de dos etapas llamado KeySync. Este marco aborda la consistencia temporal y utiliza técnicas de mascaras ajustadas para enfrentar las pérdidas y la ocultación de la cara. Demostramos que KeySync obtiene los mejores resultados en la reconstrucción de labios y la sincronización cruzada utilizando nuestro nuevo métrico de pérdida \"LipLeak\", mejorando la calidad visual y reduciendo la pérdida. Además, presentamos un nuevo enfoque de mascaras para el tratamiento de la ocultación de la cara, justificando decisiones estructurales a través de varios estudios de reducción. Los códigos y los pesos del modelo pueden obtenerse en https://antonibigata.github.io/KeySync.",
      "upvotes": 2,
      "discussionId": "68147d53687b82a9b6308e59",
      "projectPage": "https://antonibigata.github.io/KeySync/",
      "githubRepo": "https://github.com/antonibigata/keysync",
      "ai_keywords": [
        "KeySync",
        "lip synchronization",
        "audio-driven facial animation",
        "talking head generation",
        "temporal consistency",
        "expression leakage",
        "facial occlusions",
        "automated dubbing",
        "lip reconstruction",
        "cross-synchronization",
        "visual quality",
        "LipLeak",
        "masking strategy",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-05-01T08:56:17.000Z",
    "title": "KeySync: A Robust Approach for Leakage-free Lip Synchronization in High\n  Resolution",
    "summary": "Lip synchronization, known as the task of aligning lip movements in an\nexisting video with new input audio, is typically framed as a simpler variant\nof audio-driven facial animation. However, as well as suffering from the usual\nissues in talking head generation (e.g., temporal consistency), lip\nsynchronization presents significant new challenges such as expression leakage\nfrom the input video and facial occlusions, which can severely impact\nreal-world applications like automated dubbing, but are often neglected in\nexisting works. To address these shortcomings, we present KeySync, a two-stage\nframework that succeeds in solving the issue of temporal consistency, while\nalso incorporating solutions for leakage and occlusions using a carefully\ndesigned masking strategy. We show that KeySync achieves state-of-the-art\nresults in lip reconstruction and cross-synchronization, improving visual\nquality and reducing expression leakage according to LipLeak, our novel leakage\nmetric. Furthermore, we demonstrate the effectiveness of our new masking\napproach in handling occlusions and validate our architectural choices through\nseveral ablation studies. Code and model weights can be found at\nhttps://antonibigata.github.io/KeySync.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/640777812e309e65452491dd/4ao3SssNM8wCiTo5amAxn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00497.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640777812e309e65452491dd",
      "avatarUrl": "/avatars/01b927675e78f212408168522f65fe36.svg",
      "fullname": "Antoni Bigata",
      "name": "toninio19",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20605",
      "authors": [
        {
          "_id": "68131e73f0f2a4d8b2d4b06a",
          "user": {
            "_id": "642bcb8ae5b6823cde9301bd",
            "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
            "isPro": false,
            "fullname": "Mihai Dan Nadăș",
            "user": "mihainadas",
            "type": "user"
          },
          "name": "Mihai Nadas",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-02T06:34:48.889Z",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06b",
          "name": "Laura Diosan",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06c",
          "user": {
            "_id": "67b2344d0ce2aaa57c8c9997",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b2344d0ce2aaa57c8c9997/LSMjuQNjRsUllUQyt9vNo.jpeg",
            "isPro": false,
            "fullname": "Andrei Piscoran",
            "user": "andreiPiscoran",
            "type": "user"
          },
          "name": "Andrei Piscoran",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-01T07:11:31.916Z",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06d",
          "user": {
            "_id": "677e4393ef848c5a5352d082",
            "avatarUrl": "/avatars/bcb60ead58969601e2911053550fec62.svg",
            "isPro": false,
            "fullname": "Andreea Tomescu",
            "user": "andreeatomescu",
            "type": "user"
          },
          "name": "Andreea Tomescu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-01T07:10:43.780Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T10:15:28.000Z",
      "submittedOnDailyAt": "2025-05-02T06:22:34.343Z",
      "title": "TF1-EN-3M: Conjunto de datos para un pequeño modelo de lenguaje abierto entrenado con 3 millones de historias morales sintéticas",
      "submittedOnDailyBy": {
        "_id": "642bcb8ae5b6823cde9301bd",
        "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
        "isPro": false,
        "fullname": "Mihai Dan Nadăș",
        "user": "mihainadas",
        "type": "user"
      },
      "summary": "La historia de Mori es un instrumento histórico utilizado para transmitir valor y significado, mientras que el NLP moderno carece de grandes estructuras de textos coherentes y con enseñanzas éticas explícitas. Para corregir esta deficiencia, se ha preparado el conjunto de datos TF1-EN-3M. Este es el primer conjunto de datos público, que genera 3 millones de películas en inglés con un modelo de 800 millones de parámetros ajustado a instancias. Cada historia incluye un diseño de 6 potenciales (personaje -> característica -> ambiente -> conflicto -> resolución -> enseñanza ética) y un motor de formación que mantiene la coherencia de género mientras cubre un amplio espacio temático.\n\nEl pipeline de evaluación híbrido combina (i) evaluadores basados en GPT que asignan puntuaciones en función de la gramática, creatividad, claridad ética y respeto al template, y (ii) métricas de diversidad y legibilidad sin referencia. De las 10 candidatas abiertas, la versión de Llama-3 con 800 millones de parámetros ofrece la mejor combinación de calidad y velocidad, generando películas con altos puntajes en 1,000 películas. Esta tarea se puede realizar en aproximadamente 13.5 segundos con un solo GPU de consola (<24GB de VRAM).\n\nSe publica gratuitamente el conjunto de datos, el código de generación, los scripts de evaluación y la metadatos completos, permitiendo la reproducibilidad y el benchmark de costos. TF1-EN-3M abre caminos para la investigación en AI educativa para niños, demostrando que la transmisión de grandes historias éticas no necesita grandes modelos.",
      "upvotes": 2,
      "discussionId": "68131e73f0f2a4d8b2d4b087",
      "githubRepo": "https://github.com/klusai/tinyfabulist",
      "ai_keywords": [
        "instruction-tuned models",
        "combinatorial prompt engine",
        "GPT-based critic",
        "template adherence",
        "reference-free diversity",
        "Llama-3 variant",
        "computational efficiency",
        "permissive license",
        "child-friendly educational AI",
        "moral storytelling"
      ]
    },
    "publishedAt": "2025-04-29T06:15:28.000Z",
    "title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open\n  Language Models",
    "summary": "Moral stories are a time-tested vehicle for transmitting values, yet modern\nNLP lacks a large, structured corpus that couples coherent narratives with\nexplicit ethical lessons. We close this gap with TF1-EN-3M, the first open\ndataset of three million English-language fables generated exclusively by\ninstruction-tuned models no larger than 8B parameters. Each story follows a\nsix-slot scaffold (character -> trait -> setting -> conflict -> resolution ->\nmoral), produced through a combinatorial prompt engine that guarantees genre\nfidelity while covering a broad thematic space.\n  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores\ngrammar, creativity, moral clarity, and template adherence with (ii)\nreference-free diversity and readability metrics. Among ten open-weight\ncandidates, an 8B-parameter Llama-3 variant delivers the best quality-speed\ntrade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)\nat approximately 13.5 cents per 1,000 fables.\n  We release the dataset, generation code, evaluation scripts, and full\nmetadata under a permissive license, enabling exact reproducibility and cost\nbenchmarking. TF1-EN-3M opens avenues for research in instruction following,\nnarrative intelligence, value alignment, and child-friendly educational AI,\ndemonstrating that large-scale moral storytelling no longer requires\nproprietary giant models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20605.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642bcb8ae5b6823cde9301bd",
      "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
      "fullname": "Mihai Dan Nadăș",
      "name": "mihainadas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18983",
      "authors": [
        {
          "_id": "681470d72175e5e7ca0ea002",
          "name": "Xuyin Qi",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea003",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea004",
          "name": "Canxuan Gang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea005",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea006",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea007",
          "name": "Zhiwei Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea008",
          "name": "Yang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-26T17:56:56.000Z",
      "submittedOnDailyAt": "2025-05-02T05:44:53.105Z",
      "title": "MediAug: Investigación de Visualización y Augmentación de Imágenes Médicas",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "La augmentación de datos es esencial para mejorar la precisión de clasificación, la detección de daños y la segmentación de órganos en imágenes médicas, donde la condición de datos está limitada. Sin embargo, se mantienen dos problemas significativos. El primero es el gran espacio de dominio entre imágenes naturales y imágenes médicas, que puede alterar características importantes de enfermedades. El segundo es que la investigación en augmentación de datos de imágenes médicas está limitada a tareas o arquitecturas específicas, y los beneficios de estrategias mix basadas no son claros. Para resolver estos problemas, proponemos un marco de evaluación único utilizando seis métodos de augmentación de datos basados en mix, combinando dos modelos de Convolucional y Transformer en conjunto de imágenes de MRI de tumores cerebrales y de imagenes de ojos con afecciones. Nuestro contribución consiste en tres aspectos: 1. Presentamos MediAug, una evaluación avanzada para la augmentación de datos en imágenes médicas. 2. Evaluamos sistemáticamente MixUp, YOCO, CropMix, CutMix, AugMix y SnapMix utilizando ResNet-50 y ViT-B. 3. Mediante una amplia gama de experimentos, observamos que MixUp mejoró la precisión en la tarea de clasificación de tumores cerebrales en ResNet-50 al 79.19%, SnapMix al 99.44% en ViT-B, YOCO al 91.60% en ResNet-50 para la tarea de clasificación de afecciones oculares y CutMix al 97.94% en ViT-B. El código está disponible en https://github.com/AIGeeksGroup/MediAug.",
      "upvotes": 1,
      "discussionId": "681470d92175e5e7ca0ea065",
      "ai_keywords": [
        "MediAug",
        "MixUp",
        "YOCO",
        "CropMix",
        "CutMix",
        "AugMix",
        "SnapMix",
        "ResNet-50",
        "ViT-B",
        "brain tumour MRI",
        "eye disease fundus datasets",
        "domain gap",
        "lesion detection",
        "organ segmentation",
        "classification accuracy"
      ]
    },
    "publishedAt": "2025-04-26T13:56:56.000Z",
    "title": "MediAug: Exploring Visual Augmentation in Medical Imaging",
    "summary": "Data augmentation is essential in medical imaging for improving\nclassification accuracy, lesion detection, and organ segmentation under limited\ndata conditions. However, two significant challenges remain. First, a\npronounced domain gap between natural photographs and medical images can\ndistort critical disease features. Second, augmentation studies in medical\nimaging are fragmented and limited to single tasks or architectures, leaving\nthe benefits of advanced mix-based strategies unclear. To address these\nchallenges, we propose a unified evaluation framework with six mix-based\naugmentation methods integrated with both convolutional and transformer\nbackbones on brain tumour MRI and eye disease fundus datasets. Our\ncontributions are threefold. (1) We introduce MediAug, a comprehensive and\nreproducible benchmark for advanced data augmentation in medical imaging. (2)\nWe systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix\nwith ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive\nexperiments that MixUp yields the greatest improvement on the brain tumor\nclassification task for ResNet-50 with 79.19% accuracy and SnapMix yields the\ngreatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the\ngreatest improvement on the eye disease classification task for ResNet-50 with\n91.60% accuracy and CutMix yields the greatest improvement for ViT-B with\n97.94% accuracy. Code will be available at\nhttps://github.com/AIGeeksGroup/MediAug.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18983.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]