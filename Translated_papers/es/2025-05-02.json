[
  {
    "paper": {
      "id": "2504.21853",
      "authors": [
        {
          "_id": "681441e64d6a681c7c840b1f",
          "name": "Jiwen Yu",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b20",
          "name": "Yiran Qin",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b21",
          "name": "Haoxuan Che",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b22",
          "name": "Quande Liu",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b23",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b24",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b25",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b26",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b27",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "681441e64d6a681c7c840b28",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/Y9dvfNIIrOJ4rCzwIB6XI.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/5kPNXDj9LIsgShhFz5WSg.jpeg"
      ],
      "publishedAt": "2025-04-30T17:59:02.000Z",
      "submittedOnDailyAt": "2025-05-02T02:29:37.187Z",
      "title": "Interactivo generación de videos investigación",
      "submittedOnDailyBy": {
        "_id": "64105a6d14215c0775dfdd14",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
        "isPro": false,
        "fullname": "Jiwen Yu",
        "user": "VictorYuki",
        "type": "user"
      },
      "summary": "El IGV (Interactive Generative Video) ha surgido como una tecnología crucial en respuesta a la creciente demanda por contenido de vídeo interactivo de alta calidad. En este artículo, se define el IGV como una tecnología que no solo genera contenido de vídeo de alta calidad, sino que también proporciona funciones interactivas que controlan las acciones del usuario y ofrecen retroalimentación. Se examina el estado actual de las aplicaciones de IGV, con un enfoque en tres áreas principales: juegos, inteligencia artificial de visualización y conducción autónoma. En los juegos, el IGV permite explorar infinitamente el mundo base. En la inteligencia artificial de visualización, el IGV utiliza entornos sintéticos con conocimientos físicos para aprender en entornos interactivos. En la conducción autónoma, el IGV proporciona funciones de simulación y validación en ciclos cerrados para garantizar la seguridad. Para guiar el desarrollo futuro, se propone un complejo marco de trabajo que divide un sistema ideal de IGV en cinco módulos básicos: generación, control, memoria, dinámica y inteligencia. Además, se analiza sistemáticamente los problemas técnicos y las direcciones futuras de cada componente del sistema ideal de IGV. Esta análisis sistemático estima que fomenterá la investigación y el desarrollo en el campo del IGV y promoverá el desarrollo de aplicaciones más complejas y prácticas.",
      "upvotes": 24,
      "discussionId": "681441e84d6a681c7c840bae",
      "ai_keywords": [
        "generative capabilities",
        "interactive features",
        "control signals",
        "responsive feedback",
        "virtual worlds",
        "physics-aware environment synthesizer",
        "multimodal interaction",
        "dynamically evolving scenes",
        "closed-loop simulation",
        "safety-critical testing",
        "validation",
        "real-time generation",
        "open-domain control",
        "long-term coherence",
        "accurate physics",
        "causal reasoning"
      ]
    },
    "publishedAt": "2025-04-30T13:59:02.000Z",
    "title": "A Survey of Interactive Generative Video",
    "summary": "Interactive Generative Video (IGV) has emerged as a crucial technology in\nresponse to the growing demand for high-quality, interactive video content\nacross various domains. In this paper, we define IGV as a technology that\ncombines generative capabilities to produce diverse high-quality video content\nwith interactive features that enable user engagement through control signals\nand responsive feedback. We survey the current landscape of IGV applications,\nfocusing on three major domains: 1) gaming, where IGV enables infinite\nexploration in virtual worlds; 2) embodied AI, where IGV serves as a\nphysics-aware environment synthesizer for training agents in multimodal\ninteraction with dynamically evolving scenes; and 3) autonomous driving, where\nIGV provides closed-loop simulation capabilities for safety-critical testing\nand validation. To guide future development, we propose a comprehensive\nframework that decomposes an ideal IGV system into five essential modules:\nGeneration, Control, Memory, Dynamics, and Intelligence. Furthermore, we\nsystematically analyze the technical challenges and future directions in\nrealizing each component for an ideal IGV system, such as achieving real-time\ngeneration, enabling open-domain control, maintaining long-term coherence,\nsimulating accurate physics, and integrating causal reasoning. We believe that\nthis systematic analysis will facilitate future research and development in the\nfield of IGV, ultimately advancing the technology toward more sophisticated and\npractical applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/Y9dvfNIIrOJ4rCzwIB6XI.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64105a6d14215c0775dfdd14/5kPNXDj9LIsgShhFz5WSg.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64105a6d14215c0775dfdd14",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
      "fullname": "Jiwen Yu",
      "name": "VictorYuki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.00662",
      "authors": [
        {
          "_id": "68142e4a551709da9244e8d1",
          "user": {
            "_id": "64b7df742f5a966b973e25f7",
            "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
            "isPro": false,
            "fullname": "Wenkai Yang",
            "user": "Keven16",
            "type": "user"
          },
          "name": "Wenkai Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-02T06:34:18.531Z",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d2",
          "name": "Jingwen Chen",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d3",
          "name": "Yankai Lin",
          "hidden": false
        },
        {
          "_id": "68142e4a551709da9244e8d4",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T17:03:17.000Z",
      "submittedOnDailyAt": "2025-05-02T01:12:33.949Z",
      "title": "DeepCritic: Evaluación Prudiciada Mediante Modelos de Lenguaje",
      "submittedOnDailyBy": {
        "_id": "64b7df742f5a966b973e25f7",
        "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
        "isPro": false,
        "fullname": "Wenkai Yang",
        "user": "Keven16",
        "type": "user"
      },
      "summary": "El rápido desarrollo de los LLM hace que proporcionar retroalimentación precisa y un monitoreo escalable sea urgente y importante. Utilizar modelos de evaluación de LLM para lograr un supervisión automatizada es una solución deseable. En este estudio, se investiga y mejora la capacidad de los LLM para evaluar matemáticamente. Los actuales modelos de evaluación de LLM solo proporcionan evaluaciones superficiales en cada etapa, con baja precisión y no ofrecen suficiente retroalimentación para los generadores de LLM. Para resolver estos problemas, se propone un nuevo marco de dos etapas efectivo para evaluar cada etapa de la resolución de problemas matemáticos de manera intencional. En la primera etapa, se utiliza Qwen2.5-72B-Instruct para generar evaluaciones de 4.5K largas oraciones, que se utilizan como datos de entrenamiento para ajustes normativos. Cada evaluación incluye verificación desde diferentes perspectivas y evaluaciones detalladas de la evaluación inicial en cada etapa. A continuación, se utilizan datos de etiquetado humano provenientes de PRM800K y datos de notas automáticas basados en muestreo Monte Carlo y evaluación de precisión para realizar entrenamiento de reinforcement en el modelo ajustado, lo que mejora su capacidad de evaluación. El modelo de evaluación basado en Qwen2.5-7B-Instruct supera significativamente a los actuales modelos de evaluación de LLM (incluyendo DeepSeek-R1-distill de la misma tamaño y GPT-4o) en el marco de referencia de reconocimiento de errores. Además, ofrece retroalimentación más detallada que puede efectivamente impulsar la corrección de errores en los generadores de LLM.",
      "upvotes": 21,
      "discussionId": "68142e4b551709da9244e8f8",
      "ai_keywords": [
        "LLMs (Large Language Models)",
        "critique models",
        "automated supervision",
        "math critique ability",
        "supervised fine-tuning",
        "Qwen2.5-72B-Instruct",
        "seed data",
        "deliberate step-wise critiques",
        "multi-perspective verifications",
        "reinforcement learning",
        "PRM800K",
        "Monte Carlo sampling-based correctness estimation",
        "Qwen2.5-7B-Instruct",
        "DeepSeek-R1-distill models",
        "GPT-4o",
        "error identification benchmarks"
      ]
    },
    "publishedAt": "2025-05-01T13:03:17.000Z",
    "title": "DeepCritic: Deliberate Critique with Large Language Models",
    "summary": "As Large Language Models (LLMs) are rapidly evolving, providing accurate\nfeedback and scalable oversight on their outputs becomes an urgent and critical\nproblem. Leveraging LLMs as critique models to achieve automated supervision is\na promising solution. In this work, we focus on studying and enhancing the math\ncritique ability of LLMs. Current LLM critics provide critiques that are too\nshallow and superficial on each step, leading to low judgment accuracy and\nstruggling to offer sufficient feedback for the LLM generator to correct\nmistakes. To tackle this issue, we propose a novel and effective two-stage\nframework to develop LLM critics that are capable of deliberately critiquing on\neach reasoning step of math solutions. In the first stage, we utilize\nQwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for\nsupervised fine-tuning. Each seed critique consists of deliberate step-wise\ncritiques that includes multi-perspective verifications as well as in-depth\ncritiques of initial critiques for each reasoning step. Then, we perform\nreinforcement learning on the fine-tuned model with either existing\nhuman-labeled data from PRM800K or our automatically annotated data obtained\nvia Monte Carlo sampling-based correctness estimation, to further incentivize\nits critique ability. Our developed critique model built on Qwen2.5-7B-Instruct\nnot only significantly outperforms existing LLM critics (including the\nsame-sized DeepSeek-R1-distill models and GPT-4o) on various error\nidentification benchmarks, but also more effectively helps the LLM generator\nrefine erroneous steps through more detailed feedback.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00662.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "64b7df742f5a966b973e25f7",
      "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg",
      "fullname": "Wenkai Yang",
      "name": "Keven16",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.00703",
      "authors": [
        {
          "_id": "681428debcdf962d03da2797",
          "name": "Dongzhi Jiang",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da2798",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da2799",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279a",
          "name": "Zhuofan Zong",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279b",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279c",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279d",
          "name": "Shilin Yan",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279e",
          "name": "Pheng-Ann Heng",
          "hidden": false
        },
        {
          "_id": "681428debcdf962d03da279f",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T17:59:46.000Z",
      "submittedOnDailyAt": "2025-05-02T00:38:40.412Z",
      "title": "T2I-R1: Método para fortalecer la generación de imágenes mediante cooperación semántica y token-level",
      "submittedOnDailyBy": {
        "_id": "6349214f8146350b3a4c5cdf",
        "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
        "isPro": false,
        "fullname": "Dongzhi Jiang",
        "user": "CaraJ",
        "type": "user"
      },
      "summary": "El reciente desarrollo de modelos de lenguaje de gran escala ha demostrado que el chain-of-thought (CoT) y el aprendizaje por refuerzo (RL) mejoran los resultados. Sin embargo, la aplicación de estos métodos en el área de generación de imágenes ha sido poco estudiada. En este artículo, proponemos una nueva teoría de inferencia fortalecida con CoT bi-level para un modelo de generación de imágenes a partir de texto, denominado T2I-R1. Específicamente, para mejorar las diferencias durante el proceso de generación, se distinguen dos niveles de CoT: el nivel de significado y el nivel de tokens. Además, para mejorar la coordinación de estos dos niveles, se introduce BiCoT-GRPO y se utiliza una compensación de generación que divide ambos CoT en el mismo estado de entrenamiento. Al aplicar estas innovaciones a la base de modelo Janus-Pro, se logra un aumento del 13% en T2I-CompBench y del 19% en el WISE benchmark, superando los modelos más recientes como FLUX. El código está disponible en la siguiente URL: https://github.com/CaraJ7/T2I-R1",
      "upvotes": 13,
      "discussionId": "681428dfbcdf962d03da281c",
      "githubRepo": "https://github.com/CaraJ7/T2I-R1",
      "ai_keywords": [
        "chain-of-thought (CoT)",
        "reinforcement learning (RL)",
        "text-to-image generation model",
        "bi-level CoT reasoning process",
        "semantic-level CoT",
        "token-level CoT",
        "BiCoT-GRPO",
        "generation rewards",
        "Janus-Pro",
        "T2I-CompBench",
        "WISE benchmark",
        "FLUX"
      ]
    },
    "publishedAt": "2025-05-01T13:59:46.000Z",
    "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level\n  and Token-level CoT",
    "summary": "Recent advancements in large language models have demonstrated how\nchain-of-thought (CoT) and reinforcement learning (RL) can improve performance.\nHowever, applying such reasoning strategies to the visual generation domain\nremains largely unexplored. In this paper, we present T2I-R1, a novel\nreasoning-enhanced text-to-image generation model, powered by RL with a\nbi-level CoT reasoning process. Specifically, we identify two levels of CoT\nthat can be utilized to enhance different stages of generation: (1) the\nsemantic-level CoT for high-level planning of the prompt and (2) the\ntoken-level CoT for low-level pixel processing during patch-by-patch\ngeneration. To better coordinate these two levels of CoT, we introduce\nBiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes\nboth generation CoTs within the same training step. By applying our reasoning\nstrategies to the baseline model, Janus-Pro, we achieve superior performance\nwith 13% improvement on T2I-CompBench and 19% improvement on the WISE\nbenchmark, even surpassing the state-of-the-art model FLUX.1. Code is available\nat: https://github.com/CaraJ7/T2I-R1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6349214f8146350b3a4c5cdf",
      "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
      "fullname": "Dongzhi Jiang",
      "name": "CaraJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21659",
      "authors": [
        {
          "_id": "68142de6111ccf18a993c890",
          "name": "Haotian Luo",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c891",
          "name": "Haiying He",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c892",
          "name": "Yibo Wang",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c893",
          "name": "Jinluan Yang",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c894",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c895",
          "name": "Naiqiang Tan",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c896",
          "name": "Xiaochun Cao",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c897",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "68142de6111ccf18a993c898",
          "name": "Li Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T14:01:45.000Z",
      "submittedOnDailyAt": "2025-05-02T01:01:49.479Z",
      "title": "Procesamiento Logístico Adaptativo Bi-Nivel desde el Contexto de Largo Plazo hacia el Contexto Hybrid",
      "submittedOnDailyBy": {
        "_id": "632ab8f5a968c34257da5c52",
        "avatarUrl": "/avatars/59df09e6c9e1e633170514d950ad7981.svg",
        "isPro": false,
        "fullname": "Haotian Luo",
        "user": "LordNoah",
        "type": "user"
      },
      "summary": "Recientemente, modelos de pensamiento a largo plazo han demostrado un excelente rendimiento en tareas complejas de pensamiento, pero su inferencia es costosa y su eficiencia es un problema general. Nuestro análisis experimental muestra que los beneficios de la utilización de pensamiento a largo plazo dependen del problema: en algunos casos, el pensamiento complejo es necesario, mientras que en otros, no se observa mejoría o la precisión disminuye. Por lo tanto, es necesario una estrategia adaptativa para ajustar la profundidad del pensamiento a la entrada, pero los estudios previos están limitados a reducir la redundancia dentro del paso de pensamiento a largo plazo y explorar estrategias más eficientes que superen el paradigma de Long-CoT. En respuesta a esto, proponemos un nuevo marco de dos etapas para pensamiento adaptativo y eficiente. Primero, integramos modelos de pensamiento a largo plazo y de corto plazo para construir un modelo de pensamiento confuso que permita múltiples estilos de pensamiento. Luego, aplicamos entrenamiento de niveles para inducir al modelo a elegir el estilo de pensamiento adecuado y guiar a los modelos dentro de cada grupo de estilos hacia pensamientos más claros y precisos. Los experimentos muestran que este enfoque puede reducir significativamente los costos de inferencia, manteniendo o mejorando la precisión. En particular, la longitud media de los pensamientos en 5 conjuntos de datos de matemáticas se redujo en más del 50%, demostrando claramente la posibilidad de optimizar la eficiencia de pensamiento en modelos de lenguaje grandes. Nuestro código está disponible en https://github.com/StarDewXXX/AdaR1.",
      "upvotes": 3,
      "discussionId": "68142de7111ccf18a993c8ba",
      "ai_keywords": [
        "CoT models",
        "Long-CoT",
        "hybrid reasoning model",
        "bi-level preference training",
        "adaptive reasoning strategies"
      ]
    },
    "publishedAt": "2025-04-30T10:01:45.000Z",
    "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\n  Optimization",
    "summary": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21659.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632ab8f5a968c34257da5c52",
      "avatarUrl": "/avatars/59df09e6c9e1e633170514d950ad7981.svg",
      "fullname": "Haotian Luo",
      "name": "LordNoah",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.00497",
      "authors": [
        {
          "_id": "68147d4d687b82a9b6308cfd",
          "name": "Antoni Bigata",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308cfe",
          "name": "Rodrigo Mira",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308cff",
          "name": "Stella Bounareli",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d00",
          "name": "Michał Stypułkowski",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d01",
          "name": "Konstantinos Vougioukas",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d02",
          "name": "Stavros Petridis",
          "hidden": false
        },
        {
          "_id": "68147d4d687b82a9b6308d03",
          "name": "Maja Pantic",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/640777812e309e65452491dd/4ao3SssNM8wCiTo5amAxn.mp4"
      ],
      "publishedAt": "2025-05-01T12:56:17.000Z",
      "submittedOnDailyAt": "2025-05-02T06:38:20.471Z",
      "title": "KeySync: Syncronización de boca fuerte y efectiva, sin necesidad de sincronización de alta resolución o de boca fina.",
      "submittedOnDailyBy": {
        "_id": "640777812e309e65452491dd",
        "avatarUrl": "/avatars/01b927675e78f212408168522f65fe36.svg",
        "isPro": true,
        "fullname": "Antoni Bigata",
        "user": "toninio19",
        "type": "user"
      },
      "summary": "El problema de sincronización de labios es un trabajo que consiste en ajustar los movimientos de los labios de una videográfica con un nuevo entrada de voz, generalmente tratado como una versión sencilla de animación facial de voz. Sin embargo, la sincronización de labios adiciona problemas como la coincidencia temporal, mientras que en los videos de entrada presentan problemas nuevos como la falta de expresión facial y la ocultación de la cara, lo que puede afectar significativamente aplicaciones reales como la traducción automática. Sin embargo, actualmente no se analizan estos problemas de manera adecuada en la investigación. Para resolver estos inconvenientes, proponemos un marco de trabajo de 2 etapas llamado KeySync. Este marco resuelve los problemas de coincidencia temporal y utiliza una tecnología de máscara diseñada con precisión para abordar la falta de expresión y la ocultación de la cara. KeySync realiza los últimos resultados en la reconstrucción de labios y sincronización cruzada mediante el nuevo métrico de falta de expresión LipLeak, mejorando la calidad visual y reduciendo la falta de expresión. Además, muestra el efecto de nuevas técnicas de máscara y evalúa las decisiones estructurales mediante varios estudios de reducción. Los códigos y pesos del modelo están disponibles en https://antonibigata.github.io/KeySync.",
      "upvotes": 2,
      "discussionId": "68147d53687b82a9b6308e59",
      "projectPage": "https://antonibigata.github.io/KeySync/",
      "githubRepo": "https://github.com/antonibigata/keysync",
      "ai_keywords": [
        "KeySync",
        "lip synchronization",
        "audio-driven facial animation",
        "talking head generation",
        "temporal consistency",
        "expression leakage",
        "facial occlusions",
        "automated dubbing",
        "lip reconstruction",
        "cross-synchronization",
        "visual quality",
        "LipLeak",
        "masking strategy",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-05-01T08:56:17.000Z",
    "title": "KeySync: A Robust Approach for Leakage-free Lip Synchronization in High\n  Resolution",
    "summary": "Lip synchronization, known as the task of aligning lip movements in an\nexisting video with new input audio, is typically framed as a simpler variant\nof audio-driven facial animation. However, as well as suffering from the usual\nissues in talking head generation (e.g., temporal consistency), lip\nsynchronization presents significant new challenges such as expression leakage\nfrom the input video and facial occlusions, which can severely impact\nreal-world applications like automated dubbing, but are often neglected in\nexisting works. To address these shortcomings, we present KeySync, a two-stage\nframework that succeeds in solving the issue of temporal consistency, while\nalso incorporating solutions for leakage and occlusions using a carefully\ndesigned masking strategy. We show that KeySync achieves state-of-the-art\nresults in lip reconstruction and cross-synchronization, improving visual\nquality and reducing expression leakage according to LipLeak, our novel leakage\nmetric. Furthermore, we demonstrate the effectiveness of our new masking\napproach in handling occlusions and validate our architectural choices through\nseveral ablation studies. Code and model weights can be found at\nhttps://antonibigata.github.io/KeySync.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/640777812e309e65452491dd/4ao3SssNM8wCiTo5amAxn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00497.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640777812e309e65452491dd",
      "avatarUrl": "/avatars/01b927675e78f212408168522f65fe36.svg",
      "fullname": "Antoni Bigata",
      "name": "toninio19",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20605",
      "authors": [
        {
          "_id": "68131e73f0f2a4d8b2d4b06a",
          "user": {
            "_id": "642bcb8ae5b6823cde9301bd",
            "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
            "isPro": false,
            "fullname": "Mihai Dan Nadăș",
            "user": "mihainadas",
            "type": "user"
          },
          "name": "Mihai Nadas",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-02T06:34:48.889Z",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06b",
          "name": "Laura Diosan",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06c",
          "user": {
            "_id": "67b2344d0ce2aaa57c8c9997",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b2344d0ce2aaa57c8c9997/LSMjuQNjRsUllUQyt9vNo.jpeg",
            "isPro": false,
            "fullname": "Andrei Piscoran",
            "user": "andreiPiscoran",
            "type": "user"
          },
          "name": "Andrei Piscoran",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-01T07:11:31.916Z",
          "hidden": false
        },
        {
          "_id": "68131e73f0f2a4d8b2d4b06d",
          "user": {
            "_id": "677e4393ef848c5a5352d082",
            "avatarUrl": "/avatars/bcb60ead58969601e2911053550fec62.svg",
            "isPro": false,
            "fullname": "Andreea Tomescu",
            "user": "andreeatomescu",
            "type": "user"
          },
          "name": "Andreea Tomescu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-01T07:10:43.780Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T10:15:28.000Z",
      "submittedOnDailyAt": "2025-05-02T06:22:34.343Z",
      "title": "TF1-EN-3M: Dataset de entrenamiento para pequeños modelos de lenguaje abierto utilizando historias de molla de tres millones de palabras",
      "submittedOnDailyBy": {
        "_id": "642bcb8ae5b6823cde9301bd",
        "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
        "isPro": false,
        "fullname": "Mihai Dan Nadăș",
        "user": "mihainadas",
        "type": "user"
      },
      "summary": "La historia de Morna ha sido utilizada como herramienta histórica para transmitir valores, pero la NLP moderna carece de grandes estructuras de corpus que combinen una red lógica coherente y enseñanzas éticas explícitas. Para llenar esta brecha, ofrecemos TF1-EN-3M, el primer dataset abierto que incluye 3 millones de textos en inglés de Morna, generados con un modelo de 8B parámetros. Cada texto es combinado utilizando un motor de generación basado en las secuencias de 6 fósforos (personaje -> característica -> entorno -> conflicto -> resultado -> ética), manteniendo la veracidad de géneros y cubriendo una amplia gama de temas.\n\nEl proceso de evaluación híbrido evalúa (i) la gramática, creatividad, claridad ética y respeto del template con evaluadores basados en GPT, y (ii) combina métricas de diversidad y legibilidad sin referencias. Entre los 10 candidatos de weights abiertos, la versión de Llama-3 con 8B parámetros ofrece el mejor equilibrio entre calidad y velocidad. Con un solo GPU de computadora (<24GB de VRAM), puede generar 1,000 textos de Morna con altos puntajes (aproximadamente 13.5 centímetros).\n\nEl dataset, código de generación, scripts de evaluación y imágenes completas de media están disponibles bajo una licencia de uso libre, y se ofrecen también un benchmark completo de reproducibilidad y costo. TF1-EN-3M abre caminos para investigaciones en engenería narrativa, ajuste de valores, y AI educativo para niños, demostrando que no es necesario modelos de narrativa de gran escala para transmitir valores éticos.",
      "upvotes": 2,
      "discussionId": "68131e73f0f2a4d8b2d4b087",
      "githubRepo": "https://github.com/klusai/tinyfabulist",
      "ai_keywords": [
        "instruction-tuned models",
        "combinatorial prompt engine",
        "GPT-based critic",
        "template adherence",
        "reference-free diversity",
        "Llama-3 variant",
        "computational efficiency",
        "permissive license",
        "child-friendly educational AI",
        "moral storytelling"
      ]
    },
    "publishedAt": "2025-04-29T06:15:28.000Z",
    "title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open\n  Language Models",
    "summary": "Moral stories are a time-tested vehicle for transmitting values, yet modern\nNLP lacks a large, structured corpus that couples coherent narratives with\nexplicit ethical lessons. We close this gap with TF1-EN-3M, the first open\ndataset of three million English-language fables generated exclusively by\ninstruction-tuned models no larger than 8B parameters. Each story follows a\nsix-slot scaffold (character -> trait -> setting -> conflict -> resolution ->\nmoral), produced through a combinatorial prompt engine that guarantees genre\nfidelity while covering a broad thematic space.\n  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores\ngrammar, creativity, moral clarity, and template adherence with (ii)\nreference-free diversity and readability metrics. Among ten open-weight\ncandidates, an 8B-parameter Llama-3 variant delivers the best quality-speed\ntrade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)\nat approximately 13.5 cents per 1,000 fables.\n  We release the dataset, generation code, evaluation scripts, and full\nmetadata under a permissive license, enabling exact reproducibility and cost\nbenchmarking. TF1-EN-3M opens avenues for research in instruction following,\nnarrative intelligence, value alignment, and child-friendly educational AI,\ndemonstrating that large-scale moral storytelling no longer requires\nproprietary giant models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20605.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642bcb8ae5b6823cde9301bd",
      "avatarUrl": "/avatars/cddd29afbdbc2fbea90612567090147b.svg",
      "fullname": "Mihai Dan Nadăș",
      "name": "mihainadas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18983",
      "authors": [
        {
          "_id": "681470d72175e5e7ca0ea002",
          "name": "Xuyin Qi",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea003",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea004",
          "name": "Canxuan Gang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea005",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea006",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea007",
          "name": "Zhiwei Zhang",
          "hidden": false
        },
        {
          "_id": "681470d72175e5e7ca0ea008",
          "name": "Yang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-26T17:56:56.000Z",
      "submittedOnDailyAt": "2025-05-02T05:44:53.105Z",
      "title": "MediAug: Revisión de la visualización de imágenes médicas en el entorno de trabajo de atención al usuario",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "La extensión de datos es crucial para mejorar la precisión de clasificación en imágenes médicas, así como para mejorar la detección de daños y la separación de órganos en condiciones limitadas de datos. Sin embargo, presenta dos problemas significativos. El primero es el gran gap entre el domínio de las imágenes naturales y las médicas, que puede alterar características importantes de enfermedades. El segundo es que la investigación en extensión de datos médicos está limitada a una sola arquitectura o estrategia, lo que no refleja claramente los beneficios de estrategias híbridas avanzadas. Para abordar estos problemas, proponemos un marco de evaluación que integra seis métodos de extensión basados en híbrido y transformer en dos estrategias: híbrido y transformer, aplicados a un conjunto de datos de MRI de tumores cerebrales y a imágenes de fundus de afecciones oculares. Nuestro contribución incluye tres aspectos: primero, introducimos MediAug, un marco de referencia detallado y replicable para la extensión de datos médicos; segundo, realizamos una evaluación sistemática de métodos como MixUp, YOCO, CropMix, CutMix, AugMix y SnapMix, basados en ResNet-50 y ViT-B; y tercero, presentamos resultados de experimentos que muestran que MixUp es el más efectivo en la tarea de clasificación de tumores cerebrales en ResNet-50 (79.19% de precisión), SnapMix en la tarea de clasificación de tumores cerebrales en ViT-B (99.44% de precisión), YOCO en la tarea de clasificación de afecciones oculares en ResNet-50 (91.60% de precisión) y CutMix en la tarea de clasificación de afecciones oculares en ViT-B (97.94% de precisión). El código está disponible en https://github.com/AIGeeksGroup/MediAug.",
      "upvotes": 1,
      "discussionId": "681470d92175e5e7ca0ea065",
      "ai_keywords": [
        "MediAug",
        "MixUp",
        "YOCO",
        "CropMix",
        "CutMix",
        "AugMix",
        "SnapMix",
        "ResNet-50",
        "ViT-B",
        "brain tumour MRI",
        "eye disease fundus datasets",
        "domain gap",
        "lesion detection",
        "organ segmentation",
        "classification accuracy"
      ]
    },
    "publishedAt": "2025-04-26T13:56:56.000Z",
    "title": "MediAug: Exploring Visual Augmentation in Medical Imaging",
    "summary": "Data augmentation is essential in medical imaging for improving\nclassification accuracy, lesion detection, and organ segmentation under limited\ndata conditions. However, two significant challenges remain. First, a\npronounced domain gap between natural photographs and medical images can\ndistort critical disease features. Second, augmentation studies in medical\nimaging are fragmented and limited to single tasks or architectures, leaving\nthe benefits of advanced mix-based strategies unclear. To address these\nchallenges, we propose a unified evaluation framework with six mix-based\naugmentation methods integrated with both convolutional and transformer\nbackbones on brain tumour MRI and eye disease fundus datasets. Our\ncontributions are threefold. (1) We introduce MediAug, a comprehensive and\nreproducible benchmark for advanced data augmentation in medical imaging. (2)\nWe systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix\nwith ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive\nexperiments that MixUp yields the greatest improvement on the brain tumor\nclassification task for ResNet-50 with 79.19% accuracy and SnapMix yields the\ngreatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the\ngreatest improvement on the eye disease classification task for ResNet-50 with\n91.60% accuracy and CutMix yields the greatest improvement for ViT-B with\n97.94% accuracy. Code will be available at\nhttps://github.com/AIGeeksGroup/MediAug.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18983.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]