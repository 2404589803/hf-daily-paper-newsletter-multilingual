[
  {
    "paper": {
      "id": "2505.14683",
      "authors": [
        {
          "_id": "682d2fd84540abccd3b835e8",
          "name": "Chaorui Deng",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835e9",
          "name": "Deyao Zhu",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ea",
          "user": {
            "_id": "61fb81006374891646732f37",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg",
            "isPro": false,
            "fullname": "Kunchang Li",
            "user": "Andy1621",
            "type": "user"
          },
          "name": "Kunchang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:41:06.469Z",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835eb",
          "user": {
            "_id": "652e9c5774d1b0d7ff73d091",
            "avatarUrl": "/avatars/a6d2098b3dde4a8b7488a193f0ecb776.svg",
            "isPro": true,
            "fullname": "Chenhui Gou",
            "user": "gouc",
            "type": "user"
          },
          "name": "Chenhui Gou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:41:08.903Z",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ec",
          "name": "Feng Li",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ed",
          "name": "Zeyu Wang",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ee",
          "name": "Shu Zhong",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ef",
          "user": {
            "_id": "5df833bdda6d0311fd3d5403",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5df833bdda6d0311fd3d5403/62OtGJEQXdOuhV9yCd4HS.png",
            "isPro": false,
            "fullname": "Weihao Yu",
            "user": "whyu",
            "type": "user"
          },
          "name": "Weihao Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T09:31:55.569Z",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835f0",
          "user": {
            "_id": "64b6b81142134e053233c3c0",
            "avatarUrl": "/avatars/5c7455d99a7a2648f77a531c9a71eb98.svg",
            "isPro": false,
            "fullname": "Xiaonan Nie",
            "user": "codecaution",
            "type": "user"
          },
          "name": "Xiaonan Nie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:06:14.057Z",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835f1",
          "user": {
            "_id": "617fe76105423df678cef199",
            "avatarUrl": "/avatars/64c94a4d743edab18ecb4bb7c550f049.svg",
            "isPro": false,
            "fullname": "Song",
            "user": "Ziang",
            "type": "user"
          },
          "name": "Ziang Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:06:07.780Z",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835f2",
          "name": "Guang Shi",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835f3",
          "name": "Haoqi Fan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/61fb81006374891646732f37/HQOfWqrOf9B97hWczL489.png"
      ],
      "publishedAt": "2025-05-20T17:59:30.000Z",
      "submittedOnDailyAt": "2025-05-21T00:38:53.960Z",
      "title": "1. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n2. Principales Propiedades en Preentrenamiento Multimodal Unificado\n3. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n4. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n5. Principales Propiedades en Preentrenamiento Multimodal Unificado\n6. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n7. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n8. Principales Propiedades en Preentrenamiento Multimodal Unificado\n9. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n10. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n11. Principales Propiedades en Preentrenamiento Multimodal Unificado\n12. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n13. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n14. Principales Propiedades en Preentrenamiento Multimodal Unificado\n15. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n16. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n17. Principales Propiedades en Preentrenamiento Multimodal Unificado\n18. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n19. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n20. Principales Propiedades en Preentrenamiento Multimodal Unificado\n21. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n22. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n23. Principales Propiedades en Preentrenamiento Multimodal Unificado\n24. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n25. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n26. Principales Propiedades en Preentrenamiento Multimodal Unificado\n27. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n28. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n29. Principales Propiedades en Preentrenamiento Multimodal Unificado\n30. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n31. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n32. Principales Propiedades en Preentrenamiento Multimodal Unificado\n33. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n34. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n35. Principales Propiedades en Preentrenamiento Multimodal Unificado\n36. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n37. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n38. Principales Propiedades en Preentrenamiento Multimodal Unificado\n39. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n40. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n41. Principales Propiedades en Preentrenamiento Multimodal Unificado\n42. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n43. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n44. Principales Propiedades en Preentrenamiento Multimodal Unificado\n45. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n46. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n47. Principales Propiedades en Preentrenamiento Multimodal Unificado\n48. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n49. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n50. Principales Propiedades en Preentrenamiento Multimodal Unificado\n51. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n52. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n53. Principales Propiedades en Preentrenamiento Multimodal Unificado\n54. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n55. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n56. Principales Propiedades en Preentrenamiento Multimodal Unificado\n57. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n58. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n59. Principales Propiedades en Preentrenamiento Multimodal Unificado\n60. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n61. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n62. Principales Propiedades en Preentrenamiento Multimodal Unificado\n63. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n64. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n65. Principales Propiedades en Preentrenamiento Multimodal Unificado\n66. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n67. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n68. Principales Propiedades en Preentrenamiento Multimodal Unificado\n69. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n70. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n71. Principales Propiedades en Preentrenamiento Multimodal Unificado\n72. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n73. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n74. Principales Propiedades en Preentrenamiento Multimodal Unificado\n75. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n76. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n77. Principales Propiedades en Preentrenamiento Multimodal Unificado\n78. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n79. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n80. Principales Propiedades en Preentrenamiento Multimodal Unificado\n81. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n82. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n83. Principales Propiedades en Preentrenamiento Multimodal Unificado\n84. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n85. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n86. Principales Propiedades en Preentrenamiento Multimodal Unificado\n87. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n88. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n89. Principales Propiedades en Preentrenamiento Multimodal Unificado\n90. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n91. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n92. Principales Propiedades en Preentrenamiento Multimodal Unificado\n93. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n94. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n95. Principales Propiedades en Preentrenamiento Multimodal Unificado\n96. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n97. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado\n98. Principales Propiedades en Preentrenamiento Multimodal Unificado\n99. Nuevas Propiedades en Preentrenamiento Multimodal Unificado\n100. Propiedades Desarrollantes en Preentrenamiento Multimodal Unificado",
      "submittedOnDailyBy": {
        "_id": "61fb81006374891646732f37",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg",
        "isPro": false,
        "fullname": "Kunchang Li",
        "user": "Andy1621",
        "type": "user"
      },
      "summary": "La tecnología que integra la comprensión y la generación multimodal ha demostrado capacidades sorprendentes en sistemas de vanguardia. En este estudio, se presenta BAGEL, un modelo basado en código abierto que apoya de manera fundamental la comprensión y la generación multimodal. BAGEL consiste en un modelo decodificador unificado que ha sido pretrenado con cientos de millones de tokens seleccionados de grandes conjuntos de datos interactivos de texto, imágenes, videos y web. La escalabilidad de esta diversidad de datos multimodal interactivos permite que BAGEL desarrolle las emergentes capacidades de inferencia multimodal compleja. Consequentemente, supera significativamente los rendimientos de generación y comprensión multimodal en modelos abiertos de referencia, desarrollando habilidades avanzadas de manipulación de imágenes, predicción de futuros frames, manipulación 3D y navegación en el mundo. Para promover las oportunidades futuras en la investigación multimodal, este estudio comparte los principales hallazgos, detalles de pretrenamiento y protocolos de generación de datos, y se lanza código y checkpoints a la comunidad. El sitio web del proyecto está disponible en https://bagel-ai.org/.",
      "upvotes": 53,
      "discussionId": "682d2fdc4540abccd3b836ee",
      "ai_keywords": [
        "unified, decoder-only model",
        "pretrained",
        "trillions of tokens",
        "large-scale interleaved data",
        "complex multimodal reasoning",
        "multimodal generation",
        "multimodal understanding",
        "free-form image manipulation",
        "future frame prediction",
        "3D manipulation",
        "world navigation"
      ]
    },
    "publishedAt": "2025-05-20T13:59:30.000Z",
    "title": "Emerging Properties in Unified Multimodal Pretraining",
    "summary": "Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/61fb81006374891646732f37/HQOfWqrOf9B97hWczL489.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14683.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61fb81006374891646732f37",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg",
      "fullname": "Kunchang Li",
      "name": "Andy1621",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11594",
      "authors": [
        {
          "_id": "682d426251ce04237318cfe5",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:46.065Z",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfe6",
          "name": "Jia Wei",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfe7",
          "user": {
            "_id": "62cc11a4f1d37c16280a2923",
            "avatarUrl": "/avatars/265b3cfb80f0a7b11a2ef67c49e29cf7.svg",
            "isPro": false,
            "fullname": "Pengle Zhang",
            "user": "Guyan",
            "type": "user"
          },
          "name": "Pengle Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:07:09.573Z",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfe8",
          "name": "Xiaoming Xu",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfe9",
          "user": {
            "_id": "67ea1f6693f71dd8167a2d22",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/H_upra_XVG1AoBKUe9ArV.png",
            "isPro": false,
            "fullname": "haofeng huang",
            "user": "haofeng666",
            "type": "user"
          },
          "name": "Haofeng Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:06:48.450Z",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfea",
          "name": "Haoxu Wang",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfeb",
          "name": "Kai Jiang",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfec",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfed",
          "user": {
            "_id": "65fcad0ba0d7adc40b54fac2",
            "avatarUrl": "/avatars/7564b5642378fddb46ec3b5ae57c0402.svg",
            "isPro": false,
            "fullname": "Jianfei Chen",
            "user": "surfingtomchen",
            "type": "user"
          },
          "name": "Jianfei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:06:40.981Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/Tb20E3IJSV6PjcD9Nkvfg.png"
      ],
      "publishedAt": "2025-05-16T18:01:54.000Z",
      "submittedOnDailyAt": "2025-05-21T01:35:25.101Z",
      "title": "SageAttention3: Revisión de la Inferencia de Micro-Cycling FP4 Attention y el Aprendizaje de 8 Bits",
      "submittedOnDailyBy": {
        "_id": "66c0a08bac74db25de8427ec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
        "isPro": false,
        "fullname": "Jintao Zhang",
        "user": "jt-zhang",
        "type": "user"
      },
      "summary": "La eficiencia de la atención es crucialmente influenciada por su complejidad temporal bidimensional. Hemos mejorado la eficiencia de la atención a través de dos contribuciones principales. Primero, utilizamos los nuevos Cores de Tensor FP4 de la GPU Blackwell para acelerar los cálculos de atención. Nuestra implementación alcanza 1038 TOPS en el RTX5090, lo que es 5 veces más rápido que el más rápido FlashAttention en el mismo dispositivo. Estos experimentos demuestran que nuestro FP4 es un método estándar para acelerar la inferencia de diferentes modelos. Además, hemos introducido la atención en bajas tasas (por ejemplo, FlashAttention3 y SageAttention) en tareas de aprendizaje. Aunque actualmente estas técnicas se centran principalmente en la inferencia, la eficiencia de aprendizaje en modelos grandes es también importante. Para investigar si la atención en bajas tasas puede ser aplicada efectivamente en tareas de aprendizaje, hemos diseñado una atención eficiente y precisa en 8 bits tanto para la propagación como para la retropropagación. Nuestros experimentos muestran que la atención en 8 bits puede alcanzar un rendimiento sin pérdidas significativas y que el aprendizaje previo puede llegar a converger más lentamente. El código está disponible en https://github.com/thu-ml/SageAttention.",
      "upvotes": 26,
      "discussionId": "682d426551ce04237318d0b9",
      "projectPage": "https://github.com/thu-ml/SageAttention",
      "githubRepo": "https://github.com/thu-ml/SageAttention",
      "ai_keywords": [
        "attention",
        "FP4 Tensor Cores",
        "TOPS",
        "speedup",
        "inference",
        "plug-and-play",
        "low-bit attention",
        "8-bit attention",
        "forward propagation",
        "backward propagation",
        "fine-tuning",
        "pretraining",
        "convergence",
        "lossless performance"
      ]
    },
    "publishedAt": "2025-05-16T14:01:54.000Z",
    "title": "SageAttention3: Microscaling FP4 Attention for Inference and An\n  Exploration of 8-Bit Training",
    "summary": "The efficiency of attention is important due to its quadratic time\ncomplexity. We enhance the efficiency of attention through two key\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\naccelerate attention computation. Our implementation achieves 1038 TOPS on\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\nExperiments show that our FP4 attention can accelerate inference of various\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\nfocus only on inference. However, the efficiency of training large models is\nalso important. To explore whether low-bit attention can be effectively applied\nto training tasks, we design an accurate and efficient 8-bit attention for both\nforward and backward propagation. Experiments indicate that 8-bit attention\nachieves lossless performance in fine-tuning tasks but exhibits slower\nconvergence in pretraining tasks. The code will be available at\nhttps://github.com/thu-ml/SageAttention.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/Tb20E3IJSV6PjcD9Nkvfg.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11594.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14246",
      "authors": [
        {
          "_id": "682d7a2340a42d1538fada76",
          "user": {
            "_id": "66fe1334ff3ee1f7569fab6d",
            "avatarUrl": "/avatars/6868b1a545028a9b8bbded52490dc093.svg",
            "isPro": false,
            "fullname": "ziyuliu",
            "user": "ziyuliu",
            "type": "user"
          },
          "name": "Ziyu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:08:39.517Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada77",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:39:42.962Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada78",
          "user": {
            "_id": "6671341b4eee852a8b25888f",
            "avatarUrl": "/avatars/635d1821bbc960b4ea845e606883eb16.svg",
            "isPro": false,
            "fullname": "yushan zou",
            "user": "zyshan",
            "type": "user"
          },
          "name": "Yushan Zou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:08:45.972Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada79",
          "user": {
            "_id": "652768eeb723b49e8c8865da",
            "avatarUrl": "/avatars/491e02da9ec81e439ccda8a181634bca.svg",
            "isPro": false,
            "fullname": "Zijian Liang",
            "user": "steins1096",
            "type": "user"
          },
          "name": "Zijian Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:08:51.938Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7a",
          "user": {
            "_id": "67c0849ee08c178ef8d4e05c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mQ6VdnjZnRhb0H_waPclo.png",
            "isPro": false,
            "fullname": "Xiaoyi Dong",
            "user": "sweetFruit",
            "type": "user"
          },
          "name": "Xiaoyi Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:09:00.912Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7b",
          "user": {
            "_id": "65000bef18830fabea469fdd",
            "avatarUrl": "/avatars/b320c77dfad039d9f9c54127f610d44f.svg",
            "isPro": false,
            "fullname": "Cao Yuhang",
            "user": "yhcao",
            "type": "user"
          },
          "name": "Yuhang Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:09:22.645Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7c",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:09:31.442Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7d",
          "user": {
            "_id": "636317ed80c1a705a6eff396",
            "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
            "isPro": false,
            "fullname": "Dahua Lin",
            "user": "lindahua",
            "type": "user"
          },
          "name": "Dahua Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:09:37.103Z",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7e",
          "user": {
            "_id": "64b4eec4faa3181a5eab9c46",
            "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
            "isPro": true,
            "fullname": "Jiaqi Wang",
            "user": "myownskyW7",
            "type": "user"
          },
          "name": "Jiaqi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:39:45.391Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T11:59:25.000Z",
      "submittedOnDailyAt": "2025-05-21T05:32:01.442Z",
      "title": "Visual Agentic Reinforcement Fine-Tuning",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "Los tendencias clave del modelo de lógica (por ejemplo, o3 de OpenAI) destacan la importancia de las habilidades de agente originales de herramientas externas. Esto implica el uso de un navegador web para realizar búsquedas o escribir y ejecutar código para acceder a imágenes. El comité de investigación abierto ha avanzado en el desarrollo de habilidades de agente basadas en lenguaje (llamadas llamadas a funciones y integración de herramientas), pero las capacidades de agente multiformes que realmente acceden a imágenes y su comparativa evaluación aún no han sido exploradas. En este artículo, se presenta la efectividad de modelos de lenguaje de visión (LVLMs) dotados de habilidades de lógica visual a través del entrenamiento de fortalecimiento visual-ARFT. Mediante Visual-ARFT, LVLMs abierto-source pueden realizar búsquedas en sitios web para actualizar información o manipular y analizar imágenes utilizando técnicas de procesamiento de imágenes, como cortes y rotaciones, mediante códigos escritos. Además, se proponen dos configuraciones del marco de referencia de agente multiforme (MAT): MAT-Search y MAT-Coding, para evaluar las capacidades de búsqueda y codificación de los LVLMs. Los resultados de los experimentos muestran que Visual-ARFT supera a los estándares en MAT-Coding con un F1 score de +18.6% y un EM score de +13.0%, y en MAT-Search con un F1 score de +10.3% y un EM score de +8.7%, superando finalmente a GPT-4o. En marcos de referencia de QA multiforme como 2Wiki y HotpotQA, Visual-ARFT logró un F1 score de +29.3% y un EM score de +25.9%, demostrando una fuerte capacidad de generalización. Nuestros hallazgos muestran que Visual-ARFT proporciona una ruta adecuada para la construcción de agentes multiformes robustos y generalizables.",
      "upvotes": 15,
      "discussionId": "682d7a2440a42d1538fadac0",
      "githubRepo": "https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT",
      "ai_keywords": [
        "Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT)",
        "Large Vision-Language Models (LVLMs)",
        "Multi-modal Agentic Tool Bench (MAT)",
        "MAT-Search",
        "MAT-Coding",
        "F1",
        "EM",
        "GPT-4o",
        "multi-hop QA benchmarks",
        "2Wiki",
        "HotpotQA"
      ]
    },
    "publishedAt": "2025-05-20T07:59:25.000Z",
    "title": "Visual Agentic Reinforcement Fine-Tuning",
    "summary": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14246.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14460",
      "authors": [
        {
          "_id": "682d54b0396c1e613eaac5ef",
          "user": {
            "_id": "655de51982afda0fc479fb91",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/99oxuFD3OjlReuGfDZluh.png",
            "isPro": false,
            "fullname": "Tianhe Wu",
            "user": "TianheWu",
            "type": "user"
          },
          "name": "Tianhe Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:26.204Z",
          "hidden": false
        },
        {
          "_id": "682d54b0396c1e613eaac5f0",
          "name": "Jian Zou",
          "hidden": false
        },
        {
          "_id": "682d54b0396c1e613eaac5f1",
          "name": "Jie Liang",
          "hidden": false
        },
        {
          "_id": "682d54b0396c1e613eaac5f2",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "682d54b0396c1e613eaac5f3",
          "name": "Kede Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T14:56:50.000Z",
      "submittedOnDailyAt": "2025-05-21T07:48:51.670Z",
      "title": "VisualQuality-R1: Aprendizaje de rehabilitación para evaluar la calidad de la imagen utilizando un rango secuencial de calificación.",
      "submittedOnDailyBy": {
        "_id": "655de51982afda0fc479fb91",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/99oxuFD3OjlReuGfDZluh.png",
        "isPro": false,
        "fullname": "Tianhe Wu",
        "user": "TianheWu",
        "type": "user"
      },
      "summary": "DeepSeek-R1 es efectivo en la mejora de la capacidad de lógica y generalización de grandes modelos de lenguaje (LLMs) a través de aprendizaje. Sin embargo, la posibilidad de modelización computacional basada en lógica no ha sido suficientemente investigada en tareas importantes que dependen de lógicas visuales, como la evaluación de calidad de imágenes (IQA). En este artículo, se presenta el modelo VisualQuality-R1, un NR-IQA (IQA no basado en referencia) basado en lógica, que se entrena utilizando aprendizaje por refuerzo y se optimiza mediante un algoritmo de entrenamiento que se ajusta a las características relativas de la calidad de las imágenes. Específicamente, se utiliza una política de optimización relativa de grupos de imágenes para generar múltiples puntuaciones de calidad para cada imagen. Estos valores estimados calculan la probabilidad, según el modelo de Thurstone, de que una imagen tenga una calidad superior a otra. La compensación para cada estimación de calidad se define no como retroalimentación continua, sino mediante etiquetas binarias. Los experimentos extendidos muestran que VisualQuality-R1 supera a los modelos de NR-IQA basados en aprendizaje profundo y a los métodos recientes de regresión de calidad basados en lógica. Además, VisualQuality-R1 está adaptado para medir de manera confiable el desarrollo de diferentes tareas de procesamiento de imágenes, sin necesidad de ajustar escalas visuales. Estas características lo hacen especialmente adecuado para la medición confiable del desarrollo en diferentes tareas de procesamiento de imágenes.",
      "upvotes": 14,
      "discussionId": "682d54b0396c1e613eaac62a",
      "githubRepo": "https://github.com/TianheWu/VisualQuality-R1",
      "ai_keywords": [
        "reinforcement learning",
        "VisualQuality-R1",
        "reasoning-induced no-reference IQA (NR-IQA)",
        "group relative policy optimization",
        "Thurstone model",
        "continuous fidelity measures",
        "discriminative deep learning",
        "reasoning-induced quality regression",
        "super-resolution",
        "image generation"
      ]
    },
    "publishedAt": "2025-05-20T10:56:50.000Z",
    "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\n  Reinforcement Learning to Rank",
    "summary": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\nreasoning and generalization capabilities of large language models (LLMs)\nthrough reinforcement learning. Nevertheless, the potential of\nreasoning-induced computational modeling has not been thoroughly explored in\nthe context of image quality assessment (IQA), a task critically dependent on\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\nreinforcement learning to rank, a learning algorithm tailored to the\nintrinsically relative nature of visual quality. Specifically, for a pair of\nimages, we employ group relative policy optimization to generate multiple\nquality scores for each image. These estimates are then used to compute\ncomparative probabilities of one image having higher quality than the other\nunder the Thurstone model. Rewards for each quality estimate are defined using\ncontinuous fidelity measures rather than discretized binary labels. Extensive\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\ndiscriminative deep learning-based NR-IQA models as well as a recent\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\ncapable of generating contextually rich, human-aligned quality descriptions,\nand supports multi-dataset training without requiring perceptual scale\nrealignment. These features make VisualQuality-R1 especially well-suited for\nreliably measuring progress in a wide range of image processing tasks like\nsuper-resolution and image generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14460.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "655de51982afda0fc479fb91",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/99oxuFD3OjlReuGfDZluh.png",
      "fullname": "Tianhe Wu",
      "name": "TianheWu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.04388",
      "authors": [
        {
          "_id": "682d83494c4685831f85ec92",
          "name": "Dario Garcia-Gasulla",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec93",
          "user": {
            "_id": "661e8559b0338c03bd6e5054",
            "avatarUrl": "/avatars/044d69afa40ddef4485aebfe984da96b.svg",
            "isPro": false,
            "fullname": "Bayarri",
            "user": "JordiBayarri-bsc",
            "type": "user"
          },
          "name": "Jordi Bayarri-Planas",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec94",
          "name": "Ashwin Kumar Gururajan",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec95",
          "name": "Enrique Lopez-Cuena",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec96",
          "user": {
            "_id": "6622352d9fcf61dee8a1f24d",
            "avatarUrl": "/avatars/dd164c50b3ecb8861e2294337b942e6f.svg",
            "isPro": false,
            "fullname": "Adrian Tormos",
            "user": "adriantormos",
            "type": "user"
          },
          "name": "Adrian Tormos",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec97",
          "user": {
            "_id": "65b8e66f078c543033e49672",
            "avatarUrl": "/avatars/aa81fd59b4ec624245a4a84d2708962d.svg",
            "isPro": false,
            "fullname": "Daniel Hinjos García",
            "user": "danihinjos",
            "type": "user"
          },
          "name": "Daniel Hinjos",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:39:40.380Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec98",
          "user": {
            "_id": "620683e7eeb1b73d904c96e5",
            "avatarUrl": "/avatars/d0309ac9408530a74f1799e175cc5fad.svg",
            "isPro": false,
            "fullname": "Pablo Bernabeu",
            "user": "pabberpe",
            "type": "user"
          },
          "name": "Pablo Bernabeu-Perez",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec99",
          "user": {
            "_id": "65d71603ca16ef9ba7fb2efb",
            "avatarUrl": "/avatars/7d3e1436427f7f58c86fb1f8724c4244.svg",
            "isPro": false,
            "fullname": "Anna",
            "user": "annariasdu",
            "type": "user"
          },
          "name": "Anna Arias-Duart",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T07:59:53.756Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9a",
          "user": {
            "_id": "6565c2a4131d13ccc5df1435",
            "avatarUrl": "/avatars/218ceac504772e7f0fb3ee4d46d4fab7.svg",
            "isPro": false,
            "fullname": "Pablo Agustin Martin Torres",
            "user": "PabloMartinTorres",
            "type": "user"
          },
          "name": "Pablo Agustin Martin-Torres",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T09:03:22.597Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9b",
          "name": "Marta Gonzalez-Mallo",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9c",
          "user": {
            "_id": "62d16c742ad06bbc89217797",
            "avatarUrl": "/avatars/11ce629fbcb33f2431164d8a3e54c876.svg",
            "isPro": false,
            "fullname": "Sergio Alvarez-Napagao",
            "user": "tranchis",
            "type": "user"
          },
          "name": "Sergio Alvarez-Napagao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9d",
          "name": "Eduard Ayguadé-Parra",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9e",
          "name": "Ulises Cortés",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/SdO6Jth8V1sz0wvfL9Nxg.png",
        "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/Efb0dFT2ULJC-TvdQ1ERR.png"
      ],
      "publishedAt": "2025-05-07T13:13:14.000Z",
      "submittedOnDailyAt": "2025-05-21T06:12:19.909Z",
      "title": "La receta de la familia Alora: Cuidado de salud abierto y profesional con un LLM",
      "submittedOnDailyBy": {
        "_id": "62f7a16192950415b637e201",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f7a16192950415b637e201/4IIqYap43vujvSuql68Vj.jpeg",
        "isPro": false,
        "fullname": "Dario",
        "user": "dariog",
        "type": "user"
      },
      "summary": "Objetivo: Con el desarrollo de los modelos de lenguaje híbrido (LLMs), es necesario modelos abiertos de fuente competitivos para proteger el interés público en el sector de la salud y cuidados médicos. Este estudio presenta métodos para optimizar las etapas de preprocesamiento de datos y entrenamiento, mejorar la seguridad del modelo (DPO) y su eficiencia (RAG), estableciendo nuevas normas en el sector. Finalmente, proporciona modelos competitivos con los mejores modelos no públicos, bajo la licencia de libertad y se lanza.\n\nMétodo: Basándose en modelos de base fuertes como Llama 3.1 y Qwen 2.5, Aloe Beta agrega ejemplos de razonamiento de cadena de pensamiento para fortalecer los datos públicos. El modelo se optimiza directamente por preferencia (DPO) y se enfatiza en la performance de coherencia ética y política, a pesar de las amenazas de ataques. La evaluación incluye métricas de rendimiento, desarrollo, seguridad y evaluación humana, maximizando la confianza en los resultados.\n\nResultados: Basándose en la excelente performance de la familia Aloe, Aloe Beta muestra un rendimiento competitivo con modelos no públicos, ofreciendo un rendimiento competitivo en marcos de evaluación de salud y en el sector médico, también apreciado por expertos médicos. En cuanto a la sesgo y la toxicidad, Aloe Beta mejora significativamente la seguridad y muestra una resistencia a ataques desconocidos. Para la liberación responsable, se proporciona una evaluación específica de riesgos para los modelos de la familia Aloe.\n\nConclusión: El modelo Aloe Beta y su receta de diseño, ofrecen una gran contribución al campo de los modelos de lenguaje abiertos de uso médico, manteniendo altos estándares éticos y ofreciendo altos niveles de rendimiento. Este estudio establece nuevas normas en el desarrollo y reporte de modelos de lenguaje de salud y cuidados médicos.",
      "upvotes": 14,
      "discussionId": "682d834a4c4685831f85ed09",
      "ai_keywords": [
        "Direct Preference Optimization (DPO)",
        "Retrieval-Augmented Generation (RAG)",
        "Chain of Thought",
        "Llama 3.1",
        "Qwen 2.5",
        "Close-ended tests",
        "Open-ended tests",
        "Safety assessments",
        "Human assessments",
        "Jailbreaking attacks",
        "Bias",
        "Toxicity",
        "Risk assessment"
      ]
    },
    "publishedAt": "2025-05-07T09:13:14.000Z",
    "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
    "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/SdO6Jth8V1sz0wvfL9Nxg.png",
      "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/Efb0dFT2ULJC-TvdQ1ERR.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04388.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f7a16192950415b637e201",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f7a16192950415b637e201/4IIqYap43vujvSuql68Vj.jpeg",
      "fullname": "Dario",
      "name": "dariog",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14513",
      "authors": [
        {
          "_id": "682d334862cadf615f5f73e6",
          "user": {
            "_id": "63ad217b9fc40b14560e9e06",
            "avatarUrl": "/avatars/c2d33830b141fc9c73ad8302ff35ed9d.svg",
            "isPro": false,
            "fullname": "Yen-Chen Wu",
            "user": "yenchen",
            "type": "user"
          },
          "name": "Yen-Chen Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:11:09.090Z",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73e7",
          "user": {
            "_id": "643fb7332397d8eef5b844cd",
            "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
            "isPro": false,
            "fullname": "Feng-Ting Liao",
            "user": "FengTing",
            "type": "user"
          },
          "name": "Feng-Ting Liao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:41:00.821Z",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73e8",
          "user": {
            "_id": "66018c8eb1e509e1e4d9196f",
            "avatarUrl": "/avatars/5d6cbfc7b6c435264d271c958607630f.svg",
            "isPro": false,
            "fullname": "Meng-Hsi Chen",
            "user": "menghsichen",
            "type": "user"
          },
          "name": "Meng-Hsi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:11:15.604Z",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73e9",
          "name": "Pei-Chen Ho",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73ea",
          "name": "Farhang Nabiei",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73eb",
          "user": {
            "_id": "6811b1294119e4ecc92fc93b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/srmY2yyzOg9KRDSLYXJKf.png",
            "isPro": false,
            "fullname": "Dashan Shiu",
            "user": "dsshiu",
            "type": "user"
          },
          "name": "Da-shan Shiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:11:37.639Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T15:41:05.000Z",
      "submittedOnDailyAt": "2025-05-21T00:30:57.355Z",
      "title": "Latent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer\n\nLatent Flow Transformer",
      "submittedOnDailyBy": {
        "_id": "643fb7332397d8eef5b844cd",
        "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
        "isPro": false,
        "fullname": "Feng-Ting Liao",
        "user": "FengTing",
        "type": "user"
      },
      "summary": "Transformers son la implementación estándar de modelos de lenguaje grandes (LLMs), generalmente constituidos por 10 a cientos de capas separadas. Aunque es cierto que un mayor número de capas puede mejorar el rendimiento, este enfoque no es eficiente. En particular, se discute cómo modelos de flujo probabilístico y basados en flujo muestran excelentes resultados con capas continuas. Proponemos el Latent Flow Transformer (LFT). El LFT reemplaza bloques de capas con un solo transformer entrenado mediante flujo de aprendizaje, manteniendo la arquitectura circular y al mismo tiempo logrando una gran compresión. Además, introducimos el algoritmo Flow Walking (FW) para resolver las limitaciones de la conservación del flujo en métodos basados en flujo. En el modelo Pythia-410M, el LFT entrenado con flujo de aprendizaje reduce 24 capas a 6, mejorando su rendimiento (KL Divergence: 0.407) en comparación con saltar directamente 2 capas (0.529). Esto demuestra la viabilidad de esta diseño. El LFT entrenado con FW reduce 12 capas a 1, restringiendo la KL Divergence a 0.736 y reduciendo significativamente la diferencia entre el paradigma de generación automático y el paradigma de generación basado en flujo, superando el salto de 3 capas (0.932).",
      "upvotes": 11,
      "discussionId": "682d334962cadf615f5f743f",
      "githubRepo": "https://github.com/mtkresearch/latent-flow-transformer",
      "ai_keywords": [
        "Transformers",
        "large language models (LLMs)",
        "discrete layers",
        "continuous layers",
        "diffusion models",
        "flow-based models",
        "latent Flow Transformer (LFT)",
        "learned transport operator",
        "flow matching",
        "Flow Walking (FW) algorithm",
        "Pythia-410M",
        "KL Divergence",
        "autoregressive",
        "flow-based generation paradigms"
      ]
    },
    "publishedAt": "2025-05-20T11:41:05.000Z",
    "title": "Latent Flow Transformer",
    "summary": "Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in preserving\ncoupling by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14513.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643fb7332397d8eef5b844cd",
      "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
      "fullname": "Feng-Ting Liao",
      "name": "FengTing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13438",
      "authors": [
        {
          "_id": "682d84d6aa4903837eeac1dc",
          "user": {
            "_id": "63885f1d0bebb233d8ad6e5b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
            "isPro": false,
            "fullname": "Penghui Qi",
            "user": "QPHutu",
            "type": "user"
          },
          "name": "Penghui Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:39:35.138Z",
          "hidden": false
        },
        {
          "_id": "682d84d6aa4903837eeac1dd",
          "user": {
            "_id": "65f5392c68b8e0cb3c9977a2",
            "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
            "isPro": false,
            "fullname": "Zichen",
            "user": "lkevinzc",
            "type": "user"
          },
          "name": "Zichen Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T09:31:12.541Z",
          "hidden": false
        },
        {
          "_id": "682d84d6aa4903837eeac1de",
          "user": {
            "_id": "63d91b6d255ef6add20e1b38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
            "isPro": false,
            "fullname": "Tianyu Pang",
            "user": "P2333",
            "type": "user"
          },
          "name": "Tianyu Pang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:12:08.028Z",
          "hidden": false
        },
        {
          "_id": "682d84d6aa4903837eeac1df",
          "user": {
            "_id": "632407c892e07e3ca20aca28",
            "avatarUrl": "/avatars/23b51b37b12b51a0947f687d1de4d3b5.svg",
            "isPro": false,
            "fullname": "Chao Du",
            "user": "duchao",
            "type": "user"
          },
          "name": "Chao Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:12:18.649Z",
          "hidden": false
        },
        {
          "_id": "682d84d6aa4903837eeac1e0",
          "name": "Wee Sun Lee",
          "hidden": false
        },
        {
          "_id": "682d84d6aa4903837eeac1e1",
          "name": "Min Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/UeoRcEi-bKgq6eecwvo8o.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/JLzRbfpomaF02gHLNkON6.png"
      ],
      "publishedAt": "2025-05-19T17:58:44.000Z",
      "submittedOnDailyAt": "2025-05-21T06:36:34.577Z",
      "title": "Optimización de la Política Relativa de Presupuesto para la Optimización de la Teoría de la Distribución de Tiempo",
      "submittedOnDailyBy": {
        "_id": "63885f1d0bebb233d8ad6e5b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
        "isPro": false,
        "fullname": "Penghui Qi",
        "user": "QPHutu",
        "type": "user"
      },
      "summary": "El cálculo de la cantidad de computación en el tiempo de prueba de escalado es crucial para mejorar la capacidad de inferencia de grandes modelos de lenguaje (LLMs). Los métodos actuales generalmente utilizan aprendizaje por refuerzo (RL) para maximizar el recompensa provable final. Sin embargo, estos métodos optimizan únicamente el rendimiento final en un gran buffer de tokens fijo, lo que daña la eficiencia en el entrenamiento y la implementación. En este estudio, se propone un nuevo marco de trabajo llamado AnytimeReasoner, cuyo objetivo es mejorar la eficiencia y flexibilidad del inferencia en tiempos variables bajo restricciones de buffer de tokens variable, optimizando el rendimiento de inferencia en tiempos arbitrarios. Para lograr esto, primero, se cortan los procesos completos de razonamiento desde un buffer de tokens muestreado de una distribución, y se forzan a modelos para resumir las respuestas más adecuadas para cada fragmento cortado. De esta manera, se introducen recompensas densas provables en el proceso de inferencia y se promueve una distribución de confianza efectiva para la optimización por RL. A continuación, se separan las políticas de pensamiento y resumen para maximizar la recompensa acumulada y optimizar. Además, se introduce un nuevo método de reducción de varianza, la Policy Optimization Relative a la Buffer (BRPO), para mejorar la robustez y eficiencia del proceso de entrenamiento de la política de pensamiento. Los resultados de experimentos en tareas de inferencia matemática muestran que nuestro método supera GRPO en todos los buffers de pensamiento de todas las distribuciones previas, mejorando la eficiencia en el entrenamiento y los tokens.",
      "upvotes": 11,
      "discussionId": "682d84d7aa4903837eeac215",
      "githubRepo": "https://github.com/sail-sg/AnytimeReasoner",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "verifiable reward",
        "reasoning traces",
        "token budget",
        "AnytimeReasoner",
        "truncation",
        "verifiable dense rewards",
        "credit assignment",
        "thinking and summary policies",
        "decoupled manner",
        "cumulative reward",
        "Budget Relative Policy Optimization (BRPO)",
        "variance reduction"
      ]
    },
    "publishedAt": "2025-05-19T13:58:44.000Z",
    "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
    "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/UeoRcEi-bKgq6eecwvo8o.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/JLzRbfpomaF02gHLNkON6.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13438.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63885f1d0bebb233d8ad6e5b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
      "fullname": "Penghui Qi",
      "name": "QPHutu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13547",
      "authors": [
        {
          "_id": "682d6b06ec3b65b35772c0af",
          "user": {
            "_id": "668f440894dfc0ed1a7006ed",
            "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
            "isPro": false,
            "fullname": "Pengxin Guo",
            "user": "gpx333",
            "type": "user"
          },
          "name": "Pengxin Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:11.271Z",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b0",
          "user": {
            "_id": "641b065a1911d3be6742ef04",
            "avatarUrl": "/avatars/bceb4ad2a278b6e2a40af0b89c4fb48e.svg",
            "isPro": false,
            "fullname": "Yinong Wang",
            "user": "jcccy",
            "type": "user"
          },
          "name": "Yinong Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:12:52.463Z",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b1",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b2",
          "user": {
            "_id": "678f8d4b1a5e393c1c285165",
            "avatarUrl": "/avatars/533eb74376915eb63c03dc9b4df421f6.svg",
            "isPro": false,
            "fullname": "MENGTINGLIU",
            "user": "MENGTINGLIU",
            "type": "user"
          },
          "name": "Mengting Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:13:13.691Z",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b3",
          "user": {
            "_id": "637f0eb22438d7485b8ef5d7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
            "isPro": false,
            "fullname": "Ming Li",
            "user": "limingcv",
            "type": "user"
          },
          "name": "Ming Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:13:40.959Z",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b4",
          "name": "Jinkai Zheng",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b5",
          "user": {
            "_id": "663058bc2653ec94f4a6235f",
            "avatarUrl": "/avatars/f55b8c3c8100d6b6d65ba61abc4fb014.svg",
            "isPro": false,
            "fullname": "Liangqiong Qu",
            "user": "Liangqiong-QU",
            "type": "user"
          },
          "name": "Liangqiong Qu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:12:59.427Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T03:41:54.000Z",
      "submittedOnDailyAt": "2025-05-21T04:27:08.535Z",
      "title": "Exploración de modelos de lenguaje grandes en la teoría de la base de pares",
      "submittedOnDailyBy": {
        "_id": "668f440894dfc0ed1a7006ed",
        "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
        "isPro": false,
        "fullname": "Pengxin Guo",
        "user": "gpx333",
        "type": "user"
      },
      "summary": "La producción de LLMs ha surgido como una tecnología esperada para facilitar la compresión de LLMs y promover su implementación en dispositivos con limitaciones de recursos. Sin embargo, los métodos actuales generalmente requieren acceso a muestras de carabales publicadas, lo cual puede ser desafiante en campos donde la privacidad es un tema de interés. Para abordar estos desafíos, proponemos FedPrLLM, un único marco de trabajo federado para la compresión privacidad-protegida de LLMs. En FedPrLLM, cada cliente calcula una matriz de mascara de producción basada en su datos locales de carabales y comparte esto con el servidor para la producción del modelo global. Este enfoque permite la producción cooperativa del modelo global basado en la conocida de cada cliente, mientras que mantiene la privacidad de los datos locales. Además, en el marco de FedPrLLM, evaluamos diversas posibilidades evaluando diferentes grupos de comparación, estrategias de producción y decisiones de escalamiento de pesos. Una evaluación rigurosa muestra que la comparación de jugadores y la producción temporal sin escalamiento de pesos son las mejores opciones dentro del marco de FedPrLLM. Nuestro trabajo guia futuros esfuerzos en la producción de LLMs en áreas de interés por la privacidad. El código está disponible en https://github.com/Pengxin-Guo/FedPrLLM.",
      "upvotes": 10,
      "discussionId": "682d6b07ec3b65b35772c0f3",
      "githubRepo": "https://github.com/Pengxin-Guo/FedPrLLM",
      "ai_keywords": [
        "FedPrLLM",
        "federated pruning",
        "pruning mask matrix",
        "local calibration data",
        "global model",
        "collaborative pruning",
        "layer comparison",
        "weight scaling",
        "one-shot pruning"
      ]
    },
    "publishedAt": "2025-05-18T23:41:54.000Z",
    "title": "Exploring Federated Pruning for Large Language Models",
    "summary": "LLM pruning has emerged as a promising technology for compressing LLMs,\nenabling their deployment on resource-limited devices. However, current\nmethodologies typically require access to public calibration samples, which can\nbe challenging to obtain in privacy-sensitive domains. To address this issue,\nwe introduce FedPrLLM, a comprehensive federated pruning framework designed for\nthe privacy-preserving compression of LLMs. In FedPrLLM, each client only needs\nto calculate a pruning mask matrix based on its local calibration data and\nshare it with the server to prune the global model. This approach allows for\ncollaborative pruning of the global model with the knowledge of each client\nwhile maintaining local data privacy. Additionally, we conduct extensive\nexperiments to explore various possibilities within the FedPrLLM framework,\nincluding different comparison groups, pruning strategies, and the decision to\nscale weights. Our extensive evaluation reveals that one-shot pruning with\nlayer comparison and no weight scaling is the optimal choice within the\nFedPrLLM framework. We hope our work will help guide future efforts in pruning\nLLMs in privacy-sensitive fields. Our code is available at\nhttps://github.com/Pengxin-Guo/FedPrLLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13547.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668f440894dfc0ed1a7006ed",
      "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
      "fullname": "Pengxin Guo",
      "name": "gpx333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13866",
      "authors": [
        {
          "_id": "682d2dee396c1e613e9fcbe5",
          "user": {
            "_id": "662672eaebdfec5cfdf1d034",
            "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
            "isPro": false,
            "fullname": "Jiwon Song",
            "user": "jiwonsong",
            "type": "user"
          },
          "name": "Jiwon Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:41:15.525Z",
          "hidden": false
        },
        {
          "_id": "682d2dee396c1e613e9fcbe6",
          "user": {
            "_id": "639ffbc6beb95d698de9640d",
            "avatarUrl": "/avatars/7ef1aaadd5b378d00e17dc548e42cb7e.svg",
            "isPro": false,
            "fullname": "Dongwon Jo",
            "user": "dongwonjo",
            "type": "user"
          },
          "name": "Dongwon Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:41:12.000Z",
          "hidden": false
        },
        {
          "_id": "682d2dee396c1e613e9fcbe7",
          "user": {
            "_id": "6566ddb96af53c602f80b1e2",
            "avatarUrl": "/avatars/403c8e486115920e50867b6462ddfd99.svg",
            "isPro": false,
            "fullname": "Yulhwa Kim",
            "user": "YulhwaKim",
            "type": "user"
          },
          "name": "Yulhwa Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:14:05.272Z",
          "hidden": false
        },
        {
          "_id": "682d2dee396c1e613e9fcbe8",
          "name": "Jae-Joon Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T03:21:52.000Z",
      "submittedOnDailyAt": "2025-05-21T00:06:25.696Z",
      "title": "Reasoning Path Compression: Compresión del Proceso de Generación de Reasoning Eficiente de un LLM",
      "submittedOnDailyBy": {
        "_id": "662672eaebdfec5cfdf1d034",
        "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
        "isPro": false,
        "fullname": "Jiwon Song",
        "user": "jiwonsong",
        "type": "user"
      },
      "summary": "Los modelos de lanzamiento basados en razones recientes generan una larga ruta de razones intermedias antes de producir la respuesta final, lo que permite alcanzar una alta precisión. Este enfoque es efectivo para resolver problemas que requieren pensamiento lógico, pero la generación de largas rutas de razones aumenta significativamente el uso de memoria y las transacciones de tokens, lo que limita su aplicación práctica. Proponemos un método llamado \"Compresión de Rutas de Razones (RPC)\" que ayuda a acelerar la inferencia sin limitar el entrenamiento, utilizando la semántica de la raíz de razones. El RPC utiliza puntuaciones de importancia calculadas en ventanas de selección que forman la pregunta reciente para mantener en caché los KV con altas puntuaciones, lo que permite la compresión periódica del caché de KV. Los experimentos mostraron que el RPC aumentó las transacciones de generación de QwQ-32B en un 1.60 veces respecto a la inferencia con el caché de KV completo, y la pérdida de precisión en el benchmark AIME 2024 fue de 1.2%. Nuestro hallazgo demuestra la efectividad de utilizar la semántica de las rutas de razones. Esto proporciona una ruta efectiva para la aplicación eficiente de modelos de razones, y nuestro código está disponible en https://github.com/jiwonsong-dev/ReasoningPathCompression.",
      "upvotes": 9,
      "discussionId": "682d2def396c1e613e9fcc0b",
      "githubRepo": "https://github.com/jiwonsong-dev/ReasoningPathCompression",
      "ai_keywords": [
        "Reasoning Path Compression (RPC)",
        "KV cache",
        "semantic sparsity",
        "reasoning traces",
        "QwQ-32B",
        "AIME 2024 benchmark"
      ]
    },
    "publishedAt": "2025-05-19T23:21:52.000Z",
    "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
    "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60times compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13866.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662672eaebdfec5cfdf1d034",
      "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
      "fullname": "Jiwon Song",
      "name": "jiwonsong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14677",
      "authors": [
        {
          "_id": "682d718306291bf11fcf69a3",
          "user": {
            "_id": "664da76e4eb4c91c8c32cc06",
            "avatarUrl": "/avatars/050bb6e4136f8a1645fef277ad08c7fc.svg",
            "isPro": false,
            "fullname": "Jiaer Xia",
            "user": "Jiaer-Xia",
            "type": "user"
          },
          "name": "Jiaer Xia",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:14:20.493Z",
          "hidden": false
        },
        {
          "_id": "682d718306291bf11fcf69a4",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:39:56.735Z",
          "hidden": false
        },
        {
          "_id": "682d718306291bf11fcf69a5",
          "name": "Peng Gao",
          "hidden": false
        },
        {
          "_id": "682d718306291bf11fcf69a6",
          "name": "Yixuan Li",
          "hidden": false
        },
        {
          "_id": "682d718306291bf11fcf69a7",
          "user": {
            "_id": "62ac6656de8bfbb93094b8fd",
            "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
            "isPro": false,
            "fullname": "Kaiyang Zhou",
            "user": "kaiyangzhou",
            "type": "user"
          },
          "name": "Kaiyang Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T10:14:28.433Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:58:35.000Z",
      "submittedOnDailyAt": "2025-05-21T04:54:13.466Z",
      "title": "Visión Alejada-R1: Aprendizaje por Refuerzo para Reducción de Intervalos Cortos de Tiempo en la Inferencia Visual",
      "submittedOnDailyBy": {
        "_id": "62ac6656de8bfbb93094b8fd",
        "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
        "isPro": false,
        "fullname": "Kaiyang Zhou",
        "user": "kaiyangzhou",
        "type": "user"
      },
      "summary": "El motivo general de la aprendizaje es una habilidad que ha representado un problema duradero para la IA durante mucho tiempo. Según la investigación reciente en modelos de lenguaje de gran tamaño (LLMs), como DeepSeek-R1, métodos de aprendizaje por refuerzo como GRPO han demostrado que LLMs pueden desarrollar habilidades de razonamiento utilizando pares de preguntas y respuestas simples. En este artículo, se propone asignar razonamiento a modelos de lenguaje visual (VLMs) utilizando aprendizaje por refuerzo y pares de preguntas visuales y respuestas, sin incluir la supervisión explícita de la secuencia de razones (CoT). Nuestros hallazgos muestran que aplicando aprendizaje por refuerzo a un VLM solo con problemas sencillos puede promover el desarrollo de un \"sloth\" y mejorar la capacidad de generalización en la distribución de datos previamente vista. La clave para restringir el aprendizaje corto es fomentar la interpretación de las imágenes antes de la razón. Por lo tanto, se generan capturas detalladas de imágenes que el modelo nunca ha visto y se entrenan para construir secuencias de razones. Al entrenar con 273K pares de preguntas visuales y respuestas sin CoT, el modelo llamado Visionary-R1 supera a modelos potentes como GPT-4o, Claude3.5-Sonnet y Gemini-1.5-Pro en múltiples benchmarks de razonamiento visual y destaca en varios marcadores de rendimiento.",
      "upvotes": 8,
      "discussionId": "682d718406291bf11fcf69df",
      "githubRepo": "https://github.com/maifoundations/Visionary-R1",
      "ai_keywords": [
        "reinforcement learning",
        "deep learning",
        "large language models (LLMs)",
        "DeepSeek-R1",
        "GRPO",
        "visual language models (VLMs)",
        "visual question-answer pairs",
        "chain-of-thought (CoT)",
        "caption-reason-answer",
        "multimodal models",
        "GPT-4o",
        "Claude3.5-Sonnet",
        "Gemini-1.5-Pro",
        "visual reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-05-20T13:58:35.000Z",
    "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning",
    "summary": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14677.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ac6656de8bfbb93094b8fd",
      "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
      "fullname": "Kaiyang Zhou",
      "name": "kaiyangzhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14652",
      "authors": [
        {
          "_id": "682d474782567fffe1f99b7a",
          "user": {
            "_id": "5ec82854968f6028e0559f70",
            "avatarUrl": "/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg",
            "isPro": true,
            "fullname": "Xueguang Ma",
            "user": "MrLight",
            "type": "user"
          },
          "name": "Xueguang Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:41.675Z",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7b",
          "user": {
            "_id": "612ee6a7b960e78c6d2319d4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "isPro": false,
            "fullname": "Qian Liu",
            "user": "SivilTaram",
            "type": "user"
          },
          "name": "Qian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:39.513Z",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7c",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7d",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7e",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7f",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T03:23:52.529Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:41:33.000Z",
      "submittedOnDailyAt": "2025-05-21T01:56:42.807Z",
      "title": "General Robot: El desarrollo de la inferencia de LLM en todos los directorios",
      "submittedOnDailyBy": {
        "_id": "5ec82854968f6028e0559f70",
        "avatarUrl": "/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg",
        "isPro": true,
        "fullname": "Xueguang Ma",
        "user": "MrLight",
        "type": "user"
      },
      "summary": "Reinforcement Learning (RL) ha sido notado recientemente como una posibilidad poderosa para mejorar el rendimiento de los modelos de lenguaje de gran escala (LLMs). En particular, el \"Zero\" de Deepseek-R1-Zero permite entrenar directamente los LLMs de base con RL, y permite controlar los subobjetivos intermedios, lo que es crucial para la finalidad final.",
      "upvotes": 8,
      "discussionId": "682d474882567fffe1f99bc5",
      "projectPage": "https://tiger-ai-lab.github.io/General-Reasoner/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/General-Reasoner",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "large language models (LLMs)",
        "Zero reinforcement learning",
        "base LLMs",
        "supervised fine-tuning",
        "generative model-based answer verifier",
        "chain-of-thought",
        "context-awareness"
      ]
    },
    "publishedAt": "2025-05-20T13:41:33.000Z",
    "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
    "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14652.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5ec82854968f6028e0559f70",
      "avatarUrl": "/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg",
      "fullname": "Xueguang Ma",
      "name": "MrLight",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14489",
      "authors": [
        {
          "_id": "682d55ecea67e90811b09b6b",
          "user": {
            "_id": "617f679fb15f8a665f3999fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617f679fb15f8a665f3999fc/NW1vkLsGAlWpAQYTux05X.jpeg",
            "isPro": false,
            "fullname": "Dongkeun Yoon",
            "user": "DKYoon",
            "type": "user"
          },
          "name": "Dongkeun Yoon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:23.897Z",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b6c",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b6d",
          "name": "Sohee Yang",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b6e",
          "name": "Sunkyoung Kim",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b6f",
          "name": "Soyeon Kim",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b70",
          "name": "Yongil Kim",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b71",
          "name": "Eunbi Choi",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b72",
          "user": {
            "_id": "660260cf1737e5cd4a826550",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660260cf1737e5cd4a826550/AlSfoM2WtqPjLtYR6x7Wf.jpeg",
            "isPro": false,
            "fullname": "Yireun Kim",
            "user": "yireun",
            "type": "user"
          },
          "name": "Yireun Kim",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T04:27:50.114Z",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b73",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/uZZgyWD0lw8jektQyDZfV.png",
        "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/_Kq3BF1HC_aFc6mee08zl.png",
        "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/bpz_J8XujNicCvMH2n2iK.png"
      ],
      "publishedAt": "2025-05-20T15:19:00.000Z",
      "submittedOnDailyAt": "2025-05-21T02:58:45.807Z",
      "title": "Los modelos de razonamiento mejoran su expresión de confianza.",
      "submittedOnDailyBy": {
        "_id": "617f679fb15f8a665f3999fc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617f679fb15f8a665f3999fc/NW1vkLsGAlWpAQYTux05X.jpeg",
        "isPro": false,
        "fullname": "Dongkeun Yoon",
        "user": "DKYoon",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) tienen su punto fuerte, pero añaden dificultades para transmitir la confianza con precisión y para evaluar sus errores, lo que limita su confianza. En este estudio, se muestra que los modelos de lógica (LLMs) que ejecutan lógica continua (CoT) demostraron un excelente rendimiento en la resolución de problemas y también indican que pueden transmitir confianza con precisión. En particular, se evaluaron 6 modelos de lógica con 6 conjuntos de datos, y en 33 de 36 configuraciones, se logró una mejor ajuste de confianza que los modelos no de lógica, demostrando una mejor confianza. El análisis detallado muestra que este mejoramiento se debe al pensamiento gradual que se da en la lógica continua. Por ejemplo, el exploración estratégica o el retroceso, que permiten ajustar la confianza dinámicamente y mejorando gradualmente su precisión. En particular, los modelos de lógica señalan que el ajuste se mejora gradualmente a medida que avanza la lógica continua, una tendencia que no se observa en los modelos no de lógica. Además, eliminar el pensamiento gradual en la lógica continua conduce a un descenso en el ajuste. Finalmente, estos mejoramientos no solo se aplican a los modelos de lógica, sino que también a los modelos no de lógica, que mejoran su rendimiento al realizar pensamiento gradual a través del aprendizaje continuo.",
      "upvotes": 8,
      "discussionId": "682d55edea67e90811b09ba1",
      "githubRepo": "https://github.com/MattYoon/reasoning-models-confidence",
      "ai_keywords": [
        "large language models (LLMs)",
        "chain-of-thought (CoT) reasoning",
        "confidence calibration",
        "non-reasoning counterparts",
        "slow thinking behaviors",
        "backtracking",
        "in-context learning"
      ]
    },
    "publishedAt": "2025-05-20T11:19:00.000Z",
    "title": "Reasoning Models Better Express Their Confidence",
    "summary": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/uZZgyWD0lw8jektQyDZfV.png",
      "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/_Kq3BF1HC_aFc6mee08zl.png",
      "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/bpz_J8XujNicCvMH2n2iK.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14489.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "617f679fb15f8a665f3999fc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617f679fb15f8a665f3999fc/NW1vkLsGAlWpAQYTux05X.jpeg",
      "fullname": "Dongkeun Yoon",
      "name": "DKYoon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13559",
      "authors": [
        {
          "_id": "682d4ccc3b5f51f4218e12b4",
          "name": "Sathya Krishnan Suresh",
          "hidden": false
        },
        {
          "_id": "682d4ccc3b5f51f4218e12b5",
          "name": "Tanmay Surana",
          "hidden": false
        },
        {
          "_id": "682d4ccc3b5f51f4218e12b6",
          "name": "Lim Zhi Hao",
          "hidden": false
        },
        {
          "_id": "682d4ccc3b5f51f4218e12b7",
          "name": "Eng Siong Chng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T09:18:14.000Z",
      "submittedOnDailyAt": "2025-05-21T02:18:05.886Z",
      "title": "CS-Sum: Benchmark de la Dialoque Switch y Límites de los Modelos de Lenguaje de Gran Escala",
      "submittedOnDailyBy": {
        "_id": "62f0e457bc8201db9ef47f89",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f0e457bc8201db9ef47f89/zOhDptwZpDGaugKCBOWB2.jpeg",
        "isPro": false,
        "fullname": "Sathya Krishnan",
        "user": "SkAndMl",
        "type": "user"
      },
      "summary": "El código de cambio (CS) es un problema importante en los modelos de lenguaje grandes (LLMs), pero su comprensión en LLMs aún no ha sido exhaustivamente investigada. Presentamos CS-Sum, un marco de referencia para evaluar la comprensión de CS mediante resumenes en inglés realizados por LLMs. CS-Sum es el primer marco de referencia que evalúa resumenes de diálogos CS en tres pares de idiomas: inglés-chino (EN-ZH), inglés-tamil (EN-TA) y inglés-malay (EN-MS). Cada par de idiomas incluye entre 900 y 1,300 diálogos humanos.\n\nEvaluamos 10 LLMs (incluyendo modelos abiertos y cerrados), y analizamos los resultados utilizando tres métodos: entrenamiento de pocas instancias, resumenes de traducción y datos agregados con LoRA y QLoRA. Nuestros hallazgos muestran que, aunque los puntajes de los indicadores de evaluación automática sean altos, los LLMs pueden detectar errores sublevados que cambian la significación general del diálogo. Por lo tanto, presentamos los tres tipos de errores más comunes que los LLMs presentan al procesar entradas de CS. El porcentaje de errores varía entre pares de CS y entre diferentes LLMs, lo que implica que algunos LLMs pueden cometer más errores en ciertos pares de idiomas, lo que subraya la necesidad de entrenamiento especializado en datos de CS.",
      "upvotes": 8,
      "discussionId": "682d4ccd3b5f51f4218e12f4",
      "ai_keywords": [
        "code-switching (CS)",
        "Large Language Models (LLMs)",
        "CS-Sum",
        "CS dialogue to English summarization",
        "Mandarin-English (EN-ZH)",
        "Tamil-English (EN-TA)",
        "Malay-English (EN-MS)",
        "few-shot",
        "translate-summarize",
        "fine-tuning",
        "LoRA",
        "QLoRA",
        "synthetic data",
        "CS input"
      ]
    },
    "publishedAt": "2025-05-19T05:18:14.000Z",
    "title": "CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the\n  Limits of Large Language Models",
    "summary": "Code-switching (CS) poses a significant challenge for Large Language Models\n(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce\nCS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue\nto English summarization. CS-Sum is the first benchmark for CS dialogue\nsummarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and\nMalay-English (EN-MS), with 900-1300 human-annotated dialogues per language\npair. Evaluating ten LLMs, including open and closed-source models, we analyze\nperformance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA\non synthetic data) approaches. Our findings show that though the scores on\nautomated metrics are high, LLMs make subtle mistakes that alter the complete\nmeaning of the dialogue. To this end, we introduce 3 most common type of errors\nthat LLMs make when handling CS input. Error rates vary across CS pairs and\nLLMs, with some LLMs showing more frequent errors on certain language pairs,\nunderscoring the need for specialized training on code-switched data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13559.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62f0e457bc8201db9ef47f89",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f0e457bc8201db9ef47f89/zOhDptwZpDGaugKCBOWB2.jpeg",
      "fullname": "Sathya Krishnan",
      "name": "SkAndMl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14680",
      "authors": [
        {
          "_id": "682d30a37812103582f50de4",
          "name": "Sunhao Dai",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de5",
          "name": "Wenjie Wang",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de6",
          "name": "Liang Pang",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de7",
          "name": "Jun Xu",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de8",
          "name": "See-Kiong Ng",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de9",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50dea",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:59:13.000Z",
      "submittedOnDailyAt": "2025-05-21T00:18:37.037Z",
      "title": "NExT-Search: Sistema de retroalimentación de usuario para la búsqueda de IA generativa",
      "submittedOnDailyBy": {
        "_id": "64db88993725f8d9a908c077",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
        "isPro": false,
        "fullname": "Sunhao Dai",
        "user": "KID-22",
        "type": "user"
      },
      "summary": "La búsqueda de IA generativa proporciona respuestas finales a palabras clave complejas, reduciendo las búsquedas directas del usuario y el proceso de selección de términos de búsqueda, así como refrescando la búsqueda de información. Sin embargo, este paradigma impide el desarrollo de una búsqueda web clara, que ha evolucionado históricamente a través de grandes cantidades de retroalimentación de usuarios en el nivel de texto. La búsqueda web puede recopilar retroalimentación micro y a nivel de documento (como clicks y tiempos de residencia) para continuamente mejorar los modelos de clasificación. En contraste, la búsqueda de IA generativa opera a través de largas cadenas de búsqueda (como la separación de términos de búsqueda, la búsqueda de documentos y la generación de respuestas), recibiendo retroalimentación de piezas para las respuestas finales. Esto dificulta la efectiva asignación de retroalimentación al sistema, ya que el retroalimentación final no se mapea adecuadamente a los componentes específicos de los pasos intermedios, lo que complica la mejora de cada etapa y el mantenimiento del ciclo de retroalimentación. En este artículo, se imagina el paradigma NExT-Search, el siguiente generador de respuestas. Este es un paradigma que reintroduce retroalimentación micro y proceso-nivel en la búsqueda de IA generativa. NExT-Search integra dos modos de complementación: el Modo de Depuración del Usuario y el Modo de Usuario Sombra. El Modo de Depuración del Usuario permite a los usuarios interesados acceder a etapas importantes. El Modo de Usuario Sombra simula las preferencias de un usuario individual para brindar retroalimentación útil a los usuarios con interacciones bajas. Además, se examina cómo se pueden utilizar estos señales de retroalimentación en adaptación en línea y actualizaciones en línea. Esto espera que ajuste a tiempo la salida actual de la búsqueda y recopilar registros de intersección para ajustar regularmente modelos de separación de términos, búsqueda y generación. Recuperando el control humano en las etapas importantes de la cadena de la IA generativa, NExT-Search ofrece una dirección hacia la construcción de un rico sistema de búsqueda de IA que puede desarrollarse junto con retroalimentación humana.",
      "upvotes": 7,
      "discussionId": "682d30a47812103582f50e19",
      "ai_keywords": [
        "generative AI search",
        "information retrieval",
        "end-to-end answers",
        "complex queries",
        "manually browsing",
        "summarizing",
        "traditional Web search",
        "ranking models",
        "fine-grained user feedback",
        "clicks",
        "dwell time",
        "document level",
        "query decomposition",
        "document retrieval",
        "answer generation",
        "coarse-grained feedback",
        "feedback loop disconnect",
        "system components",
        "NExT-Search",
        "User Debug Mode",
        "Shadow User Mode",
        "personalized user agent",
        "AI-assisted feedback",
        "online adaptation",
        "offline update",
        "real-time refinement",
        "interaction logs",
        "fine-tune query decomposition",
        "fine-tune retrieval",
        "fine-tune generation models",
        "feedback-rich AI search systems"
      ]
    },
    "publishedAt": "2025-05-20T13:59:13.000Z",
    "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search",
    "summary": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14680.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64db88993725f8d9a908c077",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
      "fullname": "Sunhao Dai",
      "name": "KID-22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14673",
      "authors": [
        {
          "_id": "682d713e6c66e25ab59fc800",
          "user": {
            "_id": "65c38108425a226a29f00365",
            "avatarUrl": "/avatars/c276a0d2b108df446246c31a396496f0.svg",
            "isPro": false,
            "fullname": "tongyu",
            "user": "yutchina02",
            "type": "user"
          },
          "name": "Yu Tong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:01.048Z",
          "hidden": false
        },
        {
          "_id": "682d713e6c66e25ab59fc801",
          "user": {
            "_id": "65ad57da57f263e3d030187a",
            "avatarUrl": "/avatars/e1e3c4119180aee5fb660d1e5f745e99.svg",
            "isPro": false,
            "fullname": "潘子豪",
            "user": "Apostle723",
            "type": "user"
          },
          "name": "Zihao Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:39:58.888Z",
          "hidden": false
        },
        {
          "_id": "682d713e6c66e25ab59fc802",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "682d713e6c66e25ab59fc803",
          "name": "Kaiyang Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:58:02.000Z",
      "submittedOnDailyAt": "2025-05-21T04:53:16.661Z",
      "title": "Tecnología de generación de imágenes con marca de autoría para aplicaciones de generación automática de palabras en deep learning",
      "submittedOnDailyBy": {
        "_id": "62ac6656de8bfbb93094b8fd",
        "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
        "isPro": false,
        "fullname": "Kaiyang Zhou",
        "user": "kaiyangzhou",
        "type": "user"
      },
      "summary": "La protección de derechos de autor a través de marcas invisibles en imágenes permite proteger el propiedad de las imágenes y prevenir el uso malicioso de modelos generativos visuales. Sin embargo, los métodos de marcación generativa existentes se han diseñado principalmente para modelos de difusión, y el marcación generativa para modelos de generación automática de imagenes ha sido poco explorada. Proponemos el IndexMark, un marco de marcación generativa sin entrenamiento para modelos de generación automática de imagenes. El IndexMark se basa en la propiedad de repetición de un diccionario, diseñado para crear diferencias visuales mínimas al sustituir índices similares. El componente clave del IndexMark es un método sencillo pero efectivo de reemplazo después del match. Este método selecciona cuidadosamente un token de marcación en el diccionario basándose en la similitud de tokens, insertando la marcación de manera que no afecte la calidad de la imagen mediante el reemplazo de tokens. La verificación de la marcación se realiza calculando la proporción de tokens de marcación en la imagen generada, y se puede mejorar la precisión mediante un encoder de índices. Además, se introdujo un plan de verificación auxiliar para aumentar la robustez frente a ataques binarios. Los experimentos demostraron un rendimiento superior en la calidad de la imagen y la precisión de la verificación, mostrando también una robustez frente a diversas perturbaciones como destrucción, ruido, blur gaussiano, erasing aleatorio, vibración de color y compresión JPEG.",
      "upvotes": 7,
      "discussionId": "682d71426c66e25ab59fc946",
      "githubRepo": "https://github.com/maifoundations/IndexMark",
      "ai_keywords": [
        "autoregressive image generation models",
        "codebook",
        "indices",
        "token similarity",
        "watermark tokens",
        "token replacement",
        "IndexMark",
        "match-then-replace method",
        "Index Encoder",
        "auxiliary validation scheme",
        "cropping attacks",
        "Gaussian blur",
        "random erasing",
        "color jittering",
        "JPEG compression"
      ]
    },
    "publishedAt": "2025-05-20T13:58:02.000Z",
    "title": "Training-Free Watermarking for Autoregressive Image Generation",
    "summary": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14673.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ac6656de8bfbb93094b8fd",
      "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
      "fullname": "Kaiyang Zhou",
      "name": "kaiyangzhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14640",
      "authors": [
        {
          "_id": "682d488557686b8c44f257fa",
          "user": {
            "_id": "65c387c807a1445dfe1e9452",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c387c807a1445dfe1e9452/t0VnwQh2wRZ9W_UGTZ8zt.jpeg",
            "isPro": false,
            "fullname": "Wentao Ma",
            "user": "tonymwt",
            "type": "user"
          },
          "name": "Wentao Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:31.165Z",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257fb",
          "user": {
            "_id": "64405a9d518271b0d1beea38",
            "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg",
            "isPro": false,
            "fullname": "Weiming Ren",
            "user": "wren93",
            "type": "user"
          },
          "name": "Weiming Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:28.689Z",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257fc",
          "name": "Yiming Jia",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257fd",
          "user": {
            "_id": "66349404f2c753240d02952a",
            "avatarUrl": "/avatars/4f207cf5807d9629b9f4f7d13875b840.svg",
            "isPro": false,
            "fullname": "ZhuofengLi",
            "user": "ZhuofengLi",
            "type": "user"
          },
          "name": "Zhuofeng Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:35.151Z",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257fe",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257ff",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f25800",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:26:32.000Z",
      "submittedOnDailyAt": "2025-05-21T02:03:04.298Z",
      "title": "VideoEval-Pro: Evaluación de Comprensión y Realidad en Video Largos Robustes y Prácticos",
      "submittedOnDailyBy": {
        "_id": "64405a9d518271b0d1beea38",
        "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg",
        "isPro": false,
        "fullname": "Weiming Ren",
        "user": "wren93",
        "type": "user"
      },
      "summary": "Los modelos de grandes dimensiones multimodal (LMMs) han aparecido recientemente como una herramienta poderosa en el campo de la comprensión de largos videos (LVU), impulsando el desarrollo de marcos de referencia LVU estándarizados. Sin embargo, nuestra investigación presenta un mensaje más crítico sobre los actuales marcos de referencia LVU. Primero, los actuales marcos de referencia priorizan problemas de múltiples respuestas (MCQs) debido a la posibilidad de acertar la respuesta correcta, lo que facilita aumentar los resultados de evaluación. Además, algunos problemas de estos marcos de referencia pueden ser resueltos con una amplia base de conocimientos previos sin necesidad de leer el contenido del video. Por ejemplo, Gemini-1.5-Pro puede alcanzar un rendimiento del 50% o más en Video-MME al recibir frames aleatorios de largos videos. Además, no hemos encontrado que aumentando la cantidad de frames mejore significativamente el rendimiento en los actuales marcos de referencia, lo que es contraintuitivo. Esto refleja que la justificación y robustez de los actuales marcos de referencia LVU son insuficientes, y que la evaluación verdadera de la capacidad de comprensión de largos videos por parte de los LMMs se ven afectadas. Para resolver estos problemas, proponemos VideoEval-Pro, un marco de referencia práctico para LVU. Este marco de referencia incluye respuestas abiertas que requieren comprender todo el video. VideoEval-Pro evalúa la estructura visual y lógica, tanto a nivel de segmentos como en su totalidad. Evaluamos 21 modelos de video multimodal (LMMs) públicos y abiertos, obteniendo los siguientes resultados: 1) los modelos de video presentan una pérdida significativa en rendimiento (>25%) en comparación con los MCQs en problemas abiertos. 2) Sorprendentemente, aunque los puntajes en MCQs son altos, los puntajes abiertos en VideoEval-Pro no se mejoran significativamente. 3) En comparación con otros marcos de referencia MCQ, VideoEval-Pro proporciona mayor beneficio al aumentar la cantidad de frames de entrada. Nuestros resultados demuestran que VideoEval-Pro proporciona una evaluación más práctica y confiable para la comprensión de largos videos, y que contribuye claramente al desarrollo de la área.",
      "upvotes": 6,
      "discussionId": "682d488857686b8c44f258b7",
      "projectPage": "https://tiger-ai-lab.github.io/VideoEval-Pro",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VideoEval-Pro",
      "ai_keywords": [
        "multimodal models",
        "long video understanding",
        "benchmarks",
        "multiple-choice questions",
        "Video-MME",
        "random frame",
        "VideoEval-Pro",
        "open-ended questions",
        "segment-level understanding",
        "full-video understanding",
        "perception tasks",
        "reasoning tasks",
        "performance drops"
      ]
    },
    "publishedAt": "2025-05-20T13:26:32.000Z",
    "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation",
    "summary": "Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance (>25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14640.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64405a9d518271b0d1beea38",
      "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg",
      "fullname": "Weiming Ren",
      "name": "wren93",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13430",
      "authors": [
        {
          "_id": "682d70c598e3ea9be315ed85",
          "name": "Sifeng Shang",
          "hidden": false
        },
        {
          "_id": "682d70c598e3ea9be315ed86",
          "name": "Jiayi Zhou",
          "hidden": false
        },
        {
          "_id": "682d70c598e3ea9be315ed87",
          "name": "Chenyu Lin",
          "hidden": false
        },
        {
          "_id": "682d70c598e3ea9be315ed88",
          "name": "Minxian Li",
          "hidden": false
        },
        {
          "_id": "682d70c598e3ea9be315ed89",
          "name": "Kaiyang Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T17:55:15.000Z",
      "submittedOnDailyAt": "2025-05-21T04:51:33.821Z",
      "title": "Optimización de Cero Dimensiones para el Ajuste Microscopico de Red Neuronales Cuantizadas",
      "submittedOnDailyBy": {
        "_id": "62ac6656de8bfbb93094b8fd",
        "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
        "isPro": false,
        "fullname": "Kaiyang Zhou",
        "user": "kaiyangzhou",
        "type": "user"
      },
      "summary": "La memoria de GPU se convierte en un límite para aplicar modelos de aprendizaje profundo que crecen exponencialmente en el tamaño, especialmente en tareas de streaming. Este artículo busca minimizar la memoria utilizada por los pesos, gradientes y estado de optimización de un modelo y promueve el entrenamiento eficiente dentro de los límites de la memoria. Nuestra idea es eliminar los gradientes y el estado de optimización utilizando la optimización de primer orden (zeroth-order optimization). Esto consiste en aproximar los gradientes mediante la propagación de pesos, que se reducen en el proceso de pooling para determinar la dirección del gradiente. Para minimizar la memoria utilizada por los pesos, se aplica la cuantificación del modelo, como convertir bfloat16 a int4. Sin embargo, aplicar directamente la optimización de primer orden a pesos cuantificados es impracticable debido a la discrepancia entre la precisión de los pesos discretos y los gradientes continuos. Para superar este desafío, proponemos una nueva metodología llamada cuantificación de primer orden (QZO), que estima los gradientes mediante la pooling de escalas cuantificadas continuas y establece la estabilidad del entrenamiento utilizando técnicas de clipping de la función de dirección. QZO se posiciona perpendicularmente a los métodos de cuantificación posterior a entrenamiento basados en escalares y en códigobooks. En comparación con el ajuste global de parámetros basado en bfloat16, QZO puede reducir el costo de memoria en más de 18 veces para un modelo de 4 bits de longitud larga (LLM), permitiendo entrenar modelos como Llama-2-13B y Stable Diffusion 3.5 Large en un solo GPU de 24 GB.",
      "upvotes": 5,
      "discussionId": "682d70c898e3ea9be315ee64",
      "githubRepo": "https://github.com/maifoundations/QZO",
      "ai_keywords": [
        "zeroth-order optimization",
        "gradient estimation",
        "model quantization",
        "bfloat16",
        "int4",
        "quantization scale",
        "directional derivative clipping",
        "Quantized Zeroth-order Optimization (QZO)",
        "scalar-based post-training quantization",
        "codebook-based post-training quantization",
        "full-parameter fine-tuning",
        "Llama-2-13B",
        "Stable Diffusion 3.5 Large"
      ]
    },
    "publishedAt": "2025-05-19T13:55:15.000Z",
    "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization",
    "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18times for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13430.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ac6656de8bfbb93094b8fd",
      "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
      "fullname": "Kaiyang Zhou",
      "name": "kaiyangzhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.12448",
      "authors": [
        {
          "_id": "682d47aa64daf8623f1f5604",
          "user": {
            "_id": "6586817e509bcae23f3dfc60",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pwEStoey1XJYDi3F0Z930.png",
            "isPro": false,
            "fullname": "Yang Liu",
            "user": "yliu-cs",
            "type": "user"
          },
          "name": "Yang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:37.342Z",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5605",
          "name": "Ming Ma",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5606",
          "name": "Xiaomin Yu",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5607",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5608",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5609",
          "name": "Mingyang Sun",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f560a",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f560b",
          "name": "Donglin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T14:40:16.000Z",
      "submittedOnDailyAt": "2025-05-21T01:56:36.099Z",
      "title": "SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR: SSR:",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "Los modelos de espacio visual (VLMs) están evolucionando en un ritmo impresionante en tareas multimodal, pero su dependencia de entradas RGB limita la precisión en la comprensión espacial. Los métodos actuales para integrar información espacial, como puntos de cloud o profundidad, requieren sensores especializados o no pueden utilizar efectivamente la información de profundidad en inferencias de alto nivel. En respuesta a esto, proponemos un nuevo enfoque para la comprensión espacial y razonamiento lógico, llamado SSR (Spatial Sense and Reasoning). SSR es un nuevo marco de trabajo que estructura y convierte datos simples de profundidad en cadenas de texto interpretables. Estas representaciones lógicas se utilizan como representaciones intermedias para mejorar significativamente la capacidad lógica espacial. Además, utilizando técnicas de compresión de aprendizaje, se pueden reducir estas representaciones lógicas a vectores potenciales concisos, lo que permite integrarlos eficientemente en plataformas actuales de VLMs. Introducimos el conjunto de datos SSR-CoT, que proporciona un conjunto de datos visual-lengua lógico de un millón de registros anotados con representaciones espaciales intermedias diversas. A través de SSRBench, presentamos detallados benchmarks para diferentes tareas de visión computacional. Experimentos extendidos en varios benchmarks muestran que SSR es efectiva en la utilización de profundidad, mejora la capacidad lógica espacial y orienta a los VLMs hacia un entendimiento multimodal más humano. Nuestra página del proyecto está disponible en https://yliu-cs.github.io/SSR.",
      "upvotes": 5,
      "discussionId": "682d47ab64daf8623f1f5634",
      "projectPage": "https://yliu-cs.github.io/SSR/",
      "githubRepo": "https://github.com/yliu-cs/SSR",
      "ai_keywords": [
        "Spatial Sense and Reasoning (SSR)",
        "structured, interpretable textual rationales",
        "knowledge distillation",
        "compact latent embeddings",
        "resource-efficient integration",
        "SSR-CoT",
        "million-scale visual-language reasoning dataset",
        "intermediate spatial reasoning annotations",
        "SSRBench",
        "comprehensive multi-task benchmark",
        "depth utilization",
        "spatial reasoning",
        "human-like multi-modal understanding"
      ]
    },
    "publishedAt": "2025-05-18T10:40:16.000Z",
    "title": "SSR: Enhancing Depth Perception in Vision-Language Models via\n  Rationale-Guided Spatial Reasoning",
    "summary": "Despite impressive advancements in Visual-Language Models (VLMs) for\nmulti-modal tasks, their reliance on RGB inputs limits precise spatial\nunderstanding. Existing methods for integrating spatial cues, such as point\nclouds or depth, either require specialized sensors or fail to effectively\nexploit depth information for higher-order reasoning. To this end, we propose a\nnovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that\ntransforms raw depth data into structured, interpretable textual rationales.\nThese textual rationales serve as meaningful intermediate representations to\nsignificantly enhance spatial reasoning capabilities. Additionally, we leverage\nknowledge distillation to compress the generated rationales into compact latent\nembeddings, which facilitate resource-efficient and plug-and-play integration\ninto existing VLMs without retraining. To enable comprehensive evaluation, we\nintroduce a new dataset named SSR-CoT, a million-scale visual-language\nreasoning dataset enriched with intermediate spatial reasoning annotations, and\npresent SSRBench, a comprehensive multi-task benchmark. Extensive experiments\non multiple benchmarks demonstrate SSR substantially improves depth utilization\nand enhances spatial reasoning, thereby advancing VLMs toward more human-like\nmulti-modal understanding. Our project page is at\nhttps://yliu-cs.github.io/SSR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14631",
      "authors": [
        {
          "_id": "682d69ed056cf7b86cd8d6ec",
          "user": {
            "_id": "66ab80e9bfb7d73a56bc293c",
            "avatarUrl": "/avatars/9644266304c832c74ef572b5eb2d9468.svg",
            "isPro": false,
            "fullname": "Jack",
            "user": "lingjie23",
            "type": "user"
          },
          "name": "Lingjie Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:17.384Z",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6ed",
          "user": {
            "_id": "62d1227384bfbee86b6eec56",
            "avatarUrl": "/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg",
            "isPro": false,
            "fullname": "Xun Wu",
            "user": "YUSHUIWX",
            "type": "user"
          },
          "name": "Xun Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:20.741Z",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6ee",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6ef",
          "name": "Qingxiu Dong",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f0",
          "name": "Zewen Chi",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f1",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f2",
          "name": "Xingxing Zhang",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f3",
          "name": "Tengchao Lv",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f4",
          "name": "Lei Cui",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f5",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:23:25.000Z",
      "submittedOnDailyAt": "2025-05-21T04:22:54.134Z",
      "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "62d1227384bfbee86b6eec56",
        "avatarUrl": "/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg",
        "isPro": false,
        "fullname": "Xun Wu",
        "user": "YUSHUIWX",
        "type": "user"
      },
      "summary": "Recientemente, los grandes modelos de lógica (LRMs) utilizan procesos de pensamiento ampliados antes de derivar resultados, mejorando significativamente su capacidad lógica en comparación con los modelos tradicionales de grandes modelos de lenguaje (LLMs). Sin embargo, el pensamiento excesivamente largo lleva a un consumo de tokens y un gran sobrecarga en la lengua latina, particularmente cuando se trata de preguntas sencillas. En este artículo, se presentan los grandes modelos híbridos de lógica (LHRMs). Los LHRMs utilizan por primera vez un enfoque de fine-tuning híbrido (HFT) con un inicio calmado, y combinan un enfoque de optimización de políticas de grupo híbrido (HGPO) para realizar una entrenamiento en línea, permitiendo decidir adaptativamente si el modelo debe realizar pensamientos basados en la información contextual de las preguntas del usuario. Además, para evaluar cuantitativamente la capacidad de los modelos híbridos de pensamiento, se propone el métrica de Hybrid Accuracy. Los LHRMs ofrecen un punto fuerte sólido para revisar de nuevo los métodos adecuados para el uso de procesos de pensamiento ampliados, mejorando tanto la capacidad lógica como la general, y aumentando significativamente la eficiencia.",
      "upvotes": 4,
      "discussionId": "682d69ee056cf7b86cd8d730",
      "ai_keywords": [
        "Large Reasoning Models (LRMs)",
        "Large Language Models (LLMs)",
        "Hybrid-Reasoning Models (LHRMs)",
        "Hybrid Fine-Tuning (HFT)",
        "Hybrid Group Policy Optimization (HGPO)",
        "Hybrid Accuracy"
      ]
    },
    "publishedAt": "2025-05-20T13:23:25.000Z",
    "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
    "summary": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14631.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d1227384bfbee86b6eec56",
      "avatarUrl": "/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg",
      "fullname": "Xun Wu",
      "name": "YUSHUIWX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14464",
      "authors": [
        {
          "_id": "682d39e6fa24196dcd10d5e8",
          "user": {
            "_id": "621499d72be42a56cca7afad",
            "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg",
            "isPro": false,
            "fullname": "TianXiaoyu",
            "user": "Emperorizzis",
            "type": "user"
          },
          "name": "Xiaoyu Tian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:56.337Z",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5e9",
          "name": "Yunjie Ji",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ea",
          "name": "Haotian Wang",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5eb",
          "name": "Shuaiting Chen",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ec",
          "name": "Sitong Zhao",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ed",
          "name": "Yiping Peng",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ee",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ef",
          "name": "Xiangang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T15:00:51.000Z",
      "submittedOnDailyAt": "2025-05-21T01:40:28.265Z",
      "title": "Todos los respuestas no son iguales: causas relacionadas con la destructión.",
      "submittedOnDailyBy": {
        "_id": "621499d72be42a56cca7afad",
        "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg",
        "isPro": false,
        "fullname": "TianXiaoyu",
        "user": "Emperorizzis",
        "type": "user"
      },
      "summary": "El diseñador de distilladores ha aparecido como un enfoque práctico y efectivo para mejorar la capacidad lógica de modelos de lenguaje abierto. En este estudio, se realizaron grandes experimentos de investigación sobre el distillador de datos de razonamiento basado en 189 mil consultas compartidas, obtenidas de tres modelos de profesionales más avanzados (AM-Thinking-v1, Qwen3-235B-A22B, DeepSeek-R1). Se construyeron tres conjuntos de datos paralelos y se analizaron sus distribuciones, revelando que los datos distillados de AM-Thinking-v1 presentan una mayor diversidad en longitud de etiquetas y una menor perplexidad. Los modelos estudiados fueron evaluados en marcos de referencia de razonamiento como AIME2024, AIME2025, MATH500 y LiveCodeBench. Los modelos basados en AM demostraron excelentes resultados: 84.3 en AIME2024, 72.2 en AIME2025, 98.4 en MATH500 y 65.9 en LiveCodeBench. Además, los modelos generan respuestas adaptativas, prolongadas para tareas difíciles y cortas para tareas simples. Estos hallazgos destacan la valiosa contribución de trabajos de razonamiento bien probados. Se publicaron los conjuntos de datos distillados de AM-Thinking-v1 y Qwen3-235B-A22B en Hugging Face, apoyando futuras investigaciones en modelos de razonamiento abiertos y altamente eficientes. Estos conjuntos de datos están disponibles para ser utilizados de manera pública en Hugging Face.",
      "upvotes": 4,
      "discussionId": "682d39e8fa24196dcd10d643",
      "ai_keywords": [
        "distillation",
        "reasoning capabilities",
        "open-source language models",
        "empirical study",
        "reasoning data distillation",
        "AM-Thinking-v1",
        "Qwen3-235B-A22B",
        "DeepSeek-R1",
        "shared corpus",
        "parallel datasets",
        "token length diversity",
        "perplexity",
        "student models",
        "reasoning benchmarks",
        "AIME2024",
        "AIME2025",
        "MATH500",
        "LiveCodeBench",
        "adaptive output behavior"
      ]
    },
    "publishedAt": "2025-05-20T11:00:51.000Z",
    "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters",
    "summary": "Distillation has emerged as a practical and effective approach to enhance the\nreasoning capabilities of open-source language models. In this work, we conduct\na large-scale empirical study on reasoning data distillation by collecting\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\nconstruct three parallel datasets and analyze their distributions, revealing\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\nlower perplexity. Student models trained on each dataset are evaluated on\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\nThe AM-based model consistently achieves the best performance (e.g., 84.3 on\nAIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and\ndemonstrates adaptive output behavior-producing longer responses for harder\ntasks and shorter ones for simpler tasks. These findings highlight the value of\nhigh-quality, verified reasoning traces. We release the AM-Thinking-v1 and\nQwen3-235B-A22B distilled datasets to support future research on open and\nhigh-performing reasoning-oriented language models. The datasets are publicly\navailable on Hugging FaceDatasets are available on Hugging Face:\n\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled{AM-Thinking-v1-Distilled},\nhttps://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled{AM-Qwen3-Distilled}.}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14464.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "621499d72be42a56cca7afad",
      "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg",
      "fullname": "TianXiaoyu",
      "name": "Emperorizzis",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14674",
      "authors": [
        {
          "_id": "682d4145514c96fbf03f5f76",
          "name": "Jiaxin Guo",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f77",
          "name": "Zewen Chi",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f78",
          "user": {
            "_id": "5df85abada6d0311fd3d5408",
            "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
            "isPro": false,
            "fullname": "Li Dong",
            "user": "unilm",
            "type": "user"
          },
          "name": "Li Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:52.470Z",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f79",
          "name": "Qingxiu Dong",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f7a",
          "user": {
            "_id": "62d1227384bfbee86b6eec56",
            "avatarUrl": "/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg",
            "isPro": false,
            "fullname": "Xun Wu",
            "user": "YUSHUIWX",
            "type": "user"
          },
          "name": "Xun Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:50.085Z",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f7b",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f7c",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/wpApND0JXiL6s6MUpBdg7.png"
      ],
      "publishedAt": "2025-05-20T17:58:03.000Z",
      "submittedOnDailyAt": "2025-05-21T02:57:46.082Z",
      "title": "Modelo de recompensa de niveles",
      "submittedOnDailyBy": {
        "_id": "5df85abada6d0311fd3d5408",
        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
        "isPro": false,
        "fullname": "Li Dong",
        "user": "unilm",
        "type": "user"
      },
      "summary": "El modelo de recompensa desempeña un papel crucial en la dirección de los modelos de lenguaje a gran escala hacia salidas que cumplan con las expectativas humanas. Sin embargo, el desafío de mejorar el rendimiento del modelo de recompensa utilizando eficientemente la cantidad de cálculos de la validación es abierto. En este artículo, se presenta el modelo de razones de recompensa (RRMs), diseñado para realizar un tratamiento cuidadoso antes de crear la recompensa final. A través de razones continuas, los RRMs utilizan adicionales cálculos de la validación para encontrar una recompensa adecuada en preguntas complejas donde la recompensa no es inmediatamente clara. El desarrollo de los RRMs incluye la implementación de un marco de aprendizaje por refuerzo que promueve la evolución del modelo y mejora su capacidad para generar razones de recompensa, sin necesidad de rastrear razones explícitas. Los resultados de los experimentos muestran que los RRMs alcanzan excelentes resultados en diferentes áreas en el marco de las pruebas de modelos de recompensa. En particular, los RRMs demuestran la capacidad de aprovechar adecuadamente la cantidad de cálculos de la validación para mejorar la precisión de la recompensa. Los modelos de razones de recompensa pre-entrenados están disponibles en https://huggingface.co/Reward-Reasoning.",
      "upvotes": 3,
      "discussionId": "682d4146514c96fbf03f5fab",
      "ai_keywords": [
        "Reward models",
        "large language models",
        "chain-of-thought reasoning",
        "reinforcement learning",
        "self-evolved reward reasoning",
        "reward modeling benchmarks"
      ]
    },
    "publishedAt": "2025-05-20T13:58:03.000Z",
    "title": "Reward Reasoning Model",
    "summary": "Reward models play a critical role in guiding large language models toward\noutputs that align with human expectations. However, an open challenge remains\nin effectively utilizing test-time compute to enhance reward model performance.\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\nspecifically designed to execute a deliberate reasoning process before\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\nadditional test-time compute for complex queries where appropriate rewards are\nnot immediately apparent. To develop RRMs, we implement a reinforcement\nlearning framework that fosters self-evolved reward reasoning capabilities\nwithout requiring explicit reasoning traces as training data. Experimental\nresults demonstrate that RRMs achieve superior performance on reward modeling\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\nexploit test-time compute to further improve reward accuracy. The pretrained\nreward reasoning models are available at\nhttps://huggingface.co/Reward-Reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/wpApND0JXiL6s6MUpBdg7.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5df85abada6d0311fd3d5408",
      "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
      "fullname": "Li Dong",
      "name": "unilm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14352",
      "authors": [
        {
          "_id": "682d72b0d57ba1e4d132148d",
          "user": {
            "_id": "6422f416a73327caad9d1d86",
            "avatarUrl": "/avatars/aa3639277cd1732504402fc64a57eff8.svg",
            "isPro": false,
            "fullname": "Bartosz Cywiński",
            "user": "bcywinski",
            "type": "user"
          },
          "name": "Bartosz Cywiński",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:39:48.008Z",
          "hidden": false
        },
        {
          "_id": "682d72b0d57ba1e4d132148e",
          "name": "Emil Ryd",
          "hidden": false
        },
        {
          "_id": "682d72b0d57ba1e4d132148f",
          "name": "Senthooran Rajamanoharan",
          "hidden": false
        },
        {
          "_id": "682d72b0d57ba1e4d1321490",
          "name": "Neel Nanda",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T13:36:37.000Z",
      "submittedOnDailyAt": "2025-05-21T04:59:37.379Z",
      "title": "El acceso a la extracción de conocimientos potenciales automáticamente explicables en modelos de lenguaje profundo",
      "submittedOnDailyBy": {
        "_id": "6422f416a73327caad9d1d86",
        "avatarUrl": "/avatars/aa3639277cd1732504402fc64a57eff8.svg",
        "isPro": false,
        "fullname": "Bartosz Cywiński",
        "user": "bcywinski",
        "type": "user"
      },
      "summary": "Este texto solo requiere la traducción al español, sin agregar ningún comentario o texto adicional. Aquí está la traducción:\n\n**Transcripción al español:**\n\nEste texto solo requiere la traducción al español, sin agregar ningún comentario o texto adicional. Aquí está la traducción:\n\n**Traducción:**\n\nEste texto solo requiere la traducción al español, sin agregar ningún comentario o texto adicional. Aquí está la traducción:",
      "upvotes": 3,
      "discussionId": "682d72b1d57ba1e4d13214c6",
      "githubRepo": "https://github.com/EmilRyd/eliciting-secrets",
      "ai_keywords": [
        "Taboo model",
        "logit lens",
        "sparse autoencoders"
      ]
    },
    "publishedAt": "2025-05-20T09:36:37.000Z",
    "title": "Towards eliciting latent knowledge from LLMs with mechanistic\n  interpretability",
    "summary": "As language models become more powerful and sophisticated, it is crucial that\nthey remain trustworthy and reliable. There is concerning preliminary evidence\nthat models may attempt to deceive or keep secrets from their operators. To\nexplore the ability of current techniques to elicit such hidden knowledge, we\ntrain a Taboo model: a language model that describes a specific secret word\nwithout explicitly stating it. Importantly, the secret word is not presented to\nthe model in its training data or prompt. We then investigate methods to\nuncover this secret. First, we evaluate non-interpretability (black-box)\napproaches. Subsequently, we develop largely automated strategies based on\nmechanistic interpretability techniques, including logit lens and sparse\nautoencoders. Evaluation shows that both approaches are effective in eliciting\nthe secret word in our proof-of-concept setting. Our findings highlight the\npromise of these approaches for eliciting hidden knowledge and suggest several\npromising avenues for future work, including testing and refining these methods\non more complex model organisms. This work aims to be a step towards addressing\nthe crucial problem of eliciting secret knowledge from language models, thereby\ncontributing to their safe and reliable deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14352.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6422f416a73327caad9d1d86",
      "avatarUrl": "/avatars/aa3639277cd1732504402fc64a57eff8.svg",
      "fullname": "Bartosz Cywiński",
      "name": "bcywinski",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14135",
      "authors": [
        {
          "_id": "682d43cf85d5e40c81ed313d",
          "name": "Ruihuang Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed313e",
          "name": "Caijin Zhou",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed313f",
          "name": "Shoujian Zheng",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3140",
          "name": "Jianxiang Lu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3141",
          "name": "Jiabin Huang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3142",
          "name": "Comi Chen",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3143",
          "name": "Junshu Tang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3144",
          "name": "Guangzheng Xu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3145",
          "name": "Jiale Tao",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3146",
          "name": "Hongmei Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3147",
          "name": "Donghao Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3148",
          "name": "Wenqing Yu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3149",
          "name": "Senbo Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314a",
          "name": "Zhimin Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314b",
          "name": "Yetshuan Shi",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314c",
          "name": "Haoyu Yang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314d",
          "name": "Yukun Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314e",
          "name": "Wenxun Dai",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314f",
          "name": "Jiaqi Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3150",
          "name": "Linqing Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3151",
          "name": "Qixun Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3152",
          "name": "Zhiyong Xu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3153",
          "name": "Yingfang Zhang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3154",
          "name": "Jiangfeng Xiong",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3155",
          "name": "Weijie Kong",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3156",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3157",
          "name": "Hongxin Zhang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3158",
          "name": "Qiaoling Zheng",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3159",
          "name": "Weiting Guo",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315a",
          "name": "Xinchi Deng",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315b",
          "name": "Yixuan Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315c",
          "name": "Renjia Wei",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315d",
          "name": "Yulin Jian",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315e",
          "name": "Duojun Huang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315f",
          "name": "Xuhua Ren",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3160",
          "name": "Sihuan Lin",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3161",
          "name": "Yifu Sun",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3162",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3163",
          "name": "Joey Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3164",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3165",
          "name": "Jingmiao Yu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3166",
          "name": "Jihong Zhang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3167",
          "name": "Caesar Zhong",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3168",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3169",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316a",
          "name": "Linus",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316b",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316c",
          "name": "Longhuang Wu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316d",
          "name": "Shuai Shao",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316e",
          "name": "Qinglin Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T09:39:48.000Z",
      "submittedOnDailyAt": "2025-05-21T01:40:24.172Z",
      "title": "FUNYOUNG GAME: MODELO DE PRODUCCIÓN DE JUEGOS SMART A NIVEL DE ÍNDICE Y DE NIVEL DE REALIDAD",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Intelligent game development ha sido un desarrollo innovador en el desarrollo de juegos, demostrando la generación de contenido de juegos dinámico y su mejora utilizando AI. Además del avance significativo de los modelos generativos, la síntesis de alta calidad de los recursos de juego (incluyendo imágenes y vídeos) es una frontera compleja. Para producir contenido de juego de alta calidad que se ajuste tanto a las preferencias del jugador como a la eficiencia de los diseñadores, presentamos el proyecto innovador \"Alto Rendimiento Juego\" (High-Performance Game). \"Alto Rendimiento Juego\" está compuesto de dos áreas principales: generación de imágenes y generación de vídeos. El componente de generación de imágenes desarrolló un modelo de generación de imágenes personalizadas para el escenario de juego basado en un gran conjunto de datos de imágenes de juego: (1) la generación de imágenes a partir de texto general; (2) la generación de efectos visuales de juego, basados en texto y imágenes de referencia; (3) la generación de imágenes transparentes, incluyendo personajes, escenarios y efectos visuales de juego; (4) la generación de personajes de juego basados en dibujos, imágenes en blanco y negro y modelos en blanco. El componente de generación de vídeos desarrolló cinco modelos de algoritmos clave adaptables a diferentes escenarios de vídeos de juego y animación, basados en un conjunto de datos detallado que incluye miles de juegos y animaciones: (1) la generación de vídeos a partir de imágenes; (2) la síntesis de imágenes para reemplazar las posiciones 360º A/T; (3) la generación de relaciones de ilustración dinámica; (4) la generación de vídeos de alta resolución; (5) la generación de vídeos de juego interactivo. Estos modelos de generación de imágenes y vídeos muestran un alto nivel de expresión artística, profundizan la comprensión sistemática de los estilos artísticos de juegos y animaciones, y integran profundamente el conocimiento propio de la disciplina.",
      "upvotes": 3,
      "discussionId": "682d43d585d5e40c81ed3302",
      "ai_keywords": [
        "text-to-image generation",
        "game visual effects generation",
        "text-to-effect",
        "reference image-based generation",
        "transparent image generation",
        "character generation",
        "sketch-based generation",
        "black-and-white image-based generation",
        "white model-based generation",
        "image-to-video generation",
        "360 A/T Pose Avatar Video Synthesis",
        "dynamic illustration generation",
        "generative video super-resolution",
        "interactive game video generation"
      ]
    },
    "publishedAt": "2025-05-20T05:39:48.000Z",
    "title": "Hunyuan-Game: Industrial-grade Intelligent Game Creation Model",
    "summary": "Intelligent game creation represents a transformative advancement in game\ndevelopment, utilizing generative artificial intelligence to dynamically\ngenerate and enhance game content. Despite notable progress in generative\nmodels, the comprehensive synthesis of high-quality game assets, including both\nimages and videos, remains a challenging frontier. To create high-fidelity game\ncontent that simultaneously aligns with player preferences and significantly\nboosts designer efficiency, we present Hunyuan-Game, an innovative project\ndesigned to revolutionize intelligent game production. Hunyuan-Game encompasses\ntwo primary branches: image generation and video generation. The image\ngeneration component is built upon a vast dataset comprising billions of game\nimages, leading to the development of a group of customized image generation\nmodels tailored for game scenarios: (1) General Text-to-Image Generation. (2)\nGame Visual Effects Generation, involving text-to-effect and reference\nimage-based game visual effect generation. (3) Transparent Image Generation for\ncharacters, scenes, and game visual effects. (4) Game Character Generation\nbased on sketches, black-and-white images, and white models. The video\ngeneration component is built upon a comprehensive dataset of millions of game\nand anime videos, leading to the development of five core algorithmic models,\neach targeting critical pain points in game development and having robust\nadaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)\n360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)\nGenerative Video Super-Resolution. (5) Interactive Game Video Generation. These\nimage and video generation models not only exhibit high-level aesthetic\nexpression but also deeply integrate domain-specific knowledge, establishing a\nsystematic understanding of diverse game and anime art styles.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14135.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6898
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11365",
      "authors": [
        {
          "_id": "682d7ae082567fffe108b31a",
          "user": {
            "_id": "6596ca5cce76219628b8eab4",
            "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
            "isPro": false,
            "fullname": "Pierre Le Jeune",
            "user": "pierlj",
            "type": "user"
          },
          "name": "Pierre Le Jeune",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T07:16:41.247Z",
          "hidden": false
        },
        {
          "_id": "682d7ae082567fffe108b31b",
          "user": {
            "_id": "65bccff4c1a44b6ef1c100da",
            "avatarUrl": "/avatars/bf91000c78b83167958dc44c582397f0.svg",
            "isPro": false,
            "fullname": "benoit",
            "user": "bmalezieux",
            "type": "user"
          },
          "name": "Benoît Malézieux",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:04:02.669Z",
          "hidden": false
        },
        {
          "_id": "682d7ae082567fffe108b31c",
          "user": {
            "_id": "649f00fc37bfb5202be464a9",
            "avatarUrl": "/avatars/89f6c6a92c076099f5450c3cd2057619.svg",
            "isPro": false,
            "fullname": "Inoki at Giskard",
            "user": "inoki-giskard",
            "type": "user"
          },
          "name": "Weixuan Xiao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:04:02.669Z",
          "hidden": false
        },
        {
          "_id": "682d7ae082567fffe108b31d",
          "name": "Matteo Dora",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T15:31:08.000Z",
      "submittedOnDailyAt": "2025-05-21T05:35:45.784Z",
      "title": "Ferramenta de Verificación de Seguridad de Modelos de Lenguaje de Gran Tamaño",
      "submittedOnDailyBy": {
        "_id": "6596ca5cce76219628b8eab4",
        "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
        "isPro": false,
        "fullname": "Pierre Le Jeune",
        "user": "pierlj",
        "type": "user"
      },
      "summary": "La garantía de seguridad de los LLM es importante en la configuración responsable de las funciones, aunque las evaluaciones actuales a menudo priorizan el rendimiento y especifican modos de fallo. Presentamos el marco de diagnóstico multilingüe \"Phare\" para investigar y evaluar el comportamiento de los LLM en tres dimensiones cruciales: interacción y confianza, bias social, y el generación de contenido nocivo. Los resultados de la evaluación de los 17 mejores LLMs avanzados revelan patrones de vulnerabilidades sistemáticas en todos los aspectos de seguridad. La Phare, que destaca con características específicas de los modos de falla, proporciona ideas prácticas que ayudan a los investigadores y prácticos a construir sistemas de lenguaje más robustos y confiables.",
      "upvotes": 3,
      "discussionId": "682d7ae282567fffe108b40f",
      "projectPage": "https://phare.giskard.ai/",
      "githubRepo": "https://github.com/Giskard-AI/phare",
      "ai_keywords": [
        "hallucination",
        "reliability",
        "social biases",
        "harmful content generation",
        "multilingual diagnostic framework",
        "sycophancy",
        "prompt sensitivity",
        "stereotype reproduction"
      ]
    },
    "publishedAt": "2025-05-16T11:31:08.000Z",
    "title": "Phare: A Safety Probe for Large Language Models",
    "summary": "Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11365.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6596ca5cce76219628b8eab4",
      "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
      "fullname": "Pierre Le Jeune",
      "name": "pierlj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14534",
      "authors": [
        {
          "_id": "682d7f009a06bc9f9106480b",
          "user": {
            "_id": "65ef0b4b325d9aaef87eb33b",
            "avatarUrl": "/avatars/ec27dba9de7e74c1f6cdcacf5aa25528.svg",
            "isPro": false,
            "fullname": "C Shi",
            "user": "chongyangs",
            "type": "user"
          },
          "name": "Chongyang Shi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:21:37.326Z",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f9106480c",
          "name": "Sharon Lin",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f9106480d",
          "name": "Shuang Song",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f9106480e",
          "name": "Jamie Hayes",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f9106480f",
          "name": "Ilia Shumailov",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064810",
          "name": "Itay Yona",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064811",
          "name": "Juliette Pluto",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064812",
          "name": "Aneesh Pappu",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064813",
          "name": "Christopher A. Choquette-Choo",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064814",
          "name": "Milad Nasr",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064815",
          "name": "Chawin Sitawarin",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064816",
          "name": "Gena Gibson",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064817",
          "name": "Andreas Terzis",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064818",
          "name": "John \"Four\" Flynn",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T15:54:45.000Z",
      "submittedOnDailyAt": "2025-05-21T05:52:22.988Z",
      "title": "La lección de la injección falsa de Pronophtenije en Genova",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "Minima es utilizado para que los usuarios realicen tareas en su lugar. Esto ha permitido que las llamadas a funciones y las funciones de uso de herramientas se puedan ejecutar con acceso a los datos del usuario. Sin embargo, algunas herramientas necesitan acceder a datos inseguros, lo que puede generar riesgos. Estas guías pueden incluir instrucciones maliciosas en datos inseguros, llevando al modelo a desviarse de las expectativas del usuario y provocando el maltrato de datos o permisos. En este informe, explicamos cómo Google DeepMind evalúa la resistencia de Minima a su adversario, y explicamos las lecciones principales obtenidas en el proceso. Para evaluar el rendimiento de Minima frente a guías complejas, se utiliza un marco de evaluación de enfrentamiento contrario, que aplica una serie de métodos de ataque continuos en versiones pasadas, presentes y futuras de Minima. Esta evaluación continua directamente contribuye a fortalecer la resistencia de Minima.",
      "upvotes": 2,
      "discussionId": "682d7f019a06bc9f9106486e",
      "ai_keywords": [
        "function-calling",
        "tool-use capabilities",
        "adversarial robustness",
        "adaptive attack techniques",
        "resilience"
      ]
    },
    "publishedAt": "2025-05-20T11:54:45.000Z",
    "title": "Lessons from Defending Gemini Against Indirect Prompt Injections",
    "summary": "Gemini is increasingly used to perform tasks on behalf of users, where\nfunction-calling and tool-use capabilities enable the model to access user\ndata. Some tools, however, require access to untrusted data introducing risk.\nAdversaries can embed malicious instructions in untrusted data which cause the\nmodel to deviate from the user's expectations and mishandle their data or\npermissions. In this report, we set out Google DeepMind's approach to\nevaluating the adversarial robustness of Gemini models and describe the main\nlessons learned from the process. We test how Gemini performs against a\nsophisticated adversary through an adversarial evaluation framework, which\ndeploys a suite of adaptive attack techniques to run continuously against past,\ncurrent, and future versions of Gemini. We describe how these ongoing\nevaluations directly help make Gemini more resilient against manipulation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14534.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.12182",
      "authors": [
        {
          "_id": "682d31dd17608739046e1169",
          "name": "Haohang Li",
          "hidden": false
        },
        {
          "_id": "682d31dd17608739046e116a",
          "name": "Yupeng Cao",
          "hidden": false
        },
        {
          "_id": "682d31dd17608739046e116b",
          "name": "Yangyang Yu",
          "hidden": false
        },
        {
          "_id": "682d31dd17608739046e116c",
          "name": "Jordan W. Suchow",
          "hidden": false
        },
        {
          "_id": "682d31dd17608739046e116d",
          "name": "Zining Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T00:47:21.000Z",
      "submittedOnDailyAt": "2025-05-21T00:23:07.653Z",
      "title": "Truth Neurons",
      "submittedOnDailyBy": {
        "_id": "634cabd104491d9f7111eea3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cabd104491d9f7111eea3/JoqlugwfD1aGkd-wZTmP7.jpeg",
        "isPro": false,
        "fullname": "Haohang Li",
        "user": "Acatsama",
        "type": "user"
      },
      "summary": "En la tesis, aunque los modelos de lenguaje registran éxitos impresionantes y innovaciones en diferentes flujos de trabajo, a veces generan respuestas desfavorables. Entendemos que estos modelos se forman mecánicamente a partir de la verdad, lo que puede poner en riesgo la confianza y la seguridad. En este artículo, proponemos una manera de reconocer la verdad a nivel neuronal. Demostramos que existen \"neurones de verdad\" (TRUTH NUERON) en los modelos de lenguaje que codifican la verdad independientemente de la capa de neuronas. Realizamos experimentos en modelos de diferentes tamaños y demostramos la existencia de estos neurones de verdad y que su codificación es una característica compartida en muchos modelos de lenguaje. El patrón de distribución de los neurones de verdad en diferentes capas coincide con la geometría de la verdad. Al restringir la activación de los neurones de verdad en el conjunto de datos TruthfulQA, la performance en este conjunto y otros benchmarks se ve afectada, lo que demuestra que la estructura de la verdad no depende de un conjunto de datos específico. Nuestros resultados proporcionan una nueva perspectiva sobre la estructura de la verdad en modelos de lenguaje y revelan potenciales direcciones para mejorar la confianza y la confiabilidad.",
      "upvotes": 2,
      "discussionId": "682d31de17608739046e11c9",
      "ai_keywords": [
        "truth neurons",
        "neuron level",
        "truthfulness mechanisms",
        "TruthfulQA dataset"
      ]
    },
    "publishedAt": "2025-05-17T20:47:21.000Z",
    "title": "Truth Neurons",
    "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12182.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634cabd104491d9f7111eea3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cabd104491d9f7111eea3/JoqlugwfD1aGkd-wZTmP7.jpeg",
      "fullname": "Haohang Li",
      "name": "Acatsama",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09569",
      "authors": [
        {
          "_id": "682563f807f74666ec373332",
          "name": "Linbo Liu",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373333",
          "user": {
            "_id": "636c32ae181c81c337f086b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg",
            "isPro": false,
            "fullname": "Xinle Sheila Liu",
            "user": "sliuxl",
            "type": "user"
          },
          "name": "Xinle Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-15T10:30:40.773Z",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373334",
          "name": "Qiang Zhou",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373335",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373336",
          "name": "Yihan Liu",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373337",
          "name": "Hoan Nguyen",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373338",
          "name": "Behrooz Omidvar-Tehrani",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373339",
          "name": "Xi Shen",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec37333a",
          "name": "Jun Huan",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec37333b",
          "name": "Omer Tripp",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec37333c",
          "name": "Anoop Deoras",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T17:11:23.000Z",
      "submittedOnDailyAt": "2025-05-21T04:22:33.758Z",
      "title": "MIGRATION-BENCH: Jaba 8 desde el nivel de repositorio de código migración de prueba",
      "submittedOnDailyBy": {
        "_id": "636c32ae181c81c337f086b9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg",
        "isPro": false,
        "fullname": "Xinle Sheila Liu",
        "user": "sliuxl",
        "type": "user"
      },
      "summary": "Recientemente, el rápido desarrollo de potentes modelos de lenguaje natural (LLMs) ha permitido que tareas diversas en la ingeniería de software puedan ser resueltas con el uso de LLMs, lo que ha mejorado significativamente la productividad y la escalabilidad. Para evaluar las capacidades de codificación de estos modelos, se han desarrollado varios conjuntos de datos de benchmark, pero su enfoque principal ha sido principalmente en la resolución de problemas o tareas de resolución de problemas. En contraste, presentamos un nuevo benchmark de codificación llamado \"MIGRATION-BENCH\", cuyo enfoque es la migración de código. \"MIGRATION-BENCH\" es un benchmark que evalúa la migración de Java 8 a las últimas versiones de soporte largo plazo (LTS) (Java 17, 21). \"MIGRATION-BENCH\" incluye un conjunto de datos completo de 5,102 y 300 repositorios seleccionados, así como sus subconjuntos. La colección seleccionada se centra en la complejidad y la dificultad, y se proporciona como recursos varios para la investigación de migración de código. Además, proporcionamos un marco de evaluación estricto y estandarizado, lo que fomenta la evaluación de LLMs en estas tareas difíciles. Además, proponemos SD-Feedback y demostramos que LLMs pueden resolver eficazmente la migración de código a nivel de repositorio. Al utilizar Claude-3.5-Sonnet-v2, el mínimo y máximo porcentajes de éxito en la migración en las colecciones seleccionadas fueron respectivamente del 62.33% y el 27.00%. Los conjuntos de datos de benchmark y códigos se pueden acceder a través de las siguientes URLs:\nhttps://huggingface.co/collections/AmazonScience\nhttps://github.com/amazon-science/self_debug",
      "upvotes": 2,
      "discussionId": "682563f907f74666ec373369",
      "projectPage": "https://github.com/amazon-science/SDFeedback",
      "githubRepo": "https://github.com/amazon-science/MigrationBench",
      "ai_keywords": [
        "large language models (LLMs)",
        "code migration",
        "MIGRATION-BENCH",
        "Java 8",
        "long-term support (LTS) versions (Java 17, 21)",
        "repositories",
        "SD-Feedback",
        "pass@1"
      ]
    },
    "publishedAt": "2025-05-14T13:11:23.000Z",
    "title": "MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8",
    "summary": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on problem-solving and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a\ndistinct focus: code migration. MIGRATION-BENCH aims to serve as a\ncomprehensive benchmark for migration from Java 8 to the latest long-term\nsupport (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset\nand its subset selected with 5,102 and 300 repositories respectively.\nSelected is a representative subset curated for complexity and difficulty,\noffering a versatile resource to support research in the field of code\nmigration. Additionally, we provide a comprehensive evaluation framework to\nfacilitate rigorous and standardized assessment of LLMs on this challenging\ntask. We further propose SD-Feedback and demonstrate that LLMs can effectively\ntackle repository-level code migration to Java 17. For the selected subset with\nClaude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate\n(pass@1) for minimal and maximal migration respectively. The benchmark dataset\nand source code are available at:\nhttps://huggingface.co/collections/AmazonScience and\nhttps://github.com/amazon-science/self_debug respectively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09569.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636c32ae181c81c337f086b9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg",
      "fullname": "Xinle Sheila Liu",
      "name": "sliuxl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13718",
      "authors": [
        {
          "_id": "682d6ccc26146f27d1ab3d83",
          "user": {
            "_id": "64cb922ec7f30fbf7b91a9a7",
            "avatarUrl": "/avatars/457eae5e56b9641ee5543146447d1755.svg",
            "isPro": false,
            "fullname": "Safal Shrestha",
            "user": "safal312",
            "type": "user"
          },
          "name": "Safal Shrestha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:07.374Z",
          "hidden": false
        },
        {
          "_id": "682d6ccc26146f27d1ab3d84",
          "name": "Minwu Kim",
          "hidden": false
        },
        {
          "_id": "682d6ccc26146f27d1ab3d85",
          "name": "Aadim Nepal",
          "hidden": false
        },
        {
          "_id": "682d6ccc26146f27d1ab3d86",
          "name": "Anubhav Shrestha",
          "hidden": false
        },
        {
          "_id": "682d6ccc26146f27d1ab3d87",
          "name": "Keith Ross",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T20:29:15.000Z",
      "submittedOnDailyAt": "2025-05-21T04:35:24.229Z",
      "title": "Antes de iniciar el primer desafío: liberación de restricciones de recursos por razones comunes",
      "submittedOnDailyBy": {
        "_id": "64cb922ec7f30fbf7b91a9a7",
        "avatarUrl": "/avatars/457eae5e56b9641ee5543146447d1755.svg",
        "isPro": false,
        "fullname": "Safal Shrestha",
        "user": "safal312",
        "type": "user"
      },
      "summary": "Diseñar capacidades de inferencia efectivas en un LLM a menudo requiere el uso de un aprendizaje por refuerzo con recompensas provable (RLVR) o una diseño de cadenas largas de pensamiento ajustadas (Long Chain of Thoughts, CoT). Estos métodos necesitan datos de entrenamiento detallados. Por lo tanto, si la calidad de los datos de entrenamiento es baja, esto se convierte en un gran problema. Proponemos una estrategia de entrenamiento eficiente en dos etapas para desarrollar un LLM que pueda justificar razones en un espacio de datos limitado. La primera etapa consiste en entrenar el modelo utilizando \"Knights \\& Knaves (K\\&K) ロジックパズル\" para realizar una entrenamiento de CoT larga, lo que permite que el modelo \"caliente\" y aprenda habilidades generales de justificación. La segunda etapa aplica RLVR en el modelo caliente utilizando ejemplos limitados del dominio objetivo. Nuestros experimentos muestran que este método de dos etapas tiene las siguientes ventajas: (i) aunque en la etapa de caliente, promueve la expansión de la justificación y mejora el rendimiento en tareas como MATH, HumanEval^{+}, MMLU-Pro, entre otras; (ii) en el mismo pequeño conjunto de datos (menos de 100 ejemplos), el modelo caliente es superior al modelo básico durante la entrenamiento con RLVR; (iii) caliente antes de la entrenamiento con RLVR, puede mantener la generalización cruzada después de la entrenamiento en el dominio específico; (iv) introducir caliente durante la entrenamiento con RLVR mejora la eficiencia de los ejemplos de entrenamiento, aumenta la precisión y la eficiencia general de los ejemplos de entrenamiento. Los resultados de este artículo revelan la posibilidad de construir un potente LLM que puede justificar razones en entornos con escasez de datos.",
      "upvotes": 1,
      "discussionId": "682d6ccd26146f27d1ab3dbb",
      "githubRepo": "https://github.com/safal312/warmup-before-you-train",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "Long Chain of Thoughts (CoT)",
        "Knights \\& Knaves (K\\&K) logic puzzles",
        "MATH",
        "HumanEval$^{+}$",
        "MMLU-Pro",
        "warmup phase",
        "generalized reasoning",
        "cross-domain generalizability",
        "sample efficiency",
        "robust reasoning LLMs"
      ]
    },
    "publishedAt": "2025-05-19T16:29:15.000Z",
    "title": "Warm Up Before You Train: Unlocking General Reasoning in\n  Resource-Constrained Settings",
    "summary": "Designing effective reasoning-capable LLMs typically requires training using\nReinforcement Learning with Verifiable Rewards (RLVR) or distillation with\ncarefully curated Long Chain of Thoughts (CoT), both of which depend heavily on\nextensive training data. This creates a major challenge when the amount of\nquality training data is scarce. We propose a sample-efficient, two-stage\ntraining strategy to develop reasoning LLMs under limited supervision. In the\nfirst stage, we \"warm up\" the model by distilling Long CoTs from a toy domain,\nnamely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning\nskills. In the second stage, we apply RLVR to the warmed-up model using a\nlimited set of target-domain examples. Our experiments demonstrate that this\ntwo-phase approach offers several benefits: (i) the warmup phase alone\nfacilitates generalized reasoning, leading to performance improvements across a\nrange of tasks, including MATH, HumanEval^{+}, and MMLU-Pro. (ii) When both\nthe base model and the warmed-up model are RLVR trained on the same small\ndataset (leq100 examples), the warmed-up model consistently outperforms the\nbase model; (iii) Warming up before RLVR training allows a model to maintain\ncross-domain generalizability even after training on a specific domain; (iv)\nIntroducing warmup in the pipeline improves not only accuracy but also overall\nsample efficiency during RLVR training. The results in this paper highlight the\npromise of warmup for building robust reasoning LLMs in data-scarce\nenvironments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13718.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb922ec7f30fbf7b91a9a7",
      "avatarUrl": "/avatars/457eae5e56b9641ee5543146447d1755.svg",
      "fullname": "Safal Shrestha",
      "name": "safal312",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13380",
      "authors": [
        {
          "_id": "682d3787265177367e119f04",
          "user": {
            "_id": "64c2bea2ada7df214276913b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
            "isPro": false,
            "fullname": "Nguyen Van Nam",
            "user": "DavidNguyen",
            "type": "user"
          },
          "name": "Nam V. Nguyen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:58.584Z",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f05",
          "name": "Huy Nguyen",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f06",
          "name": "Quang Pham",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f07",
          "name": "Van Nguyen",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f08",
          "name": "Savitha Ramasamy",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f09",
          "name": "Nhat Ho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T17:24:26.000Z",
      "submittedOnDailyAt": "2025-05-21T00:48:58.161Z",
      "title": "Competencia para entrenamiento de combinaciones de expertos estadísticamente garantizadas (CompeteSMoE)",
      "submittedOnDailyBy": {
        "_id": "64c2bea2ada7df214276913b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
        "isPro": false,
        "fullname": "Nguyen Van Nam",
        "user": "DavidNguyen",
        "type": "user"
      },
      "summary": "Sparse mixture of experts (SMoE) es una solución atractiva para el aumento de la complejidad de un modelo debido al aumento de profundidad o ancho de la red. Sin embargo, el entrenamiento efectivo de SMoE plantea desafíos debido a que los Expertos que realizan la computación no contribuyen directamente al proceso de Routing. En este artículo, se propone una nueva estructura llamada \"Compete\", que redirige tokens hacia los Expertos más potentes en la respuesta neural de la red. Teóricamente, la estructura de Compete espera un mejor rendimiento de muestras que la Softmax Routing existente. Además, se desarrolla un algoritmo sencillo y efectivo llamado CompeteSMoE, que permite entrenar la política de Compete usando un Router, lo que permite alcanzar un buen rendimiento a bajo costo. Este algoritmo ha sido evaluado ampliamente en tareas de Instrucción de Imágenes y Pre-entrenamiento de Lenguaje, demostrando eficiencia, robustez y escalabilidad en comparación con las estrategias más recientes de SMoE. La implementación está disponible en https://github.com/Fsoft-AIC/CompeteSMoE. Este artículo es una mejora de la investigación previa en arXiv:2402.02526.",
      "upvotes": 1,
      "discussionId": "682d3788265177367e119f71",
      "githubRepo": "https://github.com/Fsoft-AIC/CompeteSMoE",
      "ai_keywords": [
        "Sparse mixture of experts (SMoE)",
        "competition mechanism",
        "sample efficiency",
        "softmax routing",
        "CompeteSMoE",
        "neural response",
        "router",
        "competition policy",
        "visual instruction tuning",
        "language pre-training"
      ]
    },
    "publishedAt": "2025-05-19T13:24:26.000Z",
    "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\n  Competition",
    "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13380.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c2bea2ada7df214276913b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
      "fullname": "Nguyen Van Nam",
      "name": "DavidNguyen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13103",
      "authors": [
        {
          "_id": "682d7f7a176087390484a412",
          "name": "Han Zheng",
          "hidden": false
        },
        {
          "_id": "682d7f7a176087390484a413",
          "name": "Ilia Shumailov",
          "hidden": false
        },
        {
          "_id": "682d7f7a176087390484a414",
          "name": "Tianqi Fan",
          "hidden": false
        },
        {
          "_id": "682d7f7a176087390484a415",
          "name": "Aiden Hall",
          "hidden": false
        },
        {
          "_id": "682d7f7a176087390484a416",
          "name": "Mathias Payer",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T13:32:51.000Z",
      "submittedOnDailyAt": "2025-05-21T05:55:51.238Z",
      "title": "1 dólar para corregir 7,400 errores: tecnología de costo bajo para corrección de programación de costo bajo",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "El rápido desarrollo de los métodos de detección de errores ha permitido a los desarrolladores encontrar muchas más vulnerabilidades que pueden ser modificadas razonablemente, lo que ha despertado la necesidad de métodos efectivos de auto-revisión y corrección de código (APR). Sin embargo, la complejidad de los errores modernos hace que el análisis del origen sea difícil, lo que genera un descontento. Para resolver estos desafíos, proponemos la revisión explosiva del código, decidimos simplificar el trabajo de revisión y asumir el riesgo de usar un foro. Además, presentamos una aproximación de generación de patches guiados por templados para mantener la eficiencia y eficacia, reduciendo significativamente los costos de tokens de modelos de lenguaje de alto nivel (LLMs).\n\nImplementamos un sistema prototipo y lo comparamos con las herramientas de APR más avanzadas. Al usar el mejor código ágil, CodeRover-S, WILLIAMT redució los costos de tokens en un 45.9% y aumentó la tasa de corrección de errores en ARVO (un benchmark de vulnerabilidades de software abierto) en un 73.5%, lo que representa un aumento del 29.6%. Además, demostró ser efectivo incluso en situaciones donde no hay acceso a modelos avanzados de LLMs: logró un rendimiento razonable en un modelo local ejecutado en Mac M4 Mini. Estos hallazgos revelan la amplia aplicabilidad y escalabilidad de WILLIAMT.",
      "upvotes": 1,
      "discussionId": "682d7f7a176087390484a448",
      "ai_keywords": [
        "crash-site repair",
        "template-guided patch generation",
        "Large Language Models (LLMs)",
        "WILLIAMT",
        "CodeRover-S",
        "ARVO",
        "ground-truth open source software vulnerabilities benchmark"
      ]
    },
    "publishedAt": "2025-05-19T09:32:51.000Z",
    "title": "Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair",
    "summary": "The rapid advancement of bug-finding techniques has led to the discovery of\nmore vulnerabilities than developers can reasonably fix, creating an urgent\nneed for effective Automated Program Repair (APR) methods. However, the\ncomplexity of modern bugs often makes precise root cause analysis difficult and\nunreliable. To address this challenge, we propose crash-site repair to simplify\nthe repair task while still mitigating the risk of exploitation. In addition,\nwe introduce a template-guided patch generation approach that significantly\nreduces the token cost of Large Language Models (LLMs) while maintaining both\nefficiency and effectiveness.\n  We implement our prototype system, WILLIAMT, and evaluate it against\nstate-of-the-art APR tools. Our results show that, when combined with the\ntop-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and\nincreases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open\nsource software vulnerabilities benchmark. Furthermore, we demonstrate that\nWILLIAMT can function effectively even without access to frontier LLMs: even a\nlocal model running on a Mac M4 Mini achieves a reasonable repair rate. These\nfindings highlight the broad applicability and scalability of WILLIAMT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13103.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13010",
      "authors": [
        {
          "_id": "682c46c70f622b7afc2e6f8f",
          "user": {
            "_id": "67d5c51817d572b960f6982a",
            "avatarUrl": "/avatars/2c22663b27326b2300444bcc84ebbdd7.svg",
            "isPro": false,
            "fullname": "Himel Ghosh",
            "user": "himel7",
            "type": "user"
          },
          "name": "Himel Ghosh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T09:42:53.022Z",
          "hidden": false
        },
        {
          "_id": "682c46c70f622b7afc2e6f90",
          "name": "Ahmed Mosharafa",
          "hidden": false
        },
        {
          "_id": "682c46c70f622b7afc2e6f91",
          "name": "Georg Groh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67d5c51817d572b960f6982a/YKwZeahX2EReZyYUdKvtE.png"
      ],
      "publishedAt": "2025-05-19T11:54:39.000Z",
      "submittedOnDailyAt": "2025-05-21T08:14:27.462Z",
      "title": "¿Bias o no: Detector de Bias en Noticias",
      "submittedOnDailyBy": {
        "_id": "67d5c51817d572b960f6982a",
        "avatarUrl": "/avatars/2c22663b27326b2300444bcc84ebbdd7.svg",
        "isPro": false,
        "fullname": "Himel Ghosh",
        "user": "himel7",
        "type": "user"
      },
      "summary": "La detección de sesgo en medios es una tarea importante para garantizar la propagación de información justa y equilibrada, pero se enfrenta a desafíos debido a la subjetividad del sesgo y la escasez de datos etiquetados de alta calidad. En este estudio, utilizamos el conjunto de datos BABE etiquetado por expertos para finejar un modelo basado en RoBERTa para clasificar sesgos a nivel documento. Combinando el test de McNemar y un 5x2 cruzado, mostramos una mejora significativa en el rendimiento estadísticamente significativa comparado con líneas de código adaptadas a áreas basadas en DA-RoBERTa. Además, en el análisis basado en atención, el modelo no es excesivamente sensible a lenguaje político y asigna puntos de interés significativos a tokens relacionados con el contexto. Para validar el sesgo en medios de mayor detalle, proponemos una pipeline que combina el modelo con un clasificador de tipos de sesgo ya existente. Este método contribuye a la construcción de sistemas NLP más robustos y explicables, a pesar de las limitaciones de la escasez de corpus con gran sesgo y el tamaño del conjunto de datos. Las direcciones futuras se centran en modelos relacionados con el contexto, neutralización del sesgo y clasificación avanzada de tipos de sesgo.",
      "upvotes": 1,
      "discussionId": "682c46c80f622b7afc2e6fcf",
      "ai_keywords": [
        "RoBERTa-based model",
        "BABE dataset",
        "McNemar's test",
        "5x2 cross-validation paired t-test",
        "domain-adaptively pre-trained",
        "DA-RoBERTa baseline",
        "attention-based analysis",
        "politically charged terms",
        "contextually relevant tokens",
        "sentence-level analysis",
        "bias-type classifier",
        "context-aware modeling",
        "bias neutralization",
        "advanced bias type classification"
      ]
    },
    "publishedAt": "2025-05-19T07:54:39.000Z",
    "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector",
    "summary": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67d5c51817d572b960f6982a/YKwZeahX2EReZyYUdKvtE.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13010.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67d5c51817d572b960f6982a",
      "avatarUrl": "/avatars/2c22663b27326b2300444bcc84ebbdd7.svg",
      "fullname": "Himel Ghosh",
      "name": "himel7",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11966",
      "authors": [
        {
          "_id": "682d42808560f4baf596643b",
          "user": {
            "_id": "6608fa4f5baec84322ec85ea",
            "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
            "isPro": false,
            "fullname": "Zhong",
            "user": "Jianyuan1",
            "type": "user"
          },
          "name": "Jianyuan Zhong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T08:40:43.900Z",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf596643c",
          "name": "Zeju Li",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf596643d",
          "name": "Zhijian Xu",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf596643e",
          "name": "Xiangyu Wen",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf596643f",
          "name": "Kezhi Li",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf5966440",
          "name": "Qiang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-17T11:41:44.000Z",
      "submittedOnDailyAt": "2025-05-21T01:36:54.831Z",
      "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative Verifier",
      "submittedOnDailyBy": {
        "_id": "6608fa4f5baec84322ec85ea",
        "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
        "isPro": false,
        "fullname": "Zhong",
        "user": "Jianyuan1",
        "type": "user"
      },
      "summary": "En el contexto de los complejos desarrollos de modelos de lenguaje (LLM) para tareas lógicas, existe una relación fundamentalmente inversada entre la precisión de las soluciones y la eficiencia computacional. En la etapa posterior de verificación, el objetivo es mejorar el rendimiento, pero se mantiene una relación inversada difícil: los modelos de recompensa generativa (GenRMs) complejos son computacionalmente costosos cuando se integran de manera inocente con un LLM, pero si son simples y rápidos, su confianza puede disminuir. Para superar estos problemas, se presenta FlexiVe, un nuevo verificador generativo. FlexiVe utiliza una estrategia de \"distribución flexible del verificador\" para asignar recursos computacionales de manera flexible entre el rápido pensamiento eficiente y el lento pensamiento detallado, logrando un equilibrio entre ellos. Además, se propone un flujo de trabajo Solve-Detect-Verify. Este flujo permite configurar FlexiVe adecuadamente, específicar puntos de conclusión de la solución activamente, realizar verificaciones objetivo y proporcionar retroalimentación eficiente al modelo de solución. Los experimentos muestran que FlexiVe tiene una precisión superior en la detección de errores lógicos en ProcessBench. Además, en los difíciles marcadores matemáticos lógicos (AIME 2024, AIME 2025, CNMO), nuestro enfoque completo es superior en precisión lógica y eficiencia de inferencia. Nuestro sistema proporciona soluciones escalables y efectivas para fortalecer los aspectos lógicos del LLM durante el tiempo de prueba.",
      "upvotes": 1,
      "discussionId": "682d42808560f4baf5966480",
      "ai_keywords": [
        "Generative Reward Models (GenRMs)",
        "Flexible Allocation of Verification Budget",
        "Solve-Detect-Verify pipeline",
        "reasoning traces",
        "ProcessBench",
        "AIME 2024",
        "AIME 2025",
        "CNMO",
        "self-consistency"
      ]
    },
    "publishedAt": "2025-05-17T07:41:44.000Z",
    "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative\n  Verifier",
    "summary": "Large Language Model (LLM) reasoning for complex tasks inherently involves a\ntrade-off between solution accuracy and computational efficiency. The\nsubsequent step of verification, while intended to improve performance, further\ncomplicates this landscape by introducing its own challenging trade-off:\nsophisticated Generative Reward Models (GenRMs) can be computationally\nprohibitive if naively integrated with LLMs at test-time, while simpler, faster\nmethods may lack reliability. To overcome these challenges, we introduce\nFlexiVe, a novel generative verifier that flexibly balances computational\nresources between rapid, reliable fast thinking and meticulous slow thinking\nusing a Flexible Allocation of Verification Budget strategy. We further propose\nthe Solve-Detect-Verify pipeline, an efficient inference-time scaling framework\nthat intelligently integrates FlexiVe, proactively identifying solution\ncompletion points to trigger targeted verification and provide focused solver\nfeedback. Experiments show FlexiVe achieves superior accuracy in pinpointing\nerrors within reasoning traces on ProcessBench. Furthermore, on challenging\nmathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full\napproach outperforms baselines like self-consistency in reasoning accuracy and\ninference efficiency. Our system offers a scalable and effective solution to\nenhance LLM reasoning at test time.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11966.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6608fa4f5baec84322ec85ea",
      "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
      "fullname": "Zhong",
      "name": "Jianyuan1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14178",
      "authors": [
        {
          "_id": "682d388c57686b8c44ede70b",
          "user": {
            "_id": "656553d89bf6665f10e3a92d",
            "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
            "isPro": false,
            "fullname": "xiang wyatt zhang",
            "user": "Wyattz23",
            "type": "user"
          },
          "name": "Xiang Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T02:21:00.902Z",
          "hidden": false
        },
        {
          "_id": "682d388c57686b8c44ede70c",
          "name": "Juntai Cao",
          "hidden": false
        },
        {
          "_id": "682d388c57686b8c44ede70d",
          "name": "Jiaqi Wei",
          "hidden": false
        },
        {
          "_id": "682d388c57686b8c44ede70e",
          "name": "Yiwei Xu",
          "hidden": false
        },
        {
          "_id": "682d388c57686b8c44ede70f",
          "user": {
            "_id": "6466d463060756d2854ab3e1",
            "avatarUrl": "/avatars/4401387180c16472a6823f78aaa86d54.svg",
            "isPro": false,
            "fullname": "Chenyu You",
            "user": "Charlesyooo",
            "type": "user"
          },
          "name": "Chenyu You",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T02:30:12.849Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T10:32:30.000Z",
      "submittedOnDailyAt": "2025-05-21T00:51:16.514Z",
      "title": "Limitaciones de tokenización de LLM: Estudio sobre las limitaciones de razones simbólicas y aritméticas",
      "submittedOnDailyBy": {
        "_id": "656553d89bf6665f10e3a92d",
        "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
        "isPro": false,
        "fullname": "xiang wyatt zhang",
        "user": "Wyattz23",
        "type": "user"
      },
      "summary": "El tokenización es la más importante en los modelos de lenguaje, y representa las primeras capas de cálculos que no se reconocen bien. El programa de pensamiento en cadena (CoT) permite a los modelos de tendencias representar pasos intermedios externamente, facilitando así la aproximación de cálculos recursivos, pero demostramos que el éxito de estas inferencias está fundamentalmente limitado por la estructura de los tokens de entrada. En este artículo, se investiga teóricamente y experimentalmente cómo el esquema de tokenización, especialmente métodos subpalabra como el byte pair encoding (BPE), pueden combinar o ocultar unidades lógicas o impidir cálculos simbólicos. Se introducen conceptos sobre tokenización y se definen formalmente cómo la granificación del vocabulario puede impedir correspondencias lógicas y la generalización de procesamiento simbólico. Se evaluan sistemáticamente cálculos con puntos de corte y tareas simbólicas, mostrando que la estructura de los tokens puede significativamente afectar el rendimiento de cálculos y que también puede fallar en el CoT. Por otro lado, formatos que corresponden a átomos permiten una generalización fuerte y muestran que pequeños modelos (por ejemplo, GPT-4o-mini) pueden superar grandes sistemas (por ejemplo, o1). Lo que hemos descubierto es que la capacidad lógica simbólica de los modelos de lenguaje no está limitada por la arquitectura, sino que depende profundamente de la representación en el nivel de tokens.",
      "upvotes": 0,
      "discussionId": "682d388c57686b8c44ede753",
      "ai_keywords": [
        "Chain-of-Thought (CoT) prompting",
        "transformer models",
        "recursive computation",
        "intermediate steps",
        "tokenization schemes",
        "subword-based methods",
        "byte-pair encoding (BPE)",
        "symbolic computation",
        "Token Awareness",
        "atomic reasoning units",
        "logical alignment",
        "structured reasoning",
        "arithmetic tasks",
        "symbolic tasks",
        "reasoning performance",
        "atomically-aligned formats",
        "structured reasoning",
        "low-level representations",
        "symbolic reasoning ability",
        "LLMs",
        "token-level representations"
      ]
    },
    "publishedAt": "2025-05-20T06:32:30.000Z",
    "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\n  Reasoning Limits",
    "summary": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14178.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656553d89bf6665f10e3a92d",
      "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
      "fullname": "xiang wyatt zhang",
      "name": "Wyattz23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13138",
      "authors": [
        {
          "_id": "682da62a975206d1caa8c9d9",
          "name": "Emile van Krieken",
          "hidden": false
        },
        {
          "_id": "682da62a975206d1caa8c9da",
          "name": "Pasquale Minervini",
          "hidden": false
        },
        {
          "_id": "682da62a975206d1caa8c9db",
          "name": "Edoardo Ponti",
          "hidden": false
        },
        {
          "_id": "682da62a975206d1caa8c9dc",
          "name": "Antonio Vergari",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62cd4ae83e5ba89c40f15156/6TK9qXUQWm6Yk3bQW_0ht.gif"
      ],
      "publishedAt": "2025-05-19T14:07:47.000Z",
      "submittedOnDailyAt": "2025-05-21T08:42:10.409Z",
      "title": "NeuroShine DiPiusion Module",
      "submittedOnDailyBy": {
        "_id": "62cd4ae83e5ba89c40f15156",
        "avatarUrl": "/avatars/f70aca85f8836edaedcee324a18140b5.svg",
        "isPro": false,
        "fullname": "Emile van Krieken",
        "user": "HEmile",
        "type": "user"
      },
      "summary": "El modelo de predicción NeSy (NeuroSymbolic) integra la observación neural y el lenguaje de símbolos para resolver tareas visuales y variadas. Sin embargo, el modelo estándar de NeSy asume la independencia condicional entre los símbolos extraídos, limitando así la capacidad de modelar interacciones e incertidumbre y provocando predicciones exageradas y generalizaciones fuera de la distribución. Para superar estos límites, introducimos una nueva clase de modelos de NeSy denominados NeuroSymbolic Diffusion Models (NeSyDMs), diseñados para modelar la dependencia entre símbolos. Nuestro enfoque permite evaluar la dependencia y la incertidumbre entre símbolos, así como una aprendizaje escalable, al reutilizar la asunción de independencia en cada etapa de la difusión. Mediante datos sintéticos y marcos de referencia reales, NeSyDMs han demostrado la mayor precisión en diversas áreas, incluyendo la planificación visual y el auto-manejo de rutas basados en reglas.",
      "upvotes": 0,
      "discussionId": "682da62a975206d1caa8ca06",
      "githubRepo": "https://github.com/HEmile/neurosymbolic-diffusion"
    },
    "publishedAt": "2025-05-19T10:07:47.000Z",
    "title": "Neurosymbolic Diffusion Models",
    "summary": "Neurosymbolic (NeSy) predictors combine neural perception with symbolic\nreasoning to solve tasks like visual reasoning. However, standard NeSy\npredictors assume conditional independence between the symbols they extract,\nthus limiting their ability to model interactions and uncertainty - often\nleading to overconfident predictions and poor out-of-distribution\ngeneralisation. To overcome the limitations of the independence assumption, we\nintroduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy\npredictors that use discrete diffusion to model dependencies between symbols.\nOur approach reuses the independence assumption from NeSy predictors at each\nstep of the diffusion process, enabling scalable learning while capturing\nsymbol dependencies and uncertainty quantification. Across both synthetic and\nreal-world benchmarks - including high-dimensional visual path planning and\nrule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among\nNeSy predictors and demonstrate strong calibration.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62cd4ae83e5ba89c40f15156/6TK9qXUQWm6Yk3bQW_0ht.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13138.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62cd4ae83e5ba89c40f15156",
      "avatarUrl": "/avatars/f70aca85f8836edaedcee324a18140b5.svg",
      "fullname": "Emile van Krieken",
      "name": "HEmile",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.12306",
      "authors": [
        {
          "_id": "682d2f9f3b5f51f42185dd7d",
          "name": "Yuwei Zhang",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd7e",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd7f",
          "name": "Shangbin Feng",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd80",
          "name": "Yifan Zhu",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd81",
          "name": "Letian Peng",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd82",
          "name": "Jayanth Srinivasa",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd83",
          "name": "Gaowen Liu",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd84",
          "name": "Jingbo Shang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T08:39:05.000Z",
      "submittedOnDailyAt": "2025-05-21T00:13:17.201Z",
      "title": "Los modelos de memoria bidireccional son los guardianes de la memoria de conocimiento? Benchmark de la inyección de conocimiento en la realidad.",
      "submittedOnDailyBy": {
        "_id": "64323dd503d81fa4d26deaf9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png",
        "isPro": false,
        "fullname": "Letian Peng",
        "user": "KomeijiForce",
        "type": "user"
      },
      "summary": "Los grandes modelos de lenguaje (LLMs) han experimentado un claro progreso, sin embargo, su capacidad de memoria de conocimientos no está normalizada y carece de un ambiente de prueba de alta calidad, lo que limita la investigación. En este artículo, se presenta un nuevo y realista marco de referencia para la introducción de grandes cantidades de conocimientos sin intervención humana, que evoluciona con el tiempo. Específicamente, se propone WikiDYK, que se basa en \"Did You Know...\" de Wikipedia, utilizando hechos recientemente agregados y escritos por humanos. Estos hechos son seleccionados cuidadosamente por editores expertos de Wikipedia, según criterios de validación y claridad. Cada hecho se convierte en pares de preguntas y respuestas variados, desde simples clozePrompt hasta complejos multi-tarea formatos. WikiDYK incluye 12,290 hechos y 77,180 preguntas, y puede expandirse fácilmente con futuras actualizaciones de los editores de Wikipedia. A través de procesamientos a largo plazo, se obtuvieron resultados impresionantes: los modelos de lenguaje causal (CLMs) ampliamente utilizados tienen una capacidad de memoria de conocimientos significativamente inferior a los modelos de lenguaje simétrico (BiLMs), con un error de confianza del 23% menor. Para complementar la escasez de escala de los BiLMs, se propone un marco cooperativo modular que integra con bases de conocimientos externas y LLMs. Las pruebas muestran que este marco puede mejorar la precisión de confianza en un 29.1% aproximadamente.",
      "upvotes": 0,
      "discussionId": "682d2fa03b5f51f42185ddb4",
      "ai_keywords": [
        "large language models (LLMs)",
        "knowledge memorization capabilities",
        "WikiDYK",
        "\"Did You Know...\" entries",
        "question-answer pairs",
        "cloze prompts",
        "multi-hop questions",
        "continued pre-training",
        "Causal Language Models (CLMs)",
        "Bidirectional Language Models (BiLMs)",
        "modular collaborative framework",
        "ensembles of BiLMs",
        "external knowledge repositories",
        "reliability accuracy"
      ]
    },
    "publishedAt": "2025-05-18T04:39:05.000Z",
    "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for\n  Real-world Knowledge Injection",
    "summary": "Despite significant advances in large language models (LLMs), their knowledge\nmemorization capabilities remain underexplored, due to the lack of standardized\nand high-quality test ground. In this paper, we introduce a novel, real-world\nand large-scale knowledge injection benchmark that evolves continuously over\ntime without requiring human intervention. Specifically, we propose WikiDYK,\nwhich leverages recently-added and human-written facts from Wikipedia's \"Did\nYou Know...\" entries. These entries are carefully selected by expert Wikipedia\neditors based on criteria such as verifiability and clarity. Each entry is\nconverted into multiple question-answer pairs spanning diverse task formats\nfrom easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290\nfacts and 77,180 questions, which is also seamlessly extensible with future\nupdates from Wikipedia editors. Extensive experiments using continued\npre-training reveal a surprising insight: despite their prevalence in modern\nLLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge\nmemorization capabilities compared to Bidirectional Language Models (BiLMs),\nexhibiting a 23% lower accuracy in terms of reliability. To compensate for the\nsmaller scales of current BiLMs, we introduce a modular collaborative framework\nutilizing ensembles of BiLMs as external knowledge repositories to integrate\nwith LLMs. Experiment shows that our framework further improves the reliability\naccuracy by up to 29.1%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12306.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64323dd503d81fa4d26deaf9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png",
      "fullname": "Letian Peng",
      "name": "KomeijiForce",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10588",
      "authors": [
        {
          "_id": "682d2dd917608739046ce410",
          "name": "Manisha Mehta",
          "hidden": false
        },
        {
          "_id": "682d2dd917608739046ce411",
          "name": "Fausto Giunchiglia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T16:46:11.000Z",
      "submittedOnDailyAt": "2025-05-21T00:06:34.563Z",
      "title": "Comprensión del lenguaje digital general: evaluación de sistemas de modelado de lenguaje basados en modelos de aprendizaje profundo sobre la seguridad de la modalidad de contenido",
      "submittedOnDailyBy": {
        "_id": "604eb19e3050a33ebb17ef58",
        "avatarUrl": "/avatars/23650002ba3befee83060fe978a251c8.svg",
        "isPro": false,
        "fullname": "Virendra Mehta",
        "user": "Veeru",
        "type": "user"
      },
      "summary": "Este estudio ofrece una evaluación única sobre cómo los sistemas de IA interpretan el lenguaje digital de la generación Alpha (Gen Alpha, 2010-2024). Gen Alpha es la primera generación que ha crecido junto con la IA, y enfrenta nuevos riesgos en línea desde la expansión de la asimetría entre la participación digital satisfactoria y los nuevos métodos de comunicación y las herramientas de seguridad actuales. Su lenguaje es particularmente formado por juegos, redes sociales y tendencias dirigidas por la IA, y puede ocultar interacciones perjudiciales con modelos personales y sistemas automatizados. Este estudio evalua la capacidad de cuatro modelos de IA pioneros (GPT-4, Claude, Gemini, Llama 3) para detectar el acoso y manipulaciones ocultas en el lenguaje de Gen Alpha. Utilizando 100 expresiones más recientes en plataformas de juegos, redes sociales y contenidos de vídeo, el estudio revela un fracaso crítico en la comprensión de los riesgos en línea. Contribuye con: (1) el primer dataset de expresiones de Gen Alpha; (2) un marco para mejorar los sistemas de modelado de IA para la protección de adolescentes; (3) una evaluación multidimensional que incluye la interacción directa de los sistemas de IA, los modelos personales y los padres; (4) el análisis de cómo la distancia lingüística puede aumentar la vulnerabilidad de los adolescentes. Estos hallazgos subrayan la urgente necesidad de redesenar sistemas de seguridad en línea que se ajusten a la comunicación de los adolescentes. En particular, el estudio registra que Gen Alpha solicita ayuda a sus padres al no entender su mundo digital. Combina las observaciones de investigadores de Gen Alpha y un análisis académico sistemático para abordar los desafíos de seguridad digital.",
      "upvotes": 0,
      "discussionId": "682d2dd917608739046ce440"
    },
    "publishedAt": "2025-05-14T12:46:11.000Z",
    "title": "Understanding Gen Alpha Digital Language: Evaluation of LLM Safety\n  Systems for Content Moderation",
    "summary": "This research offers a unique evaluation of how AI systems interpret the\ndigital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first\ncohort raised alongside AI, Gen Alpha faces new forms of online risk due to\nimmersive digital engagement and a growing mismatch between their evolving\ncommunication and existing safety tools. Their distinct language, shaped by\ngaming, memes, and AI-driven trends, often conceals harmful interactions from\nboth human moderators and automated systems. We assess four leading AI models\n(GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked\nharassment and manipulation within Gen Alpha discourse. Using a dataset of 100\nrecent expressions from gaming platforms, social media, and video content, the\nstudy reveals critical comprehension failures with direct implications for\nonline safety. This work contributes: (1) a first-of-its-kind dataset capturing\nGen Alpha expressions; (2) a framework to improve AI moderation systems for\nyouth protection; (3) a multi-perspective evaluation including AI systems,\nhuman moderators, and parents, with direct input from Gen Alpha co-researchers;\nand (4) an analysis of how linguistic divergence increases youth vulnerability.\nFindings highlight the urgent need to redesign safety systems attuned to youth\ncommunication, especially given Gen Alpha reluctance to seek help when adults\nfail to understand their digital world. This study combines the insight of a\nGen Alpha researcher with systematic academic analysis to address critical\ndigital safety challenges.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10588.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "604eb19e3050a33ebb17ef58",
      "avatarUrl": "/avatars/23650002ba3befee83060fe978a251c8.svg",
      "fullname": "Virendra Mehta",
      "name": "Veeru",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]