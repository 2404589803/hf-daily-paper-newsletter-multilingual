[
  {
    "paper": {
      "id": "2506.01939",
      "authors": [
        {
          "_id": "683e7a6d97fd742a8edee1ba",
          "user": {
            "_id": "6486dde1f74857df3f1a5828",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
            "isPro": false,
            "fullname": "Shenzhi Wang",
            "user": "shenzhi-wang",
            "type": "user"
          },
          "name": "Shenzhi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:16.481Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1bb",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1bc",
          "name": "Chang Gao",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1bd",
          "user": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "isPro": false,
            "fullname": "Chujie Zheng",
            "user": "chujiezheng",
            "type": "user"
          },
          "name": "Chujie Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:13.605Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1be",
          "name": "Shixuan Liu",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1bf",
          "name": "Rui Lu",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c0",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c1",
          "user": {
            "_id": "63f30b870a16587ea970edfe",
            "avatarUrl": "/avatars/059491b33fecec69032e6d481229ee31.svg",
            "isPro": false,
            "fullname": "Xiong-Hui Chen",
            "user": "xionghuichen",
            "type": "user"
          },
          "name": "Xionghui Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:10.358Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c2",
          "name": "Jianxin Yang",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c3",
          "user": {
            "_id": "64704e973601bb7b06643e98",
            "avatarUrl": "/avatars/52e51f4d1be6769e4397b8be2799cf32.svg",
            "isPro": false,
            "fullname": "Zhenru Zhang",
            "user": "Zhenru",
            "type": "user"
          },
          "name": "Zhenru Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:40:21.027Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c4",
          "user": {
            "_id": "666aacfb918ba11c7c598194",
            "avatarUrl": "/avatars/45bee8f1fdbdd256ee47d25e4bf01a7a.svg",
            "isPro": false,
            "fullname": "Yuqiong Liu",
            "user": "lyq333",
            "type": "user"
          },
          "name": "Yuqiong Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:40:30.240Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c5",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c6",
          "name": "Andrew Zhao",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c7",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c8",
          "name": "Shiji Song",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c9",
          "user": {
            "_id": "63d9d68c1cae35c27bf7a6a7",
            "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
            "isPro": false,
            "fullname": "Bowen Yu",
            "user": "Tigerph",
            "type": "user"
          },
          "name": "Bowen Yu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T04:30:38.648Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1ca",
          "name": "Gao Huang",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1cb",
          "user": {
            "_id": "620760a26e3b7210c2ff1943",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
            "isPro": false,
            "fullname": "Junyang Lin",
            "user": "JustinLin610",
            "type": "user"
          },
          "name": "Junyang Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:39:54.278Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:54:39.000Z",
      "submittedOnDailyAt": "2025-06-03T03:09:30.655Z",
      "title": "Más allá de la ley del 80/20: los pocos tokens de alta entropía impulsan el entrenamiento de lógica efectiva en LLMs.",
      "submittedOnDailyBy": {
        "_id": "6486dde1f74857df3f1a5828",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
        "isPro": false,
        "fullname": "Shenzhi Wang",
        "user": "shenzhi-wang",
        "type": "user"
      },
      "summary": "RLVR (Rewards with Verification) es un potente método de aprendizaje reforzado para mejorar la capacidad lógica de grandes modelos de lenguaje (LLMs), aunque su estructura aún no se ha entendido completamente. En este estudio, se investiga RLVR desde una nueva perspectiva de patrones de entropía de tokens y analiza en detalle cómo afectan diferentes tokens a la capacidad lógica. Se revisa el patrón de entropía de tokens en la lógica de cadena de pensamiento (Chain-of-Thought, CoT), y se encuentra que los tokens con alta entropía son muy raros, y que estos tokens son puntos de división cruciales que llevan a diversas rutas lógicas. Además, a través de la investigación sobre la evolución de los patrones de entropía durante el entrenamiento de RLVR, se descubre que RLVR principalmente sigue los patrones de entropía del modelo base y se puede ajustar la entropía de los tokens con alta entropía. Estos hallazgos revelan la importancia de los tokens con alta entropía (es decir, tokens de división) en RLVR. Finalmente, al limitar la actualización de la política de gradiente a los tokens de división y al utilizar solo el 20% de los tokens para lograr un rendimiento relativamente o excesivamente superior en Qwen3-8B (AIME'25: +11.04, AIME'24: +7.71) y Qwen3-14B (AIME'25: +4.79, AIME'24: +5.21), se confirma que se puede superar significativamente el rendimiento del modelo base. Estos hallazgos muestran que el efecto de RLVR se basa en la optimización de los tokens con alta entropía que determinan la dirección lógica, y que se puede entender RLVR a través de la perspectiva de la entropía de tokens, y que se pueden desarrollar las capacidades lógicas de los LLMs utilizando los pocos tokens con alta entropía.",
      "upvotes": 66,
      "discussionId": "683e7a6e97fd742a8edee227",
      "projectPage": "https://shenzhi-wang.github.io/high-entropy-minority-tokens-rlvr/",
      "ai_summary": "Token entropy patterns are crucial in RLVR, with high-entropy tokens significantly impacting reasoning performance and model optimization.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "RLVR",
        "Large Language Models",
        "LLMs",
        "token entropy patterns",
        "Chain-of-Thought",
        "CoT reasoning",
        "high-entropy tokens",
        "policy gradient updates",
        "Qwen3-8B",
        "Qwen3-32B",
        "Qwen3-14B",
        "AIME"
      ]
    },
    "publishedAt": "2025-06-02T13:54:39.000Z",
    "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective\n  Reinforcement Learning for LLM Reasoning",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful approach to enhancing the reasoning capabilities of Large Language\nModels (LLMs), while its mechanisms are not yet well understood. In this work,\nwe undertake a pioneering exploration of RLVR through the novel perspective of\ntoken entropy patterns, comprehensively analyzing how different tokens\ninfluence reasoning performance. By examining token entropy patterns in\nChain-of-Thought (CoT) reasoning, we observe that only a small fraction of\ntokens exhibit high entropy, and these tokens act as critical forks that steer\nthe model toward diverse reasoning pathways. Furthermore, studying how entropy\npatterns evolve during RLVR training reveals that RLVR largely adheres to the\nbase model's entropy patterns, primarily adjusting the entropy of high-entropy\ntokens. These findings highlight the significance of high-entropy tokens (i.e.,\nforking tokens) to RLVR. We ultimately improve RLVR by restricting policy\ngradient updates to forking tokens and uncover a finding even beyond the 80/20\nrule: utilizing only 20% of the tokens while maintaining performance comparable\nto full-gradient updates on the Qwen3-8B base model and significantly\nsurpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71\non AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,\nhighlighting a strong scaling trend. In contrast, training exclusively on the\n80% lowest-entropy tokens leads to a marked decline in performance. These\nfindings indicate that the efficacy of RLVR primarily arises from optimizing\nthe high-entropy tokens that decide reasoning directions. Collectively, our\nresults highlight the potential to understand RLVR through a token-entropy\nperspective and optimize RLVR by leveraging high-entropy minority tokens to\nfurther improve LLM reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01939.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6486dde1f74857df3f1a5828",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
      "fullname": "Shenzhi Wang",
      "name": "shenzhi-wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 326
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01049",
      "authors": [
        {
          "_id": "683e5b9a1167d9630159b27f",
          "user": {
            "_id": "640f7083208821a59b74c757",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678735253848-640f7083208821a59b74c757.jpeg",
            "isPro": false,
            "fullname": "Siyuan Li",
            "user": "Lupin1998",
            "type": "user"
          },
          "name": "Siyuan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:39.296Z",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b280",
          "user": {
            "_id": "670880950e79a8b46f7ff9dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
            "isPro": false,
            "fullname": "Juanxi Tian",
            "user": "Juanxi",
            "type": "user"
          },
          "name": "Juanxi Tian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:40:52.165Z",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b281",
          "user": {
            "_id": "6594d390674349122ce6f368",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/KdWz6lZyGYQpjAgBDeiC1.jpeg",
            "isPro": false,
            "fullname": "Zedong Wang (Jacky)",
            "user": "ZedongWangAI",
            "type": "user"
          },
          "name": "Zedong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:41.911Z",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b282",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b283",
          "user": {
            "_id": "67ee7eef2a8e2fd1445407ab",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/TelMtjU8Ki8ulQU4-b0He.jpeg",
            "isPro": false,
            "fullname": "Zicheng Liu",
            "user": "MarcusB3n",
            "type": "user"
          },
          "name": "Zicheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:41:29.426Z",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b284",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b285",
          "user": {
            "_id": "67cd1d6c96e0a33b99c78b26",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wsUk7e5BPHa6L7GroWtat.png",
            "isPro": false,
            "fullname": "Dan Xu",
            "user": "danxu",
            "type": "user"
          },
          "name": "Dan Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:41:02.065Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T15:30:37.000Z",
      "submittedOnDailyAt": "2025-06-03T00:52:33.852Z",
      "title": "Escalado de velocidad de aprendizaje para agrupamiento de gradientes en control de Erotics LLM",
      "submittedOnDailyBy": {
        "_id": "6594d390674349122ce6f368",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/KdWz6lZyGYQpjAgBDeiC1.jpeg",
        "isPro": false,
        "fullname": "Zedong Wang (Jacky)",
        "user": "ZedongWangAI",
        "type": "user"
      },
      "summary": "El entrenamiento de modelos de lenguaje grandes (LLMs) enfrenta varios problemas que dependen de su tamaño y arquitectura. Algoritmos de optimización adaptativos como AdamW pueden resolver problemas de gradiente, pero evalúar apropiadamente el aprendizaje para cada parámetro es difícil, lo que genera inestabilidad en el entrenamiento, lentitud de convergencia y incompatibilidad con métodos de ajuste de parámetros eficientes (PEFT). En este artículo, se introduce una mejora en los aprendizajes adaptativos con la Estructura de Gradiente de Grupos (SGG). La SGG utiliza un grupo dinámico y escalado por grupo para agrupar las estadísticas de gradiente de cada capa y ajusta el aprendizaje para cada parámetro. La SGG no considera las restricciones de todo el grupo, sino que se centra en mantener la reacción adecuada de cada parámetro. Los experimentos en diferentes (M) marcos de evaluación de LLMs muestran que la SGG se integra con los algoritmos de optimización actuales, demostrando un efecto consistente y rápido de convergencia frente a los baselines, y puede ser implementada en diferentes tamaños de modelo. La SGG supera la inestabilidad causada por cambios en el tamaño de la muestra y el aprendizaje, y se convierte en una opción potente para la optimización de LLMs.",
      "upvotes": 22,
      "discussionId": "683e5b9b1167d9630159b2ef",
      "ai_summary": "SGG, an optimizer wrapper, enhances adaptive learning rates for large language models by grouping gradients and applying cluster-specific scaling, improving convergence and stability.",
      "ai_keywords": [
        "large language models",
        "adaptive optimizers",
        "AdamW",
        "parameter-wise learning rate estimation",
        "training instability",
        "parameter-efficient fine-tuning",
        "Scaling with Gradient Grouping",
        "gradient grouping",
        "cluster-specific scaling",
        "LLM benchmarks",
        "robust choice for LLM optimization"
      ]
    },
    "publishedAt": "2025-06-01T11:30:37.000Z",
    "title": "Taming LLMs by Scaling Learning Rates with Gradient Grouping",
    "summary": "Training large language models (LLMs) poses challenges due to their massive\nscale and heterogeneous architectures. While adaptive optimizers like AdamW\nhelp address gradient variations, they still struggle with efficient and\neffective parameter-wise learning rate estimation, resulting in training\ninstability, slow convergence, and poor compatibility with parameter-efficient\nfine-tuning (PEFT) techniques. This work introduces Scaling with Gradient\nGrouping (SGG), an optimizer wrapper that improves adaptive learning rate\nestimation by dynamic grouping and group-specific scaling. SGG first groups\ngradient statistics in each layer into clusters and then applies\ncluster-specific scaling to calibrate learning rates for each parameter, thus\nimposing collective group-wise constraints while maintaining precise\nper-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that\nSGG integrates seamlessly with existing optimizers, and offers consistent gains\nand faster convergence over baselines, with various model sizes. Its stability\nacross varying batch sizes and learning rates establishes SGG as a robust\nchoice for LLM optimization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01049.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6594d390674349122ce6f368",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/KdWz6lZyGYQpjAgBDeiC1.jpeg",
      "fullname": "Zedong Wang (Jacky)",
      "name": "ZedongWangAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23590",
      "authors": [
        {
          "_id": "683e6b2d97fd742a8edb8a8e",
          "user": {
            "_id": "64f5c7cb65a4b1acb20ffc15",
            "avatarUrl": "/avatars/67c42a6e26bbeddc4bf706eb8f1a75b1.svg",
            "isPro": false,
            "fullname": "Zifu Wang",
            "user": "wangzifu",
            "type": "user"
          },
          "name": "Zifu Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:41:52.137Z",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a8f",
          "user": {
            "_id": "6477b2038ab7e732b6d8a9b5",
            "avatarUrl": "/avatars/c859a35b8965a904f103bbc34f36ab2a.svg",
            "isPro": false,
            "fullname": "Junyi Zhu",
            "user": "RyanZhu",
            "type": "user"
          },
          "name": "Junyi Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:42:00.703Z",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a90",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a91",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a92",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a93",
          "name": "Jiaqian Yu",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a94",
          "name": "Matthew B. Blaschko",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:01:22.000Z",
      "submittedOnDailyAt": "2025-06-03T01:56:17.990Z",
      "title": "Jigsaw-R1: Investigación sobre el aprendizaje de visualización basada en reglas mediante puzzles de Jigsaw",
      "submittedOnDailyBy": {
        "_id": "64f5c7cb65a4b1acb20ffc15",
        "avatarUrl": "/avatars/67c42a6e26bbeddc4bf706eb8f1a75b1.svg",
        "isPro": false,
        "fullname": "Zifu Wang",
        "user": "wangzifu",
        "type": "user"
      },
      "summary": "La aplicación del aprendizaje por refuerzo basado en reglas (RL) a modelos de lenguaje multilingüe (MLLMs) ha explorado la posibilidad de especificar métodos de búsqueda y características en campos donde solo existen documentos. En particular, se han encontrado desafíos visuales. Este artículo utiliza un estudio estructurado con un rompecabezas de cámel para realizar una investigación concreta sobre el RL. El rompecabezas de cámel fue seleccionado porque requiere respuestas internas, ajuste de dificultad y juicios complejos, lo que lo hace ideal para este estudio. En este trabajo se han descubierto los siguientes puntos:\n\n1. Los MLLMs muestran un rendimiento similar al predicción aleatoria en los rompecabezas más simples, pero alcanzan una precisión casi completa en el entrenamiento final y pueden generalizar a nuevas configuraciones complejas.\n2. El aprendizaje en rompecabezas de cámel se puede generalizar a otras tareas visuales, y este efecto puede variar según la configuración de la tarea.\n3. Los MLLMs aprenden con razones explícitas y pueden generalizar, pero muchos modelos abiertos prefieren respuestas directas. Por lo tanto, al aprender razones en pasos, pueden ignorar las razones al final de la respuesta.\n4. Patrones de razones complejos son comunes y su frecuencia aumenta con el nivel de entrenamiento y la dificultad de la tarea.\n5. Finalmente, el RL muestra una generalización más efectiva que la retroalimentación de subvídeo (SFT) en tareas visuales, y el inicio de la SFT puede afectar la optimización posterior del RL.\n\nEstos hallazgos, basados en rompecabezas de cámel, pueden dar resultados diferentes en otras tareas visuales, pero contribuyen a la comprensión del RL y a la posibilidad de entrenamiento de múltiples modelos. El código está disponible en https://github.com/zifuwanggg/Jigsaw-R1.",
      "upvotes": 20,
      "discussionId": "683e6b2e97fd742a8edb8ac1",
      "githubRepo": "https://github.com/zifuwanggg/Jigsaw-R1",
      "ai_summary": "Rule-based reinforcement learning applied to multimodal large language models demonstrates effective generalization in visual tasks, particularly using jigsaw puzzles, outperforming supervised fine-tuning.",
      "ai_keywords": [
        "rule-based reinforcement learning",
        "multimodal large language models",
        "visual RL",
        "jigsaw puzzles",
        "fine-tuning",
        "supervised fine-tuning",
        "complex decision-making",
        "visual tasks",
        "step-by-step reasoning",
        "generalization"
      ]
    },
    "publishedAt": "2025-05-29T12:01:22.000Z",
    "title": "Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with\n  Jigsaw Puzzles",
    "summary": "The application of rule-based reinforcement learning (RL) to multimodal large\nlanguage models (MLLMs) introduces unique challenges and potential deviations\nfrom findings in text-only domains, particularly for perception-heavy tasks.\nThis paper provides a comprehensive study of rule-based visual RL, using jigsaw\npuzzles as a structured experimental framework. Jigsaw puzzles offer inherent\nground truth, adjustable difficulty, and demand complex decision-making, making\nthem ideal for this study. Our research reveals several key findings:\nFirstly, we find that MLLMs, initially performing near to random\nguessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and\ngeneralize to complex, unseen configurations through fine-tuning.\nSecondly, training on jigsaw puzzles can induce generalization to\nother visual tasks, with effectiveness tied to specific task configurations.\nThirdly, MLLMs can learn and generalize with or without explicit\nreasoning, though open-source models often favor direct answering.\nConsequently, even when trained for step-by-step reasoning, they can ignore the\nthinking process in deriving the final answer. Fourthly, we observe\nthat complex reasoning patterns appear to be pre-existing rather than emergent,\nwith their frequency increasing alongside training and task difficulty.\nFinally, our results demonstrate that RL exhibits more effective\ngeneralization than Supervised Fine-Tuning (SFT), and an initial SFT cold start\nphase can hinder subsequent RL optimization. Although these observations are\nbased on jigsaw puzzles and may vary across other visual tasks, this research\ncontributes a valuable piece of jigsaw to the larger puzzle of collective\nunderstanding rule-based visual RL and its potential in multimodal learning.\nThe code is available at: https://github.com/zifuwanggg/Jigsaw-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f5c7cb65a4b1acb20ffc15",
      "avatarUrl": "/avatars/67c42a6e26bbeddc4bf706eb8f1a75b1.svg",
      "fullname": "Zifu Wang",
      "name": "wangzifu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00539",
      "authors": [
        {
          "_id": "683e76c17a0996f979e72700",
          "user": {
            "_id": "671a4abbef737c0abe21b3f8",
            "avatarUrl": "/avatars/da826af5472a3b9f1969f0c766672731.svg",
            "isPro": false,
            "fullname": "Ruihan Yang",
            "user": "rhyang2021",
            "type": "user"
          },
          "name": "Ruihan Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:32.053Z",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72701",
          "name": "Yikai Zhang",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72702",
          "user": {
            "_id": "63f86b099f87cc3e645b51d9",
            "avatarUrl": "/avatars/27ca5ba425640bf67474cee871e8e53a.svg",
            "isPro": false,
            "fullname": "Ellie Chen",
            "user": "sheep33333",
            "type": "user"
          },
          "name": "Aili Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:28.422Z",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72703",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72704",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72705",
          "name": "Jiangjie Chen",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72706",
          "name": "Deqing Yang",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72707",
          "name": "Yanghua Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T12:54:49.000Z",
      "submittedOnDailyAt": "2025-06-03T02:50:22.439Z",
      "title": "ARIA: Entrenamiento del lenguaje vocabulario de ARIA para centrar la recompensa según el objetivo",
      "submittedOnDailyBy": {
        "_id": "671a4abbef737c0abe21b3f8",
        "avatarUrl": "/avatars/da826af5472a3b9f1969f0c766672731.svg",
        "isPro": false,
        "fullname": "Ruihan Yang",
        "user": "rhyang2021",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje abiertos (LLMs) pueden realizar teorías complejas y tomar decisiones en entornos de acciones de lenguaje abierto que representan la distribución de las combinaciones de palabras, donde el espacio de acciones tiene una gran cantidad exponencial. Sin embargo, en entornos de acciones abiertos como el diálogo o juegos de preguntas, el espacio de acciones se vuelve exponencialmente grande al representar la distribución de las combinaciones de palabras. En estos espacios, sampling de acciones lleva a una extrema raridad de recompensas, lo que aumenta la varianza de la recompensa y dificulta el aprendizaje de recompensa reforzada (RL) efectivo. En este sentido, proponemos un método para concentrar la recompensa. Este método permite entrenar salidas de lenguaje eficientes y efectivas. ARIA proyecta acciones naturales de palabras de alto dimensionalidad en un espacio de intenciones de bajo dimensionalidad, agrupa acciones semánticamente similares y asigna recompensas compartidas. La concentración de la recompensa en las intenciones reduce la varianza de la recompensa y mejora la optimización de políticas, promoviendo un mejor aprendizaje. Las pruebas rigurosas demuestran que ARIA mejora en un promedio del 9.95% en cuatro tareas posteriores, y coincide con líneas de RL en línea y fuera de línea, demostrando su excelencia.",
      "upvotes": 19,
      "discussionId": "683e76ca7a0996f979e728e0",
      "projectPage": "https://aria-agent.github.io/",
      "githubRepo": "https://github.com/rhyang2021/ARIA",
      "ai_summary": "ARIA aggregates rewards in an intention space to mitigate reward sparsity and improve policy optimization in language-based reinforcement learning tasks.",
      "ai_keywords": [
        "large language models",
        "reinforcement learning",
        "action space",
        "token distribution",
        "extreme reward sparsity",
        "reward variance",
        "policy optimization",
        "intention space",
        "semantically similar actions",
        "shared rewards",
        "policy gradient variance",
        "performance gains",
        "offline RL",
        "online RL"
      ]
    },
    "publishedAt": "2025-05-31T08:54:49.000Z",
    "title": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation",
    "summary": "Large language models (LLMs) have enabled agents to perform complex reasoning\nand decision-making through free-form language interactions. However, in\nopen-ended language action environments (e.g., negotiation or question-asking\ngames), the action space can be formulated as a joint distribution over tokens,\nresulting in an exponentially large action space. Sampling actions in such a\nspace can lead to extreme reward sparsity, which brings large reward variance,\nhindering effective reinforcement learning (RL). To address this, we propose\nARIA, a method that Aggregates Rewards in Intention space to enable efficient\nand effective language Agents training. ARIA aims to project natural language\nactions from the high-dimensional joint token distribution space into a\nlow-dimensional intention space, where semantically similar actions are\nclustered and assigned shared rewards. This intention-aware reward aggregation\nreduces reward variance by densifying reward signals, fostering better policy\noptimization. Extensive experiments demonstrate that ARIA not only\nsignificantly reduces policy gradient variance, but also delivers substantial\nperformance gains of an average of 9.95% across four downstream tasks,\nconsistently outperforming offline and online RL baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00539.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671a4abbef737c0abe21b3f8",
      "avatarUrl": "/avatars/da826af5472a3b9f1969f0c766672731.svg",
      "fullname": "Ruihan Yang",
      "name": "rhyang2021",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00411",
      "authors": [
        {
          "_id": "683e86e31bca54bb6d169fc4",
          "user": {
            "_id": "6707eaaf5f50b17754ff9cbc",
            "avatarUrl": "/avatars/f08ca6228b124a8955787d0662a52bbd.svg",
            "isPro": false,
            "fullname": "Yang",
            "user": "Yysrc",
            "type": "user"
          },
          "name": "Yi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:40:09.713Z",
          "hidden": false
        },
        {
          "_id": "683e86e31bca54bb6d169fc5",
          "name": "Jiaxuan Sun",
          "hidden": false
        },
        {
          "_id": "683e86e31bca54bb6d169fc6",
          "name": "Siqi Kou",
          "hidden": false
        },
        {
          "_id": "683e86e31bca54bb6d169fc7",
          "name": "Yihan Wang",
          "hidden": false
        },
        {
          "_id": "683e86e31bca54bb6d169fc8",
          "name": "Zhijie Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T06:01:03.000Z",
      "submittedOnDailyAt": "2025-06-03T03:54:29.259Z",
      "title": "LoHoVLA: Modelo de Lenguaje de Visión y Acciones a Largo Plazo",
      "submittedOnDailyBy": {
        "_id": "654e330f350abceb30a1390b",
        "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg",
        "isPro": false,
        "fullname": "KouSiqi",
        "user": "karrykkk",
        "type": "user"
      },
      "summary": "El modelo de visualización de la realidad en la IA se enfrenta a tareas a largo plazo que exigen soluciones multiniveles con altos niveles de metas. Para realizar estas tareas con éxito, es necesario tener tanto un alto nivel de planificación de tareas (es decir, dividir metas en etapas) como un alto nivel de control de acciones (es decir, generar movimientos del robot con precisión). Ambos niveles son necesarios y deben funcionar de manera coordinada. Actualmente, los modelos de lenguaje de visión y acción (VLA) y sus estructuras jerárquicas ofrecen potenciales para tareas visuales, pero el primer modelo tiene dificultades en la planificación y el segundo en problemas de colaboración, lo que impide mejorar el rendimiento.\n\nPresentamos un nuevo marco integrado para tareas a largo plazo, llamado LoHoVLA (Long-Horizon Vision Language Action). LoHoVLA utiliza un modelo de lenguaje de visión y acción previamente entrenadido a gran escala como base y genera tokens de lenguaje y acción para la generación de tareas y predicciones de acciones del robot en etapas. Esta representación compartida fomenta una mejor generalización entre tareas. Además, LoHoVLA introduce una estructura de control de retroalimentación para reducir errores en la planificación alta y el control bajo. Para el entrenamiento de LoHoVLA, introducimos el conjunto de datos LoHoSet basado en el simulador Ravens, que incluye 20 tareas a largo plazo y 1,000 demostraciones de expertos, con observaciones visuales, metas lingüísticas, tareas en etapas y acciones del robot. Los resultados de los experimentos muestran que LoHoVLA supera significativamente las aproximaciones jerárquicas y estándares de VLA en tareas visuales a largo plazo, lo que destaca la posibilidad de un marco integrado para el desarrollo de una amplia inteligencia visual.",
      "upvotes": 19,
      "discussionId": "683e86e31bca54bb6d169ff5",
      "ai_summary": "A unified vision language action framework, LoHoVLA, combines a large pretrained vision language model with hierarchical closed-loop control to improve performance on long-horizon embodied tasks.",
      "ai_keywords": [
        "vision language action models",
        "hierarchical architectures",
        "high-level task planning",
        "low-level motion control",
        "LoHoVLA",
        "large pretrained vision language model",
        "shared representation",
        "hierarchical closed-loop control",
        "LoHoSet",
        "Ravens simulator",
        "long-horizon tasks",
        "sub-task generation",
        "robot action prediction",
        "embodied intelligence"
      ]
    },
    "publishedAt": "2025-05-31T02:01:03.000Z",
    "title": "LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon\n  Embodied Tasks",
    "summary": "Real-world embodied agents face long-horizon tasks, characterized by\nhigh-level goals demanding multi-step solutions beyond single actions.\nSuccessfully navigating these requires both high-level task planning (i.e.,\ndecomposing goals into sub-tasks) and low-level motion control (i.e.,\ngenerating precise robot actions). While existing vision language action (VLA)\nmodels and hierarchical architectures offer potential in embodied tasks, the\nformer often falter in planning, and the latter can suffer from coordination\nissues, both hampering performance. We introduce a new unified VLA framework\nfor long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA\nleverages a large pretrained vision language model (VLM) as the backbone to\njointly generate language and action tokens for sub-task generation and robot\naction prediction, respectively. This shared representation promotes better\ngeneralization across tasks. Additionally, LoHoVLA embraces a hierarchical\nclosed-loop control mechanism to mitigate errors originating from both\nhigh-level planning and low-level control. To train LoHoVLA, we introduce\nLoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon\ntasks, each with 1,000 expert demonstrations composed of visual observations,\nlinguistic goals, sub-tasks, and robot actions. Experimental results show that\nLoHoVLA significantly surpasses both hierarchical and standard VLA approaches\non long-horizon embodied tasks in the Ravens simulator. These findings\nunderscore the promise of unified architectures for advancing generalizable\nembodied intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654e330f350abceb30a1390b",
      "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg",
      "fullname": "KouSiqi",
      "name": "karrykkk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01844",
      "authors": [
        {
          "_id": "683eb85825fcc99d2a7fc26d",
          "user": {
            "_id": "62bdeedd01dc22b4d22a371e",
            "avatarUrl": "/avatars/6adc904fb1e08661d293a966270afabb.svg",
            "isPro": false,
            "fullname": "Mustafa Shukor",
            "user": "mshukor",
            "type": "user"
          },
          "name": "Mustafa Shukor",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:09.058Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc26e",
          "user": {
            "_id": "640e21ef3c82bd463ee5a76d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e21ef3c82bd463ee5a76d/nVR1DFPAsiLw6Boys28Rb.jpeg",
            "isPro": false,
            "fullname": "Dana Aubakirova",
            "user": "danaaubakirova",
            "type": "user"
          },
          "name": "Dana Aubakirova",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:18.586Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc26f",
          "user": {
            "_id": "63d67eac6f49aa8230601996",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d67eac6f49aa8230601996/djvtWdy718whUgh7tu1Ko.jpeg",
            "isPro": false,
            "fullname": "Francesco Capuano",
            "user": "fracapuano",
            "type": "user"
          },
          "name": "Francesco Capuano",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:29.639Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc270",
          "user": {
            "_id": "65f9d37113336392bad1e49c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f9d37113336392bad1e49c/B0Fxwconnu7lvtjBz4Ruq.jpeg",
            "isPro": false,
            "fullname": "Pepijn Kooijmans",
            "user": "pepijn223",
            "type": "user"
          },
          "name": "Pepijn Kooijmans",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:38.693Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc271",
          "user": {
            "_id": "67b124b081d4eae18b957606",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/CXvSv2l15uPkMQL_HBRDF.png",
            "isPro": false,
            "fullname": "Steven Palma",
            "user": "imstevenpmwork",
            "type": "user"
          },
          "name": "Steven Palma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:47.379Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc272",
          "user": {
            "_id": "64c255b2254239173af0570a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c255b2254239173af0570a/JvYNX4gpk0hVQJeJif8Mo.jpeg",
            "isPro": false,
            "fullname": "Adil Zouitine",
            "user": "AdilZtn",
            "type": "user"
          },
          "name": "Adil Zouitine",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:56.957Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc273",
          "user": {
            "_id": "668bd06dd58b51a628566d80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668bd06dd58b51a628566d80/II7Yr5dT5ItMrpoMkQEy3.jpeg",
            "isPro": false,
            "fullname": "Michel Aractingi",
            "user": "aractingi",
            "type": "user"
          },
          "name": "Michel Aractingi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:05.798Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc274",
          "user": {
            "_id": "67d7dea1786ddcb3af5a44b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d7dea1786ddcb3af5a44b3/gEgXTH4oO91GIzjHR-yrb.png",
            "isPro": false,
            "fullname": "Caroline Pascal",
            "user": "CarolinePascal",
            "type": "user"
          },
          "name": "Caroline Pascal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:15.340Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc275",
          "user": {
            "_id": "631365ad289cf15634c6f600",
            "avatarUrl": "/avatars/a464d228328719274a20121e2a82f703.svg",
            "isPro": false,
            "fullname": "Martino Russi",
            "user": "nepyope",
            "type": "user"
          },
          "name": "Martino Russi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:23.474Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc276",
          "user": {
            "_id": "65d66b494bbd0d92b641cdbb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
            "isPro": false,
            "fullname": "Andres Marafioti",
            "user": "andito",
            "type": "user"
          },
          "name": "Andres Marafioti",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:31.740Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc277",
          "user": {
            "_id": "65fcb7f133a3d6f126772121",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fcb7f133a3d6f126772121/BvVbNqnlQgDr2f_9dm5Es.jpeg",
            "isPro": false,
            "fullname": "Simon  Alibert",
            "user": "aliberts",
            "type": "user"
          },
          "name": "Simon Alibert",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:39.999Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc278",
          "name": "Matthieu Cord",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc279",
          "user": {
            "_id": "5df7e9e5da6d0311fd3d53f9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg",
            "isPro": false,
            "fullname": "Thomas Wolf",
            "user": "thomwolf",
            "type": "user"
          },
          "name": "Thomas Wolf",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:13:02.139Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc27a",
          "user": {
            "_id": "62f857fbb9fda55613ce80d9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f857fbb9fda55613ce80d9/d7bRniKLmOt-iFN07k1Su.png",
            "isPro": false,
            "fullname": "Remi Cadene",
            "user": "cadene",
            "type": "user"
          },
          "name": "Remi Cadene",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:13:11.882Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T16:30:19.000Z",
      "submittedOnDailyAt": "2025-06-03T07:31:41.152Z",
      "title": "SmolVLA: Modelo de Visión, Lenguaje y Acción para Ingeniería de Robótica Económica y Eficiente",
      "submittedOnDailyBy": {
        "_id": "65d66b494bbd0d92b641cdbb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
        "isPro": false,
        "fullname": "Andres Marafioti",
        "user": "andito",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje visual (VLMs) se entrenan con grandes conjuntos de datos de múltiples tipos para registrar un conocimiento visual y lingüístico rico, formando una sólida base para sistemas de robótica. Una reciente aproximación para programar políticas de robotes es convertir estos modelos en modelos de acción visual-lingüística (VLA) de manera más natural. Sin embargo, actualmente, los VLA generalmente tienen cientos de billones de parámetros, asociando altos costos de entrenamiento y una funcionalidad en el mundo real. Además, los conjuntos de datos se centran en academia e industria, ignorando el crecimiento de datos comunitarios de plataformas de robotes de bajo costo. En este estudio, se presenta SmolVLA, un modelo de acción visual-lingüística pequeño y eficiente basado en la comunidad, que reduce significativamente los costos de entrenamiento e inferencia, manteniendo una performance competitiva. SmolVLA se entrena en una sola GPU y está diseñado para funcionar con GPUs de consumo o CPUs. Además, se introduce una pila de inferencia asincrónica para mejorar la respuesta y separa la predicción de acción visual y la ejecución de acciones para generar acciones cortadas, logrando una alta velocidad de control. A pesar de su pequeño tamaño, SmolVLA logra alcanzar la misma performance que un VLA mucho más grande. SmolVLA ha sido evaluado en varios benchmarks de simulación y robotes en el mundo real, y todos los códigos, modelos pre-entrenados y datos de entrenamiento están disponibles.",
      "upvotes": 17,
      "discussionId": "683eb85925fcc99d2a7fc2dc",
      "ai_summary": "SmolVLA is a compact, efficient vision-language-action model that achieves competitive performance at reduced computational costs and can be deployed on consumer-grade hardware.",
      "ai_keywords": [
        "vision-language models",
        "multimodal datasets",
        "robotic policies",
        "vision-language-action models",
        "natural language-driven perception",
        "asynchronous inference",
        "action prediction",
        "action execution",
        "chunked action generation",
        "performance benchmarks"
      ]
    },
    "publishedAt": "2025-06-02T12:30:19.000Z",
    "title": "SmolVLA: A Vision-Language-Action Model for Affordable and Efficient\n  Robotics",
    "summary": "Vision-language models (VLMs) pretrained on large-scale multimodal datasets\nencode rich visual and linguistic knowledge, making them a strong foundation\nfor robotics. Rather than training robotic policies from scratch, recent\napproaches adapt VLMs into vision-language-action (VLA) models that enable\nnatural language-driven perception and control. However, existing VLAs are\ntypically massive--often with billions of parameters--leading to high training\ncosts and limited real-world deployability. Moreover, they rely on academic and\nindustrial datasets, overlooking the growing availability of\ncommunity-collected data from affordable robotic platforms. In this work, we\npresent SmolVLA, a small, efficient, and community-driven VLA that drastically\nreduces both training and inference costs, while retaining competitive\nperformance. SmolVLA is designed to be trained on a single GPU and deployed on\nconsumer-grade GPUs or even CPUs. To further improve responsiveness, we\nintroduce an asynchronous inference stack decoupling perception and action\nprediction from action execution, allowing higher control rates with chunked\naction generation. Despite its compact size, SmolVLA achieves performance\ncomparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both\nsimulated as well as real-world robotic benchmarks and release all code,\npretrained models, and training data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01844.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65d66b494bbd0d92b641cdbb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
      "fullname": "Andres Marafioti",
      "name": "andito",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 185
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01943",
      "authors": [
        {
          "_id": "683e6b6424742a21489ec9f8",
          "name": "Xiao Fu",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9f9",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fa",
          "name": "Xian Liu",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fb",
          "name": "Jianhong Bai",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fc",
          "name": "Runsen Xu",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fd",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fe",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9ff",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63aef2cafcca84593e6682db/9mFDJaCOc6KLHlhboYA59.mp4"
      ],
      "publishedAt": "2025-06-02T17:57:06.000Z",
      "submittedOnDailyAt": "2025-06-03T01:57:30.514Z",
      "title": "Controle de trayectoria cooperativa de manipulación de robots mediante video de aprendizaje",
      "submittedOnDailyBy": {
        "_id": "63aef2cafcca84593e6682db",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672409763337-noauth.jpeg",
        "isPro": false,
        "fullname": "Xiao Fu",
        "user": "lemonaddie",
        "type": "user"
      },
      "summary": "El desarrollo reciente de modelos de difusión de imágenes ha mostrado un potente potencial para la generación de datos de decisión de robots. Las condiciones de proyecto permiten un control más preciso. Sin embargo, los métodos actuales basados en proyectos se centran principalmente en el movimiento individual de objetos y no comprenden la interacción multi-objeto necesaria para manejos complejos de robots. Esta limitación se debe a la combinación de características en áreas repetidas. Para resolver esto, presentamos RoboMaster. RoboMaster modela la dinámica entre objetos mediante proyectos cooperativos. En contraste con los métodos existentes, RoboMaster no separa los objetos sino que divide el proceso de interacción en tres etapas: antes, durante y después de la interacción. Cada etapa se modela utilizando las características del objeto protagonista. En las etapas antes y después de la interacción, se utilizan las manos del robot, mientras que en la etapa de la interacción se utilizan los objetos de manejo como características. De esta manera, se reducen los desafíos de la fusión de características multi-objetos vistos en investigaciones previas. Además, incluye una representación potencial de la apariencia y forma de los objetos para garantizar la coherencia significativa del tema en toda la videografía. A través de experimentos detallados en el conjunto de datos Bridge V2 y la evaluación de estados naturales, nuestro método supera los métodos existentes y explora un nuevo rendimiento superior en la generación de videos de control de proyectos para la manipulación de robots.",
      "upvotes": 15,
      "discussionId": "683e6b6724742a21489eca8d",
      "ai_summary": "A novel framework, RoboMaster, enhances trajectory-controlled video generation for robotic manipulation by modeling inter-object dynamics through a collaborative trajectory formulation, achieving state-of-the-art performance on the Bridge V2 dataset.",
      "ai_keywords": [
        "video diffusion models",
        "trajectory conditions",
        "multi-object interaction",
        "multi-feature entanglement",
        "visual fidelity",
        "collaborative trajectory formulation",
        "pre-interaction",
        "interaction",
        "post-interaction",
        "appearance-aware latent representations",
        "shape-aware latent representations",
        "trajectory-controlled video generation",
        "robotic manipulation",
        "Bridge V2 dataset"
      ]
    },
    "publishedAt": "2025-06-02T13:57:06.000Z",
    "title": "Learning Video Generation for Robotic Manipulation with Collaborative\n  Trajectory Control",
    "summary": "Recent advances in video diffusion models have demonstrated strong potential\nfor generating robotic decision-making data, with trajectory conditions further\nenabling fine-grained control. However, existing trajectory-based methods\nprimarily focus on individual object motion and struggle to capture\nmulti-object interaction crucial in complex robotic manipulation. This\nlimitation arises from multi-feature entanglement in overlapping regions, which\nleads to degraded visual fidelity. To address this, we present RoboMaster, a\nnovel framework that models inter-object dynamics through a collaborative\ntrajectory formulation. Unlike prior methods that decompose objects, our core\nis to decompose the interaction process into three sub-stages: pre-interaction,\ninteraction, and post-interaction. Each stage is modeled using the feature of\nthe dominant object, specifically the robotic arm in the pre- and\npost-interaction phases and the manipulated object during interaction, thereby\nmitigating the drawback of multi-object feature fusion present during\ninteraction in prior work. To further ensure subject semantic consistency\nthroughout the video, we incorporate appearance- and shape-aware latent\nrepresentations for objects. Extensive experiments on the challenging Bridge V2\ndataset, as well as in-the-wild evaluation, demonstrate that our method\noutperforms existing approaches, establishing new state-of-the-art performance\nin trajectory-controlled video generation for robotic manipulation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63aef2cafcca84593e6682db/9mFDJaCOc6KLHlhboYA59.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01943.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63aef2cafcca84593e6682db",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672409763337-noauth.jpeg",
      "fullname": "Xiao Fu",
      "name": "lemonaddie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01853",
      "authors": [
        {
          "_id": "683e671483a130f817c4937a",
          "name": "Junliang Ye",
          "hidden": false
        },
        {
          "_id": "683e671483a130f817c4937b",
          "name": "Zhengyi Wang",
          "hidden": false
        },
        {
          "_id": "683e671483a130f817c4937c",
          "user": {
            "_id": "6522e4fbd89bc7773ddc4b58",
            "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
            "isPro": false,
            "fullname": "Ruowen Zhao",
            "user": "zzzrw",
            "type": "user"
          },
          "name": "Ruowen Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:03.565Z",
          "hidden": false
        },
        {
          "_id": "683e671483a130f817c4937d",
          "name": "Shenghao Xie",
          "hidden": false
        },
        {
          "_id": "683e671483a130f817c4937e",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/84A40qBWJeZaBv2qYO6Pj.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/Xl6IajTG3VravdQABgC6l.mp4"
      ],
      "publishedAt": "2025-06-02T16:40:50.000Z",
      "submittedOnDailyAt": "2025-06-03T03:57:42.122Z",
      "title": "ShapeLLM-Omni: LLM Monomodal para la Generación y Comprensión 3D",
      "submittedOnDailyBy": {
        "_id": "65a420cd90e65dc39a6abe9e",
        "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
        "isPro": false,
        "fullname": "yejunliang",
        "user": "yejunliang23",
        "type": "user"
      },
      "summary": "Recientemente, la capacidad de ChatGPT-4o para transformar texto fuerte en imágenes ha aumentado la alegría de los modelos multimodelos nativos de lenguaje, aunque su capacidad multimodelo está todavía limitada a texto y imágenes. Sin embargo, la comprensión y la generación de contenido 3D son igual de importantes que las imágenes. Para llenar esta brecha, se propone ShapeLLM-Omni, un modelo nativo de lenguaje 3D. Este modelo puede entender y generar 3D assets y texto en cualquier orden. Primero, se entrena un 3D vector carrier visualization variational autoencoder (VQVAE) para mapear objetos 3D a un espacio potencial disperso, logrando una representación eficiente y precisa de formas y reconstrucción. Luego, se construye un gran conjunto de datos de entrenamiento continuo llamado 3D-Alpaca, que incluye generación, comprensión y edición, proporcionando recursos ricos para futuras investigaciones y entrenamientos. Finalmente, se entrena el modelo Qwen-2.5-vl-7B-Instruct basado en instrucciones sobre el conjunto de datos 3D-Alpaca. Nuestra investigación ofrece una intento efectivo para extender modelos multimodelos con capacidades 3D básicas, contribuyendo a la investigación futura de AI nativos 3D. Página del proyecto: https://github.com/JAMESYJL/ShapeLLM-Omni",
      "upvotes": 15,
      "discussionId": "683e671683a130f817c493cd",
      "ai_summary": "A native 3D large language model, ShapeLLM-Omni, is proposed to understand and generate 3D assets and text, trained using a 3D vector-quantized variational autoencoder and a new 3D-Alpaca dataset.",
      "ai_keywords": [
        "3D vector-quantized variational autoencoder (VQVAE)",
        "discrete latent space",
        "instruction-based training",
        "3D-Alpaca dataset",
        "3D-native AI"
      ]
    },
    "publishedAt": "2025-06-02T12:40:50.000Z",
    "title": "ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and\n  Understanding",
    "summary": "Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to\ngrowing appreciation for native multimodal large language models. However, its\nmultimodal capabilities remain confined to images and text. Yet beyond images,\nthe ability to understand and generate 3D content is equally crucial. To\naddress this gap, we propose ShapeLLM-Omni-a native 3D large language model\ncapable of understanding and generating 3D assets and text in any sequence.\nFirst, we train a 3D vector-quantized variational autoencoder (VQVAE), which\nmaps 3D objects into a discrete latent space to achieve efficient and accurate\nshape representation and reconstruction. Building upon the 3D-aware discrete\ntokens, we innovatively construct a large-scale continuous training dataset\nnamed 3D-Alpaca, encompassing generation, comprehension, and editing, thus\nproviding rich resources for future research and training. Finally, by\nperforming instruction-based training of the Qwen-2.5-vl-7B-Instruct model on\nthe 3D-Alpaca dataset. Our work provides an effective attempt at extending\nmultimodal models with basic 3D capabilities, which contributes to future\nresearch in 3D-native AI. Project page:\nhttps://github.com/JAMESYJL/ShapeLLM-Omni",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/84A40qBWJeZaBv2qYO6Pj.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/Xl6IajTG3VravdQABgC6l.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65a420cd90e65dc39a6abe9e",
      "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
      "fullname": "yejunliang",
      "name": "yejunliang23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24760",
      "authors": [
        {
          "_id": "683e6af92139ea008faa74ba",
          "user": {
            "_id": "65144e46004a986ccc9d21d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65144e46004a986ccc9d21d6/oImRY64V-UeYGdrbd_ozU.jpeg",
            "isPro": false,
            "fullname": "Zafir Stojanovski",
            "user": "zafstojano",
            "type": "user"
          },
          "name": "Zafir Stojanovski",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-03T06:28:46.378Z",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74bb",
          "user": {
            "_id": "6303f5f37b50dd9d0a371b28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6303f5f37b50dd9d0a371b28/H25eCzAYVwBtpSpD8tnUV.jpeg",
            "isPro": false,
            "fullname": "Oliver Stanley",
            "user": "OllieStanley",
            "type": "user"
          },
          "name": "Oliver Stanley",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:56.277Z",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74bc",
          "name": "Joe Sharratt",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74bd",
          "name": "Richard Jones",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74be",
          "name": "Abdulhakeem Adefioye",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74bf",
          "user": {
            "_id": "6304061c0547362a22a76a17",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661339692442-6304061c0547362a22a76a17.jpeg",
            "isPro": false,
            "fullname": "Jean Kaddour",
            "user": "JeanKaddour",
            "type": "user"
          },
          "name": "Jean Kaddour",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T09:13:54.739Z",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74c0",
          "name": "Andreas Köpf",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T16:20:18.000Z",
      "submittedOnDailyAt": "2025-06-03T02:02:10.153Z",
      "title": "La lógica de aprendizaje por refuerzo con recompensas demostrables en un entorno de razonamiento",
      "submittedOnDailyBy": {
        "_id": "65144e46004a986ccc9d21d6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65144e46004a986ccc9d21d6/oImRY64V-UeYGdrbd_ozU.jpeg",
        "isPro": false,
        "fullname": "Zafir Stojanovski",
        "user": "zafstojano",
        "type": "user"
      },
      "summary": "Reasoning Gym (RG) es una biblioteca de entornos para la lógica de aprendizaje reforzado que permite probar compensaciones experimentales. Cuenta con más de 100 generadores de datos y proveedores de pruebas, y extiende campos como la lógica, aritmética, cálculo, percepción, geometría, teoría de grafos, lógica de Horn y muchos otros campos generales de juegos. La innovación principal es que, en contraste con los conjuntos de datos de lógica anteriores, puede generar datos de complejidad variable de manera infinita. Este proceso de generación permite evaluaciones continuas a diferentes niveles de dificultad. A través de los resultados de experimentos, RG demuestra su eficacia en la evaluación de modelos de lógica y en el aprendizaje reforzado.",
      "upvotes": 15,
      "discussionId": "683e6afa2139ea008faa7531",
      "githubRepo": "https://github.com/open-thought/reasoning-gym",
      "ai_summary": "Reasoning Gym provides a library of reasoning environments with verifiable rewards and procedural data generation for reinforcement learning, enabling the evaluation and training of reasoning models at varying difficulty levels.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "data generators",
        "verifiers",
        "procedural generation",
        "reasoning models"
      ]
    },
    "publishedAt": "2025-05-30T12:20:18.000Z",
    "title": "REASONING GYM: Reasoning Environments for Reinforcement Learning with\n  Verifiable Rewards",
    "summary": "We introduce Reasoning Gym (RG), a library of reasoning environments for\nreinforcement learning with verifiable rewards. It provides over 100 data\ngenerators and verifiers spanning multiple domains including algebra,\narithmetic, computation, cognition, geometry, graph theory, logic, and various\ncommon games. Its key innovation is the ability to generate virtually infinite\ntraining data with adjustable complexity, unlike most previous reasoning\ndatasets, which are typically fixed. This procedural generation approach allows\nfor continuous evaluation across varying difficulty levels. Our experimental\nresults demonstrate the efficacy of RG in both evaluating and reinforcement\nlearning of reasoning models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24760.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65144e46004a986ccc9d21d6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65144e46004a986ccc9d21d6/oImRY64V-UeYGdrbd_ozU.jpeg",
      "fullname": "Zafir Stojanovski",
      "name": "zafstojano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00996",
      "authors": [
        {
          "_id": "683e7cbf402acb186580d5ec",
          "name": "Kinam Kim",
          "hidden": false
        },
        {
          "_id": "683e7cbf402acb186580d5ed",
          "name": "Junha Hyung",
          "hidden": false
        },
        {
          "_id": "683e7cbf402acb186580d5ee",
          "name": "Jaegul Choo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64797735a68454566356b708/a-8jl8yq3KdDLnuZEaB3K.mp4"
      ],
      "publishedAt": "2025-06-01T12:57:43.000Z",
      "submittedOnDailyAt": "2025-06-03T03:25:10.796Z",
      "title": "La corrección de la diversidad temporal para modelos de diversidad de vídeo con diferentes controles",
      "submittedOnDailyBy": {
        "_id": "64797735a68454566356b708",
        "avatarUrl": "/avatars/3424d022dd8ad29b56eb41814c5c3dee.svg",
        "isPro": false,
        "fullname": "Kinam Kim",
        "user": "kinam0252",
        "type": "user"
      },
      "summary": "En el contexto reciente, el desarrollo de modelos de difusión para videos ha permitido la síntesis de videos de alta calidad, aunque la generación controlada ha sido particularmente difícil debido a limitaciones de datos y cantidad de cálculo. Los métodos de ajuste para la generación condicionada actual dependen de encoderes externos o cambios estructurales, lo cual requiere grandes conjuntos de datos y limita la flexibilidad y escalabilidad espacial de la generación condicionada. En este artículo, introducimos la técnica de ajuste condicionado en contexto temporal (Temporal In-Context Fine-Tuning, TIC-FT) para proporcionar una aproximación eficiente y amplia al acceso a tareas de generación condicionada variadas, a partir de modelos de difusión de videos previamente proporcionados. Nuestra idea principal es combinar las condiciones y las frases objetivo en el eje temporal y insertar frames de un buffer con un nivel de ruido creciente. Estos frames de buffer permiten movimientos suaves y aseguran que el proceso de ajuste coincida con la dinámica temporal del modelo proporcionado. La TIC-FT no requiere cambios estructurales y logra un buen rendimiento con solo 10-30 muestras de entrenamiento. Nuestro método se ha validado en tareas como la generación de videos a partir de imágenes y la generación de videos a partir de videos, utilizando modelos básicos como CogVideoX-5B y Wan-14B. Los experimentos extendidos demostraron superar los límites de los baselines en términos de fidelidad a la condición y calidad visual, y presentaron altas eficiencias tanto en el entrenamiento como en la inferencia. Para obtener más resultados, consulte https://kinam0252.github.io/TIC-FT/.",
      "upvotes": 14,
      "discussionId": "683e7cc1402acb186580d663",
      "ai_summary": "Temporal In-Context Fine-Tuning (TIC-FT) enhances pretrained video diffusion models for diverse conditional generation tasks with minimal data and without architectural changes.",
      "ai_keywords": [
        "text-to-video diffusion models",
        "fine-tuning",
        "external encoders",
        "architectural modifications",
        "Temporal In-Context Fine-Tuning",
        "condition and target frames",
        "buffer frames",
        "noise levels",
        "smooth transitions",
        "pretrained video diffusion models",
        "image-to-video generation",
        "video-to-video generation",
        "CogVideoX-5B",
        "Wan-14B"
      ]
    },
    "publishedAt": "2025-06-01T08:57:43.000Z",
    "title": "Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion\n  Models",
    "summary": "Recent advances in text-to-video diffusion models have enabled high-quality\nvideo synthesis, but controllable generation remains challenging, particularly\nunder limited data and compute. Existing fine-tuning methods for conditional\ngeneration often rely on external encoders or architectural modifications,\nwhich demand large datasets and are typically restricted to spatially aligned\nconditioning, limiting flexibility and scalability. In this work, we introduce\nTemporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach\nfor adapting pretrained video diffusion models to diverse conditional\ngeneration tasks. Our key idea is to concatenate condition and target frames\nalong the temporal axis and insert intermediate buffer frames with\nprogressively increasing noise levels. These buffer frames enable smooth\ntransitions, aligning the fine-tuning process with the pretrained model's\ntemporal dynamics. TIC-FT requires no architectural changes and achieves strong\nperformance with as few as 10-30 training samples. We validate our method\nacross a range of tasks, including image-to-video and video-to-video\ngeneration, using large-scale base models such as CogVideoX-5B and Wan-14B.\nExtensive experiments show that TIC-FT outperforms existing baselines in both\ncondition fidelity and visual quality, while remaining highly efficient in both\ntraining and inference. For additional results, visit\nhttps://kinam0252.github.io/TIC-FT/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64797735a68454566356b708/a-8jl8yq3KdDLnuZEaB3K.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64797735a68454566356b708",
      "avatarUrl": "/avatars/3424d022dd8ad29b56eb41814c5c3dee.svg",
      "fullname": "Kinam Kim",
      "name": "kinam0252",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24846",
      "authors": [
        {
          "_id": "683e82f2fa7ede4842f95214",
          "name": "Jingyan Shen",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f95215",
          "user": {
            "_id": "66f8689725464a7989b75845",
            "avatarUrl": "/avatars/43a61a528c5779103eaf5687ba44ee14.svg",
            "isPro": false,
            "fullname": "Jiarui Yao",
            "user": "FlippyDora",
            "type": "user"
          },
          "name": "Jiarui Yao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:40:58.100Z",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f95216",
          "user": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "isPro": false,
            "fullname": "Rui Yang",
            "user": "Ray2333",
            "type": "user"
          },
          "name": "Rui Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:40:16.572Z",
          "hidden": true
        },
        {
          "_id": "683e82f2fa7ede4842f95217",
          "name": "Yifan Sun",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f95218",
          "name": "Feng Luo",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f95219",
          "name": "Rui Pan",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f9521a",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f9521b",
          "name": "Han Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:44:28.000Z",
      "submittedOnDailyAt": "2025-06-03T03:51:59.115Z",
      "title": "MiCRO: Aprendizaje de preferencias personalizadas con modelado mixto y enrutamiento por contexto",
      "submittedOnDailyBy": {
        "_id": "64d45451c34a346181b130dd",
        "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
        "isPro": false,
        "fullname": "Rui Yang",
        "user": "Ray2333",
        "type": "user"
      },
      "summary": "El modelado de niveles es un paso importante en la construcción segura de grandes modelos de lenguaje (LLMs) basados en retroalimentación humana. Sin embargo, el modelo de Bradley-Terry (BT) basado en niveles asume una función de recompensa única y no detecta la diversidad y la diferenciación propias. Esta simplificación impide que los LLMs aporten personalización y soporte a diversas oposiciones.\n\nTeóricamente, cuando las preferencias humanas siguen una distribución mixta de grupos, un solo modelo BT presenta errores insuficientes. Actualmente, se utilizan aprendizajes multifuncionales y fine-tuning para abordar estos problemas, pero estos métodos son costosos y limitados a características predefinidas, lo que impide captar completamente la rica valencia humana.\n\nEn este artículo, se presenta un marco de dos etapas llamado MiCRo, que utiliza un conjunto de datos binarios de preferencias para fortalecer el aprendizaje de preferencias personalizadas sin necesidad de fine-tuning. En la primera etapa, MiCRo introduce un enfoque de modelado mixto relacionado con el contexto para detectar diferentes preferencias humanas. En la segunda etapa, MiCRo integra una estrategia de routing en línea que ajusta los pesos de mezcla dinámicamente en función del contexto, resolviendo así incertidumbres y permitiendo un aprendizaje adaptativo eficiente y escalable sin necesidad de adicionales fine-tuning. Los experimentos con diferentes conjuntos de datos de preferencias demostraron que MiCRo es efectivo en detectar diferentes preferencias humanas y mejora significativamente la personalización en la etapa posterior.",
      "upvotes": 11,
      "discussionId": "683e82f3fa7ede4842f95246",
      "ai_summary": "MiCRo, a two-stage framework, improves personalized preference learning for large language models by leveraging binary preference datasets and dynamically adapting mixture weights based on context, effectively capturing diverse human preferences.",
      "ai_keywords": [
        "Reward modeling",
        "reinforcement learning from human feedback (RLHF)",
        "Large Language Models (LLMs)",
        "Bradley-Terry (BT) model",
        "mixture distribution",
        "personalization",
        "pluralistic alignment",
        "multi-objective learning",
        "context-aware mixture modeling",
        "online routing strategy"
      ]
    },
    "publishedAt": "2025-05-30T13:44:28.000Z",
    "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning",
    "summary": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d45451c34a346181b130dd",
      "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
      "fullname": "Rui Yang",
      "name": "Ray2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24298",
      "authors": [
        {
          "_id": "683d12963aa5ac98190e1eda",
          "name": "Wei Fu",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1edb",
          "name": "Jiaxuan Gao",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1edc",
          "name": "Xujie Shen",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1edd",
          "name": "Chen Zhu",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ede",
          "name": "Zhiyu Mei",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1edf",
          "name": "Chuyi He",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee0",
          "name": "Shusheng Xu",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee1",
          "name": "Guo Wei",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee2",
          "name": "Jun Mei",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee3",
          "name": "Jiashu Wang",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee4",
          "name": "Tongkai Yang",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee5",
          "name": "Binhang Yuan",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee6",
          "name": "Yi Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T07:18:25.000Z",
      "submittedOnDailyAt": "2025-06-03T05:39:21.066Z",
      "title": "AReaL: La lógica del lenguaje es objeto en sistemas de aprendizaje de refuerzo no-sincrónico de gran escala.",
      "submittedOnDailyBy": {
        "_id": "63159678915d0b80682fe9f9",
        "avatarUrl": "/avatars/a19e44ffe681099a155b8f8ecb53f0ce.svg",
        "isPro": false,
        "fullname": "Shusheng Xu",
        "user": "xssstory",
        "type": "user"
      },
      "summary": "El aprendizaje por refuerzo (RL) se encuentra en particular en el ámbito de las tareas consideradas como lógicas, y ha adquirido un papel de crecimiento rápido en el desarrollo de entrenamiento de modelos de lenguaje grandes (LLMs). El aprendizaje efectivo por refuerzo en LLMs requiere una gran paralelización y subraya la necesidad urgente de un sistema de entrenamiento eficiente. Actualmente, muchos sistemas de RL para grandes LLMs se realizan con un enfoque de entrenamiento y generación en paralelo, donde en cada conjunto de entrenamiento se generan salidas con el mismo contenido (o la más reciente versión del modelo). Esto estabiliza el entrenamiento pero plantea problemas de ineficiencia sistemática. La generación debe esperar hasta que el más largo output esté completo, lo que disminuye la eficiencia del uso de GPUs. Presentamos AReaL, un sistema de aprendizaje por refuerzo completamente asincrónico. AReaL separa completamente la generación y el entrenamiento, y continua generando salidas mientras los trabajadores de entrenamiento actualizan el modelo cuando se dispongan de nuevas entradas. AReaL introduce optimizaciones a nivel de sistema para elevar significativamente la eficiencia de los GPUs. Para estabilizar el entrenamiento por refuerzo, AReaL equilibra la cantidad de trabajo de las salidas y los trabajadores de entrenamiento, controla la generación de datos y introduce una técnica de control de finalización basada en una variante del PPO para procesar mejor los ejemplos de entrenamiento finalizados. A través de experimentos distribuidos en marcos de referencia lógicos matemáticos y código, AReaL, utilizando el mismo GPU, logra un aumento de velocidad de entrenamiento del 2.57 veces, mostrando un sistema de motivación óptimo o mejores resultados finales. El código de AReaL está disponible en https://github.com/inclusionAI/AReaL/.",
      "upvotes": 10,
      "discussionId": "683d12973aa5ac98190e1f19",
      "ai_summary": "AReaL, a fully asynchronous reinforcement learning system, decouples generation and training to achieve higher GPU utilization and up to 2.57x training speedup for large language models on reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "asynchronous system",
        "rollouts",
        "model update",
        "GPU utilization",
        "PPO",
        "data staleness",
        "training speedup"
      ]
    },
    "publishedAt": "2025-05-30T03:18:25.000Z",
    "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning",
    "summary": "Reinforcement learning (RL) has become a trending paradigm for training large\nlanguage models (LLMs), particularly for reasoning tasks. Effective RL for LLMs\nrequires massive parallelization and poses an urgent need for efficient\ntraining systems. Most existing large-scale RL systems for LLMs are synchronous\nby alternating generation and training in a batch setting, where the rollouts\nin each training batch are generated by the same (or latest) model. This\nstabilizes RL training but suffers from severe system-level inefficiency.\nGeneration must wait until the longest output in the batch is completed before\nmodel update, resulting in GPU underutilization. We present AReaL, a\nfully asynchronous RL system that completely decouples generation from\ntraining. Rollout workers in AReaL continuously generate new outputs without\nwaiting, while training workers update the model whenever a batch of data is\ncollected. AReaL also incorporates a collection of system-level optimizations,\nleading to substantially higher GPU utilization. To stabilize RL training,\nAReaL balances the workload of rollout and training workers to control data\nstaleness, and adopts a staleness-enhanced PPO variant to better handle\noutdated training samples. Extensive experiments on math and code reasoning\nbenchmarks show that AReaL achieves up to 2.57times training\nspeedup compared to the best synchronous systems with the same number of GPUs\nand matched or even improved final performance. The code of AReaL is available\nat https://github.com/inclusionAI/AReaL/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24298.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63159678915d0b80682fe9f9",
      "avatarUrl": "/avatars/a19e44ffe681099a155b8f8ecb53f0ce.svg",
      "fullname": "Shusheng Xu",
      "name": "xssstory",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23907",
      "authors": [
        {
          "_id": "683e6838a6815c77acb18ea3",
          "user": {
            "_id": "650e3abcae507a2c7c847baa",
            "avatarUrl": "/avatars/a138bc6c9ba80a486fb36f4e1aa16b33.svg",
            "isPro": false,
            "fullname": "Amirhossein Alimohammadi",
            "user": "Amirhossein-Alimohammadi",
            "type": "user"
          },
          "name": "Amirhossein Almohammadi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:59.757Z",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea4",
          "name": "Aryan Mikaeili",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea5",
          "name": "Sauradip Nag",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea6",
          "name": "Negar Hassanpour",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea7",
          "name": "Andrea Tagliasacchi",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea8",
          "name": "Ali Mahdavi-Amiri",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/650e3abcae507a2c7c847baa/dRNPZqV4HrW79gs506jk7.mp4"
      ],
      "publishedAt": "2025-05-29T18:00:56.000Z",
      "submittedOnDailyAt": "2025-06-03T01:51:49.601Z",
      "title": "Kora: Edición de imágenes para comunicación de menos pasos por divergencia",
      "submittedOnDailyBy": {
        "_id": "650e3abcae507a2c7c847baa",
        "avatarUrl": "/avatars/a138bc6c9ba80a486fb36f4e1aa16b33.svg",
        "isPro": false,
        "fullname": "Amirhossein Alimohammadi",
        "user": "Amirhossein-Alimohammadi",
        "type": "user"
      },
      "summary": "La edición de imágenes es uno de los trabajos más importantes en graficas computacionales, visión artificial y VFX, y los métodos basados en difusión han logrado resultados de alta calidad de manera rápida. Sin embargo, las ediciones que requieren cambios estructurales, como deformaciones no lineales, modificaciones de objetos o generación de contenido, pueden ser complicadas. Los enfoques de edición en pocos pasos actuales enfrentan dificultades para preservar atributos de textura y características clave no relevantes (por ejemplo, la postura). Presentamos un nuevo marco de edición llamado Cora, que introduce corrección de ruido y mapas de atención interpolados basados en relaciones correspondientes para resolver estos límites. Nuestro método arregla la textura y estructura de las imágenes fuente y objetivo a través de relaciones correspondientes, y genera nuevo contenido cuando sea necesario, facilitando así una transmisión precisa de textura. Cora controla el equilibrio entre la generación y almacenamiento del contenido. Los experimentos ampliados muestran que Cora mantiene la identidad estructural, textural y de diferentes tipos de edición tanto cuantitativamente como cualitativamente, demostrando excelentes resultados en cambios de postura, adición de objetos y mejora de textura. Los usuarios confirmaron que Cora ofrece los mejores resultados y supera las alternativas existentes.",
      "upvotes": 8,
      "discussionId": "683e683aa6815c77acb18f03",
      "projectPage": "https://cora-edit.github.io/",
      "githubRepo": "https://github.com/alimohammadiamirhossein/cora",
      "ai_summary": "Cora framework enhances image editing through correspondence-aware noise correction and interpolated attention maps, excelling in structure and texture preservation and generation.",
      "ai_keywords": [
        "diffusion-based methods",
        "non-rigid deformations",
        "object modifications",
        "content generation",
        "correspondence-aware noise correction",
        "interpolated attention maps",
        "semantic correspondence",
        "texture transfer",
        "content generation",
        "preservation",
        "pose changes",
        "object addition",
        "texture refinements"
      ]
    },
    "publishedAt": "2025-05-29T14:00:56.000Z",
    "title": "Cora: Correspondence-aware image editing using few step diffusion",
    "summary": "Image editing is an important task in computer graphics, vision, and VFX,\nwith recent diffusion-based methods achieving fast and high-quality results.\nHowever, edits requiring significant structural changes, such as non-rigid\ndeformations, object modifications, or content generation, remain challenging.\nExisting few step editing approaches produce artifacts such as irrelevant\ntexture or struggle to preserve key attributes of the source image (e.g.,\npose). We introduce Cora, a novel editing framework that addresses these\nlimitations by introducing correspondence-aware noise correction and\ninterpolated attention maps. Our method aligns textures and structures between\nthe source and target images through semantic correspondence, enabling accurate\ntexture transfer while generating new content when necessary. Cora offers\ncontrol over the balance between content generation and preservation. Extensive\nexperiments demonstrate that, quantitatively and qualitatively, Cora excels in\nmaintaining structure, textures, and identity across diverse edits, including\npose changes, object addition, and texture refinements. User studies confirm\nthat Cora delivers superior results, outperforming alternatives.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650e3abcae507a2c7c847baa/dRNPZqV4HrW79gs506jk7.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23907.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650e3abcae507a2c7c847baa",
      "avatarUrl": "/avatars/a138bc6c9ba80a486fb36f4e1aa16b33.svg",
      "fullname": "Amirhossein Alimohammadi",
      "name": "Amirhossein-Alimohammadi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23001",
      "authors": [
        {
          "_id": "6839f87c49d173e7b23f220b",
          "user": {
            "_id": "66fa2c61c25c3fcb32f9f131",
            "avatarUrl": "/avatars/05387c30f2d1803fa0a5b176c3706772.svg",
            "isPro": false,
            "fullname": "Yize Cheng",
            "user": "yizecheng",
            "type": "user"
          },
          "name": "Yize Cheng",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T18:27:47.059Z",
          "hidden": false
        },
        {
          "_id": "6839f87c49d173e7b23f220c",
          "user": {
            "_id": "659dc02d72238596c24d49f5",
            "avatarUrl": "/avatars/d4600d23ccc72f296fab7f626d5895e7.svg",
            "isPro": false,
            "fullname": "Wenxiao Wang",
            "user": "wangwenxiao",
            "type": "user"
          },
          "name": "Wenxiao Wang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T18:28:18.975Z",
          "hidden": false
        },
        {
          "_id": "6839f87c49d173e7b23f220d",
          "user": {
            "_id": "63449874ee1504dbcd59af3d",
            "avatarUrl": "/avatars/57a805d82d16de9544c98585bd7a3e55.svg",
            "isPro": false,
            "fullname": "MazdaM",
            "user": "mmoayeri",
            "type": "user"
          },
          "name": "Mazda Moayeri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T20:01:34.913Z",
          "hidden": false
        },
        {
          "_id": "6839f87c49d173e7b23f220e",
          "name": "Soheil Feizi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T02:22:14.000Z",
      "submittedOnDailyAt": "2025-06-03T00:16:12.678Z",
      "title": "Dípack: Método para marcar de manera clara la contaminación del conjunto de pruebas en los LLMs por el uso de backdoors",
      "submittedOnDailyBy": {
        "_id": "66fa2c61c25c3fcb32f9f131",
        "avatarUrl": "/avatars/05387c30f2d1803fa0a5b176c3706772.svg",
        "isPro": false,
        "fullname": "Yize Cheng",
        "user": "yizecheng",
        "type": "user"
      },
      "summary": "El benchmark abierto es esencial para la evaluación y el desarrollo de modelos de lenguaje grandes, proporcionando replicabilidad y transparencia, pero su accesibilidad puede disminuir si se sufre contaminación de conjuntos de prueba. En este artículo, se utiliza el marco de trabajo 'DyePack' para determinar si un modelo ha utilizado un conjunto de prueba del benchmark. El objetivo es marcar con un flag cualquier modelo que ha aprendido de datos de prueba mezclados con muestras de Wikipedia, como si un banco identificará a un ladrón mezclando dinero con driverback packs. Además, se propone un diseño básico que incluye múltiples Wikipedias y permite calcular un Falsi Positive Rate (FPR) probabilístico para cualquier modelo marcado. De esta manera, se previenen falsos positivos mientras se proporcionan fuertes pruebas cuando se detecten fenómenos de contaminación. DyePack evalúa a 5 modelos con 3 conjuntos de datos, cubriendo tanto problemas de selección múltiple como tareas de generación abierta. En problemas de selección múltiple, se utilizan 8 Wikipedias en MMLU-Pro y Big-Bench-Hard, con un FPR probabilístico bajo de 0.000073%, detectando con éxito todos los modelos contaminados. En tareas de generación abierta, todos los modelos son detectados como contaminados, con un FPR probabilístico de 0.127% y 6 Wikipedias, detectando todos los modelos contaminados.",
      "upvotes": 8,
      "discussionId": "6839f87c49d173e7b23f222c",
      "githubRepo": "https://github.com/chengez/DyePack",
      "ai_summary": "DyePack, a framework using backdoor attacks, identifies models that leveraged benchmark test sets during training by introducing benign backdoor samples, ensuring precise false positive rates while preventing false accusations.",
      "ai_keywords": [
        "backdoor attacks",
        "test set contamination",
        "false positive rate",
        "FPR",
        "DyePack",
        "multiple backdoors",
        "stochastic targets",
        "MMLU-Pro",
        "Big-Bench-Hard",
        "Alpaca",
        "multiple-choice questions",
        "open-ended generation tasks"
      ]
    },
    "publishedAt": "2025-05-28T22:22:14.000Z",
    "title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using\n  Backdoors",
    "summary": "Open benchmarks are essential for evaluating and advancing large language\nmodels, offering reproducibility and transparency. However, their accessibility\nmakes them likely targets of test set contamination. In this work, we introduce\nDyePack, a framework that leverages backdoor attacks to identify models that\nused benchmark test sets during training, without requiring access to the loss,\nlogits, or any internal details of the model. Like how banks mix dye packs with\ntheir money to mark robbers, DyePack mixes backdoor samples with the test data\nto flag models that trained on it. We propose a principled design incorporating\nmultiple backdoors with stochastic targets, enabling exact false positive rate\n(FPR) computation when flagging every model. This provably prevents false\naccusations while providing strong evidence for every detected case of\ncontamination. We evaluate DyePack on five models across three datasets,\ncovering both multiple-choice and open-ended generation tasks. For\nmultiple-choice questions, it successfully detects all contaminated models with\nguaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard\nusing eight backdoors. For open-ended generation tasks, it generalizes well and\nidentifies all contaminated models on Alpaca with a guaranteed false positive\nrate of just 0.127% using six backdoors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66fa2c61c25c3fcb32f9f131",
      "avatarUrl": "/avatars/05387c30f2d1803fa0a5b176c3706772.svg",
      "fullname": "Yize Cheng",
      "name": "yizecheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00577",
      "authors": [
        {
          "_id": "683e646de9f216ff5a3e5dea",
          "user": {
            "_id": "658ab894c4b2004663dff3ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658ab894c4b2004663dff3ae/oPRnFuW2Imaa2KkYWNbSf.jpeg",
            "isPro": false,
            "fullname": "YUFA ZHOU",
            "user": "MasterZhou",
            "type": "user"
          },
          "name": "Yufa Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:13.058Z",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5deb",
          "user": {
            "_id": "66968099c952e09a4cb29f78",
            "avatarUrl": "/avatars/bd3a361fe5315e26e9ae328071704eed.svg",
            "isPro": false,
            "fullname": "Wang",
            "user": "Steven-Shaobo",
            "type": "user"
          },
          "name": "Shaobo Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:16.609Z",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5dec",
          "name": "Xingyu Dong",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5ded",
          "name": "Xiangqi Jin",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5dee",
          "name": "Yifang Chen",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5def",
          "name": "Yue Min",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5df0",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5df1",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5df2",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5df3",
          "name": "Linfeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T14:22:40.000Z",
      "submittedOnDailyAt": "2025-06-03T01:32:07.834Z",
      "title": "「Buscamos razones como un economista: el impacto de la generalización estratégica post-entrenamiento en problemas económicos generales」",
      "submittedOnDailyBy": {
        "_id": "658ab894c4b2004663dff3ae",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658ab894c4b2004663dff3ae/oPRnFuW2Imaa2KkYWNbSf.jpeg",
        "isPro": false,
        "fullname": "YUFA ZHOU",
        "user": "MasterZhou",
        "type": "user"
      },
      "summary": "Un entrenamiento directo en lenguaje de modelos de lenguaje de inteligencia artificial (LLMs) en sistemas de múltiples agentes (MAS) pueden enfrentar desafíos debido a la modelación de recompensas complejas, la interacción dinámica entre agentes y la necesidad de una fortaleza generalizadora. En este artículo, investigamos si se puede aplicar tecnologías de entrenamiento posterior, en particular, el entrenamiento de verificación de aprendizaje (SFT) y el aprendizaje por refuerzo (RLVR), en escenarios de múltiples agentes. Utilizamos una prueba como un testeador económico para probar nuestros algoritmos, incorporando una fuerte base en matemáticas y teoría de juegos, así como la necesidad de un análisis lógico estructurado, diseño de mercado, distribución de recursos y análisis de políticas, entre otros aspectos aplicables al mundo real. Presentamos Recon, un modelo de lenguaje de inteligencia artificial (LLM) de 7B parámetros que se entrena posteriormente con un conjunto de datos de selecciones manuales de 2,100 problemas de lógica económica de alta calidad. Según evaluaciones detalladas en marcos de referencia de lógica económica y juegos de múltiples agentes, Recon mejoró significativamente la lógica estructurada y la razonabilidad económica. Estos resultados subrayan la posibilidad de que un entrenamiento posterior mejore la lógica y la respuesta de los agentes, y destacan el papel que desempeñan el SFT y el RL en la formación de las acciones del modelo. El código está disponible en https://github.com/MasterZhou1/Recon.",
      "upvotes": 7,
      "discussionId": "683e646ee9f216ff5a3e5e2c",
      "githubRepo": "https://github.com/MasterZhou1/Recon",
      "ai_summary": "Post-training techniques such as Supervised Fine-Tuning and Reinforcement Learning with Verifiable Rewards improve the reasoning and economic rationality of Large Language Models in multi-agent scenarios through domain-aligned training.",
      "ai_keywords": [
        "Large Language Models",
        "Multi-Agent Systems",
        "Supervised Fine-Tuning",
        "Reinforcement Learning with Verifiable Rewards",
        "economic reasoning",
        "domain-aligned post-training"
      ]
    },
    "publishedAt": "2025-05-31T10:22:40.000Z",
    "title": "Reasoning Like an Economist: Post-Training on Economic Problems Induces\n  Strategic Generalization in LLMs",
    "summary": "Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS)\nremains challenging due to intricate reward modeling, dynamic agent\ninteractions, and demanding generalization requirements. This paper explores\nwhether post-training techniques, specifically Supervised Fine-Tuning (SFT) and\nReinforcement Learning with Verifiable Rewards (RLVR), can effectively\ngeneralize to multi-agent scenarios. We use economic reasoning as a\ntestbed, leveraging its strong foundations in mathematics and game theory, its\ndemand for structured analytical reasoning, and its relevance to real-world\napplications such as market design, resource allocation, and policy analysis.\nWe introduce Recon (Reasoning like an\nECONomist), a 7B-parameter open-source LLM post-trained on a\nhand-curated dataset of 2,100 high-quality economic reasoning problems.\nComprehensive evaluation on economic reasoning benchmarks and multi-agent games\nreveals clear improvements in structured reasoning and economic rationality.\nThese results underscore the promise of domain-aligned post-training for\nenhancing reasoning and agent alignment, shedding light on the roles of SFT and\nRL in shaping model behavior. Code is available at\nhttps://github.com/MasterZhou1/Recon .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00577.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658ab894c4b2004663dff3ae",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658ab894c4b2004663dff3ae/oPRnFuW2Imaa2KkYWNbSf.jpeg",
      "fullname": "YUFA ZHOU",
      "name": "MasterZhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23977",
      "authors": [
        {
          "_id": "683e380cb028cae60270adcf",
          "user": {
            "_id": "6702d9e2db3b7a57f9420e8d",
            "avatarUrl": "/avatars/2e65e83e8d13ca129f6382deb6e8bdfc.svg",
            "isPro": false,
            "fullname": "Yichen Feng",
            "user": "EthanSta",
            "type": "user"
          },
          "name": "Yichen Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:47:01.523Z",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add0",
          "user": {
            "_id": "653df1323479e9ebbe3eb6cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
            "isPro": true,
            "fullname": "Zhangchen Xu",
            "user": "zhangchenxu",
            "type": "user"
          },
          "name": "Zhangchen Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:58.968Z",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add1",
          "name": "Fengqing Jiang",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add2",
          "name": "Yuetai Li",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add3",
          "name": "Bhaskar Ramasubramanian",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add4",
          "name": "Luyao Niu",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add5",
          "name": "Bill Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add6",
          "name": "Radha Poovendran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T20:08:36.000Z",
      "submittedOnDailyAt": "2025-06-03T01:08:43.749Z",
      "title": "VisualSphinx: RL para el juego de rompecabezas visual de lógica sintética de gran escala",
      "submittedOnDailyBy": {
        "_id": "653df1323479e9ebbe3eb6cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
        "isPro": true,
        "fullname": "Zhangchen Xu",
        "user": "zhangchenxu",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje visuo-lingüístico (VLMs) esperan realizar lógicas efectivas multimodales y tomar decisiones lógicamente coherentes. Esto es crucial para tareas como la interpretación de imágenes o la resolución de problemas espaciales. Sin embargo, actualmente los VLMs se enfrentan a una falta de lógica y estructura en los grandes conjuntos de datos de entrenamiento. Para remediar esta deficiencia, proponemos VisualSphinx, el primer grande conjunto de datos de entrenamiento sintético de lógica visuo-lógica. Para abordar el desafío de la fundamentalización de las respuestas en la síntesis de imágenes, proponemos un flujo de trabajo de síntesis de imágenes basado en reglas. Este flujo permite extraer y expandir reglas ocultas a partir de diversas preguntas, generando códigos para la síntesis de imágenes fundamentales que describen la composición de muestras ocultas. Los experimentos muestran que VLMs entrenados con GRPO en VisualSphinx reciben betas a través de la lógica y la interpretación de nuestro conjunto de datos. Estos modelos presentan mejoras en tareas de lógica. Las capacidades de lógica extendidas desarrolladas en VisualSphinx ofrecen ventajas también en tareas lógicas como la aritmética, la lógica algebraica y la geometría.",
      "upvotes": 7,
      "discussionId": "683e380eb028cae60270ae82",
      "ai_summary": "VisualSphinx provides a large-scale synthetic dataset to improve multimodal reasoning in vision language models, enhancing performance on various logical reasoning tasks.",
      "ai_keywords": [
        "vision language models",
        "multimodal reasoning",
        "logical reasoning",
        "large-scale synthetic visual logical reasoning",
        "image synthesis",
        "rule-to-image synthesis",
        "GRPO"
      ]
    },
    "publishedAt": "2025-05-29T16:08:36.000Z",
    "title": "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL",
    "summary": "Vision language models (VLMs) are expected to perform effective multimodal\nreasoning and make logically coherent decisions, which is critical to tasks\nsuch as diagram understanding and spatial problem solving. However, current VLM\nreasoning lacks large-scale and well-structured training datasets. To bridge\nthis gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic\nvisual logical reasoning training data. To tackle the challenge of image\nsynthesis with grounding answers, we propose a rule-to-image synthesis\npipeline, which extracts and expands puzzle rules from seed questions and\ngenerates the code of grounding synthesis image synthesis for puzzle sample\nassembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx\nbenefit from logical coherence and readability of our dataset and exhibit\nimproved performance on logical reasoning tasks. The enhanced reasoning\ncapabilities developed from VisualSphinx also benefit other reasoning tasks\nsuch as algebraic reasoning, arithmetic reasoning and geometry reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "zhangchenxu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23059",
      "authors": [
        {
          "_id": "683b4b86c4b9677f3f7125e5",
          "user": {
            "_id": "6683c3b04905815dcffe7a21",
            "avatarUrl": "/avatars/73c50843d99b6fb8b1ee8fe11106c4ce.svg",
            "isPro": false,
            "fullname": "Dohyeon Lee",
            "user": "waylight3",
            "type": "user"
          },
          "name": "Dohyeon Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:50:07.024Z",
          "hidden": false
        },
        {
          "_id": "683b4b86c4b9677f3f7125e6",
          "user": {
            "_id": "645aedd221ab438e732bff43",
            "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
            "isPro": false,
            "fullname": "Yeonseok Jeong",
            "user": "yeonseokjeong",
            "type": "user"
          },
          "name": "Yeonseok Jeong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:42:20.758Z",
          "hidden": false
        },
        {
          "_id": "683b4b86c4b9677f3f7125e7",
          "name": "Seung-won Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T04:04:25.000Z",
      "submittedOnDailyAt": "2025-06-03T01:42:51.938Z",
      "title": "De los tokens a acciones: Teoría de la representación de estados para reducir el exceso de consideración en la búsqueda de información.",
      "submittedOnDailyBy": {
        "_id": "645aedd221ab438e732bff43",
        "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
        "isPro": false,
        "fullname": "Yeonseok Jeong",
        "user": "yeonseokjeong",
        "type": "user"
      },
      "summary": "Chain-of-Thought (CoT) prompting permite a la generación de lógicas complejas en modelos de lenguaje grandes (LLMs). Esta tecnología también se puede aplicar en búsqueda de información (IR). Sin embargo, el modelo puede generar excesivamente largas y significativas oraciones, lo que puede afectar su rendimiento. Hemos identificado dos problemas importantes en IR: primero, el recorrido excesivamente largo para revisitar estados similares, y segundo, la lógica que se aleja de los deseos del usuario. Para resolver estos problemas, hemos propuesto la Reasoning con Máquina de Estados (SMR). SMR se realiza con acciones discretas (Refine, Rerank, Stop) que proporcionan rápida conclusión y control preciso. Los experimentos en BEIR y BRIGHT mostraron que SMR reduce el uso de tokens en un 74.4% y mejora el rendimiento de búsqueda en nDCG@10 en un 3.4%. SMR es generalizable tanto para LLMs como para modelos de búsqueda, y no requiere ajustes específicos para tareas particulares, proporcionando una versión práctica de CoT. Los códigos y detalles están disponibles en https://github.com/ldilab/SMR.",
      "upvotes": 7,
      "discussionId": "683b4b87c4b9677f3f712609",
      "githubRepo": "https://github.com/ldilab/SMR",
      "ai_summary": "State Machine Reasoning (SMR) improves information retrieval performance and reduces token usage in large language models by addressing overthinking through a discrete action framework.",
      "ai_keywords": [
        "Chain-of-Thought",
        "State Machine Reasoning",
        "IR",
        "redundant trajectories",
        "misguided reasoning",
        "early stopping",
        "nDCG@10"
      ]
    },
    "publishedAt": "2025-05-29T00:04:25.000Z",
    "title": "From Token to Action: State Machine Reasoning to Mitigate Overthinking\n  in Information Retrieval",
    "summary": "Chain-of-Thought (CoT) prompting enables complex reasoning in large language\nmodels (LLMs), including applications in information retrieval (IR). However,\nit often leads to overthinking, where models produce excessively long and\nsemantically redundant traces with little or no benefit. We identify two key\nchallenges in IR: redundant trajectories that revisit similar states and\nmisguided reasoning that diverges from user intent. To address these, we\npropose State Machine Reasoning (SMR), a transition-based reasoning framework\ncomposed of discrete actions (Refine, Rerank, Stop) that support early stopping\nand fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show\nthat SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token\nusage by 74.4%. It generalizes across LLMs and retrievers without requiring\ntask-specific tuning, offering a practical alternative to conventional CoT\nreasoning. The code and details are available at https://github.com/ldilab/SMR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23059.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645aedd221ab438e732bff43",
      "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
      "fullname": "Yeonseok Jeong",
      "name": "yeonseokjeong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01881",
      "authors": [
        {
          "_id": "683e6708ef9c250c6642783c",
          "user": {
            "_id": "65c431a609672feb8cac22e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c431a609672feb8cac22e7/obXCXUOgEoCuJbGuzYOGA.jpeg",
            "isPro": false,
            "fullname": "Yaoyao Qian",
            "user": "FreaxRuby",
            "type": "user"
          },
          "name": "Yaoyao Qian",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T03:07:55.037Z",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c6642783d",
          "name": "Jindan Huang",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c6642783e",
          "name": "Yuanli Wang",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c6642783f",
          "user": {
            "_id": "636681feaa6a4af6073ba73e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636681feaa6a4af6073ba73e/u_0moYNu6as9Sszp-ej95.png",
            "isPro": true,
            "fullname": "Simon Yu",
            "user": "simonycl",
            "type": "user"
          },
          "name": "Simon Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:06.559Z",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c66427840",
          "name": "Kyrie Zhixuan Zhou",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c66427841",
          "name": "Jiayuan Mao",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c66427842",
          "name": "Mingfu Liang",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c66427843",
          "name": "Hanhan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:11:10.000Z",
      "submittedOnDailyAt": "2025-06-03T01:38:44.220Z",
      "title": "Modelo de selección temporal de trajes estructurales para la posibilidad de trigger de intenciones en el diseño de diarios para tareas: momento de acción, momento de espera",
      "submittedOnDailyBy": {
        "_id": "65c431a609672feb8cac22e7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c431a609672feb8cac22e7/obXCXUOgEoCuJbGuzYOGA.jpeg",
        "isPro": false,
        "fullname": "Yaoyao Qian",
        "user": "FreaxRuby",
        "type": "user"
      },
      "summary": "TASK-oriented Dialogue Systems enfrentan dificultades cuando las solicitudes del usuario son gramaticalmente completas pero carecen de la información estructural necesaria para el comportamiento adecuado del sistema. Esto se debe a que el usuario no entiende completamente sus propios necesidades mientras el sistema requiere definir un objetivo específico. Los agentes basados en LLM actuales no pueden efectivamente distinguir entre lenguaje completo o texto que puede ser recuperado según el contexto, lo que hace que falte un marco para la formación colaborativa de la intención. Presentamos el marco STORM. Este marco modela la información simétrica y estructural en las conversaciones entre UserLLM (acceso completo interno) y AgentLLM (solo acciones observables). STORM genera un corpus que detecta el rastro de las representaciones y las transiciones cognitivas potenciales, y se desarrolla mediante un análisis sistemático del comprensión colaborativa. Nuestro contribución incluye: (1) la formalización del procesamiento de información simétrico en sistemas de diálogo, (2) el rastreo de la formación de la intención y el desarrollo de la comprensión colaborativa, y (3) un índice de evaluación que mide tanto el rendimiento de la tarea como el aumento del conocimiento cognitivo interno. Las pruebas experienciales con cuatro modelos de lenguaje revelaron que un moderado nivel de incertidumbre (40-60%) puede exceder la transparencia total en ciertos escaneadores, y los patrones propios de los modelos contribuyen al reevaluación de la completitud de la información más adecuada en colaboraciones entre humanos y IA. Estos hallazgos contribuyen a la dinámica de la justificación simétrica y proporcionan información sobre la diseño de sistemas de diálogo ajustados a la incertidumbre.",
      "upvotes": 6,
      "discussionId": "683e670bef9c250c664278be",
      "projectPage": "https://nanostorm.netlify.app/",
      "githubRepo": "https://github.com/H-Freax/Storm",
      "ai_summary": "STORM frameworks facilitates collaborative intent formation in task-oriented dialogue systems by modeling asymmetric information dynamics between UserLLM and AgentLLM.",
      "ai_keywords": [
        "UserLLM",
        "AgentLLM",
        "asymmetric information dynamics",
        "collaborative understanding",
        "intent formation",
        "expression trajectories",
        "latent cognitive transitions",
        "uncertainty-calibrated dialogue systems"
      ]
    },
    "publishedAt": "2025-06-02T13:11:10.000Z",
    "title": "WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent\n  Triggerability in Task-Oriented Dialogue",
    "summary": "Task-oriented dialogue systems often face difficulties when user utterances\nseem semantically complete but lack necessary structural information for\nappropriate system action. This arises because users frequently do not fully\nunderstand their own needs, while systems require precise intent definitions.\nCurrent LLM-based agents cannot effectively distinguish between linguistically\ncomplete and contextually triggerable expressions, lacking frameworks for\ncollaborative intent formation. We present STORM, a framework modeling\nasymmetric information dynamics through conversations between UserLLM (full\ninternal access) and AgentLLM (observable behavior only). STORM produces\nannotated corpora capturing expression trajectories and latent cognitive\ntransitions, enabling systematic analysis of collaborative understanding\ndevelopment. Our contributions include: (1) formalizing asymmetric information\nprocessing in dialogue systems; (2) modeling intent formation tracking\ncollaborative understanding evolution; and (3) evaluation metrics measuring\ninternal cognitive improvements alongside task performance. Experiments across\nfour language models reveal that moderate uncertainty (40-60%) can outperform\ncomplete transparency in certain scenarios, with model-specific patterns\nsuggesting reconsideration of optimal information completeness in human-AI\ncollaboration. These findings contribute to understanding asymmetric reasoning\ndynamics and inform uncertainty-calibrated dialogue system design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01881.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c431a609672feb8cac22e7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c431a609672feb8cac22e7/obXCXUOgEoCuJbGuzYOGA.jpeg",
      "fullname": "Yaoyao Qian",
      "name": "FreaxRuby",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01667",
      "authors": [
        {
          "_id": "683eaa8ed8d42fc832445ebd",
          "name": "Yan Shu",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ebe",
          "name": "Bin Ren",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ebf",
          "name": "Zhitong Xiong",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec0",
          "name": "Danda Pani Paudel",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec1",
          "name": "Luc Van Gool",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec2",
          "name": "Begum Demir",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec3",
          "name": "Nicu Sebe",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec4",
          "name": "Paolo Rota",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T13:36:05.000Z",
      "submittedOnDailyAt": "2025-06-03T06:26:41.885Z",
      "title": "EarthMind: EarthMind para la observación de la Tierra de granes de granidad y granes de sensibilidad con modelos multimodales grandes\n\nEstudio de la observación de la Tierra de granes de granidad y granes de sensibilidad para EarthMind con modelos multimodales grandes",
      "submittedOnDailyBy": {
        "_id": "65c4f99b27736b5b86c2cbda",
        "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
        "isPro": false,
        "fullname": "Yan Shu",
        "user": "sy1998",
        "type": "user"
      },
      "summary": "Los grandes modelos multimodales (LMMs) han demostrado un excelente rendimiento en diversas tareas de lenguaje visual. Sin embargo, la comprensión general de datos de observación de la tierra (EO) presenta dificultades, lo que es crucial para comprender el impacto de las actividades humanas y la observación del entorno. En este artículo, se presenta un nuevo marco de trabajo de lenguaje visual \"EarthMind\". EarthMind se centra en el entendimiento de datos EO de gran variedad de gran y pequeña escalas. Este marco contiene dos elementos clave: (1) Atención espacial de notación (SAP), que reorganiza la atención de modelos de lenguaje de grandes tamaños para fortalecer la comprensión a nivel de píxel; (2) Fusión multimodal, que ajusta modelos de diferentes modalidades en un espacio común, adaptando los tokens de acuerdo a la densidad de información para lograr una fusión efectiva. Para promover la evaluación de fusión multiescala, se propone EarthMind-Bench, un marco de prueba detallado. EarthMind-Bench incluye más de 2,000 pares de imágenes multiescala con etiquetas humanas, registrando una amplia gama de tareas de observación y lógica. Los experimentos extendidos demuestran la eficacia de EarthMind. En EarthMind-Bench, EarthMind alcanza los mejores resultados y puede superar a GPT-4o. Además, EarthMind supera a los métodos actuales en la mayoría de los benchmarks públicos de observación de la tierra. Estos resultados demuestran que EarthMind puede facilmente abordar desafíos de gran y pequeña escalas a través de un único marco de trabajo integrado.",
      "upvotes": 6,
      "discussionId": "683eaa94d8d42fc832446013",
      "githubRepo": "https://github.com/shuyansy/EarthMind",
      "ai_summary": "EarthMind is a vision-language framework that uses spatial attention prompting and cross-modal fusion for efficient multi-granular and multi-sensor Earth Observation data understanding, outperforming larger models on specialized benchmarks.",
      "ai_keywords": [
        "Spatial Attention Prompting",
        "Cross-modal Fusion",
        "Earth Observation",
        "multi-granular",
        "multi-sensor",
        "EarthMind-Bench"
      ]
    },
    "publishedAt": "2025-06-02T09:36:05.000Z",
    "title": "EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation\n  with Large Multimodal Models",
    "summary": "Large Multimodal Models (LMMs) have demonstrated strong performance in\nvarious vision-language tasks. However, they often struggle to comprehensively\nunderstand Earth Observation (EO) data, which is critical for monitoring the\nenvironment and the effects of human activity on it. In this work, we present\nEarthMind, a novel vision-language framework for multi-granular and\nmulti-sensor EO data understanding. EarthMind features two core components: (1)\nSpatial Attention Prompting (SAP), which reallocates attention within the LLM\nto enhance pixel-level understanding; and (2) Cross-modal Fusion, which aligns\nheterogeneous modalities into a shared space and adaptively reweighs tokens\nbased on their information density for effective fusion. To facilitate\nmulti-sensor fusion evaluation, we propose EarthMind-Bench, a comprehensive\nbenchmark with over 2,000 human-annotated multi-sensor image-question pairs,\ncovering a wide range of perception and reasoning tasks. Extensive experiments\ndemonstrate the effectiveness of EarthMind. It achieves state-of-the-art\nperformance on EarthMind-Bench, surpassing GPT-4o despite being only 4B in\nscale. Moreover, EarthMind outperforms existing methods on multiple public EO\nbenchmarks, showcasing its potential to handle both multi-granular and\nmulti-sensor challenges in a unified framework.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01667.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c4f99b27736b5b86c2cbda",
      "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
      "fullname": "Yan Shu",
      "name": "sy1998",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24625",
      "authors": [
        {
          "_id": "683e5569fce31842c60675d7",
          "user": {
            "_id": "646e2fcaf813cfe153f1af6c",
            "avatarUrl": "/avatars/2f87d0e5c071000990da29cd744bc03d.svg",
            "isPro": false,
            "fullname": "Duo Zheng",
            "user": "zd11024",
            "type": "user"
          },
          "name": "Duo Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:45.196Z",
          "hidden": false
        },
        {
          "_id": "683e5569fce31842c60675d8",
          "name": "Shijia Huang",
          "hidden": false
        },
        {
          "_id": "683e5569fce31842c60675d9",
          "name": "Yanyang Li",
          "hidden": false
        },
        {
          "_id": "683e5569fce31842c60675da",
          "name": "Liwei Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T14:16:41.000Z",
      "submittedOnDailyAt": "2025-06-03T00:24:09.506Z",
      "title": "El mundo 3D aprendido en vídeos: Mejorar MLLM con visión 3D\nGeometría de la propiedad",
      "submittedOnDailyBy": {
        "_id": "646e2fcaf813cfe153f1af6c",
        "avatarUrl": "/avatars/2f87d0e5c071000990da29cd744bc03d.svg",
        "isPro": false,
        "fullname": "Duo Zheng",
        "user": "zd11024",
        "type": "user"
      },
      "summary": "Anteriores estudios han investigado la aplicación de Multimodal Large Language Models (MLLMs) para comprender escenas 3D al leer videos. Esta metodología generalmente requiere la entrada de datos 3D, como clusters de puntos o mapas de vista aérea de la perspectiva de la ave (BEV) reconstruidos. En nuestro estudio, hemos mejorado estas investigaciones y ampliado la capacidad de los MLLMs para comprender y explicar el entendimiento de espacios 3D directamente desde los datos de video, mejorando este campo. Nuestro equipo de investigación ha propuesto un nuevo método efectivo llamado Video-3D Geometry Large Language Model (VG LLM). Nuestro enfoque utiliza un encoder de geometría visual 3D y extrae información de datos 3D a partir de secuencias de video. Esta información se integra con tokens visuales y se proporciona como entrada a los MLLMs. Los experimentos expandidos han demostrado significativas mejoras en nuestra metodología en diferentes tareas de comprensión de escenas 3D y razonamiento espacial. Lo más impresionante es que nuestro modelo de 4B no necesita la entrada explícita de datos 3D, obteniendo resultados competitivos en comparación con los mejores métodos actuales y superando a Gemini-1.5-Pro en la evaluación VSI-Bench.",
      "upvotes": 6,
      "discussionId": "683e556efce31842c6067737",
      "projectPage": "https://lavi-lab.github.io/VG-LLM/",
      "githubRepo": "https://github.com/LaVi-Lab/VG-LLM",
      "ai_summary": "A novel Video-3D Geometry Large Language Model (VG LLM) extracts 3D information directly from video sequences to enhance 3D scene understanding without additional 3D data, achieving competitive results in various tasks.",
      "ai_keywords": [
        "MLLMs",
        "Video-3D Geometry Large Language Model",
        "VG LLM",
        "3D visual geometry encoder",
        "VSI-Bench"
      ]
    },
    "publishedAt": "2025-05-30T10:16:41.000Z",
    "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors",
    "summary": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method, the Video-3D Geometry Large\nLanguage Model (VG LLM). Our approach employs a 3D visual geometry encoder that\nextracts 3D prior information from video sequences. This information is\nintegrated with visual tokens and fed into the MLLM. Extensive experiments have\nshown that our method has achieved substantial improvements in various tasks\nrelated to 3D scene understanding and spatial reasoning, all directly learned\nfrom video sources. Impressively, our 4B model, which does not rely on explicit\n3D data inputs, achieves competitive results compared to existing\nstate-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646e2fcaf813cfe153f1af6c",
      "avatarUrl": "/avatars/2f87d0e5c071000990da29cd744bc03d.svg",
      "fullname": "Duo Zheng",
      "name": "zd11024",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01413",
      "authors": [
        {
          "_id": "683e6aa057738c5cc3616d70",
          "user": {
            "_id": "6390525c00fb8ec4a424e0ff",
            "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
            "isPro": false,
            "fullname": "Yulei Qin",
            "user": "yolay",
            "type": "user"
          },
          "name": "Yulei Qin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T03:23:18.288Z",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d71",
          "name": "Gang Li",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d72",
          "name": "Zongyi Li",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d73",
          "name": "Zihan Xu",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d74",
          "name": "Yuchen Shi",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d75",
          "name": "Zhekai Lin",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d76",
          "name": "Xiao Cui",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d77",
          "name": "Ke Li",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d78",
          "name": "Xing Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T08:11:44.000Z",
      "submittedOnDailyAt": "2025-06-03T02:06:35.145Z",
      "title": "Programa de Riesgo de Inclinación de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de Riesgo de",
      "submittedOnDailyBy": {
        "_id": "6390525c00fb8ec4a424e0ff",
        "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
        "isPro": false,
        "fullname": "Yulei Qin",
        "user": "yolay",
        "type": "user"
      },
      "summary": "Los grandes modelos de lenguaje actuales (LLMs) presentan problemas cuando se les da un comando complejo, especialmente cuando estas restricciones son dispuestas de manera secuencial y estructuradas con puntos de corte. Se espera que la técnica de \"chain-of-thought\" (CoT) pueda mejorar significativamente las capacidades de los LLMs. Sin embargo, la CoT basada en patrones superficiales de reescritura simple de las instrucciones tiene un impacto negativo en el rendimiento. Esto se debe a la falta de capacidad para identificar y manejar las relaciones entre restricciones de tipo y dimensión.\n\nEn este contexto, proponemos un método para mejorar la capacidad de los LLMs para procesar comandos complejos mediante escalado computacional para inducir razonamiento en el procesamiento de instrucciones. Primero, basándonos en la tecnología actual, proponemos una metodología para obtener datos representativos de comandos complejos. Luego, utilizamos un algoritmo de aprendizaje por refuerzo centrado en reglas provable (RL) para fomentar la formación de razones en las instrucciones. Para abordar las características superficiales y no relevantes de las razones de comandos complejos, utilizamos contrastes a nivel de muestra para imponer la necesidad de una CoT superior. Además, promovemos una transición estable hacia una distribución de razones familiares a partir de una rápida respuesta de los LLMs utilizando el cróning de acciones de expertos.\n\nUna evaluación ampliada en 7 marcos de referencia confirma la efectividad de nuestro método, demostrando que un LLM de 1.5B puede alcanzar el rendimiento de un LLM de 8B con un aumento de 11.74%. Los códigos y datos están disponibles en https://github.com/yuleiqin/RAIF.",
      "upvotes": 5,
      "discussionId": "683e6aa657738c5cc3616ecc",
      "projectPage": "https://huggingface.co/collections/yolay/raif-arxivorg-pdf-250601413-682b16e5c0c2fa9b73811369",
      "githubRepo": "https://github.com/yuleiqin/RAIF",
      "ai_summary": "A method is proposed to enhance large language models in handling complex instructions through incentivized reasoning and reinforcement learning, improving performance and reducing computational load.",
      "ai_keywords": [
        "chain-of-thought (CoT)",
        "reinforcement learning (RL)",
        "rule-centric reward signals",
        "sample-wise contrast",
        "behavior cloning",
        "instruction following",
        "decomposition of complex instructions"
      ]
    },
    "publishedAt": "2025-06-02T04:11:44.000Z",
    "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models",
    "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nare available at https://github.com/yuleiqin/RAIF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01413.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6390525c00fb8ec4a424e0ff",
      "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
      "fullname": "Yulei Qin",
      "name": "yolay",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24452",
      "authors": [
        {
          "_id": "683e8abd8e6e97efe0bf20d9",
          "name": "Anda Tang",
          "hidden": false
        },
        {
          "_id": "683e8abd8e6e97efe0bf20da",
          "name": "Yiming Dong",
          "hidden": false
        },
        {
          "_id": "683e8abd8e6e97efe0bf20db",
          "user": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "isPro": false,
            "fullname": "Yutao Zeng",
            "user": "Taoer",
            "type": "user"
          },
          "name": "Yutao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:39:15.864Z",
          "hidden": false
        },
        {
          "_id": "683e8abd8e6e97efe0bf20dc",
          "name": "zhou Xun",
          "hidden": false
        },
        {
          "_id": "683e8abd8e6e97efe0bf20dd",
          "name": "Zhouchen Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/GfsrISs7G7K3rMBB1uws5.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/eQqbJxhzxMNKVlyoqQPRA.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/q1H4zp_jhjZw57UYMnECl.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/TpjaRwRNbGOzWfL7y5C4P.png"
      ],
      "publishedAt": "2025-05-30T10:38:03.000Z",
      "submittedOnDailyAt": "2025-06-03T04:32:00.766Z",
      "title": "Uno de los tamaños de paso: el aprendizaje de rate de procesamiento de flujo de trabajo integrado",
      "submittedOnDailyBy": {
        "_id": "6371128eafbe42caa5a5222b",
        "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
        "isPro": false,
        "fullname": "Yutao Zeng",
        "user": "Taoer",
        "type": "user"
      },
      "summary": "El aumento de costos computacionales y la restricción de recursos ha resaltado la importancia del aprendizaje con presupuesto asignado. Este enfoque está diseñado para lograr el aprendizaje óptimo dentro de un conjunto de entrenamiento reservado. El ajuste del learning rate es la variable que domina el rendimiento de diferentes redes y tareas, especialmente en entrenamientos con presupuesto limitado, donde la disección es principalmente heurística y carece de una base teórica sólida. Además, la elección del learning rate óptimo se basa en una gran cantidad de pruebas, lo que reduce la eficiencia del proceso de entrenamiento. En este artículo, se propone un ajuste del learning rate teóricamente fundamentado que muestra un rendimiento excelente en diferentes arquitecturas y tareas, incluso bajo un presupuesto de entrenamiento limitado. Primero, se construye un nuevo marco de optimización relacionado con el presupuesto y se considera la robustez frente a la variación de curvas. En este marco, se puede obtener un ajuste del learning rate Unificado con Presupuesto (UBA) teóricamente fundamentado. El UBA está controlado por un parámetro único φ, proporcionando un equilibrio entre flexibilidad y simplicidad, y reduciendo la necesidad de optimización numérica para cada red. Además, se establece la relación teórica entre φ y el condicional, justificando nuestro enfoque. Se prueba la convergencia para diferentes valores de φ. Basándose en análisis teórico y resultados experimentales, se proporcionan guías prácticas para la elección de φ. A través de experimentos amplios, se muestra que el UBA supera a otros generales de entrenamiento en una amplia gama de tareas visuales y lingüísticas, diferentes arquitecturas de red (como ResNet y OLMo) y diferentes presupuestos de iteraciones de entrenamiento.",
      "upvotes": 5,
      "discussionId": "683e8abf8e6e97efe0bf2155",
      "ai_summary": "A unified budget-aware learning rate schedule is proposed to optimize training within limited iteration budgets, outperforming traditional schedules across various tasks and network architectures.",
      "ai_keywords": [
        "budgeted-iteration training",
        "learning rate schedules",
        "Unified Budget-Aware (UBA) schedule",
        "training budget-aware optimization framework",
        "robustness to landscape curvature variations",
        "condition number",
        "convergence",
        "ResNet",
        "OLMo"
      ]
    },
    "publishedAt": "2025-05-30T06:38:03.000Z",
    "title": "Stepsize anything: A unified learning rate schedule for\n  budgeted-iteration training",
    "summary": "The expanding computational costs and limited resources underscore the\ncritical need for budgeted-iteration training, which aims to achieve optimal\nlearning within predetermined iteration budgets.While learning rate schedules\nfundamentally govern the performance of different networks and tasks,\nparticularly in budgeted-iteration scenarios, their design remains largely\nheuristic, lacking theoretical foundations.In addition, the optimal learning\nrate schedule requires extensive trial-and-error selection, making the training\nprocess inefficient.In this work, we propose the Unified Budget-Aware (UBA)\nschedule, a theoretically grounded learning rate schedule that consistently\noutperforms commonly-used schedules among diverse architectures and tasks under\ndifferent constrained training budgets.First, we bridge the gap by constructing\na novel training budget-aware optimization framework, which explicitly accounts\nfor the robustness to landscape curvature variations.From this framework, we\nderive the UBA schedule, controlled by a single hyper-parameter varphi that\nprovides a trade-off between flexibility and simplicity, eliminating the need\nfor per-network numerical optimization. Moreover, we establish a theoretical\nconnection between varphi and the condition number, adding interpretation\nand justification to our approach. Besides, we prove the convergence for\ndifferent values of varphi.We offer practical guidelines for its selection\nvia theoretical analysis and empirical results.xtensive experimental results\nshow that UBA consistently surpasses the commonly-used schedules\nacross diverse vision and language tasks, spanning network architectures (e.g.,\nResNet, OLMo) and scales, under different training-iteration budgets.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/GfsrISs7G7K3rMBB1uws5.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/eQqbJxhzxMNKVlyoqQPRA.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/q1H4zp_jhjZw57UYMnECl.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/TpjaRwRNbGOzWfL7y5C4P.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24452.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6371128eafbe42caa5a5222b",
      "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
      "fullname": "Yutao Zeng",
      "name": "Taoer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00338",
      "authors": [
        {
          "_id": "683e64eb3e5a54d05365ddc6",
          "user": {
            "_id": "61809f31a367a8f5351ef353",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61809f31a367a8f5351ef353/s5eQ00YeoirakzE_rJ0cy.jpeg",
            "isPro": false,
            "fullname": "Yifan Peng",
            "user": "pyf98",
            "type": "user"
          },
          "name": "Yifan Peng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:09.029Z",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddc7",
          "name": "Shakeel Muhammad",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddc8",
          "name": "Yui Sudo",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddc9",
          "name": "William Chen",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddca",
          "name": "Jinchuan Tian",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddcb",
          "name": "Chyi-Jiunn Lin",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddcc",
          "name": "Shinji Watanabe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T01:44:44.000Z",
      "submittedOnDailyAt": "2025-06-03T01:39:05.927Z",
      "title": "OWSM v4: Escalamiento de datos y ajustes para modelos de voz de tipo Open Whisper",
      "submittedOnDailyBy": {
        "_id": "61809f31a367a8f5351ef353",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61809f31a367a8f5351ef353/s5eQ00YeoirakzE_rJ0cy.jpeg",
        "isPro": false,
        "fullname": "Yifan Peng",
        "user": "pyf98",
        "type": "user"
      },
      "summary": "Los modelos de habla de estilo Open Whisper (OWSM) del proyecto utilizan recursos académicos para desarrollar modelos completamente abiertos de habla basados en voz, pero la data de entrenamiento no es suficiente. Este artículo fortalece a los OWSM mediante la integración de un gran conjunto de datos de web crawling bajo la licencia Creative Commons, llamado YODAS. Sin embargo, la integración de YODAS presenta problemas como etiquetas de lenguaje negativo y desacoplamiento entre voz y texto. Para abordar estos desafíos, se desarrolló una pipeline de limpieza de datos escalable utilizando un paquete de herramientas abierto, y se recopiló un conjunto de datos de 166,000 horas de voz en 75 idiomas. Con este conjunto de datos de Cambodia y los datos existentes de OWSM, se entrenó un nuevo modelo OWSM v4 que mejora significativamente sobre las versiones anteriores y supera notablemente en los benchmarks multilingües. Además, el modelo alcanza el frente de la línea en varios escenarios comparado con modelos industriales avanzados como Whisper y MMS. Debido a su publicación, se publican en línea los datos limpios, modelos preentrenados y todos los criptográficos relacionados a través del paquete de herramientas ESPnet.",
      "upvotes": 4,
      "discussionId": "683e64ec3e5a54d05365ddee",
      "projectPage": "https://www.wavlab.org/activities/2024/owsm/",
      "githubRepo": "https://github.com/espnet/espnet",
      "ai_summary": "The OWSM project is enhanced with a large-scale, cleaned web dataset, leading to improved multilingual speech models comparable to leading industrial models.",
      "ai_keywords": [
        "speach foundation models",
        "YODAS",
        "data-cleaning pipeline",
        "multilingual benchmarks",
        "Whisper",
        "MMS",
        "ESPnet toolkit"
      ]
    },
    "publishedAt": "2025-05-30T21:44:44.000Z",
    "title": "OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and\n  Cleaning",
    "summary": "The Open Whisper-style Speech Models (OWSM) project has developed a series of\nfully open speech foundation models using academic-scale resources, but their\ntraining data remains insufficient. This work enhances OWSM by integrating\nYODAS, a large-scale web-crawled dataset with a Creative Commons license.\nHowever, incorporating YODAS is nontrivial due to its wild nature, which\nintroduces challenges such as incorrect language labels and audio-text\nmisalignments. To address this, we develop a scalable data-cleaning pipeline\nusing public toolkits, yielding a dataset with 166,000 hours of speech across\n75 languages. Our new series of OWSM v4 models, trained on this curated dataset\nalongside existing OWSM data, significantly outperform previous versions on\nmultilingual benchmarks. Our models even match or surpass frontier industrial\nmodels like Whisper and MMS in multiple scenarios. We will publicly release the\ncleaned YODAS data, pre-trained models, and all associated scripts via the\nESPnet toolkit.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61809f31a367a8f5351ef353",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61809f31a367a8f5351ef353/s5eQ00YeoirakzE_rJ0cy.jpeg",
      "fullname": "Yifan Peng",
      "name": "pyf98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24183",
      "authors": [
        {
          "_id": "683e9a174de2ca71b8bc915b",
          "user": {
            "_id": "67de68f4f38795c545310088",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8604Li6_OlATOsTdY9oHL.png",
            "isPro": false,
            "fullname": "Yaoyu Zhu",
            "user": "zhuyaoyu",
            "type": "user"
          },
          "name": "Yaoyu Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:39:09.237Z",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc915c",
          "user": {
            "_id": "62c581177b48ba0bb8cdb737",
            "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
            "isPro": false,
            "fullname": "di huang",
            "user": "dihuang",
            "type": "user"
          },
          "name": "Di Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:37:07.655Z",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc915d",
          "name": "Hanqi Lyu",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc915e",
          "name": "Xiaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc915f",
          "name": "Chongxiao Li",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9160",
          "name": "Wenxuan Shi",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9161",
          "name": "Yutong Wu",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9162",
          "name": "Jianan Mu",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9163",
          "name": "Jinghua Wang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9164",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9165",
          "name": "Pengwei Jin",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9166",
          "name": "Shuyao Cheng",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9167",
          "name": "Shengwen Liang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9168",
          "name": "Xishan Zhang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9169",
          "name": "Rui Zhang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc916a",
          "name": "Zidong Du",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc916b",
          "name": "Qi Guo",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc916c",
          "name": "Xing Hu",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc916d",
          "name": "Yunji Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T03:51:06.000Z",
      "submittedOnDailyAt": "2025-06-03T05:58:04.258Z",
      "title": "CodeV-R1: Creación de Verilog con Notas de Motivo",
      "submittedOnDailyBy": {
        "_id": "62c581177b48ba0bb8cdb737",
        "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
        "isPro": false,
        "fullname": "di huang",
        "user": "dihuang",
        "type": "user"
      },
      "summary": "Los grandes modelos de lenguaje (LLMs) han demostrado su eficacia en el aprendizaje reforzado basado en recompensas verificables (RLVR), logrando avances en tareas claras y automáticamente verifiables como el programación de software o la resolución de problemas matemáticos. Estos resultados han sido extendidos al campo de la automatización de diseño electrónico (EDA), con un enfoque especial en la generación automática de lenguajes de representación de hardware (HDL) como Verilog a partir de reglas de lenguaje natural (NL). Sin embargo, se han identificado tres problemas principales: la escasez de entornos de verificación precisa y automática, la falta de pares de código NL de alta calidad, y el alto costo computacional del RLVR. Para abordar estos desafíos, se presenta el marco de RLVR llamado CodeV-R1. Primero, se desarrolla un generador de bancos de prueba de lógica de base de datos que ejecuta referencias monetarias y fuertes cheques de igualdad. A continuación, se combinan snapshots de Verilog de fuente abierta con explicaciones de NL generadas por un modelo de lenguaje, para verificar la coincidencia entre el código, el NL y el código, y eliminar ejemplos no iguales para generar conjuntos de datos de alta calidad. Finalmente, se utiliza una pipeline de entrenamiento en dos etapas de «RL después de destilación»: se realiza la destilación inicial de habilidades, y se utiliza un nuevo algoritmo de RLVR (DAPO) para ajustar automáticamente las tasas de muestreo, reduciendo así los costos de entrenamiento. Basado en esto, CodeV-R1-7B alcanza un paso de 68.6% en VerilogEval v2 y un paso de 72.9% en RTLLM v1.1, superando los mejores resultados previos en un rango de 12 a 20% y potencialmente superando el rendimiento de CodeV-R1-671B. Se publica el modelo, la pipeline de entrenamiento y los conjuntos de datos para apoyar la investigación de la comunidad de EDA y de modelos de lenguaje natural.",
      "upvotes": 4,
      "discussionId": "683e9a184de2ca71b8bc91b3",
      "projectPage": "https://iprc-dip.github.io/CodeV-R1",
      "ai_summary": "CodeV-R1, an RLVR framework for Verilog generation, achieves state-of-the-art performance in EDA using a rule-based testbench, round-trip data synthesis, and adaptive RLVR training.",
      "ai_keywords": [
        "reinforcement learning with verifiable reward",
        "RLVR",
        "electronic design automation",
        "EDA",
        "hardware description languages",
        "HDLs",
        "Verilog",
        "natural-language",
        "NL",
        "testbench generator",
        "equivalence checking",
        "round-trip data synthesis",
        "dataset",
        "two-stage training pipeline",
        "distillation",
        "adaptive DAPO",
        "VerilogEval",
        "RTLLM"
      ]
    },
    "publishedAt": "2025-05-29T23:51:06.000Z",
    "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
    "summary": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24183.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c581177b48ba0bb8cdb737",
      "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
      "fullname": "di huang",
      "name": "dihuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23504",
      "authors": [
        {
          "_id": "683e501c89dc42ba0515a4d8",
          "name": "Liyun Zhu",
          "hidden": false
        },
        {
          "_id": "683e501c89dc42ba0515a4d9",
          "name": "Qixiang Chen",
          "hidden": false
        },
        {
          "_id": "683e501c89dc42ba0515a4da",
          "name": "Xi Shen",
          "hidden": false
        },
        {
          "_id": "683e501c89dc42ba0515a4db",
          "name": "Xiaodong Cun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T14:48:10.000Z",
      "submittedOnDailyAt": "2025-06-03T00:01:06.695Z",
      "title": "VAU-R1: Mejorar la comprensión de la detección de anomalías utilizando aprendizaje por refuerzo",
      "submittedOnDailyBy": {
        "_id": "63184c517ca1b876d99b7e0e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
        "isPro": false,
        "fullname": "Xiaodong Cun",
        "user": "vinthony",
        "type": "user"
      },
      "summary": "Video Anomaly Understanding (VAU) es crucial en diversas aplicaciones como ciudades inteligentes, vigilancia segura y sistemas de alerta de desastres, pero se enfrenta desafíos debido a la necesidad de reconocer detalles espaciales y temporales y de soportar lógicas robustas en situaciones complejas. Los métodos actuales presentan limitaciones en la interpretabilidad y en la comprensión facil de los causas y contextos de los eventos anormales. Estos limitaciones se acentúan debido a la falta de marcos de referencia detallados para evaluar la capacidad teórica de la detección de anomalías. Para enfrentar estos desafíos, presentamos VAU-R1. VAU-R1 es un marco eficiente basado en modelos de lenguaje multimodal grandes (MLLMs), que fortalece la teoría lógica de anomalías a través de fine-tuning por reforzamiento. Además, proponemos VAU-Bench. VAU-Bench es un marco de referencia de tipo Chain-of-Thought adecuado para la teoría lógica de anomalías, caracterizado por preguntas múltiples, razones detalladas, notaciones temporales y capturas explicativas. Los resultados de los experimentos muestran que VAU-R1 significativamente mejora la precisión de las respuestas a las preguntas, la crecimiento temporal y la coherencia de las razones en diferentes contextos. Estos métodos y marcos de referencia establecen una sólida base para la comprensión lógica explicable de imágenes con anomalías. El código está disponible en https://github.com/GVCLab/VAU-R1.",
      "upvotes": 4,
      "discussionId": "683e502189dc42ba0515a5e1",
      "ai_summary": "VAU-R1 uses Multimodal Large Language Models with Reinforcement Fine-Tuning to enhance video anomaly reasoning, complemented by VAU-Bench, a Chain-of-Thought benchmark for evaluating anomaly understanding.",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Reinforcement Fine-Tuning (RFT)",
        "Chain-of-Thought",
        "benchmark",
        "question answering",
        "temporal grounding",
        "reasoning coherence"
      ]
    },
    "publishedAt": "2025-05-29T10:48:10.000Z",
    "title": "VAU-R1: Advancing Video Anomaly Understanding via Reinforcement\n  Fine-Tuning",
    "summary": "Video Anomaly Understanding (VAU) is essential for applications such as smart\ncities, security surveillance, and disaster alert systems, yet remains\nchallenging due to its demand for fine-grained spatio-temporal perception and\nrobust reasoning under ambiguity. Despite advances in anomaly detection,\nexisting methods often lack interpretability and struggle to capture the causal\nand contextual aspects of abnormal events. This limitation is further\ncompounded by the absence of comprehensive benchmarks for evaluating reasoning\nability in anomaly scenarios. To address both challenges, we introduce VAU-R1,\na data-efficient framework built upon Multimodal Large Language Models (MLLMs),\nwhich enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT).\nBesides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored\nfor video anomaly reasoning, featuring multiple-choice QA, detailed rationales,\ntemporal annotations, and descriptive captions. Empirical results show that\nVAU-R1 significantly improves question answering accuracy, temporal grounding,\nand reasoning coherence across diverse contexts. Together, our method and\nbenchmark establish a strong foundation for interpretable and reasoning-aware\nvideo anomaly understanding. Our code is available at\nhttps://github.com/GVCLab/VAU-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23504.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63184c517ca1b876d99b7e0e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
      "fullname": "Xiaodong Cun",
      "name": "vinthony",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 323
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21179",
      "authors": [
        {
          "_id": "6838c66dc60fb2fc462cec9f",
          "user": {
            "_id": "64d0eb731ed6649d70afb136",
            "avatarUrl": "/avatars/4c591c86c575c82760126c39af5a02b4.svg",
            "isPro": true,
            "fullname": "Chen Dar-Yen",
            "user": "ChenDY",
            "type": "user"
          },
          "name": "Dar-Yen Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:47:40.163Z",
          "hidden": false
        },
        {
          "_id": "6838c66dc60fb2fc462ceca0",
          "user": {
            "_id": "638c81fa61eb51017518fa31",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/f0eCrzBrxz7Y9n25WkZ2v.png",
            "isPro": false,
            "fullname": "Hmrishav Bandyopadhyay",
            "user": "Hmrishav",
            "type": "user"
          },
          "name": "Hmrishav Bandyopadhyay",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T13:40:02.354Z",
          "hidden": false
        },
        {
          "_id": "6838c66dc60fb2fc462ceca1",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "6838c66dc60fb2fc462ceca2",
          "name": "Yi-Zhe Song",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64d0eb731ed6649d70afb136/BtItSeWg7RJJDSWqnZWIB.mp4"
      ],
      "publishedAt": "2025-05-27T13:30:46.000Z",
      "submittedOnDailyAt": "2025-06-03T07:58:27.904Z",
      "title": "Normativación de atención: guías generales negativas para modelos de difusión",
      "submittedOnDailyBy": {
        "_id": "64d0eb731ed6649d70afb136",
        "avatarUrl": "/avatars/4c591c86c575c82760126c39af5a02b4.svg",
        "isPro": true,
        "fullname": "Chen Dar-Yen",
        "user": "ChenDY",
        "type": "user"
      },
      "summary": "Guia de rutinas - la supresión explícita de características de insatisfacción - es un problema básico en modelos de expansión, especialmente bajo una riqueza de muestras muy baja. La guía de clase libre de paso (CFG) es efectiva en generales, pero falla bajo la compresión de pasos estrictos cuando la predicción por riqueza se deteriora. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pasos estrictos cuando la predicción por riqueza baja. La predicción por riqueza baja bajo la compresión de pas",
      "upvotes": 4,
      "discussionId": "6838c673c60fb2fc462cee10",
      "projectPage": "https://chendaryen.github.io/NAG.github.io/",
      "githubRepo": "https://github.com/ChenDarYen/Normalized-Attention-Guidance",
      "ai_summary": "Normalized Attention Guidance (NAG) enhances diffusion models by providing effective negative guidance across regimes and modalities without retraining.",
      "ai_keywords": [
        "negative guidance",
        "diffusion models",
        "few-step sampling",
        "Classifier-Free Guidance (CFG)",
        "Normalized Attention Guidance (NAG)",
        "attention space",
        "L1-based normalization",
        "extrapolation",
        "fidelity",
        "CLIP Score",
        "FID",
        "PFID",
        "ImageReward",
        "UNet",
        "DiT",
        "image",
        "video",
        "model-agnostic inference-time approach"
      ]
    },
    "publishedAt": "2025-05-27T09:30:46.000Z",
    "title": "Normalized Attention Guidance: Universal Negative Guidance for Diffusion\n  Model",
    "summary": "Negative guidance -- explicitly suppressing unwanted attributes -- remains a\nfundamental challenge in diffusion models, particularly in few-step sampling\nregimes. While Classifier-Free Guidance (CFG) works well in standard settings,\nit fails under aggressive sampling step compression due to divergent\npredictions between positive and negative branches. We present Normalized\nAttention Guidance (NAG), an efficient, training-free mechanism that applies\nextrapolation in attention space with L1-based normalization and refinement.\nNAG restores effective negative guidance where CFG collapses while maintaining\nfidelity. Unlike existing approaches, NAG generalizes across architectures\n(UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image,\nvideo), functioning as a universal plug-in with minimal computational\noverhead. Through extensive experimentation, we demonstrate consistent\nimprovements in text alignment (CLIP Score), fidelity (FID, PFID), and\nhuman-perceived quality (ImageReward). Our ablation studies validate each\ndesign component, while user studies confirm significant preference for\nNAG-guided outputs. As a model-agnostic inference-time approach requiring no\nretraining, NAG provides effortless negative guidance for all modern diffusion\nframeworks -- pseudocode in the Appendix!",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64d0eb731ed6649d70afb136/BtItSeWg7RJJDSWqnZWIB.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21179.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64d0eb731ed6649d70afb136",
      "avatarUrl": "/avatars/4c591c86c575c82760126c39af5a02b4.svg",
      "fullname": "Chen Dar-Yen",
      "name": "ChenDY",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01084",
      "authors": [
        {
          "_id": "683ea4388be2e40086ea9056",
          "name": "Saibo Geng",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea9057",
          "user": {
            "_id": "6420afc71ccd411979dc12dc",
            "avatarUrl": "/avatars/ee4a89ebc7a0716e21deaebc86e062e6.svg",
            "isPro": false,
            "fullname": "nathan ranchin",
            "user": "nathanrchn",
            "type": "user"
          },
          "name": "Nathan Ranchin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T09:39:27.313Z",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea9058",
          "name": "Yunzhen yao",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea9059",
          "name": "Maxime Peyrard",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea905a",
          "name": "Chris Wendler",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea905b",
          "name": "Michael Gastpar",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea905c",
          "name": "Robert West",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/VnqCCx4RG5-62le1tQcaW.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/EGD7DC-lDXkTXOLD6r0IP.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/YDPF1VlfQ0Yfynq9peK5s.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/QY0t5DXqhIVBWrLpJvZm8.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/af0uwzA8bV-7byretzGwW.png"
      ],
      "publishedAt": "2025-06-01T17:03:02.000Z",
      "submittedOnDailyAt": "2025-06-03T06:05:31.112Z",
      "title": "zip2zip: La colección de palabras aplicable en la inferencia de modelos de lenguaje se proporciona mediante la compresión de tokens.",
      "submittedOnDailyBy": {
        "_id": "5fce0cfeb3dbf216ad31836a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625748458774-5fce0cfeb3dbf216ad31836a.png",
        "isPro": false,
        "fullname": "Saibo-creator",
        "user": "Saibo-creator",
        "type": "user"
      },
      "summary": "La eficiencia de la tokenización en el análisis de conversaciones desempeña un papel crucial en el rendimiento y costos de modelos de lenguaje grandes (LLMs). Sin embargo, la mayoría de los modelos utilizan un tokenizador fijo con un vocabulario predefinido optimizado para un corpus general. Este vocabulario fijo no se adapta adecuadamente a entradas específicas de campos o lenguajes, lo que contribuye a aumentar la longitud de las secuencias de tokens, lo que incrementa los costos de cálculo. Presentamos el framework zip2zip. Este framework permite que los LLMs ajusten dinámicamente el vocabulario de tokens durante la inferencia, reduciendo las tokens generadas y acelerando la velocidad de inferencia. El zip2zip está compuesto por tres componentes importantes: 1) un tokenizador basado en la compresión Lempel-Ziv-Welch (LZW), que comprime tokens dinámicamente y los convierte en 'hyper tokens' reutilizables. 2) una capa de embedding para calcular embeddings para los nuevos hyper tokens. 3) una variante del modelado de lenguaje causal que opera con secuencias compresas de hyper tokens. Demostramos que con zip2zip, el tratamiento de modelos LLMs anteriores requiere 10 horas de GPU. Como resultado, los modelos LLMs con zip2zip pueden reducir la longitud de las secuencias de entrada y salida en un 20-60% mediante el uso de hyper tokens, mejorando significativamente la latencia de inferencia.",
      "upvotes": 3,
      "discussionId": "683ea4398be2e40086ea90b7",
      "githubRepo": "https://github.com/epfl-dlab/zip2zip",
      "ai_summary": "A framework called zip2zip dynamically adjusts token vocabulary in LLMs at inference time using LZW compression, reducing token sequence length and improving inference speed.",
      "ai_keywords": [
        "LZW compression",
        "hypertokens",
        "embedding layer",
        "causal language modeling",
        "parameter-efficient finetuning"
      ]
    },
    "publishedAt": "2025-06-01T13:03:02.000Z",
    "title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via\n  Token Compression",
    "summary": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized for general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a\nframework that enables LLMs to dynamically adjust token vocabulary at inference\ntime, allowing for fewer generated tokens and thus faster inference. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\n(LZW) compression that incrementally compresses tokens into reusable\n\"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for\nnewly formed hypertokens at runtime; and (3) a causal language modeling variant\nthat trains the model to operate on hypertokenized, compressed sequences. We\nshow that an existing LLM can be zip2zip-fied in 10 GPU-hours via\nparameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to\nuse hypertokens at inference time, reducing input and output sequence length by\n20-60\\%, with significant improvements in inference latency.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/VnqCCx4RG5-62le1tQcaW.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/EGD7DC-lDXkTXOLD6r0IP.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/YDPF1VlfQ0Yfynq9peK5s.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/QY0t5DXqhIVBWrLpJvZm8.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/af0uwzA8bV-7byretzGwW.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fce0cfeb3dbf216ad31836a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625748458774-5fce0cfeb3dbf216ad31836a.png",
      "fullname": "Saibo-creator",
      "name": "Saibo-creator",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00643",
      "authors": [
        {
          "_id": "683e61338ebad8b7519bc7f3",
          "user": {
            "_id": "63e3f57754f51ea342ce26be",
            "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
            "isPro": false,
            "fullname": "Weijie Xu",
            "user": "xwjzds",
            "type": "user"
          },
          "name": "Weijie Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T02:43:01.430Z",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f4",
          "name": "Shixian Cui",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f5",
          "name": "Xi Fang",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f6",
          "name": "Chi Xue",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f7",
          "name": "Stephanie Eckman",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f8",
          "name": "Chandan Reddy",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/lDZvyKeHC66Fk1-x6VgbB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/l-YQ7eiVk7rb7n3Hi51p5.png"
      ],
      "publishedAt": "2025-05-31T17:14:21.000Z",
      "submittedOnDailyAt": "2025-06-03T01:22:58.893Z",
      "title": "SATA-BENCH: Benchmark de Multiselección\nPregunta",
      "submittedOnDailyBy": {
        "_id": "63e3f57754f51ea342ce26be",
        "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
        "isPro": false,
        "fullname": "Weijie Xu",
        "user": "xwjzds",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) aumentan su evaluación en tareas de respuestas múltiples de respuesta única, pero se requiere específicar todas las respuestas correctas para resolver muchos problemas de la realidad. Este tipo de capacidad aún está poco investigado. Presentamos SATA-BENCH, el primer marco de referencia profesional para evaluar preguntas de selección múltiple (SATA), para evaluar LLMs en diferentes áreas como lectura, derecho y bioinformática. Según la evaluación de 27 modelos abierto fuente y de derechos de autor, se detectaron significativas errores: incluso los modelos más potentes no tienen un alto porcentaje de coincidencia correcta, es decir, LLMs no pueden confiar en identificar todas las respuestas correctas. Esta limitación genera dos problemas esenciales: sesgo de selección - los modelos prefieren una opción sin importar el contenido, y sesgo de contador - los modelos no pueden predecir la cantidad de respuestas correctas. Para resolver estos problemas, proponemos Choice Funnel, un modelo de interpretación que combina sesgo de token y discriminación adaptativa. Choice Funnel puede aumentar la coincidencia correcta en un 29% comparativamente y reducir los costos de inferencia en más del 64%. Esta descubrimiento claramente demuestra las limitaciones básicas de los LLMs y introduce un nuevo marco de trabajo para diagnosticar y mejorar la lógica de múltiples respuestas. La publicación de SATA-BENCH y Choice Funnel fomenta el desarrollo de LLMs para tomar decisiones fuertes en diversas aplicaciones de respuestas múltiples.",
      "upvotes": 3,
      "discussionId": "683e61358ebad8b7519bc8cf",
      "githubRepo": "https://github.com/sata-bench/sata-bench",
      "ai_summary": "SATA-BENCH evaluates LLMs on multi-answer questions, revealing selections biases and proposing Choice Funnel to improve accuracy and reduce costs in multi-answer reasoning tasks.",
      "ai_keywords": [
        "Select All That Apply (SATA) questions",
        "SATA-BENCH",
        "token debiasing",
        "adaptive thresholding",
        "Choice Funnel"
      ]
    },
    "publishedAt": "2025-05-31T13:14:21.000Z",
    "title": "SATA-BENCH: Select All That Apply Benchmark for Multiple Choice\n  Questions",
    "summary": "Large language models (LLMs) are increasingly evaluated on single-answer\nmultiple-choice tasks, yet many real-world problems require identifying all\ncorrect answers from a set of options. This capability remains underexplored.\nWe introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on\nSelect All That Apply (SATA) questions across diverse domains, including\nreading comprehension, law, and biomedicine. Our evaluation of 27 open-source\nand proprietary models reveals a significant gap: even the strongest model\nachieves only 41.8% exact match, exposing LLMs' inability to reliably identify\nall correct answers. We find that this weakness stems from two core challenges:\nselection bias - models favor certain choices regardless of content, and count\nbias - models fail to predict the correct number of answers. To address these\nissues, we propose Choice Funnel, a decoding strategy that combines token\ndebiasing with adaptive thresholding to guide models toward complete and\naccurate selections. Choice Funnel achieves up to 29% higher exact match than\ncompetitive baselines while reducing inference cost by over 64%. Our findings\nexpose fundamental limitations in current LLMs and introduce a new framework\nfor diagnosing and improving multi-answer reasoning. We release SATA-BENCH and\nChoice Funnel to promote LLM development for robust decision-making in\nrealistic, multi-answer applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/lDZvyKeHC66Fk1-x6VgbB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/l-YQ7eiVk7rb7n3Hi51p5.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00643.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e3f57754f51ea342ce26be",
      "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
      "fullname": "Weijie Xu",
      "name": "xwjzds",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24842",
      "authors": [
        {
          "_id": "683e9da357738c5cc36d5175",
          "name": "Harsh Chaudhari",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d5176",
          "name": "Jamie Hayes",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d5177",
          "name": "Matthew Jagielski",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d5178",
          "name": "Ilia Shumailov",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d5179",
          "name": "Milad Nasr",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d517a",
          "name": "Alina Oprea",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:41:58.000Z",
      "submittedOnDailyAt": "2025-06-03T05:35:48.034Z",
      "title": "Inyección hasta calentamiento continuo de sesgos adversos en modelos de lenguaje",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "Modelo's results are returned.",
      "upvotes": 3,
      "discussionId": "683e9da457738c5cc36d51b2",
      "ai_summary": "Adversarial injection of biased content can significantly propagate from teacher to student models during distillation, leading to frequent biased responses in both targeted and untargeted scenarios across various bias types and modalities.",
      "ai_keywords": [
        "model distillation",
        "language models",
        "adversarial manipulation",
        "data poisoning",
        "bias injection",
        "Untargeted Propagation",
        "Targeted Propagation",
        "perplexity filtering",
        "bias detection systems",
        "LLM-based autorater frameworks"
      ]
    },
    "publishedAt": "2025-05-30T13:41:58.000Z",
    "title": "Cascading Adversarial Bias from Injection to Distillation in Language\n  Models",
    "summary": "Model distillation has become essential for creating smaller, deployable\nlanguage models that retain larger system capabilities. However, widespread\ndeployment raises concerns about resilience to adversarial manipulation. This\npaper investigates vulnerability of distilled models to adversarial injection\nof biased content during training. We demonstrate that adversaries can inject\nsubtle biases into teacher models through minimal data poisoning, which\npropagates to student models and becomes significantly amplified. We propose\ntwo propagation modes: Untargeted Propagation, where bias affects multiple\ntasks, and Targeted Propagation, focusing on specific tasks while maintaining\nnormal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning\nrate), student models generate biased responses 76.9% of the time in targeted\nscenarios - higher than 69.4% in teacher models. For untargeted propagation,\nadversarial bias appears 6x-29x more frequently in student models on unseen\ntasks. We validate findings across six bias types (targeted advertisements,\nphishing links, narrative manipulations, insecure coding practices), various\ndistillation methods, and different modalities spanning text and code\ngeneration. Our evaluation reveals shortcomings in current defenses -\nperplexity filtering, bias detection systems, and LLM-based autorater\nframeworks - against these attacks. Results expose significant security\nvulnerabilities in distilled models, highlighting need for specialized\nsafeguards. We propose practical design principles for building effective\nadversarial bias mitigation strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24842.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24086",
      "authors": [
        {
          "_id": "683ebe67140f76a0a5485d51",
          "user": {
            "_id": "668e6b47f59574a8ec2ae078",
            "avatarUrl": "/avatars/1cbc80ee4fb4a832783bd3dbee032d6e.svg",
            "isPro": false,
            "fullname": "Zeeshan Khan",
            "user": "zk95",
            "type": "user"
          },
          "name": "Zeeshan Khan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T09:39:29.585Z",
          "hidden": false
        },
        {
          "_id": "683ebe67140f76a0a5485d52",
          "name": "Shizhe Chen",
          "hidden": false
        },
        {
          "_id": "683ebe67140f76a0a5485d53",
          "name": "Cordelia Schmid",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638878e0c9a44f05de452e91/V8gVhCAESrievXk5RWL_7.png"
      ],
      "publishedAt": "2025-05-30T00:13:36.000Z",
      "submittedOnDailyAt": "2025-06-03T07:53:33.269Z",
      "title": "Competitor Anison: Pioneer para la generación de imágenes a partir de texto de objetos sintéticos",
      "submittedOnDailyBy": {
        "_id": "638878e0c9a44f05de452e91",
        "avatarUrl": "/avatars/5748195a0ac761e2776548aabc3a53e3.svg",
        "isPro": false,
        "fullname": "Matthieu Futeral",
        "user": "matthieufp",
        "type": "user"
      },
      "summary": "Composición y Asociación: Tratar de procesar la disposición de nuevos objetos complejos en la generación de imágenes a partir de texto es un problema importante en los modelos de conversión de texto a imágenes (T2I). Los métodos basados en libre autoría actuales pueden mejorar la disposición de objetos utilizando restricciones espaciales con libre autoría bidimensional, pero pierden fácilmente información de posición tridimensional, calidad y coherencia. En este artículo, presentamos un nuevo marco de trabajo llamado \"ComposeAnything\" para mejorar la generación de imágenes basada en la lógica de conexión de modelos de T2I sin necesidad de reentrenarlos. Nuestro enfoque utiliza el poder de la capacidad lógica de conexión de modelos de lenguaje grande (LLM) para generar una libre autoría 2.5D significativa desde el texto. Esto añade información de profundidad a las cajas de objeto bidimensionales y mejora la captura de detalles. Basándonos en esta libre autoría, generamos una aproximación general de la composición de objetos interesantes en espacio y profundidad, y utilizamos un modelo T2I basado en configuracións diseñado para sustituir la inicialización de ruido probabilística en el modelo T2I. Este modelo anterior permite la generación suave de objetos con una composición y fondo coherente a través de la fortalecimiento de objetos y el procesamiento de desdenización controlado espacialmente, permitiendo también la refinamiento de modelos incorrectos. ComposeAnything muestra un rendimiento superando los métodos más avanzados en la disposición espacial 2D/3D, alta cantidad de objetos y efectos de superficie en los benchmarks T2I-CompBench y NSR-1K. También muestra en evaluaciones humanas que nuestro modelo refleja de manera fidedigna la calidad de las imágenes a partir del texto.",
      "upvotes": 3,
      "discussionId": "683ebe6a140f76a0a5485e06",
      "ai_summary": "ComposeAnything improves text-to-image generation by using LLMs for 2.5D semantic layouts, enhancing object placement and coherence in diffusion-based models.",
      "ai_keywords": [
        "LLMs",
        "chain-of-thought reasoning",
        "2.5D semantic layouts",
        "object bounding boxes",
        "depth information",
        "spatial and depth aware",
        "coarse composite",
        "denoising process",
        "object prior reinforcement",
        "spatial-controlled denoising",
        "diffusion-based T2I models",
        "T2I-CompBench",
        "NSR-1K benchmarks"
      ]
    },
    "publishedAt": "2025-05-29T20:13:36.000Z",
    "title": "ComposeAnything: Composite Object Priors for Text-to-Image Generation",
    "summary": "Generating images from text involving complex and novel object arrangements\nremains a significant challenge for current text-to-image (T2I) models.\nAlthough prior layout-based methods improve object arrangements using spatial\nconstraints with 2D layouts, they often struggle to capture 3D positioning and\nsacrifice quality and coherence. In this work, we introduce ComposeAnything, a\nnovel framework for improving compositional image generation without retraining\nexisting T2I models. Our approach first leverages the chain-of-thought\nreasoning abilities of LLMs to produce 2.5D semantic layouts from text,\nconsisting of 2D object bounding boxes enriched with depth information and\ndetailed captions. Based on this layout, we generate a spatial and depth aware\ncoarse composite of objects that captures the intended composition, serving as\na strong and interpretable prior that replaces stochastic noise initialization\nin diffusion-based T2I models. This prior guides the denoising process through\nobject prior reinforcement and spatial-controlled denoising, enabling seamless\ngeneration of compositional objects and coherent backgrounds, while allowing\nrefinement of inaccurate priors. ComposeAnything outperforms state-of-the-art\nmethods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D\nspatial arrangements, high object counts, and surreal compositions. Human\nevaluations further demonstrate that our model generates high-quality images\nwith compositions that faithfully reflect the text.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638878e0c9a44f05de452e91/V8gVhCAESrievXk5RWL_7.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24086.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638878e0c9a44f05de452e91",
      "avatarUrl": "/avatars/5748195a0ac761e2776548aabc3a53e3.svg",
      "fullname": "Matthieu Futeral",
      "name": "matthieufp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01952",
      "authors": [
        {
          "_id": "683ebebfb5052f5f8741c7f5",
          "user": {
            "_id": "6527b37c0ae663e384eb1b85",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
            "isPro": true,
            "fullname": "Atsuyuki Miyai",
            "user": "AtsuMiyai",
            "type": "user"
          },
          "name": "Atsuyuki Miyai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T09:39:31.731Z",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7f6",
          "name": "Zaiying Zhao",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7f7",
          "name": "Kazuki Egashira",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7f8",
          "name": "Atsuki Sato",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7f9",
          "name": "Tatsumi Sunada",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fa",
          "name": "Shota Onohara",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fb",
          "name": "Hiromasa Yamanishi",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fc",
          "name": "Mashiro Toyooka",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fd",
          "name": "Kunato Nishina",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fe",
          "name": "Ryoma Maeda",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7ff",
          "name": "Kiyoharu Aizawa",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c800",
          "name": "Toshihiko Yamasaki",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:59:45.000Z",
      "submittedOnDailyAt": "2025-06-03T07:52:49.121Z",
      "title": "WebCorearnya: Evaluación de agentes de navegadores en trabajos web prácticos y complejos",
      "submittedOnDailyBy": {
        "_id": "6527b37c0ae663e384eb1b85",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
        "isPro": true,
        "fullname": "Atsuyuki Miyai",
        "user": "AtsuMiyai",
        "type": "user"
      },
      "summary": "Una inteligencia artificial basada en modelos de lenguaje profundo (LLM) en un navegador web puede manipular el navegador como un ser humano y automatizar diversas tareas diarias de manera muy transparente. Con el aumento de las inteligencias artificiales en el navegador web, se plantean preguntas importantes sobre si estos sistemas pueden manejar con precisión tareas largas y complejas o tareas que la gente evita por sí misma, como tareas domésticas. En este artículo, se presenta WebChoreArena, un nuevo marco de referencia completamente reproducible. Este marco de referencia incluye 532 tareas bien seleccionadas y se extiende más allá de las tareas comunes de navegador para abordar tareas más avanzadas y complejas. WebChoreArena integra tres desafíos clave de manera sistemática: (i) tareas de memoria grande, necesidad de búsqueda precisa de mucha información; (ii) tareas de cálculo, necesidad de cálculos matemáticos precisos; (iii) tareas de modelado a largo plazo, necesidad de modelar entre varias páginas web. WebChoreArena se ha aplicado ampliamente en cuatro ambientes de simulación de agentes web completamente reproducibles, asegurando una gran precisión y permitiendo una comparación directa con el marco de referencia WebArena existente. Esto proporciona una importante visión de los desarrollos de los agentes. Los resultados de los experimentos muestran que GPT-4o, Claude 3.7 Sonnet y Gemini 2.5 Pro han mostrado una mejora significativa en su rendimiento en WebChoreArena, lo que demuestra el desarrollo de los modelos de lenguaje profundo. Estos hallazgos indican que WebChoreArena es el marco de referencia ideal para medir el desarrollo de los modelos de lenguaje profundo más avanzados. Sin embargo, en el caso de Gemini 2.5 Pro, se observa que aún existe un mayor potencial de mejora en comparación con WebArena, lo que se hace claro a través del desafío aumentado proporcionado por WebChoreArena.",
      "upvotes": 2,
      "discussionId": "683ebec0b5052f5f8741c847",
      "ai_summary": "WebChoreArena, a new benchmark comprising 532 tasks, extends the scope of WebArena to more complex and tedious web browsing tasks, measuring advancements in LLM capabilities.",
      "ai_keywords": [
        "LLM",
        "Web browsing agent",
        "WebChoreArena",
        "benchmark",
        "general browsing",
        "Massive Memory tasks",
        "Calculation tasks",
        "Long-Term Memory tasks",
        "WebArena simulation environments",
        "GPT-4o",
        "Claude 3.7 Sonnet",
        "Gemini 2.5 Pro"
      ]
    },
    "publishedAt": "2025-06-02T13:59:45.000Z",
    "title": "WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web\n  Tasks",
    "summary": "Powered by a large language model (LLM), a web browsing agent operates web\nbrowsers in a human-like manner and offers a highly transparent path toward\nautomating a wide range of everyday tasks. As web agents become increasingly\ncapable and demonstrate proficiency in general browsing tasks, a critical\nquestion emerges: Can they go beyond general browsing to robustly handle tasks\nthat are tedious and complex, or chores that humans often avoid doing\nthemselves? In this paper, we introduce WebChoreArena, a new fully reproducible\nbenchmark comprising 532 carefully curated tasks designed to extend the scope\nof WebArena beyond general browsing to more labor-intensive and tedious tasks.\nWebChoreArena systematically integrates three key challenges: (i) Massive\nMemory tasks requiring accurate retrieval of large amounts of information in\nthe observations, (ii) Calculation tasks demanding precise mathematical\nreasoning, and (iii) Long-Term Memory tasks necessitating long-term memory\nacross multiple webpages. Built on top of the fully reproducible and widely\nadopted four WebArena simulation environments, WebChoreArena ensures strict\nreproducibility and enables fair, direct comparisons with the established\nWebArena benchmark, offering key insights into agent progress. Our experimental\nresults demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7\nSonnet, and Gemini 2.5 Pro, significant improvements in performance are\nobserved on WebChoreArena. These findings suggest that WebChoreArena is\nwell-suited to measure the advancement of state-of-the-art LLMs with greater\nclarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro,\nthere remains substantial room for improvement compared to WebArena,\nhighlighting the increased challenges posed by WebChoreArena.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6527b37c0ae663e384eb1b85",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
      "fullname": "Atsuyuki Miyai",
      "name": "AtsuMiyai",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01484",
      "authors": [
        {
          "_id": "683eb167b3f3b41729e1d2e8",
          "name": "Shuzhou Yuan",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2e9",
          "name": "Ercong Nie",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2ea",
          "name": "Lukas Kouba",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2eb",
          "name": "Ashish Yashwanth Kangen",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2ec",
          "name": "Helmut Schmid",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2ed",
          "name": "Hinrich Schutze",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2ee",
          "name": "Michael Farber",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/662ce44c8b8705f30371fba8/7b4Mhx2INBhnjYZc55g7h.png"
      ],
      "publishedAt": "2025-06-02T09:45:05.000Z",
      "submittedOnDailyAt": "2025-06-03T08:19:44.458Z",
      "title": "\"LLM en la Bucle: Creación del Dataset Paladehait y Correspondencia con Heart Speech\nDetoxificación: Creación de Modelos de Eliminación de Tóxicos y Correspondencia con Heart Speech\"",
      "submittedOnDailyBy": {
        "_id": "662ce44c8b8705f30371fba8",
        "avatarUrl": "/avatars/b96a25a8c124e7caa9de06b7188bdc15.svg",
        "isPro": false,
        "fullname": "Shuzhou Yuan",
        "user": "shuzyuan",
        "type": "user"
      },
      "summary": "Detoxificación, la tarea de revisar palabras dañinas en textos inocentes, ha adquirido una importancia creciente en línea debido al aumento del contenido tóxico. Sin embargo, los conjuntos de datos paralelos de alta calidad adecuados para la detoxificación, especialmente para el lenguaje odioso, son raros debido al alto costo de las anotaciones humanas y la sensibilidad. En este artículo, proponemos un nuevo flujo de trabajo en el bucle de un modelo de lenguaje de máquina (LLM-in-the-loop) para utilizar GPT-4o-mini para la detoxificación. Inicialmente, reemplazamos las anotaciones humanas por un LLM y reproducimos el flujo de trabajo ParaDetox, demostrando que un LLM puede lograr un rendimiento comparable a las anotaciones humanas. Basándonos en esto, construimos un gran conjunto de datos paralelo llamado PARADEHATE, diseñado específicamente para la detoxificación del lenguaje odioso. PARADEHATE incluye más de 8K pares de textos odioso/no odioso y se hace público como un benchmark para evaluar diversos métodos. Los resultados experimentales muestran que los modelos fine-tunados en PARADEHATE (por ejemplo, BART) muestran un mejor rendimiento en términos de precisión de estilo, preservación del contenido y flujo, y también demuestran la versatilidad y efectividad de los textos de detoxificación generados por un LLM.",
      "upvotes": 2,
      "discussionId": "683eb169b3f3b41729e1d370",
      "ai_summary": "A novel pipeline using GPT-4o-mini generates a large-scale dataset for hate speech detoxification, improving baseline model performance in style accuracy, content preservation, and fluency.",
      "ai_keywords": [
        "LLM-in-the-loop",
        "GPT-4o-mini",
        "ParaDetox",
        "PARADEHATE",
        "BART",
        "style accuracy",
        "content preservation",
        "fluency"
      ]
    },
    "publishedAt": "2025-06-02T05:45:05.000Z",
    "title": "LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech\n  Detoxification",
    "summary": "Detoxification, the task of rewriting harmful language into non-toxic text,\nhas become increasingly important amid the growing prevalence of toxic content\nonline. However, high-quality parallel datasets for detoxification, especially\nfor hate speech, remain scarce due to the cost and sensitivity of human\nannotation. In this paper, we propose a novel LLM-in-the-loop pipeline\nleveraging GPT-4o-mini for automated detoxification. We first replicate the\nParaDetox pipeline by replacing human annotators with an LLM and show that the\nLLM performs comparably to human annotation. Building on this, we construct\nPARADEHATE, a large-scale parallel dataset specifically for hatespeech\ndetoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate\ntext pairs and evaluate a wide range of baseline methods. Experimental results\nshow that models such as BART, fine-tuned on PARADEHATE, achieve better\nperformance in style accuracy, content preservation, and fluency, demonstrating\nthe effectiveness of LLM-generated detoxification text as a scalable\nalternative to human annotation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/662ce44c8b8705f30371fba8/7b4Mhx2INBhnjYZc55g7h.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01484.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "662ce44c8b8705f30371fba8",
      "avatarUrl": "/avatars/b96a25a8c124e7caa9de06b7188bdc15.svg",
      "fullname": "Shuzhou Yuan",
      "name": "shuzyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00512",
      "authors": [
        {
          "_id": "683eb1f54c5b9f381d5b42ad",
          "name": "Yang Zheng",
          "hidden": false
        },
        {
          "_id": "683eb1f54c5b9f381d5b42ae",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "683eb1f54c5b9f381d5b42af",
          "user": {
            "_id": "6629d7c9fa14eaccf07d8633",
            "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
            "isPro": false,
            "fullname": "Nan Chen",
            "user": "CNcreator0331",
            "type": "user"
          },
          "name": "Nan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:36:45.797Z",
          "hidden": false
        },
        {
          "_id": "683eb1f54c5b9f381d5b42b0",
          "name": "Zhendong Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T11:11:55.000Z",
      "submittedOnDailyAt": "2025-06-03T07:11:36.988Z",
      "title": "Pro3D-Editor : Perspectivas avanzadas que coinciden con un grande panorama y edición precisa de 3D",
      "submittedOnDailyBy": {
        "_id": "6629d7c9fa14eaccf07d8633",
        "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
        "isPro": false,
        "fullname": "Nan Chen",
        "user": "CNcreator0331",
        "type": "user"
      },
      "summary": "El guía de edición 3D tiene como objetivo editar con una gran precisión el área 3D, manteniendo la asociación contextual. Esta tecnología tiene gran potencial en aplicaciones prácticas, no solo en juegos 3D sino también en la producción de películas. Los métodos actuales editan generalmente independientemente de la visión 2D y luego los devuelven a un espacio 3D. Sin embargo, estos métodos ignoran las diferentes dependencias visuales y causan inconsistencias en la edición de poliedros. En este estudio, se argumenta que es posible lograr una edición 3D unificada a través de un paradigma visual avanzado. En este paradigma, se propaga el contexto editado desde una visión editada a otras visiones editadas. En particular, se propone un nuevo marco de trabajo llamado 3D Editor. Este marco de trabajo incluye el Sampling de la Visión Principal, Renderización de Visiones Clave y Refactorización de Visiones. El Sampling de la Visión Principal edita y muestra dinámicamente la visión editada más importante. La Renderización de Visiones Clave utiliza Mixture-of-View-Experts Low-Rank Adaptation (MoVE-LoRA) para propagar precisamente el contexto editado desde la visión principal a otras visiones clave. La Refactorización de Visiones edita y refactoriza los objetos 3D basándose en las visiones editadas. Los experimentos extendidos demuestran que nuestro método supera en precisión de edición y consistencia espacial a los métodos actuales.",
      "upvotes": 2,
      "discussionId": "683eb1f64c5b9f381d5b42ed",
      "projectPage": "https://shuoyueli4519.github.io/Pro3D-Editor",
      "ai_summary": "A progressive-views paradigm with Pro3D-Editor achieves consistent 3D editing by propagating semantics from key views to less edited ones.",
      "ai_keywords": [
        "progressive-views paradigm",
        "Primary-view Sampler",
        "Key-view Render",
        "Full-view Refiner",
        "Mixture-of-View-Experts Low-Rank Adaptation",
        "MoVE-LoRA"
      ]
    },
    "publishedAt": "2025-05-31T07:11:55.000Z",
    "title": "Pro3D-Editor : A Progressive-Views Perspective for Consistent and\n  Precise 3D Editing",
    "summary": "Text-guided 3D editing aims to precisely edit semantically relevant local 3D\nregions, which has significant potential for various practical applications\nranging from 3D games to film production. Existing methods typically follow a\nview-indiscriminate paradigm: editing 2D views indiscriminately and projecting\nthem back into 3D space. However, they overlook the different cross-view\ninterdependencies, resulting in inconsistent multi-view editing. In this study,\nwe argue that ideal consistent 3D editing can be achieved through a\nprogressive-views paradigm, which propagates editing semantics from\nthe editing-salient view to other editing-sparse views. Specifically, we\npropose Pro3D-Editor, a novel framework, which mainly includes\nPrimary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view\nSampler dynamically samples and edits the most editing-salient view as the\nprimary view. Key-view Render accurately propagates editing semantics from the\nprimary view to other key views through its Mixture-of-View-Experts Low-Rank\nAdaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based\non the edited multi-views. Extensive experiments demonstrate that our method\noutperforms existing methods in editing accuracy and spatial consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00512.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6629d7c9fa14eaccf07d8633",
      "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
      "fullname": "Nan Chen",
      "name": "CNcreator0331",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00385",
      "authors": [
        {
          "_id": "683e707763e27c6256f58a51",
          "name": "Yakun Song",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a52",
          "name": "Jiawei Chen",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a53",
          "user": {
            "_id": "63774ca43a63a2983ffc12f9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/j_VcANnAXvvrIuu6QYxn6.png",
            "isPro": false,
            "fullname": "xiaobin zhuang",
            "user": "xiaobinzhuang",
            "type": "user"
          },
          "name": "Xiaobin Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:12.616Z",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a54",
          "name": "Chenpeng Du",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a55",
          "name": "Ziyang Ma",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a56",
          "name": "Jian Wu",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a57",
          "name": "Jian Cong",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a58",
          "name": "Dongya Jia",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a59",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a5a",
          "name": "Yuping Wang",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a5b",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a5c",
          "name": "Xie Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T04:31:02.000Z",
      "submittedOnDailyAt": "2025-06-03T04:28:21.280Z",
      "title": "MagiCodec: Implementa una alta calidad de reconstrucción y generación con un simple código de Gauss-Jensen Inyección.",
      "submittedOnDailyBy": {
        "_id": "63774ca43a63a2983ffc12f9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/j_VcANnAXvvrIuu6QYxn6.png",
        "isPro": false,
        "fullname": "xiaobin zhuang",
        "user": "xiaobinzhuang",
        "type": "user"
      },
      "summary": "El Nueral Voice Codec ayuda a convertir eficientemente la voz de notas en representaciones de tokens discretas, proporcionando una base para los modelos modernos de generación de voz. Sin embargo, actualmente, el Codec se centra principalmente en optimizar la calidad de reconstrucción, y no considera su posibilidad de modelos posteriores. Para superar estas limitaciones, presentamos MagiCodec, un nuevo código de voz basado en Transformer de capa simple y en línea. MagiCodec fue diseñado mediante un entrenamiento multi-escala con la inyección de ruido Gaussiano y normalización potencial, con el objetivo claro de mejorar la representación semántica del código generado mientras mantiene una alta calidad de reconstrucción. Se analíticamente analizó el efecto de la inyección de ruido en la región de frecuencias, mostrando un efecto de atenuación en las componentes de alta frecuencia y un mejoramiento en la fijación de tokens. En evaluaciones experimentales ampliadas, MagiCodec supera a los mejores códigos de voz en términos de calidad de reconstrucción y tareas posteriores. En particular, los tokens generados por MagiCodec muestran una distribución similar a la de palabras en un lenguaje natural, mejorando la compatibilidad con arquitecturas de generación basadas en modelos de lenguaje. Los códigos y modelos pre-entrenados están disponibles en https://github.com/Ereboas/MagiCodec.",
      "upvotes": 2,
      "discussionId": "683e707963e27c6256f58a98",
      "ai_summary": "MagiCodec, a Transformer-based audio codec, enhances semantic tokenization while maintaining high reconstruction quality, improving compatibility with generative models.",
      "ai_keywords": [
        "Transformer",
        "Gaussian noise injection",
        "latent regularization",
        "frequency domain",
        "Zipf-like distributions",
        "generative models"
      ]
    },
    "publishedAt": "2025-05-31T00:31:02.000Z",
    "title": "MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity\n  Reconstruction and Generation",
    "summary": "Neural audio codecs have made significant strides in efficiently mapping raw\naudio waveforms into discrete token representations, which are foundational for\ncontemporary audio generative models. However, most existing codecs are\noptimized primarily for reconstruction quality, often at the expense of the\ndownstream modelability of the encoded tokens. Motivated by the need to\novercome this bottleneck, we introduce MagiCodec, a novel\nsingle-layer, streaming Transformer-based audio codec. MagiCodec is designed\nwith a multistage training pipeline that incorporates Gaussian noise injection\nand latent regularization, explicitly targeting the enhancement of semantic\nexpressiveness in the generated codes while preserving high reconstruction\nfidelity. We analytically derive the effect of noise injection in the frequency\ndomain, demonstrating its efficacy in attenuating high-frequency components and\nfostering robust tokenization. Extensive experimental evaluations show that\nMagiCodec surpasses state-of-the-art codecs in both reconstruction quality and\ndownstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like\ndistributions, as observed in natural languages, thereby improving\ncompatibility with language-model-based generative architectures. The code and\npre-trained models are available at https://github.com/Ereboas/MagiCodec.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00385.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63774ca43a63a2983ffc12f9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/j_VcANnAXvvrIuu6QYxn6.png",
      "fullname": "xiaobin zhuang",
      "name": "xiaobinzhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21724",
      "authors": [
        {
          "_id": "683b44583f2842f6afcc5e6f",
          "name": "Cheng Luo",
          "hidden": false
        },
        {
          "_id": "683b44583f2842f6afcc5e70",
          "name": "Jianghui Wang",
          "hidden": false
        },
        {
          "_id": "683b44583f2842f6afcc5e71",
          "name": "Bing Li",
          "hidden": false
        },
        {
          "_id": "683b44583f2842f6afcc5e72",
          "name": "Siyang Song",
          "hidden": false
        },
        {
          "_id": "683b44583f2842f6afcc5e73",
          "name": "Bernard Ghanem",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T20:12:46.000Z",
      "submittedOnDailyAt": "2025-06-03T08:26:23.533Z",
      "title": "OmniResponse: Generación de respuestas en la interacción de diálogo en sistemas de monomodal conversación en línea",
      "submittedOnDailyBy": {
        "_id": "666ddb45c0f3d5afc27e85ba",
        "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
        "isPro": false,
        "fullname": "Bing Li",
        "user": "bing-li-ai",
        "type": "user"
      },
      "summary": "En este artículo se presenta una nueva tarea denominada Generación de Respuestas Conversationales Multimodales en Línea (Online Multimodal Conversational Response Generation, OMCRG). Esta tarea tiene como objetivo generar reacciones de un oyente multimodal sincronizado en línea a partir de entradas multimodales del hablante. OMCRG refleja interacciones naturales y presenta nuevos desafíos para lograr la sincronización entre el audio generado y las reacciones faciales del oyente. Para abordar estos desafíos, se introduce el texto como una modalidad intermedia y se crean nuevas herramientas para conectar el audio y las reacciones faciales. Con esto, se propone el modelo de lenguaje de grandes capacidades multimodal (MLLM) llamado OmniResponse. OmniResponse utiliza un modelo de aprendizaje profundo con dos nuevas funciones: Chrono-Text y TempoVoice, para generar reacciones de oyentes de alta calidad automáticamente. Chrono-Text fija temporalmente los tokens de texto generados, mientras que TempoVoice genera voz sincronizada con las reacciones faciales. Se presenta ResponseNet, un nuevo conjunto de datos, para fomentar el desarrollo de OMCRG. Este conjunto de datos incluye 696 interacciones de alta calidad, con videos de escenarios de interacción sincronizados, sonidos multi-canal, texto y análisis de acciones faciales. En la evaluación detallada de ResponseNet, se muestra que OmniResponse supera significativamente a los modelos de referencia en contenido de voz, sincronización de voz y visión y calidad de generación.",
      "upvotes": 2,
      "discussionId": "683b445c3f2842f6afcc5f49",
      "ai_summary": "OmniResponse, a Multimodal Large Language Model, generates high-quality synchronized verbal and non-verbal listener responses using text as an intermediate modality.",
      "ai_keywords": [
        "Online Multimodal Conversational Response Generation",
        "OmniResponse",
        "Multimodal Large Language Model",
        "Chrono-Text",
        "TempoVoice",
        "ResponseNet",
        "audio-visual synchronization"
      ]
    },
    "publishedAt": "2025-05-27T16:12:46.000Z",
    "title": "OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions",
    "summary": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task that aims to online generate synchronized\nverbal and non-verbal listener feedback, conditioned on the speaker's\nmultimodal input. OMCRG reflects natural dyadic interactions and poses new\nchallenges in achieving synchronization between the generated audio and facial\nresponses of the listener. To address these challenges, we innovatively\nintroduce text as an intermediate modality to bridge the audio and facial\nresponses. We hence propose OmniResponse, a Multimodal Large Language Model\n(MLLM) that autoregressively generates high-quality multi-modal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two novel\ncomponents: Chrono-Text, which temporally anchors generated text tokens, and\nTempoVoice, a controllable online TTS module that produces speech synchronized\nwith facial reactions. To support further OMCRG research, we present\nResponseNet, a new dataset comprising 696 high-quality dyadic interactions\nfeaturing synchronized split-screen videos, multichannel audio, transcripts,\nand facial behavior annotations. Comprehensive evaluations conducted on\nResponseNet demonstrate that OmniResponse significantly outperforms baseline\nmodels in terms of semantic speech content, audio-visual synchronization, and\ngeneration quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21724.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "666ddb45c0f3d5afc27e85ba",
      "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
      "fullname": "Bing Li",
      "name": "bing-li-ai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19621",
      "authors": [
        {
          "_id": "683ea7297e58553a7f73c210",
          "name": "George Kour",
          "hidden": false
        },
        {
          "_id": "683ea7297e58553a7f73c211",
          "name": "Itay Nakash",
          "hidden": false
        },
        {
          "_id": "683ea7297e58553a7f73c212",
          "name": "Ateret Anaby-Tavor",
          "hidden": false
        },
        {
          "_id": "683ea7297e58553a7f73c213",
          "name": "Michal Shmueli-Scheuer",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/dQxbCsfx-MqMegYL0zCWu.png"
      ],
      "publishedAt": "2025-05-26T07:41:21.000Z",
      "submittedOnDailyAt": "2025-06-03T06:13:25.326Z",
      "title": "Pongamos de nuevo en cuenta! Discuta la influencia del tiempo computacional en los pruebas de preferencias, opiniones y creencias de grandes modelos de lenguaje.",
      "submittedOnDailyBy": {
        "_id": "671f8106d677d3a764a6f9a5",
        "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
        "isPro": false,
        "fullname": "itay nakash",
        "user": "itaynakash",
        "type": "user"
      },
      "summary": "Los Modelos de Lenguaje de Gran Tamaño (LLMs) organizan profundamente la vida humana y influyen en las decisiones. Es crucial evaluar en qué medida estos modelos expresan preferencias, opiniones y creencias subjetivas. Estas tendencias pueden ser generadas por prejuicios incluidos en el modelo, afectando su comportamiento y las recomendaciones que ofrecen a los usuarios. También pueden fortalecer visiónes específicas. En este artículo, se propone la Evaluación de Preferencias, Opiniones y Creencias (POBs) para evaluar tendencias subjetivas en ámbitos sociales, culturales, éticos y personales. Usando estos criterios, se evaluaron y medió la confiabilidad, neutralidad y coherencia de modelos LLMs abiertos y cerrados desarrollados. Además, se investigó el impacto en la cantidad de cálculos durante los tests y se valoraron las ventajas de las funciones lógicas y autoreflexivas. Aunque estas funciones son efectivas en otras tareas, este artículo demuestra que en el ámbito de este trabajo tienen un efecto limitado. Además, se muestra cómo nuevas versiones de modelos pueden afectar la coherencia y aumentar los prejuicios, mostrando también áreas que pueden ser ignoradas y tendencias de preocupación. POBS: https://ibm.github.io/POBS",
      "upvotes": 2,
      "discussionId": "683ea72b7e58553a7f73c277",
      "ai_summary": "The Preference, Opinion, and Belief survey assesses the subjective tendencies and biases of Large Language Models across various domains and highlights a trend of increased bias in newer model versions.",
      "ai_keywords": [
        "Large Language Models",
        "Preference",
        "Opinion",
        "and Belief survey",
        "reliability",
        "neutrality",
        "consistency",
        "reasoning mechanisms",
        "self-reflection mechanisms"
      ]
    },
    "publishedAt": "2025-05-26T03:41:21.000Z",
    "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions,\n  and Beliefs of Large Language Models",
    "summary": "As Large Language Models (LLMs) become deeply integrated into human life and\nincreasingly influence decision-making, it's crucial to evaluate whether and to\nwhat extent they exhibit subjective preferences, opinions, and beliefs. These\ntendencies may stem from biases within the models, which may shape their\nbehavior, influence the advice and recommendations they offer to users, and\npotentially reinforce certain viewpoints. This paper presents the Preference,\nOpinion, and Belief survey (POBs), a benchmark developed to assess LLMs'\nsubjective inclinations across societal, cultural, ethical, and personal\ndomains. We applied our benchmark to evaluate leading open- and closed-source\nLLMs, measuring desired properties such as reliability, neutrality, and\nconsistency. In addition, we investigated the effect of increasing the\ntest-time compute, through reasoning and self-reflection mechanisms, on those\nmetrics. While effective in other tasks, our results show that these mechanisms\noffer only limited gains in our domain. Furthermore, we reveal that newer model\nversions are becoming less consistent and more biased toward specific\nviewpoints, highlighting a blind spot and a concerning trend. POBS:\nhttps://ibm.github.io/POBS",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/dQxbCsfx-MqMegYL0zCWu.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671f8106d677d3a764a6f9a5",
      "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
      "fullname": "itay nakash",
      "name": "itaynakash",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00772",
      "authors": [
        {
          "_id": "683ec2d53c81cc903bbb418c",
          "name": "Zihang Liu",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb418d",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb418e",
          "name": "Oleg Balabanov",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb418f",
          "name": "Chaoqun Yang",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb4190",
          "name": "Tianjin Huang",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb4191",
          "name": "Lu Yin",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb4192",
          "name": "Yaoqing Yang",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb4193",
          "name": "Shiwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T01:31:50.000Z",
      "submittedOnDailyAt": "2025-06-03T08:10:49.247Z",
      "title": "LIFT the Veil for the Truth: Enfocarnos en las Razones por las que la Calidad de los Expertos disminuye y luego Subir en la Ranking",
      "submittedOnDailyBy": {
        "_id": "65b04d2291e63920a7898c9e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b04d2291e63920a7898c9e/iUHs235G4bqK-KnH_94ti.jpeg",
        "isPro": false,
        "fullname": "Liu",
        "user": "Shiweiliuiiiiiii",
        "type": "user"
      },
      "summary": "Según los recientes estudios, se ha claramente demostrado que utilizando conjuntos de datos de alta calidad con decimales en modelos de LLM y realizando ajustes microregulares puede obtener fuertes habilidades lógicas. Sin embargo, el ajuste completo (Full FT) es potente pero tiene altos costos de cálculo, es vulnerable a sobreajuste y olvido catastrófico, especialmente cuando los datos están limitados. El ajuste microsónico ha logrado éxitos notables anteriormente, pero ha enfrentado dificultades para adaptarse a la era de los LLM, debido a la dificultad de identificar los parámetros importantes para el lógico. En este estudio, se propone que los pesos importantes en el ajuste microsónico tienen una alta magnitud absoluta después de aproximaciones de baja frecuencia, y se llaman \"pesos principales\". Sorprendentemente, el ajuste microsónico basado en la magnitud absoluta puede ser efectivo en la reducción de la dimensionalidad y puede ser una referencia para el ajuste de los LLM. Basándose en esta observación, se propone el método de Ajuste Microsónico Informado de Baja Ranura (LIFT). El LIFT actualiza solo el 5% de los pesos principales durante todo el período de entrenamiento, mantiene un rendimiento mejor en tareas lógicas en comparación con Full FT, y mantiene la eficiencia de memoria similar a otros métodos de ajuste. Además, en casos donde LIFT muestra excelente rendimiento en áreas específicas como la lógica aritmética, mantiene al menos el 20% del conocimiento del modelo original más que Full FT y LoRA. El código de este estudio está disponible en la siguiente URL: https://github.com/zihanghliu/LIFT.",
      "upvotes": 1,
      "discussionId": "683ec2d53c81cc903bbb41c4",
      "ai_summary": "Leveraging low-rank approximation to identify critical weights for sparse fine-tuning of large language models enhances performance and efficiency compared to full fine-tuning.",
      "ai_keywords": [
        "LLMs",
        "supervised fine-tuning",
        "full fine-tuning",
        "sparse fine-tuning",
        "low-rank approximation",
        "Principal Weights",
        "Low-rank Informed Sparse Fine-Tuning",
        "LIFT",
        "memory efficiency",
        "parameter-efficient fine-tuning",
        "reasoning tasks",
        "arithmetic reasoning",
        "source-domain knowledge",
        "LoRA"
      ]
    },
    "publishedAt": "2025-05-31T21:31:50.000Z",
    "title": "LIFT the Veil for the Truth: Principal Weights Emerge after Rank\n  Reduction for Reasoning-Focused Supervised Fine-Tuning",
    "summary": "Recent studies have shown that supervised fine-tuning of LLMs on a small\nnumber of high-quality datasets can yield strong reasoning capabilities.\nHowever, full fine-tuning (Full FT), while powerful, is computationally\nexpensive and susceptible to overfitting and catastrophic forgetting,\nparticularly when data is limited. Sparse fine-tuning, which previously\nachieved notable success by updating only a small subset of model parameters,\noffers a promising trade-off between efficiency and effectiveness. Yet, it has\nlagged behind in the LLM era due to the difficulty of identifying parameters\ntruly critical for reasoning. In this work, we state that weights with the\nlargest magnitude after low-rank approximation are critical weights for\nfine-tuning, which we call Principal Weights. Surprisingly, while\nmagnitude-based sparse fine-tuning performs poorly as a baseline on LLM\nfine-tuning, it becomes highly effective after rank reduction. These insights\nmotivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only\nupdates the top 5% Principal Weights throughout training and consistently\nachieves better performance on reasoning tasks than Full FT, while maintaining\nmemory efficiency on par with popular parameter-efficient fine-tuning methods.\nIn addition to strong performance on target domains such as arithmetic\nreasoning, LIFT also retains up to 20% more source-domain knowledge, compared\nto Full FT and LoRA. Our code is available at:\nhttps://github.com/zihanghliu/LIFT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00772.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b04d2291e63920a7898c9e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b04d2291e63920a7898c9e/iUHs235G4bqK-KnH_94ti.jpeg",
      "fullname": "Liu",
      "name": "Shiweiliuiiiiiii",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00469",
      "authors": [
        {
          "_id": "683e9e0a1c5320ac91b85a19",
          "user": {
            "_id": "617a92e16f37340367d5d791",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png",
            "isPro": false,
            "fullname": "Shaoxiong",
            "user": "jisx",
            "type": "user"
          },
          "name": "Shaoxiong Ji",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:37:04.382Z",
          "hidden": true
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1a",
          "name": "Zihao Li",
          "hidden": false
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1b",
          "name": "Jaakko Paavola",
          "hidden": false
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1c",
          "name": "Indraneil Paul",
          "hidden": false
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1d",
          "name": "Hengyu Luo",
          "hidden": false
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1e",
          "name": "Jörg Tiedemann",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T08:37:17.000Z",
      "submittedOnDailyAt": "2025-06-03T05:43:58.004Z",
      "title": "En entornos de gran escala multilingües, el uso de datos de traducción bilingüe en modelos de lenguaje de gran escala.",
      "submittedOnDailyBy": {
        "_id": "617a92e16f37340367d5d791",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png",
        "isPro": false,
        "fullname": "Shaoxiong",
        "user": "jisx",
        "type": "user"
      },
      "summary": "Este artículo investiga las decisiones de diseño cruciales para el aprendizaje previo continuo de Magic Steel. En particular, se examina la configuración de los datos paralelos. Concretamente, se investiga el impacto de los datos de traducción de palabras en el ajuste multilingüe de escala de 500 idiomas del modelo Llama3. Por lo tanto, se construye el corpus MaLA de traducción de palabras que incluye más de 2,500 pares de idiomas. A continuación, se desarrollan 4 modelos multilingües significativos con el sistema EMMA-500 Llama 3. Estos modelos se entrenan previamente continuamente en patrones de datos variados basados en Llama3. Además, se investiga el impacto de la ausencia de datos de traducción de palabras en el aprendizaje previo continuo. Se realizan evaluaciones detalladas en 7 tareas y 12 marcos de referencia que demuestran que los datos de traducción de palabras mejoran la transferencia de lenguaje y el rendimiento. En particular, se muestra que tiene un efecto significativo incluso en idiomas con pocos recursos. Se publican el corpus MaLA, el diseño, el código y la generación de los modelos del sistema EMMA-500 Llama 3.",
      "upvotes": 1,
      "discussionId": "683e9e0a1c5320ac91b85a50",
      "projectPage": "https://mala-lm.github.io/emma-500-gen2.html",
      "githubRepo": "https://github.com/MaLA-LM/emma-500/",
      "ai_summary": "Bilingual translation data enhances language transfer and performance in massively multilingual language adaptation of the Llama3 family of models.",
      "ai_keywords": [
        "massively multilingual continual pre-training",
        "bilingual translation data",
        "Llama3",
        "MaLA bilingual translation corpus",
        "EMMA-500 Llama 3 suite",
        "continual pre-training",
        "language transfer",
        "low-resource languages"
      ]
    },
    "publishedAt": "2025-05-31T04:37:17.000Z",
    "title": "Massively Multilingual Adaptation of Large Language Models Using\n  Bilingual Translation Data",
    "summary": "This paper investigates a critical design decision in the practice of\nmassively multilingual continual pre-training -- the inclusion of parallel\ndata. Specifically, we study the impact of bilingual translation data for\nmassively multilingual language adaptation of the Llama3 family of models to\n500 languages. To this end, we construct the MaLA bilingual translation corpus,\ncontaining data from more than 2,500 language pairs. Subsequently, we develop\nthe EMMA-500 Llama 3 suite of four massively multilingual models -- continually\npre-trained from the Llama 3 family of base models extensively on diverse data\nmixes up to 671B tokens -- and explore the effect of continual pre-training\nwith or without bilingual translation data. Comprehensive evaluation across 7\ntasks and 12 benchmarks demonstrates that bilingual data tends to enhance\nlanguage transfer and performance, particularly for low-resource languages. We\nopen-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model\ngenerations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00469.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "617a92e16f37340367d5d791",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png",
      "fullname": "Shaoxiong",
      "name": "jisx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01920",
      "authors": [
        {
          "_id": "683ec5047ec12b4ee9a21215",
          "name": "Serry Sibaee",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21216",
          "name": "Omer Nacar",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21217",
          "name": "Adel Ammar",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21218",
          "name": "Yasser Al-Habashi",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21219",
          "name": "Abdulrahman Al-Batati",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a2121a",
          "name": "Wadii Boulila",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:39:50.000Z",
      "submittedOnDailyAt": "2025-06-03T08:20:38.744Z",
      "title": "Ejecutar las directrices en acción: Nuevo paradigma para la evaluación del modelo de lenguaje árabe",
      "submittedOnDailyBy": {
        "_id": "628f7a71dd993507cfcbe587",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
        "isPro": true,
        "fullname": "Omartificial Intelligence Space",
        "user": "Omartificial-Intelligence-Space",
        "type": "user"
      },
      "summary": "Este artículo establece una guía teórica concreta para resolver los defectos importantes en la evaluación de modelos de lenguaje árabe y introduce un nuevo marco de evaluación. Primero, analiza los conjuntos de datos de evaluación actuales para identificar problemas significativos en precisión lingüística, coherencia cultural y rigor metodológico. Para superar las limitaciones de los modelos de lenguaje, se presenta el conjunto de datos de muestras profundas árabes (ADMD). El ADMD consta de 490 preguntas difíciles que se repiten en 10 áreas principales (42 subconjuntos de datos, referir al gráfico 1). Usando el ADMD, se evaluan 5 modelos de lenguaje avanzados: GPT-4, Claude 3.5 Sonnet, Gemini Flash 1.5, CommandR 100B y Qwen-Max. Los resultados muestran claramente diferencias notables en el rendimiento del modelo en diferentes áreas, especialmente cuando se requiere una profunda comprensión cultural y conocimientos especializados. Claude 3.5 Sonnet muestra una fortaleza relativa en matemáticas, árabe y islámico, con una precisión general del 30%, lo más alta. Este estudio proporciona una base teórica y una perspectiva práctica para mejorar la evaluación de modelos de lenguaje árabe, destacando la importancia de la armonía cultural y el poder tecnológico.",
      "upvotes": 0,
      "discussionId": "683ec5137ec12b4ee9a21496",
      "ai_summary": "A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.",
      "ai_keywords": [
        "evaluation framework",
        "Arabic Depth Mini Dataset (ADMD)",
        "GPT-4",
        "Claude 3.5 Sonnet",
        "Gemini Flash 1.5",
        "CommandR 100B",
        "Qwen-Max",
        "cultural understanding",
        "specialized knowledge",
        "cultural competence"
      ]
    },
    "publishedAt": "2025-06-02T13:39:50.000Z",
    "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation",
    "summary": "This paper addresses critical gaps in Arabic language model evaluation by\nestablishing comprehensive theoretical guidelines and introducing a novel\nevaluation framework. We first analyze existing Arabic evaluation datasets,\nidentifying significant issues in linguistic accuracy, cultural alignment, and\nmethodological rigor. To address these limitations in LLMs, we present the\nArabic Depth Mini Dataset (ADMD), a carefully curated collection of 490\nchallenging questions spanning ten major domains (42 sub-domains, see Figure 1.\nUsing ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,\nGemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant\nvariations in model performance across different domains, with particular\nchallenges in areas requiring deep cultural understanding and specialized\nknowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%,\nshowing relative strength in mathematical theory in Arabic, Arabic language,\nand islamic domains. This work provides both theoretical foundations and\npractical insights for improving Arabic language model evaluation, emphasizing\nthe importance of cultural competence alongside technical capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01920.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "628f7a71dd993507cfcbe587",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
      "fullname": "Omartificial Intelligence Space",
      "name": "Omartificial-Intelligence-Space",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 100
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01920",
      "authors": [
        {
          "_id": "683ec5047ec12b4ee9a21215",
          "name": "Serry Sibaee",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21216",
          "name": "Omer Nacar",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21217",
          "name": "Adel Ammar",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21218",
          "name": "Yasser Al-Habashi",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21219",
          "name": "Abdulrahman Al-Batati",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a2121a",
          "name": "Wadii Boulila",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:39:50.000Z",
      "submittedOnDailyAt": "2025-06-03T08:20:38.744Z",
      "title": "La línea de respiración hacia la práctica: Nuevo paradigma para la evaluación de modelos de lenguaje árabe",
      "submittedOnDailyBy": {
        "_id": "628f7a71dd993507cfcbe587",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
        "isPro": true,
        "fullname": "Omartificial Intelligence Space",
        "user": "Omartificial-Intelligence-Space",
        "type": "user"
      },
      "summary": "Este artículo aborda los deficiencias críticos en la evaluación de modelos de lengua árabe, estableciendo una guía teórica específica y introduciendo un nuevo marco de evaluación. Primero, analiza los conjuntos de datos de evaluación árabe existentes para identificar problemas importantes en precisión lingüística, adecuación cultural y metodología. Para resolver estas limitaciones, propone el Conjunto de Datos Profundo Árabe (ADMD). El ADMD incluye 490 preguntas desafiantes y aborda 10 áreas principales (42 subáreas, referir al gráfico 1). Usando el ADMD, se evaluan 5 modelos recientes: GPT-4, Claude 3.5 Sonnet, Gemini Flash 1.5, CommandR 100B y Qwen-Max. Los resultados muestran significativas diferencias en el rendimiento de los modelos en diferentes áreas, destacando especialmente los problemas en las áreas que requieren un profundo entendimiento cultural y conocimientos especializados. Claude 3.5 Sonnet destaca por su excelencia en matemáticas árabes y en el área árabe-Israelí, con una precisión general del 30%, lo más alta de todos. Este estudio proporciona una base teórica y retroalimentación práctica para la mejora de la evaluación de modelos de lengua árabe, resaltando la importancia del entendimiento cultural y de la capacidad técnica.",
      "upvotes": 0,
      "discussionId": "683ec5137ec12b4ee9a21496",
      "ai_summary": "A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.",
      "ai_keywords": [
        "evaluation framework",
        "Arabic Depth Mini Dataset (ADMD)",
        "GPT-4",
        "Claude 3.5 Sonnet",
        "Gemini Flash 1.5",
        "CommandR 100B",
        "Qwen-Max",
        "cultural understanding",
        "specialized knowledge",
        "cultural competence"
      ]
    },
    "publishedAt": "2025-06-02T13:39:50.000Z",
    "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation",
    "summary": "This paper addresses critical gaps in Arabic language model evaluation by\nestablishing comprehensive theoretical guidelines and introducing a novel\nevaluation framework. We first analyze existing Arabic evaluation datasets,\nidentifying significant issues in linguistic accuracy, cultural alignment, and\nmethodological rigor. To address these limitations in LLMs, we present the\nArabic Depth Mini Dataset (ADMD), a carefully curated collection of 490\nchallenging questions spanning ten major domains (42 sub-domains, see Figure 1.\nUsing ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,\nGemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant\nvariations in model performance across different domains, with particular\nchallenges in areas requiring deep cultural understanding and specialized\nknowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%,\nshowing relative strength in mathematical theory in Arabic, Arabic language,\nand islamic domains. This work provides both theoretical foundations and\npractical insights for improving Arabic language model evaluation, emphasizing\nthe importance of cultural competence alongside technical capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01920.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "628f7a71dd993507cfcbe587",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
      "fullname": "Omartificial Intelligence Space",
      "name": "Omartificial-Intelligence-Space",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 100
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01713",
      "authors": [
        {
          "_id": "683ec81753981b08324ce57b",
          "name": "Zhongwei Wan",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce57c",
          "name": "Zhihao Dou",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce57d",
          "name": "Che Liu",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce57e",
          "name": "Yu Zhang",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce57f",
          "name": "Dongfei Cui",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce580",
          "name": "Qinjian Zhao",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce581",
          "name": "Hui Shen",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce582",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce583",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce584",
          "name": "Yifan Jiang",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce585",
          "name": "Yangfan He",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce586",
          "name": "Mi Zhang",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce587",
          "name": "Shen Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T14:21:44.000Z",
      "submittedOnDailyAt": "2025-06-03T08:34:07.706Z",
      "title": "SRPO: Mejora del Teoría de la Lógica de la Reflexiva Cognición Basada en Aprendizaje por Refuerzo de Modelos de LLM",
      "submittedOnDailyBy": {
        "_id": "631b9ff5824f2502e3557c7e",
        "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
        "isPro": true,
        "fullname": "liu",
        "user": "che111",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje multimodal de DeepMind (MLLMs) muestran las capacidades esperadas en tareas de razonamiento, pero enfrentan dificultades cuando se enfrentan a problemas complejos que requieren una clara autoreflexión y autoajuste, especialmente en comparación con la lógica basada en texto en modalidades únicas. Los métodos de retroalimentación actuales son sencillos, no generan preocupaciones significativas y las limitaciones de la capacidad de razonamiento y el conocimiento están bien establecidas en el entrenamiento inicial. Para superar estas desafíos, proponemos la Política de Optimización de Grupo de Retroalimentación (SRPO) para fortalecer la razonamiento en modelos multimodal de DeepMind. SRPO es un marco de aprendizaje por refuerzo con conocimiento de retroalimentación en dos etapas, especialmente diseñado para fortalecer la razonamiento. En la primera etapa, bajo la guía de un MLLM avanzado, se construye un conjunto de datos de retroalimentación de alta calidad y se genera retroalimentación basada en respuestas iniciales para impulsar el aprendizaje simultáneo de razonamiento y autoreflexión en el modelo de política. En la segunda etapa, se introduce una nueva estructura de recompensa dentro del marco de SRPO para impulsar retroalimentación clara y cognitivamente significativa sin repetir ciclos, evitando así el desgaste cognitivo. A través de experimentos expandidos en benchmarks multimodal de razonamiento como MathVista, MathVision, MathVerse y MMMU-Pro, utilizando modelos como Qwen-2.5-VL-7B y Qwen-2.5-VL-32B, SRPO ha superado significativamente los modelos líderes, logrando un gran avance en la precisión de la razonamiento y la calidad de la autoreflexión.",
      "upvotes": 0,
      "discussionId": "683ec81853981b08324ce5f1",
      "projectPage": "https://srpo.pages.dev/"
    },
    "publishedAt": "2025-06-02T10:21:44.000Z",
    "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware\n  Reinforcement Learning",
    "summary": "Multimodal large language models (MLLMs) have shown promising capabilities in\nreasoning tasks, yet still struggle with complex problems requiring explicit\nself-reflection and self-correction, especially compared to their unimodal\ntext-based counterparts. Existing reflection methods are simplistic and\nstruggle to generate meaningful and instructive feedback, as the reasoning\nability and knowledge limits of pre-trained models are largely fixed during\ninitial training. To overcome these challenges, we propose Multimodal\nSelf-Reflection enhanced reasoning with Group Relative Policy Optimization\n(SRPO), a two-stage reflection-aware reinforcement learning (RL) framework\nexplicitly designed to enhance multimodal LLM reasoning. In the first stage, we\nconstruct a high-quality, reflection-focused dataset under the guidance of an\nadvanced MLLM, which generates reflections based on initial responses to help\nthe policy model learn both reasoning and self-reflection. In the second stage,\nwe introduce a novel reward mechanism within the GRPO framework that encourages\nconcise and cognitively meaningful reflection while avoiding redundancy.\nExtensive experiments across multiple multimodal reasoning benchmarks,\nincluding MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B\nand Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms\nstate-of-the-art models, achieving notable improvements in both reasoning\naccuracy and reflection quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01713.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631b9ff5824f2502e3557c7e",
      "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
      "fullname": "liu",
      "name": "che111",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15772",
      "authors": [
        {
          "_id": "683ec87a4246cd3c413046e1",
          "name": "Yifan Cheng",
          "hidden": false
        },
        {
          "_id": "683ec87a4246cd3c413046e2",
          "name": "Ruoyi Zhang",
          "hidden": false
        },
        {
          "_id": "683ec87a4246cd3c413046e3",
          "name": "Jiatong Shi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:23:12.000Z",
      "submittedOnDailyAt": "2025-06-03T08:35:09.353Z",
      "title": "MIKU-PAL: Método para la automatización y normalización de diversas informaciones, excluyendo el significado lingüístico semántico, y su detección.",
      "submittedOnDailyBy": {
        "_id": "6607d9c2d81d6112498810b9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6607d9c2d81d6112498810b9/mmwx-SFEP-6gjnAdjbcxb.png",
        "isPro": false,
        "fullname": "PoTaTo",
        "user": "PoTaTo721",
        "type": "user"
      },
      "summary": "La recolección de grandes cantidades de datos de oraciones con emociones que mantengan una alta coincidencia es un gran desafío en la síntesis de oraciones. En este artículo, se presenta el completamente automatizado multimodelo pipeline \"MIKU-PAL\" para extraer altas calidades de coincidencia de oraciones con emociones a partir de videos sin etiquetas. Utilizando algoritmos de detección y seguimiento de caras, se desarrolló un sistema de análisis de emociones automatizado basado en múltiples modelos y modelos de lenguaje (MLLM). MIKU-PAL logró una precisión humana (68.5% en MELD) y una alta coincidencia (Fleiss kappa de 0.93), operando a un costo más bajo y con mayor rapidez que la etiquetado humano. Con la alta calidad de etiquetado flexible y de alta coincidencia obtenido con MIKU-PAL, se registraron 26 categorías detalladas de emociones en oraciones y se evaluaron 83% de las etiquetas humanas. Basándose en este sistema, se lanzó un nuevo benchmark de oraciones y visualizaciones de emociones, \"MIKU-EmoBench\", que consiste en un conjunto de datos detallados de 131.2 horas de oraciones con emociones.",
      "upvotes": 0,
      "discussionId": "683ec87a4246cd3c41304708"
    },
    "publishedAt": "2025-05-21T13:23:12.000Z",
    "title": "MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech\n  Paralinguistic and Affect Labeling",
    "summary": "Acquiring large-scale emotional speech data with strong consistency remains a\nchallenge for speech synthesis. This paper presents MIKU-PAL, a fully automated\nmultimodal pipeline for extracting high-consistency emotional speech from\nunlabeled video data. Leveraging face detection and tracking algorithms, we\ndeveloped an automatic emotion analysis system using a multimodal large\nlanguage model (MLLM). Our results demonstrate that MIKU-PAL can achieve\nhuman-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss\nkappa score) while being much cheaper and faster than human annotation. With\nthe high-quality, flexible, and consistent annotation from MIKU-PAL, we can\nannotate fine-grained speech emotion categories of up to 26 types, validated by\nhuman annotators with 83% rationality ratings. Based on our proposed system, we\nfurther released a fine-grained emotional speech dataset MIKU-EmoBench(131.2\nhours) as a new benchmark for emotional text-to-speech and visual voice\ncloning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15772.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6607d9c2d81d6112498810b9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6607d9c2d81d6112498810b9/mmwx-SFEP-6gjnAdjbcxb.png",
      "fullname": "PoTaTo",
      "name": "PoTaTo721",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  }
]