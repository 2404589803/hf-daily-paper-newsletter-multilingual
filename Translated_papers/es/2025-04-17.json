[
  {
    "paper": {
      "id": "2504.10514",
      "authors": [
        {
          "_id": "67ffedb8b0c26d6ec0b608cf",
          "name": "Yijun Liang",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d0",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d1",
          "user": {
            "_id": "64a8121e35fab7cd04c30ed0",
            "avatarUrl": "/avatars/48849b84703158772f1022932331b143.svg",
            "isPro": false,
            "fullname": "Chenrui Fan",
            "user": "Fcr09",
            "type": "user"
          },
          "name": "Chenrui Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:05:05.662Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d2",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d3",
          "name": "Dang Nguyen",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d4",
          "user": {
            "_id": "63f546e0fcf95ecac2b0ee3e",
            "avatarUrl": "/avatars/02a401bcff91cc473d9946bbb771a985.svg",
            "isPro": false,
            "fullname": "Kwesi Cobbina",
            "user": "kweCobi",
            "type": "user"
          },
          "name": "Kwesi Cobbina",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:36.225Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d5",
          "user": {
            "_id": "639d4b8d860db464ae35c3ab",
            "avatarUrl": "/avatars/ec0fa3e91593a03fc9fb611e66b30553.svg",
            "isPro": false,
            "fullname": "Shweta Bhardwaj",
            "user": "shweta12",
            "type": "user"
          },
          "name": "Shweta Bhardwaj",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:29.680Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d6",
          "user": {
            "_id": "6393847e3e30234ae798b7be",
            "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
            "isPro": true,
            "fullname": "JiuhaiChen",
            "user": "jiuhai",
            "type": "user"
          },
          "name": "Jiuhai Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:22.786Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d7",
          "name": "Fuxiao Liu",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d8",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:05:07.396Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T16:36:26.000Z",
      "submittedOnDailyAt": "2025-04-17T00:58:33.032Z",
      "title": "ColorBench: VLMs pueden ver y comprender el mundo de los colores? Marca de prueba detallada de visualización de colores, filosofía y robustez.",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Ver colores desempeña un papel importante en la cognición humana y proporciona razones visuales para las competencias de color. Sin embargo, no está claro si los modelos de lenguaje visual (VLMs) pueden reconocer, comprender y utilizar colores de manera similar a la humana. En este artículo, se presenta un nuevo benchmark llamado \"ColorBench\" para evaluar la capacidad de comprensión de colores. Este benchmark diseña varios escenarios de prueba para evaluar la robustez en observación de colores, razones y transformación de colores. Basado en aplicaciones mundialmente famosas, ColorBench evalúa cómo estos modelos pueden ver colores, deducir significados en competencias de color y detectar y mantener cambios debidos a transformaciones de color. Para evaluar 32 VLMs (modelos de lenguaje y codificadores visuales diferentes), el artículo revela los siguientes nuevos hallazgos: (i) la ley del tamaño (mejores grandes modelos) se mantiene en ColorBench, y el modelo de lenguaje desempeña un papel más importante que el codificador visual. (ii) Sin embargo, la diferencia en rendimiento entre modelos es relativamente pequeña, y los VLMs existentes superan significativamente en la comprensión de colores. (iii) La razón de comprensión (CoT) mejora la precisión y robustez en la comprensión de colores, pero es una tarea de sensores visuales. (iv) ColorBench utiliza competencias de color, pero no lo hace para que los modelos puedan realizar tareas diferentes en diferentes contextos.",
      "upvotes": 18,
      "discussionId": "67ffedbeb0c26d6ec0b60a5b",
      "projectPage": "https://huggingface.co/datasets/umd-zhou-lab/ColorBench",
      "githubRepo": "https://github.com/tianyi-lab/ColorBench",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "color understanding",
        "color perception",
        "color-based cues",
        "color transformations",
        "scaling law",
        "language model",
        "vision encoder",
        "CoT reasoning",
        "multimodal AI"
      ]
    },
    "publishedAt": "2025-04-10T12:36:26.000Z",
    "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
    "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10514.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12240",
      "authors": [
        {
          "_id": "680057b49031335df49732fc",
          "user": {
            "_id": "64970d3d9c3b29dca8633f87",
            "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
            "isPro": false,
            "fullname": "JunhaoZhuang",
            "user": "JunhaoZhuang",
            "type": "user"
          },
          "name": "Junhao Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:04:09.222Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732fd",
          "user": {
            "_id": "66837d3c48edefb453b0640a",
            "avatarUrl": "/avatars/b16385eaa612578728e2c6460a76b38f.svg",
            "isPro": false,
            "fullname": "Lingen Li",
            "user": "l-li",
            "type": "user"
          },
          "name": "Lingen Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:09:48.492Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732fe",
          "user": {
            "_id": "62d4577bc85b0fcf7fde39bb",
            "avatarUrl": "/avatars/a3a5729e33ae89ce9ba408830db3c835.svg",
            "isPro": false,
            "fullname": "Xuan Ju",
            "user": "juxuan27",
            "type": "user"
          },
          "name": "Xuan Ju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:09:55.870Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732ff",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "680057b49031335df4973300",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "680057b49031335df4973301",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:12:47.014Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
      ],
      "publishedAt": "2025-04-16T16:45:19.000Z",
      "submittedOnDailyAt": "2025-04-17T00:32:24.205Z",
      "title": "Cóbra: Utilizando referencias amplias para una pintura eficiente de diseños",
      "submittedOnDailyBy": {
        "_id": "64970d3d9c3b29dca8633f87",
        "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
        "isPro": false,
        "fullname": "JunhaoZhuang",
        "user": "JunhaoZhuang",
        "type": "user"
      },
      "summary": "En la industria de la producción de manga, es necesario un color overlay con precisión, eficiencia, coherencia en contexto y control flexible. Una página de manga incluye diversos personajes, objetos y fondos, lo que hace que la secuencia del color overlay sea compleja. A pesar del desarrollo de los modelos de difusión para la generación de imágenes, su aplicación en el color overlay de la ilustración está limitada, presentando problemas como el procesamiento de imágenes de difusión, tiempos de inferencia largos y problemas de control flexible. Para resolver estos problemas, Cobra proporciona una metodología eficiente y flexible que soporta sugerencias de color y mantiene un bajo Latency, incluso con más de 200 imágenes de referencia. El núcleo de Cobra es la arquitectura Causal Sparse DiT, diseñada especialmente para manejar eficazmente referencias de largo contexto y garantizar la coherencia del color, utilizando codificaciones de posición especiales, atención rara causal y cachés de claves-valores. Finalmente, Cobra realiza un color overlay preciso de ilustraciones con referencias de amplio contexto, mejora significativamente la velocidad de inferencia e interacción, y satisface los requisitos importantes de la industria. Nuestro código y modelo están publicados en la página de proyecto: https://zhuang2002.github.io/Cobra/",
      "upvotes": 16,
      "discussionId": "680057b89031335df497343e",
      "projectPage": "https://zhuang2002.github.io/Cobra/",
      "githubRepo": "https://github.com/zhuang2002/Cobra",
      "ai_keywords": [
        "diffusion models",
        "line art colorization",
        "contextual image guidance",
        "color hints",
        "Causal Sparse DiT architecture",
        "positional encodings",
        "causal sparse attention",
        "Key-Value Cache",
        "long-context references",
        "color identity consistency"
      ]
    },
    "publishedAt": "2025-04-16T12:45:19.000Z",
    "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
    "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64970d3d9c3b29dca8633f87",
      "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
      "fullname": "JunhaoZhuang",
      "name": "JunhaoZhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12285",
      "authors": [
        {
          "_id": "68006e6e175c8dce4ec17f7a",
          "user": {
            "_id": "613f07f40153aafa379775f2",
            "avatarUrl": "/avatars/3965175b320d753d9a5ccb0c7d9298a4.svg",
            "isPro": false,
            "fullname": "Shuming Ma",
            "user": "shumingma",
            "type": "user"
          },
          "name": "Shuming Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:13:17.676Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7b",
          "user": {
            "_id": "63f71771d36951307fcb4dcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
            "isPro": false,
            "fullname": "Hongyu Wang",
            "user": "hongyuw",
            "type": "user"
          },
          "name": "Hongyu Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:14:45.597Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7c",
          "user": {
            "_id": "632bd2f72d6a805eeb4bc601",
            "avatarUrl": "/avatars/6e1533e8a599f3068290aa69ac82cab7.svg",
            "isPro": false,
            "fullname": "HUANG SHAOHAN",
            "user": "buaahsh",
            "type": "user"
          },
          "name": "Shaohan Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:14:59.750Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7d",
          "user": {
            "_id": "64abbcff6cadc7aca584f71b",
            "avatarUrl": "/avatars/fc6e85ad4a8befd133a37b411712c648.svg",
            "isPro": false,
            "fullname": "Xingxing Zhang",
            "user": "THU-CHUNXIA",
            "type": "user"
          },
          "name": "Xingxing Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:15:06.359Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7e",
          "name": "Ying Hu",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7f",
          "name": "Ting Song",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f80",
          "name": "Yan Xia",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f81",
          "user": {
            "_id": "6368c512fbfe97c16a40baba",
            "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
            "isPro": false,
            "fullname": "Furu Wei",
            "user": "thegenerality",
            "type": "user"
          },
          "name": "Furu Wei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:15:42.250Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T17:51:43.000Z",
      "submittedOnDailyAt": "2025-04-17T01:29:56.744Z",
      "title": "BitNet b1.58 2B4T Informe Técnico",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "Bitnet b1.58 2B4T es el primer modelo de lenguaje de programación grande (LLM) abierto fuente y nativo en 1 bit. Consta de 20 mil millones de parámetros y se entrenó con un corpus de 40 mil millones de tokens. El modelo se sometió a evaluaciones rigurosas en comprensión del lenguaje, lógica matemática, capacidades de programación y traducción. Bitnet b1.58 2B4T logró alcanzar un líder en peso abierto de la misma escala y el mismo rendimiento, ofreciendo una ventaja significativa en términos de eficiencia de cálculo. La consumo de memoria, energía y el retraso de decisión se redujeron considerablemente. Los pesos del modelo están disponibles en Hugging Face y incluyen implementaciones de inferencia abierto fuente para arquitecturas de GPU y CPU.",
      "upvotes": 12,
      "discussionId": "68006e70175c8dce4ec17fc0",
      "ai_keywords": [
        "BitNet b1.58 2B4T",
        "Large Language Model (LLM)",
        "1-bit Large Language Model",
        "Train",
        "Corpus",
        "4 trillion tokens",
        "Benchmarks",
        "Language understanding",
        "Mathematical reasoning",
        "Coding proficiency",
        "Conversational ability"
      ]
    },
    "publishedAt": "2025-04-16T13:51:43.000Z",
    "title": "BitNet b1.58 2B4T Technical Report",
    "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12285.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10326",
      "authors": [
        {
          "_id": "67ff772061373fdf16ce1d38",
          "user": {
            "_id": "66486ba1640bc89c93bcc8a2",
            "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
            "isPro": false,
            "fullname": "Yangshen Deng",
            "user": "YangshenDeng",
            "type": "user"
          },
          "name": "Yangshen Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T11:57:58.302Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d39",
          "name": "Zhengxin You",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3a",
          "name": "Long Xiang",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3b",
          "user": {
            "_id": "66d6be0bddf54fd90923c727",
            "avatarUrl": "/avatars/7bb82c8c339db944d79d47b3b9b35aa8.svg",
            "isPro": false,
            "fullname": "Li",
            "user": "Qilong00",
            "type": "user"
          },
          "name": "Qilong Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:16.665Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3c",
          "user": {
            "_id": "66efe50667c4ce2c9024cd45",
            "avatarUrl": "/avatars/7b7f650953c371f08a5beecc500b6a43.svg",
            "isPro": false,
            "fullname": "peiqiyuan",
            "user": "YuanPeiqi",
            "type": "user"
          },
          "name": "Peiqi Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:22.829Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3d",
          "name": "Zhaoyang Hong",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3e",
          "user": {
            "_id": "66d6c339b61dd11022907252",
            "avatarUrl": "/avatars/d2ed3cc003e94b2e5204ce0f8a481dcf.svg",
            "isPro": false,
            "fullname": "Yitao Zheng",
            "user": "FeTieTer",
            "type": "user"
          },
          "name": "Yitao Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:36.808Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3f",
          "name": "Wanting Li",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d40",
          "user": {
            "_id": "671a7bce2b10d343bab18637",
            "avatarUrl": "/avatars/af1c10a59236d953b42e67d3955eecc4.svg",
            "isPro": false,
            "fullname": "runzhong",
            "user": "runzhongli",
            "type": "user"
          },
          "name": "Runzhong Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:17:02.031Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d41",
          "user": {
            "_id": "63898b61ec1f539adc0f4da2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674248167280-63898b61ec1f539adc0f4da2.jpeg",
            "isPro": false,
            "fullname": "Haotian Liu",
            "user": "liuhaotian",
            "type": "user"
          },
          "name": "Haotian Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:17:25.728Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d42",
          "name": "Kyriakos Mouratidis",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d43",
          "name": "Man Lung Yiu",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d44",
          "name": "Huan Li",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d45",
          "name": "Qiaomu Shen",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d46",
          "name": "Rui Mao",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d47",
          "user": {
            "_id": "66b02a2642c34e7a212133c0",
            "avatarUrl": "/avatars/737a69b095e8c427ecd08f870b173635.svg",
            "isPro": false,
            "fullname": "Bo Tang",
            "user": "BO1022",
            "type": "user"
          },
          "name": "Bo Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:18:01.460Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T15:34:26.000Z",
      "submittedOnDailyAt": "2025-04-17T06:00:20.705Z",
      "title": "AlayaDB: Eficiente y efectivo data-based inference for long context LLMs",
      "submittedOnDailyBy": {
        "_id": "66486ba1640bc89c93bcc8a2",
        "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
        "isPro": false,
        "fullname": "Yangshen Deng",
        "user": "YangshenDeng",
        "type": "user"
      },
      "summary": "AliaDB es el sistema de base de datos de vectores más avanzado que construye eficientemente y efectivamente la inferencia de largo contexto de modelos de lenguaje del cerebro (LLM) desarrollados en AliaDB AI. En particular, se separan el sistema de caché de clave-valor (KV) y las operaciones de atención en el sistema de inferencia de LLM, y estos se empaquetan en un nuevo sistema de base de datos de vectores. Para los proveedores de servicios de modelo (Model as a Service, MaaS), proporcionan una cantidad reducida de recursos hardware y una alta calidad de generación, comparado con soluciones alternativas actuales (por ejemplo, la distribución de caché de KV y la atención basada en búsqueda). La esencia de AliaDB es la abstracción de la computación de atención y el manejo de caché en el proceso de procesamiento de consultas, lo que optimiza el rendimiento con operadores de consultas en el noteral. En este artículo, se muestra la eficiencia de AliaDB a través de tres casos de uso provenientes de socios de la industria y de resultados de experimentos de benchmark de inferencia de LLM ampliados.",
      "upvotes": 12,
      "discussionId": "67ff772261373fdf16ce1d93",
      "ai_keywords": [
        "vector database",
        "long-context inference",
        "Large Language Models (LLMs)",
        "KV cache",
        "attention computation",
        "Model as a Service (MaaS)",
        "Service Level Objectives (SLOs)",
        "KV cache disaggregation",
        "retrieval-based sparse attention",
        "query processing procedure",
        "native query optimizer",
        "LLM inference benchmarks"
      ]
    },
    "publishedAt": "2025-04-14T11:34:26.000Z",
    "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
    "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10326.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66486ba1640bc89c93bcc8a2",
      "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
      "fullname": "Yangshen Deng",
      "name": "YangshenDeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09081",
      "authors": [
        {
          "_id": "67fdbfcccaa65039e8c3d8ae",
          "user": {
            "_id": "67dcd93f73e2178fe917b893",
            "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
            "isPro": false,
            "fullname": "Prabhat Pandey",
            "user": "panprabh",
            "type": "user"
          },
          "name": "Prabhat Pandey",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:52.370Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8af",
          "name": "Rupak Vignesh Swaminathan",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b0",
          "user": {
            "_id": "66279810009f1bfdc6bf71bf",
            "avatarUrl": "/avatars/97f561c91b92e1abb4fe6f0b5c688126.svg",
            "isPro": false,
            "fullname": "Girish",
            "user": "vijaygirish2001",
            "type": "user"
          },
          "name": "K V Vijay Girish",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:49.809Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b1",
          "user": {
            "_id": "66367dfb2d6b86ff193dbbe0",
            "avatarUrl": "/avatars/0a0c230bb5fc81a28a166691146cf807.svg",
            "isPro": false,
            "fullname": "Arunasish Sen",
            "user": "svinxz",
            "type": "user"
          },
          "name": "Arunasish Sen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T11:58:01.767Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b2",
          "name": "Jian Xie",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b3",
          "name": "Grant P. Strimel",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b4",
          "name": "Andreas Schwarz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-12T04:45:48.000Z",
      "submittedOnDailyAt": "2025-04-17T07:32:43.212Z",
      "title": "SIFT-50M: Fine-tuning en un conjunto de datos multilingüe de voz grande",
      "submittedOnDailyBy": {
        "_id": "67dcd93f73e2178fe917b893",
        "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
        "isPro": false,
        "fullname": "Prabhat Pandey",
        "user": "panprabh",
        "type": "user"
      },
      "summary": "SIFT (Fine-Tuning de Instrucciones de Habla) presenta un conjunto de datos de 50M elementos. Este conjunto está diseñado para fines de ajuste micro y pre-entrenamiento de modelos de lenguaje de gran escala (LLMs) para habla. SIFT-50M ha sido construido como un corpus de habla disponible públicamente, que incluye un total de 14K horas de habla y se utiliza tanto con LLMs como con modelos profesionales comerciales. Este conjunto de datos se ha extendido a 5 idiomas, respondiendo a diversas instrucciones de comprensión de habla y generación de habla controlada. Usando SIFT-50M, se entrenó SIFT-LLM, que ha demostrado ser superior en el benchmark de correspondencia a instrucciones en los LLMs de habla y texto, alcanzando niveles de rendimiento computacional en tareas básicas de habla. Además, se presenta EvalSIFT para apoyar investigaciones avanzadas. EvalSIFT es un conjunto de datos de benchmark especialmente diseñado para evaluar la capacidad de correspondencia a instrucciones de los LLMs de habla y texto.",
      "upvotes": 9,
      "discussionId": "67fdbfe0caa65039e8c3de4b",
      "ai_keywords": [
        "SIFT (Speech Instruction Fine-Tuning)",
        "speech-text large language models (LLMs)",
        "instruction fine-tuning",
        "pre-training",
        "speech corpora",
        "off-the-shelf expert models",
        "speech understanding",
        "controllable speech generation",
        "SIFT-LLM",
        "instruction-following benchmarks",
        "foundational speech tasks",
        "EvalSIFT"
      ]
    },
    "publishedAt": "2025-04-12T00:45:48.000Z",
    "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning",
    "summary": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09081.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67dcd93f73e2178fe917b893",
      "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
      "fullname": "Prabhat Pandey",
      "name": "panprabh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10483",
      "authors": [
        {
          "_id": "67fdd3e3913c97aa32f94e9b",
          "user": {
            "_id": "641480554b1701c01cdb36c4",
            "avatarUrl": "/avatars/f1f6b294e0236d76a68c099164c81f36.svg",
            "isPro": false,
            "fullname": "Xingjian Leng",
            "user": "xingjianleng",
            "type": "user"
          },
          "name": "Xingjian Leng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:36.449Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9c",
          "name": "Jaskirat Singh",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9d",
          "user": {
            "_id": "6752870ec63bc5b670b1b27e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6752870ec63bc5b670b1b27e/3CdHxnyTKbGup-1V67nEV.jpeg",
            "isPro": false,
            "fullname": "Yunzhong Hou",
            "user": "yunzhong-hou",
            "type": "user"
          },
          "name": "Yunzhong Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:11.946Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9e",
          "user": {
            "_id": "63bf533f4a2beec65568f813",
            "avatarUrl": "/avatars/bba583653b5cada8dd4a3ff2281e9dec.svg",
            "isPro": false,
            "fullname": "Xing",
            "user": "Zhenchang",
            "type": "user"
          },
          "name": "Zhenchang Xing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:21.190Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9f",
          "user": {
            "_id": "6596422646624a86ff3b3bda",
            "avatarUrl": "/avatars/216e12b77e45ac5f1fa20932f5745411.svg",
            "isPro": false,
            "fullname": "Saining Xie",
            "user": "sainx",
            "type": "user"
          },
          "name": "Saining Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:39.700Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94ea0",
          "user": {
            "_id": "666351ebd86c026caa135e5c",
            "avatarUrl": "/avatars/50a37f7e999f660c69f518b71577eb7d.svg",
            "isPro": false,
            "fullname": "Liang Zheng",
            "user": "liangzheng06",
            "type": "user"
          },
          "name": "Liang Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:56.041Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:59:53.000Z",
      "submittedOnDailyAt": "2025-04-17T05:08:17.990Z",
      "title": "REPA-E: Utilizando un difusor potencial para ajustar el sistema de VAE desde el final hasta el final. Transformers",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "En este artículo se investiga un problema básico: \"¿Es posible entrenar un modelo de difusión potencial y un tokenizador de VAE desde principios hasta fin?\" Según el conocimiento actual de aprendizaje profundo, entrenar desde principios hasta fin es lo más óptimo posible. Sin embargo, en el caso de los transformadores de difusión potencial, entrenar desde principios hasta fin utilizando la pérdida de difusión estándar para VAE y modelos de difusión provoca una pérdida de rendimiento final. Demostramos que la pérdida de difusión no es efectiva, pero es posible entrenar desde principios hasta fin utilizando la pérdida de REPA (Representation Arrayment), lo que nos permite ajustar los modelos durante el proceso de entrenamiento. Sin diferencias significativas, el receta de entrenamiento propuesto (REPA-E) muestra un desempeño sorprendente. En comparación con las recetas de entrenamiento de REPA y VI, la velocidad de entrenamiento del modelo de difusión se acelera en más de 17 veces y más de 45 veces. Interesantemente, entrenar desde principios hasta fin con REPA-E mejora el VAE en sí mismo y mejora la estructura del espacio potencial y el rendimiento de generación en flujos descendentes. En términos de rendimiento final, nuestro enfoque es el nuevo mejor resultado. En ImageNet 256 x 256, se alcanzan los valores de FID de 1.26 y 1.83, sin y con guías de clase. El código está disponible en https://end2end-diffusion.github.io.",
      "upvotes": 3,
      "discussionId": "67fdd3e5913c97aa32f94ee3",
      "projectPage": "https://end2end-diffusion.github.io/",
      "githubRepo": "https://github.com/End2End-Diffusion/REPA-E",
      "ai_keywords": [
        "latent diffusion models",
        "variational auto-encoder (VAE) tokenizer",
        "end-to-end training",
        "diffusion-loss",
        "representation-alignment (REPA) loss",
        "diffusion model training",
        "VAE",
        "latent space structure",
        "downstream generation performance",
        "FID",
        "ImageNet 256 x 256"
      ]
    },
    "publishedAt": "2025-04-14T13:59:53.000Z",
    "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers",
    "summary": "In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10483.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 820
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11952",
      "authors": [
        {
          "_id": "6800646e0679d4ec4b9d01a7",
          "user": {
            "_id": "645c60dd7d655680b57ddbff",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
            "isPro": true,
            "fullname": "Ram Kadiyala",
            "user": "1024m",
            "type": "user"
          },
          "name": "Ram Mohan Rao Kadiyala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:04:05.627Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01a8",
          "user": {
            "_id": "63da8ba3f03c3d71ef32408c",
            "avatarUrl": "/avatars/1b2e6f3ea2bac5ab35dbd53edb7f8cf2.svg",
            "isPro": false,
            "fullname": "Siddartha Pullakhandam",
            "user": "Siddartha10",
            "type": "user"
          },
          "name": "Siddartha Pullakhandam",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:18:54.938Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01a9",
          "name": "Kanwal Mehreen",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01aa",
          "user": {
            "_id": "618c1ad1c74578e0a4a4d074",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618c1ad1c74578e0a4a4d074/8u_AkeHt4d6xtQ8hzaffU.jpeg",
            "isPro": true,
            "fullname": "Drishti Sharma",
            "user": "DrishtiSharma",
            "type": "user"
          },
          "name": "Drishti Sharma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:19:22.573Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ab",
          "name": "Siddhant Gupta",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ac",
          "user": {
            "_id": "66c578770a22b2f9ab575847",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c578770a22b2f9ab575847/-zfNho1DR3yZHDazq669-.png",
            "isPro": false,
            "fullname": "Jebish Purbey",
            "user": "jebish7",
            "type": "user"
          },
          "name": "Jebish Purbey",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:00.066Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ad",
          "user": {
            "_id": "653d84f13fc9c706fa755d03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653d84f13fc9c706fa755d03/F_jYbeuLLM9EX8hKXcbHD.png",
            "isPro": false,
            "fullname": "Ashay Srivastava",
            "user": "ashay-sriv",
            "type": "user"
          },
          "name": "Ashay Srivastava",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:15.588Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ae",
          "user": {
            "_id": "669a745a4bbe8ad52ee287cf",
            "avatarUrl": "/avatars/245644aa638b45a17ff71124bd5bbe0f.svg",
            "isPro": false,
            "fullname": "Subhasya Tippareddy",
            "user": "subhasyar",
            "type": "user"
          },
          "name": "Subhasya TippaReddy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:23.946Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01af",
          "user": {
            "_id": "669a7383c9111326dc596f5e",
            "avatarUrl": "/avatars/8bbd307fd4bb2d7055b2c8fc9140dc81.svg",
            "isPro": false,
            "fullname": "Arvind Reddy Bobbili",
            "user": "Arvindreddy",
            "type": "user"
          },
          "name": "Arvind Reddy Bobbili",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:32.804Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b0",
          "name": "Suraj Telugara Chandrashekhar",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b1",
          "user": {
            "_id": "668db0bfb09a05f3d7cc796f",
            "avatarUrl": "/avatars/969c511cca4d129b99eca6252a468385.svg",
            "isPro": false,
            "fullname": "Modabbir Adeeb",
            "user": "moda10",
            "type": "user"
          },
          "name": "Modabbir Adeeb",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:46.461Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b2",
          "user": {
            "_id": "641c3337f0b71a9743629985",
            "avatarUrl": "/avatars/820b124887f173c263a675728baf99c6.svg",
            "isPro": false,
            "fullname": "Srinadh Vura",
            "user": "SriV",
            "type": "user"
          },
          "name": "Srinadh Vura",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:53.315Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b3",
          "name": "Hamza Farooq",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T10:29:30.000Z",
      "submittedOnDailyAt": "2025-04-17T00:47:10.039Z",
      "title": "El poderoso diseño de líneas de color rosa en el texto generado por IA",
      "submittedOnDailyBy": {
        "_id": "645c60dd7d655680b57ddbff",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
        "isPro": true,
        "fullname": "Ram Kadiyala",
        "user": "1024m",
        "type": "user"
      },
      "summary": "Un sistema de detección de contenido generado por máquina ideal debe funcionar eficazmente en los muchos LLM avanzados que continúan mejorando diariamente. Actualmente, los sistemas tienen dificultades para reconocer con precisión el contenido generado por AI en breves frases. Además, no todos los textos son completamente escritos por humanos o por un LLM solo; muchos son coautores. En este estudio, se ha construido un conjunto de modelos adaptados a la clasificación de clases de etiquetas, que se entrenan en una amplia colección de frases coautoradas por humanos y máquinas. Este conjunto de modelos funciona efectivamente en nuevos dominios, nuevos generadores, frases sin soporte y entradas adversarias. Además, se proporcionaron alrededor de 2.4 millones de frases coautoradas por este tipo de modelos en 23 lenguajes populares. Además, se evaluó el rendimiento del modelo para cada dominio y generador, comparó el rendimiento contrario de métodos, y comparó las características de las frases de entrada, las frases generadas y las de los autores humanos.",
      "upvotes": 2,
      "discussionId": "680064710679d4ec4b9d0224",
      "ai_keywords": [
        "token classification",
        "adversarial inputs"
      ]
    },
    "publishedAt": "2025-04-16T06:29:30.000Z",
    "title": "Robust and Fine-Grained Detection of AI Generated Texts",
    "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645c60dd7d655680b57ddbff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
      "fullname": "Ram Kadiyala",
      "name": "1024m",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11092",
      "authors": [
        {
          "_id": "6800a43e1fd95d7dc21d6b83",
          "user": {
            "_id": "63e367d3fae035bdc4c347fc",
            "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
            "isPro": false,
            "fullname": "Jiaxin Huang",
            "user": "JaceyH919",
            "type": "user"
          },
          "name": "Jiaxin Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:03:59.476Z",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b84",
          "name": "Sheng Miao",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b85",
          "name": "BangBnag Yang",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b86",
          "name": "Yuewen Ma",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b87",
          "name": "Yiyi Liao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T11:38:14.000Z",
      "submittedOnDailyAt": "2025-04-17T07:32:38.400Z",
      "title": "Vivid4D: Video Inferencing para Mejorar la Reconstrucción 4D con Cámaras Monoculares",
      "submittedOnDailyBy": {
        "_id": "63e367d3fae035bdc4c347fc",
        "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
        "isPro": false,
        "fullname": "Jiaxin Huang",
        "user": "JaceyH919",
        "type": "user"
      },
      "summary": "Reconstruir 4D dinámico el bocado de escenas desde vídeos de vídeo de intervalo es válido pero muy difícil. Cada etapa temporal puede ser vista desde un solo punto de vista. Presentamos Vivid4D. Vivid4D es un nuevo enfoque que fortalece la perspectiva de observación para mejorar la síntesis de vídeos de escenas de 4D. A diferencia de los métodos existentes, cada uno de estos métodos no considera la línea de frente geométrica o utiliza una línea de frente generativa mientras ignora la geometría. Nosotros integramos dos enfoques y redefinimos la tarea de detección de entradas de imágenes con la perspectiva de observación. La perspectiva observada se proyecta como una nueva perspectiva. Para ello, entrenamos un modelo de detección de entradas de imágenes con vídeos de web sin límites que tienen máscaras de síntesis. Esto asegura la consistencia espacial y temporal. Además, para reducir la incertidumbre en la línea de frente de profundidad de la escena, introducimos un algoritmo de ampliación iterativa de la observación y un pérdida de reconstrucción robusta. Los experimentos demuestran la efectividad de nuestro método en mejorar la reconstrucción y completación de escenas de 4D.",
      "upvotes": 2,
      "discussionId": "6800a4441fd95d7dc21d6d46",
      "projectPage": "https://xdimlab.github.io/Vivid4D/",
      "ai_keywords": [
        "Vivid4D",
        "4D monocular video synthesis",
        "view augmentation",
        "video inpainting",
        "monocular depth priors",
        "unposed web videos",
        "synthetically generated masks",
        "iterative view augmentation strategy",
        "robust reconstruction loss"
      ]
    },
    "publishedAt": "2025-04-15T07:38:14.000Z",
    "title": "Vivid4D: Improving 4D Reconstruction from Monocular Video by Video\n  Inpainting",
    "summary": "Reconstructing 4D dynamic scenes from casually captured monocular videos is\nvaluable but highly challenging, as each timestamp is observed from a single\nviewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular\nvideo synthesis by augmenting observation views - synthesizing multi-view\nvideos from a monocular input. Unlike existing methods that either solely\nleverage geometric priors for supervision or use generative priors while\noverlooking geometry, we integrate both. This reformulates view augmentation as\na video inpainting task, where observed views are warped into new viewpoints\nbased on monocular depth priors. To achieve this, we train a video inpainting\nmodel on unposed web videos with synthetically generated masks that mimic\nwarping occlusions, ensuring spatially and temporally consistent completion of\nmissing regions. To further mitigate inaccuracies in monocular depth priors, we\nintroduce an iterative view augmentation strategy and a robust reconstruction\nloss. Experiments demonstrate that our method effectively improves monocular 4D\nscene reconstruction and completion.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11092.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e367d3fae035bdc4c347fc",
      "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
      "fullname": "Jiaxin Huang",
      "name": "JaceyH919",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11536",
      "authors": [
        {
          "_id": "6800cc7159e20f50cc282e87",
          "name": "Jiazhan Feng",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e88",
          "name": "Shijue Huang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e89",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8a",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8b",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8c",
          "name": "Baoquan Zhong",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8d",
          "name": "Chengquan Jiang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8e",
          "name": "Jinxin Chi",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8f",
          "name": "Wanjun Zhong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T18:10:22.000Z",
      "submittedOnDailyAt": "2025-04-17T08:10:39.383Z",
      "title": "ReTool: Aprendizaje de Máquina Basado en el Aprendizaje por Refuerzo sobre el Uso de Herramientas Estratégicas",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "El modelo de lenguaje de código (por ejemplo, DeepSeek R1) se ha entrenado mediante aprendizaje por refuerzo (RL), destacando su capacidad para superar la teoría de la razón del contexto, pero en casos de resolución de problemas estructurados, el modelo de interpretador de código (CI) es excepcional en áreas como la lectura de código, cálculos eficientes y la resolución de ecuaciones complejas. Para llenar estas brechas, se propone ReTool. ReTool integra herramientas en el entrenamiento para fortalecer la teoría de la razón en largos contextos y está compuesto por dos funciones principales: 1. La ejecución dinámica de código profundo en procesos de razón natural, y 2. Permite la búsqueda de políticas de políticas de retroalimentación en el aprendizaje automático, que incluye la ejecución de código profundo, y el modelo aprende qué herramienta llama basándose en los retroalimentos de los resultados. ReTool comienza con la generación de datos frios sintéticos y ajusta el modelo a través de la creación de trazas de razón del contexto largo en la adición de código. El entrenamiento por retroalimentación posterior utiliza los resultados de las tareas como recompensas, mejorando gradualmente la estrategia de uso de herramientas y autodetectando patrones de llamada de herramientas que coincidan con las opiniones previas de los humanos. En las pruebas del benchmark de Olimpiada de Matemáticas en el AIME, ReTool demostró su excelente rendimiento: nuestro modelo de 32B alcanzó una precisión del 67% en 400 pasos de entrenamiento y superó la línea de RL basada en texto (40% de precisión en 1080 pasos). Sorprendentemente, ReTool-32B alcanzó una precisión del 72.5% en configuraciones expandidas, superando a OpenAI's o1-preview en un margen de 27.9%. Un análisis avanzado reveló fenómenos como la autoajuste de código y \"grandes pistas\", lo que significa que el modelo puede adaptarse automáticamente a un uso de herramientas adecuado, lo que se conoce como \"momento de hard-coding\". Estos hallazgos ofrecen una nueva perspectiva sobre la posibilidad de la integración basada en resultados de herramientas y el desarrollo de la razón matemática compleja.",
      "upvotes": 0,
      "discussionId": "6800cc7359e20f50cc282f43",
      "ai_keywords": [
        "reinforcement learning",
        "dynamic interleaving",
        "real-time code execution",
        "natural language reasoning processes",
        "automated RL paradigm",
        "policy rollouts",
        "multi-turn real-time code execution",
        "synthetic cold-start data generation",
        "code-augmented long-form reasoning traces",
        "fine-tuning",
        "RL training",
        "task outcomes as rewards",
        "autonomous discovery",
        "optimal tool invocation patterns",
        "MATH Olympiad benchmark",
        "AIME",
        "accuracy",
        "training steps",
        "OpenAI's o1-preview",
        "code self-correction",
        "adaptive tool use",
        "hybrid neuro-symbolic systems"
      ]
    },
    "publishedAt": "2025-04-15T14:10:22.000Z",
    "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
    "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11536.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6669
    },
    "isAuthorParticipating": false
  }
]