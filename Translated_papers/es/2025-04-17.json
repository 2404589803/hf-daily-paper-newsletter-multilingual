[
  {
    "paper": {
      "id": "2504.10514",
      "authors": [
        {
          "_id": "67ffedb8b0c26d6ec0b608cf",
          "name": "Yijun Liang",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d0",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d1",
          "user": {
            "_id": "64a8121e35fab7cd04c30ed0",
            "avatarUrl": "/avatars/48849b84703158772f1022932331b143.svg",
            "isPro": false,
            "fullname": "Chenrui Fan",
            "user": "Fcr09",
            "type": "user"
          },
          "name": "Chenrui Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:05:05.662Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d2",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d3",
          "name": "Dang Nguyen",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d4",
          "user": {
            "_id": "63f546e0fcf95ecac2b0ee3e",
            "avatarUrl": "/avatars/02a401bcff91cc473d9946bbb771a985.svg",
            "isPro": false,
            "fullname": "Kwesi Cobbina",
            "user": "kweCobi",
            "type": "user"
          },
          "name": "Kwesi Cobbina",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:36.225Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d5",
          "user": {
            "_id": "639d4b8d860db464ae35c3ab",
            "avatarUrl": "/avatars/ec0fa3e91593a03fc9fb611e66b30553.svg",
            "isPro": false,
            "fullname": "Shweta Bhardwaj",
            "user": "shweta12",
            "type": "user"
          },
          "name": "Shweta Bhardwaj",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:29.680Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d6",
          "user": {
            "_id": "6393847e3e30234ae798b7be",
            "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
            "isPro": true,
            "fullname": "JiuhaiChen",
            "user": "jiuhai",
            "type": "user"
          },
          "name": "Jiuhai Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:08:22.786Z",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d7",
          "name": "Fuxiao Liu",
          "hidden": false
        },
        {
          "_id": "67ffedb8b0c26d6ec0b608d8",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:05:07.396Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T16:36:26.000Z",
      "submittedOnDailyAt": "2025-04-17T00:58:33.032Z",
      "title": "ColorBench: VLMs pueden ver y comprender el mundo de los colores? Predicción de color, teoría y criterios de robustez de evaluación",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "El color juega un papel crucial en la percepción humana y generalmente proporciona una importante guía en las inferencias visuales. Sin embargo, no está claro si los modelos de lenguaje visual (VLMs) pueden reconocer y comprender el color de manera similar a la humana. En este artículo, se presenta un nuevo benchmark llamado \"ColorBench\" para evaluar la capacidad de reconocimiento de colores. Este benchmark ha sido diseñado detalladamente para evaluar la comprensión del color, la justificación de la percepción y la consistencia en la transformación de colores. Se han seleccionado diferentes escenarios de prueba y se han colectado datos basados en aplicaciones reales para evaluar cómo estos modelos reconocen el color, inferen el significado a través de códigos de color y mantienen un rendimiento consistente en la transformación de colores. Compuesto por 32 VLMs, cada uno constituido por diferentes modelos de lenguaje y codificadores visuales, se realizaron evaluaciones detalladas, y en este artículo se destacan los siguientes nuevos hallazgos: (i) la escalación (modelos grandes son mejores) se mantiene en ColorBench, y el modelo de lenguaje juega un papel más importante que el de codificador visual. (ii) Sin embargo, la diferencia en la performance entre los modelos es relativamente pequeña, demostrando que los VLMs actuales han logrado un buen rendimiento en la comprensión del color. (iii) La inferencia de tipo CoT mejora la precisión y la robustez de la comprensión del color, pero es un proceso cognitivo. (iv) Los VLMs utilizan códigos de color en ColorBench, pero a veces estos códigos pueden llevar a errores en el modelo. Estos hallazgos claramente demuestran limitaciones importantes de los VLMs actuales y subrayan la necesidad de mejorar la comprensión del color. ColorBench sirve como una herramienta fundamental para fomentar la investigación de nivel humano en la comprensión del color de diferentes IAs.",
      "upvotes": 18,
      "discussionId": "67ffedbeb0c26d6ec0b60a5b",
      "projectPage": "https://huggingface.co/datasets/umd-zhou-lab/ColorBench",
      "githubRepo": "https://github.com/tianyi-lab/ColorBench",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "color understanding",
        "color perception",
        "color-based cues",
        "color transformations",
        "scaling law",
        "language model",
        "vision encoder",
        "CoT reasoning",
        "multimodal AI"
      ]
    },
    "publishedAt": "2025-04-10T12:36:26.000Z",
    "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
    "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10514.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12240",
      "authors": [
        {
          "_id": "680057b49031335df49732fc",
          "user": {
            "_id": "64970d3d9c3b29dca8633f87",
            "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
            "isPro": false,
            "fullname": "JunhaoZhuang",
            "user": "JunhaoZhuang",
            "type": "user"
          },
          "name": "Junhao Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:04:09.222Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732fd",
          "user": {
            "_id": "66837d3c48edefb453b0640a",
            "avatarUrl": "/avatars/b16385eaa612578728e2c6460a76b38f.svg",
            "isPro": false,
            "fullname": "Lingen Li",
            "user": "l-li",
            "type": "user"
          },
          "name": "Lingen Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:09:48.492Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732fe",
          "user": {
            "_id": "62d4577bc85b0fcf7fde39bb",
            "avatarUrl": "/avatars/a3a5729e33ae89ce9ba408830db3c835.svg",
            "isPro": false,
            "fullname": "Xuan Ju",
            "user": "juxuan27",
            "type": "user"
          },
          "name": "Xuan Ju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:09:55.870Z",
          "hidden": false
        },
        {
          "_id": "680057b49031335df49732ff",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "680057b49031335df4973300",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "680057b49031335df4973301",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:12:47.014Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
      ],
      "publishedAt": "2025-04-16T16:45:19.000Z",
      "submittedOnDailyAt": "2025-04-17T00:32:24.205Z",
      "title": "Cobra: Técnica de pintura de diseño eficiente utilizando referencias amplias",
      "submittedOnDailyBy": {
        "_id": "64970d3d9c3b29dca8633f87",
        "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
        "isPro": false,
        "fullname": "JunhaoZhuang",
        "user": "JunhaoZhuang",
        "type": "user"
      },
      "summary": "En el sector de la industria de los cómics, se requieren criterios de alta precisión, eficiencia, coherencia del contexto y control flexible para el colorado de las páginas. Una página de cómics incluye diversos personajes, objetos y fondos, lo que hace que el proceso de colorado sea complejo. Aunque ha habido avances en los modelos de difusión para la generación de imágenes, su aplicación en el colorado de los cómics está limitada. La aplicación de modelos de difusión se ve limitada por el hecho de que los modelos de difusión no se adaptan fácilmente a los requisitos específicos del colorado de los cómics, que requieren un nivel de detalle y coherencia que los modelos de difusión a menudo no pueden lograr.",
      "upvotes": 16,
      "discussionId": "680057b89031335df497343e",
      "projectPage": "https://zhuang2002.github.io/Cobra/",
      "githubRepo": "https://github.com/zhuang2002/Cobra",
      "ai_keywords": [
        "diffusion models",
        "line art colorization",
        "contextual image guidance",
        "color hints",
        "Causal Sparse DiT architecture",
        "positional encodings",
        "causal sparse attention",
        "Key-Value Cache",
        "long-context references",
        "color identity consistency"
      ]
    },
    "publishedAt": "2025-04-16T12:45:19.000Z",
    "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
    "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64970d3d9c3b29dca8633f87",
      "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
      "fullname": "JunhaoZhuang",
      "name": "JunhaoZhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12285",
      "authors": [
        {
          "_id": "68006e6e175c8dce4ec17f7a",
          "user": {
            "_id": "613f07f40153aafa379775f2",
            "avatarUrl": "/avatars/3965175b320d753d9a5ccb0c7d9298a4.svg",
            "isPro": false,
            "fullname": "Shuming Ma",
            "user": "shumingma",
            "type": "user"
          },
          "name": "Shuming Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:13:17.676Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7b",
          "user": {
            "_id": "63f71771d36951307fcb4dcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
            "isPro": false,
            "fullname": "Hongyu Wang",
            "user": "hongyuw",
            "type": "user"
          },
          "name": "Hongyu Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:14:45.597Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7c",
          "user": {
            "_id": "632bd2f72d6a805eeb4bc601",
            "avatarUrl": "/avatars/6e1533e8a599f3068290aa69ac82cab7.svg",
            "isPro": false,
            "fullname": "HUANG SHAOHAN",
            "user": "buaahsh",
            "type": "user"
          },
          "name": "Shaohan Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:14:59.750Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7d",
          "user": {
            "_id": "64abbcff6cadc7aca584f71b",
            "avatarUrl": "/avatars/fc6e85ad4a8befd133a37b411712c648.svg",
            "isPro": false,
            "fullname": "Xingxing Zhang",
            "user": "THU-CHUNXIA",
            "type": "user"
          },
          "name": "Xingxing Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:15:06.359Z",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7e",
          "name": "Ying Hu",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f7f",
          "name": "Ting Song",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f80",
          "name": "Yan Xia",
          "hidden": false
        },
        {
          "_id": "68006e6e175c8dce4ec17f81",
          "user": {
            "_id": "6368c512fbfe97c16a40baba",
            "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
            "isPro": false,
            "fullname": "Furu Wei",
            "user": "thegenerality",
            "type": "user"
          },
          "name": "Furu Wei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:15:42.250Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T17:51:43.000Z",
      "submittedOnDailyAt": "2025-04-17T01:29:56.744Z",
      "title": "BitNet b1.58 2B4T Informe Técnico",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "Bienet b1.58 2B4T introduce el primer lenguaje modelo grande (LLM) de código abierto y de origen primario con un solo bit. Este modelo tiene una escala de 2 billones de parámetros y fue entrenado con un corpus de 400 mil millones de tokens. El modelo ha publicado marcos de referencia para comprensión lingüística, inferencia matemática, capacidades de programación y habilidades de diálogo, y ha sido sometido a una evaluación rigurosa. Nuestros resultados muestran que bienet b1.58 2B4T alcanza el rendimiento de los mejores LLMs de código abierto de la misma escala, ofreciendo una gran ventaja en la eficiencia de cálculo. En particular, se reduce significativamente el uso de memoria, la consumo de energía y el tiempo de decisión. Además, para fomentar la investigación y la adopción, los pesos del modelo están disponibles en Hugging Face, y se proporcionan también implementaciones abiertas de inferencia para dos arquitecturas de GPU y CPU.",
      "upvotes": 12,
      "discussionId": "68006e70175c8dce4ec17fc0",
      "ai_keywords": [
        "BitNet b1.58 2B4T",
        "Large Language Model (LLM)",
        "1-bit Large Language Model",
        "Train",
        "Corpus",
        "4 trillion tokens",
        "Benchmarks",
        "Language understanding",
        "Mathematical reasoning",
        "Coding proficiency",
        "Conversational ability"
      ]
    },
    "publishedAt": "2025-04-16T13:51:43.000Z",
    "title": "BitNet b1.58 2B4T Technical Report",
    "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12285.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10326",
      "authors": [
        {
          "_id": "67ff772061373fdf16ce1d38",
          "user": {
            "_id": "66486ba1640bc89c93bcc8a2",
            "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
            "isPro": false,
            "fullname": "Yangshen Deng",
            "user": "YangshenDeng",
            "type": "user"
          },
          "name": "Yangshen Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T11:57:58.302Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d39",
          "name": "Zhengxin You",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3a",
          "name": "Long Xiang",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3b",
          "user": {
            "_id": "66d6be0bddf54fd90923c727",
            "avatarUrl": "/avatars/7bb82c8c339db944d79d47b3b9b35aa8.svg",
            "isPro": false,
            "fullname": "Li",
            "user": "Qilong00",
            "type": "user"
          },
          "name": "Qilong Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:16.665Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3c",
          "user": {
            "_id": "66efe50667c4ce2c9024cd45",
            "avatarUrl": "/avatars/7b7f650953c371f08a5beecc500b6a43.svg",
            "isPro": false,
            "fullname": "peiqiyuan",
            "user": "YuanPeiqi",
            "type": "user"
          },
          "name": "Peiqi Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:22.829Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3d",
          "name": "Zhaoyang Hong",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3e",
          "user": {
            "_id": "66d6c339b61dd11022907252",
            "avatarUrl": "/avatars/d2ed3cc003e94b2e5204ce0f8a481dcf.svg",
            "isPro": false,
            "fullname": "Yitao Zheng",
            "user": "FeTieTer",
            "type": "user"
          },
          "name": "Yitao Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:16:36.808Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d3f",
          "name": "Wanting Li",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d40",
          "user": {
            "_id": "671a7bce2b10d343bab18637",
            "avatarUrl": "/avatars/af1c10a59236d953b42e67d3955eecc4.svg",
            "isPro": false,
            "fullname": "runzhong",
            "user": "runzhongli",
            "type": "user"
          },
          "name": "Runzhong Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:17:02.031Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d41",
          "user": {
            "_id": "63898b61ec1f539adc0f4da2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674248167280-63898b61ec1f539adc0f4da2.jpeg",
            "isPro": false,
            "fullname": "Haotian Liu",
            "user": "liuhaotian",
            "type": "user"
          },
          "name": "Haotian Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:17:25.728Z",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d42",
          "name": "Kyriakos Mouratidis",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d43",
          "name": "Man Lung Yiu",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d44",
          "name": "Huan Li",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d45",
          "name": "Qiaomu Shen",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d46",
          "name": "Rui Mao",
          "hidden": false
        },
        {
          "_id": "67ff772061373fdf16ce1d47",
          "user": {
            "_id": "66b02a2642c34e7a212133c0",
            "avatarUrl": "/avatars/737a69b095e8c427ecd08f870b173635.svg",
            "isPro": false,
            "fullname": "Bo Tang",
            "user": "BO1022",
            "type": "user"
          },
          "name": "Bo Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:18:01.460Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T15:34:26.000Z",
      "submittedOnDailyAt": "2025-04-17T06:00:20.705Z",
      "title": "AlayaDB: Eficiente y efectivo data-based inference para el contexto largo de los modelos de lenguaje LLM",
      "submittedOnDailyBy": {
        "_id": "66486ba1640bc89c93bcc8a2",
        "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
        "isPro": false,
        "fullname": "Yangshen Deng",
        "user": "YangshenDeng",
        "type": "user"
      },
      "summary": "AlibabaDB es un sistema de base de datos vectorial de última generación diseñado para ejecutar eficientemente y de manera efectiva la inferencia de modelos de lenguaje de gran escala (LLMs) con largos contextos. En particular, se separan la caché de KV y el cálculo de atención en el sistema de inferencia de LLMs y se integran en el nuevo sistema de base de datos vectorial. En comparación con las soluciones actuales (por ejemplo, el aislamiento de la caché de KV y el cálculo de atención basado en búsqueda), AlibabaDB proporciona una menor consumo de recursos y una generación de alta calidad en diferentes cargas de trabajo y objetivos de nivel de servicio (SLOs). La clave de AlibabaDB es la abstractión de los cálculos de atención y la gestión de la caché en el proceso de procesamiento de consultas, lo que optimiza el rendimiento con operadores de consultas naturales. En este artículo, se presentan tres estudios de casos y resultados de experimentos extendidos de benchmarking de inferencia de LLMs, lo que demuestra la eficiencia de AlibabaDB.",
      "upvotes": 12,
      "discussionId": "67ff772261373fdf16ce1d93",
      "ai_keywords": [
        "vector database",
        "long-context inference",
        "Large Language Models (LLMs)",
        "KV cache",
        "attention computation",
        "Model as a Service (MaaS)",
        "Service Level Objectives (SLOs)",
        "KV cache disaggregation",
        "retrieval-based sparse attention",
        "query processing procedure",
        "native query optimizer",
        "LLM inference benchmarks"
      ]
    },
    "publishedAt": "2025-04-14T11:34:26.000Z",
    "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
    "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10326.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66486ba1640bc89c93bcc8a2",
      "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
      "fullname": "Yangshen Deng",
      "name": "YangshenDeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09081",
      "authors": [
        {
          "_id": "67fdbfcccaa65039e8c3d8ae",
          "user": {
            "_id": "67dcd93f73e2178fe917b893",
            "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
            "isPro": false,
            "fullname": "Prabhat Pandey",
            "user": "panprabh",
            "type": "user"
          },
          "name": "Prabhat Pandey",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:52.370Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8af",
          "name": "Rupak Vignesh Swaminathan",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b0",
          "user": {
            "_id": "66279810009f1bfdc6bf71bf",
            "avatarUrl": "/avatars/97f561c91b92e1abb4fe6f0b5c688126.svg",
            "isPro": false,
            "fullname": "Girish",
            "user": "vijaygirish2001",
            "type": "user"
          },
          "name": "K V Vijay Girish",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:49.809Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b1",
          "user": {
            "_id": "66367dfb2d6b86ff193dbbe0",
            "avatarUrl": "/avatars/0a0c230bb5fc81a28a166691146cf807.svg",
            "isPro": false,
            "fullname": "Arunasish Sen",
            "user": "svinxz",
            "type": "user"
          },
          "name": "Arunasish Sen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T11:58:01.767Z",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b2",
          "name": "Jian Xie",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b3",
          "name": "Grant P. Strimel",
          "hidden": false
        },
        {
          "_id": "67fdbfcccaa65039e8c3d8b4",
          "name": "Andreas Schwarz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-12T04:45:48.000Z",
      "submittedOnDailyAt": "2025-04-17T07:32:43.212Z",
      "title": "SIFT-50M: Fine-tuning en grande conjunto de datos multilingües para comandos de voz",
      "submittedOnDailyBy": {
        "_id": "67dcd93f73e2178fe917b893",
        "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
        "isPro": false,
        "fullname": "Prabhat Pandey",
        "user": "panprabh",
        "type": "user"
      },
      "summary": "SIFT (Instrucción de Habla Fine-tuning) se presenta. SIFT-50M es un conjunto de datos de 50M que se adaptan a fine-tuning de instrucciones y al entrenamiento previo de grandes modelos de lenguaje de habla y texto (LLMs). SIFT-50M se ha construido como un corpus de habla disponible públicamente, incluyendo un total de 14,000 horas de habla, y se utiliza junto con modelos profesionales de habla y texto. Este conjunto de datos se ha expandido en 5 idiomas y incluye diferentes tipos de instrucciones para comprender y controlar la habla. Usando SIFT-50M, se entrenó SIFT-LLM, superando los actuales LLMs de habla y texto en el marco de pruebas de instrucciones seguidas y demostrando excelentes resultados en tareas básicas de habla. Además, se presenta EvalSIFT para evaluar la capacidad de seguimiento de instrucciones en LLMs de habla y texto, diseñado específicamente para este propósito.",
      "upvotes": 9,
      "discussionId": "67fdbfe0caa65039e8c3de4b",
      "ai_keywords": [
        "SIFT (Speech Instruction Fine-Tuning)",
        "speech-text large language models (LLMs)",
        "instruction fine-tuning",
        "pre-training",
        "speech corpora",
        "off-the-shelf expert models",
        "speech understanding",
        "controllable speech generation",
        "SIFT-LLM",
        "instruction-following benchmarks",
        "foundational speech tasks",
        "EvalSIFT"
      ]
    },
    "publishedAt": "2025-04-12T00:45:48.000Z",
    "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning",
    "summary": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09081.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67dcd93f73e2178fe917b893",
      "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
      "fullname": "Prabhat Pandey",
      "name": "panprabh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10483",
      "authors": [
        {
          "_id": "67fdd3e3913c97aa32f94e9b",
          "user": {
            "_id": "641480554b1701c01cdb36c4",
            "avatarUrl": "/avatars/f1f6b294e0236d76a68c099164c81f36.svg",
            "isPro": false,
            "fullname": "Xingjian Leng",
            "user": "xingjianleng",
            "type": "user"
          },
          "name": "Xingjian Leng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:36.449Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9c",
          "name": "Jaskirat Singh",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9d",
          "user": {
            "_id": "6752870ec63bc5b670b1b27e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6752870ec63bc5b670b1b27e/3CdHxnyTKbGup-1V67nEV.jpeg",
            "isPro": false,
            "fullname": "Yunzhong Hou",
            "user": "yunzhong-hou",
            "type": "user"
          },
          "name": "Yunzhong Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:11.946Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9e",
          "user": {
            "_id": "63bf533f4a2beec65568f813",
            "avatarUrl": "/avatars/bba583653b5cada8dd4a3ff2281e9dec.svg",
            "isPro": false,
            "fullname": "Xing",
            "user": "Zhenchang",
            "type": "user"
          },
          "name": "Zhenchang Xing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:21.190Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94e9f",
          "user": {
            "_id": "6596422646624a86ff3b3bda",
            "avatarUrl": "/avatars/216e12b77e45ac5f1fa20932f5745411.svg",
            "isPro": false,
            "fullname": "Saining Xie",
            "user": "sainx",
            "type": "user"
          },
          "name": "Saining Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:39.700Z",
          "hidden": false
        },
        {
          "_id": "67fdd3e3913c97aa32f94ea0",
          "user": {
            "_id": "666351ebd86c026caa135e5c",
            "avatarUrl": "/avatars/50a37f7e999f660c69f518b71577eb7d.svg",
            "isPro": false,
            "fullname": "Liang Zheng",
            "user": "liangzheng06",
            "type": "user"
          },
          "name": "Liang Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:34:56.041Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:59:53.000Z",
      "submittedOnDailyAt": "2025-04-17T05:08:17.990Z",
      "title": "REPA-E: Método para liberar la VAE en el ajuste de extremo a extremo utilizando difusores potenciales\nTransformers\n\n(注意：这里将“Transformers”直接翻译为“트랜스포머스”，因为“트랜스포머스”是Transformers在韩语中的常见翻译。如果需要更正式或学术性的翻译，可以考虑使用“트랜스포머 모델”或“트랜스포머 기술”。)",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "Este artículo trata de un problema fundamental: \"¿Es posible entrenar de manera simultánea un modelo de difusión potencial y un tokenizador de autoencoder (VAE) de manera integral desde principio hasta fin?\" El conocimiento tradicional de aprendizaje profundo sugiere que es mejor entrenar de manera integral desde principio hasta fin. Sin embargo, en el caso de un transformador de difusión potencial, se ha demostrado que entrenar de manera integral desde principio hasta fin utilizando el pérdida de difusión estándar con VAE y modelo de difusión resulta en una pérdida de rendimiento final. Mostramos que la pérdida de difusión no es efectiva y que utilizar la pérdida de REPA para entrenar de manera integral es posible. El proceso de entrenamiento simple de REPA-E puede entrenar el modelo de difusión 17 veces y 45 veces más rápido que el proceso de entrenamiento de REPA y de BERT. Interesantemente, el entrenamiento de manera integral en REPA-E mejora también el VAE mismo, mejorando la estructura del espacio potencial y el rendimiento de generación en el flujo descendente. En términos de rendimiento final, nuestro enfoque llega a un nuevo estado de la arte. En ImageNet 256 x 256, al no tener un generador de juego de clases y con un generador de juego de clases, se alcanzan los valores de FID de 1.26 y 1.83, respectivamente. El código está disponible en https://end2end-diffusion.github.io.",
      "upvotes": 3,
      "discussionId": "67fdd3e5913c97aa32f94ee3",
      "projectPage": "https://end2end-diffusion.github.io/",
      "githubRepo": "https://github.com/End2End-Diffusion/REPA-E",
      "ai_keywords": [
        "latent diffusion models",
        "variational auto-encoder (VAE) tokenizer",
        "end-to-end training",
        "diffusion-loss",
        "representation-alignment (REPA) loss",
        "diffusion model training",
        "VAE",
        "latent space structure",
        "downstream generation performance",
        "FID",
        "ImageNet 256 x 256"
      ]
    },
    "publishedAt": "2025-04-14T13:59:53.000Z",
    "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers",
    "summary": "In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10483.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 820
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11952",
      "authors": [
        {
          "_id": "6800646e0679d4ec4b9d01a7",
          "user": {
            "_id": "645c60dd7d655680b57ddbff",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
            "isPro": true,
            "fullname": "Ram Kadiyala",
            "user": "1024m",
            "type": "user"
          },
          "name": "Ram Mohan Rao Kadiyala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:04:05.627Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01a8",
          "user": {
            "_id": "63da8ba3f03c3d71ef32408c",
            "avatarUrl": "/avatars/1b2e6f3ea2bac5ab35dbd53edb7f8cf2.svg",
            "isPro": false,
            "fullname": "Siddartha Pullakhandam",
            "user": "Siddartha10",
            "type": "user"
          },
          "name": "Siddartha Pullakhandam",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:18:54.938Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01a9",
          "name": "Kanwal Mehreen",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01aa",
          "user": {
            "_id": "618c1ad1c74578e0a4a4d074",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618c1ad1c74578e0a4a4d074/8u_AkeHt4d6xtQ8hzaffU.jpeg",
            "isPro": true,
            "fullname": "Drishti Sharma",
            "user": "DrishtiSharma",
            "type": "user"
          },
          "name": "Drishti Sharma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:19:22.573Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ab",
          "name": "Siddhant Gupta",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ac",
          "user": {
            "_id": "66c578770a22b2f9ab575847",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c578770a22b2f9ab575847/-zfNho1DR3yZHDazq669-.png",
            "isPro": false,
            "fullname": "Jebish Purbey",
            "user": "jebish7",
            "type": "user"
          },
          "name": "Jebish Purbey",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:00.066Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ad",
          "user": {
            "_id": "653d84f13fc9c706fa755d03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653d84f13fc9c706fa755d03/F_jYbeuLLM9EX8hKXcbHD.png",
            "isPro": false,
            "fullname": "Ashay Srivastava",
            "user": "ashay-sriv",
            "type": "user"
          },
          "name": "Ashay Srivastava",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:15.588Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01ae",
          "user": {
            "_id": "669a745a4bbe8ad52ee287cf",
            "avatarUrl": "/avatars/245644aa638b45a17ff71124bd5bbe0f.svg",
            "isPro": false,
            "fullname": "Subhasya Tippareddy",
            "user": "subhasyar",
            "type": "user"
          },
          "name": "Subhasya TippaReddy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:23.946Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01af",
          "user": {
            "_id": "669a7383c9111326dc596f5e",
            "avatarUrl": "/avatars/8bbd307fd4bb2d7055b2c8fc9140dc81.svg",
            "isPro": false,
            "fullname": "Arvind Reddy Bobbili",
            "user": "Arvindreddy",
            "type": "user"
          },
          "name": "Arvind Reddy Bobbili",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:32.804Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b0",
          "name": "Suraj Telugara Chandrashekhar",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b1",
          "user": {
            "_id": "668db0bfb09a05f3d7cc796f",
            "avatarUrl": "/avatars/969c511cca4d129b99eca6252a468385.svg",
            "isPro": false,
            "fullname": "Modabbir Adeeb",
            "user": "moda10",
            "type": "user"
          },
          "name": "Modabbir Adeeb",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:46.461Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b2",
          "user": {
            "_id": "641c3337f0b71a9743629985",
            "avatarUrl": "/avatars/820b124887f173c263a675728baf99c6.svg",
            "isPro": false,
            "fullname": "Srinadh Vura",
            "user": "SriV",
            "type": "user"
          },
          "name": "Srinadh Vura",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-17T08:23:53.315Z",
          "hidden": false
        },
        {
          "_id": "6800646e0679d4ec4b9d01b3",
          "name": "Hamza Farooq",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T10:29:30.000Z",
      "submittedOnDailyAt": "2025-04-17T00:47:10.039Z",
      "title": "Robuste Detección de Texto AI Generado por Pinguin Detector",
      "submittedOnDailyBy": {
        "_id": "645c60dd7d655680b57ddbff",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
        "isPro": true,
        "fullname": "Ram Kadiyala",
        "user": "1024m",
        "type": "user"
      },
      "summary": "Un sistema ideal para detectar contenido generado por máquinas debería funcionar bien en varios generadores. Los sistemas existentes tienen dificultades en identificar de manera precisa contenido generado por IA con textos cortos. Además, no todos los textos son completamente escritos por personas o por modelos de lenguaje grande (LLM). En nuestro artículo, presentamos un conjunto de modelos construidos para la tarea de clasificación de tokens y demostramos que estos modelos funcionan bien en dominios y generadores que nunca han visto, en textos de habla no nativa y en entradas adversariales. Además, presentamos un conjunto de datos de 2.4M de textos que incluyen casi todos los textos generados por un LLM multilingüe en 23 idiomas. También demostramos la performance de los modelos en cada dominio y generador, comparamos su performance frente a métodos adversariales, y comparamos la longitud de los textos de entrada, los textos generados y los textos originales humanos.",
      "upvotes": 2,
      "discussionId": "680064710679d4ec4b9d0224",
      "ai_keywords": [
        "token classification",
        "adversarial inputs"
      ]
    },
    "publishedAt": "2025-04-16T06:29:30.000Z",
    "title": "Robust and Fine-Grained Detection of AI Generated Texts",
    "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645c60dd7d655680b57ddbff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
      "fullname": "Ram Kadiyala",
      "name": "1024m",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11092",
      "authors": [
        {
          "_id": "6800a43e1fd95d7dc21d6b83",
          "user": {
            "_id": "63e367d3fae035bdc4c347fc",
            "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
            "isPro": false,
            "fullname": "Jiaxin Huang",
            "user": "JaceyH919",
            "type": "user"
          },
          "name": "Jiaxin Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-17T08:03:59.476Z",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b84",
          "name": "Sheng Miao",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b85",
          "name": "BangBnag Yang",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b86",
          "name": "Yuewen Ma",
          "hidden": false
        },
        {
          "_id": "6800a43e1fd95d7dc21d6b87",
          "name": "Yiyi Liao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T11:38:14.000Z",
      "submittedOnDailyAt": "2025-04-17T07:32:38.400Z",
      "title": "Vivid4D: Video-Based Unit Camera Video para Mejorar la Reconstrucción 4D",
      "submittedOnDailyBy": {
        "_id": "63e367d3fae035bdc4c347fc",
        "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
        "isPro": false,
        "fullname": "Jiaxin Huang",
        "user": "JaceyH919",
        "type": "user"
      },
      "summary": "Reconstruir una pantalla dinámica 4D a partir de un vídeo de secuencias simples es valioso pero muy difícil. Cada punto de tiempo en un slide de tiempo puede ser observado desde un solo punto de vista. Presentamos un nuevo enfoque llamado Vivid4D, que se centra en fortalecer la perspectiva de observación y mejorar la síntesis de vídeos 4D de secuencias. Una diferencia con los métodos existentes es que utiliza la prioridad de profundidad para fortalecer la perspectiva de observación y síntetiza videos desde diferentes puntos de vista. Este método reconstruye el vídeo como si se moviera desde un nuevo punto de vista, utilizando el editado de cortes del vídeo. Para esto, se entrena un modelo de edición de cortes de vídeo de web inmovil utilizando máscaras sintéticas. Esto asegura la continuidad espacial y temporal. Además, para reducir la incertidumbre de profundidad del objeto, se introduce un enfoque iterativo que fortalece la perspectiva de observación y una pérdida de reconstrucción fuerte. Los experimentos demuestran que nuestro método mejora efectivamente la reconstrucción y completitud de las pantallas 4D de secuencias.",
      "upvotes": 2,
      "discussionId": "6800a4441fd95d7dc21d6d46",
      "projectPage": "https://xdimlab.github.io/Vivid4D/",
      "ai_keywords": [
        "Vivid4D",
        "4D monocular video synthesis",
        "view augmentation",
        "video inpainting",
        "monocular depth priors",
        "unposed web videos",
        "synthetically generated masks",
        "iterative view augmentation strategy",
        "robust reconstruction loss"
      ]
    },
    "publishedAt": "2025-04-15T07:38:14.000Z",
    "title": "Vivid4D: Improving 4D Reconstruction from Monocular Video by Video\n  Inpainting",
    "summary": "Reconstructing 4D dynamic scenes from casually captured monocular videos is\nvaluable but highly challenging, as each timestamp is observed from a single\nviewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular\nvideo synthesis by augmenting observation views - synthesizing multi-view\nvideos from a monocular input. Unlike existing methods that either solely\nleverage geometric priors for supervision or use generative priors while\noverlooking geometry, we integrate both. This reformulates view augmentation as\na video inpainting task, where observed views are warped into new viewpoints\nbased on monocular depth priors. To achieve this, we train a video inpainting\nmodel on unposed web videos with synthetically generated masks that mimic\nwarping occlusions, ensuring spatially and temporally consistent completion of\nmissing regions. To further mitigate inaccuracies in monocular depth priors, we\nintroduce an iterative view augmentation strategy and a robust reconstruction\nloss. Experiments demonstrate that our method effectively improves monocular 4D\nscene reconstruction and completion.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11092.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e367d3fae035bdc4c347fc",
      "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
      "fullname": "Jiaxin Huang",
      "name": "JaceyH919",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11536",
      "authors": [
        {
          "_id": "6800cc7159e20f50cc282e87",
          "name": "Jiazhan Feng",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e88",
          "name": "Shijue Huang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e89",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8a",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8b",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8c",
          "name": "Baoquan Zhong",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8d",
          "name": "Chengquan Jiang",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8e",
          "name": "Jinxin Chi",
          "hidden": false
        },
        {
          "_id": "6800cc7159e20f50cc282e8f",
          "name": "Wanjun Zhong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T18:10:22.000Z",
      "submittedOnDailyAt": "2025-04-17T08:10:39.383Z",
      "title": "ReTool: Aprendizaje de Máquina para la Utilización Estratégica de Herramientas",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "El modelo de escucha (por ejemplo, DeepSeek R1) se entrena utilizando aprendizaje por refuerzo (RL) y es familiar con las lógicas lógicas del lenguaje natural, pero no es adecuado para la resolución de problemas estructurados. Por ejemplo, en casos como lógicas geométricas, cálculos simples y soluciones de ecuaciones complejas, un interprete de código (IC) funciona mejor como una herramienta de cálculo. Para remediar esto, se propone ReTool. ReTool fortalece las lógicas lógicas de largas oraciones que requieren el uso de herramientas integradas a través del entrenamiento, y cuenta con dos funciones principales: 1. El intervalo dinámico de ejecución de código en tiempo real durante el procesamiento de lógicas lógicas de lenguaje natural, y 2. Permite el político de políticas de retroalimentación en el entrenamiento automático, que incluye la ejecución de código en tiempo real, y que el modelo aprenda cómo llama a las herramientas. ReTool comienza con la generación de datos frios sintéticos, crea la traza lógica de largas oraciones para la adición de código, y realiza ajustes micro del modelo base. A continuación, el entrenamiento por retroalimentación mejora continuamente la estrategia de uso de herramientas del modelo, utilizando los resultados como recompensas, y puede descubrir automáticamente patrones óptimos de llamada a herramientas sin necesidad de conocer conocimientos previos humanos. Los experimentos en el AIME de los marcos de referencia estrictos de MATH OLYMPIAD muestran excelentes resultados para ReTool: nuestro modelo de 32B alcanza una precisión del 67% en 400 pasos de entrenamiento, supera al línea de RL basada en texto (40% de precisión, 1080 pasos), y es más eficiente y efectivo. En particular, ReTool-32B alcanza una precisión del 72.5% en configuraciones extendidas, superando a o1-preview de OpenAI en un margen de 27.9%. Un análisis adicional revela comportamientos similares al ajuste automático de código, donde el modelo aprende automáticamente a utilizar herramientas adaptativas, conocido como el \"¡Ahora sí!\" momento. Estos hallazgos demuestran la posibilidad de la integración de herramientas basada en resultados y proporcionan un nuevo perspectiva para el progreso en la resolución de lógicas matemáticas complejas.",
      "upvotes": 0,
      "discussionId": "6800cc7359e20f50cc282f43",
      "ai_keywords": [
        "reinforcement learning",
        "dynamic interleaving",
        "real-time code execution",
        "natural language reasoning processes",
        "automated RL paradigm",
        "policy rollouts",
        "multi-turn real-time code execution",
        "synthetic cold-start data generation",
        "code-augmented long-form reasoning traces",
        "fine-tuning",
        "RL training",
        "task outcomes as rewards",
        "autonomous discovery",
        "optimal tool invocation patterns",
        "MATH Olympiad benchmark",
        "AIME",
        "accuracy",
        "training steps",
        "OpenAI's o1-preview",
        "code self-correction",
        "adaptive tool use",
        "hybrid neuro-symbolic systems"
      ]
    },
    "publishedAt": "2025-04-15T14:10:22.000Z",
    "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
    "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11536.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6669
    },
    "isAuthorParticipating": false
  }
]