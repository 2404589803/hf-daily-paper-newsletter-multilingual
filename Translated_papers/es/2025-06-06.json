[
  {
    "paper": {
      "id": "2506.04308",
      "authors": [
        {
          "_id": "68424dc48d0422fce0273e99",
          "user": {
            "_id": "63f08dc79cf89c9ed1bb89cd",
            "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
            "isPro": false,
            "fullname": "Zhoues",
            "user": "Zhoues",
            "type": "user"
          },
          "name": "Enshen Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:21.339Z",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9a",
          "name": "Jingkun An",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9b",
          "name": "Cheng Chi",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9c",
          "name": "Yi Han",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9d",
          "name": "Shanyu Rong",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9e",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273e9f",
          "name": "Pengwei Wang",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273ea0",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273ea1",
          "name": "Tiejun Huang",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273ea2",
          "name": "Lu Sheng",
          "hidden": false
        },
        {
          "_id": "68424dc48d0422fce0273ea3",
          "name": "Shanghang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:59:27.000Z",
      "submittedOnDailyAt": "2025-06-06T00:41:30.786Z",
      "title": "RoboRefer: Referencia espacial como razón de un modelo de lenguaje visual",
      "submittedOnDailyBy": {
        "_id": "63f08dc79cf89c9ed1bb89cd",
        "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
        "isPro": false,
        "fullname": "Zhoues",
        "user": "Zhoues",
        "type": "user"
      },
      "summary": "La referencia espacial es una habilidad fundamental para que un robot concreto interactúe con el mundo físico tridimensional. Sin embargo, los métodos recientes, incluso utilizando potentes modelos visión-lengua previamente entrenados (VLMs), no logran comprender con precisión y pensar lógicamente de manera dinámica en escenas complejas 3D y en lugares específicos. Por lo tanto, proponemos un VLM adaptable a 3D que puede entender espacialmente con precisión, mediante la combinación estadística de dos encoderes profundos, separados y especializados, a través de la técnica de SFT (Tuning de Feedback de Subjeto). Además, RoboRefer expande este modelo utilizando RFT (Tuning de Aprendizaje por Reforzamiento) y procesos de recompensa asociados con métricas, para desarrollar un espacio lógico multi-paso. Para apoyar el entrenamiento de SFT y RFT, hemos introducido un gran conjunto de datos llamado RefSpatial, que consiste en 20M pares de respuestas (dos veces más que el inicio) y cubre 31 relaciones espaciales (15 más que el inicio), así como procesos lógicos complejos hasta 5 pasos. También hemos presentado RefSpatial-Bench, un estrecho marco de evaluación que complementa la falta de evaluación espacial en procesos lógicos multi-paso. Los experimentos muestran que RoboRefer, entrenado con SFT, logra un entendimiento espacial avanzado de estados, con un porcentaje de éxito promedio de 89.6%. El RoboRefer entrenado con RFT supera a los 17.4% en precisión promedio, y a todos los baselines significativamente. En particular, RoboRefer puede integrarse con diferentes políticas de control para realizar tareas largas y dinámicas en diferentes robots, como por ejemplo el UR5 y el G1 Humanoid.",
      "upvotes": 30,
      "discussionId": "68424dc88d0422fce0273fb5",
      "githubRepo": "https://github.com/Zhoues/RoboRefer",
      "ai_summary": "RoboRefer, a 3D-aware vision language model, enhances spatial understanding and multi-step reasoning in embodied robots through supervised and reinforcement fine-tuning, using the RefSpatial dataset and RefSpatial-Bench benchmark.",
      "ai_keywords": [
        "3D-aware VLM",
        "disentangled depth encoder",
        "supervised fine-tuning (SFT)",
        "reinforcement fine-tuning (RFT)",
        "metric-sensitive reward functions",
        "RefSpatial",
        "RefSpatial-Bench",
        "spatial referring tasks",
        "multi-step reasoning",
        "state-of-the-art spatial understanding",
        "long-horizon",
        "dynamic tasks"
      ]
    },
    "publishedAt": "2025-06-04T13:59:27.000Z",
    "title": "RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language\n  Models for Robotics",
    "summary": "Spatial referring is a fundamental capability of embodied robots to interact\nwith the 3D physical world. However, even with the powerful pretrained vision\nlanguage models (VLMs), recent approaches are still not qualified to accurately\nunderstand the complex 3D scenes and dynamically reason about the\ninstruction-indicated locations for interaction. To this end, we propose\nRoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding\nby integrating a disentangled but dedicated depth encoder via supervised\nfine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial\nreasoning via reinforcement fine-tuning (RFT), with metric-sensitive process\nreward functions tailored for spatial referring tasks. To support SFT and RFT\ntraining, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x\nprior), covering 31 spatial relations (vs. 15 prior) and supporting complex\nreasoning processes (up to 5 steps). In addition, we introduce\nRefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial\nreferring with multi-step reasoning. Experiments show that SFT-trained\nRoboRefer achieves state-of-the-art spatial understanding, with an average\nsuccess rate of 89.6%. RFT-trained RoboRefer further outperforms all other\nbaselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average\naccuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various\ncontrol policies to execute long-horizon, dynamic tasks across diverse robots\n(e,g., UR5, G1 humanoid) in cluttered real-world scenes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04308.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63f08dc79cf89c9ed1bb89cd",
      "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
      "fullname": "Zhoues",
      "name": "Zhoues",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05301",
      "authors": [
        {
          "_id": "68428f675738dda052f724d3",
          "name": "Jianyi Wang",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d4",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d5",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d6",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d7",
          "name": "Meng Wei",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d8",
          "name": "Zongsheng Yue",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724d9",
          "name": "Shangchen Zhou",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724da",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724db",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724dc",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724dd",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724de",
          "name": "Chen Change Loy",
          "hidden": false
        },
        {
          "_id": "68428f675738dda052f724df",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63043db17373aacccd89f49d/OfOznZMmLTV2VmgcLA0fG.mp4"
      ],
      "publishedAt": "2025-06-05T17:51:05.000Z",
      "submittedOnDailyAt": "2025-06-06T06:38:37.104Z",
      "title": "SeedVR2: 1er Paso de Aprendizaje después de la Diferenciación Antigua Mediante Reempirezado de Vídeos",
      "submittedOnDailyBy": {
        "_id": "63043db17373aacccd89f49d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63043db17373aacccd89f49d/jzP_fPCFXeYJvAD8uA_N7.jpeg",
        "isPro": false,
        "fullname": "JIANYI WANG",
        "user": "Iceclear",
        "type": "user"
      },
      "summary": "El desarrollo reciente de listas de vídeo basadas en patrones en la videorealidad (VR) ha demostrado un gran mejoramiento en la calidad visual, pero también plantea problemas de costos de cálculo en la inferencia. Por otro lado, muchos enfoques basados en estilo han mostrado la posibilidad de listas de imágenes basadas en patrones, pero extender estos enfoques a la VR es difícil, especialmente en el procesamiento de vídeo de alta resolución, donde la investigación actual es insuficiente. En este artículo, se propone un modelo basado en patrones de VR para realizar entrenamientos competitivos, llamado SeedVR2. Para procesar la VR de alta resolución de manera eficiente, se introdujeron importantes mejoras en la estructura del modelo y en el orden de entrenamiento. En particular, se propone una estructura de atención adaptativa y se ajusta el tamaño de las ventanas de manera dinámica en función de la resolución de salida, lo que permite evitar la incertidumbre de las ventanas predefinidas en la atención de ventanas, que no pueden manejarse en la VR de alta resolución. Para estabilizar y mejorar el entrenamiento competitivo, se verificó el efecto de la función de pérdida y se confirmó que el efecto de la pérdida de correspondencia propuesta no gravemente afecta la eficiencia del entrenamiento. Los experimentos extendidos muestran que SeedVR2 puede realizar un rendimiento relativamente mejor o mejor que los enfoques actuales de VR.",
      "upvotes": 28,
      "discussionId": "68428f6a5738dda052f72569",
      "projectPage": "https://iceclear.github.io/projects/seedvr2/",
      "githubRepo": "https://github.com/IceClear/SeedVR2",
      "ai_summary": "SeedVR2 is a one-step diffusion-based video restoration model that uses adaptive window attention and feature matching loss to achieve high visual quality with reduced computational cost compared to existing methods.",
      "ai_keywords": [
        "diffusion-based video restoration",
        "VR",
        "adversarial VR training",
        "adaptive window attention",
        "feature matching loss"
      ]
    },
    "publishedAt": "2025-06-05T13:51:05.000Z",
    "title": "SeedVR2: One-Step Video Restoration via Diffusion Adversarial\n  Post-Training",
    "summary": "Recent advances in diffusion-based video restoration (VR) demonstrate\nsignificant improvement in visual quality, yet yield a prohibitive\ncomputational cost during inference. While several distillation-based\napproaches have exhibited the potential of one-step image restoration,\nextending existing approaches to VR remains challenging and underexplored,\nparticularly when dealing with high-resolution video in real-world settings. In\nthis work, we propose a one-step diffusion-based VR model, termed as SeedVR2,\nwhich performs adversarial VR training against real data. To handle the\nchallenging high-resolution VR within a single step, we introduce several\nenhancements to both model architecture and training procedures. Specifically,\nan adaptive window attention mechanism is proposed, where the window size is\ndynamically adjusted to fit the output resolutions, avoiding window\ninconsistency observed under high-resolution VR using window attention with a\npredefined window size. To stabilize and improve the adversarial post-training\ntowards VR, we further verify the effectiveness of a series of losses,\nincluding a proposed feature matching loss without significantly sacrificing\ntraining efficiency. Extensive experiments show that SeedVR2 can achieve\ncomparable or even better performance compared with existing VR approaches in a\nsingle step.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63043db17373aacccd89f49d/OfOznZMmLTV2VmgcLA0fG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05301.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63043db17373aacccd89f49d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63043db17373aacccd89f49d/jzP_fPCFXeYJvAD8uA_N7.jpeg",
      "fullname": "JIANYI WANG",
      "name": "Iceclear",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05284",
      "authors": [
        {
          "_id": "6842929c46106f29d78635ad",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635ae",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635af",
          "name": "Ryan Po",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635b0",
          "name": "Yinghao Xu",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635b1",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635b2",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "6842929c46106f29d78635b3",
          "name": "Gordon Wetzstein",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b4eec4faa3181a5eab9c46/58qSYbX-_UzJE5ubWpM9W.mp4"
      ],
      "publishedAt": "2025-06-05T17:42:34.000Z",
      "submittedOnDailyAt": "2025-06-06T05:48:31.006Z",
      "title": "Video World Models with Long-term Spatial Memory",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "El nuevo modelo de mundo genera secuencialmente frames de video en respuesta a señales de control como el movimiento de la cámara o el texto de plantilla. Debido a que el tamaño del ventana de contexto temporal está limitado, estos modelos requieren mucho esfuerzo para mantener la coherencia en el mercado y a veces olvidan fácilmente los ambientes generados anteriormente. Basándonos en la estructura de la memoria humana, proponemos un nuevo marco para lograr la memoria espacial a largo plazo del modelo de video, lo que permitirá aumentar la coherencia a largo plazo. Este marco incluye una estructura para almacenar y extraer información de la memoria espacial a largo plazo y define una estructura de memoria 3D para seleccionar conjuntos de datos personalizados para entrenar y evaluar el modelo de mundo. Los resultados de la evaluación se comparan con estándares relevantes, demostrando que se han mejorado la calidad de la generación, la coherencia y la longitud del contexto, abriendo camino para la generación de mundos con una coherencia a largo plazo.",
      "upvotes": 24,
      "discussionId": "684292a046106f29d7863732",
      "ai_summary": "A new framework enhances video world models' long-term consistency by integrating a geometry-grounded long-term spatial memory mechanism.",
      "ai_keywords": [
        "world models",
        "autoregressive generation",
        "video frames",
        "control signals",
        "temporal context window",
        "scene consistency",
        "long-term spatial memory",
        "custom datasets",
        "3D memory mechanisms"
      ]
    },
    "publishedAt": "2025-06-05T13:42:34.000Z",
    "title": "Video World Models with Long-term Spatial Memory",
    "summary": "Emerging world models autoregressively generate video frames in response to\nactions, such as camera movements and text prompts, among other control\nsignals. Due to limited temporal context window sizes, these models often\nstruggle to maintain scene consistency during revisits, leading to severe\nforgetting of previously generated environments. Inspired by the mechanisms of\nhuman memory, we introduce a novel framework to enhancing long-term consistency\nof video world models through a geometry-grounded long-term spatial memory. Our\nframework includes mechanisms to store and retrieve information from the\nlong-term spatial memory and we curate custom datasets to train and evaluate\nworld models with explicitly stored 3D memory mechanisms. Our evaluations show\nimproved quality, consistency, and context length compared to relevant\nbaselines, paving the way towards long-term consistent world generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b4eec4faa3181a5eab9c46/58qSYbX-_UzJE5ubWpM9W.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05284.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05010",
      "authors": [
        {
          "_id": "6842632d542c9011f1bebf46",
          "user": {
            "_id": "639c379cdb7c5f35004066cb",
            "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
            "isPro": false,
            "fullname": "Zhenran Xu",
            "user": "imryanxu",
            "type": "user"
          },
          "name": "Zhenran Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:31.325Z",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf47",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf48",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf49",
          "name": "Qingli Hu",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4a",
          "name": "Zijiao Wu",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4b",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4c",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4d",
          "name": "Kaifu Zhang",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4e",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "6842632d542c9011f1bebf4f",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/wG9VdZ8vQGyG76QYWS4NS.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/VrBvS8a9t6I462AYXI2uc.png"
      ],
      "publishedAt": "2025-06-05T13:20:50.000Z",
      "submittedOnDailyAt": "2025-06-06T02:21:39.406Z",
      "title": "ComfyUI-Copilot: Desarrollo de un asistente inteligente para la automatización de flujos de trabajo",
      "submittedOnDailyBy": {
        "_id": "639c379cdb7c5f35004066cb",
        "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
        "isPro": false,
        "fullname": "Zhenran Xu",
        "user": "imryanxu",
        "type": "user"
      },
      "summary": "Introduzco ComfyUI-Copilot. Es un plataforma de abierto de creación de arte basada en IA, ComfyUI, que se fortalece con un plugin que tiene como función un modelo de lenguaje de gran escala. ComfyUI se caracteriza por su marco flexible y su interfaz de usuario sencilla, pero presenta problemas como la falta de documentación, la mal configuración de modelos y la complejidad de la disección de flujos de trabajo, especialmente para principiantes. ComfyUI-Copilot aborda estas limitaciones ofreciendo nodos inteligentes, recomendaciones de modelos y la automatización del flujo de trabajo, así como la construcción de flujos de trabajo. El núcleo de ComfyUI-Copilot es un marco de agentes heurísticos multiagente, compuesto por un agente central de asignación de tareas y varios agentes profesionales que se adaptan a diferentes usos. Estos agentes se basan en nuestro base de conocimientos de ComfyUI personalizado, proporcionando doble verificación y streamlined deployment. La eficiencia de ComfyUI-Copilot se ha demostrado mediante evaluaciones cuantitativas offline y feedback de usuarios online. Esto permite acelerar la recomendación de nodos y el desarrollo de flujos de trabajo. Además, los ejemplos de uso muestran cómo ComfyUI-Copilot reduce los obstáculos de entrada para principiantes y mejora la eficiencia de los flujos de trabajo de los experimentadores. Para obtener el paquete de instalación y videos de demostración de ComfyUI-Copilot, puede acceder a https://github.com/AIDC-AI/ComfyUI-Copilot.",
      "upvotes": 23,
      "discussionId": "6842632e542c9011f1bebfa3",
      "projectPage": "https://x.com/wangly0229/status/1923515826713526583",
      "githubRepo": "https://github.com/AIDC-AI/ComfyUI-Copilot",
      "ai_summary": "ComfyUI-Copilot uses a large language model and multi-agent system to enhance the usability and efficiency of the AI-driven art creation platform ComfyUI.",
      "ai_keywords": [
        "large language model",
        "multi-agent framework",
        "central assistant agent",
        "specialized worker agents",
        "knowledge bases"
      ]
    },
    "publishedAt": "2025-06-05T09:20:50.000Z",
    "title": "ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development",
    "summary": "We introduce ComfyUI-Copilot, a large language model-powered plugin designed\nto enhance the usability and efficiency of ComfyUI, an open-source platform for\nAI-driven art creation. Despite its flexibility and user-friendly interface,\nComfyUI can present challenges to newcomers, including limited documentation,\nmodel misconfigurations, and the complexity of workflow design. ComfyUI-Copilot\naddresses these challenges by offering intelligent node and model\nrecommendations, along with automated one-click workflow construction. At its\ncore, the system employs a hierarchical multi-agent framework comprising a\ncentral assistant agent for task delegation and specialized worker agents for\ndifferent usages, supported by our curated ComfyUI knowledge bases to\nstreamline debugging and deployment. We validate the effectiveness of\nComfyUI-Copilot through both offline quantitative evaluations and online user\nfeedback, showing that it accurately recommends nodes and accelerates workflow\ndevelopment. Additionally, use cases illustrate that ComfyUI-Copilot lowers\nentry barriers for beginners and enhances workflow efficiency for experienced\nusers. The ComfyUI-Copilot installation package and a demo video are available\nat https://github.com/AIDC-AI/ComfyUI-Copilot.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/wG9VdZ8vQGyG76QYWS4NS.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/VrBvS8a9t6I462AYXI2uc.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05010.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639c379cdb7c5f35004066cb",
      "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
      "fullname": "Zhenran Xu",
      "name": "imryanxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02865",
      "authors": [
        {
          "_id": "683fefbb7ed0da422d1ab676",
          "name": "Mathieu Andreux",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab677",
          "name": "Breno Baldas Skuk",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab678",
          "user": {
            "_id": "6808a8cf6b8c599b583d0fe9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6808a8cf6b8c599b583d0fe9/QPgERtP3Nl2n0BCBlp89D.jpeg",
            "isPro": false,
            "fullname": "Hamza Benchekroun",
            "user": "hamza-hcompany",
            "type": "user"
          },
          "name": "Hamza Benchekroun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T15:03:30.496Z",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab679",
          "name": "Emilien Biré",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67a",
          "name": "Antoine Bonnet",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67b",
          "name": "Riaz Bordie",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67c",
          "name": "Matthias Brunel",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67d",
          "name": "Pierre-Louis Cedoz",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67e",
          "name": "Antoine Chassang",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab67f",
          "name": "Mickaël Chen",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab680",
          "name": "Alexandra D. Constantinou",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab681",
          "name": "Antoine d'Andigné",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab682",
          "name": "Hubert de La Jonquière",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab683",
          "name": "Aurélien Delfosse",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab684",
          "name": "Ludovic Denoyer",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab685",
          "name": "Alexis Deprez",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab686",
          "name": "Augustin Derupti",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab687",
          "name": "Michael Eickenberg",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab688",
          "name": "Mathïs Federico",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab689",
          "name": "Charles Kantor",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68a",
          "name": "Xavier Koegler",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68b",
          "name": "Yann Labbé",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68c",
          "name": "Matthew C. H. Lee",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68d",
          "name": "Erwan Le Jumeau de Kergaradec",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68e",
          "name": "Amir Mahla",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab68f",
          "name": "Avshalom Manevich",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab690",
          "name": "Adrien Maret",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab691",
          "name": "Charles Masson",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab692",
          "name": "Rafaël Maurin",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab693",
          "name": "Arturo Mena",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab694",
          "name": "Philippe Modard",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab695",
          "name": "Axel Moyal",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab696",
          "name": "Axel Nguyen Kerbel",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab697",
          "name": "Julien Revelle",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab698",
          "name": "Mats L. Richter",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab699",
          "name": "María Santos",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69a",
          "name": "Laurent Sifre",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69b",
          "name": "Maxime Theillard",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69c",
          "name": "Marc Thibault",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69d",
          "name": "Louis Thiry",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69e",
          "name": "Léo Tronchon",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab69f",
          "name": "Nicolas Usunier",
          "hidden": false
        },
        {
          "_id": "683fefbb7ed0da422d1ab6a0",
          "user": {
            "_id": "6264f9655f6f2e14d6ac981c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650784534234-noauth.png",
            "isPro": false,
            "fullname": "Tony Wu",
            "user": "tonywu71",
            "type": "user"
          },
          "name": "Tony Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:28:00.518Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T13:29:03.000Z",
      "submittedOnDailyAt": "2025-06-06T07:18:18.574Z",
      "title": "Surfer-H es un agente de carga web de bajo costo basado en Open Weights. La integración con Holo1 proporciona una mejora en la eficiencia de los servicios.",
      "submittedOnDailyBy": {
        "_id": "6808a8cf6b8c599b583d0fe9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6808a8cf6b8c599b583d0fe9/QPgERtP3Nl2n0BCBlp89D.jpeg",
        "isPro": false,
        "fullname": "Hamza Benchekroun",
        "user": "hamza-hcompany",
        "type": "user"
      },
      "summary": "Surfer-H es un agente web eficiente en costos. Integra un modelo de lenguaje visual (VLM) para realizar tareas definidas por los usuarios. Surfer-H se une a la nueva colección de pesos abiertos de VLM, Holo1. Holo1 se especializa en búsqueda web e extracción de información. Se ha entrenado con fuentes de datos cuidadosamente seleccionadas, incluyendo contenido web abierto, ejemplos de generación y datos de productos de signos. Holo1 muestra los mejores resultados en marcos de referencia de interface de usuario (UI) general y nuevos marcos de referencia de UI web de WebClick. Al utilizar Holo1 como base, Surfer-H alcanza el mejor rendimiento avanzado en WebVoyager, alcanzando un rendimiento superior al 92.2%, y equilibra la precisión y la eficiencia de costo al óptimo. Para acelerar el progreso de la investigación en sistemas de agentes web, se publican los conjuntos de datos de evaluación de WebClick y los pesos del modelo Holo1.",
      "upvotes": 22,
      "discussionId": "683fefbd7ed0da422d1ab718",
      "ai_summary": "Surfer-H, paired with Holo1, an open-weight collection of Vision-Language Models, achieves top performance in web navigation tasks with high cost-efficiency.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLM",
        "web navigation",
        "information extraction",
        "generalist User Interface",
        "UI",
        "WebClick",
        "WebVoyager",
        "open-sourcing",
        "model weights"
      ]
    },
    "publishedAt": "2025-06-03T09:29:03.000Z",
    "title": "Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights",
    "summary": "We present Surfer-H, a cost-efficient web agent that integrates\nVision-Language Models (VLM) to perform user-defined tasks on the web. We pair\nit with Holo1, a new open-weight collection of VLMs specialized in web\nnavigation and information extraction. Holo1 was trained on carefully curated\ndata sources, including open-access web content, synthetic examples, and\nself-produced agentic data. Holo1 tops generalist User Interface (UI)\nbenchmarks as well as our new web UI localization benchmark, WebClick. When\npowered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on\nWebVoyager, striking a Pareto-optimal balance between accuracy and\ncost-efficiency. To accelerate research advancement in agentic systems, we are\nopen-sourcing both our WebClick evaluation dataset and the Holo1 model weights.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02865.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6808a8cf6b8c599b583d0fe9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6808a8cf6b8c599b583d0fe9/QPgERtP3Nl2n0BCBlp89D.jpeg",
      "fullname": "Hamza Benchekroun",
      "name": "hamza-hcompany",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23656",
      "authors": [
        {
          "_id": "6842520f05049fa51eed0e9f",
          "user": {
            "_id": "656d8d4b1f8d9b618de91369",
            "avatarUrl": "/avatars/884dba9e56936241034b179d11a513b9.svg",
            "isPro": false,
            "fullname": "Xiangdong Zhang",
            "user": "aHapBean",
            "type": "user"
          },
          "name": "Xiangdong Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:15.290Z",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea0",
          "name": "Jiaqi Liao",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea1",
          "name": "Shaofeng Zhang",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea2",
          "name": "Fanqing Meng",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea3",
          "name": "Xiangpeng Wan",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea4",
          "name": "Junchi Yan",
          "hidden": false
        },
        {
          "_id": "6842520f05049fa51eed0ea5",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:06:44.000Z",
      "submittedOnDailyAt": "2025-06-06T00:59:51.401Z",
      "title": "VideoREPA: Arrayments relacionados con la generación de vídeos integrados en un modelo básico para aprender física",
      "submittedOnDailyBy": {
        "_id": "63a2a51ef30c464227924fc6",
        "avatarUrl": "/avatars/e109e85abd25b97bb29dbbe007119e34.svg",
        "isPro": false,
        "fullname": "Haoyu Sun",
        "user": "Mikivis",
        "type": "user"
      },
      "summary": "La traducción al español de la frase proporcionada es:\n\n\"T2V Difu̇̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈̈",
      "upvotes": 18,
      "discussionId": "6842521205049fa51eed0f67",
      "projectPage": "https://videorepa.github.io/",
      "githubRepo": "https://github.com/aHapBean/VideoREPA",
      "ai_summary": "VideoREPA enhances text-to-video synthesis by aligning token-level relations and distilling physics understanding from foundation models into T2V models.",
      "ai_keywords": [
        "T2V diffusion models",
        "physics understanding",
        "video self-supervised learning",
        "Token Relation Distillation (TRD) loss",
        "spatio-temporal alignment",
        "representation alignment (REPA)",
        "CogVideoX",
        "physics commonsense",
        "intuitive physics"
      ]
    },
    "publishedAt": "2025-05-29T13:06:44.000Z",
    "title": "VideoREPA: Learning Physics for Video Generation through Relational\n  Alignment with Foundation Models",
    "summary": "Recent advancements in text-to-video (T2V) diffusion models have enabled\nhigh-fidelity and realistic video synthesis. However, current T2V models often\nstruggle to generate physically plausible content due to their limited inherent\nability to accurately understand physics. We found that while the\nrepresentations within T2V models possess some capacity for physics\nunderstanding, they lag significantly behind those from recent video\nself-supervised learning methods. To this end, we propose a novel framework\ncalled VideoREPA, which distills physics understanding capability from video\nunderstanding foundation models into T2V models by aligning token-level\nrelations. This closes the physics understanding gap and enable more\nphysics-plausible generation. Specifically, we introduce the Token Relation\nDistillation (TRD) loss, leveraging spatio-temporal alignment to provide soft\nguidance suitable for finetuning powerful pre-trained T2V models, a critical\ndeparture from prior representation alignment (REPA) methods. To our knowledge,\nVideoREPA is the first REPA method designed for finetuning T2V models and\nspecifically for injecting physical knowledge. Empirical evaluations show that\nVideoREPA substantially enhances the physics commonsense of baseline method,\nCogVideoX, achieving significant improvement on relevant benchmarks and\ndemonstrating a strong capacity for generating videos consistent with intuitive\nphysics. More video results are available at https://videorepa.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a2a51ef30c464227924fc6",
      "avatarUrl": "/avatars/e109e85abd25b97bb29dbbe007119e34.svg",
      "fullname": "Haoyu Sun",
      "name": "Mikivis",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05240",
      "authors": [
        {
          "_id": "684249e23fb0b2ecb854594a",
          "user": {
            "_id": "630b094f8b327c7b8b94d24c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
            "isPro": false,
            "fullname": "Yizhuo Li",
            "user": "liyz",
            "type": "user"
          },
          "name": "Yizhuo Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:23.836Z",
          "hidden": false
        },
        {
          "_id": "684249e23fb0b2ecb854594b",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "684249e23fb0b2ecb854594c",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "684249e23fb0b2ecb854594d",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "684249e23fb0b2ecb854594e",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T16:59:53.000Z",
      "submittedOnDailyAt": "2025-06-06T00:26:53.631Z",
      "title": "\"Inicio de Correspondencia y Flujo Espacial\"",
      "submittedOnDailyBy": {
        "_id": "630b094f8b327c7b8b94d24c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
        "isPro": false,
        "fullname": "Yizhuo Li",
        "user": "liyz",
        "type": "user"
      },
      "summary": "Este artículo propone un nuevo marco para ajustar espacios potenciales en función de una distribución arbitraria de objetivos. Este método normaliza el espacio potencial de un modelo generativo basado en formas previamente entrenado, utilizando características de objetivo. Este modelo de forma fija ajusta el espacio potencial utilizando una pérdida de ajuste para normalizar. Demostramos formalmente que el mínimo de esta pérdida de ajuste es equivalente a maximizar la inferior bound de la varianza de la logaritma de la probabilidad condicional del espacio potencial de la distribución de objetivos, evitando así evaluaciones probabilísticas costosas y resolver ODEs durante el proceso de optimización. En una demostración profesional, se muestra que la superficie de la pérdida de ajuste aproxima la probabilidad negativa de la distribución objetivo. Además, evaluamos el efecto de nuestro enfoque a través de experimentos de generación de imágenes en diferentes distribuciones de objetivos en ImageNet, realizando discusiones detalladas y experimentos de desvanecimiento. Mediante pruebas teóricas y experimentales, nuestro marco explora nuevas direcciones en el ajuste de espacios potenciales.",
      "upvotes": 16,
      "discussionId": "684249e73fb0b2ecb8545afb",
      "projectPage": "https://liyizhuo.com/align/",
      "githubRepo": "https://github.com/liyz15/Aligning-Latent-Spaces-with-Flow-Priors",
      "ai_summary": "A novel framework using flow-based generative models aligns learnable latent spaces to target distributions, reducing computational expense and improving log-likelihood maximization.",
      "ai_keywords": [
        "flow-based generative models",
        "latent spaces",
        "alignment loss",
        "flow matching objective",
        "variational lower bound",
        "log-likelihood",
        "ImageNet"
      ]
    },
    "publishedAt": "2025-06-05T12:59:53.000Z",
    "title": "Aligning Latent Spaces with Flow Priors",
    "summary": "This paper presents a novel framework for aligning learnable latent spaces to\narbitrary target distributions by leveraging flow-based generative models as\npriors. Our method first pretrains a flow model on the target features to\ncapture the underlying distribution. This fixed flow model subsequently\nregularizes the latent space via an alignment loss, which reformulates the flow\nmatching objective to treat the latents as optimization targets. We formally\nprove that minimizing this alignment loss establishes a computationally\ntractable surrogate objective for maximizing a variational lower bound on the\nlog-likelihood of latents under the target distribution. Notably, the proposed\nmethod eliminates computationally expensive likelihood evaluations and avoids\nODE solving during optimization. As a proof of concept, we demonstrate in a\ncontrolled setting that the alignment loss landscape closely approximates the\nnegative log-likelihood of the target distribution. We further validate the\neffectiveness of our approach through large-scale image generation experiments\non ImageNet with diverse target distributions, accompanied by detailed\ndiscussions and ablation studies. With both theoretical and empirical\nvalidation, our framework paves a new way for latent space alignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630b094f8b327c7b8b94d24c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
      "fullname": "Yizhuo Li",
      "name": "liyz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05176",
      "authors": [
        {
          "_id": "6842521939f41e76fd96ae38",
          "name": "Yanzhao Zhang",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae39",
          "name": "Mingxin Li",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3a",
          "user": {
            "_id": "616adb8578833ce5997e441a",
            "avatarUrl": "/avatars/bf5c04a6032709f35e3fb48e1be6976f.svg",
            "isPro": false,
            "fullname": "Dingkun Long",
            "user": "thenlper",
            "type": "user"
          },
          "name": "Dingkun Long",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:13.249Z",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3b",
          "user": {
            "_id": "63b6dbc8ccebeadccc888456",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
            "isPro": false,
            "fullname": "Xin Zhang",
            "user": "izhx",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:10.698Z",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3c",
          "name": "Huan Lin",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3d",
          "name": "Baosong Yang",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3e",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae3f",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae40",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae41",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae42",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6842521939f41e76fd96ae43",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T15:49:48.000Z",
      "submittedOnDailyAt": "2025-06-06T01:01:32.740Z",
      "title": "Embedimiento de Qwen3: Mejora de la representación de frases e implementación de recuperación de resultados mediante modelos de token embedding",
      "submittedOnDailyBy": {
        "_id": "616adb8578833ce5997e441a",
        "avatarUrl": "/avatars/bf5c04a6032709f35e3fb48e1be6976f.svg",
        "isPro": false,
        "fullname": "Dingkun Long",
        "user": "thenlper",
        "type": "user"
      },
      "summary": "En este artículo se presenta la serie de Qwen3 Embedding. Esta serie ha evolucionado significativamente en comparación con las anteriores GTE-Qwen, mejorando su capacidad para entender contextos y realizar búsquedas de nuevo. Esta serie se ha construido a partir de modelos basados en Qwen3. Se ha utilizado la fuerte comprensión multilingüe y capacidad de generación de los modelos Qwen3 LLMs para introducir un nuevo perfil de entrenamiento multinivel. Este perfil combina un entrenamiento de gran escala sin límites y un ajuste de precisión con base en conjuntos de datos de alta calidad. Además, una estratégia efectiva de integración del modelo garantiza la robustez y adaptabilidad de la serie Qwen3 Embedding. Durante el proceso de entrenamiento, los modelos Qwen3 LLMs desempeñan el papel de los modelos Baiju, además de contribuir significativamente al síntesis de amplios conjuntos de datos de alta calidad, ricos y diversos en diversas áreas y lenguas. La serie Qwen3 Embedding proporciona un rango de tamaños de modelo (0.6B, 4B, 8B) para tareas de comprensión contextual y búsqueda, permitiendo a los usuarios optimizar la eficiencia y efectividad en diferentes escenarios de uso. Basados en evaluaciones experimentales, la serie Qwen3 Embedding ha obtenido los mejores resultados en varios benchmarks. En particular, en la evaluación multilingüe del MTEB, destaca por su excelente rendimiento en tareas de búsqueda como comprensión contextual, búsqueda de código, búsqueda cruzada y búsqueda multilingüe. Para fomentar la reproducibilidad y el desarrollo de investigación dirigida por la comunidad, los modelos Qwen3 Embedding están disponibles bajo la licencia Apache 2.0.",
      "upvotes": 16,
      "discussionId": "6842521a39f41e76fd96ae6f",
      "projectPage": "https://qwenlm.github.io/blog/qwen3-embedding/",
      "githubRepo": "https://github.com/QwenLM/Qwen3-Embedding",
      "ai_summary": "The Qwen3 Embedding series, built on Qwen3 foundation models, offers advanced text embedding and reranking capabilities through a multi-stage training pipeline, achieving state-of-the-art performance across multilingual and retrieval benchmarks.",
      "ai_keywords": [
        "Qwen3 Embedding series",
        "GTE-Qwen series",
        "Qwen3 LLMs",
        "multilingual text understanding",
        "unsupervised pre-training",
        "supervised fine-tuning",
        "model merging",
        "embedding",
        "reranking",
        "MTEB",
        "code retrieval",
        "cross-lingual retrieval",
        "multilingual retrieval"
      ]
    },
    "publishedAt": "2025-06-05T11:49:48.000Z",
    "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models",
    "summary": "In this work, we introduce the Qwen3 Embedding series, a significant\nadvancement over its predecessor, the GTE-Qwen series, in text embedding and\nreranking capabilities, built upon the Qwen3 foundation models. Leveraging the\nQwen3 LLMs' robust capabilities in multilingual text understanding and\ngeneration, our innovative multi-stage training pipeline combines large-scale\nunsupervised pre-training with supervised fine-tuning on high-quality datasets.\nEffective model merging strategies further ensure the robustness and\nadaptability of the Qwen3 Embedding series. During the training process, the\nQwen3 LLMs serve not only as backbone models but also play a crucial role in\nsynthesizing high-quality, rich, and diverse training data across multiple\ndomains and languages, thus enhancing the training pipeline. The Qwen3\nEmbedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both\nembedding and reranking tasks, addressing diverse deployment scenarios where\nusers can optimize for either efficiency or effectiveness. Empirical\nevaluations demonstrate that the Qwen3 Embedding series achieves\nstate-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmark MTEB for text embedding, as well as in\nvarious retrieval tasks, including code retrieval, cross-lingual retrieval and\nmultilingual retrieval. To facilitate reproducibility and promote\ncommunity-driven research and development, the Qwen3 Embedding models are\npublicly available under the Apache 2.0 license.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05176.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "616adb8578833ce5997e441a",
      "avatarUrl": "/avatars/bf5c04a6032709f35e3fb48e1be6976f.svg",
      "fullname": "Dingkun Long",
      "name": "thenlper",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 96
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04633",
      "authors": [
        {
          "_id": "68426296b8d07a60074e866a",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866b",
          "name": "Mahtab Bigverdi",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866c",
          "user": {
            "_id": "645b4819f9d4ec91fdd54852",
            "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
            "isPro": false,
            "fullname": "Jiawei Gu",
            "user": "kuvvi",
            "type": "user"
          },
          "name": "Jiawei Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:38.249Z",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866d",
          "name": "Zixian Ma",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866e",
          "name": "Yinuo Yang",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e866f",
          "name": "Ziang Li",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e8670",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "68426296b8d07a60074e8671",
          "name": "Ranjay Krishna",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T05:09:46.000Z",
      "submittedOnDailyAt": "2025-06-06T02:08:57.401Z",
      "title": "Reconocimiento del espectro: Evaluación de modelos multimodal en simulación visual",
      "submittedOnDailyBy": {
        "_id": "645b4819f9d4ec91fdd54852",
        "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
        "isPro": false,
        "fullname": "Jiawei Gu",
        "user": "kuvvi",
        "type": "user"
      },
      "summary": "La percepción del espacio es una parte importante del inteligencia humana, funcionando de manera similar a la deducción lingüística sin depender principalmente de la simulación visual para resolver problemas. Sin embargo, los marcos de referencia actuales de IA evalúan principalmente la deducción lingüística y fallan en evaluar la complejidad no-lingüística y multi-nivel de la simulación visual. Presentamos STARE (Evaluación de Transformación Espacial y Teoría de Razones). STARE es un marco de referencia que evalúa de manera estricta diferentes tipos de tareas complejas que se pueden resolver mediante simulación visual. STARE incluye tareas de transformación geométrica básica (2D y 3D), teoría de razones espaciales integradas (juntas de cubos y puzzles de Tangram), y teoría de razones espaciales en el mundo real (teoría de puntos y tiempo). Estas tareas reflejan problemas cognitivos prácticos como la asambla de objetos, la interpretación de dibujos mecánicos y el movimiento espacial diario. Según nuestra evaluación, los modelos son familiares con la teoría de razones de transformaciones 2D pero presentan un comportamiento similar a su aproximación cuando se requiere una simulación visual multi-nivel, como en la resolución de juntas de cubos y puzzles de Tangram. Humanos pueden alcanzar una precisión casi exacta, aunque requieren un tiempo significativo (máximo 28.9 segundos) para complejos tareas, lo que se reduce a un promedio de 7.5 segundos mediante la utilización de simulaciones visuales intermedias. Por otro lado, los modelos no experimentan un aumento significativo en su rendimiento a partir de la simulación visual, mejorando en muchas tareas pero disminuyendo en casos específicos, como en el puzzle de Tangram (GPT-4o, o1) y las juntas de cubos (Claude-3.5, Gemini-2.0 Flash). Esto demuestra que los modelos no tienen métodos efectivos para utilizar información visual intermedia.",
      "upvotes": 15,
      "discussionId": "68426298b8d07a60074e86eb",
      "projectPage": "https://huggingface.co/datasets/kuvvi/STARE",
      "githubRepo": "https://github.com/STARE-bench/STARE/",
      "ai_summary": "A new benchmark evaluates multimodal models on visual simulation tasks, revealing varying model performances compared to human accuracy and the impact of intermediate visual simulations.",
      "ai_keywords": [
        "spatial cognition",
        "visual simulations",
        "verbal reasoning",
        "multimodal large language models",
        "STARE",
        "spatial transformations",
        "geometric transformations",
        "integrated spatial reasoning",
        "real-world spatial reasoning",
        "visual reasoning",
        "intermediate visual simulations"
      ]
    },
    "publishedAt": "2025-06-05T01:09:46.000Z",
    "title": "Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual\n  Simulations",
    "summary": "Spatial cognition is essential for human intelligence, enabling\nproblem-solving through visual simulations rather than solely relying on verbal\nreasoning. However, existing AI benchmarks primarily assess verbal reasoning,\nneglecting the complexities of non-verbal, multi-step visual simulation. We\nintroduce STARE(Spatial Transformations and Reasoning Evaluation), a benchmark\ndesigned to rigorously evaluate multimodal large language models on tasks\nbetter solved through multi-step visual simulation. STARE features 4K tasks\nspanning foundational geometric transformations (2D and 3D), integrated spatial\nreasoning (cube net folding and tangram puzzles), and real-world spatial\nreasoning (perspective and temporal reasoning), reflecting practical cognitive\nchallenges like object assembly, mechanical diagram interpretation, and\neveryday spatial navigation. Our evaluations show that models excel at\nreasoning over simpler 2D transformations, but perform close to random chance\non more complex tasks like 3D cube net folding and tangram puzzles that require\nmulti-step visual simulations. Humans achieve near-perfect accuracy but take\nconsiderable time (up to 28.9s) on complex tasks, significantly speeding up\n(down by 7.5 seconds on average) with intermediate visual simulations. In\ncontrast, models exhibit inconsistent performance gains from visual\nsimulations, improving on most tasks but declining in specific cases like\ntangram puzzles (GPT-4o, o1) and cube net folding (Claude-3.5, Gemini-2.0\nFlash), indicating that models may not know how to effectively leverage\nintermediate visual information.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04633.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b4819f9d4ec91fdd54852",
      "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
      "fullname": "Jiawei Gu",
      "name": "kuvvi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05344",
      "authors": [
        {
          "_id": "68424fe9bdc448822b31beac",
          "name": "Jiahui Wang",
          "hidden": false
        },
        {
          "_id": "68424fe9bdc448822b31bead",
          "user": {
            "_id": "64f001bfabd9fb1914398bd5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
            "isPro": false,
            "fullname": "liuzuyan",
            "user": "Zuyan",
            "type": "user"
          },
          "name": "Zuyan Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:17.520Z",
          "hidden": false
        },
        {
          "_id": "68424fe9bdc448822b31beae",
          "name": "Yongming Rao",
          "hidden": false
        },
        {
          "_id": "68424fe9bdc448822b31beaf",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-06T00:48:21.379Z",
      "title": "SparseMM: La espesitud de la cabeza se manifiesta en la respuesta visual de los conceptos de MLLM.",
      "submittedOnDailyBy": {
        "_id": "64f001bfabd9fb1914398bd5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
        "isPro": false,
        "fullname": "liuzuyan",
        "user": "Zuyan",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) con habilidades visuales (MLLMs) generalmente se obtienen añadiendo capacidades visuales a modelos pre-entrenados de lenguaje grande. En este estudio, investigamos cómo MLLMs procesan entradas visuales y analizamos la estructura de la atención. Resultó que se descubrió un fenómeno sorprendente de escasez: solo un pequeño porcentaje, menos del 5%, de los encabezados de atención de los LLMs están activamente involucrados en la comprensión visual. Para identificar eficientemente estos encabezados, diseñamos un marco de entrenamiento que cuantifica la relación visual de nivel de encabezado mediante el análisis de respuestas. Basándonos en esta descubrimiento, presentamos KV-Cache optimización y SparseMM, un estilo que asigna sesiones de cálculo no equilibradas a los encabezados de los LLMs basados en un escore visual. Esto tiene como objetivo acelerar la inferencia de MLLMs utilizando la escasez de los encabezados visuales. Comparado con los métodos de aceleración de KV-Cache previos, SparseMM se centra en ignorar las características visuales, priorizando reducciones en resolución y memoria. En evaluaciones extendidas en los principales benchmarks multimodales, SparseMM mantiene un buen equilibrio entre precisión y eficiencia. En particular, SparseMM logró mantener la igualdad de rendimiento mientras aumentó la velocidad de tiempo real en 1.38 veces y redució la memoria en un 52%. Nuestro proyecto está disponible en código abierto en https://github.com/CR400AF-A/SparseMM.",
      "upvotes": 14,
      "discussionId": "68424febbdc448822b31bf2c",
      "projectPage": "https://cr400af-a.github.io/SparseMM/",
      "githubRepo": "https://github.com/CR400AF-A/SparseMM",
      "ai_summary": "MLLLMs achieve enhanced efficiency through SparseMM, a KV-Cache optimization strategy that identifies and prioritizes visual heads, leading to significant real-time acceleration and memory reduction without compromising performance.",
      "ai_keywords": [
        "multimodal large language models",
        "LLMs",
        "visual capabilities",
        "attention mechanisms",
        "visual heads",
        "targeted response analysis",
        "KV-Cache optimization",
        "SparseMM",
        "head-level visual relevance",
        "visual semantics",
        "multimodal benchmarks"
      ]
    },
    "publishedAt": "2025-06-05T13:59:55.000Z",
    "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
    "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05344.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "64f001bfabd9fb1914398bd5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
      "fullname": "liuzuyan",
      "name": "Zuyan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05331",
      "authors": [
        {
          "_id": "684260765bfed1b94a9cc307",
          "user": {
            "_id": "647c7a4ed412b3b376572a00",
            "avatarUrl": "/avatars/9cc310fd3f9e3f211475816ed9b0cdaa.svg",
            "isPro": false,
            "fullname": "Xinyan Chen",
            "user": "xy06",
            "type": "user"
          },
          "name": "Xinyan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:42.546Z",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc308",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc309",
          "user": {
            "_id": "6349214f8146350b3a4c5cdf",
            "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
            "isPro": false,
            "fullname": "Dongzhi Jiang",
            "user": "CaraJ",
            "type": "user"
          },
          "name": "Dongzhi Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:40.308Z",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc30a",
          "name": "Aojun Zhou",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc30b",
          "name": "Shilin Yan",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc30c",
          "name": "Weifeng Lin",
          "hidden": false
        },
        {
          "_id": "684260765bfed1b94a9cc30d",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:59:02.000Z",
      "submittedOnDailyAt": "2025-06-06T02:07:44.094Z",
      "title": "MINT-CoT: Permite el uso de tokens de visión indirecta en la Reasoning Chain-of-Thought en matemáticas.",
      "submittedOnDailyBy": {
        "_id": "647c7a4ed412b3b376572a00",
        "avatarUrl": "/avatars/9cc310fd3f9e3f211475816ed9b0cdaa.svg",
        "isPro": false,
        "fullname": "Xinyan Chen",
        "user": "xy06",
        "type": "user"
      },
      "summary": "Chain-of-Thought (CoT) ha sido ampliamente mejorado para la inferencia matemática de modelos de lenguaje grandes (LLMs), pero su expansión a diversos campos es compleja. Los investigadores están tratando de aplicar una inferencia contextual similar a entradas de imagenes o superar las limitaciones de la percepción visual de un encoder visual insertando señales visuales entre tokens matemáticos. Sin embargo, existen tres limitaciones principales en la resolución de problemas matemáticos: la dependencia en el área de imagen grande, las limitaciones de la percepción de contenido matemático por el encoder visual y la dependencia en la capacidad externa de cambios visuales. En este artículo, se introduce una inferencia visual de CoT utilizando tokens interleaved matemáticos, llamado MINT-CoT. MINT-CoT selecciona de manera dinámica áreas visuales de cualquier forma dentro del grafo matemático, insertando tokens visuales adecuados en los pasos de inferencia contextual utilizando tokens interleaved. Para lograr esto, se construyó el conjunto de datos MINT-CoT, que incluye 54K problemas matemáticos y asocia a cada paso de inferencia un proceso estricto de generación de datos que incluye áreas visuales de nivel de tokens. Además, se propone una estrategia de entrenamiento en tres etapas para construir el modelo MINT-CoT-7B, incluyendo CoT SFT contextual, CoT SFT insertado y CoT RL insertado. Los experimentos expandidos muestran el efecto de la inferencia visual insertada válido en el campo matemático, y el modelo MINT-CoT-7B presenta un aumento de +34.08% en MathVista, +28.78% en GeoQA y +23.2% en MMStar. El código y los datos están disponibles en https://github.com/xinyan-cxy/MINT-CoT.",
      "upvotes": 12,
      "discussionId": "684260775bfed1b94a9cc346",
      "githubRepo": "https://github.com/xinyan-cxy/MINT-CoT",
      "ai_summary": "MINT-CoT enhances multimodal mathematical reasoning by interleaving visual tokens into textual chain-of-thought steps, enabling flexible visual perception and improved problem-solving.",
      "ai_keywords": [
        "Chain-of-Thought",
        "Large Language Models",
        "multimodal domains",
        "textual reasoning",
        "visual signals",
        "image input",
        "vision encoders",
        "math content",
        "visual modification",
        "Mathematical INterleaved Tokens",
        "Interleave Token",
        "visual regions",
        "token level",
        "MINT-CoT dataset",
        "text-only CoT SFT",
        "interleaved CoT SFT",
        "interleaved CoT RL",
        "MINT-CoT-7B",
        "MathVista",
        "GeoQA",
        "MMStar"
      ]
    },
    "publishedAt": "2025-06-05T13:59:02.000Z",
    "title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical\n  Chain-of-Thought Reasoning",
    "summary": "Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large\nLanguage Models (LLMs), but it still remains challenging for extending it to\nmultimodal domains. Existing works either adopt a similar textual reasoning for\nimage input, or seek to interleave visual signals into mathematical CoT.\nHowever, they face three key limitations for math problem-solving: reliance on\ncoarse-grained box-shaped image regions, limited perception of vision encoders\non math content, and dependence on external capabilities for visual\nmodification. In this paper, we propose MINT-CoT, introducing Mathematical\nINterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively\ninterleaves relevant visual tokens into textual reasoning steps via an\nInterleave Token, which dynamically selects visual regions of any shapes within\nmath figures. To empower this capability, we construct the MINT-CoT dataset,\ncontaining 54K mathematical problems aligning each reasoning step with visual\nregions at the token level, accompanied by a rigorous data generation pipeline.\nWe further present a three-stage MINT-CoT training strategy, progressively\ncombining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which\nderives our MINT-CoT-7B model. Extensive experiments demonstrate the\neffectiveness of our method for effective visual interleaved reasoning in\nmathematical domains, where MINT-CoT-7B outperforms the baseline model by\n+34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our\ncode and data are available at https://github.com/xinyan-cxy/MINT-CoT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05331.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647c7a4ed412b3b376572a00",
      "avatarUrl": "/avatars/9cc310fd3f9e3f211475816ed9b0cdaa.svg",
      "fullname": "Xinyan Chen",
      "name": "xy06",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05328",
      "authors": [
        {
          "_id": "68424822f0c91a7dcb64193b",
          "user": {
            "_id": "64a3de701698ad2985277148",
            "avatarUrl": "/avatars/09eebadbbea53ed2800591564ff5c931.svg",
            "isPro": false,
            "fullname": "lulidong",
            "user": "lulidong",
            "type": "user"
          },
          "name": "Lidong Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:34.755Z",
          "hidden": false
        },
        {
          "_id": "68424822f0c91a7dcb64193c",
          "user": {
            "_id": "6392c73390b8e99a6779a7b0",
            "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
            "isPro": false,
            "fullname": "Guo Chen",
            "user": "cg1177",
            "type": "user"
          },
          "name": "Guo Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:32.158Z",
          "hidden": false
        },
        {
          "_id": "68424822f0c91a7dcb64193d",
          "name": "Zhiqi Li",
          "hidden": false
        },
        {
          "_id": "68424822f0c91a7dcb64193e",
          "name": "Yicheng Liu",
          "hidden": false
        },
        {
          "_id": "68424822f0c91a7dcb64193f",
          "name": "Tong Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:58:33.000Z",
      "submittedOnDailyAt": "2025-06-06T00:16:44.777Z",
      "title": "AV-Reasoner: Siguiendo el Guía Cool para mejorar la cantidad de voz-visión y realizar pruebas de referencia en MLLM.",
      "submittedOnDailyBy": {
        "_id": "64a3de701698ad2985277148",
        "avatarUrl": "/avatars/09eebadbbea53ed2800591564ff5c931.svg",
        "isPro": false,
        "fullname": "lulidong",
        "user": "lulidong",
        "type": "user"
      },
      "summary": "El desarrollo de la comprensión del vídeo ha llevado a que los actuales MLLM se enfrenten a numerosos desafíos en la tarea de contar números. Los marcos de referencia existentes continúan enfrentando problemas como vídeos cortos, consultas cercanas, una escasez de categorías y una cobertura débil multi-modelo. En este artículo, se presenta CG-AV-Counting, un marco de referencia de contado de números basado en colores con 1,027 consultas multi-modelo y 5,845 categorías específicas. Este marco de referencia proporciona evaluaciones de blanco y negro, y ofrece un estricto test para el contado de números de end-to-end y basado en razones. Para mejorar la capacidad de contar números, se propone un modelo de AV-Reasoner entrenado utilizando GRPO y clavelearning. El AV-Reasoner registra los mejores resultados en varios marcos de referencia y demuestra el efecto de la aprendizaje por refuerzo. Sin embargo, los experimentos en benchmarks fuera del dominio no demuestran que el dominio lingüístico con razones contribuye a mejorar el rendimiento. El código y los benchmarks están disponibles en https://av-reasoner.github.io.",
      "upvotes": 12,
      "discussionId": "68424823f0c91a7dcb6419c7",
      "projectPage": "https://AV-Reasoner.github.io",
      "githubRepo": "https://github.com/AV-Reasoner/AV-Reasoner",
      "ai_summary": "CG-AV-Counting is a new benchmark for video counting tasks that includes multimodal data and supports end-to-end and reasoning-based models. AV-Reasoner, trained with GRPO and curriculum learning, achieves top results but shows limitations on out-of-domain tasks.",
      "ai_keywords": [
        "MLLMs",
        "CG-AV-Counting",
        "multimodal questions",
        "clue-grounded",
        "black-box evaluation",
        "white-box evaluation",
        "AV-Reasoner",
        "GRPO",
        "curriculum learning",
        "reinforcement learning",
        "out-of-domain benchmarks"
      ]
    },
    "publishedAt": "2025-06-05T13:58:33.000Z",
    "title": "AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual\n  Counting for MLLMs",
    "summary": "Despite progress in video understanding, current MLLMs struggle with counting\ntasks. Existing benchmarks are limited by short videos, close-set queries, lack\nof clue annotations, and weak multimodal coverage. In this paper, we introduce\nCG-AV-Counting, a manually-annotated clue-grounded counting benchmark with\n1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It\nsupports both black-box and white-box evaluation, serving as a comprehensive\ntestbed for both end-to-end and reasoning-based counting. To explore ways to\nimprove model's counting capability, we propose AV-Reasoner, a model trained\nwith GRPO and curriculum learning to generalize counting ability from related\ntasks. AV-Reasoner achieves state-of-the-art results across multiple\nbenchmarks, demonstrating the effectiveness of reinforcement learning. However,\nexperiments show that on out-of-domain benchmarks, reasoning in the language\nspace fails to bring performance gains. The code and benchmark have been\nrealeased on https://av-reasoner.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05328.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a3de701698ad2985277148",
      "avatarUrl": "/avatars/09eebadbbea53ed2800591564ff5c931.svg",
      "fullname": "lulidong",
      "name": "lulidong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03077",
      "authors": [
        {
          "_id": "683fc07a1de14546d5decf19",
          "name": "Qijun Luo",
          "hidden": false
        },
        {
          "_id": "683fc07a1de14546d5decf1a",
          "user": {
            "_id": "65a521af90b5e87bcd343828",
            "avatarUrl": "/avatars/3bbe83c1ba47df17d9c05a049147e5cc.svg",
            "isPro": false,
            "fullname": "Mengqi Li",
            "user": "Kullpar",
            "type": "user"
          },
          "name": "Mengqi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:10.607Z",
          "hidden": false
        },
        {
          "_id": "683fc07a1de14546d5decf1b",
          "name": "Lei Zhao",
          "hidden": false
        },
        {
          "_id": "683fc07a1de14546d5decf1c",
          "name": "Xiao Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T16:54:15.000Z",
      "submittedOnDailyAt": "2025-06-06T02:05:55.529Z",
      "title": "StreamBP: Algoritmo de cálculo iterativo preciso y eficiente en memoria para entrenamiento de secuencias largas",
      "submittedOnDailyBy": {
        "_id": "65a521af90b5e87bcd343828",
        "avatarUrl": "/avatars/3bbe83c1ba47df17d9c05a049147e5cc.svg",
        "isPro": false,
        "fullname": "Mengqi Li",
        "user": "Kullpar",
        "type": "user"
      },
      "summary": "Entrenar un modelo de lenguaje con secuencias largas es una tarea compleja (por ejemplo, lógica de cadenas) que exige mejorar la capacidad del modelo. Sin embargo, al aumentar la longitud de la secuencia, el costo de memoria de los valores activos en el proceso de retropropagación (BP) aumenta significativamente. Para resolver este problema, se propone StreamBP, un método de retropropagación eficiente en memoria. StreamBP descompone linealmente los escalones de la secuencia y reduce significativamente los costos de memoria de los valores activos y las funciones de activación. Este método se puede aplicar a diversos modelos generales, como SFT, GRPO y DPO. En términos de implementación, StreamBP utiliza la estructura causal del modelo de lenguaje para reducir los costos de computación FLOPs y acelerar la velocidad de la retropropagación. Comparado con Gradient Checkpointing, StreamBP puede aumentar la longitud máxima de la secuencia en un rango de 2.8 a 5.5 veces y reducir el tiempo de retropropagación relativamente o significativamente. La capacidad de escalabilidad de la longitud de la secuencia de StreamBP permite aplicar directamente la escalabilidad del tamaño de la muestra para acelerar el entrenamiento. Además, se desarrolló una versión distribuida de StreamBP eficiente en comunicación para apoyar la entrenamiento efectiva en múltiples GPUs y ampliar su aplicación. Nuestro código se integra fácilmente en cualquier pipeline de entrenamiento de modelos Transformer y está disponible en https://github.com/Ledzy/StreamBP.",
      "upvotes": 12,
      "discussionId": "683fc07e1de14546d5decfe2",
      "githubRepo": "https://github.com/Ledzy/StreamBP",
      "ai_summary": "StreamBP, a memory-efficient and exact backpropagation method, decomposes the chain rule to reduce memory costs, enabling longer sequence lengths and faster training speeds for language models compared to gradient checkpointing.",
      "ai_keywords": [
        "backpropagation (BP)",
        "memory-efficient",
        "exact BP",
        "gradient checkpointing",
        "chain rule",
        "sequence dimension",
        "layer-wise",
        "activation values",
        "logits",
        "SFT",
        "GRPO",
        "DPO",
        "computational FLOPs",
        "BP speed",
        "causal structure",
        "language model",
        "multi-GPU training",
        "distributed StreamBP"
      ]
    },
    "publishedAt": "2025-06-03T12:54:15.000Z",
    "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence\n  Training of LLMs",
    "summary": "Training language models on long sequence data is a demanding requirement for\nenhancing the model's capability on complex tasks, e.g., long-chain reasoning.\nHowever, as the sequence length scales up, the memory cost for storing\nactivation values becomes huge during the Backpropagation (BP) process, even\nwith the application of gradient checkpointing technique. To tackle this\nchallenge, we propose a memory-efficient and exact BP method called StreamBP,\nwhich performs a linear decomposition of the chain rule along the sequence\ndimension in a layer-wise manner, significantly reducing the memory cost of\nactivation values and logits. The proposed method is applicable to common\nobjectives such as SFT, GRPO, and DPO. From an implementation perspective,\nStreamBP achieves less computational FLOPs and faster BP speed by leveraging\nthe causal structure of the language model. Compared to gradient checkpointing,\nStreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,\nwhile using comparable or even less BP time. Note that StreamBP's sequence\nlength scaling ability can be directly transferred to batch size scaling for\naccelerating training. We further develop a communication-efficient distributed\nStreamBP to effectively support multi-GPU training and broaden its\napplicability. Our code can be easily integrated into the training pipeline of\nany transformer models and is available at https://github.com/Ledzy/StreamBP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03077.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65a521af90b5e87bcd343828",
      "avatarUrl": "/avatars/3bbe83c1ba47df17d9c05a049147e5cc.svg",
      "fullname": "Mengqi Li",
      "name": "Kullpar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05327",
      "authors": [
        {
          "_id": "6842591962047f5641b3b650",
          "user": {
            "_id": "661d1f83ea3df2195a7c2924",
            "avatarUrl": "/avatars/dec49fc1d79913b07b57ccbef079198f.svg",
            "isPro": false,
            "fullname": "dcshi",
            "user": "dc-walker",
            "type": "user"
          },
          "name": "Duochao Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:58.621Z",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b651",
          "user": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "isPro": false,
            "fullname": "Weijie Wang",
            "user": "lhmd",
            "type": "user"
          },
          "name": "Weijie Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:00.966Z",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b652",
          "name": "Donny Y. Chen",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b653",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b654",
          "name": "Jia-Wang Bian",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b655",
          "name": "Bohan Zhuang",
          "hidden": false
        },
        {
          "_id": "6842591962047f5641b3b656",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:58:23.000Z",
      "submittedOnDailyAt": "2025-06-06T01:28:04.514Z",
      "title": "Reevaluación de la Propagación 3D Gaussiana de Dispersión utilizando la Expresión Profunda.",
      "submittedOnDailyBy": {
        "_id": "66699aa8a33847217b5a49c7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
        "isPro": false,
        "fullname": "Weijie Wang",
        "user": "lhmd",
        "type": "user"
      },
      "summary": "El mapa de profundidad, ampliamente utilizado en la cadena de trabajo de 3DGS (3D Gaussian Splitting), está adaptado para la síntesis de nuevos puntos de vista. Este enfoque transforma el mapa de profundidad en una nube de puntos 3D para realizar la síntesis de nuevos puntos de vista. Esta metodología ofrece varios beneficios, como entrenamiento eficiente, uso de posiciones de cámara existentes y estimación precisa de la geometría. Sin embargo, las discontinuidades de profundidad en los bordes de los objetos atrapan la destrucción y la esparsidad de la nube de puntos, afectando la calidad del rendimiento. Para resolver estos problemas, presentamos una nueva pérdida de normalización llamada PM-Loss, basada en un mapa de puntos predecido por un transformador preentrenado. Aunque el mapa de puntos no es tan preciso que el mapa de profundidad, especialmente en los bordes de los objetos, impone la regularización de la geometría y mejora el mapa de profundidad. Con esta mejora del mapa de profundidad, nuestro método mejora significativamente la 3DGS en diferentes arquitecturas y escenarios, ofreciendo resultados de rendimiento consistentes y buenos. Nuestra página del proyecto está disponible en https://aim-uofa.github.io/PMLoss.",
      "upvotes": 10,
      "discussionId": "6842591a62047f5641b3b6bc",
      "projectPage": "https://aim-uofa.github.io/PMLoss",
      "githubRepo": "https://github.com/aim-uofa/PM-Loss",
      "ai_summary": "PM-Loss, a regularization technique using pointmaps from a pre-trained transformer, enhances feed-forward 3D Gaussian Splatting by improving depth map accuracy and rendering quality.",
      "ai_keywords": [
        "3D Gaussian Splatting",
        "3DGS",
        "depth maps",
        "point clouds",
        "novel view synthesis",
        "PM-Loss",
        "pre-trained transformer",
        "pointmap",
        "geometric smoothness"
      ]
    },
    "publishedAt": "2025-06-05T13:58:23.000Z",
    "title": "Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting",
    "summary": "Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS)\npipelines by unprojecting them into 3D point clouds for novel view synthesis.\nThis approach offers advantages such as efficient training, the use of known\ncamera poses, and accurate geometry estimation. However, depth discontinuities\nat object boundaries often lead to fragmented or sparse point clouds, degrading\nrendering quality -- a well-known limitation of depth-based representations. To\ntackle this issue, we introduce PM-Loss, a novel regularization loss based on a\npointmap predicted by a pre-trained transformer. Although the pointmap itself\nmay be less accurate than the depth map, it effectively enforces geometric\nsmoothness, especially around object boundaries. With the improved depth map,\nour method significantly improves the feed-forward 3DGS across various\narchitectures and scenes, delivering consistently better rendering results. Our\nproject page: https://aim-uofa.github.io/PMLoss",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05327.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66699aa8a33847217b5a49c7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
      "fullname": "Weijie Wang",
      "name": "lhmd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05349",
      "authors": [
        {
          "_id": "68424bed54a0d0e4b906baca",
          "name": "Hanoona Rasheed",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bacb",
          "name": "Abdelrahman Shaker",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bacc",
          "name": "Anqi Tang",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bacd",
          "name": "Muhammad Maaz",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bace",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bacf",
          "name": "Salman Khan",
          "hidden": false
        },
        {
          "_id": "68424bed54a0d0e4b906bad0",
          "name": "Fahad Khan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64636b2551fa6e6306046293/rVaEoWuqnZnoewKuZDe09.mp4"
      ],
      "publishedAt": "2025-06-05T17:59:58.000Z",
      "submittedOnDailyAt": "2025-06-06T04:23:10.625Z",
      "title": "VideoMathQA: Video baseado en la comprensión multiforma de matemáticas para pruebas de lógica",
      "submittedOnDailyBy": {
        "_id": "64636b2551fa6e6306046293",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64636b2551fa6e6306046293/Uuz6z2MZb_LKLGM8uxF9s.jpeg",
        "isPro": false,
        "fullname": "Hanoona Rasheed",
        "user": "Hanoona",
        "type": "user"
      },
      "summary": "En el conjunto de videos de matemática, la inferencia matemática en el mundo real es un problema estructuralmente diferente a las imágenes estáticas o textos. Esto implica la interpretación de información visual detallada, la lectura precisa de letras manuscritos o texto digital, y la necesidad de integrar vocabulario no lineal a lo largo del tiempo. El éxito en este contexto multimodal depende no solo del reconocimiento visual, sino también de la selección y integración de detalles específicos de contexto en flujos complejos de noticias. Para evaluar si modelos pueden realizar estas inferencias temporalmente expandidas en videos, se introduce el proyecto VIDEOMATHQA, un marco de referencia para evaluar este tipo de inferencia. Este marco de referencia amplía 10 áreas de matemáticas y utiliza videos de 10 segundos a 1 hora. Requiere la interpretación de contenido visual estructurado, la comprensión de narraciones educativas y la integración de archivos de modalidades visual, sonido y texto. Para garantizar alta calidad de registro, se contrató a expertos de nivel doctorado, asegurando más de 920 horas de registro. Las preguntas se diseñaron centradas en tres problemas de inferencia clave: resolución directa de problemas, transferencia conceptual y comprensión profunda de enseñanzas, que integran razones estructuradas con explicaciones extendidas y parte de las soluciones. Cada pregunta incluye un registro de razones estructuradas, lo que permite diagnosticar con precisión las capacidades del modelo. A través de VIDEOMATHQA, se revelan las limitaciones de los métodos actuales y se construye un marco de evaluación sistemático para modelos que realicen razones en problemas matemáticos multimodales temporalmente expandidos. Los marcos de referencia y códigos de evaluación de VIDEOMATHQA se pueden acceder en https://mbzuai-oryx.github.io/VideoMathQA.",
      "upvotes": 9,
      "discussionId": "68424bef54a0d0e4b906bb3e",
      "ai_summary": "VideoMathQA evaluates models' ability to perform temporally extended cross-modal reasoning across various mathematical domains in video settings, addressing direct problem solving, conceptual transfer, and deep instructional comprehension.",
      "ai_keywords": [
        "VideoMathQA",
        "temporally extended cross-modal reasoning",
        "structured visual content",
        "instructional narratives",
        "modality-rich",
        "multi-step reasoning",
        "partial solutions",
        "multi-step reasoning annotations",
        "system evaluation framework"
      ]
    },
    "publishedAt": "2025-06-05T13:59:58.000Z",
    "title": "VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal\n  Understanding in Videos",
    "summary": "Mathematical reasoning in real-world video settings presents a fundamentally\ndifferent challenge than in static images or text. It requires interpreting\nfine-grained visual information, accurately reading handwritten or digital\ntext, and integrating spoken cues, often dispersed non-linearly over time. In\nsuch multimodal contexts, success hinges not just on perception, but on\nselectively identifying and integrating the right contextual details from a\nrich and noisy stream of content. To this end, we introduce VideoMathQA, a\nbenchmark designed to evaluate whether models can perform such temporally\nextended cross-modal reasoning on videos. The benchmark spans 10 diverse\nmathematical domains, covering videos ranging from 10 seconds to over 1 hour.\nIt requires models to interpret structured visual content, understand\ninstructional narratives, and jointly ground concepts across visual, audio, and\ntextual modalities. We employ graduate-level experts to ensure high quality,\ntotaling over 920 man-hours of annotation. To reflect real-world scenarios,\nquestions are designed around three core reasoning challenges: direct problem\nsolving, where answers are grounded in the presented question; conceptual\ntransfer, which requires applying learned methods to new problems; and deep\ninstructional comprehension, involving multi-step reasoning over extended\nexplanations and partially worked-out solutions. Each question includes\nmulti-step reasoning annotations, enabling fine-grained diagnosis of model\ncapabilities. Through this benchmark, we highlight the limitations of existing\napproaches and establish a systematic evaluation framework for models that must\nreason, rather than merely perceive, across temporally extended and\nmodality-rich mathematical problem settings. Our benchmark and evaluation code\nare available at: https://mbzuai-oryx.github.io/VideoMathQA",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64636b2551fa6e6306046293/rVaEoWuqnZnoewKuZDe09.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05349.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64636b2551fa6e6306046293",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64636b2551fa6e6306046293/Uuz6z2MZb_LKLGM8uxF9s.jpeg",
      "fullname": "Hanoona Rasheed",
      "name": "Hanoona",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05345",
      "authors": [
        {
          "_id": "6842a3cb9393cafb521855aa",
          "name": "Adrian Łańcucki",
          "hidden": false
        },
        {
          "_id": "6842a3cb9393cafb521855ab",
          "name": "Konrad Staniszewski",
          "hidden": false
        },
        {
          "_id": "6842a3cb9393cafb521855ac",
          "name": "Piotr Nawrot",
          "hidden": false
        },
        {
          "_id": "6842a3cb9393cafb521855ad",
          "name": "Edoardo M. Ponti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-06T06:46:43.733Z",
      "title": "Inferencia con escalado hiper y compresión de caché KV",
      "submittedOnDailyBy": {
        "_id": "640deb5d3c82bd463ee44735",
        "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
        "isPro": false,
        "fullname": "Piotr Nawrot",
        "user": "pnawrot",
        "type": "user"
      },
      "summary": "La escalado durante la inferencia mejora la precisión de la razón intercambiándola por eficiencia al generar secuencias largas o paralelas. Sin embargo, en los Transformer LLMs, el costo de generación está limitado por el tamaño del caché de KV, no por el número de tokens generados. Por lo tanto, se investiga este punto. Por lo tanto, se investiga la hiperescalado durante la inferencia: compresión del caché de KV para aumentar el número de tokens que se pueden generar en el mismo bucle de cálculo, mejorando así la precisión de la escalado. Sin embargo, el éxito de este enfoque depende de que el método de compresión conserve una precisión alta, incluso a un alto ratio de compresión. Para lograr la prácticidad de la hiperescalado, se presenta la Dynamic Memory Sparsification (DMS). La DMS es un nuevo método para esparsificar el caché de KV. Alcanza un 8 veces compresión en 1K etapas de entrenamiento, y mantiene una precisión mejor que el esparsificado sin entrenamiento. La DMS elimina tokens almacenados y integra representaciones ocultas para mantener información importante. Se demuestra el efecto de la hiperescalado durante la inferencia con la DMS. Mejora la precisión, reduciendo relativamente el tiempo de inferencia y el carga de memoria para varios familias de LLMs. Por ejemplo, Qwen-R1 32B logra un aumento de precisión promedio de 9.1 puntos en AIME, 7.6 puntos en GPQA y 9.6 puntos en LiveCodeBench.",
      "upvotes": 9,
      "discussionId": "6842a3cc9393cafb521855dd",
      "ai_summary": "Inference-time hyper-scaling with Dynamic Memory Sparsification in Transformer LLMs allows for increased token generation within the same compute budget by compressing the key-value cache, thereby enhancing accuracy.",
      "ai_keywords": [
        "inference-time hyper-scaling",
        "key-value (KV) cache",
        "Dynamic Memory Sparsification (DMS)",
        "token eviction",
        "representation merging",
        "AIME 24",
        "GPQA",
        "LiveCodeBench",
        "Qwen-R1 32B"
      ]
    },
    "publishedAt": "2025-06-05T13:59:55.000Z",
    "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
    "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8times compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05345.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640deb5d3c82bd463ee44735",
      "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
      "fullname": "Piotr Nawrot",
      "name": "pnawrot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05287",
      "authors": [
        {
          "_id": "68425719ba04d3ceff5bea29",
          "user": {
            "_id": "64a3fe3dde901eb01df12398",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
            "isPro": false,
            "fullname": "YuqianYuan",
            "user": "CircleRadon",
            "type": "user"
          },
          "name": "Yuqian Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:05.846Z",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2a",
          "name": "Ronghao Dang",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2b",
          "name": "Long Li",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2c",
          "name": "Wentong Li",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2d",
          "name": "Dian Jiao",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2e",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea2f",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea30",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea31",
          "name": "Wenqiao Zhang",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea32",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "68425719ba04d3ceff5bea33",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/bFTB1kqlo-Oj0gQM7yzfe.png"
      ],
      "publishedAt": "2025-06-05T17:44:12.000Z",
      "submittedOnDailyAt": "2025-06-06T01:27:53.697Z",
      "title": "EOC-Bench: MLLMs pueden reconocer, memorizar y predecir objetos en un mundo centrado en sí mismos?",
      "submittedOnDailyBy": {
        "_id": "64a3fe3dde901eb01df12398",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
        "isPro": false,
        "fullname": "YuqianYuan",
        "user": "CircleRadon",
        "type": "user"
      },
      "summary": "El surgimiento de los modelos de lenguaje de lenguaje de la visión (MLLMs) ha impulsado un gran cambio en las aplicaciones visuales centrales. Estas aplicaciones requieren una comprensión continua y contextual de los objetos cuando los usuarios interactúan con herramientas en diferentes entornos dinámicos. Sin embargo, los actuales marcos de evaluación visuales se centran en el escenario dinámico, priorizando la apariencia y las propiedades espaciales de los objetos, mientras olvidan evaluar los cambios dinámicos provocados por la interacción del usuario. Para solucionar estos defectos, presentamos EOC-Bench. EOC-Bench es un innovador marco de evaluación diseñado para evaluar de manera sistemática el reconocimiento visual centrado en los objetos en un escenario dinámico. Específicamente, EOC-Bench se divide en tres categorías de tiempo (pasado, presente y futuro), con 3,277 pares de preguntas y respuestas detalladas, y cubre 11 dimensiones de evaluación detalladas y tres referencias visuales de objetos. Para garantizar la perfección de la evaluación, hemos desarrollado un marco de anotación de microformato dentro de un ropero y diseñado un nuevo métrico de precisión temporal multiescala para la evaluación de tiempo en puntos de entrada abiertos. Basándonos en EOC-Bench, hemos realizado evaluaciones de los MLLMs de propiedad, abierto-source y nivel de objeto. EOC-Bench es una herramienta esencial para fomentar el desarrollo de la capacidad de reconocimiento visual de los objetos en los MLLMs y proporciona una sólida base para el desarrollo de modelos clave confiables en sistemas visuales.",
      "upvotes": 9,
      "discussionId": "6842571dba04d3ceff5beb34",
      "projectPage": "https://circleradon.github.io/EOCBench/",
      "githubRepo": "https://github.com/alibaba-damo-academy/EOCBench",
      "ai_summary": "EOC-Bench introduces a benchmark to evaluate dynamic object-centric cognition in egocentric vision applications, focusing on temporal and interactive aspects not covered by existing benchmarks.",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "egocentric vision",
        "embodied benchmarks",
        "object-centric embodied cognition",
        "QA pairs",
        "temporal accuracy metric"
      ]
    },
    "publishedAt": "2025-06-05T13:44:12.000Z",
    "title": "EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an\n  Egocentric World?",
    "summary": "The emergence of multimodal large language models (MLLMs) has driven\nbreakthroughs in egocentric vision applications. These applications necessitate\npersistent, context-aware understanding of objects, as users interact with\ntools in dynamic and cluttered environments. However, existing embodied\nbenchmarks primarily focus on static scene exploration, emphasizing object's\nappearance and spatial attributes while neglecting the assessment of dynamic\nchanges arising from users' interactions. To address this gap, we introduce\nEOC-Bench, an innovative benchmark designed to systematically evaluate\nobject-centric embodied cognition in dynamic egocentric scenarios. Specially,\nEOC-Bench features 3,277 meticulously annotated QA pairs categorized into three\ntemporal categories: Past, Present, and Future, covering 11 fine-grained\nevaluation dimensions and 3 visual object referencing types. To ensure thorough\nassessment, we develop a mixed-format human-in-the-loop annotation framework\nwith four types of questions and design a novel multi-scale temporal accuracy\nmetric for open-ended temporal evaluation. Based on EOC-Bench, we conduct\ncomprehensive evaluations of various proprietary, open-source, and object-level\nMLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object\ncognitive capabilities of MLLMs, establishing a robust foundation for\ndeveloping reliable core models for embodied systems.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/bFTB1kqlo-Oj0gQM7yzfe.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05287.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a3fe3dde901eb01df12398",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
      "fullname": "YuqianYuan",
      "name": "CircleRadon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02620",
      "authors": [
        {
          "_id": "68425ef33b5bb39c456487e0",
          "user": {
            "_id": "64049ae20ab5e22719f35103",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg",
            "isPro": false,
            "fullname": "Dongyu Yan",
            "user": "StarYDY",
            "type": "user"
          },
          "name": "Dongyu Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:56.223Z",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e1",
          "name": "Leyi Wu",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e2",
          "name": "Jiantao Lin",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e3",
          "name": "Luozhou Wang",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e4",
          "name": "Tianshuo Xu",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e5",
          "name": "Zhifei Chen",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e6",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e7",
          "name": "Lie Xu",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e8",
          "name": "Shunsi Zhang",
          "hidden": false
        },
        {
          "_id": "68425ef33b5bb39c456487e9",
          "user": {
            "_id": "655cba1d87b67834000590e8",
            "avatarUrl": "/avatars/3bd43b7c9351f65b8f38f4c8237a0146.svg",
            "isPro": false,
            "fullname": "Yingcong Chen",
            "user": "yingcongchen",
            "type": "user"
          },
          "name": "Yingcong Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T03:22:29.750Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T08:36:03.000Z",
      "submittedOnDailyAt": "2025-06-06T02:21:59.705Z",
      "title": "FlexPainter: Generación de texturas flexibles y con un alto grado de coherencia multidimensional",
      "submittedOnDailyBy": {
        "_id": "64049ae20ab5e22719f35103",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg",
        "isPro": false,
        "fullname": "Dongyu Yan",
        "user": "StarYDY",
        "type": "user"
      },
      "summary": "La generación de mapas texturales es una parte importante de la modelación 3D, ya que determina la calidad de renderización de los personajes. Recientemente, los métodos basados en difusión han abierto nuevas rutas en la generación textural. Sin embargo, la flexibilidad de control y las limitaciones de los modelos presentados impiden a los fabricantes de lograr los resultados deseados. Además, la discontinuidad entre las imágenes generadas afecta la calidad de la generación textural. Para resolver estos problemas, se presenta FlexPainter, una nueva pipila de generación textural. Esta pipila permite la utilización de guías multimodelo con condiciones flexibles y alcanza altas coincidencias. Se establece un espacio de mapeo con condiciones comunes y realiza una integración flexible entre diferentes modelos de entrada. Utilizando este espacio de mapeo, se propone un método basado en CFG basado en imágenes, donde se separan la información estructural y estilística para realizar la estilización basada en imágenes de referencia. Se utiliza el conocimiento 3D incluido en la difusión de imágenes para generar múltiples imágenes simultáneamente utilizando representaciones de red y fortalecer la comprensión global. Además, se propone un sincronización de tiempo y un módulo de pesos adaptativos durante el proceso de muestreo para garantizar la coincidencia local. Finalmente, se combina el modelo completo textural con conocimiento 3D y el modelo textural fortalecido para generar mapas texturales continuos y de alta resolución. Los experimentos detallados muestran que nuestro framework supera significativamente a los métodos más recientes en términos de flexibilidad y calidad de generación.",
      "upvotes": 8,
      "discussionId": "68425ef53b5bb39c4564888b",
      "projectPage": "https://starydy.xyz/FlexPainter/",
      "githubRepo": "https://github.com/StarRealMan/FlexPainter",
      "ai_summary": "FlexPainter, a novel texture generation pipeline, uses a shared conditional embedding space to enable flexible multi-modal guidance, ensuring high-quality and consistent texture map generation using image diffusion priors and a 3D-aware model.",
      "ai_keywords": [
        "diffusion-based methods",
        "texture generation",
        "flexible multi-modal conditional guidance",
        "conditional embedding space",
        "image-based CFG method",
        "structural information",
        "style information",
        "reference image-based stylization",
        "image diffusion prior",
        "grid representation",
        "view synchronization",
        "adaptive weighting module",
        "3D-aware texture completion model",
        "texture enhancement model"
      ]
    },
    "publishedAt": "2025-06-03T04:36:03.000Z",
    "title": "FlexPainter: Flexible and Multi-View Consistent Texture Generation",
    "summary": "Texture map production is an important part of 3D modeling and determines the\nrendering quality. Recently, diffusion-based methods have opened a new way for\ntexture generation. However, restricted control flexibility and limited prompt\nmodalities may prevent creators from producing desired results. Furthermore,\ninconsistencies between generated multi-view images often lead to poor texture\ngeneration quality. To address these issues, we introduce FlexPainter,\na novel texture generation pipeline that enables flexible multi-modal\nconditional guidance and achieves highly consistent texture generation. A\nshared conditional embedding space is constructed to perform flexible\naggregation between different input modalities. Utilizing such embedding space,\nwe present an image-based CFG method to decompose structural and style\ninformation, achieving reference image-based stylization. Leveraging the 3D\nknowledge within the image diffusion prior, we first generate multi-view images\nsimultaneously using a grid representation to enhance global understanding.\nMeanwhile, we propose a view synchronization and adaptive weighting module\nduring diffusion sampling to further ensure local consistency. Finally, a\n3D-aware texture completion model combined with a texture enhancement model is\nused to generate seamless, high-resolution texture maps. Comprehensive\nexperiments demonstrate that our framework significantly outperforms\nstate-of-the-art methods in both flexibility and generation quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02620.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64049ae20ab5e22719f35103",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg",
      "fullname": "Dongyu Yan",
      "name": "StarYDY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04209",
      "authors": [
        {
          "_id": "68413c8eb64ba498925da6a8",
          "user": {
            "_id": "65d45fbf9f087171b805c428",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d45fbf9f087171b805c428/bgxwcn2p_D9qEa5vynSxV.jpeg",
            "isPro": false,
            "fullname": "Jingfeng Yang",
            "user": "JingfengY",
            "type": "user"
          },
          "name": "Jingfeng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:26.842Z",
          "hidden": false
        },
        {
          "_id": "68413c8eb64ba498925da6a9",
          "user": {
            "_id": "64ea89932ca4ff1d53b77548",
            "avatarUrl": "/avatars/ce3df67ba3ea3197ebf74fbe5e2c0e48.svg",
            "isPro": false,
            "fullname": "Ziyang Wu",
            "user": "robinwuzy",
            "type": "user"
          },
          "name": "Ziyang Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:57.098Z",
          "hidden": false
        },
        {
          "_id": "68413c8eb64ba498925da6aa",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "68413c8eb64ba498925da6ab",
          "name": "Yi Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:51:56.000Z",
      "submittedOnDailyAt": "2025-06-06T00:43:44.611Z",
      "title": "Utilizamos un texto encoder fijo para correspondencia de lenguaje-video.",
      "submittedOnDailyBy": {
        "_id": "65d45fbf9f087171b805c428",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d45fbf9f087171b805c428/bgxwcn2p_D9qEa5vynSxV.jpeg",
        "isPro": false,
        "fullname": "Jingfeng Yang",
        "user": "JingfengY",
        "type": "user"
      },
      "summary": "La mejor manera de establecer la correspondencia entre lenguaje y imágenes es mediante el aprendizaje comparativo, como CLIP, que entrena juntos los encoders de texto y de imágenes. En este artículo, dudamos si es necesario un aprendizaje común costoso y revisamos si un modelo de lenguaje de tamaño fijo (LLM) puede guiar el aprendizaje de representaciones visuales. Concretamente, proponemos LIFT (Language-Image alignment with a Fixed Text encoder), que utiliza un encoder de texto fijo en un LLM para entrenar la correspondencia entre lenguaje y imágenes, entrenando solo el encoder de imágenes. Este enfoque muestra una eficiencia superior a CLIP en escenarios de comprensión estructural y largas capturas, con un gran aumento en eficiencia computacional. Este artículo comienza la investigación sistemática sobre cómo guiar el aprendizaje visual con un modelo de lenguaje, presentando opciones de diseño para el aprendizaje de representaciones visuales que se alinean con el lenguaje.",
      "upvotes": 7,
      "discussionId": "68413c8fb64ba498925da720",
      "projectPage": "https://jingfeng0705.github.io/LIFT/lift.html",
      "githubRepo": "https://github.com/Jingfeng0705/LIFT",
      "ai_summary": "Learning Language-Image alignment with a Fixed Text encoder (LIFT) using pre-trained large language models effectively guides visual representation learning, outperforming joint training methods like CLIP in compositional understanding and long captions.",
      "ai_keywords": [
        "contrastive learning",
        "CLIP",
        "pre-trained fixed large language model",
        "LLM",
        "Language-Image alignment",
        "LIFT",
        "image encoder",
        "compositional understanding",
        "long captions"
      ]
    },
    "publishedAt": "2025-06-04T13:51:56.000Z",
    "title": "Language-Image Alignment with Fixed Text Encoders",
    "summary": "Currently, the most dominant approach to establishing language-image\nalignment is to pre-train text and image encoders jointly through contrastive\nlearning, such as CLIP and its variants. In this work, we question whether such\na costly joint training is necessary. In particular, we investigate if a\npre-trained fixed large language model (LLM) offers a good enough text encoder\nto guide visual representation learning. That is, we propose to learn\nLanguage-Image alignment with a Fixed Text encoder (LIFT) from an LLM by\ntraining only the image encoder. Somewhat surprisingly, through comprehensive\nbenchmarking and ablation studies, we find that this much simplified framework\nLIFT is highly effective and it outperforms CLIP in most scenarios that involve\ncompositional understanding and long captions, while achieving considerable\ngains in computational efficiency. Our work takes a first step towards\nsystematically exploring how text embeddings from LLMs can guide visual\nlearning and suggests an alternative design choice for learning\nlanguage-aligned visual representations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04209.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65d45fbf9f087171b805c428",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d45fbf9f087171b805c428/bgxwcn2p_D9qEa5vynSxV.jpeg",
      "fullname": "Jingfeng Yang",
      "name": "JingfengY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01011",
      "authors": [
        {
          "_id": "6842746e8edd398d01b68e03",
          "name": "Siqi Hui",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e04",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e05",
          "name": "Sanping Zhou",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e06",
          "name": "Ye Deng",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e07",
          "name": "Wenli Huang",
          "hidden": false
        },
        {
          "_id": "6842746e8edd398d01b68e08",
          "name": "Jinjun Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T13:44:20.000Z",
      "submittedOnDailyAt": "2025-06-06T03:26:27.710Z",
      "title": "El método de marcar imágenes basado en la orientación lingüística para enfrentar ataques de reproducción automática: el enfoque",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "Los modelos de generación de imágenes autónomas (AR) han recibido atención debido al desarrollo de imágenes sintéticas. Esto ha aumentado la necesidad de proteger con etiquetas de autoría y ha llevado a la necesidad de tecnologías de etiquetas fuertes. Sin embargo, las tecnologías de etiquetas en el proceso de generación actual son principalmente diseñadas para modelos de difusión, donde las etiquetas se esconden en el estado potencial de difusión. Este diseño plantea grandes problemas para la aplicación directa en modelos AR. Además, las ofensivas de recuperación basadas en difusión pueden eliminar eficazmente las etiquetas a través de cambios en el estado potencial. Para abordar estos problemas, proponemos el Lexical Bias Watermarking (LBW). LBW está diseñado para modelos AR y tiene como objetivo resistir ofensivas de recuperación. LBW esconde etiquetas directamente en la asignación de tokens al aplicar una bias en la selección de tokens durante la generación, utilizando una lista de palabras verdes. Este enfoque garantiza una adaptación sin restricciones con los modelos AR actuales y permite la aplicación natural de técnicas de etiquetas post-proceso. Para mejorar la seguridad frente a ofensivas de recuperación basadas en difusión, LBW no utiliza la lista de palabras verdes, sino que samplingea de manera aleatoria desde el conjunto completo de palabras verdes para cada imagen. La detección de etiquetas se realiza a través de la cuantificación de la distribución de tokens y análisis estadístico. Experimentos extensos muestran que LBW es especialmente efectiva en resistir ofensivas de recuperación y demuestra la robustez de las etiquetas.",
      "upvotes": 7,
      "discussionId": "6842747b8edd398d01b69110",
      "ai_summary": "A novel watermarking technique, Lexical Bias Watermarking, enhances the security of autoregressive image generation models by embedding watermarks into token selection, demonstrating superior resistance to regeneration attacks.",
      "ai_keywords": [
        "autoregressive models",
        "in-generation watermarking",
        "diffusion models",
        "diffusion latent states",
        "token prediction",
        "regeneration attacks",
        "Lexical Bias Watermarking",
        "token maps",
        "green list",
        "watermark detection",
        "quantization",
        "statistical analysis",
        "token distribution"
      ]
    },
    "publishedAt": "2025-06-01T09:44:20.000Z",
    "title": "Autoregressive Images Watermarking through Lexical Biasing: An Approach\n  Resistant to Regeneration Attack",
    "summary": "Autoregressive (AR) image generation models have gained increasing attention\nfor their breakthroughs in synthesis quality, highlighting the need for robust\nwatermarking to prevent misuse. However, existing in-generation watermarking\ntechniques are primarily designed for diffusion models, where watermarks are\nembedded within diffusion latent states. This design poses significant\nchallenges for direct adaptation to AR models, which generate images\nsequentially through token prediction. Moreover, diffusion-based regeneration\nattacks can effectively erase such watermarks by perturbing diffusion latent\nstates. To address these challenges, we propose Lexical Bias Watermarking\n(LBW), a novel framework designed for AR models that resists regeneration\nattacks. LBW embeds watermarks directly into token maps by biasing token\nselection toward a predefined green list during generation. This approach\nensures seamless integration with existing AR models and extends naturally to\npost-hoc watermarking. To increase the security against white-box attacks,\ninstead of using a single green list, the green list for each image is randomly\nsampled from a pool of green lists. Watermark detection is performed via\nquantization and statistical analysis of the token distribution. Extensive\nexperiments demonstrate that LBW achieves superior watermark robustness,\nparticularly in resisting regeneration attacks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01011.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05209",
      "authors": [
        {
          "_id": "684247f35d537e0e5ecb724b",
          "name": "Nikhil Kandpal",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb724c",
          "name": "Brian Lester",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb724d",
          "name": "Colin Raffel",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb724e",
          "user": {
            "_id": "636071759ddc44e710e0f5ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636071759ddc44e710e0f5ce/-gmEhY5PidmSXIQPi2-QB.jpeg",
            "isPro": true,
            "fullname": "Sebastian Majstorovic",
            "user": "storytracer",
            "type": "user"
          },
          "name": "Sebastian Majstorovic",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:37.270Z",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb724f",
          "name": "Stella Biderman",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7250",
          "name": "Baber Abbasi",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7251",
          "name": "Luca Soldaini",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7252",
          "name": "Enrico Shippole",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7253",
          "name": "A. Feder Cooper",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7254",
          "name": "Aviya Skowron",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7255",
          "name": "John Kirchenbauer",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7256",
          "name": "Shayne Longpre",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7257",
          "name": "Lintang Sutawika",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7258",
          "name": "Alon Albalak",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7259",
          "name": "Zhenlin Xu",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725a",
          "name": "Guilherme Penedo",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725b",
          "name": "Loubna Ben Allal",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725c",
          "name": "Elie Bakouch",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725d",
          "name": "John David Pressman",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725e",
          "name": "Honglu Fan",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb725f",
          "name": "Dashiell Stander",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7260",
          "name": "Guangyu Song",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7261",
          "name": "Aaron Gokaslan",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7262",
          "name": "Tom Goldstein",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7263",
          "name": "Brian R. Bartoldson",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7264",
          "name": "Bhavya Kailkhura",
          "hidden": false
        },
        {
          "_id": "684247f35d537e0e5ecb7265",
          "name": "Tyler Murray",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T16:21:30.000Z",
      "submittedOnDailyAt": "2025-06-06T05:47:06.933Z",
      "title": "「Conjunto de datos de 8TB del texto del dominio público y licencia de uso abierto Common Pile v0.1」",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) se entrenan generalmente con una gran cantidad de texto sin licencia, lo que ha llevado a investigaciones sobre la posibilidad de violaciones de derechos de autor y preocupaciones éticas. Entrenar LLMs con texto de licencia abierta es el primer paso para resolver estos problemas, y para generar un conjunto de datos de pequeño tamaño o baja calidad que permita crear un LLM de alta calidad, se han realizado esfuerzos previos en la recopilación de datos. Para complementar esto, se recopila, prepara y lanza una colección de texto de licencia abierta llamada Common Pile v0.1. Common Pile está compuesto de 30 fuentes de diferentes áreas, como artículos de investigación, código, libros, ingeniería, materiales educativos y traducciones de voz. Es importante destacar que se entrenan dos modelos de LLM de 700 millones de parámetros con el texto de Common Pile, llamándolos Comma v0.1-1T y Comma v0.1-2T, con 1 y 2 trillones de tokens respectivamente. Ambos modelos alcanzaron un rendimiento relativamente bueno en entornos de cálculo equivalentes a los entrenados con texto sin licencia. En lugar de solo lanzar la versión propia de Common Pile v0.1, se lanzan también el código utilizado en su creación y los objetivos de entrenamiento y puntos de control de los modelos Comma v0.1.",
      "upvotes": 6,
      "discussionId": "684247f85d537e0e5ecb73d3",
      "projectPage": "https://huggingface.co/common-pile",
      "githubRepo": "https://github.com/r-three/common-pile",
      "ai_summary": "The Common Pile v0.1 dataset, an 8-terabyte collection of openly licensed text, is used to train competitive 7 billion parameter LLMs.",
      "ai_keywords": [
        "Large language models",
        "LLMs",
        "openly licensed text",
        "Common Pile v0.1",
        "parameter-efficient fine-tuning",
        "Llama 1 and 2 7B",
        "training mixture",
        "checkpoints"
      ]
    },
    "publishedAt": "2025-06-05T12:21:30.000Z",
    "title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text",
    "summary": "Large language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. Training LLMs on\nopenly licensed text presents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performant LLMs. To address this gap, we collect, curate, and release\nthe Common Pile v0.1, an eight terabyte collection of openly licensed text\ndesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameter LLMs on text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance to LLMs trained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing the Common Pile v0.1 itself, we also release the\ncode used in its creation as well as the training mixture and checkpoints for\nthe Comma v0.1 models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05209.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2725
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04405",
      "authors": [
        {
          "_id": "6842454fbdc448822b2f1c03",
          "name": "Ran Xu",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c04",
          "name": "Yuchen Zhuang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c05",
          "name": "Yishan Zhong",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c06",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c07",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c08",
          "name": "Hang Wu",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c09",
          "name": "May D. Wang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0a",
          "name": "Peifeng Ruan",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0b",
          "name": "Donghan Yang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0c",
          "name": "Tao Wang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0d",
          "name": "Guanghua Xiao",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0e",
          "name": "Carl Yang",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c0f",
          "name": "Yang Xie",
          "hidden": false
        },
        {
          "_id": "6842454fbdc448822b2f1c10",
          "user": {
            "_id": "65cae89119683f9817c049ea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg",
            "isPro": false,
            "fullname": "Wenqi Shi",
            "user": "wshi83",
            "type": "user"
          },
          "name": "Wenqi Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:39.845Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T19:38:55.000Z",
      "submittedOnDailyAt": "2025-06-06T00:21:54.285Z",
      "title": "MedAgentGym: MedAgentGym es un plataforma para la entrenamiento de agentes de lenguaje de modelo grande (LLM) para resolver problemas de lógica médica basados en código a gran escala.",
      "submittedOnDailyBy": {
        "_id": "65cae89119683f9817c049ea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg",
        "isPro": false,
        "fullname": "Wenqi Shi",
        "user": "wshi83",
        "type": "user"
      },
      "summary": "MedAgentGYM es el primer entorno de aprendizaje publicado. Este entorno está diseñado para fortalecer la base de planificación médica de agentes de modelos de lenguaje grandes (LLM). MedAgentGYM incluye 72,413 instancias de tareas y se organiza en 129 categorías, que se pueden encontrar en escenarios médicos biotecnológicos reales. Las tareas se incluyen en un entorno de codificación ejecutable, con una descripción detallada de las tareas, estructuras de retroalimentación interactiva, notas reales verificables y herramientas de desarrollo de proyectos de aprendizaje escalables. Se ha confirmado una diferencia clara en el rendimiento entre modelos basados en API de negocios y contribuidores de código abierto, a través de la validación de más de 30 LLM. Al utilizar MedAgentGYM, Med-Copilot-7B obtiene un gran mejoramiento en rendimiento a través del aprendizaje supervisado (+36.44%) y el aprendizaje de refuerzo continuo (+42.47%), y se presenta como una opción de precio competitivo con gpt-4o, en el contexto de protección de datos personales. MedAgentGYM proporciona un entorno de ejecución unificado, con marcos de referencia detallados y herramientas de aprendizaje accesibles por categoría, y se ofrece como un plataforma integrada para el desarrollo de programas de asistencia de codificación basados en LLM.",
      "upvotes": 4,
      "discussionId": "68424552bdc448822b2f1cd0",
      "githubRepo": "https://github.com/wshi83/MedAgentGym",
      "ai_summary": "MedAgentGYM, a training environment for coding-based medical reasoning in LLMs, enhances performance through supervised fine-tuning and reinforcement learning, providing a benchmark and expandable resource.",
      "ai_keywords": [
        "large language model",
        "MedAgentGYM",
        "task instances",
        "biomedical scenarios",
        "coding environments",
        "task descriptions",
        "interactive feedback",
        "ground-truth annotations",
        "training trajectories",
        "LLMs",
        "supervised fine-tuning",
        "reinforcement learning",
        "Med-Copilot-7B",
        "gpt-4o",
        "coding assistants",
        "biomedical research"
      ]
    },
    "publishedAt": "2025-06-04T15:38:55.000Z",
    "title": "MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at\n  Scale",
    "summary": "We introduce MedAgentGYM, the first publicly available training environment\ndesigned to enhance coding-based medical reasoning capabilities in large\nlanguage model (LLM) agents. MedAgentGYM comprises 72,413 task instances across\n129 categories derived from authentic real-world biomedical scenarios. Tasks\nare encapsulated within executable coding environments, each featuring detailed\ntask descriptions, interactive feedback mechanisms, verifiable ground-truth\nannotations, and scalable training trajectory generation. Extensive\nbenchmarking of over 30 LLMs reveals a notable performance disparity between\ncommercial API-based models and open-source counterparts. Leveraging\nMedAgentGYM, Med-Copilot-7B achieves substantial performance gains through\nsupervised fine-tuning (+36.44%) and continued reinforcement learning\n(+42.47%), emerging as an affordable and privacy-preserving alternative\ncompetitive with gpt-4o. By offering both a comprehensive benchmark and\naccessible, expandable training resources within unified execution\nenvironments, MedAgentGYM delivers an integrated platform to develop LLM-based\ncoding assistants for advanced biomedical research and practice.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04405.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cae89119683f9817c049ea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg",
      "fullname": "Wenqi Shi",
      "name": "wshi83",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20914",
      "authors": [
        {
          "_id": "68425a585738dda052ea4c91",
          "name": "Jianman Lin",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c92",
          "name": "Haojie Li",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c93",
          "name": "Chunmei Qing",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c94",
          "name": "Zhijing Yang",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c95",
          "name": "Liang Lin",
          "hidden": false
        },
        {
          "_id": "68425a585738dda052ea4c96",
          "name": "Tianshui Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T09:05:28.000Z",
      "submittedOnDailyAt": "2025-06-06T01:33:43.629Z",
      "title": "Geometría variable y combinación de objetos para mantener la superficie exterior",
      "submittedOnDailyBy": {
        "_id": "6332e2689bf698ce68a22e8c",
        "avatarUrl": "/avatars/c1922acfda2e6d2fe7b03194a404eb10.svg",
        "isPro": false,
        "fullname": "JIANTAO LIN",
        "user": "LTT",
        "type": "user"
      },
      "summary": "La Construcción de Objetos Generales (GOC) tiene como objetivo la integración sin fronteras de los objetos en el espacio de fondo, manteniendo las características geométricas seleccionadas mientras preserva la información detallada de su apariencia. Los métodos recientes construyen codificaciones semánticas que se integran en modelos de descripción avanzados para permitir la generación geométricamente orientada. Sin embargo, estas altas codificaciones solo explican las categorías semánticas superiores y descartan la información detallada de la apariencia. Presentamos un modelo de descripción geométrica diseñado para conservar tanto la estructura geométrica como la apariencia (DGAD). Inicialmente, utiliza codificaciones semánticas para capturar de manera oculta las transformaciones geométricas seleccionadas, y luego utiliza una estructura de búsqueda de atención cruzada para ajustar las características detalladas de la apariencia con una representación orientada hacia la dirección geométrica, permitiendo una generación geométricamente precisa y la preservación de la apariencia. En particular, DGAD se basa en la red de entrada de CLIP/DINO, extrayendo codificaciones semánticas y representaciones de apariencia que se integran de manera separada y sin fronteras en la pipeline de codificación y decodificación. Primero, integra una codificación semántica en un modelo de descripción pre-entrenado con una fuerte capacidad de reconocimiento espacial, permitiendo la manipulación de objetos de manera flexible y asegurando la posibilidad de edición efectiva. Luego, utiliza la entrada del objeto entrenada para aprender de manera oculta la orientación geométrica, diseñando una estructura de atención cruzada densa para asegurar la consistencia de la apariencia. Los experimentos ampliados en marcos de prueba publicos demuestran la efectividad del marco de trabajo propuesto de DGAD.",
      "upvotes": 4,
      "discussionId": "68425a595738dda052ea4ce4",
      "githubRepo": "https://github.com/jianmanlincjx/DGAD",
      "ai_summary": "The Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model effectively integrates target objects into background scenes by using semantic embeddings for geometry and cross-attention for appearance alignment.",
      "ai_keywords": [
        "disentangled geometry-editable",
        "appearance-preserving diffusion",
        "diffusion models",
        "cross-attention retrieval",
        "CLIP/DINO",
        "reference networks",
        "semantic embeddings",
        "appearance-preserving representations",
        "flexible object manipulation",
        "spatial reasoning capabilities",
        "dense cross-attention mechanism",
        "public benchmarks"
      ]
    },
    "publishedAt": "2025-05-27T05:05:28.000Z",
    "title": "Geometry-Editable and Appearance-Preserving Object Compositon",
    "summary": "General object composition (GOC) aims to seamlessly integrate a target object\ninto a background scene with desired geometric properties, while simultaneously\npreserving its fine-grained appearance details. Recent approaches derive\nsemantic embeddings and integrate them into advanced diffusion models to enable\ngeometry-editable generation. However, these highly compact embeddings encode\nonly high-level semantic cues and inevitably discard fine-grained appearance\ndetails. We introduce a Disentangled Geometry-editable and\nAppearance-preserving Diffusion (DGAD) model that first leverages semantic\nembeddings to implicitly capture the desired geometric transformations and then\nemploys a cross-attention retrieval mechanism to align fine-grained appearance\nfeatures with the geometry-edited representation, facilitating both precise\ngeometry editing and faithful appearance preservation in object composition.\nSpecifically, DGAD builds on CLIP/DINO-derived and reference networks to\nextract semantic embeddings and appearance-preserving representations, which\nare then seamlessly integrated into the encoding and decoding pipelines in a\ndisentangled manner. We first integrate the semantic embeddings into\npre-trained diffusion models that exhibit strong spatial reasoning capabilities\nto implicitly capture object geometry, thereby facilitating flexible object\nmanipulation and ensuring effective editability. Then, we design a dense\ncross-attention mechanism that leverages the implicitly learned object geometry\nto retrieve and spatially align appearance features with their corresponding\nregions, ensuring faithful appearance consistency. Extensive experiments on\npublic benchmarks demonstrate the effectiveness of the proposed DGAD framework.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20914.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6332e2689bf698ce68a22e8c",
      "avatarUrl": "/avatars/c1922acfda2e6d2fe7b03194a404eb10.svg",
      "fullname": "JIANTAO LIN",
      "name": "LTT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05348",
      "authors": [
        {
          "_id": "6842a994497e2b62234145d7",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145d8",
          "name": "Peishan Yang",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145d9",
          "name": "Zhen Xu",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145da",
          "name": "Jiaming Sun",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145db",
          "name": "Zhanhua Zhang",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145dc",
          "name": "Yong Chen",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145dd",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145de",
          "name": "Sida Peng",
          "hidden": false
        },
        {
          "_id": "6842a994497e2b62234145df",
          "name": "Xiaowei Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:59:57.000Z",
      "submittedOnDailyAt": "2025-06-06T07:18:31.313Z",
      "title": "Solución que proporciona acceso a camaleones libres en cualquier lugar, adaptable a la reestructuración dinámica del horario.",
      "submittedOnDailyBy": {
        "_id": "6768fc1b75d8e8d042d26732",
        "avatarUrl": "/avatars/6e8c8b26effa41ba4073e69857b0c80a.svg",
        "isPro": false,
        "fullname": "Yifan Wang",
        "user": "wyf2020",
        "type": "user"
      },
      "summary": "Este artículo aborda el problema de la reconstrucción de pantallas 3D dinámicas que presentan movimientos complejos. En los últimos estudios, se han definido características gaussianas en el espacio estándar y se han usado campos de deformación para mapear estas características de manera real-time desde el espacio estándar a la observación, logrando una síntesis visual dinámica. Sin embargo, estos métodos sufren de dificultades en la optimización de los campos de deformación y en el manejo de pantallas con movimientos complejos. Para superar estos desafíos, proponemos FreeTimeGS. Este es un nuevo tipo de representación 4D que permite que las características gaussianas se manifiesten en cualquier momento y posición. Comparado con las representaciones gaussianas estándar, esta representación tiene una gran flexibilidad, lo que mejora su capacidad para modelar pantallas 3D dinámicas. Además, se asignan funciones de acción a cada característica gaussiana y permite que se muevan hacia áreas adyacentes a lo largo del tiempo. A través de experimentos en varios conjuntos de datos, se demuestra que la calidad de renderizado de nuestro método supera significativamente a los métodos recientes.",
      "upvotes": 3,
      "discussionId": "6842a996497e2b622341467e",
      "projectPage": "https://zju3dv.github.io/freetimegs/",
      "ai_summary": "A novel 4D representation, FreeTimeGS, enhances the modeling of dynamic 3D scenes by enabling Gaussian primitives to appear at arbitrary times and locations, improving rendering quality compared to existing methods.",
      "ai_keywords": [
        "3D Gaussian primitives",
        "canonical space",
        "deformation fields",
        "real-time dynamic view synthesis",
        "4D representation",
        "motion function",
        "temporal redundancy"
      ]
    },
    "publishedAt": "2025-06-05T13:59:57.000Z",
    "title": "FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene\n  Reconstruction",
    "summary": "This paper addresses the challenge of reconstructing dynamic 3D scenes with\ncomplex motions. Some recent works define 3D Gaussian primitives in the\ncanonical space and use deformation fields to map canonical primitives to\nobservation spaces, achieving real-time dynamic view synthesis. However, these\nmethods often struggle to handle scenes with complex motions due to the\ndifficulty of optimizing deformation fields. To overcome this problem, we\npropose FreeTimeGS, a novel 4D representation that allows Gaussian primitives\nto appear at arbitrary time and locations. In contrast to canonical Gaussian\nprimitives, our representation possesses the strong flexibility, thus improving\nthe ability to model dynamic 3D scenes. In addition, we endow each Gaussian\nprimitive with an motion function, allowing it to move to neighboring regions\nover time, which reduces the temporal redundancy. Experiments results on\nseveral datasets show that the rendering quality of our method outperforms\nrecent methods by a large margin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05348.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6768fc1b75d8e8d042d26732",
      "avatarUrl": "/avatars/6e8c8b26effa41ba4073e69857b0c80a.svg",
      "fullname": "Yifan Wang",
      "name": "wyf2020",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05282",
      "authors": [
        {
          "_id": "68425fce548d527097ac00bb",
          "name": "Tao Sun",
          "hidden": false
        },
        {
          "_id": "68425fce548d527097ac00bc",
          "name": "Liyuan Zhu",
          "hidden": false
        },
        {
          "_id": "68425fce548d527097ac00bd",
          "name": "Shengyu Huang",
          "hidden": false
        },
        {
          "_id": "68425fce548d527097ac00be",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "68425fce548d527097ac00bf",
          "name": "Iro Armeni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:36:03.000Z",
      "submittedOnDailyAt": "2025-06-06T01:56:27.167Z",
      "title": "Rectified Point Flow: Método de Estimación de Posición de Puntos en Datos Generales de Puntos",
      "submittedOnDailyBy": {
        "_id": "6503916e0905dd866fd129cb",
        "avatarUrl": "/avatars/818716460d9c9aaa056e0f1b43816c6a.svg",
        "isPro": false,
        "fullname": "Liyuan Zhu",
        "user": "liyzzz",
        "type": "user"
      },
      "summary": "Introduzco Rectified Point Flow. Es una parametrización que integra la registración de nubes de puntos parejas y la asambla de formas multi-parte en un solo problema condicional generativo. Al recibir un conjunto de puntos sin dirección, nuestro método aprende velocidades puntuales continuas para transportar puntos con ruido a su ubicación objetivo, lo que permite recuperar la orientación de los partes. En comparación con los estudios previos, nuestro método utiliza un tratamiento de simetría generalizado en lugar de hacerlo, aprende simetría internamente y no requiere etiquetas de simetría. Combinado con un encoder automático, nuestro método alcanza nuevos niveles de rendimiento en seis marcos de referencia. En particular, la formulación uniforme permite entrenamiento paralelo efectivo con diferentes conjuntos de datos y aprende propagación geométrica común para mejorar la precisión. La página del proyecto está disponible en https://rectified-pointflow.github.io/.",
      "upvotes": 3,
      "discussionId": "68425fcf548d527097ac011c",
      "projectPage": "https://rectified-pointflow.github.io/",
      "githubRepo": "https://github.com/GradientSpaces/Rectified-Point-Flow",
      "ai_summary": "Rectified Point Flow unifies pairwise point cloud registration and multi-part shape assembly through a continuous point-wise velocity field, achieving state-of-the-art performance on various benchmarks.",
      "ai_keywords": [
        "Rectified Point Flow",
        "pairwise point cloud registration",
        "multi-part shape assembly",
        "continuous point-wise velocity field",
        "self-supervised encoder",
        "overlapping points",
        "geometric priors"
      ]
    },
    "publishedAt": "2025-06-05T13:36:03.000Z",
    "title": "Rectified Point Flow: Generic Point Cloud Pose Estimation",
    "summary": "We introduce Rectified Point Flow, a unified parameterization that formulates\npairwise point cloud registration and multi-part shape assembly as a single\nconditional generative problem. Given unposed point clouds, our method learns a\ncontinuous point-wise velocity field that transports noisy points toward their\ntarget positions, from which part poses are recovered. In contrast to prior\nwork that regresses part-wise poses with ad-hoc symmetry handling, our method\nintrinsically learns assembly symmetries without symmetry labels. Together with\na self-supervised encoder focused on overlapping points, our method achieves a\nnew state-of-the-art performance on six benchmarks spanning pairwise\nregistration and shape assembly. Notably, our unified formulation enables\neffective joint training on diverse datasets, facilitating the learning of\nshared geometric priors and consequently boosting accuracy. Project page:\nhttps://rectified-pointflow.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05282.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6503916e0905dd866fd129cb",
      "avatarUrl": "/avatars/818716460d9c9aaa056e0f1b43816c6a.svg",
      "fullname": "Liyuan Zhu",
      "name": "liyzzz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00830",
      "authors": [
        {
          "_id": "684264d1a9584289f0053f5c",
          "name": "Zhengcong Fei",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f5d",
          "name": "Hao Jiang",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f5e",
          "user": {
            "_id": "65bef422fdb8d33cefeaccc3",
            "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
            "isPro": false,
            "fullname": "Qiu Di",
            "user": "diqiu7",
            "type": "user"
          },
          "name": "Di Qiu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:29.181Z",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f5f",
          "name": "Baoxuan Gu",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f60",
          "name": "Youqiang Zhang",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f61",
          "name": "Jiahua Wang",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f62",
          "name": "Jialin Bai",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f63",
          "name": "Debang Li",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f64",
          "name": "Mingyuan Fan",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f65",
          "name": "Guibin Chen",
          "hidden": false
        },
        {
          "_id": "684264d1a9584289f0053f66",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T04:27:13.000Z",
      "submittedOnDailyAt": "2025-06-06T02:21:50.161Z",
      "title": "SkyReels-Audio: Tablas de audio de películas representadas según las condiciones de audio en función de los traductores de tablas de Overlay Eco-echo Driving Transformers",
      "submittedOnDailyBy": {
        "_id": "65bef422fdb8d33cefeaccc3",
        "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
        "isPro": false,
        "fullname": "Qiu Di",
        "user": "diqiu7",
        "type": "user"
      },
      "summary": "En la generación y editado de resúmenes de pantalla con condiciones de audio, no se ha encontrado aún un método que se basa en diversos tipos de entrada como texto, imágenes y vídeos para proporcionar orientación. En este artículo, proponemos un marco continuo de SkyReels-Audio para la síntesis de videos de resúmenes de pantalla de alta calidad y secuencialmente consistentes. Este marco se basa en un transformador de diferenciadores preentrenadidos, permitiendo la generación y edición de vídeos de longitud infinita y la aplicación de diversas condiciones. Además, también permite un control detallado en secuencias largas. Para lograr esto, utilizamos una estrategia de aprendizaje por refuerzo que ajusta gradualmente el movimiento de los frames y la sincronización del audio. Además, introducimos la pérdida de máscara de frame y el marco de guia audio para aumentar la coherencia local de los frames. Además, utilizamos un enfoque de ruido reducción con ventanas deslizantes para fusionar las representaciones potenciales entre secuencias, asegurando la calidad visual y la coherencia de las secuencias en largo plazo y para diferentes identificadores. Un punto importante es la construcción de una pipeline de datos que incluye altas calidades de audio, video y texto sincronizados. Según evaluaciones detalladas, SkyReels-Audio muestra excelentes resultados, especialmente en condiciones complejas, en términos de precisión de sincronización de labios, coherencia de los identificadores y realismo de las imágenes.",
      "upvotes": 3,
      "discussionId": "684264d2a9584289f0053fc9",
      "ai_summary": "SkyReels-Audio is a unified framework using pretrained video diffusion transformers for generating high-fidelity and coherent audio-conditioned talking portrait videos, supported by a hybrid curriculum learning strategy and advanced loss mechanisms.",
      "ai_keywords": [
        "video diffusion transformers",
        "infinite-length generation",
        "multimodal inputs",
        "hybrid curriculum learning",
        "facial mask loss",
        "classifier-free guidance mechanism",
        "sliding-window denoising",
        "lip-sync accuracy",
        "identity consistency",
        "realistic facial dynamics"
      ]
    },
    "publishedAt": "2025-06-01T00:27:13.000Z",
    "title": "SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video\n  Diffusion Transformers",
    "summary": "The generation and editing of audio-conditioned talking portraits guided by\nmultimodal inputs, including text, images, and videos, remains under explored.\nIn this paper, we present SkyReels-Audio, a unified framework for synthesizing\nhigh-fidelity and temporally coherent talking portrait videos. Built upon\npretrained video diffusion transformers, our framework supports infinite-length\ngeneration and editing, while enabling diverse and controllable conditioning\nthrough multimodal inputs. We employ a hybrid curriculum learning strategy to\nprogressively align audio with facial motion, enabling fine-grained multimodal\ncontrol over long video sequences. To enhance local facial coherence, we\nintroduce a facial mask loss and an audio-guided classifier-free guidance\nmechanism. A sliding-window denoising approach further fuses latent\nrepresentations across temporal segments, ensuring visual fidelity and temporal\nconsistency across extended durations and diverse identities. More importantly,\nwe construct a dedicated data pipeline for curating high-quality triplets\nconsisting of synchronized audio, video, and textual descriptions.\nComprehensive benchmark evaluations show that SkyReels-Audio achieves superior\nperformance in lip-sync accuracy, identity consistency, and realistic facial\ndynamics, particularly under complex and challenging conditions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00830.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65bef422fdb8d33cefeaccc3",
      "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
      "fullname": "Qiu Di",
      "name": "diqiu7",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04245",
      "authors": [
        {
          "_id": "68425054feb46a093178003f",
          "user": {
            "_id": "64ff4b1a0e8369f6a8c47c7e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ff4b1a0e8369f6a8c47c7e/X8ocOagj89gexSa253q_8.png",
            "isPro": false,
            "fullname": "Eric Lan",
            "user": "Eric-Lan",
            "type": "user"
          },
          "name": "Guangchen Lan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-06T02:20:41.949Z",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780040",
          "name": "Huseyin A. Inan",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780041",
          "user": {
            "_id": "65e88cdd95a27dfbf6b4e63b",
            "avatarUrl": "/avatars/3d2d270398f0824b392f99e158e94f26.svg",
            "isPro": false,
            "fullname": "Sahar Abdelnabi",
            "user": "sahar-abdelnabi",
            "type": "user"
          },
          "name": "Sahar Abdelnabi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T02:20:06.391Z",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780042",
          "name": "Janardhan Kulkarni",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780043",
          "user": {
            "_id": "6380a37a5c62156ce7dff8b9",
            "avatarUrl": "/avatars/fbe5a20869cb55ec43759c1b5f9c4135.svg",
            "isPro": false,
            "fullname": "Lukas Wutschitz",
            "user": "wulu",
            "type": "user"
          },
          "name": "Lukas Wutschitz",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-06T09:45:54.243Z",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780044",
          "name": "Reza Shokri",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780045",
          "name": "Christopher G. Brinton",
          "hidden": false
        },
        {
          "_id": "68425054feb46a0931780046",
          "name": "Robert Sim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T21:26:21.000Z",
      "submittedOnDailyAt": "2025-06-06T00:52:31.028Z",
      "title": "La teoría lógica y el aprendizaje por refuerzo en torno a los problemas de consistencia en los campos de contexto de los LLM",
      "submittedOnDailyBy": {
        "_id": "64ff4b1a0e8369f6a8c47c7e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ff4b1a0e8369f6a8c47c7e/X8ocOagj89gexSa253q_8.png",
        "isPro": false,
        "fullname": "Eric Lan",
        "user": "Eric-Lan",
        "type": "user"
      },
      "summary": "El siglo de los autónomos agentes que toman decisiones en nombre de los usuarios ha comenzado, y uno de los problemas centrales de esta área es la garantía de la Integridad Contextual (CI). La CI evalúa si un agente puede compartir información adecuada para realizar una tarea específica. Argumentamos que para lograr la CI, es necesario que los agentes proporcionen razones sobre la información contextual que están procesando. Para verificar esto, primero intentamos inducir a los modelos de lenguaje de alto nivel (LLMs) a proporcionar razones relacionadas con la CI. Luego, extendemos este enfoque para desarrollar un marco de aprendizaje por refuerzo (RL) y trabajamos para proporcionar razones más profundas a los modelos. Usando conjuntos de datos sintéticos y automáticamente generados (por ejemplo, con 700 datos que incluyen diferentes contextos y reglas de divulgación de información), demostramos que nuestro método mantiene el rendimiento en diferentes tamaños y familias de modelos, mientras reducir significativamente la divulgación de información inapropiada. Un punto importante es que los mejoramientos observados en este conjunto de datos sintético también afectan a los evaluadores de errores de privacidad en los benchmarks existentes (por ejemplo, PrivacyLens), que incluyen anotaciones humanas, y también a la evaluación de errores de privacidad en las acciones de programas de asistencia AI y en las llamadas a herramientas.",
      "upvotes": 3,
      "discussionId": "68425056feb46a09317800d9",
      "ai_summary": "A reinforcement learning framework for LLMs enhances contextual integrity by reducing inappropriate information disclosure and maintaining task performance across various benchmarks.",
      "ai_keywords": [
        "LLMs",
        "reinforcement learning",
        "contextual integrity",
        "information disclosure",
        "synthetic dataset",
        "PrivacyLens",
        "privacy leakage",
        "AI assistants"
      ]
    },
    "publishedAt": "2025-05-29T17:26:21.000Z",
    "title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning",
    "summary": "As the era of autonomous agents making decisions on behalf of users unfolds,\nensuring contextual integrity (CI) -- what is the appropriate information to\nshare while carrying out a certain task -- becomes a central question to the\nfield. We posit that CI demands a form of reasoning where the agent needs to\nreason about the context in which it is operating. To test this, we first\nprompt LLMs to reason explicitly about CI when deciding what information to\ndisclose. We then extend this approach by developing a reinforcement learning\n(RL) framework that further instills in models the reasoning necessary to\nachieve CI. Using a synthetic, automatically created, dataset of only sim700\nexamples but with diverse contexts and information disclosure norms, we show\nthat our method substantially reduces inappropriate information disclosure\nwhile maintaining task performance across multiple model sizes and families.\nImportantly, improvements transfer from this synthetic dataset to established\nCI benchmarks such as PrivacyLens that has human annotations and evaluates\nprivacy leakage of AI assistants in actions and tool calls.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04245.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ff4b1a0e8369f6a8c47c7e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ff4b1a0e8369f6a8c47c7e/X8ocOagj89gexSa253q_8.png",
      "fullname": "Eric Lan",
      "name": "Eric-Lan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05278",
      "authors": [
        {
          "_id": "68426dfeb5f4d2d0f8fd098e",
          "user": {
            "_id": "60adfff0306d6873ec42d545",
            "avatarUrl": "/avatars/4a63f90638dbffebfeeee181a6d0220c.svg",
            "isPro": false,
            "fullname": "Nan",
            "user": "NanHUO",
            "type": "user"
          },
          "name": "Nan Huo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:24.419Z",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd098f",
          "name": "Jinyang Li",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0990",
          "name": "Bowen Qin",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0991",
          "name": "Ge Qu",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0992",
          "name": "Xiaolong Li",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0993",
          "name": "Xiaodong Li",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0994",
          "name": "Chenhao Ma",
          "hidden": false
        },
        {
          "_id": "68426dfeb5f4d2d0f8fd0995",
          "name": "Reynold Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T17:33:02.000Z",
      "submittedOnDailyAt": "2025-06-06T02:57:34.041Z",
      "title": "Micro-Action: Una acciónable lógica de razonamiento auto-construido para mitigar el conflicto de conocimientos en respuestas a consultas",
      "submittedOnDailyBy": {
        "_id": "6419435385030eca6ac94701",
        "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
        "isPro": false,
        "fullname": "Ge Qu",
        "user": "gq2138",
        "type": "user"
      },
      "summary": "El sistema de Revisión de Contenido (RAG) enfrenta desafíos debido a que el conocimiento extraído de fuentes externas puede conflictar con los parámetros de conocimiento propios de grandes modelos de lenguaje (LLMs), lo que genera conflictos de conocimiento. Esto afecta negativamente a la eficiencia de tareas como respuesta a preguntas (QA). Actualmente, las aproximaciones proban comparar de manera paralela dos fuentes de conocimiento para mitigar el conflicto, pero esto hace que los modelos de lenguaje soporten un exceso de contexto, lo que impide la identificación y mitigación de los conflictos. Para abordar este problema, proponemos un marco de trabajo basado en acciones micro (Micro-Act) que posee un espacio de acción de diferentes niveles. Este marco reconoce automáticamente la complejidad del contexto y decompone cada fuente de conocimiento de manera adaptativa. Estas comparaciones se expresan como pasos operacionales, permitiendo inferencias que superen el contexto superficial. A través de experimentos amplios en 5 conjuntos de datos, Micro-Act demostró un significativo aumento en la precisión de QA en todos los conjuntos y tres tipos de conflictos, especialmente en tipos temporales y semánticos, donde todos los base líneas fallaron notablemente. Más importante aún, Micro-Act muestra una robusta eficiencia en problemas no conflictivos, demostrando un valor práctico en aplicaciones reales de RAG.",
      "upvotes": 2,
      "discussionId": "68426dfeb5f4d2d0f8fd09c7",
      "ai_summary": "A framework called Micro-Act addresses Knowledge Conflicts in Retrieval-Augmented Generation by adaptively decomposing knowledge sources, leading to improved QA accuracy compared to existing methods.",
      "ai_keywords": [
        "Retrieval-Augmented Generation",
        "Knowledge Conflicts",
        "large language models",
        "parametric knowledge",
        "question answering",
        "hierarchical action space",
        "context complexity",
        "fine-grained comparisons",
        "actionable steps",
        "benchmark datasets",
        "QA accuracy",
        "conflict types",
        "temporal conflicts",
        "semantic conflicts",
        "non-conflict questions"
      ]
    },
    "publishedAt": "2025-06-05T13:33:02.000Z",
    "title": "Micro-Act: Mitigate Knowledge Conflict in Question Answering via\n  Actionable Self-Reasoning",
    "summary": "Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge\nConflicts, where retrieved external knowledge contradicts the inherent,\nparametric knowledge of large language models (LLMs). It adversely affects\nperformance on downstream tasks such as question answering (QA). Existing\napproaches often attempt to mitigate conflicts by directly comparing two\nknowledge sources in a side-by-side manner, but this can overwhelm LLMs with\nextraneous or lengthy contexts, ultimately hindering their ability to identify\nand mitigate inconsistencies. To address this issue, we propose Micro-Act a\nframework with a hierarchical action space that automatically perceives context\ncomplexity and adaptively decomposes each knowledge source into a sequence of\nfine-grained comparisons. These comparisons are represented as actionable\nsteps, enabling reasoning beyond the superficial context. Through extensive\nexperiments on five benchmark datasets, Micro-Act consistently achieves\nsignificant increase in QA accuracy over state-of-the-art baselines across all\n5 datasets and 3 conflict types, especially in temporal and semantic types\nwhere all baselines fail significantly. More importantly, Micro-Act exhibits\nrobust performance on non-conflict questions simultaneously, highlighting its\npractical value in real-world RAG applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6419435385030eca6ac94701",
      "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
      "fullname": "Ge Qu",
      "name": "gq2138",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05229",
      "authors": [
        {
          "_id": "6842bc6855574a112d5733cc",
          "name": "Danil Sivtsov",
          "hidden": false
        },
        {
          "_id": "6842bc6855574a112d5733cd",
          "name": "Ivan Rodkin",
          "hidden": false
        },
        {
          "_id": "6842bc6855574a112d5733ce",
          "name": "Gleb Kuzmin",
          "hidden": false
        },
        {
          "_id": "6842bc6855574a112d5733cf",
          "name": "Yuri Kuratov",
          "hidden": false
        },
        {
          "_id": "6842bc6855574a112d5733d0",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T16:43:48.000Z",
      "submittedOnDailyAt": "2025-06-06T08:35:33.323Z",
      "title": "Desactivar la paralelización de la transformer recursiva de memoria de contexto largo en una red neuronal diagonal.",
      "submittedOnDailyBy": {
        "_id": "618b9540682ec1c38327e586",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618b9540682ec1c38327e586/v_ZBkfh8O9Zh6C2YQpuBX.jpeg",
        "isPro": false,
        "fullname": "Yury Kuratov",
        "user": "yurakuratov",
        "type": "user"
      },
      "summary": "El modelo Transformer muestra dificultades para la inferencia de largos contextos, revelando complejidades de tiempo y memoria lineal en dos dimensiones. Los Recurrent Memory Transformers (RMTs) abordan este problema restringiendo el costo a tiempo lineal y uso de memoria constante. Sin embargo, su estructura de actualización de memoria causa caídas de rendimiento al ejecutarse secuencialmente.\n\nIntroducimos un escenario de programación que desactiva la paralelidad en bloques para mantener la recursividad precisa, llamado Diagonal Batching. Este enfoque elimina las restricciones secuenciales y permite una inferencia eficiente en GPU, evitando el uso de complejas colas y tecnologías de pipa en entradas largas y continuas. Este método reorganiza la ejecución de cálculo en cada momento, por lo que los modelos RMT existentes no requieren reentrenamiento.\n\nEl modelo LLaMA-1B ARMT aplicado con Diagonal Batching logra un aumento de velocidad del 3.3 veces más que el LLaMA-1B estándar y un aumento de velocidad del 1.8 veces más que la implementación secuencial de RMT para 131,072 tokens. Al eliminar las caídas de rendimiento secuenciales, Diagonal Batching reduce los costos de inferencia y los largos secuencias, fortaleciendo la solución práctica de RMT.",
      "upvotes": 2,
      "discussionId": "6842bc6955574a112d573421",
      "ai_summary": "Diagonal Batching enables parallel inference in Recurrent Memory Transformers, significantly improving speed and efficiency for long-context tasks.",
      "ai_keywords": [
        "Transformer models",
        "long-context inference",
        "quadratic time complexity",
        "linear memory complexity",
        "Recurrent Memory Transformers",
        "RMTs",
        "memory update mechanism",
        "sequential execution",
        "Diagonal Batching",
        "run-time computation reordering",
        "parallelism",
        "GPU inference",
        "LLaMA-1B ARMT model",
        "full-attention LLaMA-1B",
        "inference cost",
        "latency"
      ]
    },
    "publishedAt": "2025-06-05T12:43:48.000Z",
    "title": "Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers\n  for Long Contexts",
    "summary": "Transformer models struggle with long-context inference due to their\nquadratic time and linear memory complexity. Recurrent Memory Transformers\n(RMTs) offer a solution by reducing the asymptotic cost to linear time and\nconstant memory usage. However, their memory update mechanism leads to\nsequential execution, causing a performance bottleneck.\n  We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism\nacross segments in RMTs while preserving exact recurrence. This approach\neliminates the sequential constraint, enabling efficient GPU inference even for\nsingle long-context inputs without complex batching and pipelining techniques.\nBecause the technique is purely a run-time computation reordering, existing RMT\nmodels adopt it with no retraining.\n  Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup\nover standard full-attention LLaMA-1B and a 1.8x speedup over the sequential\nRMT implementation on 131,072-token sequences. By removing sequential\nbottleneck, Diagonal Batching reduces inference cost and latency, thereby\nstrengthening RMTs as a practical solution for real-world, long-context\napplications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05229.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "618b9540682ec1c38327e586",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618b9540682ec1c38327e586/v_ZBkfh8O9Zh6C2YQpuBX.jpeg",
      "fullname": "Yury Kuratov",
      "name": "yurakuratov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04734",
      "authors": [
        {
          "_id": "6842537f1c4f28a2031f499c",
          "user": {
            "_id": "632c30576bcb864974cc40a8",
            "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
            "isPro": false,
            "fullname": "sunlin",
            "user": "lincharliesun",
            "type": "user"
          },
          "name": "Lin Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:41:08.211Z",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f499d",
          "name": "Weihong Lin",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f499e",
          "name": "Jinzhu Wu",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f499f",
          "name": "Yongfu Zhu",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a0",
          "name": "Xiaoqi Jian",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a1",
          "name": "Guangxiang Zhao",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a2",
          "name": "Change Jia",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a3",
          "name": "Linglin Zhang",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a4",
          "name": "Sai-er Hu",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a5",
          "name": "Yuhan Wu",
          "hidden": false
        },
        {
          "_id": "6842537f1c4f28a2031f49a6",
          "name": "Xiangzheng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T08:09:11.000Z",
      "submittedOnDailyAt": "2025-06-06T01:04:27.438Z",
      "title": "Las evaluaciones son suficientes: Sobevaluación estratégica de la capacidad de inferencia de los modelos de lenguaje de inteligencia artificial (LLM) debido al diseño de las evaluaciones.",
      "submittedOnDailyBy": {
        "_id": "632c30576bcb864974cc40a8",
        "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
        "isPro": false,
        "fullname": "sunlin",
        "user": "lincharliesun",
        "type": "user"
      },
      "summary": "Los modelos de la serie Deepseek-R1-Distill muestran un excelente rendimiento en matemáticas, ciencias, programación y otros campos, y están ampliamente utilizados en la comunidad abierta de código. Sin embargo, según nuestro estudio, los resultados de evaluación de estos modelos suelen presentar grandes variaciones debido a diversos factores. Una pequeña diferencia en las condiciones de evaluación puede provocar una gran variación en los resultados. Este mismo fenómeno se observa en otros modelos de inferencia abierto de código que se han fine-tunado basándose en la serie Deepseek-R1-Distill, como el modelo QwQ-32B. La mejora en el rendimiento declarada de este último no se puede considerar confiable debido a la falta de reproducibilidad. Por lo tanto, nosotros proponemos la construcción de un más estricto paradigma para la evaluación del rendimiento de los modelos y presentamos evaluaciones experimentales de los modelos de la serie Deepseek-R1-Distill.",
      "upvotes": 2,
      "discussionId": "684253811c4f28a2031f4a11",
      "ai_summary": "Empirical assessments reveal significant fluctuations in benchmark evaluation results of Deepseek-R1-Distill models, questioning the reliability of claimed performance improvements and advocating for a more rigorous evaluation paradigm.",
      "ai_keywords": [
        "reasoning models",
        "Deepseek-R1-Distill",
        "benchmark evaluation",
        "open-source inference models",
        "performance variations",
        "QwQ-32B model"
      ]
    },
    "publishedAt": "2025-06-05T04:09:11.000Z",
    "title": "Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning\n  Capabilities Through Evaluation Design",
    "summary": "Reasoning models represented by the Deepseek-R1-Distill series have been\nwidely adopted by the open-source community due to their strong performance in\nmathematics, science, programming, and other domains. However, our study\nreveals that their benchmark evaluation results are subject to significant\nfluctuations caused by various factors. Subtle differences in evaluation\nconditions can lead to substantial variations in results. Similar phenomena are\nobserved in other open-source inference models fine-tuned based on the\nDeepseek-R1-Distill series, as well as in the QwQ-32B model, making their\nclaimed performance improvements difficult to reproduce reliably. Therefore, we\nadvocate for the establishment of a more rigorous paradigm for model\nperformance evaluation and present our empirical assessments of the\nDeepseek-R1-Distill series models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04734.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632c30576bcb864974cc40a8",
      "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
      "fullname": "sunlin",
      "name": "lincharliesun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02751",
      "authors": [
        {
          "_id": "68425f88fa50fdb6ce3674b5",
          "user": {
            "_id": "67e6679e4b036872ccb9448d",
            "avatarUrl": "/avatars/04dff7f816046f6e82910c894db10ab1.svg",
            "isPro": false,
            "fullname": "fcyycf",
            "user": "fcy99",
            "type": "user"
          },
          "name": "Chuanyu Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-06T07:40:53.825Z",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674b6",
          "name": "Yuqi Zhang",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674b7",
          "name": "Kunbin Yao",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674b8",
          "name": "Guanying Chen",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674b9",
          "name": "Yuan Xiong",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674ba",
          "name": "Chuan Huang",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674bb",
          "name": "Shuguang Cui",
          "hidden": false
        },
        {
          "_id": "68425f88fa50fdb6ce3674bc",
          "name": "Xiaochun Cao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67e6679e4b036872ccb9448d/qy_cnBlPGQPb1e8qZyWGs.mp4"
      ],
      "publishedAt": "2025-06-03T11:13:48.000Z",
      "submittedOnDailyAt": "2025-06-06T06:39:38.846Z",
      "title": "RobustSplat: 3DGS instantáneo basado en densidad y dinámica",
      "submittedOnDailyBy": {
        "_id": "67e6679e4b036872ccb9448d",
        "avatarUrl": "/avatars/04dff7f816046f6e82910c894db10ab1.svg",
        "isPro": false,
        "fullname": "fcyycf",
        "user": "fcy99",
        "type": "user"
      },
      "summary": "3D Gaussian Splatting (3DGS) es una técnica reciente que captura nuevas perspectivas y permite realizar renderizaciones en tiempo real en el campo de la modelación 3D. Sin embargo, los métodos actuales encuentran dificultades para modelar escenas que se ven afectadas de manera instantánea, lo que afecta a las imágenes de renderización. Hemos descubierto que el proceso de densificación de Gaussianes es efectivo para extraer detalles de la escena, pero también genera Gaussianes adicionales al modelar vistas instantáneas, lo que puede afectar la calidad de las imágenes. Para resolver esto, proponemos RobustSplat, una solución robusta. Esta solución se basa en dos diseños cruciales: primero, introducimos un delayer para retrasar la densificación de Gaussianes, optimizando la estructura dinámica de la escena y previniendo el sobreajuste inicial debido a objetos instantáneos. Segundo, diseñamos un enfoque de mascara de escala conectada, utilizando sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-",
      "upvotes": 2,
      "discussionId": "68425f8afa50fdb6ce367531",
      "projectPage": "https://fcyycf.github.io/RobustSplat/",
      "githubRepo": "https://github.com/fcyycf/RobustSplat",
      "ai_summary": "RobustSplat addresses artifacts in 3D Gaussian Splatting caused by transient objects through delayed Gaussian growth and scale-cascaded mask bootstrapping.",
      "ai_keywords": [
        "Gaussian Splatting",
        "novel-view synthesis",
        "3D modeling",
        "Gaussian densification",
        "transient objects",
        "Gaussian growth",
        "delayed Gaussian growth",
        "scale-cascaded mask bootstrapping",
        "feature similarity",
        "mask prediction"
      ]
    },
    "publishedAt": "2025-06-03T07:13:48.000Z",
    "title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free\n  3DGS",
    "summary": "3D Gaussian Splatting (3DGS) has gained significant attention for its\nreal-time, photo-realistic rendering in novel-view synthesis and 3D modeling.\nHowever, existing methods struggle with accurately modeling scenes affected by\ntransient objects, leading to artifacts in the rendered images. We identify\nthat the Gaussian densification process, while enhancing scene detail capture,\nunintentionally contributes to these artifacts by growing additional Gaussians\nthat model transient disturbances. To address this, we propose RobustSplat, a\nrobust solution based on two critical designs. First, we introduce a delayed\nGaussian growth strategy that prioritizes optimizing static scene structure\nbefore allowing Gaussian splitting/cloning, mitigating overfitting to transient\nobjects in early optimization. Second, we design a scale-cascaded mask\nbootstrapping approach that first leverages lower-resolution feature similarity\nsupervision for reliable initial transient mask estimation, taking advantage of\nits stronger semantic consistency and robustness to noise, and then progresses\nto high-resolution supervision to achieve more precise mask prediction.\nExtensive experiments on multiple challenging datasets show that our method\noutperforms existing methods, clearly demonstrating the robustness and\neffectiveness of our method. Our project page is\nhttps://fcyycf.github.io/RobustSplat/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67e6679e4b036872ccb9448d/qy_cnBlPGQPb1e8qZyWGs.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67e6679e4b036872ccb9448d",
      "avatarUrl": "/avatars/04dff7f816046f6e82910c894db10ab1.svg",
      "fullname": "fcyycf",
      "name": "fcy99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23115",
      "authors": [
        {
          "_id": "6842afaa6b5d1e675f254cf1",
          "name": "Yunshen Wang",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf2",
          "name": "Yicheng Liu",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf3",
          "name": "Tianyuan Yuan",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf4",
          "name": "Yucheng Mao",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf5",
          "name": "Yingshi Liang",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf6",
          "name": "Xiuyu Yang",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf7",
          "name": "Honggang Zhang",
          "hidden": false
        },
        {
          "_id": "6842afaa6b5d1e675f254cf8",
          "name": "Hang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T05:34:22.000Z",
      "submittedOnDailyAt": "2025-06-06T07:37:16.625Z",
      "title": "Modelo automático de conducción basado en la varianza para la predicción del porcentaje de ocupación tridimensional",
      "submittedOnDailyBy": {
        "_id": "634aab35dcf125e4dafc87b1",
        "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
        "isPro": false,
        "fullname": "YangXiuyu",
        "user": "gzzyyxy",
        "type": "user"
      },
      "summary": "Predecir de manera precisa la estructura de la grilla Occupancy 3D a partir de entradas visuales es crucial para el automóvil autónomo, pero los métodos actuales de clasificación enfrentan desafíos con datos con mucho ruido, observaciones incompletas y estructuras complejas en el espacio 3D. En este estudio, se reestructura la predicción de Occupancy 3D como una tarea de modelado de datos generativos, se aprende la distribución potencial de datos y se integra el conocimiento previo en el espacio 3D. Este enfoque mejora la consistencia de la predicción, la resistencia a la ruido y el manejo de la complejidad de la estructura espacial 3D. Según los experimentos ampliados, los modelos generativos 3D superan los métodos de clasificación más avanzados y proporcionan predicciones más realistas y precisas, especialmente en áreas ocultas o con baja visibilidad. Además, la mejora en las predicciones ofrece grandes beneficios para tareas de planificación posteriores. Los ventajas prácticos de nuestro método se manifiestan claramente en aplicaciones de entorno real en el automóvil autónomo.",
      "upvotes": 2,
      "discussionId": "6842afac6b5d1e675f254db5",
      "ai_summary": "Diffusion models improve 3D occupancy prediction from visual inputs, enhancing accuracy and robustness in complex and occluded scenes, which benefits autonomous driving.",
      "ai_keywords": [
        "diffusion models",
        "generative modeling",
        "3D occupancy grids",
        "autonomous driving",
        "noise robustness",
        "3D scene priors",
        "downstream planning tasks"
      ]
    },
    "publishedAt": "2025-05-29T01:34:22.000Z",
    "title": "Diffusion-Based Generative Models for 3D Occupancy Prediction in\n  Autonomous Driving",
    "summary": "Accurately predicting 3D occupancy grids from visual inputs is critical for\nautonomous driving, but current discriminative methods struggle with noisy\ndata, incomplete observations, and the complex structures inherent in 3D\nscenes. In this work, we reframe 3D occupancy prediction as a generative\nmodeling task using diffusion models, which learn the underlying data\ndistribution and incorporate 3D scene priors. This approach enhances prediction\nconsistency, noise robustness, and better handles the intricacies of 3D spatial\nstructures. Our extensive experiments show that diffusion-based generative\nmodels outperform state-of-the-art discriminative approaches, delivering more\nrealistic and accurate occupancy predictions, especially in occluded or\nlow-visibility regions. Moreover, the improved predictions significantly\nbenefit downstream planning tasks, highlighting the practical advantages of our\nmethod for real-world autonomous driving applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23115.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634aab35dcf125e4dafc87b1",
      "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
      "fullname": "YangXiuyu",
      "name": "gzzyyxy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03238",
      "authors": [
        {
          "_id": "684158e2f11e4b2c51fce923",
          "user": {
            "_id": "6496eae78a7c70379a512e39",
            "avatarUrl": "/avatars/f5ab483ae93cc04b43e825dfd9440905.svg",
            "isPro": false,
            "fullname": "Ziheng Zhao",
            "user": "zzh99",
            "type": "user"
          },
          "name": "Ziheng Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T10:00:00.086Z",
          "hidden": false
        },
        {
          "_id": "684158e2f11e4b2c51fce924",
          "name": "Lisong Dai",
          "hidden": false
        },
        {
          "_id": "684158e2f11e4b2c51fce925",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "684158e2f11e4b2c51fce926",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "684158e2f11e4b2c51fce927",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:57:34.000Z",
      "submittedOnDailyAt": "2025-06-06T00:32:31.925Z",
      "title": "Reset del CT de tronco completo: el enfoque para la sinovitis maligna",
      "submittedOnDailyBy": {
        "_id": "6496eae78a7c70379a512e39",
        "avatarUrl": "/avatars/f5ab483ae93cc04b43e825dfd9440905.svg",
        "isPro": false,
        "fullname": "Ziheng Zhao",
        "user": "zzh99",
        "type": "user"
      },
      "summary": "Traducción de la imagen de CT automática - especialmente, la detección de anomalías y su ubicación y descripción en múltiples planos y en un esquema de todo el cuerpo - es un problema crucial en la radiología clínica. Este estudio tiene como objetivo resolver este problema mediante cuatro contribuciones principales: (i) en la clasificación, se propone un sistema de clasificación que incluye 404 casos representativos de detección de anomalías en todo el área volumétrica, en colaboración con radiologos expertos. (ii) En los datos, se proporciona un conjunto de imágenes de CT incluyendo más de 14,500 casos en múltiples planos y todo el cuerpo, con más de 19,000 anomalías detalladamente etiquetadas, cada una con una descripción detallada y adaptada al sistema de clasificación. (iii) En el desarrollo del modelo, se propone OminiAbnorm-CT, un modelo que permite la detección automática y la descripción de anomalías en imágenes de CT de múltiples planos y todo el cuerpo, y que permite una interacción flexible mediante prompts visuales. (iv) En los benchmarks, se establecen tres tareas de evaluación representativas basadas en escritos clínicos reales. A través de experimentos extensos, OminiAbnorm-CT muestra una excelencia significativamente superior a los métodos actuales en todas las tareas y métricas.",
      "upvotes": 1,
      "discussionId": "684158e3f11e4b2c51fce9d7",
      "ai_summary": "OminiAbnorm-CT, a model for automated interpretation of CT images, outperforms existing methods in localizing and describing abnormalities across different body regions using text queries and visual prompts.",
      "ai_keywords": [
        "OminiAbnorm-CT"
      ]
    },
    "publishedAt": "2025-06-03T13:57:34.000Z",
    "title": "Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric\n  Approach",
    "summary": "Automated interpretation of CT images-particularly localizing and describing\nabnormal findings across multi-plane and whole-body scans-remains a significant\nchallenge in clinical radiology. This work aims to address this challenge\nthrough four key contributions: (i) On taxonomy, we collaborate with senior\nradiologists to propose a comprehensive hierarchical classification system,\nwith 404 representative abnormal findings across all body regions; (ii) On\ndata, we contribute a dataset containing over 14.5K CT images from multiple\nplanes and all human body regions, and meticulously provide grounding\nannotations for over 19K abnormalities, each linked to the detailed description\nand cast into the taxonomy; (iii) On model development, we propose\nOminiAbnorm-CT, which can automatically ground and describe abnormal findings\non multi-plane and whole-body CT images based on text queries, while also\nallowing flexible interaction through visual prompts; (iv) On benchmarks, we\nestablish three representative evaluation tasks based on real clinical\nscenarios. Through extensive experiments, we show that OminiAbnorm-CT can\nsignificantly outperform existing methods on all the tasks and metrics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03238.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6496eae78a7c70379a512e39",
      "avatarUrl": "/avatars/f5ab483ae93cc04b43e825dfd9440905.svg",
      "fullname": "Ziheng Zhao",
      "name": "zzh99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02587",
      "authors": [
        {
          "_id": "68428d18af4573dbb7cba864",
          "user": {
            "_id": "6526503e39fd3599e87c5c53",
            "avatarUrl": "/avatars/f45560d4f6bbb4f0dc06b27a46429726.svg",
            "isPro": false,
            "fullname": "Weiduo Yuan",
            "user": "Yewandou",
            "type": "user"
          },
          "name": "Weiduo Yuan",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T06:39:23.138Z",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba865",
          "name": "Jerry Li",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba866",
          "name": "Justin Yue",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba867",
          "name": "Divyank Shah",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba868",
          "name": "Konstantinos Karydis",
          "hidden": false
        },
        {
          "_id": "68428d18af4573dbb7cba869",
          "name": "Hang Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T08:07:18.000Z",
      "submittedOnDailyAt": "2025-06-06T05:17:25.580Z",
      "title": "BEVCALIB: Calibración de Lidar-Cámara mediante guía de 3D geometría basada en BEVCALIB",
      "submittedOnDailyBy": {
        "_id": "6526503e39fd3599e87c5c53",
        "avatarUrl": "/avatars/f45560d4f6bbb4f0dc06b27a46429726.svg",
        "isPro": false,
        "fullname": "Weiduo Yuan",
        "user": "Yewandou",
        "type": "user"
      },
      "summary": "La corrección precisa de LiDAR-cámara es fundamental para la reconocimiento multimodal en sistemas de conducción autónoma y robótica. Los métodos tradicionales de corrección requieren la recopilación de datos en entornos controlados y no pueden corregir los cambios de transformación que ocurren durante el movimiento del vehículo/robot. En este artículo, se propone un primer modelo y se busca realizar la corrección de LiDAR-cámara utilizando las características de la perspectiva de los ojos de un ciervo (BEV). Para lograr esto, es necesario extraer las características BEV de la cámara y de LiDAR y fusionarlas en un espacio BEV común. Para maximizar la información estructural proveniente de las características BEV, se introduce un nuevo seleccionador de características que filtra las más importantes, reduciendo la consumo de memoria y facilitando el aprendizaje eficiente. Se realizan evaluaciones distribuidas en los conjuntos de datos KITTI, NuScenes y nuestro propio, demostrando que BEVCALIB es el nuevo estandarte. En diferentes condiciones de ruido, en KITTI se obtiene un promedio de (47.08%, 82.32%) y en NuScenes de (78.17%, 68.29%), superando los mejores referencias de la literatura. En el ámbito de los códigos abiertos, se mejora significativamente el rendimiento, superando el mejor referencia disponible en un factor de más de 10. Los códigos y resultados de demostración del nuestro trabajo pueden ser accedidos en https://cisl.ucr.edu/BEVCalib.",
      "upvotes": 1,
      "discussionId": "68428d1baf4573dbb7cba8f5",
      "projectPage": "https://cisl.ucr.edu/BEVCalib/",
      "githubRepo": "https://github.com/UCR-CISL/BEVCalib",
      "ai_summary": "BEVCALIB model uses bird's-eye view features for accurate LiDAR-camera calibration from raw data, demonstrating superior performance under various noise conditions.",
      "ai_keywords": [
        "bird's-eye view",
        "BEVCALIB",
        "camera BEV features",
        "LiDAR BEV features",
        "shared BEV feature space",
        "feature selector",
        "transformation decoder",
        "KITTI",
        "NuScenes",
        "reproducible baseline"
      ]
    },
    "publishedAt": "2025-06-03T04:07:18.000Z",
    "title": "BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View\n  Representations",
    "summary": "Accurate LiDAR-camera calibration is fundamental to fusing multi-modal\nperception in autonomous driving and robotic systems. Traditional calibration\nmethods require extensive data collection in controlled environments and cannot\ncompensate for the transformation changes during the vehicle/robot movement. In\nthis paper, we propose the first model that uses bird's-eye view (BEV) features\nto perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve\nthis, we extract camera BEV features and LiDAR BEV features separately and fuse\nthem into a shared BEV feature space. To fully utilize the geometric\ninformation from the BEV feature, we introduce a novel feature selector to\nfilter the most important features in the transformation decoder, which reduces\nmemory consumption and enables efficient training. Extensive evaluations on\nKITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes a\nnew state of the art. Under various noise conditions, BEVCALIB outperforms the\nbest baseline in the literature by an average of (47.08%, 82.32%) on KITTI\ndataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation,\nrotation), respectively. In the open-source domain, it improves the best\nreproducible baseline by one order of magnitude. Our code and demo results are\navailable at https://cisl.ucr.edu/BEVCalib.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02587.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6526503e39fd3599e87c5c53",
      "avatarUrl": "/avatars/f45560d4f6bbb4f0dc06b27a46429726.svg",
      "fullname": "Weiduo Yuan",
      "name": "Yewandou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04996",
      "authors": [
        {
          "_id": "68429955c49e8ad3f997b24a",
          "user": {
            "_id": "622dc11fe27c88667db093fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
            "isPro": false,
            "fullname": "Edoardo Bianchi",
            "user": "EdBianchi",
            "type": "user"
          },
          "name": "Edoardo Bianchi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T07:31:34.170Z",
          "hidden": false
        },
        {
          "_id": "68429955c49e8ad3f997b24b",
          "name": "Antonio Liotta",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T13:05:23.000Z",
      "submittedOnDailyAt": "2025-06-06T06:04:33.210Z",
      "title": "PATS: Evaluación de habilidades deportivas desde múltiples perspectivas mediante reconocimiento de habilidades y muestreo de series temporales",
      "submittedOnDailyBy": {
        "_id": "622dc11fe27c88667db093fc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
        "isPro": false,
        "fullname": "Edoardo Bianchi",
        "user": "EdBianchi",
        "type": "user"
      },
      "summary": "Automation spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam spam",
      "upvotes": 0,
      "discussionId": "68429956c49e8ad3f997b288",
      "ai_summary": "PATS is a novel temporal sampling method that enhances video analysis of athletic skills by ensuring complete movement patterns are captured, outperforming existing methods across various domains.",
      "ai_keywords": [
        "Proficiency-Aware Temporal Sampling",
        "PATS",
        "EgoExo4D benchmark",
        "SkillFormer",
        "temporal continuity",
        "temporal coherence",
        "fundamental movement patterns",
        "dynamic sports",
        "sequential skills"
      ]
    },
    "publishedAt": "2025-06-05T09:05:23.000Z",
    "title": "PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill\n  Assessment",
    "summary": "Automated sports skill assessment requires capturing fundamental movement\npatterns that distinguish expert from novice performance, yet current video\nsampling methods disrupt the temporal continuity essential for proficiency\nevaluation. To this end, we introduce Proficiency-Aware Temporal Sampling\n(PATS), a novel sampling strategy that preserves complete fundamental movements\nwithin continuous temporal segments for multi-view skill assessment. PATS\nadaptively segments videos to ensure each analyzed portion contains full\nexecution of critical performance components, repeating this process across\nmultiple segments to maximize information coverage while maintaining temporal\ncoherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses\nthe state-of-the-art accuracy across all viewing configurations (+0.65% to\n+3.05%) and delivers substantial gains in challenging domains (+26.22%\nbouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that\nPATS successfully adapts to diverse activity characteristics-from\nhigh-frequency sampling for dynamic sports to fine-grained segmentation for\nsequential skills-demonstrating its effectiveness as an adaptive approach to\ntemporal sampling that advances automated skill assessment for real-world\napplications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622dc11fe27c88667db093fc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
      "fullname": "Edoardo Bianchi",
      "name": "EdBianchi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  }
]