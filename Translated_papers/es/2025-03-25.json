[
  {
    "paper": {
      "id": "2503.18878",
      "authors": [
        {
          "_id": "67e25fe88e6c927eb7794abd",
          "name": "Andrey Galichin",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794abe",
          "user": {
            "_id": "60cd95ee15ecba5f2200304a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
            "isPro": false,
            "fullname": "Alexey Dontsov",
            "user": "therem",
            "type": "user"
          },
          "name": "Alexey Dontsov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:18:43.467Z",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794abf",
          "name": "Polina Druzhinina",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794ac0",
          "user": {
            "_id": "6172aaeec8e66e2aa84c06b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6172aaeec8e66e2aa84c06b9/ZdRZSp3P1SU6CIDbvQwkv.jpeg",
            "isPro": false,
            "fullname": "Anton Razzhigaev",
            "user": "razzant",
            "type": "user"
          },
          "name": "Anton Razzhigaev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:05:58.409Z",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794ac1",
          "name": "Oleg Y. Rogov",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794ac2",
          "user": {
            "_id": "662f8d645c4db70c77a203b0",
            "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
            "isPro": false,
            "fullname": "Elena Tutubalina",
            "user": "tlenusik",
            "type": "user"
          },
          "name": "Elena Tutubalina",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:05:56.401Z",
          "hidden": false
        },
        {
          "_id": "67e25fe88e6c927eb7794ac3",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T16:54:26.000Z",
      "submittedOnDailyAt": "2025-03-25T06:45:16.781Z",
      "title": "Aquí se cubrieron todos los fundamentos: Se utilizaron Codificadores Autónomos Esparsos para interpretar las características teóricas de los grandes modelos de lenguaje.",
      "submittedOnDailyBy": {
        "_id": "60cd95ee15ecba5f2200304a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
        "isPro": false,
        "fullname": "Alexey Dontsov",
        "user": "therem",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) han tenido un éxito sorprendente en el procesamiento del lenguaje natural. Los recientes avances están vinculados al desarrollo de nuevas clases de LLMs lógicas. Por ejemplo, DeepSeek-R1, una fuente abierta, ha logrado los mejores resultados con su combinación de pensamiento profundo y compleja lógica. Sin embargo, la estructura lógica interna de estos modelos aún no ha sido investigada. En este artículo, utilizamos el método de decomposición de las características de la representación potencial de redes neuronales mediante autoencoders escasos (SAEs) para identificar las características lógicas de la serie de modelos DeepSeek-R1. Primero, proponemos un enfoque para extraer \"características lógicas\" candidatas desde las representaciones de SAE. Estas características se validan mediante un análisis experimental y métodos interpretables, mostrando una asociación directa con el rendimiento lógico del modelo. Es importante destacar que controlar estos características de manera sistemática puede mejorar el rendimiento lógico y proporcionar una explicación estructural inicial de la lógica en los LLMs. El código está disponible en: https://github.com/AIRI-Institute/SAE-Reasoning.",
      "upvotes": 63,
      "discussionId": "67e25fea8e6c927eb7794b25",
      "ai_keywords": [
        "Sparse Autoencoders (SAEs)",
        "latent representations",
        "interpretable features",
        "reasoning features",
        "empirical analysis",
        "interpretability methods",
        "systematic enhancement"
      ]
    },
    "publishedAt": "2025-03-24T12:54:26.000Z",
    "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in\n  Large Language Models via Sparse Autoencoders",
    "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing. Recent advances have led to the developing of a new class\nof reasoning LLMs; for example, open-source DeepSeek-R1 has achieved\nstate-of-the-art performance by integrating deep thinking and complex\nreasoning. Despite these impressive capabilities, the internal reasoning\nmechanisms of such models remain unexplored. In this work, we employ Sparse\nAutoencoders (SAEs), a method to learn a sparse decomposition of latent\nrepresentations of a neural network into interpretable features, to identify\nfeatures that drive reasoning in the DeepSeek-R1 series of models. First, we\npropose an approach to extract candidate ''reasoning features'' from SAE\nrepresentations. We validate these features through empirical analysis and\ninterpretability methods, demonstrating their direct correlation with the\nmodel's reasoning abilities. Crucially, we demonstrate that steering these\nfeatures systematically enhances reasoning performance, offering the first\nmechanistic account of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18878.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60cd95ee15ecba5f2200304a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
      "fullname": "Alexey Dontsov",
      "name": "therem",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17359",
      "authors": [
        {
          "_id": "67e16a266280a70b45b8a16c",
          "user": {
            "_id": "64105a6d14215c0775dfdd14",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
            "isPro": false,
            "fullname": "Jiwen Yu",
            "user": "VictorYuki",
            "type": "user"
          },
          "name": "Jiwen Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:05:33.251Z",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a16d",
          "name": "Yiran Qin",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a16e",
          "user": {
            "_id": "652404d0050781c16f1c51b0",
            "avatarUrl": "/avatars/4ad62f2c65406dd0af36c6d0697ae599.svg",
            "isPro": false,
            "fullname": "Haoxuan Che",
            "user": "chehx",
            "type": "user"
          },
          "name": "Haoxuan Che",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:06:13.592Z",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a16f",
          "name": "Quande Liu",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a170",
          "user": {
            "_id": "60e272ca6c78a8c122b12127",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
            "isPro": false,
            "fullname": "Xintao Wang",
            "user": "Xintao",
            "type": "user"
          },
          "name": "Xintao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:06:29.181Z",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a171",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a172",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:06:50.971Z",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a173",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:06:56.864Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:59:22.000Z",
      "submittedOnDailyAt": "2025-03-25T01:42:15.880Z",
      "title": "Interactivo Generativo de Video como Motor de Juego de la Siguiente Generación",
      "submittedOnDailyBy": {
        "_id": "64105a6d14215c0775dfdd14",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
        "isPro": false,
        "fullname": "Jiwen Yu",
        "user": "VictorYuki",
        "type": "user"
      },
      "summary": "El desarrollo actual de videojuegos enfrenta grandes problemas relacionados con la creatividad y el costo, debido a que los motores de videojuegos existentes contienen contenidos determinantes. Recientemente, el desarrollo de modelos de generación de imágenes ha demostrado la posibilidad de sintetizar entornos virtuales interactivos realistas, ofreciendo oportunidades para innovar en la creación de videojuegos. En este artículo, proponemos que el Interactive Generative Video (IGV) sea la base para los Generative Game Engines (GGE), con el objetivo de generar contenidos infinitamente nuevos en los próximos videojuegos. Los GGE utilizan las características de la IGV para la síntesis de contenido de alta calidad, la modelación de mundos con conocimientos físicos, la interacción controlable, la capacidad de memoria a largo plazo y la capacidad de inferir causas. Se describen en detalle los módulos clave de los GGE y se presenta una mapa de madurez estándar (L0-L4) para guiar su desarrollo. Nuestro trabajo abre nuevas perspectivas en el desarrollo de videojuegos en la era de la IA, imaginando un futuro en el que los sistemas de generación AI fundamentalmente cambiarán la creación y la experiencia de videojuegos.",
      "upvotes": 47,
      "discussionId": "67e16a276280a70b45b8a214",
      "ai_keywords": [
        "Interactive Generative Video (IGV)",
        "Generative Game Engines (GGE)",
        "video generation models",
        "high-quality content synthesis",
        "physics-aware world modeling",
        "user-controlled interactivity",
        "long-term memory capabilities",
        "causal reasoning",
        "hierarchical maturity roadmap (L0-L4)"
      ]
    },
    "publishedAt": "2025-03-21T13:59:22.000Z",
    "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
    "summary": "Modern game development faces significant challenges in creativity and cost\ndue to predetermined content in traditional game engines. Recent breakthroughs\nin video generation models, capable of synthesizing realistic and interactive\nvirtual environments, present an opportunity to revolutionize game creation. In\nthis position paper, we propose Interactive Generative Video (IGV) as the\nfoundation for Generative Game Engines (GGE), enabling unlimited novel content\ngeneration in next-generation gaming. GGE leverages IGV's unique strengths in\nunlimited high-quality content synthesis, physics-aware world modeling,\nuser-controlled interactivity, long-term memory capabilities, and causal\nreasoning. We present a comprehensive framework detailing GGE's core modules\nand a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work\ncharts a new course for game development in the AI era, envisioning a future\nwhere AI-powered generative systems fundamentally reshape how games are created\nand experienced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17359.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64105a6d14215c0775dfdd14",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
      "fullname": "Jiwen Yu",
      "name": "VictorYuki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18942",
      "authors": [
        {
          "_id": "67e226039cd910bee045e38f",
          "user": {
            "_id": "6505a02f9310ce8c400edc63",
            "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
            "isPro": false,
            "fullname": "Fangfu Liu",
            "user": "Liuff23",
            "type": "user"
          },
          "name": "Fangfu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:07:42.279Z",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e390",
          "name": "Hanyang Wang",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e391",
          "name": "Yimo Cai",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e392",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:20.614Z",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e393",
          "user": {
            "_id": "6528fc319474946b8541b36f",
            "avatarUrl": "/avatars/08ea388cbcd7c0f1361980127a8d33c3.svg",
            "isPro": false,
            "fullname": "Xiaohang Zhan",
            "user": "xhangzhan",
            "type": "user"
          },
          "name": "Xiaohang Zhan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:08:05.983Z",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e394",
          "user": {
            "_id": "66c8131afafc0fc87ca99650",
            "avatarUrl": "/avatars/a6eeba2ccf011d5c9964fd38f85bd671.svg",
            "isPro": false,
            "fullname": "Yueqi Duan",
            "user": "duanyueqi",
            "type": "user"
          },
          "name": "Yueqi Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:08:11.759Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:04.000Z",
      "submittedOnDailyAt": "2025-03-25T02:12:44.893Z",
      "title": "Video-T1: Escalado de video en la prueba de generación de vídeo",
      "submittedOnDailyBy": {
        "_id": "6505a02f9310ce8c400edc63",
        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
        "isPro": false,
        "fullname": "Fangfu Liu",
        "user": "Liuff23",
        "type": "user"
      },
      "summary": "En el campo de la generación de vídeos, los avances en la cantidad de datos de entrenamiento, el tamaño del modelo y la capacidad de escalabilidad de costos de cálculo han logrado impresionantes resultados en la generación numérica, permitiendo a los usuarios desarrollar creatividad en diversas áreas. Recientemente, los investigadores de modelos de lenguaje grandes (LLMs) han demostrado que al expandir la escalación durante el test y utilizar cálculos durante la inferencia, se puede significativamente mejorar el rendimiento de los LLMs. En lugar de realizar la escalabilidad de modelos de vídeo con altos costos de entrenamiento, el objetivo ha sido expandir la fuerza de la escalabilidad durante el test (TTS) a la generación de vídeos, encontrando métodos para utilizar la cantidad de cálculos durante la inferencia en casos donde el procesamiento de texto es difícil. En este estudio, se ha definido la escalabilidad durante el test de la generación de vídeos como el problema de sampling mejores trayectorias en el espacio de Gaussian Noise hacia la distribución de vídeos objetivo, con un enfoque especial en la construcción del espacio de búsqueda utilizando datos de test y algoritmos heurísticos. Cuando se proporciona un texto, se revisa la estrategia intuitiva de busqueda lineal para aumentar los candidatos de ruido durante la inferencia, pero debido a los limites significativos en los costos de cálculo durante el test, se diseñó un método eficiente de TTS adecuado para la generación de vídeos, llamado \"Time of Flight\" (ToF) para los frames. Se adaptó y expandió el branch de regresión automática para mejorar la calidad de la generación de vídeos. Las experimentaciones en el marco de referencia de generación de vídeos condicionados por texto y otros detalles demuestran que aumentar la cantidad de cálculos durante el test puede significativamente mejorar la calidad de los vídeos. Página del proyecto: https://liuff19.github.io/Video-T1",
      "upvotes": 41,
      "discussionId": "67e226059cd910bee045e42b",
      "projectPage": "https://liuff19.github.io/Video-T1/",
      "githubRepo": "https://github.com/liuff19/Video-T1",
      "ai_keywords": [
        "Test-Time Scaling (TTS)",
        "video foundation models",
        "inference-time computation",
        "Gaussian noise space",
        "target video distribution",
        "test-time verifiers",
        "heuristic algorithms",
        "linear search strategy",
        "noise candidates",
        "full-step denoising",
        "inference time",
        "Tree-of-Frames (ToF)",
        "autoregressive manner",
        "text-conditioned video generation benchmarks"
      ]
    },
    "publishedAt": "2025-03-24T13:59:04.000Z",
    "title": "Video-T1: Test-Time Scaling for Video Generation",
    "summary": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18942.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6505a02f9310ce8c400edc63",
      "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
      "fullname": "Fangfu Liu",
      "name": "Liuff23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18945",
      "authors": [
        {
          "_id": "67e22eca9455abdd1d257263",
          "name": "Aether Team",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257264",
          "user": {
            "_id": "6283546209aa80237c6c482c",
            "avatarUrl": "/avatars/0d6fc5846c0456d5282d82d5bf4d7056.svg",
            "isPro": false,
            "fullname": "Haoyi Zhu",
            "user": "HaoyiZhu",
            "type": "user"
          },
          "name": "Haoyi Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:15:13.586Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257265",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257266",
          "user": {
            "_id": "667e81565934c9fae29207ef",
            "avatarUrl": "/avatars/431e777c71fccf7cf48ce013e5f6f1cb.svg",
            "isPro": false,
            "fullname": "Zhou",
            "user": "ZhouTimeMachine",
            "type": "user"
          },
          "name": "Jianjun Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:05:54.719Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257267",
          "user": {
            "_id": "67a5b0fe5a8652514e67c38c",
            "avatarUrl": "/avatars/28da8e93ee00fd77c7e62d16f9b94045.svg",
            "isPro": false,
            "fullname": "Wenzheng Chang",
            "user": "AmberHeart",
            "type": "user"
          },
          "name": "Wenzheng Chang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:10.947Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257268",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257269",
          "user": {
            "_id": "65e7eb86c7a0617cc71d3df4",
            "avatarUrl": "/avatars/01020b6b5ccb08bf8aa10fd5f8b2701d.svg",
            "isPro": false,
            "fullname": "lizizun",
            "user": "lizizun",
            "type": "user"
          },
          "name": "Zizun Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:06.912Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726a",
          "user": {
            "_id": "6679bb85972a0f224cde335c",
            "avatarUrl": "/avatars/bf0649645458e206ba5224b001723641.svg",
            "isPro": false,
            "fullname": "Junyi Chen",
            "user": "Junyichen",
            "type": "user"
          },
          "name": "Junyi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:13.751Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726b",
          "name": "Chunhua Shen",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726c",
          "user": {
            "_id": "65783ee6ee33d547aecc3ffc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
            "isPro": false,
            "fullname": "Jiangmiao Pang",
            "user": "Jiangmiao",
            "type": "user"
          },
          "name": "Jiangmiao Pang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:26.855Z",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726d",
          "user": {
            "_id": "64478c64e2148488340229db",
            "avatarUrl": "/avatars/f5c23489a068e896381cdc25836ce3dd.svg",
            "isPro": false,
            "fullname": "he",
            "user": "tonghe",
            "type": "user"
          },
          "name": "Tong He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:35.824Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:51.000Z",
      "submittedOnDailyAt": "2025-03-25T02:50:34.610Z",
      "title": "Aether: Modelado de mundos integrados en geometría",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Ahora proporcionaremos la traducción al español del documento.\n\nEl desarrollo de sistemas de inteligencia artificial con capacidad para comprender la comprensión espacial es considerado un problema importante. En este contexto, se integra la reconstrucción geométrica y la modelación generativa. En este artículo, se propone un marco de trabajo universal llamado \"Aether\", que optimiza tres capacidades esenciales: la reconstrucción dinámica en 4D, la predicción de videos condicionados por acciones y la planificación visual condicionada por objetivos. Aether se centra en la compartión de conocimiento simplificado para reconstrucción, predicción y planificación. Utilizando aprendizaje de características, Aether permite la intercambio de trabajo. Como base para modelos de generación de videos, este marco de trabajo muestra un nuevo rendimiento al expandir datos de la realidad con datos sintéticos que nunca han sido vistos. Además, esta metodología logra la expansión de 0-shot en la traza de acciones y en la reconstrucción, y permite modelar geometrías únicas. En particular, no es necesario ver datos de la realidad, y el rendimiento de la reconstrucción puede ser significativamente mejor que los modelos de dominio específico. Además, Aether utiliza espacios de acciones con conocimiento geométrico para transformar predicciones en acciones de manera continua y planificar rutas automáticas. Nuestro trabajo busca impulsar la comunidad en la exploración de nuevas fronteras en la modelación físicamente razonable.",
      "upvotes": 18,
      "discussionId": "67e22ecb9455abdd1d2572af",
      "projectPage": "https://aether-world.github.io/",
      "githubRepo": "https://github.com/OpenRobotLab/Aether",
      "ai_keywords": [
        "Aether",
        "4D dynamic reconstruction",
        "action-conditioned video prediction",
        "goal-conditioned visual planning",
        "task-interleaved feature learning",
        "video generation models",
        "synthetic-to-real generalization",
        "zero-shot generalization",
        "geometric modeling",
        "geometry-informed action space",
        "autonomous trajectory planning",
        "physically-reasonable world modeling"
      ]
    },
    "publishedAt": "2025-03-24T13:59:51.000Z",
    "title": "Aether: Geometric-Aware Unified World Modeling",
    "summary": "The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates unprecedented synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nRemarkably, even without real-world data, its reconstruction performance far\nexceeds that of domain-specific models. Additionally, Aether leverages a\ngeometry-informed action space to seamlessly translate predictions into\nactions, enabling effective autonomous trajectory planning. We hope our work\ninspires the community to explore new frontiers in physically-reasonable world\nmodeling and its applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18945.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18892",
      "authors": [
        {
          "_id": "67e22ce1155ea10f2fdbe5d2",
          "user": {
            "_id": "62751082b43ccfeef483424f",
            "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
            "isPro": false,
            "fullname": "WeihaoZeng",
            "user": "AndrewZeng",
            "type": "user"
          },
          "name": "Weihao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:18.367Z",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d3",
          "user": {
            "_id": "6462def82a83863b97c0611e",
            "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
            "isPro": false,
            "fullname": "Yuzhen Huang",
            "user": "yuzhen17",
            "type": "user"
          },
          "name": "Yuzhen Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:13.781Z",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d4",
          "user": {
            "_id": "612ee6a7b960e78c6d2319d4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "isPro": false,
            "fullname": "Qian Liu",
            "user": "SivilTaram",
            "type": "user"
          },
          "name": "Qian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:15.927Z",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d5",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d6",
          "user": {
            "_id": "64bf71792915a87970c07446",
            "avatarUrl": "/avatars/b24403f9fa699e0143e441b56528e6af.svg",
            "isPro": false,
            "fullname": "Keqing He",
            "user": "HelicHe",
            "type": "user"
          },
          "name": "Keqing He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:09.770Z",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d7",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d8",
          "user": {
            "_id": "615f34ec3f6d24d67c1b5c78",
            "avatarUrl": "/avatars/6dcff6477993d9e57c5cb92b6f95eb66.svg",
            "isPro": false,
            "fullname": "Junxian He",
            "user": "jxhe",
            "type": "user"
          },
          "name": "Junxian He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:16:49.208Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:06:10.000Z",
      "submittedOnDailyAt": "2025-03-25T02:41:40.812Z",
      "title": "SimpleRL-Zoo: Investigación y Control de Reinforcement Learning Zero-Shot en Entornos Naturales de Modelos Basados en Bases Abiertas",
      "submittedOnDailyBy": {
        "_id": "62751082b43ccfeef483424f",
        "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
        "isPro": false,
        "fullname": "WeihaoZeng",
        "user": "AndrewZeng",
        "type": "user"
      },
      "summary": "DeepSeek-R1 ha demostrado que se puede obtener una lógica de pensamiento continuo y natural (CoT) en un marco de aprendizaje por refuerzo (RL) basado en reglas sencillas. Este aprendizaje se inicia directamente en los modelos básicos, lo que se conoce como \"zero RL training\". Los experimentos recientemente efectuados centrados en la serie de modelos Qwen2.5 han mostrado resultados significativos, ya que estos modelos ya muestran una fuerte capacidad para seguir instrucciones y auto-reflexar, lo que hace que no sean representativos. En este estudio, se revisaron varios modelos básicos, incluyendo LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B y la serie de modelos Qwen2.5 (desde 0.5B hasta 32B). Mediante estrategias clave como ajuste de recompensas y control de la dificultad de las preguntas, se logró un gran avance en la precisión lógica y la longitud de las respuestas. Sin embargo, al observar la dinámica del aprendizaje, se notó que cada modelo básico muestra diferentes patrones durante el aprendizaje. Por ejemplo, el aumento de la longitud de la respuesta no está directamente relacionado con la aparición de ciertos comportamientos cognitivos (por ejemplo, \"grandes conceptos\"). En particular, se destaca la aparición de \"grandes conceptos\" en pequeños modelos de la familia Qwen. Este estudio comparte los diseños clave, descubrimientos y prácticas que permiten el \"zero RL training\", y abre código, modelos y herramientas analíticas bajo la licencia de código abierto para fomentar el desarrollo futuro.",
      "upvotes": 16,
      "discussionId": "67e22ce3155ea10f2fdbe6c0",
      "githubRepo": "https://github.com/hkust-nlp/simpleRL-reason",
      "ai_keywords": [
        "reinforcement learning",
        "rule-based rewards",
        "zero RL training",
        "long chain-of-thought (CoT) reasoning",
        "instruction-following",
        "self-reflection",
        "base models",
        "Qwen2.5 model series",
        "LLama3-8B",
        "Mistral-7B/24B",
        "DeepSeek-Math-7B",
        "Qwen2.5-math-7B",
        "response length",
        "reasoning accuracy",
        "cognitive behaviors",
        "verification",
        "training dynamics"
      ]
    },
    "publishedAt": "2025-03-24T13:06:10.000Z",
    "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild",
    "summary": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18892.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62751082b43ccfeef483424f",
      "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
      "fullname": "WeihaoZeng",
      "name": "AndrewZeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17439",
      "authors": [
        {
          "_id": "67e21f63fb4213c53714be08",
          "user": {
            "_id": "6565e24fe5aac326bfd15a9d",
            "avatarUrl": "/avatars/28ad90df0e0dbc10ef25ee6499a50dec.svg",
            "isPro": false,
            "fullname": "Zhuoshi Pan",
            "user": "panzs",
            "type": "user"
          },
          "name": "Zhuoshi Pan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:23.682Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be09",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0a",
          "user": {
            "_id": "640d99628512ec51d7ef71c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg",
            "isPro": false,
            "fullname": "Honglin Lin",
            "user": "LHL3341",
            "type": "user"
          },
          "name": "Honglin Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:30.635Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0b",
          "user": {
            "_id": "6397f6081323f19c578f142e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
            "isPro": false,
            "fullname": "QizhiPei",
            "user": "QizhiPei",
            "type": "user"
          },
          "name": "Qizhi Pei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:42.836Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0c",
          "user": {
            "_id": "66580d3d80ee5b1e11a94e57",
            "avatarUrl": "/avatars/1a88e7337f9095c40c6d402fab797d83.svg",
            "isPro": false,
            "fullname": "Zinan Tang",
            "user": "Word2Li",
            "type": "user"
          },
          "name": "Zinan Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:17:50.791Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0d",
          "name": "Wei Wu",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0e",
          "user": {
            "_id": "677e133ee86d0754dc7ce296",
            "avatarUrl": "/avatars/c16511c1876b50c2d049925c5f320d15.svg",
            "isPro": false,
            "fullname": "mingchenlin",
            "user": "mingchenlin2025",
            "type": "user"
          },
          "name": "Chenlin Ming",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:18:12.485Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0f",
          "name": "H. Vicky Zhao",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be10",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:18:25.368Z",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be11",
          "name": "Lijun Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:59:10.000Z",
      "submittedOnDailyAt": "2025-03-25T01:47:52.213Z",
      "title": "El método de aprendizaje en la biblioteca de máquinas que tiene como objetivo el desarrollo matemático es un método de aprendizaje basado en errores.",
      "submittedOnDailyBy": {
        "_id": "6397f6081323f19c578f142e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
        "isPro": false,
        "fullname": "QizhiPei",
        "user": "QizhiPei",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grandes (LLMs) han demostrado un impresionante habilidad para la resolución de problemas matemáticos. Sin embargo, el enfoque actual se centra principalmente en mejorar la calidad de los datos de entrenamiento correctos. Por ejemplo, se extraen soluciones de alta calidad de modelos avanzados, ignorando el valor de los datos con errores y potencialmente limitando la capacidad de retroalimentación del modelo. Sin embargo, algunas investigaciones están intentando utilizar datos con errores, y estos intentos incluyen estructuras complejas. Por ejemplo, se utiliza la exploración de nodos con errores mediante MCTS (Exploración de Árboles de Montecarlo). En este artículo, se propone LEMMA (Learning from Errors to Enhance Mathematical Abilities) para fomentar el desarrollo de la matemática. LEMMA construye y ajusta datos que incluyen soluciones incorrectas y etapas de error, así como la retroalimentación hacia soluciones correctas. Específicamente, analiza de manera sistemática el tipo de errores generados por el modelo y introduce un método de expansión de errores basado en tipos de errores para recopilar diversos e errores representativos. Las soluciones correctas pueden surgir a partir de la modificación de los errores o desde un nuevo inicio. Las soluciones a los errores se convierten en soluciones correctas gracias a la suave retroalimentación proporcionada por el modelo. El conjunto de datos construido permite que el modelo se ajuste de manera automática durante el proceso de generación, sin depender de modelos externos de evaluación. Los resultados de los experimentos muestran que LEMMA logra una mejora significativa en el rendimiento en comparación con otros fuertes baselines.",
      "upvotes": 12,
      "discussionId": "67e21f64fb4213c53714be6b",
      "githubRepo": "https://github.com/pzs19/LEMMA",
      "ai_keywords": [
        "Learning from Errors for Mathematical Advancement (LEMMA)",
        "mistake augmentation",
        "model-aware smooth reflection connection",
        "autonomous error correction"
      ]
    },
    "publishedAt": "2025-03-21T13:59:10.000Z",
    "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
    "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17439.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6397f6081323f19c578f142e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
      "fullname": "QizhiPei",
      "name": "QizhiPei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18940",
      "authors": [
        {
          "_id": "67e21b305d20ec3277dac34a",
          "user": {
            "_id": "64e357dd825f4133e7427bf8",
            "avatarUrl": "/avatars/aeb6869d075f65a581797df2aabfb02f.svg",
            "isPro": false,
            "fullname": "tyfeld",
            "user": "tyfeld",
            "type": "user"
          },
          "name": "Ye Tian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:24.659Z",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34b",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34c",
          "user": {
            "_id": "6618d5e83b412cdc85334ca8",
            "avatarUrl": "/avatars/5fe356d58c4c822a60370dbee8d78a69.svg",
            "isPro": false,
            "fullname": "renyuxi",
            "user": "renyuxi",
            "type": "user"
          },
          "name": "Yuxi Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:19:25.116Z",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34d",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34e",
          "name": "Xing Wang",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34f",
          "user": {
            "_id": "646b7f71df2609a541c1ab9f",
            "avatarUrl": "/avatars/48b82e5fd9b06f41ff825507c36816cd.svg",
            "isPro": false,
            "fullname": "Xuefeng Xiao",
            "user": "xiaoxuefeng",
            "type": "user"
          },
          "name": "Xuefeng Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:19:05.020Z",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac350",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac351",
          "user": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "isPro": false,
            "fullname": "Ling Yang",
            "user": "Lingaaaaaaa",
            "type": "user"
          },
          "name": "Ling Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:19:44.463Z",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac352",
          "user": {
            "_id": "67b2795f0bd4ddcd84426bb4",
            "avatarUrl": "/avatars/d4346ac5a0ebbaeb828d832cc6ca9f0b.svg",
            "isPro": false,
            "fullname": "Bin Cui",
            "user": "lazybone128",
            "type": "user"
          },
          "name": "Bin Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:18:48.809Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:02.000Z",
      "submittedOnDailyAt": "2025-03-25T01:26:48.606Z",
      "title": "Notación de entrenamiento difuso para el acelerador de nódulos de redes neuronales",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Los modelos de difusión muestran una capacidad sorprendente para la generación de contenido visual, pero presentan problemas de implementación debido a los altos costos computacionales en el proceso de inferencia. Este sobrecargo computacional principalmente se debe a la complejidad bidimensional de la self-attention en relación con la resolución de las imágenes o videos. Los métodos de aceleración existentes a menudo reducen la calidad del resultado o requieren una reentrenamiento de alto costo. Sin embargo, hemos descubierto que muchos modelos de difusión pueden procesarse en resoluciones bajas, y estas resoluciones bajas ofrecen una oportunidad para realizar una inferencia más eficiente sin perder la calidad. En este artículo, presentamos un marco de aprendizaje sin restricciones llamado 'Bottleneck Sampling', que utiliza resoluciones bajas para reducir el sobrecargo computacional y mantener la precisión del output. Bottleneck Sampling realiza el procesamiento de ruido de alta resolución en las etapas inicial y final, y utiliza un flujo de trabajo de ruido 'High-Low-High' para procesar las etapas intermedias a baja resolución. Para ello, mejoramos gradualmente los puntos de transición de resolución y ajustamos de manera adaptativa el nivel de ruido en cada etapa para reducir la cantidad de operaciones y la complejidad. Bottleneck Sampling ha sido validado en tareas de generación de imágenes y videos, y comparado con el procesamiento de muestras de resolución completa, mantiene una calidad de salida similar, aumentando la velocidad de inferencia en 3 veces para la generación de imágenes y en 2.5 veces para la generación de videos. El código está disponible en la siguiente URL: https://github.com/tyfeld/Bottleneck-Sampling",
      "upvotes": 10,
      "discussionId": "67e21b365d20ec3277dac500",
      "projectPage": "https://tyfeld.github.io/BottleneckSampling.github.io",
      "githubRepo": "https://github.com/tyfeld/Bottleneck-Sampling",
      "ai_keywords": [
        "diffusion models",
        "self-attention",
        "computational overhead",
        "low-resolution priors",
        "Bottleneck Sampling",
        "denoising workflow",
        "high-resolution denoising",
        "aliasing",
        "blurring artifacts",
        "resolution transition points",
        "adaptive timesteps"
      ]
    },
    "publishedAt": "2025-03-24T13:59:02.000Z",
    "title": "Training-free Diffusion Acceleration with Bottleneck Sampling",
    "summary": "Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3times for image generation and 2.5times for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics. Code is available at:\nhttps://github.com/tyfeld/Bottleneck-Sampling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18940.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17489",
      "authors": [
        {
          "_id": "67e21f300e6b6fcc3eb38ae1",
          "name": "Shu Pu",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae2",
          "name": "Yaochen Wang",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae3",
          "user": {
            "_id": "65e2be1e630e2db23829ee8d",
            "avatarUrl": "/avatars/294f9ba909037f03669dc0bb80cabfe3.svg",
            "isPro": false,
            "fullname": "Dongping Chen",
            "user": "fjchendp",
            "type": "user"
          },
          "name": "Dongping Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:20:55.986Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae4",
          "user": {
            "_id": "64964aae457f60023c6a6f9d",
            "avatarUrl": "/avatars/342603e0028204f33fe7f5e3f3da1aa3.svg",
            "isPro": false,
            "fullname": "Yuhang Chen",
            "user": "yuhangchen",
            "type": "user"
          },
          "name": "Yuhang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:21:02.911Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae5",
          "user": {
            "_id": "67c94fd48670a35a7c05f36c",
            "avatarUrl": "/avatars/a59a7872bcc58fec7747225f2d3da3f9.svg",
            "isPro": false,
            "fullname": "Guohao Wang",
            "user": "NiuniuWang",
            "type": "user"
          },
          "name": "Guohao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:21:09.345Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae6",
          "name": "Qi Qin",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae7",
          "name": "Zhongyi Zhang",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae8",
          "name": "Zhiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae9",
          "user": {
            "_id": "6697e7e55ef2828a1ff371c3",
            "avatarUrl": "/avatars/b361ea817760f7cb5c5d39028ee6b507.svg",
            "isPro": false,
            "fullname": "Zetong Zhou",
            "user": "Frywind",
            "type": "user"
          },
          "name": "Zetong Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:21:29.118Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aea",
          "user": {
            "_id": "67575cac2f7acf9a8b4626fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1OkoZh8A4jPKHpTg5iSXP.png",
            "isPro": false,
            "fullname": "Shuang Gong",
            "user": "shuang72",
            "type": "user"
          },
          "name": "Shuang Gong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:21:35.465Z",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aeb",
          "name": "Yi Gui",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aec",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aed",
          "name": "Philip S. Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T18:59:20.000Z",
      "submittedOnDailyAt": "2025-03-25T01:46:07.985Z",
      "title": "Judge Anything: MLLM tiene la capacidad de realizar juicios en cualquier modelo.",
      "submittedOnDailyBy": {
        "_id": "643be8879f5d314db2d9ed23",
        "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
        "isPro": false,
        "fullname": "Chen Dongping",
        "user": "shuaishuaicdp",
        "type": "user"
      },
      "summary": "La evaluación de modelos basados en generación (MMG) en tareas abiertas como la comprensión multimodal (MMU) se enfrenta a grandes desafíos debido a la complejidad de las interacciones cruzadas. En respuesta a esto, se ha desarrollado la idea de utilizar modelos de comprensión multimodal (MLLMs) como evaluadores automáticos, demostrando resultados excelentes en la evaluación de tareas de comprensión multimodal. Además, este artículo expande los modelos de evaluación consistente para MLLMs a través de dos nuevos benchmarks: TaskAnything y JudgeAnything. TaskAnything evalúa las capacidades de 15 categorías de MLLMs en MMU y MMG utilizando 1,500 preguntas. JudgeAnything, por otro lado, evalua la capacidad de juicio de cinco modelos avanzados (como GPT-4o y Gemini-2.0-Flash) desde la perspectiva de comparación de pares y evaluación de puntuaciones, ofreciendo una base de prueba estandarizada que incluye juicios humanos y guías de evaluación detalladas. Nuestros experimentos de expansión muestran que los MLLMs logran los resultados deseados en la evaluación de MMU (66.55% en comparación de pares y 42.79% en evaluación de puntuaciones), pero presentan grandes desafíos en la evaluación de MMG (53.37% en comparación de pares y 30.05% en evaluación de puntuaciones), revelando sesgos cruzados y problemas de configuración. Para abordar estos desafíos, se presenta el plataforma automatizada OmniArena, cuyo objetivo es evaluar estos modelos y ofrecer recompensas para la multimodalidad. Nuestra investigación subraya la necesidad de protocolos de evaluación equitativos y una fuerte concordancia con las preferencias humanas. Los códigos fuentes y conjuntos de datos están disponibles en la siguiente URL: https://urrealhero.github.io/judgeanythingweb/",
      "upvotes": 10,
      "discussionId": "67e21f350e6b6fcc3eb38c35",
      "ai_keywords": [
        "Multimodal LLMs (MLLMs)",
        "TaskAnything",
        "JudgeAnything",
        "open-ended multimodal understanding (MMU)",
        "open-ended multimodal generation (MMG)",
        "cross-modal interactions",
        "vision-language understanding tasks",
        "any-to-any modality tasks",
        "Pair Comparison",
        "Score Evaluation",
        "omni-models",
        "multimodal reward models",
        "cross-modality biases",
        "hallucination issues"
      ]
    },
    "publishedAt": "2025-03-21T14:59:20.000Z",
    "title": "Judge Anything: MLLM as a Judge Across Any Modality",
    "summary": "Evaluating generative foundation models on open-ended multimodal\nunderstanding (MMU) and generation (MMG) tasks across diverse modalities (e.g.,\nimages, audio, video) poses significant challenges due to the complexity of\ncross-modal interactions. To this end, the idea of utilizing Multimodal LLMs\n(MLLMs) as automated judges has emerged, with encouraging results in assessing\nvision-language understanding tasks. Moving further, this paper extends\nMLLM-as-a-Judge across modalities to a unified manner by introducing two\nbenchmarks, TaskAnything and JudgeAnything, to respectively evaluate the\noverall performance and judging capabilities of MLLMs across any-to-any\nmodality tasks. Specifically, TaskAnything evaluates the MMU and MMG\ncapabilities across 15 any-to-any modality categories, employing 1,500 queries\ncurated from well-established benchmarks. Furthermore, JudgeAnything evaluates\nthe judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from\nthe perspectives of Pair Comparison and Score Evaluation, providing a\nstandardized testbed that incorporates human judgments and detailed rubrics.\nOur extensive experiments reveal that while these MLLMs show promise in\nassessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting\nand 42.79% in Score Evaluation setting), they encounter significant challenges\nwith MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and\n30.05% in Score Evaluation setting), exposing cross-modality biases and\nhallucination issues. To address this, we present OmniArena, an automated\nplatform for evaluating omni-models and multimodal reward models. Our work\nhighlights the need for fairer evaluation protocols and stronger alignment with\nhuman preferences. The source code and dataset are publicly available at:\nhttps://urrealhero.github.io/judgeanythingweb/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17489.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18948",
      "authors": [
        {
          "_id": "67e24217db11e1d382285cd4",
          "user": {
            "_id": "6447a5806ffed6ece1fcf723",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/NjOA7G_QCa3bCluA69hSs.jpeg",
            "isPro": false,
            "fullname": "Ruixiao Dong",
            "user": "dongruixiao",
            "type": "user"
          },
          "name": "Ruixiao Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:20:32.045Z",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd5",
          "user": {
            "_id": "63f5993afcf95ecac2b419b5",
            "avatarUrl": "/avatars/a8c020080a84d9a663789c4fb19270e9.svg",
            "isPro": false,
            "fullname": "Mengde Xu",
            "user": "Mendel192",
            "type": "user"
          },
          "name": "Mengde Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:20:25.156Z",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd6",
          "name": "Zigang Geng",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd7",
          "name": "Li Li",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd8",
          "user": {
            "_id": "665d88640e92f92b0e7eb17f",
            "avatarUrl": "/avatars/ff3a410e1e7bfb00ff0ec8ce4d5b1463.svg",
            "isPro": false,
            "fullname": "han hu",
            "user": "hanhu2",
            "type": "user"
          },
          "name": "Han Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:20:10.271Z",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd9",
          "name": "Shuyang Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:57.000Z",
      "submittedOnDailyAt": "2025-03-25T05:57:15.975Z",
      "title": "La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de las caras del modelo 3D en la superficie 2D. Este proceso implica proyectar cada una de las caras del modelo 3D en la superficie 2D para que se representen en la pantalla 2D. La modelización de imágenes planas es el proceso de proyectar un modelo 3D en una superficie 2D, con el objetivo de representar cada una de",
      "submittedOnDailyBy": {
        "_id": "64c38fcf573c5a427e12cd37",
        "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
        "isPro": false,
        "fullname": "cientgu",
        "user": "cientgu",
        "type": "user"
      },
      "summary": "Los modelos generativos actuales (por ejemplo, modelos de auto-completación y métodos de acceso distribuido) se basan en la decomposición de la aprendizaje de la distribución de datos de alta dimensión en tareas subordinadas en un orden simple. Sin embargo, en el proceso de optimización común de estas tareas subordinadas, ocurren colisiones únicas que los actuales enfoques no pueden resolver sin perder eficiencia y escalabilidad. Utilizamos la invarianza del movimiento del señal visual natural para alinear los objetivos de optimización entre las tareas subordinadas de manera interna. Proponemos un nuevo marco de modelado de imágenes con simetrías equivariantes. Nuestro método consiste en: (1) introducir la columnar bipolaridad para fortalecer la simetría de movimiento en la dirección horizontal y (2) introducir la atención causal sobre el tamaño de ventana para imponer relaciones de contexto consistentes entre las posiciones. La evaluación de generación de ImageNet condicionada por clase (resolución 256x256) muestra que nuestro enfoque alcanza un rendimiento similar al mejor modelo AR de estado de la arte, mientras que utiliza menos costo computacional. Un análisis sistemático muestra que la extensión de la simetría equivariante puede reducir las colisiones entre tareas, mejorar significativamente la capacidad de expansión de 0 shot y permitir la síntesis de imágenes a largo plazo. Esta investigación establece el primer marco de trabajo para la decomposición de la coincidencia de tareas en modelos generativos, y proporciona una visión sobre la comparte eficiente de parámetros y la optimización sin colisiones. El código y el modelo están disponibles en https://github.com/drx-code/EquivariantModeling.",
      "upvotes": 9,
      "discussionId": "67e2421edb11e1d382285f9b",
      "ai_keywords": [
        "autoregressive",
        "diffusion approaches",
        "high-dimensional data distribution learning",
        "subtasks",
        "joint optimization",
        "equivariant image modeling framework",
        "translation invariance",
        "column-wise tokenization",
        "translational symmetry",
        "windowed causal attention",
        "contextual relationships",
        "class-conditioned ImageNet generation",
        "state-of-the-art AR models",
        "computational resources",
        "enhanced equivariance",
        "zero-shot generalization",
        "ultra-long image synthesis",
        "task-aligned decomposition",
        "efficient parameter sharing",
        "conflict-free optimization"
      ]
    },
    "publishedAt": "2025-03-24T13:59:57.000Z",
    "title": "Equivariant Image Modeling",
    "summary": "Current generative models, such as autoregressive and diffusion approaches,\ndecompose high-dimensional data distribution learning into a series of simpler\nsubtasks. However, inherent conflicts arise during the joint optimization of\nthese subtasks, and existing solutions fail to resolve such conflicts without\nsacrificing efficiency or scalability. We propose a novel equivariant image\nmodeling framework that inherently aligns optimization targets across subtasks\nby leveraging the translation invariance of natural visual signals. Our method\nintroduces (1) column-wise tokenization which enhances translational symmetry\nalong the horizontal axis, and (2) windowed causal attention which enforces\nconsistent contextual relationships across positions. Evaluated on\nclass-conditioned ImageNet generation at 256x256 resolution, our approach\nachieves performance comparable to state-of-the-art AR models while using fewer\ncomputational resources. Systematic analysis demonstrates that enhanced\nequivariance reduces inter-task conflicts, significantly improving zero-shot\ngeneralization and enabling ultra-long image synthesis. This work establishes\nthe first framework for task-aligned decomposition in generative modeling,\noffering insights into efficient parameter sharing and conflict-free\noptimization. The code and models are publicly available at\nhttps://github.com/drx-code/EquivariantModeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18948.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c38fcf573c5a427e12cd37",
      "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
      "fullname": "cientgu",
      "name": "cientgu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18886",
      "authors": [
        {
          "_id": "67e21d3484513315a9169aae",
          "user": {
            "_id": "6481764e8af4675862efb22e",
            "avatarUrl": "/avatars/fc2e076bc861693f598a528a068a696e.svg",
            "isPro": true,
            "fullname": "weichenfan",
            "user": "weepiess2383",
            "type": "user"
          },
          "name": "Weichen Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:22.659Z",
          "hidden": false
        },
        {
          "_id": "67e21d3484513315a9169aaf",
          "name": "Amber Yijia Zheng",
          "hidden": false
        },
        {
          "_id": "67e21d3484513315a9169ab0",
          "name": "Raymond A. Yeh",
          "hidden": false
        },
        {
          "_id": "67e21d3484513315a9169ab1",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T16:59:57.000Z",
      "submittedOnDailyAt": "2025-03-25T06:34:48.098Z",
      "title": "CFG-Zero*: Mejora de la guía de clases sin parejas en el modelo de flujo de ajuste",
      "submittedOnDailyBy": {
        "_id": "6481764e8af4675862efb22e",
        "avatarUrl": "/avatars/fc2e076bc861693f598a528a068a696e.svg",
        "isPro": true,
        "fullname": "weichenfan",
        "user": "weepiess2383",
        "type": "user"
      },
      "summary": "Classifier-Free Guidance (CFG) es una tecnología ampliamente utilizada en modelos de red neuronal para mejorar la calidad y la controlabilidad de las imágenes. En este estudio, se realiza una investigación analítica sobre el impacto de CFG en un modelo de Flow Matching entrenado con mezclas de Gaussianas. En los primeros estados de entrenamiento, cuando la estimación del Flow es inaccurate, CFG guia los muestras a rutas erróneas. Con base en esta observación, se propone CFG-Zero*. CFG-Zero* tiene dos contribuciones: primero, ajusta la escala para optimizar un escalar que corrige la precisión de la velocidad estimada; segundo, inicializa a cero los primeros pasos de un ODE Solver. En la generación de imágenes desde el texto (Lumina-Next, Stable Diffusion 3, Flux) y de videos desde el texto (Wan-2.1), CFG-Zero* muestra resultados que superan a CFG, demostrando efectivamente su utilidad como guía en modelos de Flow Matching. El código está disponible en github.com/WeichenFan/CFG-Zero-star.",
      "upvotes": 7,
      "discussionId": "67e21d3884513315a9169bba",
      "projectPage": "https://weichenfan.github.io/webpage-cfg-zero-star/",
      "githubRepo": "https://github.com/WeichenFan/CFG-Zero-star",
      "ai_keywords": [
        "Classifier-Free Guidance (CFG)",
        "diffusion/flow models",
        "image fidelity",
        "controllability",
        "flow matching models",
        "Gaussian mixtures",
        "ground-truth flow",
        "flow estimation",
        "estimated velocity",
        "scalar optimization",
        "ODE solver",
        "text-to-image",
        "Lumina-Next",
        "Stable Diffusion 3",
        "Flux",
        "text-to-video",
        "Wan-2.1",
        "CFG-Zero*"
      ]
    },
    "publishedAt": "2025-03-24T12:59:57.000Z",
    "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
    "summary": "Classifier-Free Guidance (CFG) is a widely adopted technique in\ndiffusion/flow models to improve image fidelity and controllability. In this\nwork, we first analytically study the effect of CFG on flow matching models\ntrained on Gaussian mixtures where the ground-truth flow can be derived. We\nobserve that in the early stages of training, when the flow estimation is\ninaccurate, CFG directs samples toward incorrect trajectories. Building on this\nobservation, we propose CFG-Zero*, an improved CFG with two contributions: (a)\noptimized scale, where a scalar is optimized to correct for the inaccuracies in\nthe estimated velocity, hence the * in the name; and (b) zero-init, which\ninvolves zeroing out the first few steps of the ODE solver. Experiments on both\ntext-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video\n(Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG,\nhighlighting its effectiveness in guiding Flow Matching models. (Code is\navailable at github.com/WeichenFan/CFG-Zero-star)",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18886.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6481764e8af4675862efb22e",
      "avatarUrl": "/avatars/fc2e076bc861693f598a528a068a696e.svg",
      "fullname": "weichenfan",
      "name": "weepiess2383",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18923",
      "authors": [
        {
          "_id": "67e226f401cdb8cf3a1c7cd8",
          "user": {
            "_id": "640222f83e3d0f2745b097b2",
            "avatarUrl": "/avatars/c5dbac84734855369a7f57b051f16caa.svg",
            "isPro": false,
            "fullname": "Meng Cao",
            "user": "mengcao",
            "type": "user"
          },
          "name": "Meng Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:22:38.738Z",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cd9",
          "name": "Pengfei Hu",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cda",
          "name": "Yingyao Wang",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdb",
          "user": {
            "_id": "65733c1b244aefdfc45cc771",
            "avatarUrl": "/avatars/7223cedbeed065c28a400e130cea30ae.svg",
            "isPro": false,
            "fullname": "Jihao Guo",
            "user": "grejioh",
            "type": "user"
          },
          "name": "Jihao Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:22:23.718Z",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdc",
          "name": "Haoran Tang",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdd",
          "name": "Haoze Zhao",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cde",
          "name": "Jiahua Dong",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdf",
          "user": {
            "_id": "63f095be6309c84d5f48848a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f095be6309c84d5f48848a/pL2CKi-r-0mMfIGhYSAsm.jpeg",
            "isPro": false,
            "fullname": "Wangbo Yu",
            "user": "Drexubery",
            "type": "user"
          },
          "name": "Wangbo Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:23:02.455Z",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7ce0",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:22:08.334Z",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7ce1",
          "name": "Ian Reid",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7ce2",
          "name": "Xiaodan Liang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:46:09.000Z",
      "submittedOnDailyAt": "2025-03-25T02:17:41.453Z",
      "title": "Video SimpleQA: La cuestión de la evaluación de la verdad en modelos de lenguaje de video de gran escala",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de los grandes modelos de lenguaje de vídeo (LVLMs) ha demostrado la posibilidad de entender diversos modos, pero la evaluación de la base factual en contenidos de vídeo es un problema importante que aún no ha sido resuelto. Para abordar este problema, presentamos Video SimpleQA, el primer marco de evaluación detallado para la evaluación de la factualidad en LVLMs. Nuestro estudio presenta las siguientes características principales:\n\n1. Necesidad de conocimiento: Integra la adición de conocimiento externo y supera la narración explícita.\n2. Preguntas de exploración de la verdad: Evita la interpretación subjetiva y se centra en eventos o relaciones objetivos y sin controversia.\n3. Respuestas seguras y breves: Las respuestas se escriben en un formato corto, fácil de entender y con una alta certidumbre, y se pueden evaluar automáticamente mediante el marco de juicio LLM-as-a-judge, con un mínimo de variación puntual.\n4. Verificación de fuentes externas: Todos los comentarios se verifican con recursos externos de alto estatus y un riguroso proceso de verificación, asegurando su credibilidad.\n5. Necesidad de inferencia temporal cíclica: La naturaleza de las preguntas comentadas incluye tanto la comprensión de un solo frame estático como la inferencia temporal cíclica dinámica, y se evalúa la dependencia de contexto a largo plazo para evaluar de manera clara la factualidad de los LVLMs.\n\nHemos evaluado extensamente 41 de los LVLMs más avanzados. Las principales conclusiones son:\n\n1. Los LVLMs actuales, especialmente los modelos de código abierto, presentan una clara falta de confianza en su base factual. El modelo con mejores resultados, Gemini-1.5-Pro, alcanza un F-score de aproximadamente 54.4%.\n2. El paradigma de cálculo durante las pruebas no proporciona una mejora significativa en el rendimiento. Se ha demostrado una límite básico para la mejora de la factualidad al realizar cálculos posteriores.\n3. Las búsquedas adicionales consumen tiempo adicional para la inferencia, pero muestran una mejora consistente, indicando un importante trade-off entre eficiencia y rendimiento.",
      "upvotes": 6,
      "discussionId": "67e226f601cdb8cf3a1c7d73",
      "projectPage": "https://videosimpleqa.github.io",
      "ai_keywords": [
        "Large Video Language Models (LVLMs)",
        "multi-modal understanding",
        "factuality evaluation",
        "Video SimpleQA",
        "external knowledge",
        "objective events",
        "relationships",
        "short-form answer",
        "LLM-as-a-judge",
        "automated evaluation",
        "scQUIre",
        "authoritative external references",
        "temporal reasoning",
        "long-context dependencies",
        "F-score",
        "test-time compute",
        "Retrieval-Augmented Generation",
        "inference time overhead",
        "efficiency-performance trade-off"
      ]
    },
    "publishedAt": "2025-03-24T13:46:09.000Z",
    "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models",
    "summary": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18923.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14428",
      "authors": [
        {
          "_id": "67e217941cb9bded659267f0",
          "name": "Hongyu Zhang",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f1",
          "user": {
            "_id": "64210d1fd039a891a914986d",
            "avatarUrl": "/avatars/b178a768657eb223bdbfbd9e0a2000ff.svg",
            "isPro": false,
            "fullname": "Yufan Deng",
            "user": "dyf",
            "type": "user"
          },
          "name": "Yufan Deng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:28:50.390Z",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f2",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:33.315Z",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f3",
          "user": {
            "_id": "63ad0b04e3b217fb36d36c13",
            "avatarUrl": "/avatars/5a3715ba20859052ba04c048db9e03c2.svg",
            "isPro": false,
            "fullname": "Peng Jin",
            "user": "Pengjin",
            "type": "user"
          },
          "name": "Peng Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:29:15.403Z",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f4",
          "user": {
            "_id": "65b2529285b6c21448a10d65",
            "avatarUrl": "/avatars/1b09e2742aecce1bbdc57f0c4504cf38.svg",
            "isPro": false,
            "fullname": "Zesen Cheng",
            "user": "ClownRat",
            "type": "user"
          },
          "name": "Zesen Cheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:28:42.575Z",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f5",
          "name": "Yian Zhao",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f6",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f7",
          "name": "Jie Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:02:14.000Z",
      "submittedOnDailyAt": "2025-03-25T01:10:50.245Z",
      "title": "Último componente: Configuración de video generación utilizando ajustes micro de dos etapas sin entrenamiento",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "La generación de vídeo a partir de texto (T2V) se está enfocando en el desarrollo de modelos de difusión. Sin embargo, los métodos actuales enfrentan desafíos en la combinación precisa de características, la determinación de relaciones espaciales y la comprensión de interacciones complejas de acciones. Para resolver estos limitaciones, proponemos un método llamado MagicComp, que fortalece la generación estructurada de T2V a través de dos etapas, sin limitaciones de aprendizaje. Específicamente:\n\n1. En la etapa condicionada, introducimos la Semantic Anchor Disambiguation para fortalecer el significado propio del tema y resolver gradualmente las incertidumbres entre temas, inyectando vectores de dirección de los semánticos a los embeddings de texto.\n\n2. En la etapa de desdenosa, proponemos la Dynamic Layout Fusion Attention para integrar conocimientos locales y una reconocimiento espacial adaptable al modelo, y ajustando la atención con mascaras para combinar los temas de manera flexible en el dominio temporal espectral.\n\nMagicComp es independiente de modelo y funcional, lo que permite su fácil integración en las arquitecturas actuales de T2V. Los experimentos extendidos en T2V-CompBench y VBench muestran que MagicComp supera los mejores métodos existentes, destacando la posibilidad de generar videos complejos basados en prompts o controlables por rutas. Página del proyecto: https://hong-yu-zhang.github.io/MagicComp-Page/",
      "upvotes": 6,
      "discussionId": "67e217981cb9bded65926978",
      "projectPage": "https://hong-yu-zhang.github.io/MagicComp-Page/",
      "githubRepo": "https://github.com/Hong-yu-Zhang/MagicComp",
      "ai_keywords": [
        "Semantic Anchor Disambiguation",
        "Dynamic Layout Fusion Attention",
        "grounding priors",
        "model-adaptive spatial perception",
        "masked attention modulation"
      ]
    },
    "publishedAt": "2025-03-18T13:02:14.000Z",
    "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation",
    "summary": "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14428.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 35
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18908",
      "authors": [
        {
          "_id": "67e230fd4b9f234b60d06389",
          "user": {
            "_id": "66857bd849a4ed9de4c31936",
            "avatarUrl": "/avatars/f6f016bf36fad5b29f30fbec6cde3e4d.svg",
            "isPro": false,
            "fullname": "Akhiad Bercovich",
            "user": "abercovich",
            "type": "user"
          },
          "name": "Akhiad Bercovich",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:30:49.482Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638a",
          "user": {
            "_id": "6756aa3741b39ab0d327de52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VjjYWljIgPn9HEMDyKtft.png",
            "isPro": false,
            "fullname": "Mohammad Dabbah",
            "user": "mdabbah-nvidia",
            "type": "user"
          },
          "name": "Mohammad Dabbah",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:05.696Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638b",
          "user": {
            "_id": "6509a96c61c4bb4636fd0fd2",
            "avatarUrl": "/avatars/8ffa9b4dd698469f7d70d4d9144aac82.svg",
            "isPro": false,
            "fullname": "Omri Puny",
            "user": "omripuny",
            "type": "user"
          },
          "name": "Omri Puny",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:13.161Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638c",
          "name": "Ido Galil",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638d",
          "user": {
            "_id": "65006cac12c1442d993d6d51",
            "avatarUrl": "/avatars/6700109303b902d453f3d8e2b45a103f.svg",
            "isPro": false,
            "fullname": "Geifman",
            "user": "AmnonGeifman",
            "type": "user"
          },
          "name": "Amnon Geifman",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:22.938Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638e",
          "user": {
            "_id": "604bc69e0fe8ff3ec13d71cd",
            "avatarUrl": "/avatars/fe4b14b24befdbed02eecb43a25c67f4.svg",
            "isPro": false,
            "fullname": "Yonatan Geifman",
            "user": "geifmany",
            "type": "user"
          },
          "name": "Yonatan Geifman",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:30.392Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638f",
          "name": "Izhak Golan",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06390",
          "name": "Ehud Karpas",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06391",
          "user": {
            "_id": "668578fdd24e614fec97eac8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668578fdd24e614fec97eac8/n5xYnqo5nQbVX2tgaRfEi.jpeg",
            "isPro": false,
            "fullname": "Itay Levy",
            "user": "itlevy",
            "type": "user"
          },
          "name": "Itay Levy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:50.928Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06392",
          "user": {
            "_id": "61ee58f1af500c0acfc4d8eb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643010228464-noauth.png",
            "isPro": false,
            "fullname": "Zach Moshe",
            "user": "zachmoshe",
            "type": "user"
          },
          "name": "Zach Moshe",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:31:56.910Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06393",
          "user": {
            "_id": "63a16d5d5d09b819fee9a350",
            "avatarUrl": "/avatars/d1a3fef0131688e92e272cbd80856fc3.svg",
            "isPro": false,
            "fullname": "Najeeb Nabwani",
            "user": "NajeebDeci",
            "type": "user"
          },
          "name": "Najeeb Nabwani",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:02.613Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06394",
          "user": {
            "_id": "6671634f1820f293a9995b12",
            "avatarUrl": "/avatars/50c8f7b4bfb00f2169b808f3c72c7686.svg",
            "isPro": false,
            "fullname": "Tomer Ronen",
            "user": "tomer-nv",
            "type": "user"
          },
          "name": "Tomer Ronen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:10.072Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06395",
          "user": {
            "_id": "665f0a46f065b1d42806000d",
            "avatarUrl": "/avatars/927f042a3c95c5846621e2a381c66bbf.svg",
            "isPro": false,
            "fullname": "Itamar Schen",
            "user": "ischen-nvidia",
            "type": "user"
          },
          "name": "Itamar Schen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:17.605Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06396",
          "user": {
            "_id": "5f5b0efe10b2753d9000c888",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1628140531144-5f5b0efe10b2753d9000c888.jpeg",
            "isPro": false,
            "fullname": "Elad Segal",
            "user": "eladsegal",
            "type": "user"
          },
          "name": "Elad Segal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:24.511Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06397",
          "user": {
            "_id": "666ef13c14f1c262feeb706c",
            "avatarUrl": "/avatars/7dca59acf5e069d96bdbb98dace9199b.svg",
            "isPro": false,
            "fullname": "Ido Shahaf",
            "user": "ishahaf",
            "type": "user"
          },
          "name": "Ido Shahaf",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:30.927Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06398",
          "user": {
            "_id": "66b089f14ae4ae811218cdb6",
            "avatarUrl": "/avatars/a50fe725922dfdbe0e731fade381b22e.svg",
            "isPro": false,
            "fullname": "Oren Tropp",
            "user": "otropp",
            "type": "user"
          },
          "name": "Oren Tropp",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:38.161Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06399",
          "user": {
            "_id": "666027917c3f9c72113cc75c",
            "avatarUrl": "/avatars/a276ebe8e2731b6a05e3c61c2ae0ddae.svg",
            "isPro": false,
            "fullname": "Ran Zilberstein",
            "user": "RanZilberstein-Nvidia",
            "type": "user"
          },
          "name": "Ran Zilberstein",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:44.768Z",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0639a",
          "user": {
            "_id": "65758349983403462a54ac06",
            "avatarUrl": "/avatars/4f337c732f31bd748738c2717b50a99c.svg",
            "isPro": false,
            "fullname": "Ran El-Yaniv",
            "user": "ranielyaniv",
            "type": "user"
          },
          "name": "Ran El-Yaniv",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:32:56.726Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:20:35.000Z",
      "submittedOnDailyAt": "2025-03-25T02:59:12.174Z",
      "title": "FFN Fusion: Recuperación de cálculos secuenciales en el entrenamiento de grandes modelos de lenguaje",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "FFN Fusion es un método de optimización para reducir el cálculo secuencial de grandes modelos de lenguaje. Este método se beneficia naturalmente de la procesamiento paralelo. Nuestra principal idea es que la permutación de la capa de Feed-Forward Network (FFN) puede ser removida sin afectar significativamente la precisión, y que esta capa puede ser procesada de manera paralela. Hemos desarrollado la teoría básica de esta permutación y su combinación, y hemos transformado estas operaciones en cálculos paralelos para reducir significativamente el tiempo de inferencia y mantener el comportamiento del modelo. Aplicamos estas técnicas en Llama-3.1-405B-Instruct para crear Llama-Nemotron-Ultra-253B-Base (Ultra-253B-Base). Este modelo es eficiente, será publicado en el futuro, y ha demostrado un tiempo de inferencia 1.71 veces más rápido, un costo por tarjeta de 35 veces menor y una excelente performance en los benchmarks. Hemos realizado experimentos de dispersión para modelos con entre 49B y 253B de parámetros, y hemos demostrado que FFN Fusion funciona efectivamente en grandes escalas y puede complementar otros métodos de optimización (por ejemplo, cuantización y reducción). Una de las interesantes observaciones es que bloques completos de transformer que incluyen capas de atención y FFN a veces pueden ser procesados de manera paralela, proporcionando una nueva dirección en el diseño de arquitecturas neuronales.",
      "upvotes": 5,
      "discussionId": "67e230fe4b9f234b60d063ec",
      "ai_keywords": [
        "FFN Fusion",
        "Feed-Forward Network (FFN)",
        "parallelization",
        "inference latency",
        "Llama-3.1-405B-Instruct",
        "Ultra-253B-Base",
        "model behavior",
        "per-token cost",
        "benchmarks",
        "transformer blocks",
        "quantization",
        "pruning"
      ]
    },
    "publishedAt": "2025-03-24T13:20:35.000Z",
    "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
    "summary": "We introduce FFN Fusion, an architectural optimization technique that reduces\nsequential computation in large language models by identifying and exploiting\nnatural opportunities for parallelization. Our key insight is that sequences of\nFeed-Forward Network (FFN) layers, particularly those remaining after the\nremoval of specific attention layers, can often be parallelized with minimal\naccuracy impact. We develop a principled methodology for identifying and fusing\nsuch sequences, transforming them into parallel operations that significantly\nreduce inference latency while preserving model behavior. Applying these\ntechniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base\n(Ultra-253B-Base), an efficient and soon-to-be publicly available model that\nachieves a 1.71X speedup in inference latency and 35X lower per-token cost\nwhile maintaining strong performance across benchmarks. Through extensive\nexperiments on models from 49B to 253B parameters, we demonstrate that FFN\nFusion becomes increasingly effective at larger scales and can complement\nexisting optimization techniques like quantization and pruning. Most\nintriguingly, we find that even full transformer blocks containing both\nattention and FFN layers can sometimes be parallelized, suggesting new\ndirections for neural architecture design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18908.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18102",
      "authors": [
        {
          "_id": "67e22206ddc9b120cbde6fbe",
          "name": "Samuel Schmidgall",
          "hidden": false
        },
        {
          "_id": "67e22206ddc9b120cbde6fbf",
          "user": {
            "_id": "6438d1d843d932c462404500",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
            "isPro": false,
            "fullname": "Michael Moor",
            "user": "mdmoor",
            "type": "user"
          },
          "name": "Michael Moor",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:34:00.877Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T15:16:42.000Z",
      "submittedOnDailyAt": "2025-03-25T05:09:36.523Z",
      "title": "AgentRxiv: Anomalia para la Investigación Automática del Modelo de Colaboración",
      "submittedOnDailyBy": {
        "_id": "6438d1d843d932c462404500",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
        "isPro": false,
        "fullname": "Michael Moor",
        "user": "mdmoor",
        "type": "user"
      },
      "summary": "El desarrollo de descubrimientos científicos no se reduce a un momento instantáneo de \"Eureka\" sino a un resultado de la colaboración estructurada de cientos de científicos que trabajan en un objetivo común. El actual flujo de trabajo de agentes puede realizar investigaciones automáticamente, pero es aislado y no tiene la capacidad de mejorar continuamente los resultados de investigación anteriores. Para abordar estos problemas, presentamos el framework AgentRxiv, que permite a los laboratorios de agentes subir y descargar informes desde un servidor de trabajos comunes, facilitando colaboración, compartir conocimientos y construir investigaciones mutuamente. Los laboratorios de agentes han desarrollado nuevas técnicas de razonamiento y prompting, logrando un aumento de 11.4% en rendimiento comparado con agentes aislados. La estrategia que mejor se ha comportado también puede generalizarse a otros campos, con un aumento promedio de 3.3%. A través de AgentRxiv, los laboratorios de agentes que comparten investigaciones pueden colaborar para alcanzar un objetivo común, avanzar más rápidamente que aquellos aislados, y alcanzar una precisión general de 13.7% más alta. Estos hallazgos demuestran que los agentes autónomos pueden contribuir al diseño de sistemas de IA de manera humanizada. Esperamos que AgentRxiv permita a los agentes colaborar hacia el objetivo de la investigación y acelerar las descubrimientos de los investigadores.",
      "upvotes": 5,
      "discussionId": "67e22207ddc9b120cbde702c",
      "ai_keywords": [
        "LLM (Large Language Model)",
        "agent laboratories",
        "preprint server",
        "reasoning techniques",
        "prompting techniques",
        "performance improvements",
        "benchmarks",
        "accuracy"
      ]
    },
    "publishedAt": "2025-03-23T11:16:42.000Z",
    "title": "AgentRxiv: Towards Collaborative Autonomous Research",
    "summary": "Progress in scientific discovery is rarely the result of a single \"Eureka\"\nmoment, but is rather the product of hundreds of scientists incrementally\nworking together toward a common goal. While existing agent workflows are\ncapable of producing research autonomously, they do so in isolation, without\nthe ability to continuously improve upon prior research results. To address\nthese challenges, we introduce AgentRxiv-a framework that lets LLM agent\nlaboratories upload and retrieve reports from a shared preprint server in order\nto collaborate, share insights, and iteratively build on each other's research.\nWe task agent laboratories to develop new reasoning and prompting techniques\nand find that agents with access to their prior research achieve higher\nperformance improvements compared to agents operating in isolation (11.4%\nrelative improvement over baseline on MATH-500). We find that the best\nperforming strategy generalizes to benchmarks in other domains (improving on\naverage by 3.3%). Multiple agent laboratories sharing research through\nAgentRxiv are able to work together towards a common goal, progressing more\nrapidly than isolated laboratories, achieving higher overall accuracy (13.7%\nrelative improvement over baseline on MATH-500). These findings suggest that\nautonomous agents may play a role in designing future AI systems alongside\nhumans. We hope that AgentRxiv allows agents to collaborate toward research\ngoals and enables researchers to accelerate discovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18102.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6438d1d843d932c462404500",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
      "fullname": "Michael Moor",
      "name": "mdmoor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.15879",
      "authors": [
        {
          "_id": "67dea7cc5b44ace7a30e237e",
          "user": {
            "_id": "6540fbf9cb7fffd683942b43",
            "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
            "isPro": false,
            "fullname": "DongGeon Lee",
            "user": "oneonlee",
            "type": "user"
          },
          "name": "DongGeon Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:52:18.850Z",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e237f",
          "name": "Ahjeong Park",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e2380",
          "user": {
            "_id": "666a8be869a08ea4aac5e73e",
            "avatarUrl": "/avatars/be42632414bafb0af74b5f4d4f03d223.svg",
            "isPro": false,
            "fullname": "keira lee",
            "user": "keirahrlee",
            "type": "user"
          },
          "name": "Hyeri Lee",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-22T12:06:36.938Z",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e2381",
          "name": "Hyeonseo Nam",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e2382",
          "name": "Yunho Maeng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T06:04:12.000Z",
      "submittedOnDailyAt": "2025-03-25T03:48:51.972Z",
      "title": "Typed-RAG: Método para obtener respuestas a preguntas no factorías mediante la decomposición multidimensional de tipos",
      "submittedOnDailyBy": {
        "_id": "6540fbf9cb7fffd683942b43",
        "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
        "isPro": false,
        "fullname": "DongGeon Lee",
        "user": "oneonlee",
        "type": "user"
      },
      "summary": "La respuesta no-factoid (NFQA) tiene problemas graves debido a su naturaleza abierta, la diversidad de intenciones y la necesidad multifacética. Los métodos basados en simples enfoques de respuesta no-factoid (especialmente, generación de argumentos de revisión (RAG)) no son suficientes para abordar estos desafíos. Comparado con problemas factuales, los problemas no-factoid (NFQ) no tienen respuestas decisivas, lo que requiere la integración de información de múltiples fuentes y la búsqueda de razones en diferentes niveles. Para resolver estas limitaciones, presentamos un marco de trabajo de multidimensionalidad \"Typed-RAG\" dentro del paradigma de RAG. Typed-RAG clasifica NFQ en tipos específicos como debate, experiencia, comparación, y aplica un enfoque de resolución basado en aspectos para refinar la estrategia de revisión y generación. Al decomponer NFQ multifacéticas en sub-consultas de un solo aspecto y sumar los resultados, Typed-RAG genera respuestas informativas y contextuales. En la evaluación de Typed-RAG, presentamos un conjunto de datos de referencia \"Wiki-NFQA\" que cubre diversos tipos de NFQ. Los resultados de los experimentos muestran que Typed-RAG supera los estándares y demuestra la importancia de la decomposición por tipos en la revisión y generación de NFQ. Nuestro código y conjunto de datos están disponibles en la siguiente URL:\nhttps://github.com/TeamNLP/Typed-RAG",
      "upvotes": 5,
      "discussionId": "67dea7cc5b44ace7a30e23b8",
      "githubRepo": "https://github.com/TeamNLP/Typed-RAG",
      "ai_keywords": [
        "retrieval-augmented generation (RAG)",
        "non-factoid question-answering (NFQA)",
        "multi-aspect reasoning",
        "type-aware multi-aspect decomposition framework",
        "single-aspect sub-queries",
        "Wiki-NFQA",
        "type-aware decomposition"
      ]
    },
    "publishedAt": "2025-03-20T02:04:12.000Z",
    "title": "Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid\n  Question Answering",
    "summary": "Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at\nhttps://github.com/TeamNLP/Typed-RAG{https://github.com/TeamNLP/Typed-RAG}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6540fbf9cb7fffd683942b43",
      "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
      "fullname": "DongGeon Lee",
      "name": "oneonlee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18866",
      "authors": [
        {
          "_id": "67e2290da4525cbb1d718ae2",
          "user": {
            "_id": "65619949d2e4352d64365606",
            "avatarUrl": "/avatars/4241b6e901e146a9ff8b5bc7de3f960a.svg",
            "isPro": true,
            "fullname": "Yangjun Ruan",
            "user": "ryoungj",
            "type": "user"
          },
          "name": "Yangjun Ruan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:33:10.423Z",
          "hidden": false
        },
        {
          "_id": "67e2290da4525cbb1d718ae3",
          "user": {
            "_id": "630bc38809eceb8fafe5ed7f",
            "avatarUrl": "/avatars/5f2a1268f8a7b51cca8446ef0be6445f.svg",
            "isPro": true,
            "fullname": "Neil Band",
            "user": "nband",
            "type": "user"
          },
          "name": "Neil Band",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:33:16.895Z",
          "hidden": false
        },
        {
          "_id": "67e2290da4525cbb1d718ae4",
          "user": {
            "_id": "66a7f54fbb22d7e78a2aeaf4",
            "avatarUrl": "/avatars/3ab8899935f8f7b14e89c623cc6c0fd2.svg",
            "isPro": false,
            "fullname": "Chris J. Maddison",
            "user": "cmaddis",
            "type": "user"
          },
          "name": "Chris J. Maddison",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:33:22.633Z",
          "hidden": false
        },
        {
          "_id": "67e2290da4525cbb1d718ae5",
          "name": "Tatsunori Hashimoto",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T16:41:23.000Z",
      "submittedOnDailyAt": "2025-03-25T02:25:31.127Z",
      "title": "Los motivos para aprender en Rotunda Takeda",
      "submittedOnDailyBy": {
        "_id": "65619949d2e4352d64365606",
        "avatarUrl": "/avatars/4241b6e901e146a9ff8b5bc7de3f960a.svg",
        "isPro": true,
        "fullname": "Yangjun Ruan",
        "user": "ryoungj",
        "type": "user"
      },
      "summary": "El escalado de entrenamiento previo de un Modelo de Lenguaje (LM) avanza rápidamente con la cantidad de frases escritas por humanos, y se preocupa que los datos sean la límite de este escalado. En estas condiciones de limitación de datos, proponemos que clarificar y estimar las pensamientos potenciales durante el proceso de generación de frases puede ser de gran efecto en la eficiencia del entrenamiento previo. Intuitivamente, nuestro enfoque no ve a los textos web como resultados finales de pensamientos humanos abreviados, sino que los pensamientos potenciales incluyen importantes conocimientos contextuales y etapas de racionalización que son cruciales para el aprendizaje eficiente de los datos. Demostramos la eficiencia de nuestro enfoque en un entrenamiento previo con restricciones de datos. Primero, el enfoque de datos sintéticos para estimar pensamientos potenciales mejora significativamente la eficiencia del entrenamiento previo y permite aprender con la misma cantidad de datos (de 5.7% a 25.4% en MATH). Además, muestran que se pueden estimar pensamientos potenciales incluso en la ausencia de texturas fuertes. El LM se entrena continuamente con el algoritmo EM para mejorar la calidad del entrenamiento previo, incluyendo la combinación de la capacidad del LM y los pensamientos potenciales. Un LM de 1B puede escalar su rendimiento en al menos 3 iteraciones, superando significativamente los estándares con solo datos, y el cálculo adicional de estimaciones en la etapa E puede aumentar su efecto. Los efectos de la escalado de estimaciones y las iteraciones de EM ofrecen nuevas oportunidades para el escalado de entrenamiento previo bajo restricciones de datos.",
      "upvotes": 4,
      "discussionId": "67e2290ea4525cbb1d718b18",
      "ai_keywords": [
        "latent thoughts",
        "data-efficient learning",
        "web text",
        "verbose human thought process",
        "synthetic data",
        "data-constrained regime",
        "EM algorithm",
        "thought-augmented pretraining data",
        "inference compute",
        "data-constrained pretraining"
      ]
    },
    "publishedAt": "2025-03-24T12:41:23.000Z",
    "title": "Reasoning to Learn from Latent Thoughts",
    "summary": "Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% rightarrow 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18866.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65619949d2e4352d64365606",
      "avatarUrl": "/avatars/4241b6e901e146a9ff8b5bc7de3f960a.svg",
      "fullname": "Yangjun Ruan",
      "name": "ryoungj",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18013",
      "authors": [
        {
          "_id": "67e22902af6628c90b525a2b",
          "name": "Yufei Zhan",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2c",
          "name": "Yousong Zhu",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2d",
          "name": "Shurong Zheng",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2e",
          "name": "Hongyin Zhao",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2f",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a30",
          "name": "Ming Tang",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a31",
          "name": "Jinqiao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T10:21:14.000Z",
      "submittedOnDailyAt": "2025-03-25T02:25:32.811Z",
      "title": "Visión-R1: El desarrollo de un lenguaje de visión modelo sin humanos se basa en una guía de visión para la reenforcamiento de entrenamiento.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje visuo-lingüístico (LVLMs) utilizan generalmente un patrón de entrenamiento de dos etapas: entrenamiento previo y ajuste de control de normalización. Recientemente, la optimización de preferencias derivada del campo de la lenguaje ha surgido como una estrategia efectiva de entrenamiento posterior para mejorar las capacidades de los LVLMs. Sin embargo, la construcción de datos de preferencias annotados de alta calidad y el desarrollo de modelos de recompensa potentes que modelen estas preferencias implican ambos costos y dificultades. Basándonos en esta observación, proponemos Vision-R1. Vision-R1 es un algoritmo de aprendizaje reforzado similar a R1, que proporciona una recompensa a modelos que reciben retroalimentación visual clara. Esto elimina la necesidad de modelos de recompensa profesionales y conjuntos de datos de preferencias directamente annotados. Además, utilizamos una función de recompensa impulsada por reglas que integran retroalimentación multidimensional para evaluar evaluaciones basadas en la lógica de tareas visuales. También introducimos una estrategia de entrenamiento por reglas evolutivas y ajustamos dinamicamente los criterios de evaluación de recompensa durante el entrenamiento para lograr el continuo mejoramiento del modelo y la prevención del hacking de la recompensa. Experimentos ampliados en marcos de referencia dentro y fuera de distribución muestran que el ajuste de Vision-R1 en un LVLM de 7B logró un mejoramiento uniforme, con un aumento del 50% y superación de los modelos de la escala más avanzada de 10 veces.",
      "upvotes": 4,
      "discussionId": "67e22903af6628c90b525a71",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "pretraining",
        "supervised fine-tuning",
        "preference optimization",
        "reinforcement learning",
        "Vision-R1",
        "criterion-driven reward function",
        "progressive rule refinement strategy",
        "reward criteria",
        "model completions",
        "vision task logic",
        "reward hacking",
        "state-of-the-art"
      ]
    },
    "publishedAt": "2025-03-23T06:21:14.000Z",
    "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models\n  via Vision-Guided Reinforcement Learning",
    "summary": "Large Vision-Language Models (LVLMs) typically follow a two-stage training\nparadigm-pretraining and supervised fine-tuning. Recently, preference\noptimization, derived from the language domain, has emerged as an effective\npost-training reinforcement strategy to enhance capabilities of LVLMs. However,\nconstructing high-quality human-annotated preference data and developing robust\nreward models to mimic these preferences are both costly and challenging.\nMotivated by this observation, we propose Vision-R1, a novel vision-guided\nR1-like reinforcement learning algorithm for LVLMs that rewards models with\ndefinitive vision feedback. It only leverages curated instruction data,\neliminating the need for specialized reward models and handcrafted preference\ndatasets. We incorporate a criterion-driven reward function that further\nintegrates multi-dimensional feedback to evaluate model completions\ncomprehensively based on the vision task logic. Furthermore, we introduce a\nprogressive rule refinement strategy that dynamically adjusts the reward\ncriteria during training, enabling continuous model improvement and mitigating\nreward hacking. Extensive experiments on both in-distribution and\nout-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with\nVision-R1 achieves consistent performance gains, with even up to 50%\nimprovement and surpassing the state-of-the-art 10x size model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18013.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17422",
      "authors": [
        {
          "_id": "67e226b3b1acaf8a7680e926",
          "name": "Javier J. Poveda Rodrigo",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e927",
          "name": "Mohamed Amine Ahmdi",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e928",
          "name": "Alessio Burrello",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e929",
          "name": "Daniele Jahier Pagliari",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e92a",
          "name": "Luca Benini",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T09:00:19.000Z",
      "submittedOnDailyAt": "2025-03-25T02:15:09.508Z",
      "title": "V-Seek: Plataforma para acelerar la lógica de los LLM en un RISC-V plataforma de servidores de hardware abierto",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Recientemente, el crecimiento指数性的 de los Modelos de Lenguaje Grande (LLMs) ha dependido de sistemas basados en GPU, pero los CPU se están extendiendo ampliamente como una opción flexible y de bajo costo para tareas de inferencia y lógica. RISC-V está expandiéndose rápidamente en esta área. RISC-V cuenta con una arquitectura de instrucciones abierta y no dependiente de versiones comerciales, lo que significa que el ecosistema de hardware y software RISC-V adaptados para tareas de LLMs no está completamente maduro debido a la necesidad de ajustes específicos. Este artículo tiene como objetivo llenar esta brecha. En particular, se centra en la optimización de la inferencia de LLMs en la primera CPU multi-core RISC-V vendida, Sophon SG2042.\n\nEn los más avanzados de los LLMs, se ha alcanzado una velocidad de 4.32/2.29 tokens/segundo en la generación de tokens y 6.54/3.68 tokens/segundo en la procesamiento de prompts, lo que representa un aumento del 2.9/3.0 veces respecto a los estándares.",
      "upvotes": 3,
      "discussionId": "67e226b3b1acaf8a7680e96b",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "GPU-based systems",
        "CPUs",
        "RISC-V",
        "ISA",
        "RISC-V hardware",
        "software ecosystem",
        "domain-specific tuning",
        "Sophon SG2042",
        "many-core RISC-V CPU",
        "vector processing capabilities",
        "DeepSeek R1 Distill Llama 8B",
        "DeepSeek R1 Distill QWEN 14B",
        "token generation",
        "prompt processing"
      ]
    },
    "publishedAt": "2025-03-21T05:00:19.000Z",
    "title": "V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V\n  Platforms",
    "summary": "The recent exponential growth of Large Language Models (LLMs) has relied on\nGPU-based systems. However, CPUs are emerging as a flexible and lower-cost\nalternative, especially when targeting inference and reasoning workloads.\nRISC-V is rapidly gaining traction in this area, given its open and\nvendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the\ncorresponding software ecosystem are not fully mature and streamlined, given\nthe requirement of domain-specific tuning. This paper aims at filling this gap,\nfocusing on optimizing LLM inference on the Sophon SG2042, the first\ncommercially available many-core RISC-V CPU with vector processing\ncapabilities.\n  On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1\nDistill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32/2.29 token/s\nfor token generation and 6.54/3.68 token/s for prompt processing, with a speed\nup of up 2.9x/3.0x compared to our baseline.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17422.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18813",
      "authors": [
        {
          "_id": "67e24a997210beea5ecae330",
          "user": {
            "_id": "631dd96f6d6a5870f3d42528",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631dd96f6d6a5870f3d42528/YMsxboRfIBBGE-z-_DeNx.jpeg",
            "isPro": false,
            "fullname": "Edoardo Debenedetti",
            "user": "dedeswim",
            "type": "user"
          },
          "name": "Edoardo Debenedetti",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:26:38.111Z",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae331",
          "user": {
            "_id": "6475c2794766357252e69e9f",
            "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
            "isPro": false,
            "fullname": "i",
            "user": "iliashum",
            "type": "user"
          },
          "name": "Ilia Shumailov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:27:10.362Z",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae332",
          "name": "Tianqi Fan",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae333",
          "name": "Jamie Hayes",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae334",
          "user": {
            "_id": "6303fa9ba362e7e8b51d8f2a",
            "avatarUrl": "/avatars/53e53c84f987989deb351dd2ae6ee558.svg",
            "isPro": false,
            "fullname": "Nicholas Carlini",
            "user": "carlini",
            "type": "user"
          },
          "name": "Nicholas Carlini",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:27:26.752Z",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae335",
          "name": "Daniel Fabian",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae336",
          "name": "Christoph Kern",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae337",
          "name": "Chongyang Shi",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae338",
          "name": "Andreas Terzis",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae339",
          "user": {
            "_id": "63568f18ba90b4ea9fe91cb5",
            "avatarUrl": "/avatars/3e8b3c573e20cf80d329a312bfc34728.svg",
            "isPro": false,
            "fullname": "Florian Tramer",
            "user": "ftramer",
            "type": "user"
          },
          "name": "Florian Tramèr",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:27:51.364Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T15:54:10.000Z",
      "submittedOnDailyAt": "2025-03-25T04:48:52.478Z",
      "title": "Diseño para la recuperación de la Inyección de Prompts",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) están aumentando su uso en sistemas de agentes que interactúan con el entorno externo. Sin embargo, los agentes basados en LLMs son vulnerables a ataques de injección de prompt cuando procesan datos inseguros. En este artículo, se propone una estratégia de defensa fuerte llamada CaMeL. Esta metodología crea capas de protección alrededor del modelo LLM, asegurando su seguridad incluso si el modelo es vulnerable a los ataques. CaMeL extrae claramente el flujo de control y el flujo de datos procesados, asegurando que los datos inseguros no afecten el flujo de ejecución del modelo. Además, para prevenir la salida de datos confidenciales, CaMeL utiliza el concepto de capa de protección. Demostramos la efectividad de CaMeL en el marco de un reciente benchmark de seguridad de agentes, AgentDojo [NeurIPS 2024], donde se logró resolver con seguridad y eficiencia el 67% de las tareas.",
      "upvotes": 2,
      "discussionId": "67e24a9b7210beea5ecae3a0",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "prompt injection attacks",
        "protective system layer",
        "trusted query",
        "untrusted data",
        "program flow",
        "capability",
        "private data exfiltration",
        "unauthorized data flows",
        "AgentDojo"
      ]
    },
    "publishedAt": "2025-03-24T11:54:10.000Z",
    "title": "Defeating Prompt Injections by Design",
    "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models may be susceptible to attacks.\nTo operate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL relies on a notion\nof a capability to prevent the exfiltration of private data over unauthorized\ndata flows. We demonstrate effectiveness of CaMeL by solving 67% of tasks\nwith provable security in AgentDojo [NeurIPS 2024], a recent agentic security\nbenchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18769",
      "authors": [
        {
          "_id": "67e2177c77d32fd1ed8a496b",
          "user": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "isPro": false,
            "fullname": "Alan Dao",
            "user": "alandao",
            "type": "user"
          },
          "name": "Alan Dao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:35.578Z",
          "hidden": false
        },
        {
          "_id": "67e2177c77d32fd1ed8a496c",
          "name": "Dinh Bach Vu",
          "hidden": false
        },
        {
          "_id": "67e2177c77d32fd1ed8a496d",
          "name": "Bui Quang Huy",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/505sM0N1UIUHUOunaHtAG.mp4"
      ],
      "publishedAt": "2025-03-24T15:16:51.000Z",
      "submittedOnDailyAt": "2025-03-25T01:11:03.544Z",
      "title": "AlphaSpace: Semántica de Tokenización y Inferencia Significativa para la Optimización de Acciones de Robótica",
      "submittedOnDailyBy": {
        "_id": "62d7b2339b629105a5d6888a",
        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
        "isPro": false,
        "fullname": "Alan Dao",
        "user": "alandao",
        "type": "user"
      },
      "summary": "En este artículo se presenta un nuevo método llamado AlphaSpace. AlphaSpace ha sido diseñado para mejorar la capacidad cognitiva espacial de grandes modelos de lenguaje (LLMs) en el espacio cartesiano tridimensional. Utiliza una estrategia de tokenización basada en semántica, codificando información de alto valor para tokenes semánticos específicos y integrando principalmente datos cognitivos complejos e iconicos. De esta manera, los LLMs pueden localizar y manipular objetos con precisión en coordenadas [x, y, z]. Los resultados de los experimentos muestran que AlphaSpace supera significativamente a los modelos existentes. La precisión en tareas parciales alcanza un 66.67% más que el 37.5% de GPT-4o y un 29.17% más que la de Claude 3.5 Sonnet.",
      "upvotes": 2,
      "discussionId": "67e2177d77d32fd1ed8a49ac",
      "ai_keywords": [
        "semantics-based tokenization",
        "semantic tokens",
        "Cartesian space",
        "3D Cartesian space",
        "positioning",
        "manipulation subtasks"
      ]
    },
    "publishedAt": "2025-03-24T11:16:51.000Z",
    "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning",
    "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of large language models (LLMs) for 3D Cartesian\nspace navigation. AlphaSpace employs a semantics-based tokenization strategy,\nencoding height information through specialized semantic tokens, and integrates\nprimarily symbolic synthetic reasoning data. This approach enables LLMs to\naccurately manipulate objects by positioning them at specific [x, y, z]\ncoordinates. Experimental results demonstrate that AlphaSpace significantly\noutperforms existing models on manipulation subtasks, achieving a total\naccuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5\nSonnet.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/505sM0N1UIUHUOunaHtAG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18769.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d7b2339b629105a5d6888a",
      "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
      "fullname": "Alan Dao",
      "name": "alandao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18559",
      "authors": [
        {
          "_id": "67e22d5236076dc847989434",
          "name": "Takashi Isobe",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989435",
          "name": "He Cui",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989436",
          "name": "Dong Zhou",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989437",
          "user": {
            "_id": "6463685fd2044cd1d7c74b81",
            "avatarUrl": "/avatars/334637e2d63efb7cc2129fec6ea54725.svg",
            "isPro": false,
            "fullname": "gemengmeng",
            "user": "gemengmeng",
            "type": "user"
          },
          "name": "Mengmeng Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:30:24.973Z",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989438",
          "name": "Dong Li",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989439",
          "user": {
            "_id": "65adc9d086f88a686be41215",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65adc9d086f88a686be41215/xizVHuZPkE0Gu8_ulx0Fm.jpeg",
            "isPro": false,
            "fullname": "Emad Barsoum",
            "user": "ebarsoum",
            "type": "user"
          },
          "name": "Emad Barsoum",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:30:09.267Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T11:13:33.000Z",
      "submittedOnDailyAt": "2025-03-25T02:43:21.795Z",
      "title": "AMD-Hummingbird: Dirección hacia un modelo desde texto eficiente a vídeo\n\n(Nota: El texto original \"効率的なテキストから動画へのモデルへの向け方\" se ha traducido a \"효율적인 텍스트에서 동영상으로 모델로의 방향\" para mantener el significado y contexto originales.)",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "La generación de video a partir de texto (T2V) se centra en la capacidad de sintetizar vídeos realistas a partir de contextos. Sin embargo, los modelos actuales enfrentan desafíos en la equilibrio entre eficiencia computacional y calidad visual, especialmente en dispositivos con limitaciones de recursos como iGPUs y teléfonos inteligentes. Los estudios previos priorizan la realismo visual a costa de la necesidad de modelos de aprendizaje automático realistas, pequeños y eficientes. En respuesta a estos desafíos, proponemos un nuevo marco de trabajo ligero T2V llamado \"Hummingbird\" a través de un aprendizaje de reacción para mejorar la calidad visual. Nuestro enfoque reduce la cantidad de parámetros de U-Net de 1.4 billones a 700 millones, mejorando significativamente la eficiencia mientras mantiene la generación de vídeo de alta calidad. Además, introducimos un nuevo procesamiento de datos utilizando modelos de grandes modelos de lenguaje (LLMs) y evaluación de calidad de vídeo (VQA), mejorando tanto la producción contextual como la calidad del vídeo. Para la entrenamiento dirigido por usuario y personalización estilística, publicamos código completo que incluye el procesamiento de datos y el entrenamiento del modelo. A través de una amplia gama de experimentos, nuestro método logró un aumento de 31 veces en comparación con los modelos más avanzados como VideoCrafter2, alcanzando los mejores puntajes en VBench. Además, nuestro método supera las limitaciones de la generación de vídeos largos en U-Net basados actuales, permitiendo la creación de vídeos de 26 frames. En particular, el proceso de entrenamiento completo es posible con solo cuatro GPU, ofreciendo un rendimiento comparable a los métodos más avanzados. Hummingbird es una solución práctica que combina alta eficiencia, escalabilidad y flexibilidad, proponiendo aplicaciones realistas de la generación de vídeo a partir de texto.",
      "upvotes": 2,
      "discussionId": "67e22d5836076dc84798964e",
      "ai_keywords": [
        "U-Net",
        "Visual feedback learning",
        "Large Language Models (LLMs)",
        "Video Quality Assessment (VQA)",
        "VBench"
      ]
    },
    "publishedAt": "2025-03-24T07:13:33.000Z",
    "title": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model",
    "summary": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18559.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18033",
      "authors": [
        {
          "_id": "67e2706af6cf2764a534d4a5",
          "user": {
            "_id": "630f0d48982455e61cc4cc08",
            "avatarUrl": "/avatars/eea6ed2e112e830effa98a4661c5474f.svg",
            "isPro": false,
            "fullname": "Samuel",
            "user": "Dvir",
            "type": "user"
          },
          "name": "Dvir Samuel",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-25T09:00:01.542Z",
          "hidden": false
        },
        {
          "_id": "67e2706af6cf2764a534d4a6",
          "user": {
            "_id": "66633be10875aaaa9153c963",
            "avatarUrl": "/avatars/f47aaaf7b029ad3e99f49676a8f9a479.svg",
            "isPro": false,
            "fullname": "Matan Levy",
            "user": "m98levy",
            "type": "user"
          },
          "name": "Matan Levy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:25:56.024Z",
          "hidden": false
        },
        {
          "_id": "67e2706af6cf2764a534d4a7",
          "name": "Nir Darshan",
          "hidden": false
        },
        {
          "_id": "67e2706af6cf2764a534d4a8",
          "user": {
            "_id": "6493393f357b252af72196c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6493393f357b252af72196c5/EWSy18XRcMRa_4XMM3Fu-.jpeg",
            "isPro": false,
            "fullname": "Gal Chechik",
            "user": "galchechik",
            "type": "user"
          },
          "name": "Gal Chechik",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:26:05.156Z",
          "hidden": false
        },
        {
          "_id": "67e2706af6cf2764a534d4a9",
          "user": {
            "_id": "64c5f22c2581696666ebed88",
            "avatarUrl": "/avatars/e85cd2d82f16ec10cad2b63929b2f05a.svg",
            "isPro": false,
            "fullname": "Rami Ben-Ari",
            "user": "ramiben",
            "type": "user"
          },
          "name": "Rami Ben-Ari",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:26:11.853Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T11:26:48.000Z",
      "submittedOnDailyAt": "2025-03-25T07:31:06.785Z",
      "title": "OmnimatteZero: Modelo de difusión de video previamente entrenado utilizando Omnimatte en tiempo real (sin entrenamiento)",
      "submittedOnDailyBy": {
        "_id": "630f0d48982455e61cc4cc08",
        "avatarUrl": "/avatars/eea6ed2e112e830effa98a4661c5474f.svg",
        "isPro": false,
        "fullname": "Samuel",
        "user": "Dvir",
        "type": "user"
      },
      "summary": "Omnimatte tiene como objetivo la separación de un vídeo en capas significativamente profundas. Esto incluye el fondo, objetos individuales y efectos relacionados con ellos (por ejemplo, iluminación y reflexión). Los métodos actuales requieren una entrenamiento muy complejo y una optimización automática cara. En este artículo, se propone OmnimatteZero, un enfoque sin necesidad de entrenamiento. Este método utiliza un modelo simple de depisón de vídeo para realizar la tarea de Omnimatte. Su objetivo es eliminar objetos de un vídeo y extraer sus efectos, lo que permite incluir estos objetos en una nueva secuencia de vídeo. Para lograr esto, se aplica un método de completación de imágenes 0-shot para eliminar objetos de vídeo, y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Además, se aplica un método de completación de imágenes 0-shot y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cuando se aplica a un vídeo. Este método se aplica y se complementa con mejoras para corregir cualquier desafío que pueda presentarse cua",
      "upvotes": 2,
      "discussionId": "67e2706df6cf2764a534d570",
      "ai_keywords": [
        "diffusion models",
        "zero-shot image inpainting",
        "self-attention maps",
        "latent arithmetic",
        "real-time performance",
        "frame runtime"
      ]
    },
    "publishedAt": "2025-03-23T07:26:48.000Z",
    "title": "OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video\n  Diffusion Models",
    "summary": "Omnimatte aims to decompose a given video into semantically meaningful\nlayers, including the background and individual objects along with their\nassociated effects, such as shadows and reflections. Existing methods often\nrequire extensive training or costly self-supervised optimization. In this\npaper, we present OmnimatteZero, a training-free approach that leverages\noff-the-shelf pre-trained video diffusion models for omnimatte. It can remove\nobjects from videos, extract individual object layers along with their effects,\nand composite those objects onto new videos. We accomplish this by adapting\nzero-shot image inpainting techniques for video object removal, a task they\nfail to handle effectively out-of-the-box. We then show that self-attention\nmaps capture information about the object and its footprints and use them to\ninpaint the object's effects, leaving a clean background. Additionally, through\nsimple latent arithmetic, object layers can be isolated and recombined\nseamlessly with new video layers to produce new videos. Evaluations show that\nOmnimatteZero not only achieves superior performance in terms of background\nreconstruction but also sets a new record for the fastest Omnimatte approach,\nachieving real-time performance with minimal frame runtime.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18033.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630f0d48982455e61cc4cc08",
      "avatarUrl": "/avatars/eea6ed2e112e830effa98a4661c5474f.svg",
      "fullname": "Samuel",
      "name": "Dvir",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17500",
      "authors": [
        {
          "_id": "67e23d503ef5318b1550f1bc",
          "user": {
            "_id": "6071c4b270e11b30cfcfd7a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
            "isPro": false,
            "fullname": "Louis Owen",
            "user": "louisowen6",
            "type": "user"
          },
          "name": "Louis Owen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:19:59.211Z",
          "hidden": false
        },
        {
          "_id": "67e23d503ef5318b1550f1bd",
          "user": {
            "_id": "62cd4b03c5cc157be82f0b56",
            "avatarUrl": "/avatars/351e963c1c763d507ae78cbcd62966a3.svg",
            "isPro": false,
            "fullname": "Abhay kumar",
            "user": "akanyaani",
            "type": "user"
          },
          "name": "Abhay Kumar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:23:34.033Z",
          "hidden": false
        },
        {
          "_id": "67e23d503ef5318b1550f1be",
          "user": {
            "_id": "645a0d3dd6648853107c5fdc",
            "avatarUrl": "/avatars/1e3b6a4f5ce81a707ba7cbdf81631091.svg",
            "isPro": false,
            "fullname": "Nilabhra Roy Chowdhury",
            "user": "nilabhra",
            "type": "user"
          },
          "name": "Nilabhra Roy Chowdhury",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T09:15:43.961Z",
          "hidden": false
        },
        {
          "_id": "67e23d503ef5318b1550f1bf",
          "user": {
            "_id": "65e4be59e8b017ee1310a1b6",
            "avatarUrl": "/avatars/c3f7cdf5d0859cb80bfb2b970a675dfa.svg",
            "isPro": false,
            "fullname": "Fabian",
            "user": "gueraf",
            "type": "user"
          },
          "name": "Fabian Güra",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:19:56.909Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6071c4b270e11b30cfcfd7a3/AaY617TTwvPiB-ub2B9bA.png"
      ],
      "publishedAt": "2025-03-21T19:23:08.000Z",
      "submittedOnDailyAt": "2025-03-25T03:52:18.158Z",
      "title": "Control de la Varianza mediante Re-ajuste de Pesos en el Aprendizaje de Reserva de LLM",
      "submittedOnDailyBy": {
        "_id": "6071c4b270e11b30cfcfd7a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
        "isPro": false,
        "fullname": "Louis Owen",
        "user": "louisowen6",
        "type": "user"
      },
      "summary": "Los resultados de entrenamiento previo de un LLM están muy afectados por la inicialización de pesos y las estrategias de control de la varianza. La importancia de controlar la varianza inicial en redes neuronales generales ya está bien documentada, pero la literatura sobre la inicialización y el manejo de crecimiento en el entrenamiento previo de un LLM es limitada. En este artículo, se presentan las técnicas de inicialización de pesos Layer Index Rescaling (LIR) y la estrategia de control de varianza Target Variance Rescaling (TVR). Los experimentos con un modelo LLaMA de 1B parámetros muestran que estas metodologías mejoran significativamente la gestión de la varianza, lo que lleva a un aumento considerable en el rendimiento en tareas posteriores, reduce los valores extremos de activación y mitiga los problemas relacionados con el entrenamiento de activación y baja precisión. El código está disponible en la siguiente URL: https://github.com/bluorion-com/weight_rescaling.",
      "upvotes": 2,
      "discussionId": "67e23d513ef5318b1550f22c",
      "githubRepo": "https://github.com/bluorion-com/weight_rescaling",
      "ai_keywords": [
        "Layer Index Rescaling (LIR)",
        "Target Variance Rescaling (TVR)",
        "weight initialization",
        "variance control"
      ]
    },
    "publishedAt": "2025-03-21T15:23:08.000Z",
    "title": "Variance Control via Weight Rescaling in LLM Pre-training",
    "summary": "The outcome of Large Language Model (LLM) pre-training strongly depends on\nweight initialization and variance control strategies. Although the importance\nof initial variance control has been well documented in neural networks in\ngeneral, the literature on initialization and management of its growth during\nLLM pre-training, specifically, is somewhat sparse. In this paper, we introduce\nthe Layer Index Rescaling (LIR) weight initialization scheme, and the Target\nVariance Rescaling (TVR) variance control strategy. Experiments on a 1B\nparameter LLaMA model demonstrate that better variance management using these\ntechniques yields substantial improvements in downstream task performance (up\nto 4.6% on common pre-training benchmarks) and reduces extreme activation\nvalues, thus mitigating challenges associated with quantization and\nlow-precision training. Our code is available at:\nhttps://github.com/bluorion-com/weight_rescaling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6071c4b270e11b30cfcfd7a3/AaY617TTwvPiB-ub2B9bA.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17500.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6071c4b270e11b30cfcfd7a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
      "fullname": "Louis Owen",
      "name": "louisowen6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18470",
      "authors": [
        {
          "_id": "67e23d7ddb11e1d38226cafd",
          "user": {
            "_id": "669794c5813d96b4eb0b3fd6",
            "avatarUrl": "/avatars/b4e2bb7b07cc281932d783dcbb64f211.svg",
            "isPro": true,
            "fullname": "Zhenyu Pan",
            "user": "zhenyupan",
            "type": "user"
          },
          "name": "Zhenyu Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:19:54.569Z",
          "hidden": false
        },
        {
          "_id": "67e23d7ddb11e1d38226cafe",
          "name": "Han Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T09:18:01.000Z",
      "submittedOnDailyAt": "2025-03-25T06:54:21.536Z",
      "title": "MetaSpatial: Desarrollo de VLMs con mejor reconocimiento de espacios 3D basado en metas",
      "submittedOnDailyBy": {
        "_id": "669794c5813d96b4eb0b3fd6",
        "avatarUrl": "/avatars/b4e2bb7b07cc281932d783dcbb64f211.svg",
        "isPro": true,
        "fullname": "Zhenyu Pan",
        "user": "zhenyupan",
        "type": "user"
      },
      "summary": "MetaSpatial es el primer marco de trabajo basado en aprendizaje por refuerzo (RL). Este marco de trabajo fortalece la percepción espacial 3D en modelos de lenguaje visuo-lingüístico (VLMs) y evita la necesidad de optimización codificada para la generación en tiempo real de escenas 3D. MetaSpatial resuelve dos problemas fundamentales: (i) la falta de percepción espacial 3D en VLMs, lo que limita la capacidad de generación de disposiciones realistas; y (ii) la inadecuación de la técnica de fine-tuning de alta presición (SFT) para tareas de creación de objetos, debido a la falta de etiquetado completo de datos reales. La innovación principal es la estructura de optimización basada en aprendizaje por refuerzo en etapas, que integra la evaluación de restricciones físicas y imágenes renderizadas. Esto resulta en disposiciones 3D que son coherentes, físicamente posibles y artísticamente acordes. Metodológicamente, MetaSpatial introduce un proceso iterativo adaptativo, donde los VLMs analizan las salidas renderizadas para mejorar gradualmente la disposición espacial. En evaluaciones experimentales, MetaSpatial significativamente mejora la coherencia espacial y estabilidad de formato en modelos de diferentes tamaños. Después del entrenamiento, las disposiciones de objetos son realistas y funcionalmente consistentes. Esto demuestra el efecto de la RL en la percepción espacial 3D en metaverse, AR/VR, digital humanoides y desarrollo de juegos. Código, datos y pipeline de entrenamiento están disponibles en https://github.com/PzySeere/MetaSpatial.",
      "upvotes": 1,
      "discussionId": "67e23d7fdb11e1d38226cb7b",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "vision-language models (VLMs)",
        "3D spatial reasoning",
        "real-time 3D scene generation",
        "internalized 3D spatial reasoning",
        "supervised fine-tuning (SFT)",
        "multi-turn RL-based optimization",
        "physics-aware constraints",
        "rendered image evaluations",
        "adaptive, iterative reasoning process",
        "scene coherence",
        "spatial consistency",
        "formatting stability",
        "object placements",
        "metaverse",
        "AR/VR",
        "digital twins",
        "game development",
        "empirical evaluations"
      ]
    },
    "publishedAt": "2025-03-24T05:18:01.000Z",
    "title": "MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse",
    "summary": "We present MetaSpatial, the first reinforcement learning (RL)-based framework\ndesigned to enhance 3D spatial reasoning in vision-language models (VLMs),\nenabling real-time 3D scene generation without the need for hard-coded\noptimizations. MetaSpatial addresses two core challenges: (i) the lack of\ninternalized 3D spatial reasoning in VLMs, which limits their ability to\ngenerate realistic layouts, and (ii) the inefficiency of traditional supervised\nfine-tuning (SFT) for layout generation tasks, as perfect ground truth\nannotations are unavailable. Our key innovation is a multi-turn RL-based\noptimization mechanism that integrates physics-aware constraints and rendered\nimage evaluations, ensuring generated 3D layouts are coherent, physically\nplausible, and aesthetically consistent. Methodologically, MetaSpatial\nintroduces an adaptive, iterative reasoning process, where the VLM refines\nspatial arrangements over multiple turns by analyzing rendered outputs,\nimproving scene coherence progressively. Empirical evaluations demonstrate that\nMetaSpatial significantly enhances the spatial consistency and formatting\nstability of various scale models. Post-training, object placements are more\nrealistic, aligned, and functionally coherent, validating the effectiveness of\nRL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game\ndevelopment applications. Our code, data, and training pipeline are publicly\navailable at https://github.com/PzySeere/MetaSpatial.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18470.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669794c5813d96b4eb0b3fd6",
      "avatarUrl": "/avatars/b4e2bb7b07cc281932d783dcbb64f211.svg",
      "fullname": "Zhenyu Pan",
      "name": "zhenyupan",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18352",
      "authors": [
        {
          "_id": "67e217a272e17348c5b3f0a2",
          "name": "Jinjin Zhang",
          "hidden": false
        },
        {
          "_id": "67e217a272e17348c5b3f0a3",
          "user": {
            "_id": "6708e399672d9dcd31575fbc",
            "avatarUrl": "/avatars/0f947f17b5426186aadaa4224571f47b.svg",
            "isPro": false,
            "fullname": "qiuyuhuang",
            "user": "qiuyuhuang",
            "type": "user"
          },
          "name": "Qiuyu Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:24:20.054Z",
          "hidden": false
        },
        {
          "_id": "67e217a272e17348c5b3f0a4",
          "name": "Junjie Liu",
          "hidden": false
        },
        {
          "_id": "67e217a272e17348c5b3f0a5",
          "user": {
            "_id": "64905cd589f22918ecaca080",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/2S7I7uZL49CXbUN2T7p63.jpeg",
            "isPro": false,
            "fullname": "Xiefan Guo",
            "user": "xiefan-guo",
            "type": "user"
          },
          "name": "Xiefan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:24:02.736Z",
          "hidden": false
        },
        {
          "_id": "67e217a272e17348c5b3f0a6",
          "user": {
            "_id": "62c581177b48ba0bb8cdb737",
            "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
            "isPro": false,
            "fullname": "di huang",
            "user": "dihuang",
            "type": "user"
          },
          "name": "Di Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:23:56.412Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T05:25:07.000Z",
      "submittedOnDailyAt": "2025-03-25T07:48:32.671Z",
      "title": "Diffusion-4K: Uso de un modelo de difusión potencial para la síntesis de imágenes de alta resolución\n\n(Nota: En el texto original, \"잠재적 디퓨젼 모형\" es la traducción habitual del \"潜在ディフュージョンモデル\" en coreano, aunque en algunos contextos puede ser traducido como \"추정 디퓨젼 모형\". Aquí se utiliza \"잠재적 디퓨젼 모형\" para mantener la coincidencia con el texto original.)",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "En este artículo se propone el nuevo framework denominado Diffusion-4K, y se presenta el uso de modelos de difusión para la síntesis directa de imágenes de alta resolución. Los puntos clave de desarrollo son los siguientes:\n\n(1) Benchmark Aesthetic-4K: Se aborda la problemática de que no se ha publicado un conjunto de datos de síntesis de imágenes en 4K. Se construyó el benchmark Aesthetic-4K, un conjunto de datos integral para evaluación. Este benchmark se construyó utilizando imágenes de alta calidad y capturas provenientes de GPT-4o. Además, se realizan evaluaciones detalladas utilizando GLCM score y razón de cálculo, y se combinan con indicadores generales como FID, artística y CLIPScore para evaluar las imágenes de alta resolución.\n\n(2) Ajuste micro basado en Wavelet: Se propone un enfoque de ajuste micro basado en Wavelet para entrenar imágenes realistas en 4K directamente. Este enfoque es aplicable a diferentes modelos de difusión y es efectivo para la síntesis de imágenes de alta calidad. Por lo tanto, Diffusion-4K logra un buen rendimiento al combinar la sincronización de la síntesis de imágenes de alta calidad con la planificación de texto, especialmente con modelos grandes de difusión modernos (como SD3-2B y Flux-12B). Los resultados de experimentos extendidos obtenidos en este benchmark demuestran la excelente capacidad de síntesis de imágenes de alta resolución de Diffusion-4K.",
      "upvotes": 1,
      "discussionId": "67e217a772e17348c5b3f20a",
      "ai_keywords": [
        "diffusion models",
        "text-to-image diffusion models",
        "Aesthetic-4K Benchmark",
        "wavelet-based fine-tuning",
        "latent diffusion models",
        "SD3-2B",
        "Flux-12B",
        "GLCM Score",
        "Compression Ratio",
        "FID",
        "Aesthetics",
        "CLIPScore",
        "ultra-high-resolution image synthesis",
        "photorealistic 4K images",
        "high-quality image synthesis",
        "text prompt adherence"
      ]
    },
    "publishedAt": "2025-03-24T01:25:07.000Z",
    "title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent\n  Diffusion Models",
    "summary": "In this paper, we present Diffusion-4K, a novel framework for direct\nultra-high-resolution image synthesis using text-to-image diffusion models. The\ncore advancements include: (1) Aesthetic-4K Benchmark: addressing the absence\nof a publicly available 4K image synthesis dataset, we construct Aesthetic-4K,\na comprehensive benchmark for ultra-high-resolution image generation. We\ncurated a high-quality 4K dataset with carefully selected images and captions\ngenerated by GPT-4o. Additionally, we introduce GLCM Score and Compression\nRatio metrics to evaluate fine details, combined with holistic measures such as\nFID, Aesthetics and CLIPScore for a comprehensive assessment of\nultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a\nwavelet-based fine-tuning approach for direct training with photorealistic 4K\nimages, applicable to various latent diffusion models, demonstrating its\neffectiveness in synthesizing highly detailed 4K images. Consequently,\nDiffusion-4K achieves impressive performance in high-quality image synthesis\nand text prompt adherence, especially when powered by modern large-scale\ndiffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results\nfrom our benchmark demonstrate the superiority of Diffusion-4K in\nultra-high-resolution image synthesis.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18352.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 799
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17735",
      "authors": [
        {
          "_id": "67e22bc349edf14060e5747a",
          "name": "Zhiqiang Yuan",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747b",
          "name": "Ting Zhang",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747c",
          "name": "Ying Deng",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747d",
          "name": "Jiapei Zhang",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747e",
          "name": "Yeshuang Zhu",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747f",
          "name": "Zexi Jia",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e57480",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e57481",
          "name": "Jinchao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-22T11:28:25.000Z",
      "submittedOnDailyAt": "2025-03-25T02:36:41.359Z",
      "title": "RDTF: Framework de entrenamiento de doble máscara eficiente en recursos para la generación de estripes de animación múltiples",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Recientemente, el avance significativo en la tecnología de generación de imágenes ha llevado a que los académicos reciban gran atención. Para aplicar esta tecnología en aplicaciones de bajo recursos, los investigadores generalmente utilizan métodos de ajuste de parámetros eficientes y otros modelos de aprendizaje básico para fine-tuning. Esta aproximación permite transmitir el conocimiento de la región de datos fuentes a la región de datos objetivos, pero también puede que el bajo número de parámetros de entrenamiento dé a una pérdida de capacidad de generalización y a la posibilidad de sesionar el conocimiento de la región de datos fuentes en la región de datos objetivos durante el proceso de inferencia. En este artículo, se argumenta que, cuando hay limitaciones de recursos, entrenar un pequeño modelo de generación de imágenes desde el principio con un nivel de muestras de unos millones, supera el ajuste de parámetros eficientes de grandes modelos en aplicaciones de bajo recursos: esto se basa en la eficientia en el uso de datos y en la efectividad de la implementación de estrategias correctivas. Se utiliza el caso experimental de la Generación de Stick Animations (ASG) para demostrar este punto. Se construye una red de generación de frames discretos para la creación de estos sticks en baja velocidad de frame, y se satisface la demanda de parámetros del modelo de entrenamiento bajo las limitaciones de recursos. Para proporcionar el apoyo de datos al modelo de entrenamiento desde el principio, se propone la construcción de una red de generación de frames discretos, y se presenta una estrategia de uso de datos basada en dos mascaras para mejorar la eficiencia en el uso de datos y expandir la diversidad de los datos limitados. Se propone un método de aprendizaje correctivo adaptativo a la dificultad para acelerar la convergencia en estas dos mascaras, y se decompone la entropía de muestras en componentes estáticas y adaptativas para facilitar el proceso de entrenamiento desde el principio hasta el final. Los experimentos comparan cuantitativa y cualitativamente con métodos de ajuste de parámetros eficientes como I2V-Adapter y SimDA, demostrando que este método supera a los otros y demuestra la posibilidad de este enfoque en tareas de bajo recursos. El código está disponible.",
      "upvotes": 1,
      "discussionId": "67e22bc449edf14060e574e3",
      "ai_keywords": [
        "parameter-efficient tuning",
        "Adapter",
        "Lora",
        "discrete frame generation network",
        "dual-mask based data utilization strategy",
        "curriculum learning method",
        "difficulty-adaptive curriculum learning",
        "sample entropy",
        "I2V-Adapter",
        "SimDA"
      ]
    },
    "publishedAt": "2025-03-22T07:28:25.000Z",
    "title": "RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame\n  Animated Sticker Generation",
    "summary": "Recently, great progress has been made in video generation technology,\nattracting the widespread attention of scholars. To apply this technology to\ndownstream applications under resource-constrained conditions, researchers\nusually fine-tune the pre-trained models based on parameter-efficient tuning\nmethods such as Adapter or Lora. Although these methods can transfer the\nknowledge from the source domain to the target domain, fewer training\nparameters lead to poor fitting ability, and the knowledge from the source\ndomain may lead to the inference process deviating from the target domain. In\nthis paper, we argue that under constrained resources, training a smaller video\ngeneration model from scratch using only million-level samples can outperform\nparameter-efficient tuning on larger models in downstream applications: the\ncore lies in the effective utilization of data and curriculum strategy. Take\nanimated sticker generation (ASG) as a case study, we first construct a\ndiscrete frame generation network for stickers with low frame rates, ensuring\nthat its parameters meet the requirements of model training under constrained\nresources. In order to provide data support for models trained from scratch, we\ncome up with a dual-mask based data utilization strategy, which manages to\nimprove the availability and expand the diversity of limited data. To\nfacilitate convergence under dual-mask situation, we propose a\ndifficulty-adaptive curriculum learning method, which decomposes the sample\nentropy into static and adaptive components so as to obtain samples from easy\nto difficult. The experiment demonstrates that our resource-efficient dual-mask\ntraining framework is quantitatively and qualitatively superior to\nefficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the\nfeasibility of our method on downstream tasks under constrained resources. Code\nwill be available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6456
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16924",
      "authors": [
        {
          "_id": "67e230f384513315a91c5602",
          "user": {
            "_id": "664207e5af62c6c26653b369",
            "avatarUrl": "/avatars/031453340e114efb10b7d650e1d1c384.svg",
            "isPro": false,
            "fullname": "Joo Chan Lee",
            "user": "maincold2",
            "type": "user"
          },
          "name": "Joo Chan Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:07.628Z",
          "hidden": false
        },
        {
          "_id": "67e230f384513315a91c5603",
          "name": "Jong Hwan Ko",
          "hidden": false
        },
        {
          "_id": "67e230f384513315a91c5604",
          "user": {
            "_id": "655e0141d36a195f663ee4b0",
            "avatarUrl": "/avatars/97bb695ccefdcb2139b94bcae808cf99.svg",
            "isPro": false,
            "fullname": "Eunbyung Park",
            "user": "epark",
            "type": "user"
          },
          "name": "Eunbyung Park",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-25T09:25:09.995Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T07:41:45.000Z",
      "submittedOnDailyAt": "2025-03-25T02:59:24.076Z",
      "title": "Optimización del mínimo 3D gaussiano de espuma",
      "submittedOnDailyBy": {
        "_id": "664207e5af62c6c26653b369",
        "avatarUrl": "/avatars/031453340e114efb10b7d650e1d1c384.svg",
        "isPro": false,
        "fullname": "Joo Chan Lee",
        "user": "maincold2",
        "type": "user"
      },
      "summary": "3D Gaussian Splatting (3DGS) ha surgido como una técnica renovadora que permite renderizar imágenes de alta eficiencia en tiempo, presentando una fuerte representación aplicable en una amplia gama de aplicaciones. Sin embargo, la representación explícita de la visión 3D a través de Gaussian primitives exige un gran espacio de almacenamiento y sobrecarga de memoria. Según los últimos estudios, se ha demostrado que se puede reducir significativamente el número de Gaussianes para lograr un rendimiento de alta calidad, pero los métodos de compresión actuales de 3DGS se centran principalmente en la compresión de propiedades, lo que implica el uso de un número relativamente alto de Gaussianes. Esto puede hacer que un pequeño conjunto de Gaussianes sea vulnerable a compresión ineficientes de propiedades y puede llevar a una notable pérdida de calidad. El número de Gaussianes está directamente relacionado con el costo de cálculo, y es crucial reducirlo eficazmente para mejorar la eficiencia de almacenamiento. En este artículo, se propone la representación Optimized Minimal Gaussians (OMG). OMG reduce significativamente el espacio de almacenamiento mientras utiliza un número mínimo de primitives. Primero, se identifican los Gaussianes que difieren significativamente de los cercanos, minimizando partes innecesarias sin sacrificar la calidad. Luego, se propone una representación eficiente de la continuidad y discontinuidad de los primitives, así como una representación precisa de las propiedades. Además, se mejora la representación de la discontinuidad mediante la técnica de sub-vector quantización, manteniendo un entrenamiento rápido independiente del tamaño del códigobook. Según los experimentos extendidos, OMG reduce aproximadamente el 50% del espacio de almacenamiento necesario en comparación con los límites actuales, permitiendo un rendimiento de alta calidad y un rendimiento de más de 600 FPS. El código fuente está disponible en https://maincold2.github.io/omg/.",
      "upvotes": 1,
      "discussionId": "67e230f484513315a91c5678",
      "projectPage": "https://maincold2.github.io/omg/",
      "githubRepo": "https://github.com/maincold2/OMG",
      "ai_keywords": [
        "Gaussian Splatting (3DGS)",
        "real-time",
        "high-performance rendering",
        "3D scenes",
        "explicit Gaussian primitives",
        "storage",
        "memory overhead",
        "high-quality rendering",
        "attribute compression",
        "quality degradation",
        "computational costs",
        "Optimized Minimal Gaussians representation (OMG)",
        "distinct Gaussian",
        "redundancy",
        "attribute representation",
        "continuity",
        "irregularity",
        "sub-vector quantization",
        "codebook size",
        "FPS rendering",
        "rendering quality"
      ]
    },
    "publishedAt": "2025-03-21T03:41:45.000Z",
    "title": "Optimized Minimal 3D Gaussian Splatting",
    "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for\nreal-time, high-performance rendering, enabling a wide range of applications.\nHowever, representing 3D scenes with numerous explicit Gaussian primitives\nimposes significant storage and memory overhead. Recent studies have shown that\nhigh-quality rendering can be achieved with a substantially reduced number of\nGaussians when represented with high-precision attributes. Nevertheless,\nexisting 3DGS compression methods still rely on a relatively large number of\nGaussians, focusing primarily on attribute compression. This is because a\nsmaller set of Gaussians becomes increasingly sensitive to lossy attribute\ncompression, leading to severe quality degradation. Since the number of\nGaussians is directly tied to computational costs, it is essential to reduce\nthe number of Gaussians effectively rather than only optimizing storage. In\nthis paper, we propose Optimized Minimal Gaussians representation (OMG), which\nsignificantly reduces storage while using a minimal number of primitives.\nFirst, we determine the distinct Gaussian from the near ones, minimizing\nredundancy without sacrificing quality. Second, we propose a compact and\nprecise attribute representation that efficiently captures both continuity and\nirregularity among primitives. Additionally, we propose a sub-vector\nquantization technique for improved irregularity representation, maintaining\nfast training with a negligible codebook size. Extensive experiments\ndemonstrate that OMG reduces storage requirements by nearly 50% compared to the\nprevious state-of-the-art and enables 600+ FPS rendering while maintaining high\nrendering quality. Our source code is available at\nhttps://maincold2.github.io/omg/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16924.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "664207e5af62c6c26653b369",
      "avatarUrl": "/avatars/031453340e114efb10b7d650e1d1c384.svg",
      "fullname": "Joo Chan Lee",
      "name": "maincold2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18494",
      "authors": [
        {
          "_id": "67e21a81e2e69ea26eee4f67",
          "user": {
            "_id": "65bef46337491e7adc5ee7c9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bQ2WY0bU65IQ3IBn6SrnA.png",
            "isPro": false,
            "fullname": "Hao-Yuan Chen",
            "user": "MarkChenX",
            "type": "user"
          },
          "name": "Hao-Yuan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:20:30.277Z",
          "hidden": false
        },
        {
          "_id": "67e21a81e2e69ea26eee4f68",
          "name": "Cheng-Pong Huang",
          "hidden": false
        },
        {
          "_id": "67e21a81e2e69ea26eee4f69",
          "name": "Jui-Ming Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/kVdnqHbvP4VXfEWb2MAs_.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/amlMqiikDojWAwsnS3d5e.png",
        "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/cndDykrtJViO6gRavkF9D.png"
      ],
      "publishedAt": "2025-03-24T09:48:59.000Z",
      "submittedOnDailyAt": "2025-03-25T06:52:38.289Z",
      "title": "Mejora de los agentes de codificación por medio de procesamiento del lenguaje",
      "submittedOnDailyBy": {
        "_id": "65bef46337491e7adc5ee7c9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bQ2WY0bU65IQ3IBn6SrnA.png",
        "isPro": false,
        "fullname": "Hao-Yuan Chen",
        "user": "MarkChenX",
        "type": "user"
      },
      "summary": "El surgimiento de lenguajes de código de alto nivel y la aplicación de agentes de IA han llevado a un gran avance en los más avanzados marcadores de código generación, cambiando así las tareas de desarrollo de software. Sin embargo, incluso utilizando modelos de razonamiento que se calculan en el momento del test, estos sistemas enfrentan desafíos en la complejidad del desarrollo de software. En este artículo, se presenta un sistema de entendimiento de código y razonamiento de agente, llamado CURA, que se ha fortalecido con la función de Visualización de Procesos Lingüísticos (VPS). Este sistema ha logrado mejorar los modelos de referencia en marcadores difíciles como BigCodeBench en un 3.65%. Además, la combinación de CURA, el modelo o3-mini y el método de VPS ha permitido alcanzar los mejores resultados. Esta investigación avanza en la integración de la arquitectura de generación de código basada en modelos de lenguaje de alto nivel y la razonamiento de agente, permitiendo resolver tareas complejas de desarrollo de software utilizando la razonamiento agente de modelos de lenguaje.",
      "upvotes": 0,
      "discussionId": "67e21a82e2e69ea26eee4fba",
      "ai_keywords": [
        "large language models (LLMs)",
        "code generation",
        "software engineering tasks",
        "CURA",
        "code understanding and reasoning agent system",
        "verbal process supervision (VPS)",
        "BigCodeBench",
        "o3-mini model",
        "reasoning-driven architectures",
        "agentic reasoning"
      ]
    },
    "publishedAt": "2025-03-24T05:48:59.000Z",
    "title": "Verbal Process Supervision Elicits Better Coding Agents",
    "summary": "The emergence of large language models and their applications as AI agents\nhave significantly advanced state-of-the-art code generation benchmarks,\ntransforming modern software engineering tasks. However, even with test-time\ncomputed reasoning models, these systems still struggle with complex software\nengineering challenges. This work introduces CURA, a code understanding and\nreasoning agent system enhanced with verbal process supervision (VPS),\nachieving a 3.65\\% improvement over baseline models on challenging benchmarks\nlike BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and\nVPS techniques, attains state-of-the-art performance. This work represents a\nstep forward in integrating reasoning-driven architectures with LLM-based code\ngeneration, enabling agentic reasoning for language models to solve complex\nsoftware engineering tasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/kVdnqHbvP4VXfEWb2MAs_.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/amlMqiikDojWAwsnS3d5e.png",
      "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/cndDykrtJViO6gRavkF9D.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18494.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65bef46337491e7adc5ee7c9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bQ2WY0bU65IQ3IBn6SrnA.png",
      "fullname": "Hao-Yuan Chen",
      "name": "MarkChenX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]