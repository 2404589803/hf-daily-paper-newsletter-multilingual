[
  {
    "paper": {
      "id": "2504.17761",
      "authors": [
        {
          "_id": "680af2df3b93130c9b2b90a7",
          "name": "Shiyu Liu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90a8",
          "name": "Yucheng Han",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90a9",
          "name": "Peng Xing",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90aa",
          "name": "Fukun Yin",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ab",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ac",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:36.757Z",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ad",
          "name": "Jiaqi Liao",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ae",
          "name": "Yingming Wang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90af",
          "name": "Honghao Fu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b0",
          "name": "Chunrui Han",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b1",
          "name": "Guopeng Li",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b2",
          "name": "Yuang Peng",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b3",
          "name": "Quan Sun",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b4",
          "name": "Jingwei Wu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b5",
          "name": "Yan Cai",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b6",
          "name": "Zheng Ge",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b7",
          "name": "Ranchen Ming",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b8",
          "name": "Lei Xia",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b9",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ba",
          "name": "Yibo Zhu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bb",
          "name": "Binxing Jiao",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bc",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bd",
          "user": {
            "_id": "63417332c5565a4b8d43a0d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
            "isPro": false,
            "fullname": "Gang Yu",
            "user": "skicy",
            "type": "user"
          },
          "name": "Gang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:34.650Z",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90be",
          "name": "Daxin Jiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/lRHqqMDr1SxDfhelcO26J.mp4"
      ],
      "publishedAt": "2025-04-24T17:25:12.000Z",
      "submittedOnDailyAt": "2025-04-25T01:12:44.269Z",
      "title": "Step 1X-Edit: Marco práctico de edición de imágenes generales",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "Recientemente, los modelos de edición de imágenes están demostrando un desarrollo impresionante y rápido. La publicación de modelos avanzados como GPT-4o y Gemini2 Flash ha introducido altas capacidades de edición de imágenes. Estos modelos muestran una excelente capacidad para satisfacer las solicitudes de edición que los usuarios requieren y han demostrado un gran avance en el campo de la manipulación de imágenes. Sin embargo, existen notables diferencias con respecto a modelos de fuente cerrada. Por lo tanto, en este artículo se presenta Step1X-Edit, el modelo de edición de imágenes más avanzado, con el objetivo de ofrecer un rendimiento comparable a los modelos de fuente cerrada. Concretamente, se procesan las imágenes de referencia y las instrucciones de edición del usuario mediante un Multimodal LLM, se extraen los embbedinges potenciales y se integran con un decoder de imágenes de diferenciación para obtener la imagen objetivo. Para el entrenamiento del modelo, se construyó una pipila de generación de datos de alta calidad, y para la evaluación se desarrolló GEdit-Bench, un nuevo criterio de evaluación basado en instrucciones de usuario reales. Los resultados de las pruebas en GEdit-Bench muestran que Step1X-Edit destaca significativamente en comparación con los límites de base de los modelos de fuente abierta y se aproxima a los rendimientos de los modelos avanzados de fuente cerrada, contribuyendo de manera significativa al campo de la edición de imágenes.",
      "upvotes": 38,
      "discussionId": "680af2e13b93130c9b2b9132",
      "githubRepo": "https://github.com/stepfun-ai/Step1X-Edit",
      "ai_keywords": [
        "Multimodal LLM",
        "latent embedding",
        "diffusion image decoder",
        "data generation pipeline",
        "GEdit-Bench",
        "real-world user instructions"
      ]
    },
    "publishedAt": "2025-04-24T13:25:12.000Z",
    "title": "Step1X-Edit: A Practical Framework for General Image Editing",
    "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/lRHqqMDr1SxDfhelcO26J.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17761.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17502",
      "authors": [
        {
          "_id": "680b44fb426b7d5bc2018c75",
          "user": {
            "_id": "631da07f6d6a5870f3d2c375",
            "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
            "isPro": false,
            "fullname": "Aviv Slobodkin",
            "user": "lovodkin93",
            "type": "user"
          },
          "name": "Aviv Slobodkin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-25T08:17:03.471Z",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c76",
          "name": "Hagai Taitelbaum",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c77",
          "name": "Yonatan Bitton",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c78",
          "name": "Brian Gordon",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c79",
          "name": "Michal Sokolik",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7a",
          "name": "Nitzan Bitton Guetta",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7b",
          "name": "Almog Gueta",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7c",
          "name": "Royi Rassin",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7d",
          "name": "Itay Laish",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7e",
          "name": "Dani Lischinski",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7f",
          "name": "Idan Szpektor",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T12:44:51.000Z",
      "submittedOnDailyAt": "2025-04-25T06:50:21.552Z",
      "title": "RefVNLI: Investigación sobre la conversión de frases de tema en imágenes para evaluaciones intercambiables",
      "submittedOnDailyBy": {
        "_id": "631da07f6d6a5870f3d2c375",
        "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
        "isPro": false,
        "fullname": "Aviv Slobodkin",
        "user": "lovodkin93",
        "type": "user"
      },
      "summary": "La generación de imágenes basada en texto (T2I) en un texto principal tiene como objetivo crear imágenes que coincidan con la descripción textual dada, mientras mantiene la identidad visual de las imágenes de referencia. Su aplicación ampliada abarca desde la personalización de la generación de imágenes hasta la representación coherente de personajes en renderizaciones de videos. Sin embargo, su desarrollo está limitado por la falta de evaluaciones automáticas confiables. Los métodos actuales solo evalúan partes de la tarea y no pueden evaluar aspectos como la coincidencia con el texto o la preservación del tema, dependiendo en ocasiones de juicios humanos o de APIs costosos. En respuesta a esto, presentamos RefVNLI. RefVNLI es un métrico de bajo costo que evalúa de manera simultánea la coincidencia del texto y la preservación del tema en una sola predicción. Se ha entrenado en un marco de referencia de lógica de video y en grandes conjuntos de datos y imágenes variadas. RefVNLI supera o alcanza niveles similares a los actuales, obteniendo ganancias máximas de 6.4 puntos en la coincidencia del texto y 8.5 puntos en la preservación del tema. Además, muestra excelente performance con conceptos desconocidos, alcanzando una precisión del 87% o más, lo que se alinea con las preferencias humanas.",
      "upvotes": 35,
      "discussionId": "680b44ff426b7d5bc2018d85",
      "ai_keywords": [
        "RefVNLI",
        "video-reasoning benchmarks",
        "image perturbations"
      ]
    },
    "publishedAt": "2025-04-24T08:44:51.000Z",
    "title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image\n  Generation",
    "summary": "Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability --\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering -- progress in this field is\nlimited by the lack of reliable automatic evaluation. Existing methods either\nassess only one aspect of the task (i.e., textual alignment or subject\npreservation), misalign with human judgments, or rely on costly API-based\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\nevaluates both textual alignment and subject preservation in a single\nprediction. Trained on a large-scale dataset derived from video-reasoning\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\nbaselines across multiple benchmarks and subject categories (e.g.,\nAnimal, Object), achieving up to 6.4-point gains in textual\nalignment and 8.5-point gains in subject consistency. It also excels with\nlesser-known concepts, aligning with human preferences at over 87\\% accuracy.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631da07f6d6a5870f3d2c375",
      "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
      "fullname": "Aviv Slobodkin",
      "name": "lovodkin93",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17192",
      "authors": [
        {
          "_id": "680aee7bcf67477f2c00ca53",
          "user": {
            "_id": "64f7bf0c7565a69eb693ad1f",
            "avatarUrl": "/avatars/aba6910aa39a3437a7f0df3f5cd49e6d.svg",
            "isPro": false,
            "fullname": "minju",
            "user": "iaminju",
            "type": "user"
          },
          "name": "Minju Seo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:41.304Z",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca54",
          "user": {
            "_id": "63036b6c5c70c21d0ea79d48",
            "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
            "isPro": false,
            "fullname": "Jinheon Baek",
            "user": "jinheon",
            "type": "user"
          },
          "name": "Jinheon Baek",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:38.982Z",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca55",
          "name": "Seongyun Lee",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca56",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T01:57:01.000Z",
      "submittedOnDailyAt": "2025-04-25T04:17:48.790Z",
      "title": "Paper2Code: Generación automática de código a partir de artículos científicos (aprendizaje profundo)",
      "submittedOnDailyBy": {
        "_id": "6550c4f27bbfce1878f5f280",
        "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
        "isPro": false,
        "fullname": "seongyun_lee",
        "user": "Seongyun",
        "type": "user"
      },
      "summary": "El rápido desarrollo de la investigación en aprendizaje automático (Machine Learning) ha llevado a que la implementación de código para responder a estos avances sea generalmente impracticable. Por lo tanto, los investigadores se enfrentan a tiempos largos y esfuerzos significativos para reproducir resultados y expandir estudios previos. En cambio, los modelos de lenguaje grandes (LLMs) recientes tienen la capacidad de comprender artículos científicos y generar código de alta calidad. En este contexto, se propone PaperCoder, un marco de trabajo de agentes LLM que transforma artículos de aprendizaje automático en repositorios de código funcional. PaperCoder funciona en tres etapas: planificación, análisis y generación. En la etapa de planificación, se construye un mapa de alto nivel, se diseña una arquitectura del sistema en diagramas y se especifican las dependencias entre archivos y se crean archivos de configuración. En la etapa de análisis, se interpretan detalles específicos adaptados a la implementación, y en la etapa de generación se producen códigos modulares que conocen las dependencias. Además, cada etapa se instancia con un conjunto de agentes profesionales que colaboran efectivamente en la pipeline. Además, PaperCoder evalua la generación de código a partir de artículos de aprendizaje automático utilizando evaluaciones basadas en modelos y evaluaciones humanas, y muestra su eficacia al demostrar resultados que pueden ser utilizados por los autores originales de los artículos. Además, muestra una capacidad muy fuerte al superar significativamente los límites de PaperBench, un marco de referencia reciente.",
      "upvotes": 31,
      "discussionId": "680aee7dcf67477f2c00ca96",
      "githubRepo": "https://github.com/going-doer/Paper2Code"
    },
    "publishedAt": "2025-04-23T21:57:01.000Z",
    "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning",
    "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17192.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6550c4f27bbfce1878f5f280",
      "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
      "fullname": "seongyun_lee",
      "name": "Seongyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17432",
      "authors": [
        {
          "_id": "680adfbe464a44cea0b843c1",
          "name": "Tiancheng Gu",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c2",
          "user": {
            "_id": "63e202f352b7578dba448ab5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
            "isPro": false,
            "fullname": "Yang",
            "user": "Kaichengalex",
            "type": "user"
          },
          "name": "Kaicheng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:46.935Z",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c3",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c4",
          "name": "Xingjun Wang",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c5",
          "name": "Yanzhao Zhang",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c6",
          "name": "Dingkun Long",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c7",
          "name": "Yingda Chen",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c8",
          "name": "Weidong Cai",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c9",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T10:51:52.000Z",
      "submittedOnDailyAt": "2025-04-25T01:11:53.967Z",
      "title": "El muro de módulos roto: aprendizaje interno generalizado con módulos de diferenciación de niveles de LLM",
      "submittedOnDailyBy": {
        "_id": "63e202f352b7578dba448ab5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
        "isPro": false,
        "fullname": "Yang",
        "user": "Kaichengalex",
        "type": "user"
      },
      "summary": "El marco de trabajo CLIP (Contrastive Language-Image Pre-training) se utiliza principalmente para la búsqueda y agrupamiento de documentos de imagen, pero su eficacia está limitada debido a tres factores: la transacción de tokens de documento, la codificación de documentos de imagen separados y las fallas estructurales causadas por la acción palabra por palabra. Los modelos de lenguaje multimodal generalizados (MLLM) han demostrado un gran avance en la comprensión general del lenguaje visual, pero la posibilidad de aprendizaje de representaciones dinámicas de múltiples modelos aún requiere más investigación. En este artículo, se propone un nuevo marco de trabajo de dos etapas, Universal Multimodal Embedding (UniME), para entrenar expresiones judiciales en diferentes tareas posteriores utilizando MLLM. En la primera etapa, se aprende la capacidad de incorporar conocimientos judiciales de la comprensión del lenguaje a través de un modelo profesional basado en una fuerte LLM. En la segunda etapa, se introduce una regulación de instrucciones con ejemplos negativos fortalecidos para fomentar el aprendizaje de expresiones judiciales. Específicamente, se intenta reducir la contaminación de ejemplos negativos erroneos al principio y se muestran varios ejemplos negativos difíciles para cada instancia dentro de cada batch para centrar el modelo en muestras difíciles. Este enfoque mejora la capacidad de judicio y aumenta la capacidad de seguimiento de instrucciones en tareas posteriores. Se realizan experimentos extendidos en el benchmark MMEB y varias tareas de búsqueda, mostrando un mejoramiento uniforme en todas las tareas y demostrando excelentes capacidades de judicio y estructura.",
      "upvotes": 21,
      "discussionId": "680adfbf464a44cea0b8440f",
      "projectPage": "https://garygutc.github.io/UniME/",
      "githubRepo": "https://github.com/deepglint/UniME",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training (CLIP)",
        "Multimodal Large Language Models (MLLMs)",
        "Generalized vision-language understanding",
        "UniME (Universal Multimodal Embedding)",
        "Discriminative representations",
        "Textual discriminative knowledge distillation",
        "LLM-based teacher model",
        "Hard negative enhanced instruction tuning",
        "False negative contamination",
        "Challenging samples",
        "Discriminative power",
        "Instruction-following ability",
        "MMEB benchmark",
        "Short caption retrieval",
        "Long caption retrieval",
        "Compositional retrieval"
      ]
    },
    "publishedAt": "2025-04-24T06:51:52.000Z",
    "title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs",
    "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17432.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17207",
      "authors": [
        {
          "_id": "680af2bf2fa10fbf21684bde",
          "name": "Phillip Y. Lee",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684bdf",
          "name": "Jihyeon Je",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be0",
          "name": "Chanho Park",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be1",
          "name": "Mikaela Angelina Uy",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be2",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be3",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T02:41:34.000Z",
      "submittedOnDailyAt": "2025-04-25T00:59:29.327Z",
      "title": "Simulación de imágenes psicológicas a través de la teoría de los puntos de vista (Simulación de imágenes psicológicas a través de la teoría de los puntos de vista)",
      "submittedOnDailyBy": {
        "_id": "6342796a0875f2c99cfd313b",
        "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
        "isPro": false,
        "fullname": "Yuseung \"Phillip\" Lee",
        "user": "phillipinseoul",
        "type": "user"
      },
      "summary": "Proponemos un marco de trabajo utilizando un modelo de lenguaje visual (VLMs) para la reconocimiento de relaciones visuales con el uso de la memoria imaginativa. El reconocimiento de relaciones visuales es un criterio importante para una comprensión visual humana, y es necesario para la interacción con el entorno y el colaboración de los agentes de vehículos autónomos. Los recientes estudios han demostrado que, a medida que los VLMs mejoran su reconocimiento espacial, tienen una notable falta de capacidades en el reconocimiento de relaciones visuales y una tendencia hacia una interpretación centralizada. Enfatizamos el papel de la memoria imaginativa para cerrar la brecha entre el reconocimiento visual de los VLMs y el cognitivo humano. La humanidad ve el mundo a través de representaciones abstractas, lo que fomenta la movilidad visual. Basándonos en esto, proponemos un marco de trabajo llamado \"Transformación de Relaciones Visuales Abstractas (TRA)\" que utiliza modelos basados en la visión (busqueda de objetos, segmentación, medición de dirección) para facilitar la abstractización espacial y la transformación de relaciones visuales. Mediante experimentos en marcos de referencia de imágenes sintéticas y reales, demostramos una mejora significativa en el reconocimiento de relaciones visuales comparado con los VLMs, mostrando que el modelo de ajuste microespacial y el nuevo enfoque basado en la síntesis de puntos visuales superan aún más.",
      "upvotes": 14,
      "discussionId": "680af2c02fa10fbf21684c1f",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "mental imagery simulation",
        "perspective-taking",
        "visual understanding",
        "environmental interaction",
        "autonomous agents",
        "spatial reasoning",
        "perspective-aware reasoning capabilities",
        "egocentric interpretations",
        "mental imagery",
        "scene abstractions",
        "perspective transformations",
        "object detection",
        "segmentation",
        "orientation estimation",
        "synthetic benchmarks",
        "real-image benchmarks",
        "fine-tuned spatial reasoning models",
        "novel-view-synthesis-based approaches"
      ]
    },
    "publishedAt": "2025-04-23T22:41:34.000Z",
    "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery\n  Simulation",
    "summary": "We present a framework for perspective-aware reasoning in vision-language\nmodels (VLMs) through mental imagery simulation. Perspective-taking, the\nability to perceive an environment or situation from an alternative viewpoint,\nis a key benchmark for human-level visual understanding, essential for\nenvironmental interaction and collaboration with autonomous agents. Despite\nadvancements in spatial reasoning within VLMs, recent research has shown that\nmodern VLMs significantly lack perspective-aware reasoning capabilities and\nexhibit a strong bias toward egocentric interpretations. To bridge the gap\nbetween VLMs and human perception, we focus on the role of mental imagery,\nwhere humans perceive the world through abstracted representations that\nfacilitate perspective shifts. Motivated by this, we propose a framework for\nperspective-aware reasoning, named Abstract Perspective Change (APC), that\neffectively leverages vision foundation models, such as object detection,\nsegmentation, and orientation estimation, to construct scene abstractions and\nenable perspective transformations. Our experiments on synthetic and real-image\nbenchmarks, compared with various VLMs, demonstrate significant improvements in\nperspective-aware reasoning with our framework, further outperforming\nfine-tuned spatial reasoning models and novel-view-synthesis-based approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17207.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6342796a0875f2c99cfd313b",
      "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
      "fullname": "Yuseung \"Phillip\" Lee",
      "name": "phillipinseoul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16511",
      "authors": [
        {
          "_id": "680b2a95c94724c1465c20dd",
          "name": "Fengze Liu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20de",
          "name": "Weidong Zhou",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20df",
          "name": "Binbin Liu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e0",
          "name": "Zhimiao Yu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e1",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e2",
          "name": "Haobin Lin",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e3",
          "name": "Yifeng Yu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e4",
          "name": "Xiaohuan Zhou",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e5",
          "name": "Taifeng Wang",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e6",
          "name": "Yong Cao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T08:36:50.000Z",
      "submittedOnDailyAt": "2025-04-25T04:55:18.773Z",
      "title": "QuaDMix: Selección de datos para entrenamiento de LLM eficiente basada en el equilibrio de masa y diversidad",
      "submittedOnDailyBy": {
        "_id": "668f5875b5b3081d776e4094",
        "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
        "isPro": false,
        "fullname": "Xiaohuan Zhou",
        "user": "XiaohuanZhou",
        "type": "user"
      },
      "summary": "La calidad y la diversidad son dos indicadores importantes en los datos de entrenamiento de modelos de lenguaje grandes (LLMs) y tienen un impacto positivo en el mejoramiento de la eficiencia. Actualmente, los estudios optimizan estas indicadores individualmente, pero generalmente, realizan filtrados de calidad y ajustan después la proporción de los datos. Sin embargo, estas aproximaciones ignoran el trade-off inherente entre calidad y diversidad. Es crucial evaluar la calidad de cada punto de datos y el efecto complementario de la colección de datos en su conjunto. En este artículo, se propone un marco de selección de datos integrado llamado QuaDMix, que optimiza automáticamente la distribución de datos para el entrenamiento previo de LLMs, al mismo tiempo que equilibra calidad y diversidad. Específicamente, se propone múltiples criterios para medir la calidad de los datos y se utiliza la clasificación de regiones para diferenciar los puntos de datos, así como medir la diversidad de la colección. QuaDMix utiliza una función de sampling paramétrica para determinar la probabilidad de muestreo de cada punto de datos basada en etiquetas relacionadas con calidad y diversidad. Para acelerar la exploración de parámetros óptimos del marco de QuaDMix, se realizan experimentos computacionales con pequeños modelos y se utiliza RegMix, que implementa LightGBM. Los resultados de los experimentos en un amplio rango de modelos y conjuntos de datos muestran que QuaDMix logra un aumento de eficiencia promedio del 7.2%. Estos resultados demuestran que QuaDMix es superior a estrategias independientes de calidad y diversidad, y resalta la necesidad y posibilidad de equilibrio entre la calidad y la diversidad de los datos.",
      "upvotes": 13,
      "discussionId": "680b2a97c94724c1465c21a3",
      "ai_keywords": [
        "large language models (LLMs)",
        "QuaDMix",
        "data selection framework",
        "parameterized data sampling function",
        "domain classification",
        "LightGBM",
        "RegMix"
      ]
    },
    "publishedAt": "2025-04-23T04:36:50.000Z",
    "title": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining",
    "summary": "Quality and diversity are two critical metrics for the training data of large\nlanguage models (LLMs), positively impacting performance. Existing studies\noften optimize these metrics separately, typically by first applying quality\nfiltering and then adjusting data proportions. However, these approaches\noverlook the inherent trade-off between quality and diversity, necessitating\ntheir joint consideration. Given a fixed training quota, it is essential to\nevaluate both the quality of each data point and its complementary effect on\nthe overall dataset. In this paper, we introduce a unified data selection\nframework called QuaDMix, which automatically optimizes the data distribution\nfor LLM pretraining while balancing both quality and diversity. Specifically,\nwe first propose multiple criteria to measure data quality and employ domain\nclassification to distinguish data points, thereby measuring overall diversity.\nQuaDMix then employs a unified parameterized data sampling function that\ndetermines the sampling probability of each data point based on these quality\nand diversity related labels. To accelerate the search for the optimal\nparameters involved in the QuaDMix framework, we conduct simulated experiments\non smaller models and use LightGBM for parameters searching, inspired by the\nRegMix method. Our experiments across diverse models and datasets demonstrate\nthat QuaDMix achieves an average performance improvement of 7.2% across\nmultiple benchmarks. These results outperform the independent strategies for\nquality and diversity, highlighting the necessity and ability to balance data\nquality and diversity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16511.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668f5875b5b3081d776e4094",
      "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
      "fullname": "Xiaohuan Zhou",
      "name": "XiaohuanZhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17789",
      "authors": [
        {
          "_id": "680b318bbbebf87944bc9595",
          "name": "Xu Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9596",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9597",
          "name": "Haoyu Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9598",
          "name": "Hao Tang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9599",
          "name": "Chih-Yao Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959a",
          "name": "Jialiang Wang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959b",
          "name": "Kunpeng Li",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959c",
          "name": "Xiaoliang Dai",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959d",
          "name": "Yujun Shi",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959e",
          "name": "Xuan Ju",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959f",
          "name": "Yushi Hu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a0",
          "name": "Artsiom Sanakoyeu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a1",
          "name": "Felix Juefei-Xu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a2",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a3",
          "name": "Junjiao Tian",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a4",
          "name": "Tao Xu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a5",
          "name": "Tingbo Hou",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a6",
          "name": "Yen-Cheng Liu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a7",
          "name": "Zecheng He",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a8",
          "name": "Zijian He",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a9",
          "name": "Matt Feiszli",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95aa",
          "name": "Peizhao Zhang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ab",
          "name": "Peter Vajda",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ac",
          "name": "Sam Tsai",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ad",
          "name": "Yun Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-25T05:28:51.493Z",
      "title": "Token Shuffle: Un Estudio sobre la Generación de Imágenes de Alta Resolución Utilizando Modelos Automáticos de Regresión",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "El modelo de auto-regresión (AR) ha sido dominante en la generación de lenguaje a largo plazo y se está aplicando cada vez más en la síntesis de imágenes, aunque comparado con modelos basados en dispersión, consideramos que tiene muchos problemas. Uno de los principales límites es que el número de tokens de imagen que requiere el modelo AR es muy elevado, lo que impide una eficiencia en la entrenamiento y la inferencia y limita la resolución de las imágenes. Para resolver este problema, proponemos el método de shuffle de tokens. Este es un nuevo método pero sencillo y reduce el número de tokens de imágen en modelos transformadores. Nuestra idea principal es la redundancia de la dimensión del vocabulario visual en modelos multimodales de lenguaje (MLLMs). En este contexto, códigos visuales de baja dimensión en el encoder visual se mapean directamente a vocabularios de lenguaje de alta dimensión. Utilizando esto, consideramos dos tareas principales: shuffle de tokens y mezcla de tokens. El shuffle de tokens integra tokens locales espacialmente en la dimensión espectral y reduce el número de tokens de entrada. La mezcla de tokens conecta los tokens inferidos después de los bloques de transformador y recupera la secuencia espacial de la salida. La entrenamiento junto con un planteamiento de contexto es parte de nuestra estrategia, ya que no requiere un codificador de contexto adicional entrenado. De esta manera, MLLMs pueden predecir el siguiente token de una forma integrada de lenguaje y imagen, permitiendo la síntesis de imágenes de alta resolución. Por primera vez, hemos llegado a promover la frontera de la generación de imágenes desde texto AR hasta una resolución de 2048x2048, obteniendo resultados satisfactorios. En el benchmark GenAI, nuestro modelo de 2.7B alcanzó un puntaje total de 0.77 en planteamientos difíciles, superando al modelo AR LlamaGen con un puntaje de 0.18 y al modelo basado en dispersión LDM con un puntaje de 0.15. También en evaluaciones de escala humana, nuestra capacidad de generación de imágenes notable en correspondencia lingüística, defectos visuales y apariencia visual se ha demostrado. Esperamos que el shuffle de tokens sea la base fundamental para la generación eficiente de imágenes de alta resolución en MLLMs.",
      "upvotes": 4,
      "discussionId": "680b3191bbebf87944bc9739",
      "ai_keywords": [
        "autoregressive (AR) models",
        "image synthesis",
        "diffusion-based models",
        "image tokens",
        "training and inference efficiency",
        "Transformer",
        "dimensional redundancy",
        "visual vocabularies",
        "Multimodal Large Language Models (MLLMs)",
        "visual encoder",
        "high-dimensional language vocabularies",
        "token-shuffle",
        "spatially local tokens",
        "channel dimension",
        "token-unshuffle",
        "spatial arrangement",
        "unified next-token prediction",
        "text-to-image generation",
        "resolution",
        "generation performance",
        "GenAI-benchmark",
        "textual prompts",
        "pretrained text-encoder",
        "text-alignment",
        "visual flaw",
        "visual appearance"
      ]
    },
    "publishedAt": "2025-04-24T13:59:56.000Z",
    "title": "Token-Shuffle: Towards High-Resolution Image Generation with\n  Autoregressive Models",
    "summary": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17789.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17069",
      "authors": [
        {
          "_id": "680b1b33388bb2cfd497ebdb",
          "user": {
            "_id": "62bb84f82ada492aa5775709",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62bb84f82ada492aa5775709/jv4yKL75t8QzHDHLbhBPT.png",
            "isPro": false,
            "fullname": "Rishav Pramanik",
            "user": "rishavpramanik",
            "type": "user"
          },
          "name": "Rishav Pramanik",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:27.724Z",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdc",
          "name": "Antoine Poupon",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdd",
          "name": "Juan A. Rodriguez",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebde",
          "name": "Masih Aminbeidokhti",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdf",
          "name": "David Vazquez",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe0",
          "name": "Christopher Pal",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe1",
          "name": "Zhaozheng Yin",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe2",
          "name": "Marco Pedersoli",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/NUImFkmaqDnfEIBB2l94Q.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/OuC3dzu5XCUb8dxTlVf72.png"
      ],
      "publishedAt": "2025-04-23T19:33:58.000Z",
      "submittedOnDailyAt": "2025-04-25T03:50:09.534Z",
      "title": "Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automática simple. Usamos una función automáti",
      "submittedOnDailyBy": {
        "_id": "63a614d264f470027818b066",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
        "isPro": false,
        "fullname": "Juan A. Rodriguez",
        "user": "joanrodai",
        "type": "user"
      },
      "summary": "Automática generación de imágenes basada en formas secuenciales ha demostrado resultados relativamente excelentes en calidad de imagen y calidad de escalabilidad comparados con otros métodos recientes. Estos modelos pueden ser fácilmente integrados dentro de modelos de lenguaje visual y también son escalables. Sin embargo, los modelos secuenciales requieren una secuencia definida para la generación de las formas. Una secuencia natural se basa en instrucciones de palabras. Esto tiene significado en la generación de texto pero no existe una secuencia inherente para la generación de imágenes. Tradicionalmente, el orden de raster escaneo (desde la esquina superior izquierda hacia la esquina inferior derecha) guia a los modelos secuenciales de generación de imágenes. En este artículo, se argumenta que esta secuencia no es la más adecuada y que no respeta la causalidad de los contenidos de las imágenes: por ejemplo, un modelo secuencial automático podría crear nubes antes del sol basándose en la explicación visual del atardecer. Sin embargo, el color de las nubes debe determinarse por el color del sol, lo que significa que la secuencia actual es inversa. En este artículo, se muestra que se puede entrenar la secuencia de generación de las formas de manera arbitraria para inferir el contenido y la posición (orden) de las formas durante el proceso de generación. Además, se demuestra que con esta secuencia extraída, se puede ajustar un modelo de secuencia arbitraria para generar imágenes de mejor calidad. Las experimentaciones muestran que esta nueva metodología de generación produce imágenes más buenas que la tradicional raster escaneo en dos conjuntos de datos, sin necesidad de mayores costos de entrenamiento ni análisis adicionales.",
      "upvotes": 4,
      "discussionId": "680b1b35388bb2cfd497ec76",
      "ai_keywords": [
        "autoregressive patch-based image generation",
        "Vision-Language models",
        "raster-scan order",
        "causality",
        "any-given-order",
        "patch content",
        "patch location",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-04-23T15:33:58.000Z",
    "title": "Distilling semantically aware orders for autoregressive image generation",
    "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/NUImFkmaqDnfEIBB2l94Q.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/OuC3dzu5XCUb8dxTlVf72.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17069.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a614d264f470027818b066",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
      "fullname": "Juan A. Rodriguez",
      "name": "joanrodai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17040",
      "authors": [
        {
          "_id": "680af0c4175842e433ae348e",
          "name": "Zhenhailong Wang",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae348f",
          "name": "Senthil Purushwalkam",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3490",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3491",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3492",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3493",
          "name": "Ran Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T18:38:18.000Z",
      "submittedOnDailyAt": "2025-04-25T06:12:13.135Z",
      "title": "Métodos Eficientes para la Implementación de VLMs de Merge de Dinámico y Merge Básico Americano",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "DyMU es un marco de trabajo sin entrenamiento para reducir eficientemente la carga computacional de modelos de lenguaje visuo-lingüísticos (VLMs) y mantener altas prestaciones de tareas. Nuestro enfoque está constituido por dos principales componentes: 1. Dinamic Token Merging (DToMe) resuelve la inadecuación de la salida de longitud fija en un transformador visuo-lingüístico mediante la integración de tokens similares basada en la complejidad de la imagen. 2. View Token Unification (VTU) simula las secuencias de tokens esperadas por un modelo de lenguaje grande (LLMs) para reestructurar eficientemente las operaciones de atención en todas las secuencias, permitiendo una mejora en el rendimiento de los modelos.",
      "upvotes": 3,
      "discussionId": "680af0c7175842e433ae3544",
      "ai_keywords": [
        "Dynamic Token Merging (DToMe)",
        "Virtual Token Unmerging (VTU)",
        "vision transformers",
        "token compression",
        "attention dynamics",
        "visual encoders",
        "image complexity",
        "computational costs"
      ]
    },
    "publishedAt": "2025-04-23T14:38:18.000Z",
    "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs",
    "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17040.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16921",
      "authors": [
        {
          "_id": "680b40774d69b6950c4eabda",
          "name": "José Ángel González",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdb",
          "name": "Ian Borrego Obrador",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdc",
          "name": "Álvaro Romo Herrero",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdd",
          "name": "Areg Mikael Sarvazyan",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabde",
          "user": {
            "_id": "60f95c8fda0985b973d59d77",
            "avatarUrl": "/avatars/5606f0191b9f86e6b55f7e5ab6cc8bb6.svg",
            "isPro": false,
            "fullname": "Mara Chinea Rios",
            "user": "mchinea",
            "type": "user"
          },
          "name": "Mara Chinea-Ríos",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-25T07:58:32.606Z",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdf",
          "name": "Angelo Basile",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabe0",
          "name": "Marc Franco-Salvador",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T17:48:25.000Z",
      "submittedOnDailyAt": "2025-04-25T06:28:29.231Z",
      "title": "IberBench: Evaluación de LLM para el Idioma Ibérico",
      "submittedOnDailyBy": {
        "_id": "62308d13e4673fe4985d7fc9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655555697880-62308d13e4673fe4985d7fc9.jpeg",
        "isPro": false,
        "fullname": "Areg Mikael Sarvazyan",
        "user": "asarvazyan",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) son especialmente difíciles de evaluar en lenguas que no sean el inglés, debido a la limitada disponibilidad de datos de alta calidad. Los actuales marcos de evaluación y los leaderboards están centrados en el inglés, lo que reduce significativamente la evaluación en otros idiomas. Estos marcos de evaluación priorizan las habilidades básicas de procesamiento del lenguaje natural (NLP) en lugar de tareas industriales, y son poco flexibles y poco variados. Reconociendo estas limitaciones, presentamos IberBench. IberBench es un marco de evaluación detallado y expandible para evaluar el rendimiento de LLMs en tareas básicas e industriales de NLP en las lenguas usadas en la Península Ibérica y América Iberoamericana. IberBench integra 101 conjuntos de datos y cubre 22 categorías de tareas, como análisis de emociones, detección de ironía, resumen, entre otras. Este marco de evaluación soluciona las deficiencias del proceso actual de evaluación, así como la falta de diversidad lingüística y la falta de preparación de evaluación fija. IberBench permite actualizaciones continuas a través de la colaboración de la comunidad y la participación de expertos en el desarrollo de modelos y conjuntos de datos. Se evaluaron 23 modelos de LLM (con 100 millones a 14 mil millones de parámetros), proporcionando una comprensión experimental de sus fortalezas y debilidades. Nuestros hallazgos muestran que (i) los LLMs presentan un rendimiento inferior en tareas industriales en comparación con tareas básicas, (ii) el rendimiento promedio en gallego y vasco es bajo, (iii) en algunas tareas, los resultados son similares a los de un resultado aleatorio, y (iv) en otras tareas, el rendimiento es superior a un resultado aleatorio pero inferior a los sistemas de tareas compartidas. IberBench ofrece una implementación abierta que incluye la normalización de conjuntos de datos, hosting, evaluación incremental de LLMs y un leaderboard accesible públicamente.",
      "upvotes": 3,
      "discussionId": "680b407a4d69b6950c4eac96",
      "projectPage": "https://huggingface.co/spaces/iberbench/leaderboard",
      "githubRepo": "https://github.com/IberBench",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "sentiment and emotion analysis",
        "toxicity detection",
        "summarization",
        "IberBench",
        "dataset normalization",
        "incremental evaluation",
        "leaderboard"
      ]
    },
    "publishedAt": "2025-04-23T13:48:25.000Z",
    "title": "IberBench: LLM Evaluation on Iberian Languages",
    "summary": "Large Language Models (LLMs) remain difficult to evaluate comprehensively,\nparticularly for languages other than English, where high-quality data is often\nlimited. Existing benchmarks and leaderboards are predominantly\nEnglish-centric, with only a few addressing other languages. These benchmarks\nfall short in several key areas: they overlook the diversity of language\nvarieties, prioritize fundamental Natural Language Processing (NLP)\ncapabilities over tasks of industrial relevance, and are static. With these\naspects in mind, we present IberBench, a comprehensive and extensible benchmark\ndesigned to assess LLM performance on both fundamental and industry-relevant\nNLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.\nIberBench integrates 101 datasets from evaluation campaigns and recent\nbenchmarks, covering 22 task categories such as sentiment and emotion analysis,\ntoxicity detection, and summarization. The benchmark addresses key limitations\nin current evaluation practices, such as the lack of linguistic diversity and\nstatic evaluation setups by enabling continual updates and community-driven\nmodel and dataset submissions moderated by a committee of experts. We evaluate\n23 LLMs ranging from 100 million to 14 billion parameters and provide empirical\ninsights into their strengths and limitations. Our findings indicate that (i)\nLLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)\nperformance is on average lower for Galician and Basque, (iii) some tasks show\nresults close to random, and (iv) in other tasks LLMs perform above random but\nbelow shared task systems. IberBench offers open-source implementations for the\nentire evaluation pipeline, including dataset normalization and hosting,\nincremental evaluation of LLMs, and a publicly accessible leaderboard.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62308d13e4673fe4985d7fc9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655555697880-62308d13e4673fe4985d7fc9.jpeg",
      "fullname": "Areg Mikael Sarvazyan",
      "name": "asarvazyan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15921",
      "authors": [
        {
          "_id": "680ab2f808464b525df64b07",
          "name": "Jian Hu",
          "hidden": false
        },
        {
          "_id": "680ab2f808464b525df64b08",
          "name": "Dimitrios Korkinof",
          "hidden": false
        },
        {
          "_id": "680ab2f808464b525df64b09",
          "name": "Shaogang Gong",
          "hidden": false
        },
        {
          "_id": "680ab2f808464b525df64b0a",
          "name": "Mariano Beguerisse-Diaz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T14:06:01.000Z",
      "submittedOnDailyAt": "2025-04-25T06:39:24.280Z",
      "title": "ViSMaP: Reducción no manual de vídeos de tiempo real basada en entrenamiento experiencial no sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub",
      "submittedOnDailyBy": {
        "_id": "65e1b6e9501590df0173cbd3",
        "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
        "isPro": false,
        "fullname": "Jian Hu",
        "user": "lwpyh",
        "type": "user"
      },
      "summary": "Introducing ViSMap: A Meta-Prompting System for Unmanned Background Video Summarization. This system can summarize a one-hour video without a manual background. Current video understanding models are highly effective for short videos or pre-segmented event videos, but they face challenges with long videos where related events are sparsely distributed and not pre-segmented, making summarization difficult. Additionally, understanding long videos requires multiple training steps with detailed annotations, which are expensive, time-consuming, and inconsistent. ViSMap bridges the gap between short (rich in annotation data) and long videos. By using LLMs, it generates optimized fact summaries for long videos from segment explanations obtained from short videos. These fact summaries are used as training data for long video summarization, avoiding the need for detailed annotations. Specifically, ViSMap adopts a meta-prompting strategy to iteratively generate and improve fact summaries for long videos, guided by explanations of short clips. In each iteration, three LLMs are sequentially used: one to generate fact summaries from clip explanations, another to evaluate them, and a third to optimize the generator's prompt. This iteration is necessary because the quality of fact summaries highly depends on the generator's prompt and can vary significantly between videos. Evaluated across a wide range of vision datasets, ViSMap achieves state-of-the-art performance comparable to the best models across all domains without performance degradation. The code will be released after the paper publication.",
      "upvotes": 3,
      "discussionId": "680ab2fb08464b525df64bd2",
      "ai_keywords": [
        "LLMs",
        "ViSMap",
        "Unsupervised Video Summarisation",
        "Meta Prompting",
        "long-form video understanding",
        "supervised hierarchical training",
        "pseudo-summaries",
        "short clip descriptions",
        "meta-prompting strategy",
        "generator prompt"
      ]
    },
    "publishedAt": "2025-04-22T10:06:01.000Z",
    "title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting",
    "summary": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e1b6e9501590df0173cbd3",
      "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
      "fullname": "Jian Hu",
      "name": "lwpyh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17601",
      "authors": [
        {
          "_id": "680b2b8c6bd146aa35a48222",
          "user": {
            "_id": "64d496b04ab89be0de7fb1a9",
            "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
            "isPro": false,
            "fullname": "Erik Bergh",
            "user": "erikbergh",
            "type": "user"
          },
          "name": "Erik Bergh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-25T06:39:03.785Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T14:26:42.000Z",
      "submittedOnDailyAt": "2025-04-25T05:00:56.733Z",
      "title": "El método de reducción de dimensionalidad no-lineal analítico utilizando transformaciones lineales con pesos gaussianos",
      "submittedOnDailyBy": {
        "_id": "64d496b04ab89be0de7fb1a9",
        "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
        "isPro": false,
        "fullname": "Erik Bergh",
        "user": "erikbergh",
        "type": "user"
      },
      "summary": "El método de reducción de dimensiones desempeña un papel fundamental en el análisis y visualización de datos de alta dimensión. Las técnicas existentes como t-SNE y PCA presentan un compromiso entre representatividad y explicabilidad. Este artículo presenta una nueva aproximación para resolver este compromiso, integrando la explicabilidad de los métodos lineales con la representatividad de las transformaciones no lineales. El algoritmo propuesto utiliza transformaciones lineales y funciones gaussianas para construir un mapeo no lineal entre espacios de alta y baja dimensión. Esta arquitectura permite transformaciones no lineales complejas, manteniendo al mismo tiempo las ventajas de explicabilidad de los métodos lineales, y diseñada para que cada transformación pueda ser analizada independientemente. Finalmente, proporciona una visión transparente de la reducción de dimensiones y del espacio transformado. También se presenta un método para explicar las transformaciones aprendidas, incluyendo la determinación de la importancia de las dimensiones suprimidas y los métodos para ampliar y reducir el espacio. Estos instrumentos permiten a los usuarios entender las relaciones geométricas que se mantienen o cambian durante el proceso de reducción de dimensiones. Para garantizar la utilidad práctica del algoritmo, se enfatiza el desarrollo de un paquete de software amigable con el usuario y se promueve su introducción en el ámbito académico e industrial.",
      "upvotes": 1,
      "discussionId": "680b2b8d6bd146aa35a48252",
      "projectPage": "https://github.com/erikbergh/interpretable_dim_reduction/",
      "githubRepo": "https://github.com/erikbergh/interpretable_dim_reduction/",
      "ai_keywords": [
        "t-SNE",
        "PCA",
        "non-linear mapping",
        "Gaussian functions",
        "linear transformations",
        "interpretability",
        "dimensionality reduction",
        "suppressed dimensions",
        "geometric relationships"
      ]
    },
    "publishedAt": "2025-04-24T10:26:42.000Z",
    "title": "Interpretable non-linear dimensionality reduction using gaussian\n  weighted linear transformation",
    "summary": "Dimensionality reduction techniques are fundamental for analyzing and\nvisualizing high-dimensional data. With established methods like t-SNE and PCA\npresenting a trade-off between representational power and interpretability.\nThis paper introduces a novel approach that bridges this gap by combining the\ninterpretability of linear methods with the expressiveness of non-linear\ntransformations. The proposed algorithm constructs a non-linear mapping between\nhigh-dimensional and low-dimensional spaces through a combination of linear\ntransformations, each weighted by Gaussian functions. This architecture enables\ncomplex non-linear transformations while preserving the interpretability\nadvantages of linear methods, as each transformation can be analyzed\nindependently. The resulting model provides both powerful dimensionality\nreduction and transparent insights into the transformed space. Techniques for\ninterpreting the learned transformations are presented, including methods for\nidentifying suppressed dimensions and how space is expanded and contracted.\nThese tools enable practitioners to understand how the algorithm preserves and\nmodifies geometric relationships during dimensionality reduction. To ensure the\npractical utility of this algorithm, the creation of user-friendly software\npackages is emphasized, facilitating its adoption in both academia and\nindustry.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17601.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d496b04ab89be0de7fb1a9",
      "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
      "fullname": "Erik Bergh",
      "name": "erikbergh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17414",
      "authors": [
        {
          "_id": "680b3f12c131c3be24f80ce0",
          "name": "Min Wei",
          "hidden": false
        },
        {
          "_id": "680b3f12c131c3be24f80ce1",
          "name": "Chaohui Yu",
          "hidden": false
        },
        {
          "_id": "680b3f12c131c3be24f80ce2",
          "name": "Jingkai Zhou",
          "hidden": false
        },
        {
          "_id": "680b3f12c131c3be24f80ce3",
          "name": "Fan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T10:12:40.000Z",
      "submittedOnDailyAt": "2025-04-25T06:22:09.592Z",
      "title": "3DV-TON: 3D Güide por Modelo Distribuido para la Fotografía 3D Coincidiente en la Vídeo Estreñido",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Los videochats de vestido sustituyen el vestido de una película por un vestido específico. Los métodos existentes tienen dificultades para mantener la calidad y la coincidencia temporal en resultados de alta calidad cuando se tratan de patrones complejos de vestidos y varias posiciones corporales. Presentamos un nuevo marco de trabajo basado en difusión que mantiene la calidad y la coincidencia temporal. Nuestro enfoque utiliza un tequete dinámico generado de manera explícita como guía en cada frame, permitiendo al modelo priorizar la calidad del exterior mientras mantiene la coincidencia del comportamiento, lo que resolve el problema de perder la coincidencia del comportamiento al priorizar la calidad del exterior. Esto se logra al referirse directamente al movimiento de tequete de vestido que coincide en toda la secuencia de video. Nuestro método caracteriza un adaptativo flujo de pila para generar un guía 3D dinámico: 1) seleccionamos un frame clave de un videochat de vestido 2D, y 2) reconstruimos y animamos la pose original de la película y el tequete de textura sincronizado. Además, presentamos una estrategia de mascarado rectangular fuerte para reducir la propagación de artefactos causados por la pérdida directa de información de vestido debido al movimiento humano y del vestido. Para la investigación de videochats de vestido, presentamos un conjunto de datos de alta resolución que incluye 130 videos con diferentes tipos de vestidos y escenarios. Demostramos nuestro excelente rendimiento a través de resultados cualitativos y cualitativos. El sitio web del proyecto está disponible en: https://2y7c3.github.io/3DV-TON/",
      "upvotes": 1,
      "discussionId": "680b3f15c131c3be24f80d65",
      "ai_keywords": [
        "diffusion-based framework",
        "animatable textured 3D meshes",
        "frame-level guidance",
        "motion coherence",
        "garment texture movements",
        "adaptive pipeline",
        "keyframe",
        "2D image try-on",
        "textured 3D mesh",
        "synchronized with original video poses",
        "rectangular masking strategy",
        "artifact propagation",
        "HR-VVT",
        "high-resolution benchmark dataset"
      ]
    },
    "publishedAt": "2025-04-24T06:12:40.000Z",
    "title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models",
    "summary": "Video try-on replaces clothing in videos with target garments. Existing\nmethods struggle to generate high-quality and temporally consistent results\nwhen handling complex clothing patterns and diverse body poses. We present\n3DV-TON, a novel diffusion-based framework for generating high-fidelity and\ntemporally consistent video try-on results. Our approach employs generated\nanimatable textured 3D meshes as explicit frame-level guidance, alleviating the\nissue of models over-focusing on appearance fidelity at the expanse of motion\ncoherence. This is achieved by enabling direct reference to consistent garment\ntexture movements throughout video sequences. The proposed method features an\nadaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe\nfor initial 2D image try-on, followed by (2) reconstructing and animating a\ntextured 3D mesh synchronized with original video poses. We further introduce a\nrobust rectangular masking strategy that successfully mitigates artifact\npropagation caused by leaking clothing information during dynamic human and\ngarment movements. To advance video try-on research, we introduce HR-VVT, a\nhigh-resolution benchmark dataset containing 130 videos with diverse clothing\ntypes and scenarios. Quantitative and qualitative results demonstrate our\nsuperior performance over existing methods. The project page is at this link\nhttps://2y7c3.github.io/3DV-TON/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17414.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17343",
      "authors": [
        {
          "_id": "680b24471c5fbd15909bf1f9",
          "name": "Linli Yao",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fa",
          "name": "Yicheng Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fb",
          "name": "Yuancheng Wei",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fc",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fd",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fe",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1ff",
          "name": "Kun Ouyang",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf200",
          "name": "Lean Wang",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf201",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf202",
          "name": "Sida Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf203",
          "name": "Lingpeng Kong",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf204",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf205",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf206",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T07:59:46.000Z",
      "submittedOnDailyAt": "2025-04-25T06:34:54.517Z",
      "title": "TIMECHAT ONLINE: En los streaming de películas, el 80% de los tokens visuales son naturalmente redundantes.",
      "submittedOnDailyBy": {
        "_id": "655ca347f426a304c6b393a1",
        "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
        "isPro": false,
        "fullname": "Linli Yao",
        "user": "yaolily",
        "type": "user"
      },
      "summary": "El rápido crecimiento de los plataformas de vídeo en línea, especialmente debido a los servicios de streaming en vivo, ha generado una urgente demanda de sistemas que puedan comprender videos en tiempo real. Estos sistemas deben procesar vídeos en flujo continuo y responder inmediatamente a las solicitudes del usuario. Actualmente, los Modelos de Lenguaje de Video Grande (VideoLLMs) superan la comprensión completa de los videos, pero presentan limitaciones para procesar eficientemente los largos y continuos frames de vídeo en streaming. Se presenta TimeChat-Online, un nuevo VideoLLM que innova en la creación de modelos interactivos de vídeo en tiempo real. El nucleo es nuestro innovador Modulo de Diferencial de Token Drop (DTD). DTD aborda los problemas básicos de la redundancia visual en vídeos de streaming. Atraendo energía de la falta de percepción visual humana (Change Blindness), DTD filtra el contenido estatico y redundante entre frames, manteniendo la variación temporal. En los experimentos, DTD redució el 82.8% de los tokens de video en StreamingBench, manteniendo el 98% de la eficiencia, y demostró que más del 80% del contenido visual se mostró naturalmente redundante. Se muestra que no es necesario un guia para el video de streaming. Se repite 11 veces: \"Se muestra que no es necesario un guia para el video de streaming.\"",
      "upvotes": 1,
      "discussionId": "680b244a1c5fbd15909bf2ff",
      "ai_keywords": [
        "Video Large Language Models (VideoLLMs)",
        "Differential Token Drop (DTD)",
        "Change Blindness phenomenon",
        "TimeChat-Online",
        "TimeChat-Online-139K",
        "StreamingBench",
        "OvOBench",
        "Video-MME",
        "MLVU",
        "Proactive Response",
        "real-time video interaction",
        "continuous video streams",
        "user queries",
        "visual redundancy",
        "dense, redundant frames",
        "visual content",
        "video tokens",
        "meaningful temporal changes",
        "static, redundant content",
        "backward-tracing",
        "current-perception",
        "future-responding scenarios"
      ]
    },
    "publishedAt": "2025-04-24T03:59:46.000Z",
    "title": "TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming\n  Videos",
    "summary": "The rapid growth of online video platforms, particularly live streaming\nservices, has created an urgent need for real-time video understanding systems.\nThese systems must process continuous video streams and respond to user queries\ninstantaneously, presenting unique challenges for current Video Large Language\nModels (VideoLLMs). While existing VideoLLMs excel at processing complete\nvideos, they face significant limitations in streaming scenarios due to their\ninability to handle dense, redundant frames efficiently. We introduce\nTimeChat-Online, a novel online VideoLLM that revolutionizes real-time video\ninteraction. At its core lies our innovative Differential Token Drop (DTD)\nmodule, which addresses the fundamental challenge of visual redundancy in\nstreaming videos. Drawing inspiration from human visual perception's Change\nBlindness phenomenon, DTD preserves meaningful temporal changes while filtering\nout static, redundant content between frames. Remarkably, our experiments\ndemonstrate that DTD achieves an 82.8% reduction in video tokens while\nmaintaining 98% performance on StreamingBench, revealing that over 80% of\nvisual content in streaming videos is naturally redundant without requiring\nlanguage guidance. To enable seamless real-time interaction, we present\nTimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse\ninteraction patterns including backward-tracing, current-perception, and\nfuture-responding scenarios. TimeChat-Online's unique Proactive Response\ncapability, naturally achieved through continuous monitoring of video scene\ntransitions via DTD, sets it apart from conventional approaches. Our extensive\nevaluation demonstrates TimeChat-Online's superior performance on streaming\nbenchmarks (StreamingBench and OvOBench) and maintaining competitive results on\nlong-form video tasks such as Video-MME and MLVU.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17343.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655ca347f426a304c6b393a1",
      "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
      "fullname": "Linli Yao",
      "name": "yaolily",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16064",
      "authors": [
        {
          "_id": "680b494c6bd146aa35ab2e1c",
          "name": "Theodoros Kouzelis",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e1d",
          "name": "Efstathios Karypidis",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e1e",
          "name": "Ioannis Kakogeorgiou",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e1f",
          "name": "Spyros Gidaris",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e20",
          "name": "Nikos Komodakis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/677272184d148b904333e874/QkosGJUTQX94tAZPCF46-.png"
      ],
      "publishedAt": "2025-04-22T17:41:42.000Z",
      "submittedOnDailyAt": "2025-04-25T07:32:33.466Z",
      "title": "Modelo de imágenes-características sintéticas para mejorar el modelado de imágenes generadas",
      "submittedOnDailyBy": {
        "_id": "677272184d148b904333e874",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5dUau7gxLk4Wm1TiiJJri.jpeg",
        "isPro": false,
        "fullname": "Efstathios Karypidis",
        "user": "Sta8is",
        "type": "user"
      },
      "summary": "Los módulos de dispersión potencial (LDMs) ocupan la mayoría de la posición en el campo de la generación de imágenes de alta calidad, pero la integración de aprendizaje de representación y modelado de generación es un problema complejo. Presentamos un nuevo marco de modelado de imágenes generadas utilizando modelos de redes neuronales profundas, que permiten modelar juntos los potenciales de imágenes de bajo nivel (obtenidos de un autoencoder variational) y características significativas de nivel alto (obtenidas de un autoencoder de reconocimiento automático pre-entrenado como DINO). Nuestro enfoque de aprendizaje profundo potencial permite que el modelo aprenda a generar pares de características de la imagen a partir de un puro ruido, mejorando significativamente la calidad de la generación y la eficiencia del entrenamiento, con un mínimo de cambios en la arquitectura estándar de transformadores de difusión. Su ventaja es que no requiere diseños complejos, lo que simplifica el entrenamiento y desarrolla una nueva estrategia de inferencia potente llamada \"guia de representación\". Evaluados tanto condicionalmente como no condicionalmente, nuestro método mejora significativamente la calidad de las imágenes y la velocidad de convergencia del entrenamiento, estableciendo una nueva dirección en el modelado de generación de representaciones.",
      "upvotes": 1,
      "discussionId": "680b494e6bd146aa35ab2e97",
      "githubRepo": "https://github.com/zelaki/ReDi",
      "ai_keywords": [
        "latent diffusion models (LDMs)",
        "generative image modeling",
        "diffusion model",
        "low-level image latents",
        "variational autoencoder",
        "high-level semantic features",
        "pretrained self-supervised encoder",
        "DINO",
        "latent-semantic diffusion",
        "coherent image-feature pairs",
        "generative quality",
        "training efficiency",
        "Diffusion Transformer architectures",
        "complex distillation objectives",
        "Representation Guidance",
        "image quality",
        "training convergence speed",
        "representation-aware generative modeling"
      ]
    },
    "publishedAt": "2025-04-22T13:41:42.000Z",
    "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
    "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/677272184d148b904333e874/QkosGJUTQX94tAZPCF46-.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16064.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "677272184d148b904333e874",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5dUau7gxLk4Wm1TiiJJri.jpeg",
      "fullname": "Efstathios Karypidis",
      "name": "Sta8is",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]