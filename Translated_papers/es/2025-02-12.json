[
  {
    "paper": {
      "id": "2502.06807",
      "authors": [
        {
          "_id": "67ac1b080686a1e0690741ce",
          "name": "OpenAI",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d0",
          "name": "Ahmed El-Kishky",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d1",
          "name": "Alexander Wei",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d2",
          "name": "Andre Saraiva",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d3",
          "name": "Borys Minaev",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d4",
          "name": "Daniel Selsam",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d5",
          "name": "David Dohan",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d6",
          "name": "Francis Song",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d7",
          "name": "Hunter Lightman",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d8",
          "name": "Ignasi Clavera",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d9",
          "name": "Jakub Pachocki",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741da",
          "name": "Jerry Tworek",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741db",
          "name": "Lorenz Kuhn",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741dc",
          "name": "Lukasz Kaiser",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741dd",
          "name": "Mark Chen",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741de",
          "name": "Max Schwarzer",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741df",
          "name": "Mostafa Rohaninejad",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e0",
          "name": "Nat McAleese",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e1",
          "name": "o3 contributors",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e2",
          "name": "Oleg Mürk",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e3",
          "name": "Rhythm Garg",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e4",
          "name": "Rui Shu",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e5",
          "name": "Szymon Sidor",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e6",
          "name": "Vineet Kosaraju",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e7",
          "name": "Wenda Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T23:00:15.000Z",
      "title": "Competencia de Programación y Modelos de Lógica de Gran Escala",
      "summary": "Mostramos que el aprendizaje por refuerzo aplicado a modelos de lenguaje grandes (LLMs) mejora significativamente el rendimiento de códigos complejos y tareas de inferencia. Además, comparando los puntos iniciales de los modelos de inferencia generales como o1 y o3 de OpenAI, y comparando el área de sistemas que utilizan estrategias de inferencia diseñadas para competir en la Olimpiada Internacional de Informática (IOI) de 2024, como o1-ioi, que utiliza estrategias manuales. En la IOI de 2024, o1-ioi competió directamente y ocupó el 49% de los tiempos de prueba. En condiciones de competencia más lenientes, o1-ioi logró un oro. Sin embargo, al evaluar el modelo más tarde, el modelo o3 puede alcanzar un oro sin utilizar estrategias manuales ni condiciones de competencia relaxadas. Nuestros resultados de investigación muestran que, como o1-ioi, un pipeline especializado puede traer buenos resultados, mientras que el modelo general expandido como o3 puede superar esos resultados sin utilizar estrategias de inferencia manuales. En particular, el modelo o3 obtuvo un oro en la IOI de 2024 y obtuvo una calificación de clase elite en Codeforces. En general, estos resultados indican que extender el aprendizaje por refuerzo general puede proporcionar una ruta fuerte en el campo de la tecnología de inteligencia artificial de la inferencia, sin depender de técnicas específicas para un dominio.",
      "upvotes": 28,
      "discussionId": "67ac1b090686a1e069074208"
    },
    "publishedAt": "2025-02-11T22:53:19.310Z",
    "title": "Competitive Programming with Large Reasoning Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07316",
      "authors": [
        {
          "_id": "67ac0ab720e98bddc5c19fed",
          "name": "Junlong Li",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19fee",
          "name": "Daya Guo",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19fef",
          "name": "Dejian Yang",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff0",
          "name": "Runxin Xu",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff1",
          "name": "Yu Wu",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff2",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T07:26:50.000Z",
      "title": "CodeI/O: Compresión de patrones de causa por predicción de entrada y salida de código",
      "summary": "La razón es una capacidad básica de los modelos de lenguaje grandes. El estudio de la semana pasada se centraba en mejorar las capacidades especializadas en matemáticas o generación de código, pero el mejoramiento del rendimiento en tareas de razonamiento de diversas razones era raro y difícil debido a los datos de entrenamiento raros y continuos. Para resolver estos problemas, proponemos un nuevo enfoque. Este enfoque selecciona patrones de razonamiento de razones únicos de manera sistemática basado en el contexto del código. Esto se llama CodeI/O. Este método transforma el código original en un formato de predicción de entrada/salida de código. Utilizando un modelo que predice entrada/salida a partir de código y casos de prueba proporcionados en naturaleza, se entrena para aprender razonamientos de tipo \"Chain-of-Thought\" y exponer los elementos básicos de razonamiento general. Este permite mantener la estructura de razonamiento sintácticamente independiente de la codificación y preservar la precisión de los procedimientos. Los resultados de los experimentos muestran un mejoramiento consistente en tareas de razonamiento de signos, ciencia, lógica, matemáticas y números, así como de conocimientos comunes. Para confirmar cada predicción, se ejecuta el código con salidas o entradas predecidas que coinciden con las realidades, y se modifican varias veces el \"Chain-of-Thought\" para implementar CodeI/O++ y obtener un rendimiento más alto. Los datos y el modelo están disponibles en https://github.com/hkust-nlp/CodeIO.",
      "upvotes": 14,
      "discussionId": "67ac0ab820e98bddc5c1a039"
    },
    "publishedAt": "2025-02-11T23:00:20.080Z",
    "title": "CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07316.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "621e40ac944c7e36aaec2369",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621e40ac944c7e36aaec2369/Yj-FJRWps3rvsS_B2bnKo.jpeg",
      "fullname": "Junlong Li",
      "name": "lockon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07701",
      "authors": [
        {
          "_id": "67ac23166def89f9aae56abd",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56abe",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56abf",
          "user": {
            "_id": "66015e8aa4d296af07de538e",
            "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
            "isPro": false,
            "fullname": "Ye",
            "user": "Owen777",
            "type": "user"
          },
          "name": "Tian Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:12.141Z",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac0",
          "name": "Jiantong Zhao",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac1",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac2",
          "name": "Michael Lingelbach",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac3",
          "name": "Li Yuan",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac4",
          "name": "Yonghong Tian",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac5",
          "name": "Enze Xie",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac6",
          "name": "Daquan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T16:58:15.000Z",
      "title": "Magic 1 a 1: Se genera un clip de vídeo de 1 minuto dentro de 1 minuto.",
      "summary": "En este informe técnico se presenta el modelo de generación de vídeo \"Magic 1-For-1 (Magic141)\" para el uso eficiente de memoria y la optimización del tiempo de inferencia. La idea principal es sencilla: la generación de vídeo a partir de texto se divide en dos tareas simples, implementadas mediante procesos de aprendizaje diferentes. Se trata de la generación de imagenes a partir de texto y la generación de vídeo a partir de imagenes. Debido a que se utiliza el mismo algoritmo de optimización, se ha confirmado que la generación de vídeo a partir de imagenes es más sencilla y converge más rápidamente que la generación de vídeo a partir de texto. Además, se investigaron trucos de optimización para reducir los costos de cálculo de entrenamiento del modelo de generación de vídeo a partir de imagenes en tres aspectos: 1) la Inyección de condiciones de antecedencia multimodal para acelerar la convergencia del modelo, 2) el aprendizaje de pasos adversarios para acelerar el tiempo de inferencia, y 3) la optimización del costo de memoria durante la inferencia. Con estas tecnologías, es posible generar un clip de 5 segundos de vídeo en 3 segundos. Al aplicar un ventana de ventana de tiempo de prueba, se puede generar un vídeo de 1 minuto en menos de 1 minuto, con un gran mejoramiento en la calidad visual y el comportamiento, y el tiempo promedio para generar un clip de 1 segundo de vídeo se reduce a menos de 1 segundo. Con estas herramientas, se busca encontrar la mejor compromiso entre el costo computacional y la calidad del vídeo, lo que se espera que se convierta en un guia de exploración abierto. Los códigos y los pesos del modelo están disponibles en https://github.com/DA-Group-PKU/Magic-1-For-1.",
      "upvotes": 11,
      "discussionId": "67ac23186def89f9aae56b69"
    },
    "publishedAt": "2025-02-11T23:27:13.769Z",
    "title": "Magic 1-For-1: Generating One Minute Video Clips within One Minute",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07701.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03492",
      "authors": [
        {
          "_id": "67a5a8e595df68b0a167c298",
          "user": {
            "_id": "622f103fc78da4c7ebd7c887",
            "avatarUrl": "/avatars/b0c7cd29835d92c2cd584947fcd5d520.svg",
            "isPro": false,
            "fullname": "Xie",
            "user": "Zhihui",
            "type": "user"
          },
          "name": "Zhihui Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:17:02.682Z",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c299",
          "name": "Jie chen",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29a",
          "name": "Liyu Chen",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29b",
          "name": "Weichao Mao",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29c",
          "name": "Jingjing Xu",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29d",
          "name": "Lingpeng Kong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T02:18:46.000Z",
      "title": "「Guía de Evaluación de Modelos de Lenguaje por Aprendizaje por Refuerzo」",
      "summary": "Aprender a criticar y mejorar los LLM es importante para la construcción de sistemas continuamente mejorables, pero está limitado por su capacidad para hacer juicios precisos y proponer consejos operables. En este estudio, se investiga la crítica de la generación de código por parte de los LLM y se propone el marco de trabajo CTRL (Aprendizaje por Reforzamiento para el Aprendizaje Critico). CTRL entrena un modelo de crítica que genera retroalimentación para mejorar el rendimiento de un generador fijo, sin supervisor humano. Nuestros resultados muestran que los críticos entrenados con CTRL mejoran significativamente las tasas de paso de los modelos generadores básicos y reforzados, y pueden inhibir errores continuos. Además, estos modelos de crítica funcionan como modelos de recompensa precisa, permitiendo escalar las pruebas y alcanzar un aumento relativo del 106.1% en el rendimiento en los benchmark de código difícil.",
      "upvotes": 10,
      "discussionId": "67a5a8e695df68b0a167c2c6"
    },
    "publishedAt": "2025-02-11T23:55:37.671Z",
    "title": "Teaching Language Models to Critique via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622f103fc78da4c7ebd7c887",
      "avatarUrl": "/avatars/b0c7cd29835d92c2cd584947fcd5d520.svg",
      "fullname": "Xie",
      "name": "Zhihui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06329",
      "authors": [
        {
          "_id": "67ab4174757d2eb190af0375",
          "user": {
            "_id": "621d6f532165dc431641e438",
            "avatarUrl": "/avatars/56ccef10a8426d7160ef3586a771bd63.svg",
            "isPro": false,
            "fullname": "Kiran Kamble",
            "user": "kiranr",
            "type": "user"
          },
          "name": "Kiran Kamble",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:55.367Z",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0376",
          "name": "Melisa Russak",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0377",
          "name": "Dmytro Mozolevskyi",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0378",
          "user": {
            "_id": "6320a906a023aad6a7670e99",
            "avatarUrl": "/avatars/48071559b0c7660bf6861cfe008b3006.svg",
            "isPro": false,
            "fullname": "Muayad Sayed Ali",
            "user": "muayad",
            "type": "user"
          },
          "name": "Muayad Ali",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:53.157Z",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0379",
          "name": "Mateusz Russak",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af037a",
          "name": "Waseem AlShikh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T10:29:28.000Z",
      "title": "Un evento inesperado: Finance's FailSafe en contexto de preguntas basadas en la lenguaje\n\n**Nota:** La traducción se ha realizado manteniendo la profundidad y la precisión del texto original.",
      "summary": "Se propone un marco de referencia para el sector financiero llamado \"FailSafeQA\". Este marco de referencia está diseñado para medir la robustez y la comprensión del contexto de un sistema de respuestas de consultas basado en modelos de lenguaje (LLM) frente a seis interacciones de interacción con la interfaz humana. Se enfoca en dos casos principales: archivos de consultas y archivos de contexto. En el caso de los archivos de consultas, se modifican las consultas originales para implementar cambios en especialidad, completitud y precisión del lenguaje. En los archivos de contexto, se simulan sobreescrituras de documentos deteriorados, irrelevantes o vacíos. Se utiliza Qwen2.5-72B-Instruct para adoptar el enfoque de un modelo de lenguaje como jurado, definiendo puntuaciones de robustez, contexto y cumplimiento para calcular estos valores en 24 modelos de prueba. Los resultados muestran que algunos modelos tienen excelente capacidad para mitigar la influencia de los datos de entrada, pero también demuestran la necesidad de equilibrar la capacidad de responder de manera robusta y evitar la fabricación de información. En particular, el modelo con la mayor cumplimiento, Palmyra-Fin-128k-Instruct, mantuvo su rendimiento básico pero tuvo dificultades en mantener predicciones robustas en 17% de los casos de prueba. Por otro lado, el modelo más robusto, OpenAI o3-mini, fabricó información en 41% de los casos de prueba. Estos resultados demuestran que aunque los modelos de alto rendimiento tienen mucho potencial, aún tienen grandes oportunidades de mejora. FailSafeQA juega un papel crucial como herramienta para optimizar la confianza en el desarrollo de modelos de lenguaje en aplicaciones financieras. El dataset está disponible en la siguiente URL: https://huggingface.co/datasets/Writer/FailSafeQA",
      "upvotes": 8,
      "discussionId": "67ab4175757d2eb190af03ca"
    },
    "publishedAt": "2025-02-12T02:51:41.003Z",
    "title": "Expect the Unexpected: FailSafe Long Context QA for Finance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06329.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60e61b3969bd0df25c9375da",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
      "fullname": "Melisa Russak",
      "name": "melisa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07617",
      "authors": [
        {
          "_id": "67ac1d68c29356f92ed772c5",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c6",
          "name": "Ibrahim Alabdulmohsin",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c7",
          "name": "Daniel Salz",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c8",
          "name": "Zhe Li",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c9",
          "name": "Keran Rong",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772ca",
          "name": "Xiaohua Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T15:05:33.000Z",
      "title": "Modelo de lenguaje visual para escalar entre 100 billones de datos",
      "summary": "Estamos investigando la potencial potencial de este negocio de manera demonstrativa: el entrenamiento previo de un modelo de Visión-Lenguaje de escala sin precedentes con 1000 billones de ejemplos. En esta escala, el rendimiento del modelo generalmente se satura en muchos clasificadores y benchmark de búsqueda centrados en el occidente (por ejemplo, COCO Captions). Sin embargo, las tareas de diversidad cultural se benefician aún más de la amplitud de los 1000 billones de datos de internet. Esto se debe al concepto de la \"long tail\". Además, analizamos la multilingüedad del modelo y obtenemos resultados en lenguajes de bajo recurso. Además, utilizamos filtros como CLIP para reducir el tamaño del conjunto de datos de entrenamiento previo, descubierto la presencia de ejemplos negativos que afectan negativamente. Este tamaño de datos es clave para construir un sistema multilingüe con diversidad cultural.",
      "upvotes": 8,
      "discussionId": "67ac1d6ac29356f92ed77354"
    },
    "publishedAt": "2025-02-11T23:03:08.578Z",
    "title": "Scaling Pre-training to One Hundred Billion Data for Vision Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07617.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07374",
      "authors": [
        {
          "_id": "67ac1c6436464325ebe3c6e3",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e4",
          "name": "Shiyi Cao",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e5",
          "name": "Tyler Griggs",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e6",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e7",
          "name": "Xiangxi Mo",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e8",
          "name": "Shishir G. Patil",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e9",
          "name": "Matei Zaharia",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6ea",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6eb",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T08:48:48.000Z",
      "title": "Los LLMs pueden entender por qué se les da un comando y lo que importa es la estructura, no el contenido.",
      "summary": "Los modelos lógicos de larga cadena (LRMs) resuelven problemas lógicos complejos mediante la seguimiento de fuentes de cadenas largas de off sine (Long CoT). Esto incluye técnicas como reflexión, retroceso y auto-prueba. Sin embargo, los métodos de entrenamiento y los requisitos de datos para extraer estas cadenas largas de off sine no están bien comprendidos. En este estudio, hemos descubierto que los modelos de lenguaje grande (LLMs) pueden aprender eficazmente la lógica de cadenas largas de off sine mediante ajustes normativos eficientes de datos (SFT) y adaptadores de parámetros eficientes de bajo rendimiento (LoRA). Con 17k muestras de entrenamiento de cadenas largas de off sine, el modelo Qwen2.5-32B-Instruct presentó notables mejoras en marcos de evaluación matemáticos y de programación, registrando un 56.7% (+40.0%) en el AIME 2024 y un 57.0% (+8.1%) en LiveCodeBench. Estos resultados son competitivos con los de los modelos propios, como o1-preview (44.6% y 59.1%). Un punto importante es que la estructura de las cadenas largas de off sine es crucial para el proceso de aprendizaje, y el contenido de los pasos lógicos no afecta significativamente su rendimiento. La adicción de perturbaciones relacionadas con el contenido (por ejemplo, entrenamiento con muestras negativas o eliminación de palabras clave de lógica) tiene un impacto ligeramente negativo. En contraste, cambios estructurales que destruyen la coherencia lógica (como el barajado o eliminación de pasos lógicos) significativamente afectan la precisión. Por ejemplo, entrenando con muestras de cadenas largas de off sine que incluyen respuestas negativas resulta en una pérdida de precisión del 3.2% en comparación con entrenamiento con muestras completamente precisas. Estos hallazgos profundizan la comprensión sobre cómo se desarrollan las capacidades lógicas de los modelos de lenguaje grande y identifican puntos clave para la entrenamiento eficiente de los siguientes modelos lógicos. Este artículo es una publicación académica del modelo Sky-T1-32B-Preview anteriormente publicado. El código está disponible en https://github.com/NovaSky-AI/SkyThought.",
      "upvotes": 8,
      "discussionId": "67ac1c6536464325ebe3c723"
    },
    "publishedAt": "2025-02-11T22:58:37.585Z",
    "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07374.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03997",
      "authors": [
        {
          "_id": "67ac206214d5fe7767e7ec4e",
          "name": "Yu Yuan",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec4f",
          "user": {
            "_id": "63eb00a191a1b8ec4fbba2a9",
            "avatarUrl": "/avatars/0cc7cf9b6d05337603f700e0d592edf5.svg",
            "isPro": false,
            "fullname": "ShizhaoSun",
            "user": "ShizhaoSun",
            "type": "user"
          },
          "name": "Shizhao Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:14.580Z",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec50",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec51",
          "name": "Jiang Bian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T11:57:14.000Z",
      "title": "Editor CAD: Especificación de ubicación para el insertar trabajo de marco de trabajo y síntesis de datos de entrenamiento automática basada en texto.",
      "summary": "El apoyo a la diseño de computadoras (CAD) es un elemento esencial en diversas industrias. La edición de CAD basada en contexto tiene gran potencial para automatizar cambios en modelos CAD según instrucciones textuales, aunque aún no se ha descubierto. Los métodos actuales se centran principalmente en la generación de cambios de diseño o en la creación de CAD basada en texto, pero carecen de soporte basado en texto o tratan modelos CAD existentes como restricciones. Se presenta CAD-Editor, el primer framework para la edición de CAD basada en contexto. Para abordar la respuesta a los datos democráticos necesarios para la entrenamiento, se propone una pipeline de síntesis de datos automatizada. Este pipeline genera pares de modelos CAD originales y editados utilizando modelos de cambios de diseño, y resume las diferencias utilizando grandes modelos de lenguaje visuolingüístico (LVLMs). Para abordar las características complejas de la edición de CAD basada en contexto, se propone un marco de filtrado localizado. Este marco divide en dos sub-tareas: identificar áreas que requieren cambios y agregar editaciones adecuadas en esas áreas. Grandes modelos de lenguaje (LLMs) se utilizan basados en estas dos sub-tareas, explotando sus capacidades de comprensión natural y conocimiento de CAD. Las experimentaciones demuestran que CAD-Editor logra un rendimiento altamente efectivo.",
      "upvotes": 6,
      "discussionId": "67ac206314d5fe7767e7ec98"
    },
    "publishedAt": "2025-02-11T23:16:28.213Z",
    "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03997.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63eb00a191a1b8ec4fbba2a9",
      "avatarUrl": "/avatars/0cc7cf9b6d05337603f700e0d592edf5.svg",
      "fullname": "ShizhaoSun",
      "name": "ShizhaoSun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07527",
      "authors": [
        {
          "_id": "67ac1eaac61306b0ac95d2c6",
          "name": "Yingce Xia",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c7",
          "name": "Peiran Jin",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c8",
          "name": "Shufang Xie",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c9",
          "name": "Liang He",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ca",
          "name": "Chuan Cao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cb",
          "name": "Renqian Luo",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cc",
          "name": "Guoqing Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cd",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ce",
          "name": "Zequn Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cf",
          "name": "Yuan-Jyue Chen",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d0",
          "name": "Zekun Guo",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d1",
          "name": "Yeqi Bai",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d2",
          "name": "Pan Deng",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d3",
          "name": "Yaosen Min",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d4",
          "name": "Ziheng Lu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d5",
          "name": "Hongxia Hao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d6",
          "name": "Han Yang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d7",
          "name": "Jielan Li",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d8",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d9",
          "name": "Jia Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2da",
          "name": "Jianwei Zhu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2db",
          "name": "Kehan Wu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2dc",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2dd",
          "name": "Kaiyuan Gao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2de",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2df",
          "name": "Qian Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e0",
          "name": "Xixian Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e1",
          "name": "Yanting Li",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e2",
          "name": "Houtian Zhu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e3",
          "name": "Yeqing Lu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e4",
          "name": "Mingqian Ma",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e5",
          "name": "Zun Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e6",
          "name": "Tian Xie",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e7",
          "name": "Krzysztof Maziarz",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e8",
          "name": "Marwin Segler",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e9",
          "name": "Zhao Yang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ea",
          "name": "Zilong Chen",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2eb",
          "name": "Yu Shi",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ec",
          "name": "Shuxin Zheng",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ed",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ee",
          "name": "Chen Hu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ef",
          "name": "Peggy Dai",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f0",
          "name": "Tie-Yan Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f1",
          "name": "Haiguang Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f2",
          "name": "Tao Qin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T13:08:03.000Z",
      "title": "Interpreta el lenguaje de la naturaleza para impulsar la descubrimiento científico.",
      "summary": "El modelo básico ha tenido un impacto innovador en el procesamiento del lenguaje natural y la inteligencia artificial, mejorando significativamente la capacidad de los máquinas para comprender y generar lenguajes humanos. Con el éxito de estos modelos, los investigadores han desarrollado modelos básicos en diversas áreas científicas, como moléculas, materiales, proteínas, ADN y ARN. Sin embargo, estos modelos se entrenan generalmente de manera independiente, lo que limita su capacidad para integrar diferentes áreas científicas. En vista de esta situación, se han desarrollado modelos que pueden representar todas las entidades de una área científica como expresiones de orden, lo que se unen bajo el nombre de \"lenguaje de la naturaleza\" para introducir el Nature Language Model (abreviatura: NatureLM). Este modelo está diseñado para ser un modelo básico de ciencia basado en el orden, entrenado previamente con datos de múltiples áreas científicas, lo que permite realizar aplicaciones diversas, incluyendo: (i) la generación y optimización de moléculas, proteínas, RNA y materiales a través de instrucciones textuales; (ii) la generación y diseño entre diferentes áreas; y (iii) la traducción de SMILES a IUPAC y la optimización de la síntesis en USPTO-50k, alcanzando los mejores resultados. NatureLM proporciona una aproximación general para diversas tareas científicas, como la descubrimiento de fármacos (generación y optimización visual, optimización de ADMET, síntesis), diseño de nuevos materiales, desarrollo de proteínas terapéuticas o nucleotidos, entre otros. Se han desarrollado modelos de NatureLM de diferentes tamaños (aproximadamente 100 millones, 800 millones y 4,670 millones de parámetros), y con el aumento del tamaño, la mejora de los resultados es claramente notable.",
      "upvotes": 6,
      "discussionId": "67ac1eabc61306b0ac95d346"
    },
    "publishedAt": "2025-02-11T23:10:26.895Z",
    "title": "NatureLM: Deciphering the Language of Nature for Scientific Discovery",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06589",
      "authors": [
        {
          "_id": "67ac1d45e6f1e95ccf6de3b7",
          "user": {
            "_id": "6471bddd609ae9f56368f132",
            "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
            "isPro": true,
            "fullname": "Yuchen Zhuang",
            "user": "yczhuang",
            "type": "user"
          },
          "name": "Yuchen Zhuang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-12T04:02:14.866Z",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3b8",
          "name": "Jingfeng Yang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3b9",
          "name": "Haoming Jiang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3ba",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bb",
          "name": "Kewei Cheng",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bc",
          "name": "Sanket Lokegaonkar",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bd",
          "name": "Yifan Gao",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3be",
          "name": "Qing Ping",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bf",
          "name": "Tianyi Liu",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c0",
          "name": "Binxuan Huang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c1",
          "name": "Zheng Li",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c2",
          "name": "Zhengyang Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c3",
          "name": "Pei Chen",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c4",
          "name": "Ruijie Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c5",
          "name": "Rongzhi Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c6",
          "name": "Nasser Zalmout",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c7",
          "name": "Priyanka Nigam",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c8",
          "name": "Bing Yin",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c9",
          "name": "Chao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T15:54:34.000Z",
      "title": "Hefey Stews: Mejora continua de las habilidades básicas de los agentes del modelo de lenguaje de alto nivel a través del aprendizaje continuo",
      "summary": "Hefaesthus Forju es el primer gran corpus de datos previos para fortalecer las capacidades básicas de los agentes de LLM. Este corpus tiene como objetivo fortalecer la capacidad de llamadas a API, la lógica interna y el planificación, así como la adaptación al retroalimento de la entorno. Hefaesthus Forju incluye 103B de datos únicos para los agentes y registra 76,537 API. Estas API incluyen documentos de herramientas que proporcionan conocimiento sobre las funciones de la API y fortalecen el proceso de llamadas a funciones. Además, a través del entrenamiento continuo que provoca este corpus, Hefaesthus supera los pequeños LLM abiertos de tamaño intermedio y alcanza niveles comparables a los LLM comerciales en tres marcos de referencia de agentes, demostrando tanto la capacidad básica de los agentes de LLM como la efectividad de la extensibilidad para nuevas tareas o entornos.",
      "upvotes": 6,
      "discussionId": "67ac1d46e6f1e95ccf6de419"
    },
    "publishedAt": "2025-02-11T23:04:08.153Z",
    "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6471bddd609ae9f56368f132",
      "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
      "fullname": "Yuchen Zhuang",
      "name": "yczhuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07508",
      "authors": [
        {
          "_id": "67ac2006a6b5a26040fc94f7",
          "name": "Yang Luo",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94f8",
          "name": "Xuanlei Zhao",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94f9",
          "name": "Mengzhao Chen",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fa",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fb",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fc",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fd",
          "name": "Zhangyang Wang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fe",
          "name": "Yang You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T12:22:35.000Z",
      "title": "Mejora de lenguaje en Abidoo: Crea videos generados de mejor calidad gratuitamente.",
      "summary": "El generado de vídeo basado en DiT ha logrado resultados sorprendentes, aunque la investigación sobre la expansión de los modelos existentes está en un estado relativamente difícil de encontrar. En este estudio, presentamos un enfoque sin necesidad de entrenamiento para mejorar la continuidad y la calidad del vídeo generado basado en DiT. Este enfoque tiene como concepto clave la mejora de las relaciones interframe basada en la distribución no diagonal del tiempo. Caracterizado por su diseño sencillo, se puede aplicar fácilmente a casi todos los marcos de trabajo de generación de vídeo basado en DiT, sin necesidad de retenimiento o ajustes. En cada modelo de generación de vídeo basado en DiT, este enfoque muestra claros mejoramientos en la consistencia temporal y la calidad visual. Esperamos que este estudio lleve a la esperanza para la investigación futura en la expansión de la generación de vídeo.",
      "upvotes": 5,
      "discussionId": "67ac200ea6b5a26040fc9709"
    },
    "publishedAt": "2025-02-11T23:14:10.293Z",
    "title": "Enhance-A-Video: Better Generated Video for Free",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04223",
      "authors": [
        {
          "_id": "67ac5e0d653d273eeaf25e59",
          "name": "Ilia Karmanov",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5a",
          "user": {
            "_id": "67ac5d85a19e34140ea1013b",
            "avatarUrl": "/avatars/e5b7446787dbbd17553dc9e11b58a0b4.svg",
            "isPro": false,
            "fullname": "Amala Sanjay Deshmukh",
            "user": "amalad",
            "type": "user"
          },
          "name": "Amala Sanjay Deshmukh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:14:49.009Z",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5b",
          "name": "Lukas Voegtle",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5c",
          "name": "Philipp Fischer",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5d",
          "user": {
            "_id": "64c7a43e0d3d1b209df90b9c",
            "avatarUrl": "/avatars/1d0d2f129b799a72345b17fd5307aa5e.svg",
            "isPro": false,
            "fullname": "Kateryna Chumachenko",
            "user": "katerynaCh",
            "type": "user"
          },
          "name": "Kateryna Chumachenko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:14:47.025Z",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5e",
          "name": "Timo Roman",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5f",
          "user": {
            "_id": "60098ca06e8ac78787773f85",
            "avatarUrl": "/avatars/be6539e5706bf07c71e553254c1751b5.svg",
            "isPro": false,
            "fullname": "Jarno Seppänen",
            "user": "jseppanen",
            "type": "user"
          },
          "name": "Jarno Seppänen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:14:51.062Z",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e60",
          "name": "Jupinder Parmar",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e61",
          "name": "Joseph Jennings",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e62",
          "name": "Andrew Tao",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e63",
          "name": "Karan Sapra",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T17:07:22.000Z",
      "title": "El texto en español es:\n\n\"ECLARAR - EXTRACCIÓN DEL CONTENIDO Y SEQUENCIA DE ORDEN INTEGRADO\"",
      "summary": "La tecnología de Reconocimiento Óptico de Caracteres (OCR) se utiliza ampliamente para extraer texto de imágenes de documentos, promoviendo la digitalización eficiente y la búsqueda de datos. Sin embargo, cuando se trata de documentos complejos, extraer solo el texto no es suficiente. Para comprender completamente la estructura del documento, se necesita información sobre formatos, fórmulas, tablas, el orden de lectura de bloques y columnas en varias páginas, y el detectado de elementos como referencias y capturas de imágenes. Esta comprensión detallada es esencial para tareas como la búsqueda, la respuesta a preguntas en documentos y la elaboración de datos de entrenamiento para modelos de lenguaje grandes (LLMs) y modelos de lenguaje y visión grandes (VLMs). Para enfrentar estas desafíos, presentamos 'Eclair', una herramienta especialmente diseñada para extraer texto general de diversos tipos de documentos. Dada una imagen, 'Eclair' extrae texto formateado de manera ordenada y también extrae los bordes de los bloques y las clases de significado correspondientes. Para evaluar estas nuevas capacidades, presentamos diferentes bases de datos humanas que evalúan OCR a nivel de documento y clasificación de significado. 'Eclair' alcanza la precisión más avanzada en estos benchmarks y supera a otros métodos en los principales métricas de componentes. Además, se ha evaluado 'Eclair' en otros benchmarks existentes, demostrando su capacidad de adaptación a diferentes criterios de evaluación y sus fortalezas.",
      "upvotes": 3,
      "discussionId": "67ac5e0f653d273eeaf25eea"
    },
    "publishedAt": "2025-02-12T04:25:54.558Z",
    "title": "Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60098ca06e8ac78787773f85/BfZ57W-gCoY32J60tx7dN.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04223.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60098ca06e8ac78787773f85",
      "avatarUrl": "/avatars/be6539e5706bf07c71e553254c1751b5.svg",
      "fullname": "Jarno Seppänen",
      "name": "jseppanen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07445",
      "authors": [
        {
          "_id": "67ac216d602eb9ca8a517be6",
          "name": "Nurit Cohen-Inger",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be7",
          "name": "Yehonatan Elisha",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be8",
          "name": "Bracha Shapira",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be9",
          "name": "Lior Rokach",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517bea",
          "name": "Seffi Cohen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T10:43:36.000Z",
      "title": "¡Dejamos de olvidar que la evaluación de los LLMs es como la de un chamelan!",
      "summary": "Los modelos de lenguaje grande (LLMs) a menudo obtienen altos puntajes en marcos de prueba públicos, pero estos altos puntajes pueden ocultar la realidad de que la capacidad de comprensión lingüística es alta. Introducimos C-BOD (Detector de Detección de Sobreajuste en el Benchmark Chameleon). C-BOD es un marco de evaluación meta que detecta el sobreajuste de los LLMs a través de la transformación sistemática de los prompts del benchmark mediante la conversión de parámetros. Al reestructurar la entrada manteniendo el significado y guardando los etiquetas, C-BOD evalúa si el rendimiento del modelo está guiado por patrones memorizados. Con 26 modelos de LLMs desarrollados en el marco de MMLU, nuestro método muestra una pérdida promedio del rendimiento del 2.15% y una diferencia significativa estadística en 20 de los 26 modelos. En particular, los modelos con alta precisión básica presentan una mayor pérdida de rendimiento debido a la transformación, y los grandes LLMs son más sensibles a la reestructuración, lo que sugiere una mayor dependencia de patrones de prompts fijos. En contraste, los modelos de la familia Llama y los modelos con baja precisión no muestran diferencias estadísticamente significativas y sugieren una menor dependencia de códigos superficiales. Además, C-BOD, diseñado de manera que no dependa de los conjuntos de datos o modelos, puede fácilmente ser integrado en el proceso de entrenamiento y potencializa una comprensión lingüística más fuerte. Nuestros hallazgos demuestran que la comunidad debe priorizar la sorprendencia y la extensibilidad en la evaluación de los LLMs más que los puntajes de clasificación.",
      "upvotes": 3,
      "discussionId": "67ac216e602eb9ca8a517c1d"
    },
    "publishedAt": "2025-02-11T23:22:50.454Z",
    "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07445.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6731e56a07cf693a1104d2cb",
      "avatarUrl": "/avatars/46a3269a19c7e6bfb7004a5da9701459.svg",
      "fullname": "Seffi Cohen",
      "name": "seffico",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04465",
      "authors": [
        {
          "_id": "67a953844ea315a67e02461d",
          "user": {
            "_id": "63195d0582e7eec0eac040e3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
            "isPro": false,
            "fullname": "Luca Della Libera",
            "user": "lucadellalib",
            "type": "user"
          },
          "name": "Luca Della Libera",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T10:03:10.257Z",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e02461e",
          "name": "Francesco Paissan",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e02461f",
          "name": "Cem Subakan",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e024620",
          "name": "Mirco Ravanelli",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T19:24:50.000Z",
      "title": "FocalCodec: Uso de una red de modulación focal para codificación de voz a baja tasa de bitrate",
      "summary": "Los modelos de lenguaje natural han avanzado de manera innovadora a través de la aprendizaje previo en grandes conjuntos de datos, aplicando reglas internas. Con este éxito, los investigadores han estudiado la utilización de codificadores de red neuronal para tokenizar señales sonoras continuas y aplicar esta técnica a la exploración de sonidos. Sin embargo, el enfoque actual depende de la diseño de múltiples códigobooks para minimizar pérdidas de bitrate alto, información semántica o datos acústicos, lo que incrementa la complejidad arquitectónica de tareas posteriores y presenta varias limitaciones. Para resolver estos problemas, presentamos un códigobook eficiente de bajo bitrate. Este códigobook, basado en Focker Codier, utiliza un solo códigobook binético para comprimir señales de sonido en un rango de 0.16 a 0.65 kbps. El Focker Codier ofrece un desempeño competitivo en la recreación de sonido y la transformación de sonido a bitrates más bajos que las tecnologías actuales, y es efectivo en señales de lenguaje multilingüe y en ambientes con ruido. En evaluaciones de tareas posteriores, el Focker Codier conserva suficiente información semántica y acústica, y es adecuado para modelos de generación. Los ejemplos de demostración, códigos y checkpoints están disponibles en https://lucadellalib.github.io/focalcodec-web/.",
      "upvotes": 2,
      "discussionId": "67a953854ea315a67e024659"
    },
    "publishedAt": "2025-02-12T01:31:44.368Z",
    "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04465.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63195d0582e7eec0eac040e3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
      "fullname": "Luca Della Libera",
      "name": "lucadellalib",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07531",
      "authors": [
        {
          "_id": "67ac21acaa680a0f8782d273",
          "name": "Sixiao Zheng",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d274",
          "name": "Zimian Peng",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d275",
          "name": "Yanpeng Zhou",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d276",
          "name": "Yi Zhu",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d277",
          "name": "Hang Xu",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d278",
          "name": "Xiangru Huang",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d279",
          "name": "Yanwei Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T13:11:59.000Z",
      "title": "VidCRAFT3: Creación de videos a partir de imágenes que controlan cámara, objeto y luz.",
      "summary": "Recientemente, la tecnología de generación de vídeos ha demostrado su capacidad para controlar elementos visuales como el movimiento de la cámara o el movimiento de objetos, pero no existe la capacidad para controlar múltiples elementos visuales simultáneamente, dependiendo de la eficiencia de los datos y la red. En este artículo, se presenta un nuevo marco llamado VidCRAFT3, que permite la generación de vídeos a partir de imágenes precisas, controlando simultáneamente el movimiento de la cámara, el movimiento de objetos y la dirección del luz. Para lograr un control más detallado de cada elemento visual, se propone el espectral triple attention transformer, con el objetivo de integrar de manera simétrica la dirección del luz, el texto y las imágenes. Debido a que muchos conjuntos de datos de vídeo reales no tienen análisis de la dirección del luz, se ha construido un conjunto de datos de vídeo de alta calidad, VideoLightingDirection (VLD), que se ha publicado para su uso. Este conjunto de datos incluye análisis de la dirección del luz y diferentes objetos exteriores, lo que permite a VidCRAFT3 manejar eficazmente la transmisión y reflexión del luz. Además, para eliminar la necesidad de datos de entrenamiento análisados para todos los elementos visuales, se propone una estrategia de entrenamiento en tres etapas. Las pruebas en conjuntos de datos de referencia muestran que VidCRAFT3 es capaz de generar contenido de vídeo de alta calidad, superando en precisión y coherencia visual a la tecnología actual. Todo el código y los datos están disponibles para su uso público. Página del proyecto: https://sixiaozheng.github.io/VidCRAFT3/.",
      "upvotes": 2,
      "discussionId": "67ac21b2aa680a0f8782d3bd"
    },
    "publishedAt": "2025-02-11T23:21:13.452Z",
    "title": "VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07531.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07776",
      "authors": [
        {
          "_id": "67ac1f7851c7f3b53ffc4def",
          "name": "Chenchen Gu",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df0",
          "name": "Xiang Lisa Li",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df1",
          "name": "Rohith Kuditipudi",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df2",
          "name": "Percy Liang",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df3",
          "user": {
            "_id": "661595d1b3d0b21da55cde7d",
            "avatarUrl": "/avatars/ba3fa065536518637d21a5c46cee5dd1.svg",
            "isPro": false,
            "fullname": "Tatsu Hashimoto",
            "user": "thashim",
            "type": "user"
          },
          "name": "Tatsunori Hashimoto",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-12T04:11:36.912Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:58:04.000Z",
      "title": "Revisión del Caché de Prompts para el API del Modelo de Lenguaje",
      "summary": "El caching de prompts es un elemento principal que provoca cambios temporales en la dependencia de datos en modelos de lenguaje grandes (LLMs), ya que los prompts cacheados se procesan más rápidamente que los que no están cacheados. Esta diferencia de tiempo puede aumentar el riesgo de ataques de canales laterales. Por ejemplo, si el caché es compartido entre usuarios, un atacante puede identificar y utilizar los prompts cacheados para obtener información sobre los prompts de otros usuarios. Debido a que el caching de prompts puede ser considerado una causa de la divulgación de información personal, la transparencia en las políticas de caché de las empresas que proporcionan API es crucial. Con este enfoque, hemos desarrollado y realizado evaluaciones estadísticas para detectar el uso de caching de prompts en las API de la realidad. Hemos detectado el uso de un global caché entre usuarios en 7 proveedores de API y hemos confirmado potenciales riesgos de divulgación de información personal relacionados con los prompts. Además, la variación en el tiempo provocada por el caching de prompts también puede llevar a la divulgación de información sobre la arquitectura del modelo. En particular, hemos demostrado que el modelo integrado de OpenAI solo tiene un decoder que no había sido publicado anteriormente.",
      "upvotes": 2,
      "discussionId": "67ac1f7851c7f3b53ffc4e1b"
    },
    "publishedAt": "2025-02-11T23:11:49.993Z",
    "title": "Auditing Prompt Caching in Language Model APIs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05932",
      "authors": [
        {
          "_id": "67ac4356401012b81050022a",
          "user": {
            "_id": "67ac430c4ab9207cc227d23f",
            "avatarUrl": "/avatars/59c499cc191e28a66ae917963c28ffb3.svg",
            "isPro": false,
            "fullname": "Tenglong Liu",
            "user": "LTL07",
            "type": "user"
          },
          "name": "Tenglong Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:15:09.627Z",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022b",
          "name": "Jianxiong Li",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022c",
          "name": "Yinan Zheng",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022d",
          "name": "Haoyi Niu",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022e",
          "name": "Yixing Lan",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022f",
          "name": "Xin Xu",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b810500230",
          "name": "Xianyuan Zhan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T15:22:38.000Z",
      "title": "En el espacio de parámetros combinar tecnologías de expansión y extensión",
      "summary": "La humanidad ha adquirido la habilidad de reciclar capacidades previamente conocidas para resolver nuevos problemas y desarrollar nuevas tecnologías en el proceso de resolución. Este paradigma se ha extendido ampliamente en el desarrollo de sistemas que evolucionan automáticamente problemas nuevos. Sin embargo, los métodos anteriores presentaban limitaciones en la eficiencia de entrenamiento al expandir tecnologías, lo que impidió que las capacidades previamente conocidas fueran completamente explotadas para el aprendizaje de nuevas tareas. En este artículo, se propone un nuevo marco de trabajo llamado PSEC (Parameter Skill Expansion and Combination). Este marco permite la gestión de bibliotecas de tecnologías, facilitando la resolución eficiente de problemas nuevos y la evolución de las capacidades de los agentes. Las bibliotecas de PSEC permiten la configuración de portfólios de evolución progresiva de los componentes básicos de las tecnologías, utilizando ajustes microparamétricos. Esta estructura combina módulos LoRA para combinar tecnologías directamente y explotar información de comparación entre tecnologías para programar nuevas tecnologías de manera efectiva. Basándose en esto, se propone la idea de módulos contextuales que pueden activarse dinámicamente para procesar tareas nuevas de manera conjunta. A través de la combinación de aplicaciones como multitarea, dinámica shift y continuos shift, PSEC ha demostrado su capacidad para utilizar eficientemente las capacidades previamente conocidas para resolver problemas nuevos y expandir las bibliotecas de tecnologías para evolucionar las capacidades. Los resultados obtenidos en D4RL, DSRL benchmark y DeepMind Control Suite confirman que PSEC es capaz de expandir y evolucionar las capacidades de los agentes a través de la eficiente utilización de capacidades previamente conocidas. Sitio web del proyecto: https://ltlhuuu.github.io/PSEC/",
      "upvotes": 0,
      "discussionId": "67ac435b401012b8105003dc"
    },
    "publishedAt": "2025-02-12T04:53:50.325Z",
    "title": "Skill Expansion and Composition in Parameter Space",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05932.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ac430c4ab9207cc227d23f",
      "avatarUrl": "/avatars/59c499cc191e28a66ae917963c28ffb3.svg",
      "fullname": "Tenglong Liu",
      "name": "LTL07",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]