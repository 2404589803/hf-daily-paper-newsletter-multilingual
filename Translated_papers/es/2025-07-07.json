[
  {
    "paper": {
      "id": "2507.01853",
      "authors": [
        {
          "_id": "686b4e69213f123a1f88bd76",
          "name": "Samridhi Raj Sinha",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd77",
          "name": "Rajvee Sheth",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd78",
          "name": "Abhishek Upperwal",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd79",
          "name": "Mayank Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T16:07:54.000Z",
      "submittedOnDailyAt": "2025-07-07T03:06:37.666Z",
      "title": "Eca-Ebal: Marco de Evaluación Detallado de Modelos de Lenguaje de Grandes Escalas en Índio",
      "submittedOnDailyBy": {
        "_id": "66e1425c919f283fbd7dfb5e",
        "avatarUrl": "/avatars/b43ee9b9042486b5faf91472b4c49c5d.svg",
        "isPro": false,
        "fullname": "Rajvee Sheth",
        "user": "RajveeSheth",
        "type": "user"
      },
      "summary": "El rápido desarrollo de los LLMs ha llevado a que la necesidad de un marco de evaluación que supere los marcos de referencia centrados en inglés se ha vuelto más importante. En respuesta a las demandas de regiones con alta diversidad lingüística, como el India, se presenta EKA-EVAL, un marco de evaluación uniforme y de uso comercial. Este marco integra más de 35 marcos de referencia y incluye más de 10 conjuntos de datos únicos, cubriendo ampliamente diversas áreas como razonamiento, matemáticas, uso de herramientas, comprensión de contextos largos y evaluación de lectura. Comparado con las herramientas de evaluación actuales en hindi, EKA-EVAL incorpora una amplia cobertura de marcos de referencia, cálculo distribuido, escalabilidad, uso de múltiples GPU y otras características. En nuestro análisis sistemático, EKA-EVAL se ha establecido como un sistema de evaluación expandible desde el principio hasta el fin, proporcionando herramientas de evaluación adecuadas para los modelos de lenguaje globales e indexados y disminuyendo significativamente las barreras para el benchmarking multilingüe. Este marco de evaluación es abierto-source y está disponible en https://github.com/lingo-iitgn/eka-eval. Además, forma parte de la iniciativa EKA-IN, cuyo objetivo es expandir a más de 100 marcos de referencia y construir una sólida ecosistema de evaluación multilingüe.",
      "upvotes": 2,
      "discussionId": "686b4e69213f123a1f88bd7a",
      "ai_summary": "EKA-EVAL is a comprehensive multilingual evaluation framework for large language models, supporting diverse benchmarks and features for efficient distributed inference and GPU usage.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "unified evaluation framework",
        "production-ready",
        "Indic-specific datasets",
        "reasoning",
        "mathematics",
        "tool use",
        "long-context understanding",
        "reading comprehension",
        "distributed inference",
        "quantization",
        "multi-GPU usage",
        "end-to-end",
        "extensible evaluation suite",
        "multilingual benchmarking",
        "open-source"
      ]
    },
    "publishedAt": "2025-07-02T12:07:54.000Z",
    "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages",
    "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e1425c919f283fbd7dfb5e",
      "avatarUrl": "/avatars/b43ee9b9042486b5faf91472b4c49c5d.svg",
      "fullname": "Rajvee Sheth",
      "name": "RajveeSheth",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01955",
      "authors": [
        {
          "_id": "686b8347213f123a1f88bdc8",
          "name": "Rahul Ramachandran",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdc9",
          "name": "Ali Garjani",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdca",
          "name": "Roman Bachmann",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcb",
          "name": "Andrei Atanov",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcc",
          "name": "Oğuzhan Fatih Kar",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcd",
          "name": "Amir Zamir",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:59:07.000Z",
      "submittedOnDailyAt": "2025-07-07T06:51:04.452Z",
      "title": "GPT-4o evalúa cuánto entiende visualmente y evalúa un modelo basado en la visión computacional multimodal para tareas estándar.",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": true,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "Los modelos de transformadores (por ejemplo, GPT-4o) han experimentado una sorprendente evolución reciente, pero su capacidad para comprender la visión es poco clara. En este artículo, se benchmarkean los rendimientos de modelos basados en transformadores populares en tareas comunes de visión computacional (segmentación semántica, detección de objetos, clasificación de imágenes, predicción de profundidad y normalización de superficies) como GPT-4o, o4-mini, Gemini 1.5 Pro, Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2. Los dataset utilizados incluyen COCO, ImageNet y sus variantes.\n\nLa principal desafío para realizar esta evaluación es: 1) muchos modelos se entrenan para generar texto, lo que les impide expresar de manera natural el segmento o el área 3D generalizada. 2) Varios modelos avanzados son accesibles solo a través de API, lo que impide ajustar los pesos del modelo. Para resolver estos problemas, se traducen los tareas visuales a prompts de texto equivalentes y a tareas que se correspondan con la API, y se crea un marco de referencia benchmark estándar utilizando técnicas de fine-tuning.\n\nLos resultados de estos modelos muestran: 1) que ninguno de ellos se acerca a los modelos de especialización más avanzados en ninguna tarea. 2) Sin embargo, se destacan por su generalidad, lo cual se debe principalmente a que se han entrenado en tareas basadas en imágenes y texto. 3) Su rendimiento en tareas semánticas es significativamente superior a las generales. 4) La técnica de fine-tuning afecta el rendimiento, pero los mejores modelos son menos sensibles a los cambios en los prompts. 5) GPT-4o no es un modelo lógico. 6) GPT-4o es el mejor en cuatro de las seis tareas evaluadas. 7) El modelo lógico o3 muestra mejoras en tareas generales. 8) El análisis inicial de los modelos más recientes como GPT-4o, que poseen funciones originales de generación de imágenes, muestra que estos modelos tienen características notables.",
      "upvotes": 0,
      "discussionId": "686b8348213f123a1f88bdce",
      "ai_summary": "Multimodal foundation models, despite being primarily trained on image-text tasks, demonstrate respectable performance across various vision tasks when adapted through prompt chaining, though they fall short compared to specialized models.",
      "ai_keywords": [
        "GPT-4o",
        "o4-mini",
        "Gemini 1.5 Pro",
        "Gemini 2.0 Flash",
        "Claude 3.5 Sonnet",
        "Qwen2-VL",
        "Llama 3.2",
        "semantic segmentation",
        "object detection",
        "image classification",
        "depth prediction",
        "surface normal prediction",
        "COCO",
        "ImageNet",
        "prompt chaining",
        "reasoning models",
        "hallucinations",
        "spatial misalignments"
      ]
    },
    "publishedAt": "2025-07-02T13:59:07.000Z",
    "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation\n  Models on Standard Computer Vision Tasks",
    "summary": "Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 908
    },
    "isAuthorParticipating": false
  }
]