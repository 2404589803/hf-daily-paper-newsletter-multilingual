[
  {
    "paper": {
      "id": "2503.06053",
      "authors": [
        {
          "_id": "67cfd2d7bc539099da9ebecb",
          "name": "Runze Zhang",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebecc",
          "user": {
            "_id": "6474a63f7d131daf633d10f2",
            "avatarUrl": "/avatars/5e5d1ce5731987a810448835a1a69c91.svg",
            "isPro": false,
            "fullname": "GeorgeDu",
            "user": "georgedu",
            "type": "user"
          },
          "name": "Guoguang Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:45:36.478Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebecd",
          "user": {
            "_id": "66b01dc4e48856bb718f2ba8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
            "isPro": false,
            "fullname": "Xiaochuan Li",
            "user": "lixiaochuan",
            "type": "user"
          },
          "name": "Xiaochuan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:45:39.724Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebece",
          "name": "Qi Jia",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebecf",
          "name": "Liang Jin",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed0",
          "user": {
            "_id": "66f67725cdcb9a4eaef04027",
            "avatarUrl": "/avatars/fb5f4b467cc4d73e129fa9aa60ef344d.svg",
            "isPro": false,
            "fullname": "Ellen Liu",
            "user": "EllenAP",
            "type": "user"
          },
          "name": "Lu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:45:32.476Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed1",
          "name": "Jingjing Wang",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed2",
          "user": {
            "_id": "6297889a64501abb8d002c6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-tfSq05d4nkLkU_E-N75e.png",
            "isPro": false,
            "fullname": "Cong Xu",
            "user": "NeilXu",
            "type": "user"
          },
          "name": "Cong Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T09:16:48.232Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed3",
          "name": "Zhenhua Guo",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed4",
          "name": "Yaqian Zhao",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed5",
          "name": "Xiaoli Gong",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed6",
          "name": "Rengang Li",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed7",
          "name": "Baoyu Fan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b01dc4e48856bb718f2ba8/_R9279HPwaWKB34BDJmAv.png"
      ],
      "publishedAt": "2025-03-08T04:37:38.000Z",
      "submittedOnDailyAt": "2025-03-18T05:40:31.378Z",
      "title": "\"Drop Rizubervido: Conjuntos de datos y métodos de acceso para explorar la coherencia espacio-temporal\"",
      "submittedOnDailyBy": {
        "_id": "66b01dc4e48856bb718f2ba8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
        "isPro": false,
        "fullname": "Xiaochuan Li",
        "user": "lixiaochuan",
        "type": "user"
      },
      "summary": "La consistencia temporal es un tema importante en el campo de la generación de vídeos. Un segmento de vídeo generado correctamente debe mantener una coherencia visual y asegurar la coherencia entre objetos y escenas, asegurando además la posibilidad de flotación e interacciones. En los estudios previos, especialmente en proyectos abiertos, se centraba en mantener una coherencia temporal o espacial, o una combinación básica de ambas. Por ejemplo, se explicaba el movimiento de la cámara después de un prompt, pero sin limitar los resultados del movimiento. Sin embargo, el movimiento de la cámara puede añadir nuevos objetos a la escena o eliminar objetos existentes, lo que puede afectar a los objetos previos. En particular, el movimiento de la cámara es un factor complejo en videos que requieren múltiples interacciones entre flotaciones. En este artículo, se presenta y revisa una consistencia temporal integrada que considera la simplificación entre el proceso de flotación y la tecnología de la cámara, así como el impacto a largo plazo en la generación posterior. Este estudio abarca una amplia gama de perspectivas desde la construcción de conjuntos de datos hasta el desarrollo de modelos. Primero, se construyó el conjunto de datos DropletVideo-10M, que incluye 10 millones de vídeos que incluyen movimientos de cámara y acciones de objetos. Cada vídeo está explicado con una media de 206 captiones, detallando diferentes movimientos de cámara y el proceso de flotación. Luego, se desarrolló el modelo DropletVideo para mantener la consistencia temporal en la generación de vídeos. El conjunto de datos y el modelo DropletVideo pueden acceder a través de https://dropletx.github.io.",
      "upvotes": 47,
      "discussionId": "67cfd2debc539099da9ec061",
      "ai_keywords": [
        "spatio-temporal consistency",
        "video generation",
        "plot plausibility",
        "visual consistency",
        "objects",
        "scenes",
        "viewpoints",
        "camera movement",
        "prompt",
        "narrative",
        "plot progression",
        "camera techniques",
        "long-term impact",
        "dataset construction",
        "DropletVideo-10M dataset",
        "dynamic camera motion",
        "object actions",
        "caption",
        "DropletVideo model",
        "spatio-temporal coherence"
      ]
    },
    "publishedAt": "2025-03-07T23:37:38.000Z",
    "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation",
    "summary": "Spatio-temporal consistency is a critical research topic in video generation.\nA qualified generated video segment must ensure plot plausibility and coherence\nwhile maintaining visual consistency of objects and scenes across varying\nviewpoints. Prior research, especially in open-source projects, primarily\nfocuses on either temporal or spatial consistency, or their basic combination,\nsuch as appending a description of a camera movement after a prompt without\nconstraining the outcomes of this movement. However, camera movement may\nintroduce new objects to the scene or eliminate existing ones, thereby\noverlaying and affecting the preceding narrative. Especially in videos with\nnumerous camera movements, the interplay between multiple plots becomes\nincreasingly complex. This paper introduces and examines integral\nspatio-temporal consistency, considering the synergy between plot progression\nand camera techniques, and the long-term impact of prior content on subsequent\ngeneration. Our research encompasses dataset construction through to the\ndevelopment of the model. Initially, we constructed a DropletVideo-10M dataset,\nwhich comprises 10 million videos featuring dynamic camera motion and object\nactions. Each video is annotated with an average caption of 206 words,\ndetailing various camera movements and plot developments. Following this, we\ndeveloped and trained the DropletVideo model, which excels in preserving\nspatio-temporal coherence during video generation. The DropletVideo dataset and\nmodel are accessible at https://dropletx.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b01dc4e48856bb718f2ba8/_R9279HPwaWKB34BDJmAv.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06053.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b01dc4e48856bb718f2ba8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
      "fullname": "Xiaochuan Li",
      "name": "lixiaochuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12533",
      "authors": [
        {
          "_id": "67d8eadc045f869fea1ce3f2",
          "name": "Haoqi Yuan",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f3",
          "name": "Yu Bai",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f4",
          "user": {
            "_id": "67d92b2218de6ef86c60f7d4",
            "avatarUrl": "/avatars/9758522c99bc38bc7b60845eff8bf8d7.svg",
            "isPro": false,
            "fullname": "Yuhui Fu",
            "user": "fuyh",
            "type": "user"
          },
          "name": "Yuhui Fu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:17:46.443Z",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f5",
          "name": "Bohan Zhou",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f6",
          "user": {
            "_id": "6655b86e607894ea80d74910",
            "avatarUrl": "/avatars/663c0135c903c9c127fe1b8d8aaf279c.svg",
            "isPro": false,
            "fullname": "yicheng feng",
            "user": "takenpeanut",
            "type": "user"
          },
          "name": "Yicheng Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:17:31.771Z",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f7",
          "user": {
            "_id": "653238fdcd5377e9adee0c41",
            "avatarUrl": "/avatars/78aea70cde6ab0050c7e18b5e148075c.svg",
            "isPro": false,
            "fullname": "Xinrun Xu",
            "user": "SherryXu",
            "type": "user"
          },
          "name": "Xinrun Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:17:05.200Z",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f8",
          "name": "Yi Zhan",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f9",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:07:34.005Z",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3fa",
          "user": {
            "_id": "67d905c0e27ba28109384f5c",
            "avatarUrl": "/avatars/26712594ac9d43c8d1a3e75e36b5df16.svg",
            "isPro": false,
            "fullname": "Zongqing Lu",
            "user": "chungtsing",
            "type": "user"
          },
          "name": "Zongqing Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:16:58.409Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:53:53.000Z",
      "submittedOnDailyAt": "2025-03-18T02:11:08.263Z",
      "title": "Agente de robot humanoide: Introducción de modelos de visión y tecnología modularizada",
      "submittedOnDailyBy": {
        "_id": "61e52be53d6dbb1da842316a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
        "isPro": false,
        "fullname": "Börje Karlsson",
        "user": "tellarin",
        "type": "user"
      },
      "summary": "El objetivo final de la investigación en robotes autónomos humanoides es el desarrollo de un robot autónomo humanoide que logre un rendimiento humano en tareas concretas de la realidad. Los últimos avances han logrado un avance claro en el desarrollo de habilidades de bajo nivel en robotes humanoides mediante el FM (Modelo Base), pero la integración directa de estas componentes puede llevar a la acumulación de errores y a una pérdida de eficiencia en tareas a largo plazo debido a la diferencia de retrasos entre los módulos. En este contexto, el objetivo es mejorar esta situación mediante el framework heurístico agregante \"Being-0\", que integra el FM y una biblioteca de habilidades modularizadas. El FM se encarga de tareas cognitivas de alto nivel como la comprensión de órdenes, la planificación de tareas y la razón, mientras que la biblioteca de habilidades proporciona habilidades de bajo nivel de control, movimientos estables y manipulaciones flexibles. Para cerrar el gap entre estos niveles, se propone un nuevo módulo Connector. Este módulo cuenta con un modelo de lenguaje visuo-lenguaje (VLM) ligero, fortalece las capacidades de visualización del FM, traduce planes basados en lenguaje en comandos de habilidades que pueden moverse y manipular de manera dinámica, y mejora la tasa de éxito de las tareas. Being-0 permite que todos los componentes, excepto el FM, se implementen en dispositivos de computación virtual de bajo costo, transformando a la persona de planificación en un robot humanoide eficiente a través de herramientas de visualización y un VLM activo. Las experimentaciones distribuidas en entornos interiores de gran tamaño han demostrado que Being-0 puede resolver tareas complejas a largo plazo, implementando sub-tareas difíciles de navegación y manipulación. Para más detalles, consulte https://beingbeyond.github.io/being-0.",
      "upvotes": 35,
      "discussionId": "67d8eadd045f869fea1ce44a",
      "projectPage": "https://beingbeyond.github.io/Being-0/",
      "ai_keywords": [
        "Foundation Models (FMs)",
        "modular skill library",
        "high-level cognitive tasks",
        "instruction understanding",
        "task planning",
        "reasoning",
        "stable locomotion",
        "dexterous manipulation",
        "low-level control",
        "Connector module",
        "lightweight vision-language model (VLM",
        "embodied capabilities",
        "language-based plans",
        "actionable skill commands",
        "dynamic coordination",
        "full-sized humanoid robot",
        "dexterous hands",
        "active vision",
        "complex, long-horizon tasks",
        "challenging navigation",
        "manipulation subtasks"
      ]
    },
    "publishedAt": "2025-03-16T10:53:53.000Z",
    "title": "Being-0: A Humanoid Robotic Agent with Vision-Language Models and\n  Modular Skills",
    "summary": "Building autonomous robotic agents capable of achieving human-level\nperformance in real-world embodied tasks is an ultimate goal in humanoid robot\nresearch. Recent advances have made significant progress in high-level\ncognition with Foundation Models (FMs) and low-level skill development for\nhumanoid robots. However, directly combining these components often results in\npoor robustness and efficiency due to compounding errors in long-horizon tasks\nand the varied latency of different modules. We introduce Being-0, a\nhierarchical agent framework that integrates an FM with a modular skill\nlibrary. The FM handles high-level cognitive tasks such as instruction\nunderstanding, task planning, and reasoning, while the skill library provides\nstable locomotion and dexterous manipulation for low-level control. To bridge\nthe gap between these levels, we propose a novel Connector module, powered by a\nlightweight vision-language model (VLM). The Connector enhances the FM's\nembodied capabilities by translating language-based plans into actionable skill\ncommands and dynamically coordinating locomotion and manipulation to improve\ntask success. With all components, except the FM, deployable on low-cost\nonboard computation devices, Being-0 achieves efficient, real-time performance\non a full-sized humanoid robot equipped with dexterous hands and active vision.\nExtensive experiments in large indoor environments demonstrate Being-0's\neffectiveness in solving complex, long-horizon tasks that require challenging\nnavigation and manipulation subtasks. For further details and videos, visit\nhttps://beingbeyond.github.io/being-0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12533.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e52be53d6dbb1da842316a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
      "fullname": "Börje Karlsson",
      "name": "tellarin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12885",
      "authors": [
        {
          "_id": "67d8e23afa59a8b15a9057e8",
          "user": {
            "_id": "65eaa1e2b11eeb516a973508",
            "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
            "isPro": false,
            "fullname": "Dewei Zhou",
            "user": "limuloo1999",
            "type": "user"
          },
          "name": "Dewei Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:20:39.038Z",
          "hidden": false
        },
        {
          "_id": "67d8e23afa59a8b15a9057e9",
          "user": {
            "_id": "64551bc2c9c0dcc8c2484cf6",
            "avatarUrl": "/avatars/0d1ed4f4502f6f54ac6ba071e4c9a220.svg",
            "isPro": false,
            "fullname": "Mingwei Li",
            "user": "aiJojosh",
            "type": "user"
          },
          "name": "Mingwei Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:20:46.810Z",
          "hidden": false
        },
        {
          "_id": "67d8e23afa59a8b15a9057ea",
          "user": {
            "_id": "619bf9b3cbedb87e1a92fb3b",
            "avatarUrl": "/avatars/ee280db0232e21416c948ab9a9a2344e.svg",
            "isPro": false,
            "fullname": "Zongxin Yang",
            "user": "z-x-yang",
            "type": "user"
          },
          "name": "Zongxin Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:20:52.869Z",
          "hidden": false
        },
        {
          "_id": "67d8e23afa59a8b15a9057eb",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T07:30:16.000Z",
      "submittedOnDailyAt": "2025-03-18T01:33:30.593Z",
      "title": "DREAM-RENDER: Tecnología que permite controlar las propiedades dinámicas de un objeto de dibujo de texto de gran escala hacia una imagen.",
      "submittedOnDailyBy": {
        "_id": "65eaa1e2b11eeb516a973508",
        "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
        "isPro": false,
        "fullname": "Dewei Zhou",
        "user": "limuloo1999",
        "type": "user"
      },
      "summary": "Según las condiciones de dibujo, los métodos de generación de profundidad o según las condiciones de Canny muestran una capacidad sorprendente para la síntesis de imágenes precisas. Sin embargo, los modelos actuales tienen dificultades para controlar con precisión el contenido de múltiples instancias (o áreas). Entre los más avanzados, FLUX y 3DIS han identificado problemas como la pérdida de propiedades entre instancias y han limitado el control del usuario. Para resolver estos problemas, presentamos DreamRenderer, un enfoque de aprendizaje limitado basado en el modelo FLUX. DreamRenderer permite al usuario controlar el contenido de cada instancia mediante Bounding Box o máscara, mientras mantiene la armonía visual del todo. Propomos dos innovaciones: 1) una puente de imágenes para combinar propiedades complejas de contexto y 2) la combinación de propiedades de imagen estrictas en las capas de Vitral. A través del análisis de FLUX, se identifican los jugadores clave responsables del renderizado de propiedades de instancia y se aplican conjuntos estrictos de propiedades de imagen solo en las capas de Vitral, mientras se realiza una unión suave en las demás capas. Este enfoque permite una precisión controlada mientras se mantiene la calidad de la imagen. Según los evaluaciones en las marcas de referencia COCO-POS y COCO-MIG, DreamRenderer mejora la tasa de éxito de imágenes en un 17.7% sobre FLUX y mejora el rendimiento de modelos de diseño para imágenes en un máximo de 26.8% respecto a GLIGEN y 3DIS. Página del proyecto: https://limuloo.github.io/DreamRenderer/",
      "upvotes": 24,
      "discussionId": "67d8e23cfa59a8b15a9058ba",
      "projectPage": "https://limuloo.github.io/DreamRenderer/",
      "githubRepo": "https://github.com/limuloo/DreamRenderer",
      "ai_keywords": [
        "Bridge Image Tokens",
        "Hard Text Attribute Binding",
        "Replicated image tokens",
        "T5 text embeddings",
        "Joint Attention",
        "Hard Image Attribute Binding",
        "Vital layers",
        "Soft binding",
        "Image Success Ratio",
        "COCO-POS",
        "COCO-MIG",
        "layout-to-image models",
        "GLIGEN",
        "3DIS"
      ]
    },
    "publishedAt": "2025-03-17T03:30:16.000Z",
    "title": "DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale\n  Text-to-Image Models",
    "summary": "Image-conditioned generation methods, such as depth- and canny-conditioned\napproaches, have demonstrated remarkable abilities for precise image synthesis.\nHowever, existing models still struggle to accurately control the content of\nmultiple instances (or regions). Even state-of-the-art models like FLUX and\n3DIS face challenges, such as attribute leakage between instances, which limits\nuser control. To address these issues, we introduce DreamRenderer, a\ntraining-free approach built upon the FLUX model. DreamRenderer enables users\nto control the content of each instance via bounding boxes or masks, while\nensuring overall visual harmony. We propose two key innovations: 1) Bridge\nImage Tokens for Hard Text Attribute Binding, which uses replicated image\ntokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely\non text data, bind the correct visual attributes for each instance during Joint\nAttention; 2) Hard Image Attribute Binding applied only to vital layers.\nThrough our analysis of FLUX, we identify the critical layers responsible for\ninstance attribute rendering and apply Hard Image Attribute Binding only in\nthese layers, using soft binding in the others. This approach ensures precise\ncontrol while preserving image quality. Evaluations on the COCO-POS and\nCOCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success\nRatio by 17.7% over FLUX and enhances the performance of layout-to-image models\nlike GLIGEN and 3DIS by up to 26.8%. Project Page:\nhttps://limuloo.github.io/DreamRenderer/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12885.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65eaa1e2b11eeb516a973508",
      "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
      "fullname": "Dewei Zhou",
      "name": "limuloo1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13327",
      "authors": [
        {
          "_id": "67d8e00f0922c3dc8866520c",
          "user": {
            "_id": "640d704c8036cc2142299c19",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
            "isPro": false,
            "fullname": "Lan Chen",
            "user": "Orannue",
            "type": "user"
          },
          "name": "Lan Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:21:18.982Z",
          "hidden": false
        },
        {
          "_id": "67d8e00f0922c3dc8866520d",
          "name": "Qi Mao",
          "hidden": false
        },
        {
          "_id": "67d8e00f0922c3dc8866520e",
          "user": {
            "_id": "63021630a35b21bd8a53305a",
            "avatarUrl": "/avatars/7a7e8b39749eda61e57d8a1908726558.svg",
            "isPro": true,
            "fullname": "Gu Yuchao",
            "user": "guyuchao",
            "type": "user"
          },
          "name": "Yuchao Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:21:44.702Z",
          "hidden": false
        },
        {
          "_id": "67d8e00f0922c3dc8866520f",
          "user": {
            "_id": "661ab3da2b14565c7acccf5c",
            "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
            "isPro": false,
            "fullname": "Mike Zheng Shou",
            "user": "AnalMom",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:21:29.754Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T16:04:44.000Z",
      "submittedOnDailyAt": "2025-03-18T01:26:49.605Z",
      "title": "Aprendizaje de la relación visual del contexto en edición de imágenes",
      "submittedOnDailyBy": {
        "_id": "640d704c8036cc2142299c19",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
        "isPro": false,
        "fullname": "Lan Chen",
        "user": "Orannue",
        "type": "user"
      },
      "summary": "Nuevos ajustes \"Edit Transfer\" permiten que el modelo aprenda a transformar imágenes de preguntas nuevas a partir de un ejemplo de fuente y objetivo único. Los métodos basados en contexto superan tareas significativas mediante procesamiento de contexto, pero sufren dificultades con detalles geométricos de alta precisión (como cambios de postura y punto de vista). El editing basado en objetivo se centra generalmente en estilo o apariencia, sin lograr transformaciones no rigidas. \"Edit Transfer\" aprende transformaciones editables explícitas a partir de pares fuente-objetivo únicos, lo que desafía restricciones de referencia tanto en contexto como en apariencia. Basándose en el aprendizaje de contexto en modelos de lenguaje grandes, proponemos construir un modelo de imagen en contextos de DiT y un patrón de aprendizaje de relaciones visuales. Combinamos imágenes editadas y preguntas en un componente de 4 páginas para detectar transformaciones espaciales complejas con una pequeña cantidad de ejemplos, aplicando un ajuste LoRA ligero. Con solo 42 muestras de entrenamiento, \"Edit Transfer\" supera significativamente los métodos de TIE y RIE líderes en escalas no rigidas, demostrando efectos de aprendizaje visual de pocas muestras.",
      "upvotes": 19,
      "discussionId": "67d8e0100922c3dc88665285",
      "projectPage": "https://cuc-mipg.github.io/EditTransfer.github.io",
      "githubRepo": "https://github.com/CUC-MIPG/Edit-Transfer",
      "ai_keywords": [
        "Edit Transfer",
        "visual relation in-context learning",
        "DiT-based text-to-image model",
        "four-panel composite",
        "LoRA fine-tuning",
        "few-shot visual relation learning",
        "non-rigid transformations",
        "TIE methods",
        "RIE methods"
      ]
    },
    "publishedAt": "2025-03-17T12:04:44.000Z",
    "title": "Edit Transfer: Learning Image Editing via Vision In-Context Relations",
    "summary": "We introduce a new setting, Edit Transfer, where a model learns a\ntransformation from just a single source-target example and applies it to a new\nquery image. While text-based methods excel at semantic manipulations through\ntextual prompts, they often struggle with precise geometric details (e.g.,\nposes and viewpoint changes). Reference-based editing, on the other hand,\ntypically focuses on style or appearance and fails at non-rigid\ntransformations. By explicitly learning the editing transformation from a\nsource-target pair, Edit Transfer mitigates the limitations of both text-only\nand appearance-centric references. Drawing inspiration from in-context learning\nin large language models, we propose a visual relation in-context learning\nparadigm, building upon a DiT-based text-to-image model. We arrange the edited\nexample and the query image into a unified four-panel composite, then apply\nlightweight LoRA fine-tuning to capture complex spatial transformations from\nminimal examples. Despite using only 42 training samples, Edit Transfer\nsubstantially outperforms state-of-the-art TIE and RIE methods on diverse\nnon-rigid scenarios, demonstrating the effectiveness of few-shot visual\nrelation learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13327.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "640d704c8036cc2142299c19",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
      "fullname": "Lan Chen",
      "name": "Orannue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12590",
      "authors": [
        {
          "_id": "67d8de85f7809eea577c4805",
          "name": "Haoran Feng",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4806",
          "user": {
            "_id": "6375d136dee28348a9c63cbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
            "isPro": false,
            "fullname": "zehuan-huang",
            "user": "huanngzh",
            "type": "user"
          },
          "name": "Zehuan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:06.131Z",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4807",
          "name": "Lin Li",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4808",
          "user": {
            "_id": "674ded8ee50d988a4b9e108b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8oQITwlb7AB8LeIJjooYc.png",
            "isPro": false,
            "fullname": "Hairong Lv",
            "user": "lvhairong",
            "type": "user"
          },
          "name": "Hairong Lv",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:22:23.294Z",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4809",
          "name": "Lu Sheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T17:51:16.000Z",
      "submittedOnDailyAt": "2025-03-18T01:18:31.307Z",
      "title": "En el Transformador de Difusión, se pueden individualizar todos los elementos de manera automática y gratuitamente.",
      "submittedOnDailyBy": {
        "_id": "6375d136dee28348a9c63cbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
        "isPro": false,
        "fullname": "zehuan-huang",
        "user": "huanngzh",
        "type": "user"
      },
      "summary": "La generación de imágenes personalizadas tiene como objetivo la creación de imágenes con conceptos personalizados y funciones de edición flexibles. Los métodos recientes que no requieren entrenamiento han mostrado una mayor eficiencia computacional en comparación con los métodos basados en entrenamiento, pero presentan problemas en la mantención de la identidad, la aplicabilidad y la compatibilidad con los Transformers de Difusión (DiT). En este artículo, se descubre el potencial no desarrollado del DiT y se muestra que reemplazando los tokens de ruido de objetos referencia con 0, es posible realizar reconfiguraciones temáticas de 0 shot. Esta técnica sencilla y efectiva de entrada de características resuelve diversos escenarios desde la personalización de imágenes hasta su edición. Basándonos en esta descubrimiento, se propone un marco sin entrenamiento \"Personalize Anything\" que utiliza el DiT para realizar la generación de imágenes personalizadas. 1) Reemplazando los tokens de etapa temporal adaptativa lo introducido en las etapas iniciales para forzar la coincidencia del tema y normalizando en las etapas posteriores para mejorar la flexibilidad. 2) Usando Patch Ferritic para aumentar la diversidad estructural. Nuestro método apoya la generación batch-oriented, la personalización de múltiples temas y el editado de control de masa. La evaluación muestra el mejor rendimiento en la mantención de la identidad y la diversidad. Nuestro estudio ofrece nuevas insights sobre el DiT y proporciona un patrón práctico para la personalización eficiente.",
      "upvotes": 18,
      "discussionId": "67d8de89f7809eea577c4930",
      "projectPage": "https://fenghora.github.io/Personalize-Anything-Page/",
      "githubRepo": "https://github.com/fenghora/personalize-anything",
      "ai_keywords": [
        "diffusion transformers (DiTs)",
        "denoising tokens",
        "zero-shot subject reconstruction",
        "timestep-adaptive token replacement",
        "early-stage injection",
        "late-stage regularization",
        "patch perturbation strategies",
        "layout-guided generation",
        "multi-subject personalization",
        "mask-controlled editing",
        "identity preservation",
        "versatility"
      ]
    },
    "publishedAt": "2025-03-16T13:51:16.000Z",
    "title": "Personalize Anything for Free with Diffusion Transformer",
    "summary": "Personalized image generation aims to produce images of user-specified\nconcepts while enabling flexible editing. Recent training-free approaches,\nwhile exhibit higher computational efficiency than training-based methods,\nstruggle with identity preservation, applicability, and compatibility with\ndiffusion transformers (DiTs). In this paper, we uncover the untapped potential\nof DiT, where simply replacing denoising tokens with those of a reference\nsubject achieves zero-shot subject reconstruction. This simple yet effective\nfeature injection technique unlocks diverse scenarios, from personalization to\nimage editing. Building upon this observation, we propose Personalize\nAnything, a training-free framework that achieves personalized image\ngeneration in DiT through: 1) timestep-adaptive token replacement that enforces\nsubject consistency via early-stage injection and enhances flexibility through\nlate-stage regularization, and 2) patch perturbation strategies to boost\nstructural diversity. Our method seamlessly supports layout-guided generation,\nmulti-subject personalization, and mask-controlled editing. Evaluations\ndemonstrate state-of-the-art performance in identity preservation and\nversatility. Our work establishes new insights into DiTs while delivering a\npractical paradigm for efficient personalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6375d136dee28348a9c63cbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
      "fullname": "zehuan-huang",
      "name": "huanngzh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13435",
      "authors": [
        {
          "_id": "67d8dd0b924be985c277c8f6",
          "user": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "isPro": false,
            "fullname": "Ling Yang",
            "user": "Lingaaaaaaa",
            "type": "user"
          },
          "name": "Ling Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:22:51.251Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8f7",
          "user": {
            "_id": "6708920aeae29d1cd41a703b",
            "avatarUrl": "/avatars/922427a86523b0aa810412fd2d75f88e.svg",
            "isPro": false,
            "fullname": "kaixin zhu",
            "user": "czkk566",
            "type": "user"
          },
          "name": "Kaixin Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T09:16:44.192Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8f8",
          "user": {
            "_id": "670880950e79a8b46f7ff9dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
            "isPro": false,
            "fullname": "Juanxi Tian",
            "user": "Juanxi",
            "type": "user"
          },
          "name": "Juanxi Tian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:12.128Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8f9",
          "user": {
            "_id": "6671214c92412fd4640714eb",
            "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
            "isPro": false,
            "fullname": "bohan zeng",
            "user": "zbhpku",
            "type": "user"
          },
          "name": "Bohan Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:15.654Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fa",
          "user": {
            "_id": "64a2a8127adb12be606ec33e",
            "avatarUrl": "/avatars/f85dc39a23727a4d50f8a5f5a3865b0d.svg",
            "isPro": false,
            "fullname": "Mingbao Lin",
            "user": "mingbao",
            "type": "user"
          },
          "name": "Mingbao Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:22:58.991Z",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fb",
          "name": "Hongjuan Pei",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fc",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fd",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T17:58:18.000Z",
      "submittedOnDailyAt": "2025-03-18T01:44:02.731Z",
      "title": "Extensa reconstrucción 4D: se puede realizar una reconstrucción de alta calidad en un amplio rango de movimientos y escenas.",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "La rápida evolución de la tecnología de reconstrucción 3D ha llevado a un desarrollo similar en la investigación de reconstrucción 4D. Los métodos actuales de reconstrucción 4D pueden generar escenarios de alta calidad, pero presentan problemas en la captura de datos de video multiángulo y los benchmarks de reconstrucción 4D se centran principalmente en acciones limitadas (por ejemplo, baile) dentro de escenarios restringidos. En escenarios reales, muchos de estos contienen amplios movimientos espaciales, lo que señala claramente las limitaciones de los conjuntos de datos de reconstrucción 4D actuales. Además, los métodos de reconstrucción 4D actuales dependen de campos de deformación para evaluar la dinámica de objetos 3D, pero estos campos de deformación tienen dificultades con amplios movimientos espaciales, lo que limita la capacidad de reconstrucción de alta calidad de escenarios 4D que incluyan amplios movimientos espaciales. En este artículo, se centra en la reconstrucción de escenarios 4D que incluyen amplios movimientos espaciales y se propone un nuevo benchmark de reconstrucción 4D llamado \"WideRange4D\". Este benchmark está compuesto de datos de escenarios 4D ricos que incluyen grandes cambios espaciales, lo que permite una evaluación más detallada de la capacidad de creación de métodos de reconstrucción 4D. Además, se presenta un nuevo método de reconstrucción 4D llamado \"Progress4D\", que muestra su capacidad para generar resultados de alta calidad en tareas de reconstrucción de escenarios 4D complejos de manera estable. Se realizan experimentos de comparación cualitativa y cuantitativa en WideRange4D y se demuestra que nuestro Progress4D supera a los métodos de reconstrucción 4D más avanzados actuales. Proyecto: https://github.com/Gen-Verse/WideRange4D",
      "upvotes": 14,
      "discussionId": "67d8dd0d924be985c277c998",
      "projectPage": "https://huggingface.co/datasets/Gen-Verse/WideRange4D",
      "githubRepo": "https://github.com/Gen-Verse/WideRange4D",
      "ai_keywords": [
        "deformation fields",
        "4D reconstruction",
        "scene reconstruction",
        "multi-view video",
        "spatial movements",
        "3D objects",
        "4D scene data",
        "generation capabilities",
        "4D generation methods",
        "WideRange4D",
        "Progress4D"
      ]
    },
    "publishedAt": "2025-03-17T13:58:18.000Z",
    "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes",
    "summary": "With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps://github.com/Gen-Verse/WideRange4D",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13434",
      "authors": [
        {
          "_id": "67d916b500030726e0df2a67",
          "user": {
            "_id": "6362801380c1a705a6ea54ac",
            "avatarUrl": "/avatars/041ad5abf9be42e336938f51ebb8746c.svg",
            "isPro": false,
            "fullname": "Yaowei Li",
            "user": "Yw22",
            "type": "user"
          },
          "name": "Yaowei Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:23:48.215Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a68",
          "user": {
            "_id": "66837d3c48edefb453b0640a",
            "avatarUrl": "/avatars/b16385eaa612578728e2c6460a76b38f.svg",
            "isPro": false,
            "fullname": "Lingen Li",
            "user": "l-li",
            "type": "user"
          },
          "name": "Lingen Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:23:54.498Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a69",
          "user": {
            "_id": "658409ceca19ccf6d9989add",
            "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
            "isPro": false,
            "fullname": "Zhaoyang Zhang",
            "user": "ZyZcuhk",
            "type": "user"
          },
          "name": "Zhaoyang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:07:22.583Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6a",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6b",
          "user": {
            "_id": "6422b973ef9e8971003cdd22",
            "avatarUrl": "/avatars/8564a2e984e2e79e46d90cc9c35e5773.svg",
            "isPro": false,
            "fullname": "Guangzhi Wang",
            "user": "daoyuan98",
            "type": "user"
          },
          "name": "Guangzhi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:24:18.787Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6c",
          "user": {
            "_id": "653eb3bd4a52f10eaf72fbaf",
            "avatarUrl": "/avatars/b525482b61c6f6054bf44bbc3113c29f.svg",
            "isPro": false,
            "fullname": "Hongxiang Li",
            "user": "HongxiangLi",
            "type": "user"
          },
          "name": "Hongxiang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:24:25.700Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6d",
          "user": {
            "_id": "63184c517ca1b876d99b7e0e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
            "isPro": false,
            "fullname": "Xiaodong Cun",
            "user": "vinthony",
            "type": "user"
          },
          "name": "Xiaodong Cun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:24:32.118Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6e",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:24:39.725Z",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6f",
          "name": "Yuexian Zou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658409ceca19ccf6d9989add/fhyGe83BiDhDf9a71kUyP.mp4"
      ],
      "publishedAt": "2025-03-17T17:58:05.000Z",
      "submittedOnDailyAt": "2025-03-18T05:20:30.708Z",
      "title": "BlobCtrl: Gestión uniforme de la generación y edición de imágenes de nivel de elemento y ofrecer un marco flexible.",
      "submittedOnDailyBy": {
        "_id": "658409ceca19ccf6d9989add",
        "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
        "isPro": false,
        "fullname": "Zhaoyang Zhang",
        "user": "ZyZcuhk",
        "type": "user"
      },
      "summary": "Los trabajos de visualización a nivel de elemento son importantes en el desarrollo de contenido digital, pero los métodos basados en expansión actual presentan deficiencias en precisión y flexibilidad cuando se comparan con herramientas tradicionales. En este artículo, se presenta un marco de trabajo que integra la generación y edición a nivel de elemento utilizando representaciones blob basadas en probabilidades, llamado BlobCtrl. Utilizando blobs como base de visualización, nuestro enfoque permite la separación adecuada de información sobre la posición espacial, contenido literario e identidad, así como la manipulación precisa a nivel de elemento. Los principales contribuidores son: 1) la integración continua de fondo y trasero mediante una estructura doble de brancher utilizando fusiones de características jerárquicas; 2) un paradigma de aprendizaje autosupervisado que se ajusta a expansión de datos y funciones de puntuación; 3) estrategia de dropout para equilibrar retroalimentación y diversidad. Para futuras investigaciones, se presentan BlobData, que apoya entrenamiento a gran escala, y BlobBench, que permite evaluaciones sistemáticas. Los resultados de los experimentos demuestran que BlobCtrl presenta excelentes resultados en tareas de manipulación de elementos a nivel de componente. Además, muestra la posibilidad de ser una solución práctica y eficiente para la creación de contenido visualizado con precisión y flexibilidad. Página del proyecto: https://liyaowei-stu.github.io/project/BlobCtrl/",
      "upvotes": 12,
      "discussionId": "67d916bc00030726e0df2c3e",
      "projectPage": "https://liyaowei-stu.github.io/project/BlobCtrl/",
      "githubRepo": "https://github.com/TencentARC/BlobCtrl",
      "ai_keywords": [
        "probabilistic blob-based representation",
        "dual-branch diffusion architecture",
        "hierarchical feature fusion",
        "self-supervised training paradigm",
        "tailored data augmentation",
        "score functions",
        "controllable dropout strategies",
        "BlobData",
        "BlobBench"
      ]
    },
    "publishedAt": "2025-03-17T13:58:05.000Z",
    "title": "BlobCtrl: A Unified and Flexible Framework for Element-level Image\n  Generation and Editing",
    "summary": "Element-level visual manipulation is essential in digital content creation,\nbut current diffusion-based methods lack the precision and flexibility of\ntraditional tools. In this work, we introduce BlobCtrl, a framework that\nunifies element-level generation and editing using a probabilistic blob-based\nrepresentation. By employing blobs as visual primitives, our approach\neffectively decouples and represents spatial location, semantic content, and\nidentity information, enabling precise element-level manipulation. Our key\ncontributions include: 1) a dual-branch diffusion architecture with\nhierarchical feature fusion for seamless foreground-background integration; 2)\na self-supervised training paradigm with tailored data augmentation and score\nfunctions; and 3) controllable dropout strategies to balance fidelity and\ndiversity. To support further research, we introduce BlobData for large-scale\ntraining and BlobBench for systematic evaluation. Experiments show that\nBlobCtrl excels in various element-level manipulation tasks while maintaining\ncomputational efficiency, offering a practical solution for precise and\nflexible visual content creation. Project page:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658409ceca19ccf6d9989add/fhyGe83BiDhDf9a71kUyP.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13434.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658409ceca19ccf6d9989add",
      "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
      "fullname": "Zhaoyang Zhang",
      "name": "ZyZcuhk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11751",
      "authors": [
        {
          "_id": "67d8e861fa59a8b15a921052",
          "user": {
            "_id": "6351712b40dffad651f128c7",
            "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
            "isPro": false,
            "fullname": "Zhaofeng Wu",
            "user": "ZhaofengWu",
            "type": "user"
          },
          "name": "Zhaofeng Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:27:29.372Z",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921053",
          "user": {
            "_id": "621e9388345a1d9ab65391c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621e9388345a1d9ab65391c3/RxurNzyAWJOUdgeSHQi1R.jpeg",
            "isPro": false,
            "fullname": "Michihiro Yasunaga",
            "user": "michiyasunaga",
            "type": "user"
          },
          "name": "Michihiro Yasunaga",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:27:35.318Z",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921054",
          "name": "Andrew Cohen",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921055",
          "name": "Yoon Kim",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921056",
          "name": "Asli Celikyilmaz",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921057",
          "user": {
            "_id": "660f0fd377a1e2509aa5a679",
            "avatarUrl": "/avatars/e04ef05bed0bf6cefdc7e3e39674e2f9.svg",
            "isPro": false,
            "fullname": "Marjan Ghazvininejad",
            "user": "mghazvininejad",
            "type": "user"
          },
          "name": "Marjan Ghazvininejad",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:28:15.386Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T17:59:41.000Z",
      "submittedOnDailyAt": "2025-03-18T01:59:35.073Z",
      "title": "reWordBench: Evaluación de la robustez de modelos de recompensa utilizando entradas transformadas y mejora",
      "submittedOnDailyBy": {
        "_id": "6351712b40dffad651f128c7",
        "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
        "isPro": false,
        "fullname": "Zhaofeng Wu",
        "user": "ZhaofengWu",
        "type": "user"
      },
      "summary": "El modelo de revisión es una función estándar en la NLP moderna. Es un evaluador de texto escalable y está compuesto por múltiples componentes esenciales, como recetas de alineamiento y algoritmos de inferencia. Sin embargo, los modelos de revisión recientes han mostrado mejoras en los marcadores estándar, pero algunos han sido afectados por el sobreajuste, lo que puede obstaculizar la comprensión de sus capacidades fundamentales. En este artículo, se investiga la robustez y el grado de sobreajuste de los modelos de revisión. Construimos **reWordBench** y transformamos de manera sistemática los entradas de los modelos de revisión para mantener su significado y orden. Demostramos que los modelos de revisión más recientes sufren una considerable pérdida de rendimiento con solo algunas transformaciones de entrada, a veces incluso bajo una precisión menos que la aleatoria. Para mejorar la robustez de los modelos de revisión, proponemos que se entreneen para asignar claramente puntuaciones a frases transformadas de manera significativa. Este enfoque también mejora la robustez frente a otras transformaciones. Por ejemplo, nuestro modelo de revisión robusto puede reducir la pérdida en el subconjunto Chat Hard de RewardBench en aproximadamente la mitad. Además, cuando se utiliza alineamiento, nuestro modelo de revisión robusto muestra una mayor eficiencia, genera salidas de alta calidad y supera a los modelos de entrenamiento estándar en el 59% de los casos.",
      "upvotes": 12,
      "discussionId": "67d8e866fa59a8b15a92117c",
      "ai_keywords": [
        "reward models",
        "NLP",
        "text evaluator",
        "alignment",
        "inference-time algorithms",
        "overfitting",
        "reWordBench",
        "meaning-preserving",
        "ranking-preserving",
        "state-of-the-art",
        "performance degradation",
        "below-random accuracy",
        "brittleness",
        "paraphrases",
        "robust reward model",
        "Chat Hard subset",
        "RewardBench",
        "utility",
        "higher-quality outputs"
      ]
    },
    "publishedAt": "2025-03-14T13:59:41.000Z",
    "title": "reWordBench: Benchmarking and Improving the Robustness of Reward Models\n  with Transformed Inputs",
    "summary": "Reward models have become a staple in modern NLP, serving as not only a\nscalable text evaluator, but also an indispensable component in many alignment\nrecipes and inference-time algorithms. However, while recent reward models\nincrease performance on standard benchmarks, this may partly be due to\noverfitting effects, which would confound an understanding of their true\ncapability. In this work, we scrutinize the robustness of reward models and the\nextent of such overfitting. We build **reWordBench**, which systematically\ntransforms reward model inputs in meaning- or ranking-preserving ways. We show\nthat state-of-the-art reward models suffer from substantial performance\ndegradation even with minor input transformations, sometimes dropping to\nsignificantly below-random accuracy, suggesting brittleness. To improve reward\nmodel robustness, we propose to explicitly train them to assign similar scores\nto paraphrases, and find that this approach also improves robustness to other\ndistinct kinds of transformations. For example, our robust reward model reduces\nsuch degradation by roughly half for the Chat Hard subset in RewardBench.\nFurthermore, when used in alignment, our robust reward models demonstrate\nbetter utility and lead to higher-quality outputs, winning in up to 59% of\ninstances against a standardly trained RM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6351712b40dffad651f128c7",
      "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
      "fullname": "Zhaofeng Wu",
      "name": "ZhaofengWu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13399",
      "authors": [
        {
          "_id": "67d8d99a0983992037cdf33f",
          "user": {
            "_id": "650871aeb44445e9b3625c7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
            "isPro": false,
            "fullname": "James Burgess",
            "user": "jmhb",
            "type": "user"
          },
          "name": "James Burgess",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:18.287Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf340",
          "name": "Jeffrey J Nirschl",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf341",
          "name": "Laura Bravo-Sánchez",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf342",
          "name": "Alejandro Lozano",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf343",
          "name": "Sanket Rajan Gupte",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf344",
          "name": "Jesus G. Galaz-Montoya",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf345",
          "name": "Yuhui Zhang",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf346",
          "user": {
            "_id": "666aa5183263a8feca6b7003",
            "avatarUrl": "/avatars/6ac4d52e8abea0df9f83da408502c076.svg",
            "isPro": false,
            "fullname": "Yuchang Su",
            "user": "suyc21",
            "type": "user"
          },
          "name": "Yuchang Su",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:27:03.753Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf347",
          "name": "Disha Bhowmik",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf348",
          "name": "Zachary Coman",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf349",
          "name": "Sarina M. Hasan",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34a",
          "name": "Alexandra Johannesson",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34b",
          "name": "William D. Leineweber",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34c",
          "name": "Malvika G Nair",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34d",
          "name": "Ridhi Yarlagadda",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34e",
          "name": "Connor Zuraski",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34f",
          "name": "Wah Chiu",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf350",
          "user": {
            "_id": "655b8dbb83186f133f7f8a98",
            "avatarUrl": "/avatars/15f41d0efb3a59a2e389cdb5338e0c1e.svg",
            "isPro": false,
            "fullname": "Sarah Cohen",
            "user": "shcohen",
            "type": "user"
          },
          "name": "Sarah Cohen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:26:24.760Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf351",
          "name": "Jan N. Hansen",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf352",
          "name": "Manuel D Leonetti",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf353",
          "user": {
            "_id": "64fc5c1cc45dd732acc2ec48",
            "avatarUrl": "/avatars/b1f072cbfec014f1a054d4a433cff93c.svg",
            "isPro": false,
            "fullname": "Chad Liu",
            "user": "chadliu",
            "type": "user"
          },
          "name": "Chad Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:25:42.249Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf354",
          "user": {
            "_id": "678af263320331c7e008f842",
            "avatarUrl": "/avatars/e2cde80f018f3dd278000270fdbc104d.svg",
            "isPro": false,
            "fullname": "emma lundberg",
            "user": "lundbergemma",
            "type": "user"
          },
          "name": "Emma Lundberg",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:25:33.946Z",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf355",
          "name": "Serena Yeung-Levy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T17:33:10.000Z",
      "submittedOnDailyAt": "2025-03-18T01:06:54.667Z",
      "title": "MicroVQA: Benchmark de Microestructuras de Ciencia Basada en Visión por Visión para la Teoría de la Estructura Multimodal",
      "submittedOnDailyBy": {
        "_id": "650871aeb44445e9b3625c7b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
        "isPro": false,
        "fullname": "James Burgess",
        "user": "jmhb",
        "type": "user"
      },
      "summary": "La investigación científica exige una compleja inferencia sobre datos multimodal, un desafío que es particularmente común en la biología. Aunque ha habido avances recientes en los modelos de lenguaje multimodal (MLLMs), los test de referencia actuales para inferencia multimodal se centran en dificultades universitarias, y los test de referencia de nivel de investigación enfatizan un bajo nivel de sensibilidad, no cubriendo las complejas inferencias multimodal necesarias para la investigación científica. Para llenar esta brecha, presentamos MicroVQA. Este es un test de referencia de preguntas y respuestas visuales (VQA) que evalúa tres capacidades de inferencia cruciales en el proceso de trabajo de investigación. MicroVQA incluye 1,042 preguntas de elección múltiple (MCQs) seleccionadas con cuidado por expertos en biología de diferentes modalidades de microscopía, representando ejemplos de prácticas científicas reales. Al construir el test de referencia, descubrimos que los métodos estándar para la generación de MCQs pueden conducirse a un aprendizaje de lenguaje, proponemos una nueva pipeline en dos etapas: primero, estructurar pares de pregunta-respuesta como MCQs con ayuda de hints de modelos de lenguaje optimizados, y segundo, eliminar este aprendizaje de lenguaje mediante `RefineBot`. Al realizar el test de referencia con los MLLMs más avanzados, obtuvimos un rendimiento máximo de 53%. Aunque modelos más pequeños de lenguaje de lenguaje de lenguaje se vean ligeramente afectados, esto demuestra que la inferencia basada en lenguaje es más desafiante que la inferencia multimodal. Verificó que se puede mejorar el rendimiento al fine-tunar artículos científicos. Según el análisis de respuestas de pensamiento en cadena, los errores de sensibilidad son los más comunes, seguidos por los errores de conocimiento y luego por los errores de generalización excesiva. Estas observaciones resaltan los desafíos de la inferencia multimodal científica y demuestran que MicroVQA es una valiosa fuente para la investigación biomédica dirigida por la inteligencia artificial. MicroVQA puede obtenerse en https://huggingface.co/datasets/jmhb/microvqa y el página del proyecto está en https://jmhb0.github.io/microvqa.",
      "upvotes": 10,
      "discussionId": "67d8d99f0983992037cdf47e",
      "projectPage": "https://jmhb0.github.io/microvqa/",
      "githubRepo": "https://github.com/jmhb0/microvqa",
      "ai_keywords": [
        "multimodal large language models",
        "visual-question answering (VQA)",
        "multiple-choice questions (MCQs)",
        "biology experts",
        "microscopy modalities",
        "standard MCQ generation methods",
        "optimized LLM prompt",
        "agent-based `RefineBot'",
        "chain-of-thought responses",
        "perception errors",
        "knowledge errors",
        "overgeneralization errors"
      ]
    },
    "publishedAt": "2025-03-17T13:33:10.000Z",
    "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research",
    "summary": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13399.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650871aeb44445e9b3625c7b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
      "fullname": "James Burgess",
      "name": "jmhb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12605",
      "authors": [
        {
          "_id": "67d939f6fa59a8b15aa931a8",
          "user": {
            "_id": "64ff369d9abcc85a5519b33e",
            "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
            "isPro": false,
            "fullname": "Yaoting Wang",
            "user": "Gh0stAR",
            "type": "user"
          },
          "name": "Yaoting Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:29:33.845Z",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931a9",
          "user": {
            "_id": "64c139d867eff857ea51caa8",
            "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
            "isPro": false,
            "fullname": "Shengqiong Wu",
            "user": "ChocoWu",
            "type": "user"
          },
          "name": "Shengqiong Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:29:41.386Z",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931aa",
          "name": "Yuecheng Zhang",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931ab",
          "name": "William Wang",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931ac",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:30:17.710Z",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931ad",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "67d939f6fa59a8b15aa931ae",
          "user": {
            "_id": "647773a1168cb428e00e9a8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
            "isPro": false,
            "fullname": "Hao Fei",
            "user": "scofield7419",
            "type": "user"
          },
          "name": "Hao Fei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T09:30:49.867Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/YhPHIT3BwUWkXcOT1r1LJ.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/7SvBogepNT-uNYcY5dAgv.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/VqwQO1NsLKrjaKEBZxXUb.png"
      ],
      "publishedAt": "2025-03-16T18:39:13.000Z",
      "submittedOnDailyAt": "2025-03-18T07:53:47.429Z",
      "title": "Damodar Core Obsidian Implementation: Full Investigation",
      "submittedOnDailyBy": {
        "_id": "64ff369d9abcc85a5519b33e",
        "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
        "isPro": false,
        "fullname": "Yaoting Wang",
        "user": "Gh0stAR",
        "type": "user"
      },
      "summary": "MCoT (Multi-Type Chain of Thought) es un enfoque que aprovecha las ventajas de procesos que se desarrollan de manera humana, paso a paso, y que se pueden expandir a diferentes contextos. En particular, ha recibido atención reciente por su integración con modelos de lenguaje multi-tipo (MLLMs). Se aplica con éxito en campos como robótica, medicina, conducción autónoma y generación multi-tipo.",
      "upvotes": 8,
      "discussionId": "67d939f7fa59a8b15aa9322a",
      "projectPage": "https://github.com/yaotingwangofficial/Awesome-MCoT",
      "githubRepo": "https://github.com/yaotingwangofficial/Awesome-MCoT",
      "ai_keywords": [
        "chain-of-thought (CoT) reasoning",
        "multimodal CoT (MCoT) reasoning",
        "multimodal large language models (MLLMs)",
        "image",
        "video",
        "speech",
        "audio",
        "3D",
        "structured data",
        "robotics",
        "healthcare",
        "autonomous driving",
        "multimodal generation",
        "multimodal AGI"
      ]
    },
    "publishedAt": "2025-03-16T14:39:13.000Z",
    "title": "Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey",
    "summary": "By extending the advantage of chain-of-thought (CoT) reasoning in human-like\nstep-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning\nhas recently garnered significant research attention, especially in the\nintegration with multimodal large language models (MLLMs). Existing MCoT\nstudies design various methodologies and innovative reasoning paradigms to\naddress the unique challenges of image, video, speech, audio, 3D, and\nstructured data across different modalities, achieving extensive success in\napplications such as robotics, healthcare, autonomous driving, and multimodal\ngeneration. However, MCoT still presents distinct challenges and opportunities\nthat require further focus to ensure consistent thriving in this field, where,\nunfortunately, an up-to-date review of this domain is lacking. To bridge this\ngap, we present the first systematic survey of MCoT reasoning, elucidating the\nrelevant foundational concepts and definitions. We offer a comprehensive\ntaxonomy and an in-depth analysis of current methodologies from diverse\nperspectives across various application scenarios. Furthermore, we provide\ninsights into existing challenges and future research directions, aiming to\nfoster innovation toward multimodal AGI.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/YhPHIT3BwUWkXcOT1r1LJ.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/7SvBogepNT-uNYcY5dAgv.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/VqwQO1NsLKrjaKEBZxXUb.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12605.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ff369d9abcc85a5519b33e",
      "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
      "fullname": "Yaoting Wang",
      "name": "Gh0stAR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12937",
      "authors": [
        {
          "_id": "67d8eb0c18de6ef86c4eb457",
          "name": "Jingyi Zhang",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb458",
          "user": {
            "_id": "65237910b80dc49ba03a96d9",
            "avatarUrl": "/avatars/9d81c4c8fb2d597079e8dd9d9b79a8d8.svg",
            "isPro": false,
            "fullname": "jiaxing",
            "user": "huangjiaxing",
            "type": "user"
          },
          "name": "Jiaxing Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:01.731Z",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb459",
          "user": {
            "_id": "6590e03454f8826173ed5ee6",
            "avatarUrl": "/avatars/b2fbaaf444e1e53c5e914cd42a41389a.svg",
            "isPro": false,
            "fullname": "Huanjin Yao",
            "user": "HuanjinYao",
            "type": "user"
          },
          "name": "Huanjin Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:08.069Z",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45a",
          "user": {
            "_id": "6713afea187a20dc579e121b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6713afea187a20dc579e121b/ELxVQLVF9ifuT-TCWPK22.jpeg",
            "isPro": false,
            "fullname": "Shunyu Liu",
            "user": "liushunyu",
            "type": "user"
          },
          "name": "Shunyu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:15.587Z",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45b",
          "user": {
            "_id": "6274a9620d11b4f675085fbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651812606924-noauth.jpeg",
            "isPro": false,
            "fullname": "Xikun Zhang",
            "user": "Xikun",
            "type": "user"
          },
          "name": "Xikun Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:21.950Z",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45c",
          "name": "Shijian Lu",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45d",
          "name": "Dacheng Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T08:51:44.000Z",
      "submittedOnDailyAt": "2025-03-18T02:10:10.429Z",
      "title": "R1-VL: Método de optimización de políticas grupales relativas en grupos para el aprendizaje de lógica utilizando el modelo de lenguaje de Damo",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Recientes estudios han mejorado la capacidad de inferencia de modelos lógicos de ajuste micro basados en datos de inferencia de contenido de alta calidad (MLLMs), pero a menudo se enfocan en simplemente mimetizar los pasos de inferencia que generalmente funcionan bien, sin comprender en su totalidad qué partes están fallando. En este artículo, se supera la aproximación indirecta de la capacidad de inferencia de los MLLMs y se enfoca en mejorar dicha capacidad. Para ello, se diseñó un nuevo marco de aprendizaje en línea de eficiencia de reentrenamiento llamado \"Step-wise Group Relative Policy Optimization (StepGRPO)\". Este marco permite a los MLLMs mejorar su capacidad de inferencia a través de una compensación eficiente y densa en etapas. Específicamente, StepGRPO introduce dos nuevas reglas de compensación basadas en inferencia: \"Step-wise Reasoning Accuracy Reward (StepRAR)\" y \"Step-wise Reasoning Validity Reward (StepRVR)\". StepRAR proporciona una compensación mediante un método flexible de ajuste de pasos clave, incluyendo etapas intermedias en el paso de inferencia, mientras que StepRVR ofrece una compensación a través de la coherencia lógica del proceso de inferencia y la complejidad y estrategia de evaluación lógica. Mediante el StepGRPO, se introdujo una serie de MLLMs con excelente capacidad de inferencia en etapas, como R1-VL. Los experimentos en 8 benchmarks expandidos demostraron la excelente performance de nuestro método.",
      "upvotes": 7,
      "discussionId": "67d8eb0d18de6ef86c4eb4aa",
      "ai_keywords": [
        "Step-wise Group Relative Policy Optimization (StepGRPO)",
        "online reinforcement learning",
        "Step-wise Reasoning Accuracy Reward (StepRAR)",
        "Step-wise Reasoning Validity Reward (StepRVR)",
        "soft key-step matching",
        "reasoning completeness",
        "logic evaluation",
        "R1-VL",
        "step-by-step reasoning"
      ]
    },
    "publishedAt": "2025-03-17T04:51:44.000Z",
    "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization",
    "summary": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12937.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6390
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13444",
      "authors": [
        {
          "_id": "67d8eeb17e184aa2954d19f4",
          "name": "Ye Liu",
          "hidden": false
        },
        {
          "_id": "67d8eeb17e184aa2954d19f5",
          "user": {
            "_id": "64440be5af034cdfd69ca3a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
            "isPro": true,
            "fullname": "Qinghong (Kevin) Lin",
            "user": "KevinQHLin",
            "type": "user"
          },
          "name": "Kevin Qinghong Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:28:47.081Z",
          "hidden": false
        },
        {
          "_id": "67d8eeb17e184aa2954d19f6",
          "name": "Chang Wen Chen",
          "hidden": false
        },
        {
          "_id": "67d8eeb17e184aa2954d19f7",
          "user": {
            "_id": "661ab3da2b14565c7acccf5c",
            "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
            "isPro": false,
            "fullname": "Mike Zheng Shou",
            "user": "AnalMom",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:29:07.309Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T17:59:33.000Z",
      "submittedOnDailyAt": "2025-03-18T02:25:58.731Z",
      "title": "VideoMind: Video de larga duración y lógica de chain-of-LoRA algoritmo",
      "submittedOnDailyBy": {
        "_id": "64440be5af034cdfd69ca3a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
        "isPro": true,
        "fullname": "Qinghong (Kevin) Lin",
        "user": "KevinQHLin",
        "type": "user"
      },
      "summary": "Videos, con su dimension temporal única, exigen una comprensión precisa y fundamentada, donde las respuestas están directamente relacionadas con evidencia visual y interpretable. A pesar de los avances significativos en las capacidades de razonamiento dentro de los Grandes Modelos de Lenguaje, la razonamiento multimodal, especialmente para videos, sigue sin explorarse. En este trabajo, presentamos VideoMind, un nuevo agente de lenguaje y video diseñado para la comprensión temporal de videos. VideoMind incorpora dos innovaciones clave: (i) identificamos las capacidades esenciales para la razonamiento temporal de videos y desarrollamos un flujo de trabajo agente basado en roles, incluyendo un planificador para coordinar diferentes roles, un grounder para la localización temporal, un verificador para evaluar la precisión de los intervalos temporales y una responder para la respuesta a preguntas. (ii) Para integrar eficientemente estas roles diversas, proponemos una estrategia de cadena de LoRA novedosa, permitiendo una transición de roles sin peso mediante adaptadores LoRA ligeros, evitando así el sobrecargo de múltiples modelos, balanceando así eficiencia y flexibilidad. Experimentos extensos en 14 marcadores públicos demostraron que nuestro agente logra un rendimiento de estado de la arte en tareas diversas de comprensión de videos, incluyendo 3 en respuesta a preguntas de video fundamentado, 6 en grounding temporal de video y 5 en respuesta a preguntas de video general, destacando su efectividad en el avance del agente de video y la razonamiento temporal de largo plazo.",
      "upvotes": 6,
      "discussionId": "67d8eeb37e184aa2954d1a39",
      "ai_keywords": [
        "VideoMind",
        "temporal-grounded video understanding",
        "role-based agentic workflow",
        "planner",
        "grounder",
        "temporale localization",
        "verifier",
        "temporal interval accuracy",
        "answerer",
        "question-answering",
        "Chain-of-LoRA",
        "LoRA adaptors",
        "grounded video question-answering",
        "video temporal grounding",
        "general video question-answering",
        "temporal reasoning"
      ]
    },
    "publishedAt": "2025-03-17T13:59:33.000Z",
    "title": "VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning",
    "summary": "Videos, with their unique temporal dimension, demand precise grounded\nunderstanding, where answers are directly linked to visual, interpretable\nevidence. Despite significant breakthroughs in reasoning capabilities within\nLarge Language Models, multi-modal reasoning - especially for videos - remains\nunexplored. In this work, we introduce VideoMind, a novel video-language agent\ndesigned for temporal-grounded video understanding. VideoMind incorporates two\nkey innovations: (i) We identify essential capabilities for video temporal\nreasoning and develop a role-based agentic workflow, including a planner for\ncoordinating different roles, a grounder for temporal localization, a verifier\nto assess temporal interval accuracy, and an answerer for question-answering.\n(ii) To efficiently integrate these diverse roles, we propose a novel\nChain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA\nadaptors while avoiding the overhead of multiple models, thus balancing\nefficiency and flexibility. Extensive experiments on 14 public benchmarks\ndemonstrate that our agent achieves state-of-the-art performance on diverse\nvideo understanding tasks, including 3 on grounded video question-answering, 6\non video temporal grounding, and 5 on general video question-answering,\nunderscoring its effectiveness in advancing video agent and long-form temporal\nreasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13444.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64440be5af034cdfd69ca3a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
      "fullname": "Qinghong (Kevin) Lin",
      "name": "KevinQHLin",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13070",
      "authors": [
        {
          "_id": "67d8fbf641d31cc626e4d7b9",
          "user": {
            "_id": "65f7e6856bd4bac5b6a4ecc3",
            "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
            "isPro": false,
            "fullname": "Yihong Luo",
            "user": "Luo-Yihong",
            "type": "user"
          },
          "name": "Yihong Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:07:30.236Z",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7ba",
          "user": {
            "_id": "636a40faa6f948c4f0c62ae5",
            "avatarUrl": "/avatars/30c35b194ba84d6e274df30e91a8cc45.svg",
            "isPro": false,
            "fullname": "Tianyang Hu",
            "user": "whatlegequ",
            "type": "user"
          },
          "name": "Tianyang Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:33:01.489Z",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7bb",
          "name": "Weijian Luo",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7bc",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7bd",
          "name": "Jing Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T11:21:43.000Z",
      "submittedOnDailyAt": "2025-03-18T03:24:24.806Z",
      "title": "La compensación es suficiente para generar imágenes de alta calidad de fotos rápidamente.",
      "submittedOnDailyBy": {
        "_id": "65f7e6856bd4bac5b6a4ecc3",
        "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
        "isPro": false,
        "fullname": "Yihong Luo",
        "user": "Luo-Yihong",
        "type": "user"
      },
      "summary": "La complejidad de las imágenes generadas y su adecuación a textos complejos y preferencias humanas es un problema esencial en el contenido generado por la inteligencia artificial (AIGC). Para mejorar la posibilidad de control y confianza en modelos que transforman texto en imágenes, se ha desarrollado una excelente solución con la aplicación de un desusterreo distribuido con recompensas fortalecidas. Esto ha llevado a una cambiación fundamental en el paradigma de la generación, donde la recompensa se convierte en el motor principal, ya que las condiciones se especifican y las señales de recompensa se fortalecen. Por otro lado, se considera que el desusterreo distribuido es un ajuste excesivo. Para probar estas hipótesis, se propone un nuevo enfoque de generación condicionada llamado R0. R0 muestra la dominancia de la recompensa en condiciones complejas y propone una nueva perspectiva en la generación de imágenes, tratando la optimización como un problema en el espacio de datos sin confiar en el desusterreo distribuido. Usando R0, se puede entrenar modelos de generación de imágenes desde textos complejos a través de un simple paso, basándose en la diseño único de los parámetros de generador y en teorías de ajuste adecuadas. De esta manera, R0 desafía el proceso de post-procesamiento de la diversidad y las tradicionales perspectivas de la generación condicionada, demostrando la dominancia de la recompensa en condiciones complejas. Nuestros resultados de investigación contribuyen al desarrollo de un paradigma de generación centrado en la persona y la recompensa en el ámbito más amplio de AIGC. El código está disponible en https://github.com/Luo-Yihong/R0.",
      "upvotes": 5,
      "discussionId": "67d8fbf841d31cc626e4d812",
      "githubRepo": "https://github.com/Luo-Yihong/R0",
      "ai_keywords": [
        "reward-enhanced diffusion distillation",
        "diffusion losses",
        "R0",
        "regularized reward maximization",
        "optimization problem in data space",
        "compositional rewards",
        "generator parameterization",
        "state-of-the-art few-step text-to-image generative models",
        "diffusion post-training",
        "human-centric generation",
        "reward-centric generation paradigms"
      ]
    },
    "publishedAt": "2025-03-17T07:21:43.000Z",
    "title": "Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation",
    "summary": "Aligning generated images to complicated text prompts and human preferences\nis a central challenge in Artificial Intelligence-Generated Content (AIGC).\nWith reward-enhanced diffusion distillation emerging as a promising approach\nthat boosts controllability and fidelity of text-to-image models, we identify a\nfundamental paradigm shift: as conditions become more specific and reward\nsignals stronger, the rewards themselves become the dominant force in\ngeneration. In contrast, the diffusion losses serve as an overly expensive form\nof regularization. To thoroughly validate our hypothesis, we introduce R0, a\nnovel conditional generation approach via regularized reward maximization.\nInstead of relying on tricky diffusion distillation losses, R0 proposes a new\nperspective that treats image generations as an optimization problem in data\nspace which aims to search for valid images that have high compositional\nrewards. By innovative designs of the generator parameterization and proper\nregularization techniques, we train state-of-the-art few-step text-to-image\ngenerative models with R0 at scales. Our results challenge the conventional\nwisdom of diffusion post-training and conditional generation by demonstrating\nthat rewards play a dominant role in scenarios with complex conditions. We hope\nour findings can contribute to further research into human-centric and\nreward-centric generation paradigms across the broader field of AIGC. Code is\navailable at https://github.com/Luo-Yihong/R0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13070.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f7e6856bd4bac5b6a4ecc3",
      "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
      "fullname": "Yihong Luo",
      "name": "Luo-Yihong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11495",
      "authors": [
        {
          "_id": "67d8ca56e94f1237cb3ba3ca",
          "user": {
            "_id": "667ee096b0fad0fdee319ed4",
            "avatarUrl": "/avatars/d9df687e8522d47f7fcefe40fd9b575b.svg",
            "isPro": false,
            "fullname": "Zixu Cheng",
            "user": "Cade921",
            "type": "user"
          },
          "name": "Zixu Cheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:31:58.589Z",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cb",
          "user": {
            "_id": "65e1b6e9501590df0173cbd3",
            "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
            "isPro": false,
            "fullname": "Jian Hu",
            "user": "lwpyh",
            "type": "user"
          },
          "name": "Jian Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:20.643Z",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cc",
          "name": "Ziquan Liu",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cd",
          "user": {
            "_id": "635f8ed47c05eb9f59963d3a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg",
            "isPro": false,
            "fullname": "ChenyangSi",
            "user": "ChenyangSi",
            "type": "user"
          },
          "name": "Chenyang Si",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:32:21.924Z",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3ce",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cf",
          "name": "Shaogang Gong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65e1b6e9501590df0173cbd3/A2JrOm6FBAq5keCRRdVMK.png"
      ],
      "publishedAt": "2025-03-14T15:21:44.000Z",
      "submittedOnDailyAt": "2025-03-18T01:11:08.161Z",
      "title": "V-STaR: Benchmark de LLMs de Imágenes para Espacio Espectral-Espacio Temporal-Causalidad",
      "submittedOnDailyBy": {
        "_id": "65e1b6e9501590df0173cbd3",
        "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
        "isPro": false,
        "fullname": "Jian Hu",
        "user": "lwpyh",
        "type": "user"
      },
      "summary": "La humanidad comprende la causa de los cines mediante la lógica espacial-temporal secuencial. Primero, se especifica un marco relacionado (\"en qué momento\"), luego se analiza la relación espacial de los objetos clave (\"dónde están\"), y finalmente se utiliza esta relación para hacer inferencias (\"qué sucede o qué se hace\"). Sin embargo, la naturaleza de los grandes modelos de lenguaje de video (Video-LLMs) en cines es incierta respecto a su capacidad para encontrar causas mediante la lógica espacial-temporal secuencial. Los marcos de evaluación actuales de Video-LLMs se centran principalmente en la existencia de objetos, superando así la lógica relacional. Por lo tanto, es difícil evaluar si los modelos realmente comprenden las interacciones entre los objetos (acciones/eventos) en los cines o si generan respuestas basadas en la \"memoria\" de estas interacciones. Este artículo introduce el benchmark de lógica espacial-temporal de cines (V-STaR) para resolver estas limitaciones. La idea principal es decompor el entendimiento de los cines en tareas de lógica espacial-temporal inversa (RSTR), evaluando simultáneamente la existencia de objetos, el momento de los eventos y sus ubicaciones, y comprender la lógica de razonamiento de cadena (CoT) subyacente. Para evaluar este proceso, se construye un conjunto de datos que extrae la tratamiento de la lógica espacial-temporal de Video-LLMs. Este conjunto de datos se genera automáticamente a través de un proceso de retroalimentación en un trabajo de feedback core basado en GPT-4, que mimetiza la cognición humana y incluye cadenas explícitas de razonamiento. En 14 experimentos con Video-LLMs, nuestro V-STaR demostró claramente la necesidad y la robustez de una lógica espacial-temporal consistente en comparación con los Video-LLMs actuales.",
      "upvotes": 5,
      "discussionId": "67d8ca59e94f1237cb3ba47c",
      "ai_keywords": [
        "Video Large Language Models (Video-LLMs)",
        "Reverse Spatio-Temporal Reasoning (RSTR)",
        "Chain-of-thought (CoT)",
        "GPT-4",
        "Video Spatio-Temporal Reasoning (V-STaR)",
        "CoT questions",
        "CoT logic",
        "human cognition"
      ]
    },
    "publishedAt": "2025-03-14T11:21:44.000Z",
    "title": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning",
    "summary": "Human processes video reasoning in a sequential spatio-temporal reasoning\nlogic, we first identify the relevant frames (\"when\") and then analyse the\nspatial relationships (\"where\") between key objects, and finally leverage these\nrelationships to draw inferences (\"what\"). However, can Video Large Language\nModels (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in\nvideos? Existing Video-LLM benchmarks primarily focus on assessing object\npresence, neglecting relational reasoning. Consequently, it is difficult to\nmeasure whether a model truly comprehends object interactions (actions/events)\nin videos or merely relies on pre-trained \"memory\" of co-occurrences as biases\nin generating answers. In this work, we introduce a Video Spatio-Temporal\nReasoning (V-STaR) benchmark to address these shortcomings. The key idea is to\ndecompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR)\ntask that simultaneously evaluates what objects are present, when events occur,\nand where they are located while capturing the underlying Chain-of-thought\n(CoT) logic. To support this evaluation, we construct a dataset to elicit the\nspatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine\nCoT questions generated by a semi-automated GPT-4-powered pipeline, embedding\nexplicit reasoning chains to mimic human cognition. Experiments from 14\nVideo-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and\nthe needs for robust and consistent spatio-temporal reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e1b6e9501590df0173cbd3/A2JrOm6FBAq5keCRRdVMK.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11495.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e1b6e9501590df0173cbd3",
      "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
      "fullname": "Jian Hu",
      "name": "lwpyh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11412",
      "authors": [
        {
          "_id": "67d8f8b77f61dda9ea6512b7",
          "name": "Shiyuan Yang",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512b8",
          "user": {
            "_id": "678f49878af7a399877b87c0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3IHnM4AKbIW_wmL15wdZf.png",
            "isPro": false,
            "fullname": "GuZheng",
            "user": "GuZheng",
            "type": "user"
          },
          "name": "Zheng Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:33:49.096Z",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512b9",
          "user": {
            "_id": "64560a2aaaaf85a98fa9a4b9",
            "avatarUrl": "/avatars/e81e21f353baf48f0d91bf29ad200eea.svg",
            "isPro": false,
            "fullname": "Liang Hou",
            "user": "lianghou",
            "type": "user"
          },
          "name": "Liang Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:33:57.501Z",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512ba",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512bb",
          "user": {
            "_id": "662f93942510ef5735d7ad00",
            "avatarUrl": "/avatars/dc9486db75869ce902d0a638eea126bd.svg",
            "isPro": false,
            "fullname": "magicwpf",
            "user": "magicwpf",
            "type": "user"
          },
          "name": "Pengfei Wan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-18T04:42:06.437Z",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512bc",
          "user": {
            "_id": "67be7cd21616162fc336cb44",
            "avatarUrl": "/avatars/e58cc3c2d1484419222a5ccfc11f5c48.svg",
            "isPro": false,
            "fullname": "Xiaodong Chen",
            "user": "XiaodongChen",
            "type": "user"
          },
          "name": "Xiaodong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:34:15.858Z",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512bd",
          "user": {
            "_id": "65e77726767bfc7d109c45bf",
            "avatarUrl": "/avatars/24e68c86e06055ea1209598ba49ce8b9.svg",
            "isPro": false,
            "fullname": "Jing Liao",
            "user": "CeciliaJL",
            "type": "user"
          },
          "name": "Jing Liao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:34:23.653Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63316d499e3604f3f17f5d89/jV8ETjZh0NTDoFBhSNv8I.mp4"
      ],
      "publishedAt": "2025-03-14T13:54:10.000Z",
      "submittedOnDailyAt": "2025-03-18T03:20:01.050Z",
      "title": "MTV-Inpaint: Multi-Task Long Video Inpainting",
      "submittedOnDailyBy": {
        "_id": "63316d499e3604f3f17f5d89",
        "avatarUrl": "/avatars/c7e0c8bdd7e598276cfd133d5222c5e8.svg",
        "isPro": false,
        "fullname": "catfood",
        "user": "ysy31415926",
        "type": "user"
      },
      "summary": "Video inpainting incluye la modificación de áreas específicas dentro de un video y garantiza la coherencia espacial y temporal. Los métodos actuales se centran principalmente en la completación espacial (completar áreas vacías), pero tienen una limitada capacidad para insertar objetos nuevos de manera controlada en el espacio. A pesar de esto, el desarrollo reciente de modelos de difusión de texto a video (T2V) ha conectado a la inpainting de video guiado por texto. Sin embargo, aplicar directamente un modelo T2V para inpainting es complejo debido a la integración de dos tareas (completar y insertar), la posibilidad de control del input, las limitaciones en procesar videos largos y la flexibilidad de aplicación. Para resolver estos problemas, proponemos MTV-Inpaint, un marco de trabajo integrado que aborda tanto la completación espacial como la inserción de nuevos objetos en un video. Para integrar estas tareas, diseñamos una estructura de atención espacial en la U-Net de difusión T2V de manera dual, lo que permite combinar la diferencia entre completación espacial y inserción de objetos. MTV-Inpaint también incluye un modo de inpainting de imagen a video (I2V) guiado por texto, proporcionando diversas opciones de control. Además, presentamos una pipeline de dos etapas que combina inpainting de frame clave y propagación de frames indirectos, lo que permite procesar videos largos de cientos de frames eficientemente. Las experimentaciones extendidas muestran que MTV-Inpaint presenta los mejores resultados en ambas tareas, así como una amplia gama de aplicaciones expandibles, como inpainting multi-modo, edición y eliminación de objetos, creación de brasas de objetos en imagenes y procesamiento de videos largos. La página del proyecto está disponible en https://mtv-inpaint.github.io/.",
      "upvotes": 5,
      "discussionId": "67d8f8bf7f61dda9ea6514a7",
      "projectPage": "https://mtv-inpaint.github.io/",
      "ai_keywords": [
        "text-to-video (T2V) diffusion models",
        "text-guided video inpainting",
        "dual-branch spatial attention mechanism",
        "T2V diffusion U-Net",
        "multimodal control",
        "image-to-video (I2V) inpainting mode",
        "keyframe inpainting",
        "in-between frame propagation",
        "multi-modal inpainting",
        "object editing",
        "object removal",
        "image object brush"
      ]
    },
    "publishedAt": "2025-03-14T09:54:10.000Z",
    "title": "MTV-Inpaint: Multi-Task Long Video Inpainting",
    "summary": "Video inpainting involves modifying local regions within a video, ensuring\nspatial and temporal consistency. Most existing methods focus primarily on\nscene completion (i.e., filling missing regions) and lack the capability to\ninsert new objects into a scene in a controllable manner. Fortunately, recent\nadvancements in text-to-video (T2V) diffusion models pave the way for\ntext-guided video inpainting. However, directly adapting T2V models for\ninpainting remains limited in unifying completion and insertion tasks, lacks\ninput controllability, and struggles with long videos, thereby restricting\ntheir applicability and flexibility. To address these challenges, we propose\nMTV-Inpaint, a unified multi-task video inpainting framework capable of\nhandling both traditional scene completion and novel object insertion tasks. To\nunify these distinct tasks, we design a dual-branch spatial attention mechanism\nin the T2V diffusion U-Net, enabling seamless integration of scene completion\nand object insertion within a single framework. In addition to textual\nguidance, MTV-Inpaint supports multimodal control by integrating various image\ninpainting models through our proposed image-to-video (I2V) inpainting mode.\nAdditionally, we propose a two-stage pipeline that combines keyframe inpainting\nwith in-between frame propagation, enabling MTV-Inpaint to effectively handle\nlong videos with hundreds of frames. Extensive experiments demonstrate that\nMTV-Inpaint achieves state-of-the-art performance in both scene completion and\nobject insertion tasks. Furthermore, it demonstrates versatility in derived\napplications such as multi-modal inpainting, object editing, removal, image\nobject brush, and the ability to handle long videos. Project page:\nhttps://mtv-inpaint.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63316d499e3604f3f17f5d89/jV8ETjZh0NTDoFBhSNv8I.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11412.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63316d499e3604f3f17f5d89",
      "avatarUrl": "/avatars/c7e0c8bdd7e598276cfd133d5222c5e8.svg",
      "fullname": "catfood",
      "name": "ysy31415926",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10704",
      "authors": [
        {
          "_id": "67d7eba831dd5b46c3e6fdcb",
          "user": {
            "_id": "633c2310c0fb6fd232f0accf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633c2310c0fb6fd232f0accf/AnvEvxMtd-BpSZbaoA4ha.jpeg",
            "isPro": false,
            "fullname": "Wang Jing",
            "user": "k-nick",
            "type": "user"
          },
          "name": "Jing Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:09:01.169Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdcc",
          "user": {
            "_id": "64b8c1a995bd42c7707f7918",
            "avatarUrl": "/avatars/08c2929f8f150ecd6f8e5a06c4cb9034.svg",
            "isPro": false,
            "fullname": "Fengzhuo Zhang",
            "user": "Fengzhuo",
            "type": "user"
          },
          "name": "Fengzhuo Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:34:37.712Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdcd",
          "user": {
            "_id": "67aa01782183876b1ec5760f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hZd1iBn_2yjHVoavcXPQo.png",
            "isPro": false,
            "fullname": "xiaolili",
            "user": "xiaolili",
            "type": "user"
          },
          "name": "Xiaoli Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:34:47.006Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdce",
          "name": "Vincent Y. F. Tan",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdcf",
          "user": {
            "_id": "661a4a556fb488fa078c60aa",
            "avatarUrl": "/avatars/c77401fa9c6d2db896b4a337bb3f8add.svg",
            "isPro": false,
            "fullname": "Tianyu Pang",
            "user": "TIanyupang",
            "type": "user"
          },
          "name": "Tianyu Pang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:35:17.732Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdd0",
          "user": {
            "_id": "632407c892e07e3ca20aca28",
            "avatarUrl": "/avatars/23b51b37b12b51a0947f687d1de4d3b5.svg",
            "isPro": false,
            "fullname": "Chao Du",
            "user": "duchao",
            "type": "user"
          },
          "name": "Chao Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:35:29.239Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdd1",
          "user": {
            "_id": "664aab898fa42b4fe70ebf52",
            "avatarUrl": "/avatars/a38455fd17bbc74ce3111f2c3da9aa59.svg",
            "isPro": false,
            "fullname": "Aixin Sun",
            "user": "aixinsun",
            "type": "user"
          },
          "name": "Aixin Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:35:35.567Z",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdd2",
          "user": {
            "_id": "6397873ec0b27f432db8693f",
            "avatarUrl": "/avatars/1db65fe55002ad5c137c4a59bbcd239d.svg",
            "isPro": false,
            "fullname": "Zhuoran Yang",
            "user": "zhuoran",
            "type": "user"
          },
          "name": "Zhuoran Yang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-17T09:30:18.764Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T15:32:44.000Z",
      "submittedOnDailyAt": "2025-03-18T03:10:46.254Z",
      "title": "Modelo automático de retroalimentación para el análisis de errores en videodifusores: un solo marco integrado",
      "submittedOnDailyBy": {
        "_id": "633c2310c0fb6fd232f0accf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633c2310c0fb6fd232f0accf/AnvEvxMtd-BpSZbaoA4ha.jpeg",
        "isPro": false,
        "fullname": "Wang Jing",
        "user": "k-nick",
        "type": "user"
      },
      "summary": "Varios modelos de difusión video de retroceso automático (ARVDM) han logrado un éxito notable en la generación de videos de larga duración realista. Sin embargo, la análisis teórico de estos modelos es insuficiente. En este artículo, se desarrolla la base teórica de estos modelos y se utiliza para mejorar el rendimiento de los modelos existentes. Primero, se desarrolla un marco integrado de ARVDM, Meta-ARVDM, que integra todos los métodos existentes para ARVDM. Mediante Meta-ARVDM, se analiza la variación de Kullback-Leibler (KL) entre el video generado por Meta-ARVDM y el video real. El análisis revela dos fenómenos importantes únicos de ARVDM: el acumulación de errores y el botellaje de memoria. Se calculan resultados de imposibilidad teórica que muestran que el fenómeno del botellaje de memoria no puede ser evitado. Para mitigar el botellaje de memoria, se diseñan diferentes estructuras de redes que utilizan explícitamente los frames pasados. Además, se mejora significativamente la mitigación del botellaje de memoria y la eficiencia de la inferencia al compresionar los frames. Los resultados de los experimentos realizados en DMLab y Minecraft prueban el efecto de los métodos propuestos en este artículo. Además, se muestran las ondas de ondas de errores acumulados y del botellaje de memoria.",
      "upvotes": 4,
      "discussionId": "67d7ebaa31dd5b46c3e6fe5a",
      "projectPage": "https://sail-sg.github.io/AR-Video-Diffusion",
      "ai_keywords": [
        "Auto-Regressive Video Diffusion Models",
        "Meta-ARVDM",
        "KL-divergence",
        "error accumulation",
        "memory bottleneck",
        "information-theoretic impossibility result",
        "network structures",
        "frame compression",
        "DMLab",
        "Minecraft",
        "Pareto-frontier"
      ]
    },
    "publishedAt": "2025-03-12T11:32:44.000Z",
    "title": "Error Analyses of Auto-Regressive Video Diffusion Models: A Unified\n  Framework",
    "summary": "A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved\nremarkable successes in generating realistic long-form videos. However,\ntheoretical analyses of these models remain scant. In this work, we develop\ntheoretical underpinnings for these models and use our insights to improve the\nperformance of existing models. We first develop Meta-ARVDM, a unified\nframework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we\nanalyze the KL-divergence between the videos generated by Meta-ARVDM and the\ntrue videos. Our analysis uncovers two important phenomena inherent to ARVDM --\nerror accumulation and memory bottleneck. By deriving an information-theoretic\nimpossibility result, we show that the memory bottleneck phenomenon cannot be\navoided. To mitigate the memory bottleneck, we design various network\nstructures to explicitly use more past frames. We also achieve a significantly\nimproved trade-off between the mitigation of the memory bottleneck and the\ninference efficiency by compressing the frames. Experimental results on DMLab\nand Minecraft validate the efficacy of our methods. Our experiments also\ndemonstrate a Pareto-frontier between the error accumulation and memory\nbottleneck across different methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10704.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633c2310c0fb6fd232f0accf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633c2310c0fb6fd232f0accf/AnvEvxMtd-BpSZbaoA4ha.jpeg",
      "fullname": "Wang Jing",
      "name": "k-nick",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10719",
      "authors": [
        {
          "_id": "67d91dadb533888991ade4e1",
          "user": {
            "_id": "672c6f3d4c1e2de12c6f174e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mv-8GX6OBBpJLzVvyPYbz.png",
            "isPro": false,
            "fullname": "Yehang Zhang",
            "user": "Buzz-lightyear",
            "type": "user"
          },
          "name": "Yehang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:07:19.937Z",
          "hidden": false
        },
        {
          "_id": "67d91dadb533888991ade4e2",
          "user": {
            "_id": "64b4ab62eec33e27dcd733b5",
            "avatarUrl": "/avatars/0a9bf220c9a5efe7279f9b287b087d36.svg",
            "isPro": false,
            "fullname": "Xinli XU",
            "user": "Xxlbigbrother",
            "type": "user"
          },
          "name": "Xinli Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:35:50.412Z",
          "hidden": false
        },
        {
          "_id": "67d91dadb533888991ade4e3",
          "name": "Xiaojie Xu",
          "hidden": false
        },
        {
          "_id": "67d91dadb533888991ade4e4",
          "name": "Li Liu",
          "hidden": false
        },
        {
          "_id": "67d91dadb533888991ade4e5",
          "user": {
            "_id": "655cba1d87b67834000590e8",
            "avatarUrl": "/avatars/3bd43b7c9351f65b8f38f4c8237a0146.svg",
            "isPro": false,
            "fullname": "Yingcong Chen",
            "user": "yingcongchen",
            "type": "user"
          },
          "name": "Yingcong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:19.820Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T07:58:23.000Z",
      "submittedOnDailyAt": "2025-03-18T06:41:21.358Z",
      "title": "Traducción al español:\n\n\"En la síntesis de audios de larga duración, el uso efectivo de técnicas de procesamiento audio es crucial para mejorar la calidad del sonido, crear nuevos sonidos, y adaptar el audio a diferentes contextos. Estas técnicas pueden incluir el procesamiento digital, el análisis de señales, y la generación de sonidos. El objetivo es crear audios que sean más claros, más suaves, y que resuenen de manera más natural, mejorando la experiencia del oyente. Además, la síntesis de audio puede ser utilizada para crear sonidos especiales, para reemplazar o mejorar partes del sonido original, y para adaptar el audio a diferentes ambientes o dispositivos. Esto requiere un profundo conocimiento de la teoría de sonido, la experiencia práctica, y la utilización de herramientas de procesamiento audio avanzadas.\"",
      "submittedOnDailyBy": {
        "_id": "672c6f3d4c1e2de12c6f174e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mv-8GX6OBBpJLzVvyPYbz.png",
        "isPro": false,
        "fullname": "Yehang Zhang",
        "user": "Buzz-lightyear",
        "type": "user"
      },
      "summary": "La síntesis de voz a partir de vídeo es una tecnología que genera vozes sincronizadas con el contenido visual, lo que significa que puede significativamente mejorar la participación y la narración en películas y medios interactivos. Sin embargo, la traducción de voz a partir de vídeo para contenidos largos (por ejemplo, películas) sigue presentando problemas que no han sido resueltos, como la movilidad dinámica del significado, la desacordo temporal y la falta de conjuntos de datos específicos. Los métodos existentes funcionan bien para vídeos cortos, pero fallan cuando se trata de contenidos largos, debido a la falta de división de la síntesis y la falta de coherencia espectral. Proponemos un nuevo marco de trabajo multi-agente llamado \"LVAS-Agent\", que modela el flujo de trabajo de una traducción profesional. Nuestro enfoque se desglosa en cuatro etapas: división del escenario, generación de scripts, diseño de sonidos y síntesis de voz. Las innovaciones principales incluyen el uso de un mecanismo de revisión y modificación para la precisión y el ajuste temporal y significativo de los escenarios/scripts. Para evaluar el sistema de manera sistemática, presentamos el primer benchmark \"LVAS-Bench\", que incluye varios videos largos editados por 207 profesionales. Los experimentos muestran un mejor ajuste sonoro y visual que los métodos de referencia. Página del proyecto: https://lvas-agent.github.io",
      "upvotes": 2,
      "discussionId": "67d91dafb533888991ade557",
      "projectPage": "https://lvas-agent.github.io/",
      "ai_keywords": [
        "scene segmentation",
        "script generation",
        "sound design",
        "audio synthesis",
        "discussion-correction mechanism",
        "generation-retrieval loop",
        "temporal-semantic alignment",
        "LVAS-Agent",
        "LVAS-Bench",
        "audio-visual alignment"
      ]
    },
    "publishedAt": "2025-03-13T03:58:23.000Z",
    "title": "Long-Video Audio Synthesis with Multi-Agent Collaboration",
    "summary": "Video-to-audio synthesis, which generates synchronized audio for visual\ncontent, critically enhances viewer immersion and narrative coherence in film\nand interactive media. However, video-to-audio dubbing for long-form content\nremains an unsolved challenge due to dynamic semantic shifts, temporal\nmisalignment, and the absence of dedicated datasets. While existing methods\nexcel in short videos, they falter in long scenarios (e.g., movies) due to\nfragmented synthesis and inadequate cross-scene consistency. We propose\nLVAS-Agent, a novel multi-agent framework that emulates professional dubbing\nworkflows through collaborative role specialization. Our approach decomposes\nlong-video synthesis into four steps including scene segmentation, script\ngeneration, sound design and audio synthesis. Central innovations include a\ndiscussion-correction mechanism for scene/script refinement and a\ngeneration-retrieval loop for temporal-semantic alignment. To enable systematic\nevaluation, we introduce LVAS-Bench, the first benchmark with 207\nprofessionally curated long videos spanning diverse scenarios. Experiments\ndemonstrate superior audio-visual alignment over baseline methods. Project\npage: https://lvas-agent.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10719.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "672c6f3d4c1e2de12c6f174e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mv-8GX6OBBpJLzVvyPYbz.png",
      "fullname": "Yehang Zhang",
      "name": "Buzz-lightyear",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13369",
      "authors": [
        {
          "_id": "67d8e38e5fde3b1be16874e4",
          "user": {
            "_id": "64b214c4f4361a032002cdcf",
            "avatarUrl": "/avatars/3188a4edec4d6c945a0ed9f36050a03c.svg",
            "isPro": false,
            "fullname": "Andrew Wan Ju Kang",
            "user": "soarhigh",
            "type": "user"
          },
          "name": "Wan Ju Kang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T08:08:01.967Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e5",
          "user": {
            "_id": "628e3b87a2cb9819d4391ba6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653488512816-noauth.jpeg",
            "isPro": false,
            "fullname": "Eunki Kim",
            "user": "eunkey",
            "type": "user"
          },
          "name": "Eunki Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:36.532Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e6",
          "user": {
            "_id": "65cccd8c80d3c4b865d3b262",
            "avatarUrl": "/avatars/e6f6d8f06dd54e1e7b6d686835a9c075.svg",
            "isPro": false,
            "fullname": "Na Min An",
            "user": "namin0202",
            "type": "user"
          },
          "name": "Na Min An",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:43.821Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e7",
          "user": {
            "_id": "62f2638d04674e28535d40f8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672818467177-62f2638d04674e28535d40f8.png",
            "isPro": false,
            "fullname": "Sangryul Kim",
            "user": "sangryul",
            "type": "user"
          },
          "name": "Sangryul Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:49.867Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e8",
          "user": {
            "_id": "6639807fc9648d06126d7ec4",
            "avatarUrl": "/avatars/d996b5102ae9092da6db5f44f5142b54.svg",
            "isPro": false,
            "fullname": "haemin choi",
            "user": "hammnii",
            "type": "user"
          },
          "name": "Haemin Choi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:36:55.951Z",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874e9",
          "name": "Ki Hoon Kwak",
          "hidden": false
        },
        {
          "_id": "67d8e38e5fde3b1be16874ea",
          "name": "James Thorne",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T16:52:46.000Z",
      "submittedOnDailyAt": "2025-03-18T06:53:48.028Z",
      "title": "Señal de: Uso de retroalimentación de visión para el desarrollo de un conjunto de datos de explicaciones de diagramas correspondientes a BLV",
      "submittedOnDailyBy": {
        "_id": "64b214c4f4361a032002cdcf",
        "avatarUrl": "/avatars/3188a4edec4d6c945a0ed9f36050a03c.svg",
        "isPro": false,
        "fullname": "Andrew Wan Ju Kang",
        "user": "soarhigh",
        "type": "user"
      },
      "summary": "Por lo general, las demandas y capacidades visuales de los Grupos de Análisis y de los Usuarios Finales son diferentes. La generación de explicaciones detalladas de imágenes para usuarios como los ciegos y los con visión reducida (BLV) constituye un área compleja. Los Grupos de Análisis con visión son capaces de explicar fácilmente el contenido visual, pero, según investigaciones previas, crear estas explicaciones directamente es costoso, sesgado y poco adecuado a los estándares de los BLV. En este estudio, no solicitamos que los personas con visión evalúen las explicaciones de las imágenes ni que crean ellas. El Modelo de Lenguaje y Visión (VLM) se guia por subconjeturas potenciales y realiza múltiples etapas de inferencia para evaluar las explicaciones de las imágenes generadas. La evaluación de personas con visión puede ser efectiva y útil para profesionales de educación especializados en enseñar a estudiantes con discapacidad visual, como los BLV. Se publica Sightation, un conjunto de datos de 5k explicaciones de imágenes y 137k muestras, que se utiliza para entrenamiento en tareas de reconocimiento, búsqueda, respuesta a preguntas y lógica, y se muestra también la posibilidad de fine-tuning.",
      "upvotes": 1,
      "discussionId": "67d8e3905fde3b1be168759f",
      "projectPage": "https://huggingface.co/Sightation",
      "ai_keywords": [
        "vision-language models (VLM)",
        "latent supervision",
        "multi-pass inference",
        "diagram description datasets",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-03-17T12:52:46.000Z",
    "title": "Sightation Counts: Leveraging Sighted User Feedback in Building a\n  BLV-aligned Dataset of Diagram Descriptions",
    "summary": "Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13369.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b214c4f4361a032002cdcf",
      "avatarUrl": "/avatars/3188a4edec4d6c945a0ed9f36050a03c.svg",
      "fullname": "Andrew Wan Ju Kang",
      "name": "soarhigh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12530",
      "authors": [
        {
          "_id": "67d8f0fa53f713733d6c6b1c",
          "user": {
            "_id": "6737d99b728a96aa64a2b00a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/p3vMX0xkvL_4IpWwyURLM.png",
            "isPro": false,
            "fullname": "Hunter Sawyer",
            "user": "HTSawyer",
            "type": "user"
          },
          "name": "Hunter Sawyer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:37:16.399Z",
          "hidden": false
        },
        {
          "_id": "67d8f0fa53f713733d6c6b1d",
          "user": {
            "_id": "63c19eb3a0ffa3857eae2efa",
            "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
            "isPro": false,
            "fullname": "Jesse Roberts",
            "user": "JesseTNRoberts",
            "type": "user"
          },
          "name": "Jesse Roberts",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-18T04:05:16.739Z",
          "hidden": false
        },
        {
          "_id": "67d8f0fa53f713733d6c6b1e",
          "user": {
            "_id": "675ec603e4d6d0e820ad9d3f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ruPDRCZlcbDhZ6zp4xi_P.png",
            "isPro": false,
            "fullname": "Olzhas",
            "user": "KyleMoore",
            "type": "user"
          },
          "name": "Kyle Moore",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-18T09:37:25.148Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:50:54.000Z",
      "submittedOnDailyAt": "2025-03-18T02:37:23.781Z",
      "title": "Visión y Lenguaje Models (Vision Language Models)",
      "submittedOnDailyBy": {
        "_id": "63c19eb3a0ffa3857eae2efa",
        "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
        "isPro": false,
        "fullname": "Jesse Roberts",
        "user": "JesseTNRoberts",
        "type": "user"
      },
      "summary": "El campo de la psicología ha reconocido durante mucho tiempo la clasificación básica de los estímulos visuales en relación con la etiquetado presentado por Rosch en 1976. Esta clasificación básica incluye tareas de lenguaje visual como el suministro, y aumenta la densidad de información y la comprensión humana. En este artículo, se investiga la clasificación básica en dos modelos de lenguaje de negocios abiertos (VLMs) recientemente publicados. Se encontró que Llama 3.2 Vision Instruct (11B) y Molmo 7B-D prefieren una clasificación básica que coincide con el comportamiento humano, mostrando efectos biológicos e no biológicos, así como variaciones en la clasificación básica de expertos existentes. Estos resultados demuestran que los VLMs aprenden comportamientos cognitivos humanos a partir de datos entrenados.",
      "upvotes": 1,
      "discussionId": "67d8f0fc53f713733d6c6b89",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "basic level categorization",
        "biological versus non-biological basic level effects",
        "expert basic level shift",
        "cognitive categorization behaviors"
      ]
    },
    "publishedAt": "2025-03-16T10:50:54.000Z",
    "title": "Basic Category Usage in Vision Language Models",
    "summary": "The field of psychology has long recognized a basic level of categorization\nthat humans use when labeling visual stimuli, a term coined by Rosch in 1976.\nThis level of categorization has been found to be used most frequently, to have\nhigher information density, and to aid in visual language tasks with priming in\nhumans. Here, we investigate basic level categorization in two recently\nreleased, open-source vision-language models (VLMs). This paper demonstrates\nthat Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic level\ncategorization consistent with human behavior. Moreover, the models'\npreferences are consistent with nuanced human behaviors like the biological\nversus non-biological basic level effects and the well established expert basic\nlevel shift, further suggesting that VLMs acquire cognitive categorization\nbehaviors from the human data on which they are trained.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12530.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c19eb3a0ffa3857eae2efa",
      "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
      "fullname": "Jesse Roberts",
      "name": "JesseTNRoberts",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12528",
      "authors": [
        {
          "_id": "67d8f044f8b0e148f60cef0d",
          "name": "Kyle Moore",
          "hidden": false
        },
        {
          "_id": "67d8f044f8b0e148f60cef0e",
          "user": {
            "_id": "63c19eb3a0ffa3857eae2efa",
            "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
            "isPro": false,
            "fullname": "Jesse Roberts",
            "user": "JesseTNRoberts",
            "type": "user"
          },
          "name": "Jesse Roberts",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-18T04:02:14.260Z",
          "hidden": false
        },
        {
          "_id": "67d8f044f8b0e148f60cef0f",
          "name": "Daryl Watson",
          "hidden": false
        },
        {
          "_id": "67d8f044f8b0e148f60cef10",
          "name": "Pamela Wisniewski",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:45:43.000Z",
      "submittedOnDailyAt": "2025-03-18T02:34:14.582Z",
      "title": "Investigación de la Incertidumbre en Modelos de Lenguaje de Respuesta Humana",
      "submittedOnDailyBy": {
        "_id": "63c19eb3a0ffa3857eae2efa",
        "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
        "isPro": false,
        "fullname": "Jesse Roberts",
        "user": "JesseTNRoberts",
        "type": "user"
      },
      "summary": "Recientes estudios se centran en cuantificar la incertidumbre de los modelos de lenguaje de gran escala y en ajustar la control del modelo y la relación de confianza con el usuario. Los estudios previos han enfocado en métodos de medida teóricamente fundamentados para la incertidumbre y en métodos de medida que reflejan el comportamiento promedio del modelo. En este estudio, investigamos diversos métodos de medida de la incertidumbre y buscamos determinar métodos de medida relacionados con la incertidumbre a nivel de grupo de usuarios. Hemos encontrado que las medidas bayesianas y las variantes de la medida de entropía (top-k entropía) coinciden con el comportamiento del usuario según el tamaño del modelo. Además, algunos métodos fuertes han mostrado una disminución de la similitud con el usuario según el tamaño del modelo, pero hemos confirmado que combinando diversos métodos de medida de la incertidumbre a través de múltiples regresiones lineales, es posible reducir la dependencia del tamaño del modelo y aumentar la respuesta del usuario.",
      "upvotes": 1,
      "discussionId": "67d8f046f8b0e148f60cef95",
      "ai_keywords": [
        "Bayesian measures",
        "entropy measures",
        "top-k entropy",
        "human-similarity",
        "human-alignment",
        "multiple linear regression"
      ]
    },
    "publishedAt": "2025-03-16T10:45:43.000Z",
    "title": "Investigating Human-Aligned Large Language Model Uncertainty",
    "summary": "Recent work has sought to quantify large language model uncertainty to\nfacilitate model control and modulate user trust. Previous works focus on\nmeasures of uncertainty that are theoretically grounded or reflect the average\novert behavior of the model. In this work, we investigate a variety of\nuncertainty measures, in order to identify measures that correlate with human\ngroup-level uncertainty. We find that Bayesian measures and a variation on\nentropy measures, top-k entropy, tend to agree with human behavior as a\nfunction of model size. We find that some strong measures decrease in\nhuman-similarity with model size, but, by multiple linear regression, we find\nthat combining multiple uncertainty measures provide comparable human-alignment\nwith reduced size-dependency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12528.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c19eb3a0ffa3857eae2efa",
      "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
      "fullname": "Jesse Roberts",
      "name": "JesseTNRoberts",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]