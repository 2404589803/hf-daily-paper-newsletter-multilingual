[
  {
    "paper": {
      "id": "2503.00865",
      "authors": [
        {
          "_id": "67c666245e2443d7d5e9b76a",
          "user": {
            "_id": "64802face9ff472e30dc1ceb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64802face9ff472e30dc1ceb/bcwTlgpaUrU7m2RMB5zCc.png",
            "isPro": false,
            "fullname": "Yiran Zhao",
            "user": "Yiran0924",
            "type": "user"
          },
          "name": "Yiran Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:51:21.231Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76b",
          "user": {
            "_id": "61657b0b20606e5e73f611cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61657b0b20606e5e73f611cc/6ZPne2GYlWkxrx35ND1P8.png",
            "isPro": false,
            "fullname": "CHAOQUN LIU",
            "user": "lukecq",
            "type": "user"
          },
          "name": "Chaoqun Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:27:33.956Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76c",
          "name": "Yue Deng",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76d",
          "user": {
            "_id": "671609f7664f44a151f1f0e8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fEQLuH1kdW5Pd9Y_J64hN.png",
            "isPro": false,
            "fullname": "jiahao ying",
            "user": "jhying",
            "type": "user"
          },
          "name": "Jiahao Ying",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:39.926Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76e",
          "user": {
            "_id": "6539c87ba318a98bf0d15dd8",
            "avatarUrl": "/avatars/beb9ba6eeacb61addc5897836bd59f55.svg",
            "isPro": false,
            "fullname": "Mahani Aljunied",
            "user": "maljunied",
            "type": "user"
          },
          "name": "Mahani Aljunied",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:33.285Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76f",
          "name": "Zhaodonghui Li",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b770",
          "user": {
            "_id": "6454685a548f22be598414c4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg",
            "isPro": false,
            "fullname": "Lidong Bing",
            "user": "LidongBing",
            "type": "user"
          },
          "name": "Lidong Bing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:19.611Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b771",
          "user": {
            "_id": "604f67ef0fe8ff3ec13d71ef",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
            "isPro": false,
            "fullname": "Hou Pong (Ken) Chan",
            "user": "kenchan0226",
            "type": "user"
          },
          "name": "Hou Pong Chan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:50.272Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b772",
          "name": "Yu Rong",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b773",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b774",
          "user": {
            "_id": "60dff6ae19a362a8c27862aa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60dff6ae19a362a8c27862aa/LIYzLB3cdPh-B3XIBgBCC.jpeg",
            "isPro": false,
            "fullname": "Wenxuan Zhang",
            "user": "isakzhang",
            "type": "user"
          },
          "name": "Wenxuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:27:36.769Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T11:53:55.000Z",
      "title": "Bella: El modelo de lenguaje multilingüe de Google ofrece servicios a más del 90% de los usuarios del mundo.",
      "summary": "Los modelos de lenguaje de lenguaje (LLMs) han traído innovación a la procesamiento del lenguaje natural (NLP), pero los modelos de LLMs multilingües de código abierto son raros, y los modelos existentes están limitados en la gama de lenguajes. Estos modelos generalmente priorizan a los lenguajes de recursos abundantes y son ampliamente utilizados, mientras que los lenguajes de recursos escasos son descartados. Para resolver esta diferencia, presentamos Babel, un modelo de LLMs multilingüe de código abierto. Este modelo cubre 25 lenguajes y apoya a la población del 90% del mundo, incluyendo muchos lenguajes que se han descartado por otros modelos de código abierto multilingüe. Estos modelos difieren de los otros en su enfoque generalmente de aprendizaje predictivo continuo. La capacidad de los parámetros se expande a través de tecnologías de capas para elevar los límites de la rendición. Presentamos dos versiones de Babel: Babel-9B y Babel-83B. La primera se enfoca en la eficiencia de la inferencia y en la fine-tuning, mientras que la segunda establece nuevas referencias para los modelos de LLMs multilingües de código abierto. En una amplia evaluación de tareas multilingües, Babel muestra un rendimiento relativamente alto en comparación con otros modelos de LLMs de código abierto del mismo tamaño. Además, utilizando el conjunto de datos de super-fine-tuning de Super-Bible Dining Infinity, Babel logra un rendimiento excelente. Babel-9B-Chat es el mejor modelo entre los de 10B de tamaño, mientras que Babel-83B-Chat establece nuevas referencias en tareas multilingües y alcanza un nivel de rendimiento de modelo de negocio.",
      "upvotes": 32,
      "discussionId": "67c666255e2443d7d5e9b7b3",
      "projectPage": "https://babel-llm.github.io/babel-llm/",
      "githubRepo": "https://github.com/babel-llm/babel-llm"
    },
    "publishedAt": "2025-03-05T21:49:03.700Z",
    "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00865.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64802face9ff472e30dc1ceb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64802face9ff472e30dc1ceb/bcwTlgpaUrU7m2RMB5zCc.png",
      "fullname": "Yiran Zhao",
      "name": "Yiran0924",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.00329",
      "authors": [
        {
          "_id": "67c755f898a2e37274c62c96",
          "name": "Benjamin Schneider",
          "hidden": false
        },
        {
          "_id": "67c755f898a2e37274c62c97",
          "name": "Florian Kerschbaum",
          "hidden": false
        },
        {
          "_id": "67c755f898a2e37274c62c98",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:50:07.881Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-01T03:29:02.000Z",
      "title": "ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC: ABC",
      "summary": "Los modelos de embbeding visual son excelentes para tareas como búsqueda visual y clasificación, pero no pueden utilizarse para tareas que requieren ideas o indicaciones del usuario. Estas tareas necesitan un modelo de embbeding multimodal que combine entradas visuales y de lenguaje natural. El enfoque basado en CLIP actual separa la imagen y el texto para embbedarlos y luego fusiona los resultados. Hemos observado que esto lleva a una interacción débil entre modalidades y un mal control de la expresión del usuario. Presentamos ABC, un modelo de embbeding multimodal abierto que integra profundamente las características de imágenes y indicaciones de lenguaje natural. ABC alcanza los mejores resultados en búsqueda de texto a partir de imágenes de MSCOCO y es excelente en tareas de clasificación y VQA en el marco de los pruebas multimodales de MAGIC. Con su fuerte representación visual, ABC puede resolver problemas visuales ambiguos o con ideas no claras utilizando solo lenguaje natural. Para evaluar esta capacidad, hemos diseñado CtrlBench, un marco de prueba que intercambia texto de instrucción y contenido de imagen para buscar resultados precisos. ABC proporciona una alta calidad de expresión y un control flexible de lenguaje natural, avanzando el estado de la arte en el embbeding multimodal. Nuestro modelo y conjunto de datos están disponibles en la página del proyecto.",
      "upvotes": 10,
      "discussionId": "67c7560298a2e37274c6311d",
      "projectPage": "https://tiger-ai-lab.github.io/ABC/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/ABC"
    },
    "publishedAt": "2025-03-05T21:33:37.945Z",
    "title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/b2vg-4UWwvcEboAZgK-Sv.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00329.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03278",
      "authors": [
        {
          "_id": "67c94e5f8c4ef8be73583f4b",
          "name": "Jun Li",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4c",
          "user": {
            "_id": "631b9ff5824f2502e3557c7e",
            "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
            "isPro": false,
            "fullname": "liu",
            "user": "che111",
            "type": "user"
          },
          "name": "Che Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:03:06.882Z",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4d",
          "name": "Wenjia Bai",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4e",
          "name": "Rossella Arcucci",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4f",
          "name": "Cosmin I. Bercea",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f50",
          "name": "Julia A. Schnabel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T09:02:33.000Z",
      "title": "Mejora de la base de conocimiento basada en anomalías en modelos de lenguaje visuolingüístico",
      "summary": "Los Modelos de Lenguaje Visual (VLMs) han demostrado capacidades sorprendentes en tareas basadas en la visión. Sin embargo, la eficacia y, en particular, la detección de anomalías y la localización en imágenes médicas siguen siendo temas que requieren mayor profundidad. Uno de los principales desafíos es la complejidad y la abstracción de los términos médicos, así como la dificultad en establecer una asociación directa entre los equipos de anomalías patológicas y los característicos visuales. En este artículo, se propone una nueva aproximación para mejorar la detección y localización de anomalías en VLMs mediante la utilización de conocimientos médicos desglosados. En lugar de reconocer directamente el modelo para ciertos anomalías, se centra en la descomposición de los conceptos médicos en atributos básicos y patrones visuales comunes. Esta estrategia fomenta una mayor concordancia entre la explicación y los característicos visuales, mejorando tanto la detección de anomalías como su localización en imágenes médicas. Nuestro método se evaluó en la base del modelo Florence-2 de 0.23B, logrando un rendimiento similar al de un VLM médico basado en LLaVA de 7B. En particular, este modelo mejora la detección de anomalías aunque se entrena solo con el 1.5% de los datos. Los resultados de los experimentos muestran que nuestra aproximación demostra una eficacia y una fuerte capacidad de generalización tanto para anomalías conocidas como para anomalías nunca vistas antes.",
      "upvotes": 9,
      "discussionId": "67c94e608c4ef8be73583f7b",
      "projectPage": "https://lijunrio.github.io/AG-KD/"
    },
    "publishedAt": "2025-03-06T02:29:15.964Z",
    "title": "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631b9ff5824f2502e3557c7e",
      "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
      "fullname": "liu",
      "name": "che111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03751",
      "authors": [
        {
          "_id": "67c912b1b5903dd437cc2370",
          "user": {
            "_id": "658529d61c461dfe88afe8e8",
            "avatarUrl": "/avatars/a22c1b07d28c2662833c462c6537d835.svg",
            "isPro": false,
            "fullname": "Xuanchi Ren",
            "user": "xrenaa",
            "type": "user"
          },
          "name": "Xuanchi Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:55:04.321Z",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2371",
          "name": "Tianchang Shen",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2372",
          "name": "Jiahui Huang",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2373",
          "name": "Huan Ling",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2374",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2375",
          "name": "Merlin Nimier-David",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2376",
          "name": "Thomas Müller",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2377",
          "name": "Alexander Keller",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2378",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2379",
          "name": "Jun Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T18:59:50.000Z",
      "title": "GEN3C: Generación de mitologías globales mediante información 3D y control preciso de cámaras",
      "summary": "GEN3C es un modelo de imágenes generativas que tiene la característica de control estructural de cámaras y la consistencia 3D en el tiempo. Los modelos de imágenes existentes generaban imágenes realistas, pero con poca información 3D y muchas incertidumbres sobre la existencia de objetos. El control de cámaras requiere que los parámetros de la cámara sean entrada para la red neuronal, lo que hace que la inferencia de cómo cambia la imagen con respecto a la cámara sea compleja y difícil de controlar. GEN3C utiliza un buffer de captura 3D como guía: se predice la profundidad pixel a pixel de una imagen de seed o de un frame generado anteriormente, y se utiliza junto con las nuevas acciones de cámara proporcionadas por el usuario y la renderización 2D del buffer 3D para generar el siguiente frame. Un punto clave es que GEN3C no necesita recordar lo generado anteriormente ni inferir la estructura de la imagen a partir de la posición de la cámara, lo que le permite concentrar su generación en áreas que nunca han sido vistas antes. El modelo explota su generación en áreas nunca vistas antes y puede continuar simulando la siguiente frame. Nuestros resultados muestran un control de cámara más preciso que las tecnologías existentes y también presentan resultados líder en la síntesis de nuevas vistas en vistas raras. En particular, muestra excelente rendimiento incluso en configuraciones difíciles como escenarios de rotación y imágenes con poca variación. Los resultados son más claramente visibles en la imagen. Consulte nuestra página web: https://research.nvidia.com/labs/toronto-ai/GEN3C/",
      "upvotes": 9,
      "discussionId": "67c912b9b5903dd437cc2505"
    },
    "publishedAt": "2025-03-05T22:13:22.552Z",
    "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03751.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6288
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02951",
      "authors": [
        {
          "_id": "67c907ea7568a12737ad4535",
          "user": {
            "_id": "653df1323479e9ebbe3eb6cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
            "isPro": true,
            "fullname": "Zhangchen Xu",
            "user": "flydust",
            "type": "user"
          },
          "name": "Zhangchen Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:26:50.636Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4536",
          "user": {
            "_id": "637c88b6d55081513c5690d8",
            "avatarUrl": "/avatars/6766e23ebf46b46d6c8b48351c571907.svg",
            "isPro": false,
            "fullname": "Yang Liu",
            "user": "nlpyang",
            "type": "user"
          },
          "name": "Yang Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-06T02:26:54.940Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4537",
          "user": {
            "_id": "605e8dfd5abeb13e714c4c18",
            "avatarUrl": "/avatars/bc27a0ed17b2bd4311e89d3028fa327b.svg",
            "isPro": false,
            "fullname": "yueqin yin",
            "user": "yyqoni",
            "type": "user"
          },
          "name": "Yueqin Yin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:26:48.614Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4538",
          "user": {
            "_id": "653b2524b77b5e255f2d29d2",
            "avatarUrl": "/avatars/f69aea8de84c435295e7638bad5bd82e.svg",
            "isPro": false,
            "fullname": "Mingyuan Zhou",
            "user": "mingyuanzhou",
            "type": "user"
          },
          "name": "Mingyuan Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:03:56.474Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4539",
          "name": "Radha Poovendran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T19:17:36.000Z",
      "title": "KodCode: Diversidad, dificultad, conjunto de datos de síntesis verificable.",
      "summary": "KodCode es un conjunto de datos de código sintético diseñado para resolver los problemas de largo plazo en la obtención de datos de entrenamiento de alta calidad para el aprendizaje del modelo de lenguaje de código de Largo Paso de la Ruta. Los recursos actuales de foco en codificación no pueden garantizar tanto una amplia cobertura (desde tareas de código sencillos hasta problemas algorítmicos de alto nivel de dificultad) como una precisión verificable (como pruebas unitarias). Por otro lado, KodCode configura problemas-soluciones-pruebas con un proceso de certificación automático sistemático. Nuestro pipeline genera código para una amplia variedad de problemas y distribuye experimentos adicionales para resolver problemas más difíciles, creando soluciones y casos de prueba. Finalmente, la síntesis de datos posterior a la entrenamiento cambia las consultas y genera respuestas a través de un proceso de muestreo de muestras rechazadas de la base de pruebas de DeepSeek R1. Este pipeline genera conjuntos de datos de código amplio, robusto y diverso. KodCode facilita entrenamientos normativos y tiene potencial en las pruebas de parejas de unidades y la ajuste de RL. Los experimentos de entrenamiento en los marcos de referencia de codificación (HumanEval(+), MBPP(+), BigCodeBench, LiveCodeBench) muestran que los modelos ajustados con KodCode alcanzan el mejor rendimiento y muestran un desempeño superior a Qwen2.5-Coder-32B-Instruct y DeepSeek-R1-Distill-Llama-70B.",
      "upvotes": 7,
      "discussionId": "67c907ee7568a12737ad4633",
      "projectPage": "https://kodcode-ai.github.io/",
      "githubRepo": "https://github.com/KodCode-AI/kodcode"
    },
    "publishedAt": "2025-03-05T21:31:01.626Z",
    "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "flydust",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01836",
      "authors": [
        {
          "_id": "67c94c32dd505e6a4db201a2",
          "user": {
            "_id": "65d9a453f20e4fc19480afba",
            "avatarUrl": "/avatars/27bfa034a13a5e7cb5fc3b647515a201.svg",
            "isPro": false,
            "fullname": "yisen li",
            "user": "yisenL",
            "type": "user"
          },
          "name": "Yisen Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:04:09.687Z",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a3",
          "name": "Lingfeng Yang",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a4",
          "name": "Wenxuan Shen",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a5",
          "name": "Pan Zhou",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a6",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a7",
          "name": "Weiwei Lin",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a8",
          "user": {
            "_id": "643be8879f5d314db2d9ed23",
            "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
            "isPro": false,
            "fullname": "Chen Dongping",
            "user": "shuaishuaicdp",
            "type": "user"
          },
          "name": "Dongping Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:09:00.496Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T18:56:44.000Z",
      "title": "CrowdSelect: Selección de datos de comandos compuestos para la conocimiento de múltiples LLMs",
      "summary": "Un método que utiliza subconjuntos seleccionados por su capacidad para seguir reglas de orientación se ha convertido en el estado de la arte en la entrenamiento de modelos. La estrategia actual de selección de datos de orientación es principalmente basada en señales unidimensionales (por ejemplo, puntuaciones de recompensa, dificultad del modelo), lo cual no captura la complejidad de seguir reglas en diversos campos. Por lo tanto, hemos abordado este desafío proponiendo tres métricas fundamentales basadas en el conocimiento de múltiples modelos de lenguaje grande (Multi-LLM) y su uso en un enfoque basado en clustering llamado CrowdSelect. Nuestros resultados de validación muestran un aumento en el rendimiento de 4.81% en Arena-Hard y 11.1% en MT-bench utilizando Llama-3.2-3b-instruct. Nuestro trabajo proporciona áreas de investigación útiles para futuros estudios. El código está disponible en https://github.com/listentm/crowdselect.",
      "upvotes": 6,
      "discussionId": "67c94c33dd505e6a4db201f6",
      "githubRepo": "https://github.com/listentm/crowdselect"
    },
    "publishedAt": "2025-03-06T02:20:38.735Z",
    "title": "CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01836.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.18860",
      "authors": [
        {
          "_id": "67c95acf88c3b4201c10b9e9",
          "name": "Md Mehrab Tanjim",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ea",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9eb",
          "name": "Mike Rimer",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ec",
          "name": "Xiang Chen",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ed",
          "name": "Sungchul Kim",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ee",
          "name": "Vaishnavi Muppala",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ef",
          "name": "Tong Yu",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f0",
          "name": "Zhengmian Hu",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f1",
          "name": "Ritwik Sinha",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f2",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f3",
          "name": "Iftikhar Ahamath Burhanuddin",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f4",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:24:24.968Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T06:05:29.000Z",
      "title": "Respuesta: Métodos de retriéndencia desarrollados según las diferencias en las tareas de conversación",
      "summary": "El conversante socio a veces necesita un algoritmo de cambio de preguntas para ofrecer respuestas significativas (exactas) a las preguntas o solicitudes del usuario, utilizando parte de las interacciones pasadas. Sin embargo, el modo preciso de cambio depende de las tareas que el conversante socio soporta o de otras restricciones. En este artículo, se investiga sistemáticamente dos enfoques diferentes en dos tareas generativas fundamentalmente distintas: \"cambio\" y \"sintesis\". Estas tareas incluyen la generación de texto a partir de texto y la creación de visuais o tablas de datos en respuesta a preguntas del usuario, a partir de texto como entrada. Nuestros resultados muestran que cierto enfoque de cambio o sintesis es más adecuado para casos ligeros y tareas generativas. En particular, el enfoque de cambio de preguntas es más adecuado para un conversante socio, mientras que el enfoque de sintesis de preguntas es más adecuado para un analista de datos socio. En particular, en los casos de uso de un analista de datos socio, la investigación de dos tipos de conjuntos de datos cortos y largos muestra que la sintesis de preguntas siempre muestra mejores resultados, mientras que el enfoque de cambio de preguntas es el más adecuado para la respuesta de preguntas basadas en texto de conversación.",
      "upvotes": 2,
      "discussionId": "67c95acf88c3b4201c10ba22"
    },
    "publishedAt": "2025-03-06T03:20:40.127Z",
    "title": "Exploring Rewriting Approaches for Different Conversational Tasks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18860.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.03044",
      "authors": [
        {
          "_id": "67c94e6ad325e95d82f23433",
          "user": {
            "_id": "5e7749883d77a72421292d07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
            "isPro": false,
            "fullname": "Gabriele Sarti",
            "user": "gsarti",
            "type": "user"
          },
          "name": "Gabriele Sarti",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:24:30.601Z",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23434",
          "name": "Vilém Zouhar",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23435",
          "name": "Grzegorz Chrupała",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23436",
          "name": "Ana Guerberof-Arenas",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23437",
          "name": "Malvina Nissim",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23438",
          "name": "Arianna Bisazza",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T22:50:17.000Z",
      "title": "QE4PE: Herramienta de evaluación de calidad de nivel de lenguaje para posterior corrección humana",
      "summary": "La evaluación de calidad a nivel de palabra (QE) detecta errores en traducciones automáticas y guia y fomenta la edición posterior humana. La precisión de los sistemas de QE a nivel de palabra ha sido ampliamente evaluada; sin embargo, su posibilidad de uso, su impacto en la velocidad, calidad y decisiones de edición humana aún requieren más investigación. Nuestro equipo de investigación, QE4PE, estudia el impacto de la QE a nivel de palabra en la edición posterior de traducciones automáticas en un entorno práctico, donde participaron 42 editores profesionales de 2 direcciones de traducción. Comparamos diferentes modos de highlighting para identificar errores potenciales en las salidas de los modelos de MT más avanzados. Evaluamos la productividad de la edición posterior y la mejora de calidad mediante análisis a nivel de palabra y frase. Encontramos que la velocidad de campo, lenguaje y editores es un factor crucial para el efecto de los highlights. Además, observamos que hay una pequeña diferencia entre los highlights humanos y los de QE automatizado, lo que demuestra el interés de encontrar un equilibrio entre precisión y eficacia.",
      "upvotes": 2,
      "discussionId": "67c94e6fd325e95d82f23524"
    },
    "publishedAt": "2025-03-06T02:30:17.431Z",
    "title": "QE4PE: Word-level Quality Estimation for Human Post-Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03044.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e7749883d77a72421292d07",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
      "fullname": "Gabriele Sarti",
      "name": "gsarti",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 213
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20317",
      "authors": [
        {
          "_id": "67c95b2f1fcfdc62ba3a620b",
          "name": "Yongjia Lei",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620c",
          "name": "Haoyu Han",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620d",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620e",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:24:22.970Z",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620f",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a6210",
          "user": {
            "_id": "637c6d95a8716d642050b50f",
            "avatarUrl": "/avatars/0955a10113807348f24db968c7bd7c7a.svg",
            "isPro": false,
            "fullname": "Mahantesh Halappanavar",
            "user": "mhalappa",
            "type": "user"
          },
          "name": "Mahantesh M Halappanavar",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-06T08:22:09.090Z",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a6211",
          "name": "Jiliang Tang",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a6212",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T17:42:52.000Z",
      "title": "La fusión de estructuras de grafos y contexto en búsqueda basada en conocimiento",
      "summary": "La Técnica de Recuperación Mixta de Estructura y Texto (MoR) propone combinar búsqueda estructural y textual para abordar el problema de respuesta a consultas en el contexto de conocimiento basado en grafos (TG-KB). Actualmente, los métodos de búsqueda consideran separadamente ambos tipos de conocimiento, lo que limita su efectividad. MoR utiliza un marco de planificación, razonamiento y organización para integrar ambas formas de búsqueda. En la etapa de planificación, MoR genera un grafo de planificación para clarificar la lógica de respuesta a la consulta. En la etapa de razonamiento, combina búsqueda estructural y coincidencia textual para obtener candidatos en el TG-KB. Finalmente, en la etapa de organización, MoR reescala los candidatos basados en el tráfico estructural. Los experimentos muestran que MoR mejora la eficiencia de la búsqueda estructural y textual, reduciendo la desigualdad en la calidad de la respuesta y mejorando la reescalación de los candidatos mediante el tráfico estructural. El código está disponible en GitHub: https://github.com/Yoega/MoR.",
      "upvotes": 1,
      "discussionId": "67c95b311fcfdc62ba3a62a5"
    },
    "publishedAt": "2025-03-06T03:22:14.664Z",
    "title": "Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20317.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01763",
      "authors": [
        {
          "_id": "67c92e9c746bbcdbdfa8ebd4",
          "name": "Zhengliang Shi",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd5",
          "name": "Yuhan Wang",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd6",
          "name": "Lingyong Yan",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd7",
          "name": "Pengjie Ren",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd8",
          "name": "Shuaiqiang Wang",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd9",
          "name": "Dawei Yin",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebda",
          "name": "Zhaochun Ren",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T17:37:16.000Z",
      "title": "Los modelos de evaluación no tienen suficiente conocimiento sobre un determinado instrumento: se utilizan como marcos de referencia para evaluar la calidad de los modelos de lenguaje grandes.",
      "summary": "El objetivo de Tool Learning es utilizar grandes modelos de lenguaje (LLMs) que poseen una variedad de herramientas para que desempeñen un rol de agente que resuelva tareas prácticas. Es importante considerar la limitación de la longitud del contexto de los LLMs que utilizan herramientas, y seleccionar herramientas útiles desde un gran conjunto de herramientas utilizando modelos de búsqueda de información (IR). Sin embargo, el rendimiento de los modelos IR en tareas de búsqueda de herramientas no ha sido exhaustivamente investigado. En los marcos de evaluación de herramientas, se simplifican estas tareas al usar pequeñas colecciones de herramientas relacionadas con cada tarea, pero esto se aleja de las escenarios reales. En este artículo, se propone un nuevo marco de evaluación de búsqueda de herramientas llamado \"ToolRet\", que consiste en 7.6k tareas de búsqueda diferentes y un conjunto de 43k herramientas. En ToolRet se evaluan seis modelos. Sorprendentemente, modelos que muestran excelentes resultados en los marcos tradicionales de IR presentan bajos rendimientos en ToolRet. Este bajo rendimiento de búsqueda afecta la tasa de éxito en las tareas de los LLMs que utilizan herramientas. Además, se proporciona un conjunto de datos de entrenamiento de más de 200k instancias para mejorar significativamente la capacidad de los modelos IR para la búsqueda de herramientas.",
      "upvotes": 1,
      "discussionId": "67c92e9e746bbcdbdfa8ec57"
    },
    "publishedAt": "2025-03-06T00:12:07.867Z",
    "title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01763.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01729",
      "authors": [
        {
          "_id": "67c92e8b5650d7efeba5b48c",
          "name": "Santiago Bou Betran",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48d",
          "name": "Alberta Longhini",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48e",
          "name": "Miguel Vasco",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48f",
          "name": "Yuchong Zhang",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b490",
          "name": "Danica Kragic",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T16:49:15.000Z",
      "title": "FLAME: Benchmark de Aprendizaje Federado para la Manipulación de Robótica",
      "summary": "El desarrollo reciente de la manipulación de robots está impulsado por grandes conjuntos de datos recopilados en diferentes entornos. Este tipo de datos ha sido utilizado para entrenar las políticas de manipulación de robots, siendo esta práctica centrada en modelos motivados hasta ahora, con preocupaciones sobre la escalabilidad, la adaptabilidad y la privacidad de los datos. Por otro lado, el aprendizaje federado (federated learning) permite la distribución y la protección de la privacidad durante el aprendizaje, lo que ha permitido su aplicación en el campo de la manipulación de robots. Presentamos FLAME (Federated Learning in Manipulation Environments), el primer benchmark adecuado para la manipulación de robots. FLAME incluye dos componentes principales: (i) un gran conjunto de datos que incluye más de 160,000 demostraciones de tareas de manipulación de expertos; (ii) un marco de trabajo para entrenar y evaluar políticas de robots en un entorno de aprendizaje federado. En FLAME, se evalúan algoritmos estándares de aprendizaje federado, y se exploran la posibilidad de entrenamiento de políticas distribuidas, así como problemas importantes. Este benchmark forma la base para la construcción de un aprendizaje de robots escalable y adaptable.",
      "upvotes": 1,
      "discussionId": "67c92e8d5650d7efeba5b519"
    },
    "publishedAt": "2025-03-06T00:11:48.501Z",
    "title": "FLAME: A Federated Learning Benchmark for Robotic Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01729.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01449",
      "authors": [
        {
          "_id": "67c92e738d5fe8c860571103",
          "name": "Ting Zhang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571104",
          "name": "Chengran Yang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571105",
          "name": "Yindu Su",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571106",
          "name": "Martin Weyssow",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571107",
          "name": "Hung Nguyen",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571108",
          "name": "Tan Bui",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571109",
          "name": "Hong Jin Kang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110a",
          "name": "Yikun Li",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110b",
          "name": "Eng Lieh Ouh",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110c",
          "name": "Lwin Khin Shar",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110d",
          "name": "David Lo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T11:56:00.000Z",
      "title": "Verifique la versión del archivo. Descarga el archivo de verificación de la versión.",
      "summary": "El desarrollo reciente de los modelos de lenguaje generativos (Generative AI) ha impulsado la amplia introducción de grandes modelos de lenguaje (LLMs) en el campo de la ingeniería de software, resuelviendo problemas que durante mucho tiempo no habían sido abordados. Sin embargo, en la parte importante de la seguridad del software, la detección de vulnerabilidades en software (SVD), la capacidad de los LLMs ha sido poco estudiada. Actualmente, la mayoría de los estudios evalúan LLMs utilizando conjuntos de datos de C/C++. Generalmente, se examinan un o dos de los siguientes aspectos: ingeniería de proyectos, entrenamiento de comandos, clasificación de orden final. Como resultado de estos estudios, se observa una notable falta de conocimiento sobre la eficacia de los LLMs en la detección de vulnerabilidades en diversos lenguajes de programación. Para abordar esta lacuna, se está llevando a cabo un estudio experimental específico para evaluar el rendimiento de los LLMs en la SVD. Se está preparando el proyecto para evaluar 5 modelos de LLMs de tamaño pequeño final y 2 herramientas de seguridad aplicada estática de código de proyectos utilizando diversas metodologías, como ingeniería de proyectos, entrenamiento de comandos y clasificación de orden final. Estos modelos se comparan con 5 modelos de LLMs finales de tamaño pequeño y 2 herramientas de seguridad aplicada estática de código de proyectos. Además, se está investigando dos enfoques para mejorar el rendimiento de la SVD en los LLMs: la visión de los datos y la visión del modelo. En la visión de los datos, se reentrena el modelo utilizando conjuntos de datos balanceados obtenidos a través de submuestreo. En la visión del modelo, se investiga el método de entrenamiento de ensamble que combina las predicciones de varios modelos de LLMs. Los resultados específicos de los experimentos muestran que la SVD sigue siendo un desafío para los LLMs. Este estudio proporciona una comprensión detallada de los roles que los LLMs desempeñan en la SVD y ofrece guías prácticas para mejorar la práctica de seguridad del software utilizando el AI generativo.",
      "upvotes": 1,
      "discussionId": "67c92e748d5fe8c860571142"
    },
    "publishedAt": "2025-03-06T00:11:25.013Z",
    "title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01449.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01378",
      "authors": [
        {
          "_id": "67c92e537ae0115c7a7b9fa3",
          "name": "Artem Lykov",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa4",
          "name": "Valerii Serpiva",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa5",
          "name": "Muhammad Haris Khan",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa6",
          "name": "Oleg Sautenkov",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa7",
          "name": "Artyom Myshlyaev",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa8",
          "name": "Grik Tadevosyan",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa9",
          "name": "Yasheerah Yaqoot",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9faa",
          "name": "Dzmitry Tsetserukou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:21:36.000Z",
      "title": "Cocagoden: Tareas cognitivas en tiempo real en UAVs y el modelo VLA de lógica y marcos de evaluación",
      "summary": "En este artículo, se presenta un nuevo modelo de Visión-Lenguaje-Acción (VLA) llamado \"CognitiveDrone\" adecuado para tareas complejas de aviones sin piloto (UAV). Este modelo se basa en un conjunto de datos de entrenamiento de más de 8,000 ejemplos, y genera comandos de acción 4D secuencialmente según entradas visuales y indicaciones de texto, en tres categorías: reconocimiento humano, comprensión de símbolos y lógica. Para mejorar su rendimiento en escenarios complejos, incluye un módulo de lógica adicional basado en un modelo de Visión-Lenguaje (VLM), y propone \"CognitiveDrone-R1\" para simplificar las instrucciones antes de la control de alta frecuencia. Según las evaluaciones de experimentos realizadas con nuestro benchmark abierto \"CognitiveDroneBench\", el modelo especializado para la carrera (RaceVLA) alcanzó un éxito total de 31.3%, mientras que el modelo básico de CognitiveDrone alcanzó un éxito de 59.6% y CognitiveDrone-R1 un éxito de 77.2%. Estos resultados muestran una mejora significativa de aproximadamente 30% en tareas cognitivas importantes y destacan el efecto de integrar capacidades de lógica avanzada en sistemas de control de aviones sin piloto. Nuestro contribución incluye el desarrollo de uno de los modelos VLA más avanzados y la introducción del primer benchmark especializado para la evaluación de tareas cognitivas en el manejo de aviones sin piloto. El repositorio completo está disponible en cognitivedrone.github.io.",
      "upvotes": 1,
      "discussionId": "67c92e547ae0115c7a7b9fe6"
    },
    "publishedAt": "2025-03-06T00:10:56.364Z",
    "title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.00502",
      "authors": [
        {
          "_id": "67c8427047c2aa135346dced",
          "user": {
            "_id": "66d3290364c1e9b73208af82",
            "avatarUrl": "/avatars/e0ea5f2b366927c7b146f248028a2e59.svg",
            "isPro": false,
            "fullname": "Shiyu Fang",
            "user": "FanGShiYuu",
            "type": "user"
          },
          "name": "Shiyu Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-05T15:46:48.691Z",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcee",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcef",
          "name": "Chengkai Xu",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf0",
          "name": "Chen Lv",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf1",
          "name": "Peng Hang",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf2",
          "name": "Jian Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-01T14:15:52.000Z",
      "title": "Interact, Instruct to Improve: Marco de Agentes Duales Impulsado por un Modelo de Lenguaje Automático para Mejorar las Interacciones de Vehículos Autónomos",
      "summary": "Los autonomous vehicles (AVs) entran en la fase de comercialización, pero su capacidad limitada de interacción y expresión de intenciones sigue siendo un problema en su interacción con vehículos humanos (HVs). El reciente avance de los grandes modelos de lenguaje (LLMs) ha facilitado la comunicación bidireccional entre humanos y máquinas, pero presenta problemas debido a la velocidad lenta de la inferencia y la necesidad de tomar decisiones en tiempo real. Para resolver estos problemas, este artículo presenta un marco de lenguaje actor-reasoner para permitir una interacción bidireccional clara en múltiples escenarios entre AVs y HVs. Primero, se introduce el nombre de \"actor\" para el registro de memoria que promueve la interacción con otros HVs y el registro de memoria de la LLM. Luego, se introducen el módulo de partión de memoria y el módulo de búsqueda de memoria de dos capas para mejorar significativamente la capacidad del actor de procesar diferentes HVs. La prueba de la eficiencia y estabilidad del marco de lenguaje actor-reasoner se ha demostrado a través de la comparación con estudios de inferencia y métodos deterministas. Finalmente, se combina la información de la interfaz externa de humanos y máquinas (eHMI) y las decisiones de acción aplicables del actor, basadas en el razonamiento del registro, para confirmar el efecto del marco de lenguaje actor-reasoner en varios escenarios de interacción en el campo. El código está disponible en https://github.com/FanGShiYuu/Actor-Reasoner.",
      "upvotes": 1,
      "discussionId": "67c8427247c2aa135346dd84",
      "projectPage": "https://fangshiyuu.github.io/Actor-Reasoner/",
      "githubRepo": "https://github.com/FanGShiYuu/Actor-Reasoner"
    },
    "publishedAt": "2025-03-05T21:37:18.981Z",
    "title": "Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d3290364c1e9b73208af82",
      "avatarUrl": "/avatars/e0ea5f2b366927c7b146f248028a2e59.svg",
      "fullname": "Shiyu Fang",
      "name": "FanGShiYuu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01372",
      "authors": [
        {
          "_id": "67c6bd6e8f3e7fd471affd06",
          "name": "Joel Niklaus",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd07",
          "name": "Jakob Merane",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd08",
          "name": "Luka Nenadic",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd09",
          "name": "Sina Ahmadi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0a",
          "name": "Yingqiang Gao",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0b",
          "name": "Cyrill A. H. Chevalley",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0c",
          "name": "Claude Humbel",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0d",
          "name": "Christophe Gösken",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0e",
          "name": "Lorenzo Tanzi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0f",
          "name": "Thomas Lüthi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd10",
          "name": "Stefan Palombo",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd11",
          "name": "Spencer Poff",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd12",
          "name": "Boling Yang",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd13",
          "name": "Nan Wu",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd14",
          "name": "Matthew Guillod",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd15",
          "name": "Robin Mamié",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd16",
          "name": "Daniel Brunner",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd17",
          "name": "Julio Pereyra",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd18",
          "name": "Niko Grupen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:10:30.000Z",
      "title": "SwiLTra-Bench: Benchmark de Traducción de Reglamentos Shumplados",
      "summary": "La traducción de leyes en Suiza es especialmente importante debido a las cuatro lenguas oficiales del país y a las demandas de documentos multilingües. Sin embargo, este proceso depende de especialistas en leyes y traductores competentes, lo que genera un estrangulamiento y afecta a la aproximación justa y efectiva. Para resolver estas desafíos, presentamos SwiLTra-Bench, un marco de referencia detallado que incluye más de 180K pares de traducciones correspondientes para todas las lenguas suecas (incluyendo el inglés). Este marco de referencia está diseñado para evaluar sistemas de traducción basados en modelos de lenguaje grande (LLM). De acuerdo con un análisis lógico, los modelos avanzados muestran un excelente rendimiento de traducción en todos los tipos de documentos, aunque el rendimiento de los sistemas de traducción profesionales es particularmente alto en el ámbito de las leyes pero disminuye en los head notes. Mediante una evaluación rigurosa y la evaluación de expertos humanos, demostramos que la fine-tuning de sistemas abiertos significativamente mejora la calidad de la traducción, lo que supera a modelos más recientes sin fine-tuning (por ejemplo, Claude-3.5-Sonnet). Además, presentamos SwiLTra-Judge, un sistema de evaluación de LLM especializado que coincide más con la evaluación de expertos humanos.",
      "upvotes": 0,
      "discussionId": "67c6bd708f3e7fd471affd5d"
    },
    "publishedAt": "2025-03-06T00:10:21.173Z",
    "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01372.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  }
]