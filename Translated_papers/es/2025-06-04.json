[
  {
    "paper": {
      "id": "2505.24120",
      "authors": [
        {
          "_id": "683fc08da33aeee1124887c4",
          "name": "Ai Jian",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c5",
          "user": {
            "_id": "660aab2c878289c5b34f9e97",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660aab2c878289c5b34f9e97/yxx1-lR8x5o6KaEpZDXQq.jpeg",
            "isPro": false,
            "fullname": "weijie qiu",
            "user": "qiuwj",
            "type": "user"
          },
          "name": "Weijie Qiu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:56:39.772Z",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c6",
          "user": {
            "_id": "62be9b5aae56e75e4d689e7c",
            "avatarUrl": "/avatars/6772bc09d6eeb4e86b1210481be91720.svg",
            "isPro": false,
            "fullname": "wangxiaokun",
            "user": "shawn0wang",
            "type": "user"
          },
          "name": "Xiaokun Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:01.766Z",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c7",
          "name": "Peiyu Wang",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c8",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c9",
          "name": "Jiangbo Pei",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887ca",
          "name": "Yichen Wei",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887cb",
          "name": "Yi Peng",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887cc",
          "name": "Xuchen Song",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/620f5a1c3f76c50e6458a9b6/l5zN70u8jAQBCjl41TWoi.png"
      ],
      "publishedAt": "2025-05-30T01:34:25.000Z",
      "submittedOnDailyAt": "2025-06-04T05:57:36.826Z",
      "title": "CSVQA: Evaluación del capacidad de inferencia científica teórica de modelos VLM en el benchmark de modelos de China",
      "submittedOnDailyBy": {
        "_id": "620f5a1c3f76c50e6458a9b6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
        "isPro": false,
        "fullname": "Peiyu Wang",
        "user": "OrlandoHugBot",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje visual (VLMs) muestran un impresionante avance en diversas comprensiones, pero su capacidad para la lógica científica no se evalua suficientemente. Actualmente, varios benchmarks evalúan la comprensión general de imágenes o texto basados en lógica, pero no abordan problemas que requieren un contexto científico real, lo que resulta insuficiente para la análisis de evidencias visuales y el conocimiento específico de la disciplina. Para remediar esta deficiencia, se presenta CSVQA (Benchmark de Respuestas de Categoría de la Versión de la Virtual QA), diseñado para evaluar la lógica científica a través de preguntas visuales basadas en dominios específicos. CSVQA es un diagnóstico de varios benchmarks que evalúan la lógica científica mediante respuestas a preguntas visuales. Este benchmark incluye 1,378 pares de preguntas y respuestas bien construidas, se extiende a diversas áreas STEM y requiere el conocimiento del dominio, la integración de evidencias visuales y una lógica de alto nivel. En comparación con otros benchmarks, CSVQA enfatiza el contenido científico real y la complejidad de la lógica. Además, propone un estricto protocolo de evaluación para sistemáticamente justificar si las predicciones del modelo se basan en un grado lógico adecuado. Los resultados detallados de los 15 modelos evaluados en una prueba de línea de computadora muestran claras diferencias de rendimiento. En particular, el modelo con la propiedad más alta no alcanzó la precisión de 49.6%. Estos resultados experimentales subrayan la necesidad de desarrollar la lógica científica en los VLMs. CSVQA está disponible en https://huggingface.co/datasets/Skywork/CSVQA.",
      "upvotes": 41,
      "discussionId": "683fc091a33aeee1124888a8",
      "ai_summary": "A new benchmark, CSVQA, evaluates scientific reasoning in vision-language models through domain-specific visual question answering, highlighting the need for improvement in these models.",
      "ai_keywords": [
        "Vision-Language Models",
        "multimodal benchmark",
        "scientific reasoning",
        "domain-grounded",
        "visual question answering",
        "domain-specific knowledge",
        "higher-order reasoning",
        "evaluation protocol",
        "intermediate reasoning steps",
        "curated explanations"
      ]
    },
    "publishedAt": "2025-05-29T21:34:25.000Z",
    "title": "CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning\n  Capabilities of VLMs",
    "summary": "Vision-Language Models (VLMs) have demonstrated remarkable progress in\nmultimodal understanding, yet their capabilities for scientific reasoning\nremains inadequately assessed. Current multimodal benchmarks predominantly\nevaluate generic image comprehension or text-driven reasoning, lacking\nauthentic scientific contexts that require domain-specific knowledge\nintegration with visual evidence analysis. To fill this gap, we present CSVQA,\na diagnostic multimodal benchmark specifically designed for evaluating\nscientific reasoning through domain-grounded visual question answering.Our\nbenchmark features 1,378 carefully constructed question-answer pairs spanning\ndiverse STEM disciplines, each demanding domain knowledge, integration of\nvisual evidence, and higher-order reasoning. Compared to prior multimodal\nbenchmarks, CSVQA places greater emphasis on real-world scientific content and\ncomplex reasoning.We additionally propose a rigorous evaluation protocol to\nsystematically assess whether model predictions are substantiated by valid\nintermediate reasoning steps based on curated explanations. Our comprehensive\nevaluation of 15 VLMs on this benchmark reveals notable performance\ndisparities, as even the top-ranked proprietary model attains only 49.6\\%\naccuracy.This empirical evidence underscores the pressing need for advancing\nscientific reasoning capabilities in VLMs. Our CSVQA is released at\nhttps://huggingface.co/datasets/Skywork/CSVQA.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620f5a1c3f76c50e6458a9b6/l5zN70u8jAQBCjl41TWoi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24120.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620f5a1c3f76c50e6458a9b6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
      "fullname": "Peiyu Wang",
      "name": "OrlandoHugBot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02387",
      "authors": [
        {
          "_id": "683fa95ea0770843560c7ae3",
          "user": {
            "_id": "653a5b0f7c01c693a16dd184",
            "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
            "isPro": false,
            "fullname": "Zelai Xu",
            "user": "zelaix",
            "type": "user"
          },
          "name": "Zelai Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-04T02:03:11.372Z",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae4",
          "name": "Zhexuan Xu",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae5",
          "name": "Xiangmin Yi",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae6",
          "name": "Huining Yuan",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae7",
          "name": "Xinlei Chen",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae8",
          "name": "Yi Wu",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae9",
          "name": "Chao Yu",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7aea",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T02:57:38.000Z",
      "submittedOnDailyAt": "2025-06-04T00:58:29.506Z",
      "title": "VS-Bench: Evaluación de lógica estratégica y decisiones en entornos multi-agentes con VLMs",
      "submittedOnDailyBy": {
        "_id": "653a5b0f7c01c693a16dd184",
        "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
        "isPro": false,
        "fullname": "Zelai Xu",
        "user": "zelaix",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los modelos de lenguaje visuolingüístico (VLMs) ha expandido sus capacidades para tareas interactivas de agentes, aunque los actuales benchmarks están limitados a entornos que incluyen solo un agente o solo texto. En contraste, escenarios reales y globales implican interacciones entre diferentes agentes en contextos ricos de visión y lenguaje, resolviendo problemas a través de observaciones multimodelos y interacciones estratégicas. Para abordar este desafío, se presenta el Vision Strategy Bench (VS-Bench). VS-Bench es un benchmark multiagente que evalúa VLMs para evaluar lógica estratégica y decisiones en entornos multiagente. Está configurado con 8 entornos visuales específicos que incluyen interacciones colaborativas, competitivas o mixtas, y evalúa la capacidad de los agentes en predecir el futuro de otros agentes y optimizar objetivos a largo plazo. Se revisan dos dimensiones de evaluación complementarias: la evaluación de lógica estratégica en línea y la evaluación de retornos episódicos normalizados en línea. A través de experimentos extendidos con 14 VLMs líderes, se ha demostrado claramente una gran diferencia entre los modelos actuales y los mejores rendimientos. El mejor modelo alcanzó una precisión de predicción del 47.8% y un retorno normalizado del 24.3%. Además, se realizó un análisis detallado de observaciones multimodeles, escalado de prueba, comportamientos sociales y casos de fracaso de VLM agentes, revelando las limitaciones actuales de los modelos y estableciendo VS-Bench como la base para futuras investigaciones en agentes multimodelo estratégicos. Los códigos y datos están disponibles en https://vs-bench.github.io.",
      "upvotes": 34,
      "discussionId": "683fa95fa0770843560c7b3d",
      "projectPage": "https://vs-bench.github.io",
      "githubRepo": "https://github.com/zelaix/VS-Bench",
      "ai_summary": "VS-Bench is a multimodal benchmark designed to evaluate Vision Language Models' strategic reasoning and decision-making in complex multi-agent environments.",
      "ai_keywords": [
        "Vision Language Models",
        "VS-Bench",
        "multimodal benchmark",
        "strategic reasoning",
        "decision-making",
        "multi-agent environments",
        "vision-grounded environments",
        "cooperative",
        "competitive",
        "mixed-motive interactions",
        "next-action prediction",
        "normalized episode return",
        "multimodal observations",
        "test-time scaling",
        "social behaviors",
        "failure cases"
      ]
    },
    "publishedAt": "2025-06-02T22:57:38.000Z",
    "title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments",
    "summary": "Recent advancements in Vision Language Models (VLMs) have expanded their\ncapabilities to interactive agent tasks, yet existing benchmarks remain limited\nto single-agent or text-only environments. In contrast, real-world scenarios\noften involve multiple agents interacting within rich visual and linguistic\ncontexts, posing challenges with both multimodal observations and strategic\ninteractions. To bridge this gap, we introduce Visual Strategic Bench\n(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning\nand decision-making in multi-agent environments. VS-Bench comprises eight\nvision-grounded environments spanning cooperative, competitive, and\nmixed-motive interactions, designed to assess agents' ability to predict\nothers' future moves and optimize for long-term objectives. We consider two\ncomplementary evaluation dimensions, including offline evaluation of strategic\nreasoning by next-action prediction accuracy and online evaluation of\ndecision-making by normalized episode return. Extensive experiments of fourteen\nleading VLMs reveal a significant gap between current models and optimal\nperformance, with the best models attaining 47.8% prediction accuracy and 24.3%\nnormalized return. We further conduct in-depth analyses on multimodal\nobservations, test-time scaling, social behaviors, and failure cases of VLM\nagents. By standardizing the evaluation and highlighting the limitations of\nexisting models, we envision VS-Bench as a foundation for future research on\nstrategic multimodal agents. Code and data are available at\nhttps://vs-bench.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653a5b0f7c01c693a16dd184",
      "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
      "fullname": "Zelai Xu",
      "name": "zelaix",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03147",
      "authors": [
        {
          "_id": "683fae55c6b71c5994ccd4fe",
          "user": {
            "_id": "6367a8175bb06007ea099b8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6367a8175bb06007ea099b8f/IjG7HyWyWRlVt_XwRbxRW.jpeg",
            "isPro": false,
            "fullname": "linbin",
            "user": "LanguageBind",
            "type": "user"
          },
          "name": "Bin Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:49.923Z",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd4ff",
          "name": "Zongjian Li",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd500",
          "name": "Xinhua Cheng",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd501",
          "name": "Yuwei Niu",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd502",
          "name": "Yang Ye",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd503",
          "name": "Xianyi He",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd504",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:52.748Z",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd505",
          "name": "Wangbo Yu",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd506",
          "name": "Shaodong Wang",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd507",
          "name": "Yunyang Ge",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd508",
          "name": "Yatian Pang",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd509",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:59:33.000Z",
      "submittedOnDailyAt": "2025-06-04T00:55:35.016Z",
      "title": "UniWorld: Integración de Visión y Comprensión Semántica de un Encoder de Alta Resolución\n\nUniWorld: Integración de Visión y Comprensión Semántica de un Encoder de Alta Resolución",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "¡Claro! A continuación, aquí está el texto en español:\n\n「Por supuesto, actualmente el modelo integrado ofrece un excelente rendimiento en la comprensión de lenguaje visual y la generación de imágenes a partir de texto, sin embargo, presenta limitaciones en la reconocimiento de imágenes y en tareas de trabajo que los usuarios requieren urgentemente. Recientemente, OpenAI ha presentado GPT-4o-Image, un modelo que utiliza características significativas de un modelo de lenguaje visual para lograr una amplia gama de funciones en reconocimiento de imágenes y tareas de trabajo, manteniendo el interés de la comunidad. A partir de nuestros experimentos detallados, se ha deducido que GPT-4o-Image utiliza características extraídas de un encoder de significado en lugar de un VAE (Autoencoder Variational). Este hecho interesante nos ha motivado a proponer un marco de generación integrado basado en un modelo de lenguaje visual fuerte y características significativas extraídas de un encoder de significado. Como resultado, hemos construido un modelo muy potente utilizando solo el 1% de los datos de BAGEL, y obtuvimos un rendimiento superior a BAGEL en un marco de prueba de edición de imágenes. UniWorld mantiene una excelente capacidad de comprensión y generación de imágenes, y también muestra un rendimiento fuerte en diversas tareas de reconocimiento de imágenes. Hemos abierto completamente nuestro modelo, los scripts de entrenamiento y evaluación, y los conjuntos de datos.」",
      "upvotes": 33,
      "discussionId": "683fae56c6b71c5994ccd548",
      "githubRepo": "https://github.com/PKU-YuanGroup/UniWorld-V1",
      "ai_summary": "A unified generative framework called UniWorld uses semantic features from visual-language models for image perception and manipulation, outperforming BAGEL with reduced data.",
      "ai_keywords": [
        "GPT-4o-Image",
        "semantic encoders",
        "VAE",
        "UniWorld",
        "visual-language models",
        "contrastive semantic encoders",
        "image editing benchmarks",
        "image perception tasks"
      ]
    },
    "publishedAt": "2025-06-03T13:59:33.000Z",
    "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation",
    "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03147.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00123",
      "authors": [
        {
          "_id": "683e709a3a4c2c3b2750fc32",
          "name": "Gen Luo",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc33",
          "user": {
            "_id": "6565d7149afd51867e55520b",
            "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
            "isPro": false,
            "fullname": "Ganlin Yang",
            "user": "ganlinyang",
            "type": "user"
          },
          "name": "Ganlin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:03.857Z",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc34",
          "user": {
            "_id": "660691330be1fbe3b9e4c33d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660691330be1fbe3b9e4c33d/TxrDFH_cRu3AlpMC3xmhv.jpeg",
            "isPro": false,
            "fullname": "ZiYang Gong",
            "user": "Cusyoung",
            "type": "user"
          },
          "name": "Ziyang Gong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:00.364Z",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc35",
          "name": "Guanzhou Chen",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc36",
          "user": {
            "_id": "66ab30dfd456f0408b93f27b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ab30dfd456f0408b93f27b/nps4Kni_eOExO5Z92RiiF.jpeg",
            "isPro": false,
            "fullname": "Haonan Duan",
            "user": "robot-haonan",
            "type": "user"
          },
          "name": "Haonan Duan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:03:03.236Z",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc37",
          "name": "Erfei Cui",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc38",
          "name": "Ronglei Tong",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc39",
          "name": "Zhi Hou",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3a",
          "name": "Tianyi Zhang",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3b",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3c",
          "name": "Shenglong Ye",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3d",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3e",
          "name": "Jingbo Wang",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3f",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc40",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc41",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc42",
          "name": "Rongrong Ji",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc43",
          "name": "Xizhou Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T18:00:34.000Z",
      "submittedOnDailyAt": "2025-06-04T03:43:23.019Z",
      "title": "El modelo de lenguaje cerebral de la dimensión espacial que ejecuta la visión, el oído, el pensamiento y el control en el espacio.",
      "submittedOnDailyBy": {
        "_id": "6565d7149afd51867e55520b",
        "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
        "isPro": false,
        "fullname": "Ganlin Yang",
        "user": "ganlinyang",
        "type": "user"
      },
      "summary": "El significativo avance de los modelos de lenguaje multimodal (MLLMs) ha recibido más atención al ser capaces de expandirse a entidades físicas como robots de pie. Esto implica que los MLLMs deben adquirir una comprensión multimodal que incluya inferencia espacial visual y interacción física. Sin embargo, las diferencias fundamentales de estas capacidades hacen que los métodos existentes enfrenten dificultades para integrarlas. En este artículo, se propone \"VeBrain\" (Visual Body-Brain), un marco integrado que combina percepción, inferencia y control de la realidad. VeBrain recónfigura el control de los robots en tareas basadas en texto en espacio visual 2D, unificando las metas y espacios de trabajo de diferentes tareas. Además, se propone un nuevo adaptador para transformar los señales de control de texto de MLLM en estrategias de movimiento de los robots. Desde el punto de vista de los datos, introducimos VeBrain-600k, un conjunto de datos de alta calidad que incluye varias capacidades de VeBrain. VeBrain-600k se ha colectado, seleccionado y etiquetado durante cientos de horas, y utilizamos la cadena de pensamiento multimodal (CoT) para mezclar diferentes capacidades en una sola conversación. A través de 13 pruebas de referencia multimodal y 5 de inteligencia espacial, se demostró que VeBrain presenta un rendimiento excelente en comparación con otros MLLMs (por ejemplo, Qwen2.5-VL). Al utilizar robots de pie y brazos mecánicos, VeBrain demostró una fuerte adaptabilidad, flexibilidad y combinación en comparación con los métodos existentes. Por ejemplo, en comparación con Qwen2.5-VL, VeBrain logró significativas mejoras en MMVet y un aumento promedio del 50% en tareas de robots de pie.",
      "upvotes": 23,
      "discussionId": "683e70a13a4c2c3b2750fd76",
      "projectPage": "https://internvl.github.io/blog/2025-05-26-VeBrain/",
      "githubRepo": "https://github.com/OpenGVLab/VeBrain",
      "ai_summary": "VeBrain is a unified framework that integrates multimodal understanding, visual-spatial reasoning, and physical interaction for legged robots, demonstrating superior performance compared to existing methods across various benchmarks.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "VeBrain",
        "Visual Embodied Brain",
        "text-based MLLM tasks",
        "robotic adapter",
        "VeBrain-600k",
        "multimodal chain-of-thought",
        "MMVet",
        "compositional capabilities"
      ]
    },
    "publishedAt": "2025-05-30T14:00:34.000Z",
    "title": "Visual Embodied Brain: Let Multimodal Large Language Models See, Think,\n  and Control in Spaces",
    "summary": "The remarkable progress of Multimodal Large Language Models (MLLMs) has\nattracted increasing attention to extend them to physical entities like legged\nrobot. This typically requires MLLMs to not only grasp multimodal understanding\nabilities, but also integrate visual-spatial reasoning and physical interaction\ncapabilities. Nevertheless,existing methods struggle to unify these\ncapabilities due to their fundamental differences.In this paper, we present the\nVisual Embodied Brain (VeBrain), a unified framework for perception, reasoning,\nand control in real world. VeBrain reformulates robotic control into common\ntext-based MLLM tasks in the 2D visual space, thus unifying the objectives and\nmapping spaces of different tasks. Then, a novel robotic adapter is proposed to\nconvert textual control signals from MLLMs to motion policies of real robots.\nFrom the data perspective, we further introduce VeBrain-600k, a high-quality\ninstruction dataset encompassing various capabilities of VeBrain. In\nVeBrain-600k, we take hundreds of hours to collect, curate and annotate the\ndata, and adopt multimodal chain-of-thought(CoT) to mix the different\ncapabilities into a single conversation. Extensive experiments on 13 multimodal\nbenchmarks and 5 spatial intelligence benchmarks demonstrate the superior\nperformance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to\nlegged robots and robotic arms, VeBrain shows strong adaptability, flexibility,\nand compositional capabilities compared to existing methods. For example,\ncompared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by\n+5.6%, but also excels in legged robot tasks with +50% average gains.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00123.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6565d7149afd51867e55520b",
      "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
      "fullname": "Ganlin Yang",
      "name": "ganlinyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03135",
      "authors": [
        {
          "_id": "683fe55868402c738a8e5ee4",
          "name": "Mengdi Jia",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee5",
          "user": {
            "_id": "63c3e8abc7d7f4c63a515a02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
            "isPro": false,
            "fullname": "Zekun Qi",
            "user": "qizekun",
            "type": "user"
          },
          "name": "Zekun Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:22.042Z",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee6",
          "name": "Shaochen Zhang",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee7",
          "name": "Wenyao Zhang",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee8",
          "name": "Xinqiang Yu",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee9",
          "name": "Jiawei He",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5eea",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5eeb",
          "name": "Li Yi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63c3e8abc7d7f4c63a515a02/ZZsfC9We_mrTAc2s-h5ng.mp4"
      ],
      "publishedAt": "2025-06-03T17:58:29.000Z",
      "submittedOnDailyAt": "2025-06-04T05:47:51.969Z",
      "title": "OmniSpatial: Lógica Espacial Extensa para un Marco de Prueba Completo de Lógica Espacial",
      "submittedOnDailyBy": {
        "_id": "63c3e8abc7d7f4c63a515a02",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
        "isPro": false,
        "fullname": "Zekun Qi",
        "user": "qizekun",
        "type": "user"
      },
      "summary": "La teoría espacial es una de las áreas más importantes en psicología cognitiva y actualmente es el equilibrio principal de los modelos de lenguaje visual (VLMs). Los estudios anteriores se han utilizado principalmente para evaluar o mejorar la comprensión de las relaciones espaciales básicas. Sin embargo, estos trabajos solo abordan los niveles más básicos de la inferencia espacial. En este artículo, se presenta OmniSpatial, una evaluación detallada de la inferencia espacial basada en psicología cognitiva. OmniSpatial incluye cuatro categorías principales: inferencia dinámica, lógica espacial compleja, interacción espacial y revisión de puntos, y tiene 50 categorías detalladas. A través de la crónesis de datos de internet y notas rigurosamente escritas, se construyeron más de 1,500 pares de preguntas y respuestas. A través de una amplia gama de experimentos, se demostró que los VLMs abiertos y cerrados, así como los modelos actuales de lógica y comprensión espacial, presentan deficiencias en la comprensión espacial detallada. Además, se analizaron casos de fracaso y se propusieron direcciones potenciales para futuras investigaciones.",
      "upvotes": 22,
      "discussionId": "683fe55c68402c738a8e5ff4",
      "ai_summary": "A comprehensive benchmark called OmniSpatial evaluates vision-language models' understanding of advanced spatial reasoning tasks, revealing significant limitations across various models.",
      "ai_keywords": [
        "vision-language models",
        "spatial reasoning",
        "cognitive psychology",
        "dynamic reasoning",
        "complex spatial logic",
        "spatial interaction",
        "perspective-taking",
        "question-answer pairs"
      ]
    },
    "publishedAt": "2025-06-03T13:58:29.000Z",
    "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for\n  Vision Language Models",
    "summary": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63c3e8abc7d7f4c63a515a02/ZZsfC9We_mrTAc2s-h5ng.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03135.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c3e8abc7d7f4c63a515a02",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
      "fullname": "Zekun Qi",
      "name": "qizekun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01674",
      "authors": [
        {
          "_id": "683faa6515abeae85e13336b",
          "name": "Yipeng Du",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336c",
          "name": "Tiehan Fan",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336d",
          "name": "Kepan Nan",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336e",
          "name": "Rui Xie",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336f",
          "name": "Penghao Zhou",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133370",
          "name": "Xiang Li",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133371",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133372",
          "name": "Zhenheng Yang",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133373",
          "user": {
            "_id": "65734004769f3ee9bde1af10",
            "avatarUrl": "/avatars/d6310ed861972fd691687d8f47413f33.svg",
            "isPro": false,
            "fullname": "Ying Tai",
            "user": "yingtai",
            "type": "user"
          },
          "name": "Ying Tai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:59.253Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T13:44:56.000Z",
      "submittedOnDailyAt": "2025-06-04T00:38:03.935Z",
      "title": "MotionSight: Mejorar la capacidad de entender pequeñas movimientos en diferentes modos",
      "submittedOnDailyBy": {
        "_id": "65927f3b754092f6b1e187a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65927f3b754092f6b1e187a7/gUrNvIQHmsl1vLwSUxpmL.jpeg",
        "isPro": false,
        "fullname": "tiehan fan",
        "user": "AnonMegumi",
        "type": "user"
      },
      "summary": "Aunque los avanzados Multimodal Large Language Models (MLLMs) también presentan limitaciones en la comprensión de pequeñas movimientos. Generalmente, falta una diferenciación temporal adecuada, y frecuentemente se promedia o se ignora el contador visual entre imágenes. Además, los prompts visuales en imágenes estáticas han mostrado potenciales, pero su aplicación en videos para comprender movimientos pequeños aún no ha sido explorada en gran medida debido a la complejidad temporal. Estamos investigando si MLLMs pueden mejorar su reconocimiento de movimientos y distinguir entre el movimiento de objetos y el movimiento de la cámara, generando señales visuales especiales. En este estudio, presentamos una nueva metodología zero-shot llamada \"MotionSight\", que utiliza un espléndido de objetos centrado y un breve de movimiento como prompts visuales, sin necesidad de entrenamiento, para comprender eficazmente pequeñas movimientos. Para convertir esto en un conjunto de datos útil, hemos creado el primer grande conjunto de datos de escala, MotionVid-QA. Este conjunto de datos cuenta con anotaciones lógicas que incluyen entrenamiento y datos de preferencia, y está compuesto por aproximadamente 40K de clips de video y 87K de preguntas y respuestas. Los resultados de los experimentos muestran que MotionSight alcanza los mejores rendimientos abiertos y demostra su competencia con modelos comerciales. En particular, ofrece una nueva tecnología zero-shot y un gran conjunto de datos de alta calidad para comprender movimientos pequeños. Todo el código y los datos de anotación están disponibles públicamente.",
      "upvotes": 19,
      "discussionId": "683faa6615abeae85e1333c2",
      "projectPage": "https://nju-pcalab.github.io/projects/MotionSight/",
      "githubRepo": "https://github.com/NJU-PCALab/MotionSight",
      "ai_summary": "MotionSight, a zero-shot method using object-centric visual spotlight and motion blur as prompts, enhances fine-grained video motion understanding and achieves state-of-the-art performance on MotionVid-QA, a large-scale dataset with hierarchical annotations.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "fine-grained video motion understanding",
        "inter-frame differencing",
        "visual prompting",
        "temporal complexities",
        "MotionSight",
        "object-centric visual spotlight",
        "motion blur",
        "MotionVid-QA",
        "hierarchical annotations",
        "SFT",
        "preference data",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-06-02T09:44:56.000Z",
    "title": "MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal\n  LLMs",
    "summary": "Despite advancements in Multimodal Large Language Models (MLLMs), their\nproficiency in fine-grained video motion understanding remains critically\nlimited. They often lack inter-frame differencing and tend to average or ignore\nsubtle visual cues. Furthermore, while visual prompting has shown potential in\nstatic images, its application to video's temporal complexities, particularly\nfor fine-grained motion understanding, remains largely unexplored. We\ninvestigate whether inherent capability can be unlocked and boost MLLMs' motion\nperception and enable distinct visual signatures tailored to decouple object\nand camera motion cues. In this study, we introduce MotionSight, a novel\nzero-shot method pioneering object-centric visual spotlight and motion blur as\nvisual prompts to effectively improve fine-grained motion understanding without\ntraining. To convert this into valuable data assets, we curated MotionVid-QA,\nthe first large-scale dataset for fine-grained video motion understanding, with\nhierarchical annotations including SFT and preference data, {\\Theta}(40K) video\nclips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves\nstate-of-the-art open-source performance and competitiveness with commercial\nmodels. In particular, for fine-grained motion understanding we present a novel\nzero-shot technique and a large-scale, high-quality dataset. All the code and\nannotations will be publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65927f3b754092f6b1e187a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65927f3b754092f6b1e187a7/gUrNvIQHmsl1vLwSUxpmL.jpeg",
      "fullname": "tiehan fan",
      "name": "AnonMegumi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03065",
      "authors": [
        {
          "_id": "683fc6241de14546d5e02775",
          "user": {
            "_id": "64c9bac33cfe45b07179568d",
            "avatarUrl": "/avatars/4a8206cdb1770a8cdaae0d0a2b7b59f2.svg",
            "isPro": false,
            "fullname": "Pengtao Chen",
            "user": "PengtaoChen",
            "type": "user"
          },
          "name": "Pengtao Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:45.116Z",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02776",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02777",
          "name": "Maosen Zhao",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02778",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02779",
          "name": "Mingzhu Shen",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e0277a",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:42.268Z",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e0277b",
          "user": {
            "_id": "63417332c5565a4b8d43a0d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
            "isPro": false,
            "fullname": "Gang Yu",
            "user": "skicy",
            "type": "user"
          },
          "name": "Gang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:39.068Z",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e0277c",
          "name": "Tao Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/iewcVFqJgelW9EQTlH_LH.png"
      ],
      "publishedAt": "2025-06-03T16:42:37.000Z",
      "submittedOnDailyAt": "2025-06-04T04:37:13.352Z",
      "title": "Sparse-vDiT: El poder de la atención poco densa acelera el Video Diffusion Transformer.",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "DiTs ha sido capaces de lograr avances en la generación de películas, pero el desafío de la generación de secuencias largas está limitado por una estructura de atención bidimensional compleja y requiere tiempos de inferencia significativos. Analizando la mapa de atención del Difusor Transformer (vDiT), se identificaron tres patrones de hipérsparsidad posibles: estructuras diagonal, multidiagonal y verticales. Estos patrones limitan la dependencia entre las capas y las posiciones de los cabezas de atención en función de la profundidad de las capas. Estos hallazgos se utilizaron para proponer un marco de aceleración de hipérsparsidad llamado Sparse-vDiT. Sparse-vDiT sustituye la atención densa con un kernel de hipérsparsidad optimizado para cada patron de hipérsparsidad, lo cual es implementado de manera eficiente computacionalmente. También incluye un algoritmo de búsqueda de hipérsparsidad en línea que modela los costos de hardware para seleccionar la estrategia de cálculo de hipérsparsidad óptima para cada capa y cabeza. Después de determinar la configuración óptima, se integran los cabezas que comparten la misma estrategia de atención dentro de la misma capa para mejorar la eficiencia de la inferencia. Al integrar el modelo vDiT más reciente (CogVideoX1.5, HunyuanVideo, Wan2.1), Sparse-vDiT logró reducciones teóricas de FLOP del 2.09, 2.38 y 1.67 veces, aumentos en la velocidad de inferencia del 1.76, 1.85 y 1.58 veces, y mantuvo altos niveles de calidad visual, alcanzando valores de PSNR de 24.13, 27.09 y 22.59. Nuestro estudio muestra que el potencial estructural de hipérsparsidad en vDiT puede ser sistemáticamente explotado en la generación de películas largas.",
      "upvotes": 18,
      "discussionId": "683fc62b1de14546d5e02931",
      "githubRepo": "https://github.com/Peyton-Chen/Sparse-vDiT",
      "ai_summary": "Sparse-vDiT accelerates Video Diffusion Transformer (vDiT) by leveraging sparsity patterns in attention maps, reducing theoretical FLOPs and improving inference speed without significant loss in visual quality.",
      "ai_keywords": [
        "Diffusion Transformers",
        "DiTs",
        "Video Diffusion Transformer",
        "vDiT",
        "sparsity patterns",
        "diagonal",
        "multi-diagonal",
        "vertical-stripe structures",
        "sparse kernels",
        "sparse diffusion search algorithm",
        "FLOP reduction",
        "inference speedups",
        "PSNR",
        "latent structural sparsity"
      ]
    },
    "publishedAt": "2025-06-03T12:42:37.000Z",
    "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers",
    "summary": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09times, 2.38times, and 1.67times theoretical\nFLOP reduction, and actual inference speedups of 1.76times, 1.85times,\nand 1.58times, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/iewcVFqJgelW9EQTlH_LH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03065.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02096",
      "authors": [
        {
          "_id": "683fa36f7ed99d0040761114",
          "user": {
            "_id": "626d268d5f7327906f05cad1",
            "avatarUrl": "/avatars/18bda74612a3ee63a17f991bcc695106.svg",
            "isPro": true,
            "fullname": "Zijian Wu",
            "user": "Jakumetsu",
            "type": "user"
          },
          "name": "Zijian Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:57:09.765Z",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761115",
          "name": "Jinjie Ni",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761116",
          "name": "Xiangyan Liu",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761117",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761118",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761119",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:45:16.000Z",
      "submittedOnDailyAt": "2025-06-04T00:44:38.612Z",
      "title": "SynthRL: Expansión de la Inferencia Visual Demonstrablemente Asegurada mediante la Síntesis de Datos",
      "submittedOnDailyBy": {
        "_id": "6486b09e8315b19342f0bf5e",
        "avatarUrl": "/avatars/bc5f22f231c884146d373fe1042d81bd.svg",
        "isPro": false,
        "fullname": "Xiangyan Liu",
        "user": "xyliu6",
        "type": "user"
      },
      "summary": "MODELOS VISIÓN-IDIOMA (VLMs) SON CONSTRUIDOS MEDIANTE APRENDIZAJE REINFORCIDO CON REMPENSABLES VERIFICABLES (RLVR) Y HAN DEMOSTRADO ADVANCES SIGNIFICATIVOS EN EL ESCALADO EFICIENTE DURANTE EL TIEMPO DE PRUEBA. ESTE PAPER INVESTIGARÍA CUAL ES LA FORMA EN LA QUE LOS DATOS DE APRENDIZAJE REINFORCIDO SYNTHESIZED PUEDEN MEJORAR FURTAMENTE EL RLVR. PARA ESTO, SE PROPUNE UNA PIPELINE SCALABLE Y ASEGURADO PARA LA AUTOMÁTICA SCALING DE DATOS EN EL ENTRENAMIENTO DE RL LOGÍCO. SynthRL ESTÁ COMPUESTO POR TRES ETAPAS CLAVE: (1) SELECCIÓN DE PREGUNTAS SEED CON DISTRIBUCIONES APROPRIADAS, (2) MEJORACIÓN DE ESTAS A MÁS VARIANTES DEMASIADO DESARROLLADAS MISMO QUE MANTENGAN LAS RESPUESTAS ORIGINALES, Y (3) UNA ETAPA DE VERIFICACIÓN ASEGURADA PARA ASEGURAR UNA EXCELENTE EXACTITUD Y AUMENTAR LA DIFICULTAD. LOS RESULTADOS EXPERIMENTALES DEMOSTRAN LA ESCALABILIDAD Y EFICIENCIA DE SynthRL. AL APLICARSE AL DATASET MMK12, SynthRL SYNTHESIZES APROXIMATEMENTE 3.3K PREGUNTAS ADICIONALES DE HARD Y VERIFICABLES DESDE APROXIMATEMENTE 8K SAMPLES SEED. EL MODELO ENTRENADO EN LOS DATOS SYNTHESIZED CONTINUAMENTE DA PROGRESOS EN CINCO ÁREAS DISTINTAS DEL BENCHMARK DE IDIOMA MATEMÁTICO VISUAL Y DEMOSTRA TAMBIÉN ADVANCES SIGNIFICATIVOS SOBRE EL MODELO DE BASE ENTRENADO SOLO CON LOS DATOS SEED. NOTABLEMENTE, UNA ANALISIS DETALLADO DEMOSTRA QUE EL GAIN ES MUY MÁS PRONOUNCED EN LOS SAMPLES DE EVALUACIÓN MÁS DESARROLLADOS Y QUE SynthRL EFECTUAMENTE EXTRAE PATRONES COMPLEJOS DE REASONAMIENTO DENTRO.",
      "upvotes": 18,
      "discussionId": "683fa3707ed99d0040761154",
      "ai_summary": "SynthRL, a scalable pipeline for data synthesis in reinforcement learning with verifiable rewards, enhances visual math reasoning VLMs by generating challenging, verifiable questions.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable reward",
        "RLVR",
        "SynthRL",
        "seed questions",
        "data augmentation",
        "verification stage",
        "MMK12 dataset",
        "visual math reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-06-02T13:45:16.000Z",
    "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis",
    "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose SynthRL-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02096.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6486b09e8315b19342f0bf5e",
      "avatarUrl": "/avatars/bc5f22f231c884146d373fe1042d81bd.svg",
      "fullname": "Xiangyan Liu",
      "name": "xyliu6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03143",
      "authors": [
        {
          "_id": "683fc5599363b50c19f17d42",
          "user": {
            "_id": "63ef330b1e695b35aa484e11",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ef330b1e695b35aa484e11/bXwpGy0dl8JXeJwJ--ilr.jpeg",
            "isPro": false,
            "fullname": "Qianhui WU",
            "user": "qianhuiwu",
            "type": "user"
          },
          "name": "Qianhui Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T04:21:31.171Z",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d43",
          "user": {
            "_id": "63340dbbd92c5842ae71d1e9",
            "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
            "isPro": false,
            "fullname": "Kanzhi Cheng",
            "user": "cckevinn",
            "type": "user"
          },
          "name": "Kanzhi Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:48.476Z",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d44",
          "user": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "isPro": false,
            "fullname": "Rui Yang",
            "user": "Ray2333",
            "type": "user"
          },
          "name": "Rui Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:51.070Z",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d45",
          "user": {
            "_id": "654dbac9938fbf1e696be8aa",
            "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
            "isPro": false,
            "fullname": "Chaoyun Zhang",
            "user": "vyokky",
            "type": "user"
          },
          "name": "Chaoyun Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:54.267Z",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d46",
          "name": "Jianwei Yang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d47",
          "name": "Huiqiang Jiang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d48",
          "name": "Jian Mu",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d49",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4a",
          "name": "Bo Qiao",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4b",
          "name": "Reuben Tan",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4c",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4d",
          "name": "Lars Liden",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4e",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4f",
          "name": "Huan Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d50",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d51",
          "name": "Jianbing Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d52",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d53",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:59:08.000Z",
      "submittedOnDailyAt": "2025-06-04T02:44:41.730Z",
      "title": "GUI Actor: Interfaz gráfica del grupo de visualización sin coordenadas\n\nGUI Actor: Interfaz gráfica del grupo de visualización sin coordenadas",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "El desarrollo de un agente para la protección de GUI (Interfaz Gráfica de Usuario) en el contexto de modelos de lenguaje visuolingüísticos (VLM) presenta una de las principales desafíos en la visualización, es decir, la detección de áreas de la pantalla adecuadas basadas en contenidos visuales y planificación del contexto. Actualmente, muchos estudios tratan de este problema como un desafío de generación de coordenadas basadas en texto, pero esta aproximación presenta varias limitaciones: deficiencias en alineamiento espacial y sintáctico, imposibilidad de tratar objetivos de sub-área inciertos, y una asignación inadecuada debido a la densidad de coordenadas de la pantalla y la gran cantidad de características visuales extraídas por Vision Transformers. En este artículo, se propone un método de detección de áreas de GUI sin coordenadas. El agente GUI-Actor supera las limitaciones de la generación de coordenadas basada en texto introduciendo una cabeza de acción basada en atach y asegurando que un token <ACTOR> se alinee con todos los tokens de patches visuales relevantes, permitiendo que el modelo proponga múltiples áreas de acción en una sola pasada de frente. Para evaluar y seleccionar la área de acción más probable, se diseña una función de verificación de áreas. Experimentos extensos muestran que el agente GUI-Actor supera los métodos superiores en varios benchmarks de acciones de GUI, mejorando la generalización a nuevas resoluciones de pantalla y nuevos diseños de pantalla. Específicamente, el modelo GUI-Actor-7B supera a UI-TARS-72B en ScreenSpot-Pro, alcanzando puntuaciones de 40.7 y 44.6 con Qwen2-VL y Qwen2.5-VL, respectivamente. Además, al introducir la función de verificación de áreas, se logra un modelo de VLM que, con un ajuste mínimo de la cabeza de acción, alcanza el mismo rendimiento que los modelos superiores anteriores, demostrando que el agente GUI-Actor puede dotar efectivamente a un modelo básico de VLM de habilidades de detección de áreas, sin perder sus ventajas generales.",
      "upvotes": 17,
      "discussionId": "683fc55d9363b50c19f17e6b",
      "projectPage": "https://microsoft.github.io/GUI-Actor/",
      "githubRepo": "https://github.com/microsoft/GUI-Actor",
      "ai_summary": "GUI-Actor, a VLM-based method using attention for coordinate-free GUI grounding, outperforms existing methods with better generalization and efficient fine-tuning.",
      "ai_keywords": [
        "visual grounding",
        "text-based coordinate generation",
        "spatial-semantic alignment",
        "Vision Transformers",
        "attention-based action head",
        "grounding verifier",
        "GUI action grounding benchmarks",
        "GUI-Actor",
        "GUI-Actor-7B",
        "UI-TARS-72B",
        "ScreenSpot-Pro",
        "Qwen2-VL",
        "Qwen2.5-VL",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-06-03T13:59:08.000Z",
    "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
    "summary": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03131",
      "authors": [
        {
          "_id": "683fb2786a2b978ca4e62493",
          "user": {
            "_id": "64b7aa374df206a3ed1947d2",
            "avatarUrl": "/avatars/a7c7e703ccf8824259fc5a8a90a25746.svg",
            "isPro": false,
            "fullname": "wzd",
            "user": "GoodEnough",
            "type": "user"
          },
          "name": "Zidong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:13.762Z",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62494",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62495",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62496",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62497",
          "name": "Yiyuan Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63176933b58b0184630d2c74/VS2G-SBuSY5ltQmCLAaq3.png"
      ],
      "publishedAt": "2025-06-03T17:57:33.000Z",
      "submittedOnDailyAt": "2025-06-04T01:13:44.155Z",
      "title": "Nebit Registración Imagen Sintética",
      "submittedOnDailyBy": {
        "_id": "63176933b58b0184630d2c74",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
        "isPro": false,
        "fullname": "Yiyuan Zhang",
        "user": "Yiyuan",
        "type": "user"
      },
      "summary": "Introduzco la notación de síntesis de imágenes. Este es un nuevo paradigma de modelado generativo que permite la síntesis de imágenes con cualquier resolución y aspecto de ancho a altura. Este enfoque supera los limites de métodos tradicionales que se basan en una sola resolución fija y imágenes cuadradas, y procesa de manera nativa tokens visuales de longitud variable. De esta manera, se resuelven las principales limitaciones de los métodos tradicionales. Por lo tanto, se introduce la Native-resolution diffusion Transformer (NiT). Esta arquitectura tiene como objetivo modelar explícitamente la resolución y el aspecto de ancho a altura que varían durante el proceso de tratamiento de ruido. Liberada de las restricciones de formato fijo, la NiT aprende una distribución visual única para imágenes de diferentes resoluciones y aspectos de ancho a altura. En particular, un solo modelo de NiT logra alcanzar el mejor rendimiento en los marcos de referencia de ImageNet-256x256 y 512x512. Sorprendentemente, la NiT, que se entrenó solo en ImageNet y muestra capacidad de expansión 0-shot similar a la de grandes modelos de lenguaje, logra generar imágenes de alta calidad en resoluciones altas (por ejemplo, 1536 x 1536) y diferentes aspectos de ancho a altura (por ejemplo, 16:9, 3:1, 4:3). Estos hallazgos demuestran que la modelación de imágenes con notación de óperadores tiene potencial importante entre la modelación visual y los materiales avanzados de modelos de lenguaje de grandes escala.",
      "upvotes": 13,
      "discussionId": "683fb27e6a2b978ca4e625bd",
      "projectPage": "https://wzdthu.github.io/NiT/",
      "githubRepo": "https://github.com/WZDTHU/NiT",
      "ai_summary": "A novel generative model, Native-resolution diffusion Transformer (NiT), synthesizes high-resolution and varied aspect ratio images with state-of-the-art performance and zero-shot generalization capabilities.",
      "ai_keywords": [
        "native-resolution image synthesis",
        "generative modeling paradigm",
        "variable-length visual tokens",
        "diffusion Transformer",
        "denoising process",
        "ImageNet-256x256",
        "ImageNet-512x512",
        "high-fidelity images",
        "aspect ratios",
        "zero-shot generalization"
      ]
    },
    "publishedAt": "2025-06-03T13:57:33.000Z",
    "title": "Native-Resolution Image Synthesis",
    "summary": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63176933b58b0184630d2c74/VS2G-SBuSY5ltQmCLAaq3.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03131.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63176933b58b0184630d2c74",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
      "fullname": "Yiyuan Zhang",
      "name": "Yiyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00070",
      "authors": [
        {
          "_id": "683fd7d79d4fb703271ad9ac",
          "name": "Dongyoung Kim",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9ad",
          "name": "Sumin Park",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9ae",
          "name": "Huiwon Jang",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9af",
          "name": "Jinwoo Shin",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9b0",
          "name": "Jaehyung Kim",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9b1",
          "name": "Younggyo Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:41:12.000Z",
      "submittedOnDailyAt": "2025-06-04T03:53:04.651Z",
      "title": "Robótica R1: Mejora de la lógica mecanizada mediante aprendizaje por refuerzo en el objetivo de la ingeniería robótica",
      "submittedOnDailyBy": {
        "_id": "658d79663a5202a485a76d9b",
        "avatarUrl": "/avatars/1893384f28a7cb82d4588576c4c264f1.svg",
        "isPro": false,
        "fullname": "dongyoung kim",
        "user": "vangard703",
        "type": "user"
      },
      "summary": "Los grandes modelos de lenguaje de visión (LVLMs) muestran un gran potencial para el desarrollo de la tecnología de robots al combinar lógicas mecanizadas de control de robots basadas en aprendizaje estadístico. El enfoque general utiliza principalmente la entrenamiento de tareas de lógicas mecanizadas con Fine-Tuning Supervisado (SFT), pero los conjuntos de datos SFT son a menudo construidos heurísticamente, lo que limita su optimización explícita para el mejoramiento del control de robots. Además, el SFT suele presentar problemas como la pérdida catastrófica y la degradación de la generalización. Para superar estos límites, se está desarrollando un nuevo marco de referencia para mejorar las lógicas mecanizadas en el control de robots utilizando aprendizaje por refuerzo. El robot R1 aprende a predecir los puntos clave necesarios para completar una tarea basándose en imágenes y metadatos de entorno actuales de demostraciones específicas. Con su enfoque de aprendizaje, el robot R1 muestra respuestas basadas en razonamiento y mejora su predicción de forma más precisa. Los resultados de los experimentos demuestran que el modelo entrenado en el robot R1 supera a los métodos de SFT en tareas de lógicas mecanizadas. Aunque tiene una limitación con 7B parámetros, es capaz de superar a GPT-4o en tareas de razonamiento relacionadas con el control de acciones avanzadas.",
      "upvotes": 13,
      "discussionId": "683fd7d79d4fb703271ad9d9",
      "ai_summary": "Robot-R1, a reinforcement learning framework, enhances embodied reasoning for robotics by predicting keypoint states, outperforming supervised fine-tuning methods and even surpassing GPT-4o in low-level action control tasks.",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "embodied reasoning",
        "robot control",
        "Supervised Fine-Tuning (SFT)",
        "catastrophic forgetting",
        "generalization performance",
        "reinforcement learning",
        "keypoint state",
        "scene image",
        "environment metadata",
        "expert demonstrations",
        "DeepSeek-R1",
        "reasoning-based responses",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-05-29T12:41:12.000Z",
    "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in\n  Robotics",
    "summary": "Large Vision-Language Models (LVLMs) have recently shown great promise in\nadvancing robotics by combining embodied reasoning with robot control. A common\napproach involves training on embodied reasoning tasks related to robot control\nusing Supervised Fine-Tuning (SFT). However, SFT datasets are often\nheuristically constructed and not explicitly optimized for improving robot\ncontrol. Furthermore, SFT often leads to issues such as catastrophic forgetting\nand reduced generalization performance. To address these limitations, we\nintroduce Robot-R1, a novel framework that leverages reinforcement learning to\nenhance embodied reasoning specifically for robot control. Robot-R1 learns to\npredict the next keypoint state required for task completion, conditioned on\nthe current scene image and environment metadata derived from expert\ndemonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples\nreasoning-based responses and reinforces those that lead to more accurate\npredictions. Our experiments show that models trained with Robot-R1 outperform\nSFT methods on embodied reasoning tasks. Despite having only 7B parameters,\nRobot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action\ncontrol, such as spatial and primitive movement reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00070.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658d79663a5202a485a76d9b",
      "avatarUrl": "/avatars/1893384f28a7cb82d4588576c4c264f1.svg",
      "fullname": "dongyoung kim",
      "name": "vangard703",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03136",
      "authors": [
        {
          "_id": "683fa2ddbde0ae60c2f16183",
          "name": "Yinjie Wang",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16184",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16185",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16186",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16187",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:58:42.000Z",
      "submittedOnDailyAt": "2025-06-04T00:39:07.151Z",
      "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Proponemos el nuevo marco de aprendizaje por refuerzo llamado CURE. Este marco utiliza una diseñación de recompensas especiales para que se evolucionen simultáneamente en función de los resultados de la interacción entre código y pruebas unitarias, sin recibir guías a través de códigos reales. Este enfoque permite entrenamientos flexibles y expandibles y permite que las pruebas unitarias lean directamente de los errores del código. Después de optimizar el modelo Qwen2.5-Instruct, nuestros modelos ReasonFlux-Coder-7B y 14B, que hemos desarrollado, aumentaron la precisión de generación de código en un 5.3% y la precisión del mejor de N en un 9.0%. Estos modelos superan en tamaño a Qwen-Coder, DeepSeek-Coder y Seed-Coder, que son capaces de expandir naturalmente tareas como ajuste de tiempo de prueba y código de agente. Además, el modelo ReasonFlux-Coder-4B para el modelo LONG-COT alcanzó una eficiencia de inferencia en la generación de pruebas unitarias del 64.8%, superando a Qwen3-4B. En particular, se destaca que nuestros modelos pueden ser utilizados como modelos de recompensa efectivos en el aprendizaje por refuerzo de los modelos básicos. Proyecto: https://github.com/Gen-Verse/CURE",
      "upvotes": 12,
      "discussionId": "683fa2debde0ae60c2f161cb",
      "projectPage": "https://huggingface.co/collections/Gen-Verse/reasonflux-coder-6833109ed9300c62deb32c6b",
      "githubRepo": "https://github.com/Gen-Verse/CURE",
      "ai_summary": "CURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "reward design",
        "coding",
        "unit test generation",
        "ReasonFlux-Coder",
        "Qwen2.5-Instruct",
        "Qwen-Coder",
        "DeepSeek-Coder",
        "Seed-Coder",
        "test-time scaling",
        "agentic coding",
        "long-CoT",
        "inference efficiency",
        "reward model"
      ]
    },
    "publishedAt": "2025-06-03T13:58:42.000Z",
    "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
    "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03136.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23061",
      "authors": [
        {
          "_id": "683fc4028de3ffc5838c3fa8",
          "name": "Tarun Suresh",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3fa9",
          "name": "Debangshu Banerjee",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3faa",
          "name": "Shubham Ugare",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3fab",
          "name": "Sasa Misailovic",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3fac",
          "name": "Gagandeep Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T04:04:54.000Z",
      "submittedOnDailyAt": "2025-06-04T02:27:09.092Z",
      "title": "DINGO: La Defensa de LLM contra la Inferencia Limitada",
      "submittedOnDailyBy": {
        "_id": "65e7bb35e5e78134ab049942",
        "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
        "isPro": false,
        "fullname": "Tarun Suresh",
        "user": "tarsur909",
        "type": "user"
      },
      "summary": "Diffusion LLMs muestra un gran potencial para la eficiencia en el tiempo de ejecución en comparación con los tradicionales LLMs autorregresivos. Sin embargo, los modelos de difusión actuales no pueden forzar de manera demonstrativa restricciones formales como expresiones regulares, lo que hace que sean menos confiables en tareas que requieren salidas estructuradas como JSON. En contraste con los modelos autorregresivos, los LLMs de difusión predicen bloques de tokens completos. Esta parallelización hace que los algoritmos de decodificación con restricciones diseñados para predecir la secuencia de tokens no puedan mantener la distribución real del output. Para resolver esto, proponemos DINGO, una estrategia de decodificación con restricciones basada en programación dinámica dinámica y eficiente que mantiene la distribución. DINGO permite que se muestren cadenas de caracteres de salida con la mayor probabilidad de la distribución de predicción del modelo y cumpla estrictamente con las expresiones regulares especificadas por el usuario. En los benchmarks estándares de creación de símbolos matemáticos y JSON, DINGO logra mejoras máximas del 68% respecto a la inferencia ilimitada.",
      "upvotes": 12,
      "discussionId": "683fc4038de3ffc5838c3fd8",
      "ai_summary": "DINGO, a dynamic programming-based decoding strategy, enhances diffusion language models by enforcing structured output constraints, significantly improving performance on symbolic math and JSON generation tasks.",
      "ai_keywords": [
        "diffusion LLMs",
        "autoregressive LLMs",
        "formal constraints",
        "regular expressions",
        "sequential token prediction",
        "parallel token prediction",
        "dynamic programming",
        "constrained decoding",
        "output distribution",
        "symbolic math generation",
        "JSON generation"
      ]
    },
    "publishedAt": "2025-05-29T00:04:54.000Z",
    "title": "DINGO: Constrained Inference for Diffusion LLMs",
    "summary": "Diffusion LLMs have emerged as a promising alternative to conventional\nautoregressive LLMs, offering significant potential for improved runtime\nefficiency. However, existing diffusion models lack the ability to provably\nenforce user-specified formal constraints, such as regular expressions, which\nmakes them unreliable for tasks that require structured outputs, such as\nfixed-schema JSON generation. Unlike autoregressive models that generate tokens\nsequentially, diffusion LLMs predict a block of tokens in parallel. This\nparallelism makes traditional constrained decoding algorithms, which are\ndesigned for sequential token prediction, ineffective at preserving the true\noutput distribution. To address this limitation, we propose DINGO, a dynamic\nprogramming-based constrained decoding strategy that is both efficient and\nprovably distribution-preserving. DINGO enables sampling of output strings with\nthe highest probability under the model's predicted distribution, while\nstrictly satisfying any user-specified regular expression. On standard symbolic\nmath and JSON generation benchmarks, DINGO achieves up to a 68 percentage point\nimprovement over unconstrained inference",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23061.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e7bb35e5e78134ab049942",
      "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
      "fullname": "Tarun Suresh",
      "name": "tarsur909",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02528",
      "authors": [
        {
          "_id": "683fb8bc3bcb592f18f5b866",
          "name": "Yan Gong",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b867",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b868",
          "name": "Yicheng Li",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b869",
          "name": "Chenglin Li",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b86a",
          "name": "Yin Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T07:06:35.000Z",
      "submittedOnDailyAt": "2025-06-04T01:39:07.675Z",
      "title": "RelationAdapter: Aprendizaje y Transmisión de Relaciones Visuais con Transformer Difyunjion",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "El nuevo paradigma de edición generalizada de imágenes surge basándose en la estructura de aprendizaje de los grandes modelos de lenguaje (LLMs). Actualmente, el método de referencia única se centra generalmente en estilos o características, lo que a menudo resulta en dificultades para aplicar transformaciones fijas. Para resolver estas limitaciones, se propone utilizar pares de imágenes fuente y objetivo para extraer y transmitir el significado de edición de contenido relacionado con las nuevas imágenes. Para ello, se introduce un módulo ligero llamado RelationAdapter, que permite que modelos basados en Transformer de Difusión (DiT) detecten y apliquen transformaciones visuales incluso en casos mínimos, facilitando así la comprensión y la transmisión de la intención de edición. Además, se introduce el conjunto de datos detallado Relation252K, que incluye 218 tareas de edición, con el objetivo de evaluar la capacidad de generalización y la aplicabilidad del modelo. Los experimentos realizados en Relation252K muestran que RelationAdapter mejora significativamente la comprensión y la transmisión de la intención de edición, y que, en general, mejora notablemente la calidad de generación y el rendimiento de edición.",
      "upvotes": 11,
      "discussionId": "683fb8bd3bcb592f18f5b89f",
      "ai_summary": "RelationAdapter, a lightweight module, enhances Diffusion Transformer models to capture and apply visual transformations effectively using source-target image pairs, improving editing performance and generalization on diverse tasks.",
      "ai_keywords": [
        "RelationAdapter",
        "Diffusion Transformer",
        "DiT",
        "visual prompt-based image editing",
        "content-aware editing intent",
        "Relation252K",
        "visual transformations",
        "editing intent",
        "generation quality",
        "overall editing performance"
      ]
    },
    "publishedAt": "2025-06-03T03:06:35.000Z",
    "title": "RelationAdapter: Learning and Transferring Visual Relation with\n  Diffusion Transformers",
    "summary": "Inspired by the in-context learning mechanism of large language models\n(LLMs), a new paradigm of generalizable visual prompt-based image editing is\nemerging. Existing single-reference methods typically focus on style or\nappearance adjustments and struggle with non-rigid transformations. To address\nthese limitations, we propose leveraging source-target image pairs to extract\nand transfer content-aware editing intent to novel query images. To this end,\nwe introduce RelationAdapter, a lightweight module that enables Diffusion\nTransformer (DiT) based models to effectively capture and apply visual\ntransformations from minimal examples. We also introduce Relation252K, a\ncomprehensive dataset comprising 218 diverse editing tasks, to evaluate model\ngeneralization and adaptability in visual prompt-driven scenarios. Experiments\non Relation252K show that RelationAdapter significantly improves the model's\nability to understand and transfer editing intent, leading to notable gains in\ngeneration quality and overall editing performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02528.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24714",
      "authors": [
        {
          "_id": "683d04c751706d12b2c262ea",
          "user": {
            "_id": "642da1cd99f3110ac27caca5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
            "isPro": false,
            "fullname": "junyu",
            "user": "luojunyu",
            "type": "user"
          },
          "name": "Junyu Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:03:26.483Z",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262eb",
          "user": {
            "_id": "65a9c8652bf3e0cbbfcab2c8",
            "avatarUrl": "/avatars/fc690a78b5f2e94e08a40059ae40625c.svg",
            "isPro": false,
            "fullname": "Alan KOU",
            "user": "alan1027",
            "type": "user"
          },
          "name": "Zhizhuo Kou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:03:22.133Z",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ec",
          "user": {
            "_id": "6434c115a5aed21dd11981c5",
            "avatarUrl": "/avatars/d51e6e384cfc3affe578e7816bcebb35.svg",
            "isPro": false,
            "fullname": "Yang Liming",
            "user": "chunfenri",
            "type": "user"
          },
          "name": "Liming Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:03:24.346Z",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ed",
          "name": "Xiao Luo",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ee",
          "name": "Jinsheng Huang",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ef",
          "name": "Zhiping Xiao",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f0",
          "name": "Jingshu Peng",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f1",
          "name": "Chengzhong Liu",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f2",
          "name": "Jiaming Ji",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f3",
          "name": "Xuanzhe Liu",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f4",
          "name": "Sirui Han",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f5",
          "name": "Ming Zhang",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f6",
          "name": "Yike Guo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/ilkQ2K5LD4SPQ5oCcV2aB.png"
      ],
      "publishedAt": "2025-05-30T15:36:19.000Z",
      "submittedOnDailyAt": "2025-06-04T01:04:36.480Z",
      "title": "FinMME: Conjunto de datos de evaluación para el marco de referencia de datos de modelos financieros",
      "submittedOnDailyBy": {
        "_id": "642da1cd99f3110ac27caca5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
        "isPro": false,
        "fullname": "junyu",
        "user": "luojunyu",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje de máquina (MLLMs) están experimentando un rápido crecimiento reciente. Sin embargo, se encuentra una carencia de conjuntos de datos de evaluación de MLLMs efectivos y profesionales en el ámbito financiero. Para fomentar el desarrollo de MLLMs en el sector financiero, presentamos FinMME. FinMME extiende a 18 áreas financieras y a 6 clases de activos, aborda 10 tipos de gráficos principales y 21 sub-tipos, y incluye más de 11,000 muestras de estudio financiero de alta calidad. Para garantizar la calidad del datos, se utilizaron 20 anotadores y una estructura de verificación cuidadosamente diseñada. Además, desarrollamos FinScore, que introduce penalizaciones de simulación y evaluaciones multidimensionales para proporcionar evaluaciones no sesgadas. A través de experimentos extensos, se demostró que incluso el modelo más reciente, GPT-4o, presenta un desempeño insatisfactorio en FinMME, revelando las dificultades que enfrenta. El benchmark muestra que las variaciones en las predicciones con respecto a otros modelos son del 1% o menos, mostrando una alta confianza en comparación con los conjuntos de datos actuales. Nuestro conjunto de datos y protocolo de evaluación están disponibles en https://huggingface.co/datasets/luojunyu/FinMME y https://github.com/luo-junyu/FinMME.",
      "upvotes": 11,
      "discussionId": "683d04c951706d12b2c26367",
      "projectPage": "https://huggingface.co/datasets/luojunyu/FinMME",
      "githubRepo": "https://github.com/luo-junyu/FinMME",
      "ai_summary": "FinMME is a comprehensive multimodal dataset for financial research and FinScore is an evaluation system that highlights the challenges faced by even advanced models like GPT-4o in the finance domain.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "FinMME",
        "financial research samples",
        "high-quality dataset",
        "financial domains",
        "asset classes",
        "chart types",
        "data quality",
        "FinScore",
        "hallucination penalties",
        "multi-dimensional capability assessment",
        "benchmark dataset",
        "prediction robustness"
      ]
    },
    "publishedAt": "2025-05-30T11:36:19.000Z",
    "title": "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation",
    "summary": "Multimodal Large Language Models (MLLMs) have experienced rapid development\nin recent years. However, in the financial domain, there is a notable lack of\neffective and specialized multimodal evaluation datasets. To advance the\ndevelopment of MLLMs in the finance domain, we introduce FinMME, encompassing\nmore than 11,000 high-quality financial research samples across 18 financial\ndomains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We\nensure data quality through 20 annotators and carefully designed validation\nmechanisms. Additionally, we develop FinScore, an evaluation system\nincorporating hallucination penalties and multi-dimensional capability\nassessment to provide an unbiased evaluation. Extensive experimental results\ndemonstrate that even state-of-the-art models like GPT-4o exhibit\nunsatisfactory performance on FinMME, highlighting its challenging nature. The\nbenchmark exhibits high robustness with prediction variations under different\nprompts remaining below 1%, demonstrating superior reliability compared to\nexisting datasets. Our dataset and evaluation protocol are available at\nhttps://huggingface.co/datasets/luojunyu/FinMME and\nhttps://github.com/luo-junyu/FinMME.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/ilkQ2K5LD4SPQ5oCcV2aB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24714.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "642da1cd99f3110ac27caca5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
      "fullname": "junyu",
      "name": "luojunyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03126",
      "authors": [
        {
          "_id": "683fb3719f37285365b080c9",
          "user": {
            "_id": "672a037c19f1f942483f680c",
            "avatarUrl": "/avatars/a48464044e9eb11a2bc062be05d9aa9a.svg",
            "isPro": false,
            "fullname": "qiulu",
            "user": "qiulu66",
            "type": "user"
          },
          "name": "Lu Qiu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:04.988Z",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080ca",
          "user": {
            "_id": "630b094f8b327c7b8b94d24c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
            "isPro": false,
            "fullname": "Yizhuo Li",
            "user": "liyz",
            "type": "user"
          },
          "name": "Yizhuo Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:02.456Z",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080cb",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080cc",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080cd",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080ce",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:55:18.000Z",
      "submittedOnDailyAt": "2025-06-04T03:58:52.500Z",
      "title": "AnimeShooter: Guía de referencia para el generación de vídeos en el conjunto de datos de animación",
      "submittedOnDailyBy": {
        "_id": "630b094f8b327c7b8b94d24c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
        "isPro": false,
        "fullname": "Yizhuo Li",
        "user": "liyz",
        "type": "user"
      },
      "summary": "El desarrollo reciente del contenido generado por IA (AIGC) ha acelerado significativamente la producción de animacións. Para crear una animación interesante, es crucial crear videoclips de corta duración que incluyan una nota de borde y un enlace de personaje en un video de corta duración. Sin embargo, los conjuntos de datos publicados actualmente se centran principalmente en escenarios realistas mundiales, lo que resulta en una deficiencia de imágenes de referencia para garantizar la coherencia de los personajes. Para complementar esto, presentamos el conjunto de datos de animación de corta duración que incluye referencias, llamado AnimeShooter. AnimeShooter caracteriza su proceso de automatización con análisis heurísticos detallados y una fuerte coherencia visual entre los shots. Los análisis a nivel de historia tienen la función de proporcionar resumenes de las notas de borde, incluyendo la historia, personajes clave, perfiles de personajes principales y imágenes de referencia. Por otro lado, los análisis a nivel de shot se ocupan de dividir la historia en shots continuos, y incluyen descripciones visuales detalladas de la escena, personajes, notas de borde y captiones explicativas. Además, el subconjunto especial AnimeShooter-audio proporciona tractos de audio sincronizados, explicaciones vocales y sonidos para cada shot. Para demostrar el efecto de AnimeShooter, hemos presentado el modelo de generación de videos de corta duración con referencias, llamado AnimeShooterGen. Este modelo extiende los modelos de lenguaje grande multimodal (MLLM) y de difusión de video. Las imágenes de referencia y los shots generados anteriormente se procesan por primera vez en el MLLM, generando representaciones que tienen un entendimiento de la referencia y el contexto. Estas representaciones se utilizan como condiciones para el modelo de difusión, decidiéndose el siguiente shot. Según los resultados de las experimentaciones, los modelos entrenados en AnimeShooter funcionan de manera coherente visual entre shots y según las guías visuales de referencia, demostrando la valiosa contribución de nuestro conjunto de datos para la generación de videos de animación de corta duración.",
      "upvotes": 10,
      "discussionId": "683fb3749f37285365b08167",
      "projectPage": "https://qiulu66.github.io/animeshooter",
      "githubRepo": "https://github.com/qiulu66/Anime-Shooter",
      "ai_summary": "AnimeShooter, a reference-guided multi-shot animation dataset, enhances coherent animated video generation by incorporating comprehensive hierarchical annotations and visual consistency, and AnimeShooterGen leverages MLLMs and video diffusion models to achieve superior results.",
      "ai_keywords": [
        "AnimeShooter",
        "reference-guided",
        "multimodal large language models (MLLMs)",
        "video diffusion models",
        "hierarchical annotations",
        "visual consistency",
        "cross-shot visual consistency",
        "reference visual guidance"
      ]
    },
    "publishedAt": "2025-06-03T13:55:18.000Z",
    "title": "AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video\n  Generation",
    "summary": "Recent advances in AI-generated content (AIGC) have significantly accelerated\nanimation production. To produce engaging animations, it is essential to\ngenerate coherent multi-shot video clips with narrative scripts and character\nreferences. However, existing public datasets primarily focus on real-world\nscenarios with global descriptions, and lack reference images for consistent\ncharacter guidance. To bridge this gap, we present AnimeShooter, a\nreference-guided multi-shot animation dataset. AnimeShooter features\ncomprehensive hierarchical annotations and strong visual consistency across\nshots through an automated pipeline. Story-level annotations provide an\noverview of the narrative, including the storyline, key scenes, and main\ncharacter profiles with reference images, while shot-level annotations\ndecompose the story into consecutive shots, each annotated with scene,\ncharacters, and both narrative and descriptive visual captions. Additionally, a\ndedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each\nshot, along with audio descriptions and sound sources. To demonstrate the\neffectiveness of AnimeShooter and establish a baseline for the reference-guided\nmulti-shot video generation task, we introduce AnimeShooterGen, which leverages\nMultimodal Large Language Models (MLLMs) and video diffusion models. The\nreference image and previously generated shots are first processed by MLLM to\nproduce representations aware of both reference and context, which are then\nused as the condition for the diffusion model to decode the subsequent shot.\nExperimental results show that the model trained on AnimeShooter achieves\nsuperior cross-shot visual consistency and adherence to reference visual\nguidance, which highlight the value of our dataset for coherent animated video\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630b094f8b327c7b8b94d24c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
      "fullname": "Yizhuo Li",
      "name": "liyz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00910",
      "authors": [
        {
          "_id": "683fcd9956d0f1cfb34f1d51",
          "name": "Seongjae Kang",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d52",
          "name": "Dong Bok Lee",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d53",
          "name": "Hyungjoon Jang",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d54",
          "name": "Dongseop Kim",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d55",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T08:54:37.000Z",
      "submittedOnDailyAt": "2025-06-04T03:10:05.743Z",
      "title": "PCoreSet: Efectividad de la Aprendizaje de Activación en Modelos de Lenguaje Visuo-Semántico mediante la Aceptación de Conocimiento",
      "submittedOnDailyBy": {
        "_id": "6357a08f8ed056fa1ccd3b38",
        "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
        "isPro": false,
        "fullname": "erjui",
        "user": "erjui",
        "type": "user"
      },
      "summary": "La Transferencia de Conocimiento (KD) es un marco ampliamente utilizado para entrenar modelos ligeros o modelos especializados en tareas específicas, mediante la utilización del conocimiento de un modelo docente. Sin embargo, en el aprendizaje activo (AL) como aplicación de KD, el objetivo de minimizar los costos de notación a través de la selección repetitiva de muestras aún está en un estado de investigación muy superficial. Este espacio se basa en la asunción de acceso a una cantidad suficiente de datos etiquetados para el KD, mientras que el AL se centra en escenarios de datos limitados, donde un modelo docente especializado generalmente no es disponible. En este artículo, se presenta el framework ActiveKD, que integra AL y KD utilizando las capacidades de 0 shot y poco shot de grandes modelos de lenguaje visual (VLMs). Un punto clave de ActiveKD es la inclinación estructurada de las VLMs, que forman clusters en el espacio de probabilidades. Esta estructura puede interpretarse como una inclinación de inferencia del modelo docente y ayuda a entender patrones de salida generalizados que benefician el aprendizaje del estudiante. Para aprovechar esta inclinación, se propone el Probabilistic CoreSet (PCoreSet). En contraste con CoreSet, PCoreSet maximiza la cobertura en el espacio de probabilidades, lo que permite transmitir eficientemente el conocimiento del modelo docente dentro de los límites del gestión de notación, sin perder eficiencia. Los resultados de evaluación en 11 conjuntos de datos muestran que PCoreSet coincide con los métodos actuales de selección y fomenta la investigación en los puntos de cruzamiento entre AL y KD.",
      "upvotes": 9,
      "discussionId": "683fcd9d56d0f1cfb34f1eb1",
      "githubRepo": "https://github.com/erjui/PCoreSet",
      "ai_summary": "ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.",
      "ai_keywords": [
        "knowledge distillation",
        "active learning",
        "task-specific models",
        "teacher models",
        "zero-shot",
        "few-shot",
        "large vision-language models",
        "structured prediction bias",
        "inductive bias",
        "Probabilistic CoreSet",
        "PCoreSet",
        "probability space",
        "categorically diverse samples"
      ]
    },
    "publishedAt": "2025-06-01T04:54:37.000Z",
    "title": "PCoreSet: Effective Active Learning through Knowledge Distillation from\n  Vision-Language Models",
    "summary": "Knowledge distillation (KD) is a widely used framework for training compact,\ntask-specific models by leveraging the knowledge of teacher models. However,\nits application to active learning (AL), which aims to minimize annotation\ncosts through iterative sample selection, remains underexplored. This gap stems\nfrom the fact that KD typically assumes access to sufficient labeled data,\nwhereas AL operates in data-scarce scenarios where task-specific teacher models\nare often unavailable. In this paper, we introduce ActiveKD, a framework that\nintegrates AL with KD by leveraging the zero- and few-shot capabilities of\nlarge vision-language models (VLMs). A key aspect of ActiveKD is the structured\nprediction bias of VLMs -- i.e., their predictions form clusters in the\nprobability space. We regard this structure as an inductive bias of the teacher\nmodel, capturing generalizable output patterns beneficial to student learning.\nTo exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection\nstrategy that maximizes coverage in the probability space rather than the\nfeature space. PCoreSet strategically selects categorically diverse unlabeled\nsamples, facilitating more efficient transfer of teacher knowledge under\nlimited annotation budgets. Evaluations on 11 datasets show that PCoreSet\nconsistently outperforms existing selection methods within the ActiveKD\nframework, advancing research at the intersection of AL and KD.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00910.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6357a08f8ed056fa1ccd3b38",
      "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
      "fullname": "erjui",
      "name": "erjui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02497",
      "authors": [
        {
          "_id": "683fbc32917306517315589f",
          "name": "Jiahao Chen",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a0",
          "name": "Hangjie Yuan",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a1",
          "name": "Yichen Qian",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a2",
          "name": "Jingyun Liang",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a3",
          "name": "Jiazheng Xing",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a4",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a5",
          "name": "Weihua Chen",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a6",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a7",
          "name": "Bing Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T06:25:00.000Z",
      "submittedOnDailyAt": "2025-06-04T01:55:34.274Z",
      "title": "LumosFlow: Creación de videos largos que guian a la acción",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "La generación de vídeos largos se utiliza ampliamente en diversas áreas como el entretenimiento y la simulación, y es un campo que recibe mucha atención. Mientras avanza, sin embargo, la síntesis de secuencias visuales largas en tiempo es un desafío complejo. Los métodos existentes generan cortos clips secuencialmente y los combinan o crean marcas clave y interpolan los frames intermedios de manera heurística para formar un vídeo largo. Sin embargo, ambos enfrentan desafíos significativos, incluyendo repeticiones temporales y transiciones no naturales. En este artículo, se revisa la pipeline de generación de vídeos largos y se introduce LumosFlow, una nueva estructura. LumosFlow adopta una guía explícita de movimiento. Concretamente, se utiliza un modelo de difusión del video (LMTV-DM) basado en texto para generar marcas clave a intervalos de grandes movimientos, asegurando así la diversidad del contenido del vídeo. Dado que las transiciones contextuales entre las marcas clave son complejas, se divide la interpolación de los frames intermedios en la generación de movimiento y el procesamiento posterior. Para cada par de marcas clave, se utiliza un modelo de difusión de flujo óptico (LOF-DM) para sintetizar flujos ópticos complejos de grandes movimientos y, posteriormente, se utiliza MotionControlNet para guiar la generación de los frames, mejorando su calidad. Comparado con la interpolación de frames existentes, LumosFlow logra una interpolación 15 veces más eficiente, asegurando movimientos continuos adecuados entre frames adyacentes. Los experimentos demuestran que nuestro método permite generar vídeos largos con movimientos coherentes y características definidas. Los códigos y modelos se publicarán después de la aprobación. La página del proyecto está disponible en https://jiahaochen1.github.io/LumosFlow/.",
      "upvotes": 7,
      "discussionId": "683fbc36917306517315597d",
      "projectPage": "https://jiahaochen1.github.io/LumosFlow/",
      "ai_summary": "LumosFlow uses LMTV-DM for key frame generation and LOF-DM followed by MotionControlNet for smooth intermediate frame interpolation, ensuring temporally coherent long video generation.",
      "ai_keywords": [
        "Large Motion Text-to-Video Diffusion Model",
        "LMTV-DM",
        "Latent Optical Flow Diffusion Model",
        "LOF-DM",
        "MotionControlNet",
        "optical flows",
        "frame interpolation",
        "key frames",
        "long video generation",
        "motion guidance",
        "synthetic long videos"
      ]
    },
    "publishedAt": "2025-06-03T02:25:00.000Z",
    "title": "LumosFlow: Motion-Guided Long Video Generation",
    "summary": "Long video generation has gained increasing attention due to its widespread\napplications in fields such as entertainment and simulation. Despite advances,\nsynthesizing temporally coherent and visually compelling long sequences remains\na formidable challenge. Conventional approaches often synthesize long videos by\nsequentially generating and concatenating short clips, or generating key frames\nand then interpolate the intermediate frames in a hierarchical manner. However,\nboth of them still remain significant challenges, leading to issues such as\ntemporal repetition or unnatural transitions. In this paper, we revisit the\nhierarchical long video generation pipeline and introduce LumosFlow, a\nframework introduce motion guidance explicitly. Specifically, we first employ\nthe Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames\nwith larger motion intervals, thereby ensuring content diversity in the\ngenerated long videos. Given the complexity of interpolating contextual\ntransitions between key frames, we further decompose the intermediate frame\ninterpolation into motion generation and post-hoc refinement. For each pair of\nkey frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes\ncomplex and large-motion optical flows, while MotionControlNet subsequently\nrefines the warped results to enhance quality and guide intermediate frame\ngeneration. Compared with traditional video frame interpolation, we achieve 15x\ninterpolation, ensuring reasonable and continuous motion between adjacent\nframes. Experiments show that our method can generate long videos with\nconsistent motion and appearance. Code and models will be made publicly\navailable upon acceptance. Our project page:\nhttps://jiahaochen1.github.io/LumosFlow/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02497.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01144",
      "authors": [
        {
          "_id": "683fca92c1e51fea3a470e93",
          "name": "Ariel Shaulov",
          "hidden": false
        },
        {
          "_id": "683fca92c1e51fea3a470e94",
          "user": {
            "_id": "64972b8d27e41e26a32835d4",
            "avatarUrl": "/avatars/00c76991b5421f592d632a750ec8b998.svg",
            "isPro": false,
            "fullname": "Itay Hazan",
            "user": "itayhzn",
            "type": "user"
          },
          "name": "Itay Hazan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T06:31:03.556Z",
          "hidden": false
        },
        {
          "_id": "683fca92c1e51fea3a470e95",
          "name": "Lior Wolf",
          "hidden": false
        },
        {
          "_id": "683fca92c1e51fea3a470e96",
          "user": {
            "_id": "6181c72cdcc1df2c9de8a4d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
            "isPro": false,
            "fullname": "Hila Chefer",
            "user": "Hila",
            "type": "user"
          },
          "name": "Hila Chefer",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:36.782Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6181c72cdcc1df2c9de8a4d8/z3VoHZmxOtL3agHCaWWg4.mp4"
      ],
      "publishedAt": "2025-06-01T19:55:33.000Z",
      "submittedOnDailyAt": "2025-06-04T02:58:51.483Z",
      "title": "FlowMo: Mapa de flujo basado en la registro de movimientos continuos de videos",
      "submittedOnDailyBy": {
        "_id": "6181c72cdcc1df2c9de8a4d8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
        "isPro": false,
        "fullname": "Hila Chefer",
        "user": "Hila",
        "type": "user"
      },
      "summary": "El modelo de distribución de animación en contexto tiene limitaciones recordativas para modelar la apariencia temporal (por ejemplo, movimiento, física, interacciones dinámicas). Los métodos actuales para abordar este problema involucran el retenimiento de modelos o la introducción de señales de condiciones externas para imponer la consistencia temporal. En este estudio, investigamos si es posible extraer una representación significativa del tiempo a partir de las predicciones de un modelo previamente entrenado, sin incluir más entrenamiento o entradas auxiliares. Se introdujo un nuevo método de guía sin necesidad de entrenamiento llamado FlowMo, que mejora la consistencia del movimiento utilizando las predicciones del modelo en cada etapa de la distribución. FlowMo mide la distancia entre las variables latentes correspondientes a los frame continuos para obtener una representación del tiempo independiente de la superficie, lo que permite a los modelos predecir con claridad la estructura del tiempo potencial. A continuación, se mide la varianza por dimensiones de tiempo y se evalua la consistencia del movimiento, guiando el modelo hacia un reducimiento de esta varianza. En una amplia gama de experimentos con modelos de animación en contexto, FlowMo mejoró significativamente la consistencia del movimiento, manteniendo la calidad o la compatibilidad con el proyecto, y mejoró la precisión del tiempo en modelos de distribución de animación previamente entrenados, proporcionando una efectiva solución \"jugar y jugar\".",
      "upvotes": 7,
      "discussionId": "683fca94c1e51fea3a470eee",
      "projectPage": "https://arielshaulov.github.io/FlowMo/",
      "githubRepo": "https://github.com/arielshaulov/FlowMo",
      "ai_summary": "FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.",
      "ai_keywords": [
        "text-to-video diffusion models",
        "temporal aspects",
        "motion",
        "physics",
        "dynamic interactions",
        "pre-trained model",
        "FlowMo",
        "guidance method",
        "appearance-debiased",
        "temporal representation",
        "latents",
        "patch-wise variance",
        "sampling",
        "temporal fidelity"
      ]
    },
    "publishedAt": "2025-06-01T15:55:33.000Z",
    "title": "FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video\n  Generation",
    "summary": "Text-to-video diffusion models are notoriously limited in their ability to\nmodel temporal aspects such as motion, physics, and dynamic interactions.\nExisting approaches address this limitation by retraining the model or\nintroducing external conditioning signals to enforce temporal consistency. In\nthis work, we explore whether a meaningful temporal representation can be\nextracted directly from the predictions of a pre-trained model without any\nadditional training or auxiliary inputs. We introduce FlowMo, a novel\ntraining-free guidance method that enhances motion coherence using only the\nmodel's own predictions in each diffusion step. FlowMo first derives an\nappearance-debiased temporal representation by measuring the distance between\nlatents corresponding to consecutive frames. This highlights the implicit\ntemporal structure predicted by the model. It then estimates motion coherence\nby measuring the patch-wise variance across the temporal dimension and guides\nthe model to reduce this variance dynamically during sampling. Extensive\nexperiments across multiple text-to-video models demonstrate that FlowMo\nsignificantly improves motion coherence without sacrificing visual quality or\nprompt alignment, offering an effective plug-and-play solution for enhancing\nthe temporal fidelity of pre-trained video diffusion models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6181c72cdcc1df2c9de8a4d8/z3VoHZmxOtL3agHCaWWg4.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01144.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6181c72cdcc1df2c9de8a4d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
      "fullname": "Hila Chefer",
      "name": "Hila",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01274",
      "authors": [
        {
          "_id": "683fe4caf8916dcd6d1c936a",
          "user": {
            "_id": "673060959e631f353ae1b5e0",
            "avatarUrl": "/avatars/d4b1e23de90ff1d02c38186a259b8d1e.svg",
            "isPro": false,
            "fullname": "Hosu Lee",
            "user": "lakelee",
            "type": "user"
          },
          "name": "Hosu Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:26.062Z",
          "hidden": false
        },
        {
          "_id": "683fe4caf8916dcd6d1c936b",
          "user": {
            "_id": "653238bed0f5a9e537ed966d",
            "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
            "isPro": false,
            "fullname": "Junho Kim",
            "user": "arkimjh",
            "type": "user"
          },
          "name": "Junho Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:30.239Z",
          "hidden": false
        },
        {
          "_id": "683fe4caf8916dcd6d1c936c",
          "name": "Hyunjun Kim",
          "hidden": false
        },
        {
          "_id": "683fe4caf8916dcd6d1c936d",
          "name": "Yong Man Ro",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T03:08:07.000Z",
      "submittedOnDailyAt": "2025-06-04T04:50:22.651Z",
      "title": "ReFoCUS: Optimización de Frames mediante Comprensión del Contexto con Aprendizaje por Refuerzo",
      "submittedOnDailyBy": {
        "_id": "653238bed0f5a9e537ed966d",
        "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
        "isPro": false,
        "fullname": "Junho Kim",
        "user": "arkimjh",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de grandes modelos de lenguaje y visión (LMMs) ha permitido una inferencia visual efectiva, aunque su comprensión de contenido visual está limitada por problemas de la estrategia de selección de frames. Los métodos actuales principalmente utilizan heurísticas estáticas o módulos de búsqueda externos para proporcionar información de frames a los modelos de video-LLM, lo que no proporciona información relevante. En este artículo, se presenta ReFoCUS (Reinforcement-guided Frame Optimization for Contextual Understanding). ReFoCUS es un marco de trabajo de optimización de políticas a nivel de frames, que transfiere el objetivo de optimización al selección de frames basado en la entrada de imagen en respuesta de cadenas de caracteres. ReFoCUS utiliza aprendizaje por refuerzo para entrenar una política de selección de frames que refleja la preferencia interna de frames óptimos para respuestas temporalmente secuenciales, utilizando señales de recompensa obtenidas en el LMM de referencia. Para explorar eficientemente, se utiliza una arquitectura de selección condicional automática para garantizar la continuidad temporal y reducir la complejidad. Nuestro enfoque no requiere control explícito a nivel de frames, pero mejora consistentemente el rendimiento lógico en varios benchmarks de preguntas y respuestas de video, y demuestra la coincidencia entre la selección de frames y la utilidad interna del modelo mediante beta.",
      "upvotes": 4,
      "discussionId": "683fe4ccf8916dcd6d1c93b6",
      "ai_summary": "ReFoCUS uses reinforcement learning to optimize frame selection for video-LLM, enhancing reasoning performance in video QA by aligning with model preferences.",
      "ai_keywords": [
        "Large Multi-modal Models",
        "vision-language reasoning",
        "frame selection strategies",
        "reinforcement learning",
        "frame selection policy",
        "reference LMM",
        "autoregressive architecture",
        "conditional selection architecture",
        "temporal coherence",
        "video QA benchmarks"
      ]
    },
    "publishedAt": "2025-06-01T23:08:07.000Z",
    "title": "ReFoCUS: Reinforcement-guided Frame Optimization for Contextual\n  Understanding",
    "summary": "Recent progress in Large Multi-modal Models (LMMs) has enabled effective\nvision-language reasoning, yet the ability to understand video content remains\nconstrained by suboptimal frame selection strategies. Existing approaches often\nrely on static heuristics or external retrieval modules to feed frame\ninformation into video-LLMs, which may fail to provide the query-relevant\ninformation. In this work, we introduce ReFoCUS (Reinforcement-guided Frame\nOptimization for Contextual UnderStanding), a novel frame-level policy\noptimization framework that shifts the optimization target from textual\nresponses to visual input selection. ReFoCUS learns a frame selection policy\nvia reinforcement learning, using reward signals derived from a reference LMM\nto reflect the model's intrinsic preferences for frames that best support\ntemporally grounded responses. To efficiently explore the large combinatorial\nframe space, we employ an autoregressive, conditional selection architecture\nthat ensures temporal coherence while reducing complexity. Our approach does\nnot require explicit supervision at the frame-level and consistently improves\nreasoning performance across multiple video QA benchmarks, highlighting the\nbenefits of aligning frame selection with model-internal utility.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01274.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653238bed0f5a9e537ed966d",
      "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
      "fullname": "Junho Kim",
      "name": "arkimjh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24726",
      "authors": [
        {
          "_id": "683ffca568402c738a947f4e",
          "name": "Shelly Bensal",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f4f",
          "name": "Umar Jamil",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f50",
          "name": "Christopher Bryant",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f51",
          "user": {
            "_id": "60e61b3969bd0df25c9375da",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
            "isPro": false,
            "fullname": "Melisa Russak",
            "user": "melisa",
            "type": "user"
          },
          "name": "Melisa Russak",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:53:45.047Z",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f52",
          "name": "Kiran Kamble",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f53",
          "name": "Dmytro Mozolevskyi",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f54",
          "name": "Muayad Ali",
          "hidden": false
        },
        {
          "_id": "683ffca568402c738a947f55",
          "name": "Waseem AlShikh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T15:49:42.000Z",
      "submittedOnDailyAt": "2025-06-04T06:34:02.563Z",
      "title": "Reflexión, Prueba de Nuevo, Recompensa: Aplicar la mejora personal a los modelos de aprendizaje profundo (LLMs) con aprendizaje por refuerzo",
      "submittedOnDailyBy": {
        "_id": "60e61b3969bd0df25c9375da",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
        "isPro": false,
        "fullname": "Melisa Russak",
        "user": "melisa",
        "type": "user"
      },
      "summary": "Examinamos métodos para mejorar el rendimiento de grandes modelos de lenguaje mediante la reflexión sobre sí mismos. Demostramos que, cuando el modelo responde incorrectamente, se le proporciona motivación para que reflexione más y mejora su capacidad para resolver tareas complejas y verificables, incluso en casos donde la generación de datos sintéticos es imposible y cuando exista un feedback binario. Nuestro marco funciona en dos etapas: primero, si la tarea falla, el modelo crea comentarios que lo ayuden a reflexionar sobre su propio error. Luego, con esos comentarios, intenta la tarea nuevamente. Si la nueva intentación es exitosa, los tokens generados en el paso de reflexión reciben recompensa. Los resultados de los experimentos muestran una mejora significativa en diferentes arquitecturas de modelo, con un aumento del 34.7% en expresiones matemáticas y del 18.1% en llamadas a funciones. En particular, modelos de ajuste micro con entre 150 millones y 700 millones de parámetros superan a modelos de la misma familia con un orden de magnitud mayor. Nuestro nuevo paradigma ofrece una interesante ruta para desarrollar modelos de lenguaje más útiles y estables que mejoran automáticamente en tareas difíciles, a pesar de limitaciones en el feedback externo.",
      "upvotes": 4,
      "discussionId": "683ffca568402c738a947f7b",
      "ai_summary": "A method using self-reflection and reinforcement learning improves the performance of large language models, especially with limited feedback, by rewarding self-reflections that lead to better task performance.",
      "ai_keywords": [
        "self-reflection",
        "reinforcement learning",
        "self-reflective commentary",
        "performance gains",
        "math equation writing",
        "function calling",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-05-30T11:49:42.000Z",
    "title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
    "summary": "We explore a method for improving the performance of large language models\nthrough self-reflection and reinforcement learning. By incentivizing the model\nto generate better self-reflections when it answers incorrectly, we demonstrate\nthat a model's ability to solve complex, verifiable tasks can be enhanced even\nwhen generating synthetic data is infeasible and only binary feedback is\navailable. Our framework operates in two stages: first, upon failing a given\ntask, the model generates a self-reflective commentary analyzing its previous\nattempt; second, the model is given another attempt at the task with the\nself-reflection in context. If the subsequent attempt succeeds, the tokens\ngenerated during the self-reflection phase are rewarded. Our experimental\nresults show substantial performance gains across a variety of model\narchitectures, as high as 34.7% improvement at math equation writing and 18.1%\nimprovement at function calling. Notably, smaller fine-tuned models (1.5\nbillion to 7 billion parameters) outperform models in the same family that are\n10 times larger. Our novel paradigm is thus an exciting pathway to more useful\nand reliable language models that can self-improve on challenging tasks with\nlimited external feedback.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24726.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60e61b3969bd0df25c9375da",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
      "fullname": "Melisa Russak",
      "name": "melisa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03079",
      "authors": [
        {
          "_id": "683fee0a179d710da07d4352",
          "user": {
            "_id": "634aab35dcf125e4dafc87b1",
            "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
            "isPro": false,
            "fullname": "YangXiuyu",
            "user": "gzzyyxy",
            "type": "user"
          },
          "name": "Xiuyu Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:54:13.216Z",
          "hidden": true
        },
        {
          "_id": "683fee0a179d710da07d4353",
          "name": "Bohan Li",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4354",
          "name": "Shaocong Xu",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4355",
          "name": "Nan Wang",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4356",
          "name": "Chongjie Ye",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4357",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4358",
          "name": "Minghan Qin",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4359",
          "name": "Yikang Ding",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d435a",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d435b",
          "name": "Hang Zhao",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d435c",
          "name": "Hao Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:00:32.000Z",
      "submittedOnDailyAt": "2025-06-04T05:27:07.241Z",
      "title": "ORV: Creación de videos de robot en el centro de operaciones 4D",
      "submittedOnDailyBy": {
        "_id": "634aab35dcf125e4dafc87b1",
        "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
        "isPro": false,
        "fullname": "YangXiuyu",
        "user": "gzzyyxy",
        "type": "user"
      },
      "summary": "Obtener datos de simulación de robots en la realidad en teleoperación requiere tiempo y esfuerzo. Recientemente, modelos de genética de acciones han sido ampliamente introducidos en el aprendizaje de robots y la simulación, reduciendo el esfuerzo de mantenimiento y evitando preocupaciones de seguridad. Sin embargo, las secuencias de acciones utilizadas en estos métodos presentan problemas de precisión de control y capacidad de generalización debido a la falta de codificación global. Para resolver estos limitaciones, se propone el marco de creación de videos de robots (ORV). ORV utiliza secuencias 4D significativas de Occupancy Flow para representar la acción linealmente, proporcionando una guía más precisa y geométrica. Al convertir la representación basada en Occupancy Flow, ORV permite la conversión fácil de datos de simulación en videos de robots realistas, asegurando una alta consistencia temporal y la posibilidad de control preciso. Además, soporta la generación simultánea de videos multifocales para fines de manipulación micro, proporcionando capacidades cruciales para tareas de aprendizaje de robots. Los resultados experimentales extendidos muestran que ORV es consistente y supera los métodos básicos en diferentes conjuntos de datos y sub-tareas. Demo, código y modelo: https://orangesodahub.github.io/ORV",
      "upvotes": 3,
      "discussionId": "683fee12179d710da07d45f4",
      "projectPage": "https://orangesodahub.github.io/ORV/",
      "githubRepo": "https://github.com/OrangeSodahub/ORV",
      "ai_summary": "ORV, an Occupancy-centric Robot Video generation framework, uses 4D semantic occupancy sequences to produce photorealistic, temporally consistent, and precisely controllable robot videos, enhancing existing methods.",
      "ai_keywords": [
        "action-driven generative models",
        "occupancy-centric",
        "4D semantic occupancy sequences",
        "video generation",
        "photorealistic robot videos",
        "temporal consistency",
        "precise controllability",
        "multi-view videos"
      ]
    },
    "publishedAt": "2025-06-03T13:00:32.000Z",
    "title": "ORV: 4D Occupancy-centric Robot Video Generation",
    "summary": "Acquiring real-world robotic simulation data through teleoperation is\nnotoriously time-consuming and labor-intensive. Recently, action-driven\ngenerative models have gained widespread adoption in robot learning and\nsimulation, as they eliminate safety concerns and reduce maintenance efforts.\nHowever, the action sequences used in these methods often result in limited\ncontrol precision and poor generalization due to their globally coarse\nalignment. To address these limitations, we propose ORV, an Occupancy-centric\nRobot Video generation framework, which utilizes 4D semantic occupancy\nsequences as a fine-grained representation to provide more accurate semantic\nand geometric guidance for video generation. By leveraging occupancy-based\nrepresentations, ORV enables seamless translation of simulation data into\nphotorealistic robot videos, while ensuring high temporal consistency and\nprecise controllability. Furthermore, our framework supports the simultaneous\ngeneration of multi-view videos of robot gripping operations - an important\ncapability for downstream robotic learning tasks. Extensive experimental\nresults demonstrate that ORV consistently outperforms existing baseline methods\nacross various datasets and sub-tasks. Demo, Code and Model:\nhttps://orangesodahub.github.io/ORV",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634aab35dcf125e4dafc87b1",
      "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
      "fullname": "YangXiuyu",
      "name": "gzzyyxy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01789",
      "authors": [
        {
          "_id": "683fb99c7c8d720be438000a",
          "name": "Genta Indra Winata",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000b",
          "name": "David Anugraha",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000c",
          "name": "Emmy Liu",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000d",
          "name": "Alham Fikri Aji",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000e",
          "name": "Shou-Yi Hung",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000f",
          "name": "Aditya Parashar",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380010",
          "user": {
            "_id": "64d1e3a87e20ec9ea0020d03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d1e3a87e20ec9ea0020d03/xm-afh1AaqS0e9qpaPo7y.jpeg",
            "isPro": false,
            "fullname": "Patrick Amadeus Irawan",
            "user": "patrickamadeus",
            "type": "user"
          },
          "name": "Patrick Amadeus Irawan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:30.937Z",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380011",
          "name": "Ruochen Zhang",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380012",
          "name": "Zheng-Xin Yong",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380013",
          "name": "Jan Christian Blaise Cruz",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380014",
          "name": "Niklas Muennighoff",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380015",
          "user": {
            "_id": "6469949654873f0043b09c22",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6469949654873f0043b09c22/Lk7IJAR16Wa_sGJ2g81AQ.jpeg",
            "isPro": false,
            "fullname": "Seungone Kim",
            "user": "seungone",
            "type": "user"
          },
          "name": "Seungone Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:22.158Z",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380016",
          "name": "Hanyang Zhao",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380017",
          "user": {
            "_id": "63139ff6b46fc4e24332fa84",
            "avatarUrl": "/avatars/ee6923a7cb218f22535064a87761e497.svg",
            "isPro": false,
            "fullname": "Sudipta Kar",
            "user": "cryptexcode",
            "type": "user"
          },
          "name": "Sudipta Kar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:28.286Z",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380018",
          "name": "Kezia Erina Suryoraharjo",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380019",
          "name": "M. Farid Adilazuarda",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001a",
          "name": "En-Shiun Annie Lee",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001b",
          "name": "Ayu Purwarianti",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001c",
          "name": "Derry Tanti Wijaya",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001d",
          "name": "Monojit Choudhury",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T15:31:52.000Z",
      "submittedOnDailyAt": "2025-06-04T01:42:44.251Z",
      "title": "Los hojas de datos no son suficientes: medida y responsabilidad de la rivalidad de datos y la automatización",
      "submittedOnDailyBy": {
        "_id": "5f5c4b20e56d546cd6233098",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
        "isPro": false,
        "fullname": "Genta Indra Winata",
        "user": "gentaiscool",
        "type": "user"
      },
      "summary": "Los conjuntos de datos de alta calidad son una existencia fundamental para la entrenamiento y evaluación de modelos de aprendizaje automático, y su generación, especialmente mediante anotación humana precisa, es considerada un gran desafío. Muchos artículos sobre conjuntos de datos presentan deficiencias en términos de originalidad, diversidad o gestión de calidad estricta, y estas limitaciones a menudo se ignoran durante la evaluación. También, los detalles importantes sobre la construcción y características de los conjuntos de datos a menudo se omiten en los artículos. Los herramientas existentes, como los datasheets, están preparados para promover la visualización, pero principalmente proporcionan métodos explicativos y no una evaluación cuantificable y estandarizada de la calidad de los datos. Además, la exigencia de metadatos en las conferencias ha sido un incentivo para la responsabilidad, pero también ha sido aplicada de manera desigual. Para resolver estas limitaciones, este artículo argumenta la necesidad de integrar puntos de revisión sistemáticos en los criterios de evaluación de conjuntos de datos. Además, se revisa una metodología escalable y costo eficiente para la generación de datos sintéticos, utilizando herramientas adecuadas y el enfoque de un LLM como jurado. Como incentivo para las acciones, se presenta DataRubrics, un marco estructurado para evaluar la calidad de conjuntos de datos generados por humanos o modelos. Los avances recientes en la evaluación basada en LLM proporcionan soluciones reproducibles, escalables y accionables para la evaluación de la calidad de los conjuntos de datos, permitiendo a los investigadores mantener altos estándares en su investigación centrada en los datos. Además, el código para soportar la reproducibilidad de la evaluación basada en LLM está disponible en https://github.com/datarubrics/datarubrics.",
      "upvotes": 3,
      "discussionId": "683fb99e7c8d720be4380090"
    },
    "publishedAt": "2025-06-02T11:31:52.000Z",
    "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and\n  Accountability",
    "summary": "High-quality datasets are fundamental to training and evaluating machine\nlearning models, yet their creation-especially with accurate human\nannotations-remains a significant challenge. Many dataset paper submissions\nlack originality, diversity, or rigorous quality control, and these\nshortcomings are often overlooked during peer review. Submissions also\nfrequently omit essential details about dataset construction and properties.\nWhile existing tools such as datasheets aim to promote transparency, they are\nlargely descriptive and do not provide standardized, measurable methods for\nevaluating data quality. Similarly, metadata requirements at conferences\npromote accountability but are inconsistently enforced. To address these\nlimitations, this position paper advocates for the integration of systematic,\nrubric-based evaluation metrics into the dataset review process-particularly as\nsubmission volumes continue to grow. We also explore scalable, cost-effective\nmethods for synthetic data generation, including dedicated tools and\nLLM-as-a-judge approaches, to support more efficient evaluation. As a call to\naction, we introduce DataRubrics, a structured framework for assessing the\nquality of both human- and model-generated datasets. Leveraging recent advances\nin LLM-based evaluation, DataRubrics offers a reproducible, scalable, and\nactionable solution for dataset quality assessment, enabling both authors and\nreviewers to uphold higher standards in data-centric research. We also release\ncode to support reproducibility of LLM-based evaluations at\nhttps://github.com/datarubrics/datarubrics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01789.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f5c4b20e56d546cd6233098",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
      "fullname": "Genta Indra Winata",
      "name": "gentaiscool",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03096",
      "authors": [
        {
          "_id": "684002718bd5bff9918ce018",
          "user": {
            "_id": "6310a6bb0a43f97f6c5567d3",
            "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
            "isPro": false,
            "fullname": "Christian Schlarmann",
            "user": "chs20",
            "type": "user"
          },
          "name": "Christian Schlarmann",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:53:34.919Z",
          "hidden": false
        },
        {
          "_id": "684002718bd5bff9918ce019",
          "name": "Francesco Croce",
          "hidden": false
        },
        {
          "_id": "684002718bd5bff9918ce01a",
          "name": "Nicolas Flammarion",
          "hidden": false
        },
        {
          "_id": "684002718bd5bff9918ce01b",
          "name": "Matthias Hein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:27:12.000Z",
      "submittedOnDailyAt": "2025-06-04T08:12:15.755Z",
      "title": "FuseLIP: Fusión Discreta de Tokenes Iniciales para Entendimiento Multimodal",
      "submittedOnDailyBy": {
        "_id": "6310a6bb0a43f97f6c5567d3",
        "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
        "isPro": false,
        "fullname": "Christian Schlarmann",
        "user": "chs20",
        "type": "user"
      },
      "summary": "Contrastive language-image pre-training utiliza diferentes encoders en cada modelo y ajusta la características de pares de texto e imagen en un espacio potencial común. Este enfoque muestra notables resultados en muchas tareas de cero-shot, pero originalmente no puede procesar entradas de múltiples modelos, por lo que no es posible codificar imágenes y texto en un solo vector de características. Por consiguiente, la práctica común es utilizar módulos adicionales para integrar las características extraídas de un solo modelo. En este artículo, se propone una arquitectura alternativa para un encoder de múltiples modelos llamada FuseLIP. Utilizando el desarrollo reciente de tokenizadores discretos de imágenes, se propone un modelo de transformación efectivo para el espacio de vocabulario extendido de tokenizadores de texto e imagen. En este enfoque inicial de fusión, los modelos diferentes interactúan mutuamente y obtienen representaciones más ricas que las fusiones generales de retraso. Se recopila un nuevo conjunto de datos y se diseñan tareas difíciles para un encoder de múltiples modelos. FuseLIP supera a otros enfoques en tareas como VQA y búsqueda de imágenes guiadas por texto, pero en tareas de un solo modelo muestra un rendimiento similar al de referencia.",
      "upvotes": 2,
      "discussionId": "684002758bd5bff9918ce109"
    },
    "publishedAt": "2025-06-03T13:27:12.000Z",
    "title": "FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens",
    "summary": "Contrastive language-image pre-training aligns the features of text-image\npairs in a common latent space via distinct encoders for each modality. While\nthis approach achieves impressive performance in several zero-shot tasks, it\ncannot natively handle multimodal inputs, i.e., encoding image and text into a\nsingle feature vector. As a remedy, it is common practice to use additional\nmodules to merge the features extracted by the unimodal encoders. In this work,\nwe present FuseLIP, an alternative architecture for multimodal embedding.\nLeveraging recent progress in discrete image tokenizers, we propose to use a\nsingle transformer model which operates on an extended vocabulary of text and\nimage tokens. This early fusion approach allows the different modalities to\ninteract at each depth of encoding and obtain richer representations compared\nto common late fusion. We collect new datasets for multimodal pre-training and\nevaluation, designing challenging tasks for multimodal encoder models. We show\nthat FuseLIP outperforms other approaches in multimodal embedding tasks such as\nVQA and text-guided image transformation retrieval, while being comparable to\nbaselines on unimodal tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03096.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310a6bb0a43f97f6c5567d3",
      "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
      "fullname": "Christian Schlarmann",
      "name": "chs20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02454",
      "authors": [
        {
          "_id": "683fb5d592425f86f2be5c40",
          "name": "Zhaorui Yang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c41",
          "name": "Bo Pan",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c42",
          "name": "Han Wang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c43",
          "name": "Yiyao Wang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c44",
          "name": "Xingyu Liu",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c45",
          "name": "Minfeng Zhu",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c46",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c47",
          "name": "Wei Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a568f5764b1dce366f9fd2/Jup3eQX_IL2nKhCQVaSWS.mp4"
      ],
      "publishedAt": "2025-06-03T05:18:19.000Z",
      "submittedOnDailyAt": "2025-06-04T01:28:03.959Z",
      "title": "Multimodal Deep Learning: Generation of Reports Written Cross-Referencing Text and Charts\nStart with an Effective Framework from Scratch",
      "submittedOnDailyBy": {
        "_id": "64a568f5764b1dce366f9fd2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a568f5764b1dce366f9fd2/9THW2AJhEVmzltEAm9mMY.jpeg",
        "isPro": false,
        "fullname": "Zhaorui Yang",
        "user": "zhaoruiyang",
        "type": "user"
      },
      "summary": "La visualización desempeña un papel importante en la transmisión efectiva de conceptos e información. Con el desarrollo reciente de razones y palabras clave de búsqueda, los modelos de lenguaje de gran escala (LLMs) pueden realizar investigaciones profundas y generar informes detallados. Sin embargo, los marcos de trabajo de investigación profunda actualmente se centran principalmente en la generación de texto, y no se han investigado la generación automática de textos cruzados y visualizaciones. Esta nueva tarea plantea problemas cruciales en el diseño de visualizaciones informativas y la integración efectiva de informes de texto. Para resolver estos problemas, proponemos la formalización de la explicación de visualización (FDV). La FDV es una representación de texto estructurada que permite que los LLMs aprendan y generan visualizaciones de alta calidad diversas. Basándonos en esta representación, dividimos el marco de agente Multimodal DeepResearcher en los siguientes cuatro pasos: (1) investigación, (2) texturización de informes de muestra, (3) planificación, y (4) generación de informes de versión modelo. Para evaluar los informes de versión modelo generados, desarrollamos MultimodalReportBench, que incluye 100 temas diversos y utiliza 5 métricas profesionales. Los experimentos de extensión de modelos y métodos de evaluación demuestran la efectividad de Multimodal DeepResearcher. En particular, al utilizar el mismo modelo Claude 3.7 Sonnet, Multimodal DeepResearcher alcanzó un rendimiento general de 82%, superando significativamente los métodos de referencia.",
      "upvotes": 2,
      "discussionId": "683fb5d792425f86f2be5c78",
      "projectPage": "https://rickyang1114.github.io/multimodal-deepresearcher/",
      "ai_summary": "A new framework, Multimodal DeepResearcher, enables Large Language Models to generate high-quality multimodal reports combining text and diverse visualizations through structured textual representations.",
      "ai_keywords": [
        "Formal Description of Visualization",
        "FDV",
        "Multimodal DeepResearcher",
        "researching",
        "exemplar report textualization",
        "planning",
        "multimodal report generation",
        "MultimodalReportBench",
        "multimodal reports"
      ]
    },
    "publishedAt": "2025-06-03T01:18:19.000Z",
    "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports\n  From Scratch with Agentic Framework",
    "summary": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a568f5764b1dce366f9fd2/Jup3eQX_IL2nKhCQVaSWS.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a568f5764b1dce366f9fd2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a568f5764b1dce366f9fd2/9THW2AJhEVmzltEAm9mMY.jpeg",
      "fullname": "Zhaorui Yang",
      "name": "zhaoruiyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02338",
      "authors": [
        {
          "_id": "683fbc730ffa93c1611d513b",
          "user": {
            "_id": "64c8f4cec547ed5243ebd0a8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
            "isPro": false,
            "fullname": "Hyungjoo Chae",
            "user": "hyungjoochae",
            "type": "user"
          },
          "name": "Hyungjoo Chae",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:14.169Z",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513c",
          "name": "Dongjin Kang",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513d",
          "name": "Jihyuk Kim",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513e",
          "name": "Beong-woo Kwak",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513f",
          "name": "Sunghyun Park",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5140",
          "name": "Haeju Park",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5141",
          "name": "Jinyoung Yeo",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5142",
          "name": "Moontae Lee",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5143",
          "name": "Kyungjae Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T00:29:15.000Z",
      "submittedOnDailyAt": "2025-06-04T01:55:22.453Z",
      "title": "Una de las desventajas del modelo de razonamiento abierto: conjunto de datos de LLM de corta contexto para evitar un inicio profundo usando RL",
      "submittedOnDailyBy": {
        "_id": "64c8f4cec547ed5243ebd0a8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
        "isPro": false,
        "fullname": "Hyungjoo Chae",
        "user": "hyungjoochae",
        "type": "user"
      },
      "summary": "La publicación de R1 ha permitido el uso de lógicos modelos de mayor escala (LRM) y los investigadores utilizan generalmente la larga inferencia de la Cadena de Pensamiento (CoT) de R1 para entrenar nuevos LRM. Los estudios previos han demostrado que la entrenamiento directo puede recrear las capacidades de los LRM. Sin embargo, se ha revelado que la dependencia actual del modelo (por ejemplo, R1) actúa como una importante limitación para el desarrollo. Como primera etapa para el desarrollo independiente de LRM, este artículo revisa la posibilidad de construir un conjunto de datos de larga CoT que se adapte a la escala en la inferencia de un LLM no entrenado. Para ello, proporcionamos el Conjunto de Long CoT Collection que incluye 100K razones de CoT. En este conjunto, se introduce una nueva estrategia lógica de o1 en un LLM de corta CoT, lo que permite hacer más largas las predicciones y controlar las mismas de manera más efectiva, así como gestionar mejor los problemas de pensamiento excesivo. El análisis ampliado demuestra que el conjunto de datos puede alcanzar la calidad de R1 o algo inferior. Además, los experimentos muestran que el entrenamiento en el conjunto de datos proporciona un beneficio de 2-3 veces en la mejora de las capacidades lógicas en RLVR.",
      "upvotes": 2,
      "discussionId": "683fbc760ffa93c1611d51cc",
      "ai_summary": "The Long CoT Collection dataset, generated by short CoT LLMs, enhances general reasoning skills and provides a strong foundation for reinforcement learning, achieving quality comparable to R1.",
      "ai_keywords": [
        "long chain-of-thought",
        "CoT inferences",
        "LRMs",
        "direct distillation",
        "inference-time scaling",
        "CoT rationales",
        "short CoT LLMs",
        "reasoning strategies",
        "thought budget",
        "overthinking",
        "reinforcement learning",
        "RLVR"
      ]
    },
    "publishedAt": "2025-06-02T20:29:15.000Z",
    "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to\n  Mitigate Cold-Starting Short CoT LLMs in RL",
    "summary": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c8f4cec547ed5243ebd0a8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
      "fullname": "Hyungjoo Chae",
      "name": "hyungjoochae",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00413",
      "authors": [
        {
          "_id": "683e703c33ac56778c2e51cd",
          "user": {
            "_id": "630139f1f6bea7dd15bdaf4e",
            "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
            "isPro": false,
            "fullname": "Daniel Israel",
            "user": "danielmisrael",
            "type": "user"
          },
          "name": "Daniel Israel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:22.988Z",
          "hidden": false
        },
        {
          "_id": "683e703c33ac56778c2e51ce",
          "name": "Guy Van den Broeck",
          "hidden": false
        },
        {
          "_id": "683e703c33ac56778c2e51cf",
          "name": "Aditya Grover",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T06:10:10.000Z",
      "submittedOnDailyAt": "2025-06-04T03:04:01.663Z",
      "title": "Utilizando Adaptive Parallel Decoding para acelerar los Modelos de Difusión de LLMs.",
      "submittedOnDailyBy": {
        "_id": "630139f1f6bea7dd15bdaf4e",
        "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
        "isPro": false,
        "fullname": "Daniel Israel",
        "user": "danielmisrael",
        "type": "user"
      },
      "summary": "La velocidad de generación de los LLMs está marcada por líneas rojas debido a la decodificación automática, donde cada token se predice secuencialmente en líneas. En cambio, los modelos de difusión y lenguaje (dLLMs) han demostrado teóricamente que la generación en paralelo de tokens es posible, pero en práctica, ha sido difícil lograr la velocidad de los modelos automáticos de recuperación sin significativamente perder calidad. En consecuencia, presentamos un nuevo método llamado Decodificación Paralela Automática Adaptativa (APD), que ajusta dinamicamente la cantidad de tokens extraídos en paralelo. Para ello, definimos el producto del margen de probabilidad del dLLM y la probabilidad de un arreglo de pequeños modelos automáticos de recuperación. Esto invierte el estado estándar de la decodificación de predicción y busca intentar decodificar desde grandes modelos automáticos de recuperación hasta pequeños modelos. Además, para mejorar APD, establecemos límites en la caché de KV y el tamaño de los inputs mascarados. Esta metodología proporciona tres parámetros de ajuste flexibles para adaptar tanto a los transformadores como a la calidad. La APD minimiza la pérdida de calidad en los benchmarks de trayectos y proporciona un transformador claramente de alta calidad.",
      "upvotes": 2,
      "discussionId": "683e703d33ac56778c2e51fe",
      "ai_summary": "Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.",
      "ai_keywords": [
        "autoregressive decoding",
        "diffusion large language models",
        "dLLMs",
        "adaptive parallel decoding",
        "APD",
        "marginal probabilities",
        "joint probability",
        "speculative decoding",
        "KV caching",
        "masked input"
      ]
    },
    "publishedAt": "2025-05-31T02:10:10.000Z",
    "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
    "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00413.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630139f1f6bea7dd15bdaf4e",
      "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
      "fullname": "Daniel Israel",
      "name": "danielmisrael",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16994",
      "authors": [
        {
          "_id": "683fb5d3a09aefea70733fa3",
          "user": {
            "_id": "64e14c5b12a5504dda70e60d",
            "avatarUrl": "/avatars/944b486bb037364ef7d9d2c826526708.svg",
            "isPro": false,
            "fullname": "Runyang",
            "user": "dd101bb",
            "type": "user"
          },
          "name": "Runyang You",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T07:34:53.226Z",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa4",
          "user": {
            "_id": "674038313ccfb67446ae2b35",
            "avatarUrl": "/avatars/8a3c0fdf971363988731f9eb8b13658c.svg",
            "isPro": false,
            "fullname": "tensorslow",
            "user": "tensorslow",
            "type": "user"
          },
          "name": "Yongqi Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-04T02:56:21.008Z",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa5",
          "name": "Xinyu Lin",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa6",
          "user": {
            "_id": "63b6dbc8ccebeadccc888456",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
            "isPro": false,
            "fullname": "Xin Zhang",
            "user": "izhx",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:55:59.107Z",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa7",
          "name": "Wenjie Wang",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa8",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa9",
          "name": "Liqiang Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:55:43.000Z",
      "submittedOnDailyAt": "2025-06-04T01:27:54.567Z",
      "title": "R^2ec: Método de dirección para módulos de recomendación grandes con justificación",
      "submittedOnDailyBy": {
        "_id": "63b6dbc8ccebeadccc888456",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
        "isPro": false,
        "fullname": "Xin Zhang",
        "user": "izhx",
        "type": "user"
      },
      "summary": "Recomendamos expandir el modelo de recomendación como modelo recomendado para utilizar un fuerte modelo de recomendación, y el reciente desarrollo de la inferencia de los LLMs colabora con la revisión de la inferencia de recomendación. El actual estudio está en proceso de colocar los LLMs como módulos de inferencia externos para fortalecer el proceso de recomendación. Sin embargo, esta diseño separado tiene costos de recursos importantes y falta de optimización común. Para resolver estos problemas, proponemos un modelo de grandes escalas integrados con capacidades de inferencia propias, llamado \\name\\. Primero, revisamos la arquitectura del modelo y el objetivo es promover la inferencia y la recomendación de manera autónoma. Luego, proponemos un marco de aprendizaje reforzado llamado RecPO, y optimizamos \\name\\ para mejorar tanto la capacidad de inferencia como la de recomendación con una sola actualización de política. RecPO simula la capacidad de inferencia utilizando todos los etiquetas de recomendación, evitando depender de etiquetaciones específicas de inferencia. Hemos realizado experimentos en tres conjuntos de datos con diferentes criterios y demostrado el efecto de \\name\\. El aumento relativo en Hit@5 es del 68.67% y en NDCG@20 es del 45.21%. El código está disponible en https://github.com/YRYangang/RRec.",
      "upvotes": 2,
      "discussionId": "683fb5d5a09aefea70734001",
      "githubRepo": "https://github.com/YRYangang/RRec",
      "ai_summary": "A unified large recommender model with intrinsic reasoning capabilities is proposed, facilitating interleaved reasoning and recommendation using a reinforcement learning framework called RecPO.",
      "ai_keywords": [
        "recommender models",
        "LLMs",
        "intrinsic reasoning",
        "autoregressive process",
        "reinforcement learning",
        "RecPO",
        "fused reward scheme",
        "Hit@5",
        "NDCG@20"
      ]
    },
    "publishedAt": "2025-05-22T13:55:43.000Z",
    "title": "R^2ec: Towards Large Recommender Models with Reasoning",
    "summary": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16994.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b6dbc8ccebeadccc888456",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
      "fullname": "Xin Zhang",
      "name": "izhx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03144",
      "authors": [
        {
          "_id": "684015f08bd5bff99191dff4",
          "user": {
            "_id": "644b71ddb2e7823a76abcf91",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
            "isPro": false,
            "fullname": "zhou wei",
            "user": "WeiChow",
            "type": "user"
          },
          "name": "Wei Chow",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:56:45.423Z",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff5",
          "name": "Yuan Gao",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff6",
          "name": "Linfeng Li",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff7",
          "name": "Xian Wang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff8",
          "name": "Qi Xu",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dff9",
          "name": "Hang Song",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffa",
          "name": "Lingdong Kong",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffb",
          "name": "Ran Zhou",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffc",
          "name": "Yi Zeng",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffd",
          "name": "Yidong Cai",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dffe",
          "name": "Botian Jiang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191dfff",
          "name": "Shilin Xu",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e000",
          "name": "Jiajun Zhang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e001",
          "name": "Minghui Qiu",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e002",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e003",
          "name": "Tianshu Yang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e004",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "684015f08bd5bff99191e005",
          "name": "Juncheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:59:14.000Z",
      "submittedOnDailyAt": "2025-06-04T08:16:40.362Z",
      "title": "\"Punto de valor: Exploración de contextos multilingües cruzados para consultas multicondicionales\"",
      "submittedOnDailyBy": {
        "_id": "644b71ddb2e7823a76abcf91",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
        "isPro": false,
        "fullname": "zhou wei",
        "user": "WeiChow",
        "type": "user"
      },
      "summary": "El Semántico-Retailer es muy importante para las aplicaciones modernas, pero aún no se ha investigado su impacto en la actualidad. Los datasets existentes están limitados a un lenguaje, una imagen o un solo retailer, y no pueden expresar completamente la capacidad de representación visual de las imágenes cuando se reemplazan por captiones, ya que se considera la continuidad del rendimiento. Sin embargo, los casos prácticos de retailer incluyen consultas que combinan varias imágenes. En este trabajo, se presenta MERIT (Semántico-Retailer Multilingual y Multi-Condition), el primer dataset multilingüe que incluye 7 categorías de productos en 5 idiomas, con 320,000 consultas y 135,000 productos. Los experimentos realizados en MERIT evidenciaron las limitaciones actuales de los modelos: no se consideran elementos de condición específicos y se centran en información global. En respuesta, se propone un nuevo marco de ajuste micro, Coral, que aplica modelos MLLM previamente entrenados, incluyendo reestructuración de codificación, preservación de elementos de condición pequeños y aprendizaje contrastivo para extraer información global detallada. Los experimentos mostraron un aumento del rendimiento del 45.9% en MERIT y demostraron una fuerte capacidad de generalización en 8 benchmarks existentes. Esta contribución establece una base para la investigación futura en Semántico-Retailer Multilingual y Multi-Condition, reconociendo limitaciones actuales y proponiendo un nuevo marco de ajuste micro.",
      "upvotes": 1,
      "discussionId": "684015f38bd5bff99191e158"
    },
    "publishedAt": "2025-06-03T13:59:14.000Z",
    "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition\n  Query",
    "summary": "Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03144.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644b71ddb2e7823a76abcf91",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
      "fullname": "zhou wei",
      "name": "WeiChow",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02510",
      "authors": [
        {
          "_id": "683faa31f0564d1fb4b9ffc6",
          "user": {
            "_id": "642656cbad1e3b0e6e91b752",
            "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
            "isPro": false,
            "fullname": "Jie Zhu",
            "user": "amazingj",
            "type": "user"
          },
          "name": "Jie Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:57:01.885Z",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffc7",
          "name": "Junhui Li",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffc8",
          "name": "Yalong Wen",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffc9",
          "name": "Xiandong Li",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffca",
          "name": "Lifan Guo",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffcb",
          "name": "Feng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T06:41:09.000Z",
      "submittedOnDailyAt": "2025-06-04T00:38:17.377Z",
      "title": "M^3FinMeeting: Dataset de Comprensión y Evaluación de Datos de Reunión Financiera Multilingüe, Multiindustrial y Multifuncional",
      "submittedOnDailyBy": {
        "_id": "642656cbad1e3b0e6e91b752",
        "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
        "isPro": false,
        "fullname": "Jie Zhu",
        "user": "amazingj",
        "type": "user"
      },
      "summary": "Recientemente, ha surgido un nuevo marco de referencia para evaluar el rendimiento de los grandes modelos de lenguaje (LLMs) en el sector financiero. Sin embargo, los actuales marcos de referencia financieros se basan principalmente en artículos de noticias, informes de beneficios o presentaciones, y no son suficientes para capturar fácilmente las tendencias de las reuniones financieras. Para remediar esto, proponemos un nuevo marco de referencia para entender las reuniones financieras. Este marco de referencia incluye conjuntos de datos multilingües, multiindustriales y multitareas. Primero, M^3FinMeeting soporta inglés, chino y japonés, fortaleciendo la comprensión de las discusiones financieras en diferentes contextos lingüísticos. Luego, según la clasificación global de industrias (GICS), el marco de referencia incluye diversas industrias, cubriendo una amplia gama de actividades financieras. Finalmente, M^3FinMeeting extrae pares de consultas y respuestas, incluyendo tres tareas relacionadas con las respuestas, lo que fomenta una comprensión más realista y detallada. Los resultados de experimentos con 7 modelos LLMs populares demuestran que aunque los modelos de contexto largo más avanzados tienen todavía mucho por mejorar, M^3FinMeeting ha demostrado su eficacia como un marco de referencia necesario para evaluar la capacidad de los modelos LLMs para entender las reuniones financieras.",
      "upvotes": 1,
      "discussionId": "683faa32f0564d1fb4ba0005",
      "projectPage": "https://github.com/aliyun/qwen-dianjin",
      "githubRepo": "https://github.com/aliyun/qwen-dianjin",
      "ai_summary": "A new multilingual, multi-sector, and multi-task benchmark, M³FinMeeting, evaluates large language models' performance in understanding financial meetings across different languages and industries.",
      "ai_keywords": [
        "large language models",
        "multilingual",
        "multi-sector",
        "multi-task",
        "benchmark",
        "financial meeting understanding",
        "Global Industry Classification Standard (GICS)",
        "summarization",
        "question-answer pair extraction",
        "question answering"
      ]
    },
    "publishedAt": "2025-06-03T02:41:09.000Z",
    "title": "M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial\n  Meeting Understanding Evaluation Dataset",
    "summary": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called M^3FinMeeting, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, M^3FinMeeting supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\nM^3FinMeeting includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\nM^3FinMeeting as a benchmark for assessing LLMs' financial meeting\ncomprehension skills.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02510.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642656cbad1e3b0e6e91b752",
      "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
      "fullname": "Jie Zhu",
      "name": "amazingj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24362",
      "authors": [
        {
          "_id": "68401045dd25841d998788cc",
          "user": {
            "_id": "61fbe8d2c5e6410373a76b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61fbe8d2c5e6410373a76b2a/m4yM6m2uagJxMselzEZfo.jpeg",
            "isPro": false,
            "fullname": "Anum Afzal",
            "user": "anumafzal94",
            "type": "user"
          },
          "name": "Anum Afzal",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T09:25:18.174Z",
          "hidden": false
        },
        {
          "_id": "68401045dd25841d998788cd",
          "name": "Florian Matthes",
          "hidden": false
        },
        {
          "_id": "68401045dd25841d998788ce",
          "user": {
            "_id": "6493393f357b252af72196c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6493393f357b252af72196c5/EWSy18XRcMRa_4XMM3Fu-.jpeg",
            "isPro": false,
            "fullname": "Gal Chechik",
            "user": "galchechik",
            "type": "user"
          },
          "name": "Gal Chechik",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-04T09:22:13.900Z",
          "hidden": false
        },
        {
          "_id": "68401045dd25841d998788cf",
          "user": {
            "_id": "66810e5877ed01ba880a4b40",
            "avatarUrl": "/avatars/3068f4b16f03a51772e652d76b37f9c3.svg",
            "isPro": false,
            "fullname": "Yftah Ziser",
            "user": "yziser",
            "type": "user"
          },
          "name": "Yftah Ziser",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-04T09:22:13.900Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T08:54:28.000Z",
      "submittedOnDailyAt": "2025-06-04T07:54:00.025Z",
      "title": "Conociendo que: La expresión de un LLM incluye información exitosamente relacionada con las pensamientos y reconocimientos que se completan antes de que la expresión sea completa.",
      "submittedOnDailyBy": {
        "_id": "61fbe8d2c5e6410373a76b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61fbe8d2c5e6410373a76b2a/m4yM6m2uagJxMselzEZfo.jpeg",
        "isPro": false,
        "fullname": "Anum Afzal",
        "user": "anumafzal94",
        "type": "user"
      },
      "summary": "Estamos investigando si es posible predecir antes de que el proceso de 0 dimensiones de la Chain-of-Thought (CoT) se complete. Hemos encontrado que un clasificador de exploración basado en la representación del modelo de lenguaje profundo (LLM) muestra un excelente rendimiento antes de que se generen los tokens, lo que indica que ya en las primeras etapas de la representación se incluye información importante sobre el proceso de razonamiento. Por otro lado, un fuerte baseline basado en BERT depende únicamente de la creación de tokens, mostrando una rendimiento más bajo basado en códigos de lenguaje simple, y no puede mejorar la clasificación incluso con etapas posteriores de razonamiento. Sorprendentemente, no es posible mejorar la clasificación incluso cuando se utilizan etapas posteriores de razonamiento. Si no se proporciona más contexto, las representaciones iniciales son similares a las posteriores, y el modelo de lenguaje profundo muestra que codifica información clave de inicio. Esto significa que, incluso si el proceso de razonamiento se cierra rápidamente, no hay pérdida. Para verificar esto, realizamos experimentos de interrupción inicial, demostrando que el rendimiento se mejora mientras el proceso de razonamiento de la CoT se completa, pero que se dejan errores en comparación con un proceso completo de razonamiento. Sin embargo, el uso de enfoques como entrenamiento de subconjuntos para reducir la CoT o aprendizaje por refuerzo nos permite reconocer si la interrupción inicial es efectiva. Lo que hemos descubierto es que estas aproximaciones pueden ser soportadas y ayudan a optimizar la eficiencia de la CoT mientras mantienen sus beneficios.",
      "upvotes": 1,
      "discussionId": "68401045dd25841d998788f9"
    },
    "publishedAt": "2025-05-30T04:54:28.000Z",
    "title": "Knowing Before Saying: LLM Representations Encode Information About\n  Chain-of-Thought Success Before Completion",
    "summary": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT)\nprocess can be predicted before completion. We discover that a probing\nclassifier, based on LLM representations, performs well even before a\nsingle token is generated, suggesting that crucial information about the\nreasoning process is already present in the initial steps representations. In\ncontrast, a strong BERT-based baseline, which relies solely on the generated\ntokens, performs worse, likely because it depends on shallow linguistic cues\nrather than deeper reasoning dynamics. Surprisingly, using later reasoning\nsteps does not always improve classification. When additional context is\nunhelpful, earlier representations resemble later ones more, suggesting LLMs\nencode key information early. This implies reasoning can often stop early\nwithout loss. To test this, we conduct early stopping experiments, showing that\ntruncating CoT reasoning still improves performance over not using CoT at all,\nthough a gap remains compared to full reasoning. However, approaches like\nsupervised learning or reinforcement learning designed to shorten CoT chains\ncould leverage our classifier's guidance to identify when early stopping is\neffective. Our findings provide insights that may support such methods, helping\nto optimize CoT's efficiency while preserving its benefits.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24362.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61fbe8d2c5e6410373a76b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61fbe8d2c5e6410373a76b2a/m4yM6m2uagJxMselzEZfo.jpeg",
      "fullname": "Anum Afzal",
      "name": "anumafzal94",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24273",
      "authors": [
        {
          "_id": "683f21ed6ba11d78e3e383f6",
          "user": {
            "_id": "65f7c56fc6356b5cc5ab8245",
            "avatarUrl": "/avatars/ff8d3f3526e915f269274cc0dc5ca8ef.svg",
            "isPro": false,
            "fullname": "James Cai",
            "user": "jamescai20",
            "type": "user"
          },
          "name": "Hongyi James Cai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T09:01:32.571Z",
          "hidden": false
        },
        {
          "_id": "683f21ed6ba11d78e3e383f7",
          "name": "Junlin Wang",
          "hidden": false
        },
        {
          "_id": "683f21ed6ba11d78e3e383f8",
          "user": {
            "_id": "65d66cb2b06abf924b07ff76",
            "avatarUrl": "/avatars/de94e2fe07040b7dc3053bcaafa64ffe.svg",
            "isPro": false,
            "fullname": "Xiaoyin Chen",
            "user": "chenyn66",
            "type": "user"
          },
          "name": "Xiaoyin Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T16:25:17.851Z",
          "hidden": false
        },
        {
          "_id": "683f21ed6ba11d78e3e383f9",
          "name": "Bhuwan Dhingra",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T06:49:00.000Z",
      "submittedOnDailyAt": "2025-06-04T08:05:26.080Z",
      "title": "¿Cuál es la cantidad adecuada de riesgo? Investigación sobre la interacción entre la SFT y el RL para mejorar la teoría de los LLM.",
      "submittedOnDailyBy": {
        "_id": "65f7c56fc6356b5cc5ab8245",
        "avatarUrl": "/avatars/ff8d3f3526e915f269274cc0dc5ca8ef.svg",
        "isPro": false,
        "fullname": "James Cai",
        "user": "jamescai20",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los modelos de lenguaje grande (LLMs) ha notablemente mejorado su capacidad para resolver problemas matemáticos y lógicos, siendo especialmente efectivos en la resolución de problemas donde la respuesta es claramente determinada. En particular, se utilizan técnicas como la fine-tuning normal (SFT) y el aprendizaje por refuerzo (RL). Según estudios previos, el aprendizaje por refuerzo incluye estrategias de exploración, permite la inferencia de cadenas largas de razonamiento (CoT) y naturalmente aprende la habilidad de retroceder. Sin embargo, los beneficios concretos de la retrocedencia, especialmente su impacto en la mejora de la inferencia y sus áreas de uso más adecuadas, aún no están completamente comprendidos. En este estudio, se investigaron los cambios dinámicos entre SFT y RL, y se realizaron experimentos en 8 tareas (Countdown, Sudoku, Arc 1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, Self Reference). Los resultados indican que en SFT, secuencias cortas de CoT ofrecen una contribución moderada frente a un RL frío, pero que esta contribución disminuye cuando las tareas se vuelven más difíciles. Basándose en estas observaciones, se construyó un conjunto de datos de entrenamiento con un número de pasos de retrocedencia sistemáticamente ajustado, y se investigó separadamente el impacto de la precisión (contenido) y la estructura (frecuencia de retrocedencia). Los resultados revelan que: (1) las cadenas largas de CoT que incluyen retrocedencia generalmente impulsan un aprendizaje más bueno y estable del RL. (2) Las tareas más difíciles y con mayores espacios de búsqueda tienden a tener un número más alto de pasos de retrocedencia en el proceso de SFT. Además, en experimentos con datos limitados, se observó que el aprendizaje del RL no se vea significativamente afectado por la precisión de las secuencias largas de CoT, y que el RL prioriza la precisión de la estructura en comparación con la precisión del contenido. En resumen, los resultados de este estudio proporcionan consejos prácticos para la diseño de estrategias de entrenamiento más adecuadas para la escalabilidad efectiva de la inferencia en LLMs.",
      "upvotes": 1,
      "discussionId": "683f21ed6ba11d78e3e38434",
      "ai_summary": "This study investigates the interplay between supervised fine-tuning and reinforcement learning in large language models, focusing on the role of backtracking in enhancing reasoning capabilities across various tasks.",
      "ai_keywords": [
        "supervised finetuning",
        "reinforcement learning",
        "chain-of-thought",
        "backtracking",
        "countdown",
        "sudoku",
        "arc 1d",
        "geometry",
        "color cube rotation",
        "list functions",
        "zebra puzzles",
        "self reference",
        "synthetic datasets",
        "distilled data"
      ]
    },
    "publishedAt": "2025-05-30T02:49:00.000Z",
    "title": "How Much Backtracking is Enough? Exploring the Interplay of SFT and RL\n  in Enhancing LLM Reasoning",
    "summary": "Recent breakthroughs in large language models (LLMs) have effectively\nimproved their reasoning abilities, particularly on mathematical and logical\nproblems that have verifiable answers, through techniques such as supervised\nfinetuning (SFT) and reinforcement learning (RL). Prior research indicates that\nRL effectively internalizes search strategies, enabling long chain-of-thought\n(CoT) reasoning, with backtracking emerging naturally as a learned capability.\nHowever, the precise benefits of backtracking, specifically, how significantly\nit contributes to reasoning improvements and the optimal extent of its use,\nremain poorly understood. In this work, we systematically investigate the\ndynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc\n1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self\nReference. Our findings highlight that short CoT sequences used in SFT as a\nwarm-up do have moderate contribution to RL training, compared with cold-start\nRL; however such contribution diminishes when tasks become increasingly\ndifficult. Motivated by this observation, we construct synthetic datasets\nvarying systematically in the number of backtracking steps and conduct\ncontrolled experiments to isolate the influence of either the correctness\n(content) or the structure (i.e., backtrack frequency). We find that (1) longer\nCoT with backtracks generally induce better and more stable RL training, (2)\nmore challenging problems with larger search space tend to need higher numbers\nof backtracks during the SFT stage. Additionally, we demonstrate through\nexperiments on distilled data that RL training is largely unaffected by the\ncorrectness of long CoT sequences, suggesting that RL prioritizes structural\npatterns over content correctness. Collectively, our results offer practical\ninsights into designing optimal training strategies to effectively scale\nreasoning in LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24273.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f7c56fc6356b5cc5ab8245",
      "avatarUrl": "/avatars/ff8d3f3526e915f269274cc0dc5ca8ef.svg",
      "fullname": "James Cai",
      "name": "jamescai20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18079",
      "authors": [
        {
          "_id": "68354eec0830dfc6782ba1c4",
          "name": "Xiaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c5",
          "name": "Zhaoyang Jia",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c6",
          "name": "Zongyu Guo",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c7",
          "name": "Jiahao Li",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c8",
          "name": "Bin Li",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1c9",
          "name": "Houqiang Li",
          "hidden": false
        },
        {
          "_id": "68354eec0830dfc6782ba1ca",
          "name": "Yan Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T16:37:36.000Z",
      "submittedOnDailyAt": "2025-06-04T07:20:42.262Z",
      "title": "Deep Video Search: Agentic Search for Long-form Video including Tool Usage\n\nComprendiendo\n\nEsta traducción se proporciona con profesionalidad y precisión.",
      "submittedOnDailyBy": {
        "_id": "6582b79aafc6b50a2cbaa5c8",
        "avatarUrl": "/avatars/65fa21ca144177f232347084b0e057c5.svg",
        "isPro": false,
        "fullname": "Xiaoyi Zhang",
        "user": "xyzhang626",
        "type": "user"
      },
      "summary": "La comprensión de vídeos largos enfrenta desafíos significativos debido a la complejidad temporal y espacial, así como a la dificultad de procesar contextos a largo plazo para responder a preguntas. Los modelos de lenguaje de gran escala (LLM) que han demostrado avances significativos en análisis de vídeo y procesamiento de contextos a largo plazo presentan límites cuando se trata de procesar vídeos de una duración de 1 hora con alta densidad de información. Para superar estos límites, proponemos el Deep Video Discovery agent. En contraste con los agentes de vídeo anteriores, los cuales se diseñaron con un flujo de trabajo rígido y requieren acciones manuales, nuestro enfoque enfatiza la autonomía del agente. Ofrecemos herramientas centradas en la búsqueda en una base de datos de vídeos de múltiples granularidades, permitiendo que el DVD agente utilice las capacidades de inferencia avanzadas de los LLM para planificar desde las observaciones actuales, seleccionar estratégicamente herramientas, configurar parámetros adecuados de acciones y ajustar de manera iterativa su inferencia interna basándose en la información recopilada. Mediante evaluaciones detalladas en el marco de un benchmark de comprensión de vídeos a largo plazo, demostramos la excelencia de la arquitectura del sistema. El DVD agente logra un desempeño pionero en el conjunto de datos LVBench difícil, superando significativamente los resultados de los estudios previos. Además, proporcionamos pruebas de desaparición y análisis de herramientas para ofrecer retroalimento conectado al desarrollo de agentes inteligentes adecuados para tareas de comprensión de vídeos a largo plazo. El código se publicará posteriormente.",
      "upvotes": 1,
      "discussionId": "68354eed0830dfc6782ba1fe",
      "ai_summary": "The Deep Video Discovery agent uses an autonomous agentic search strategy with large language models to overcome limitations in long-form video understanding, achieving state-of-the-art results on benchmarks like LVBench.",
      "ai_keywords": [
        "Deep Video Discovery agent",
        "agentic search strategy",
        "segmented video clips",
        "multi-granular video database",
        "advanced reasoning capability",
        "LLM",
        "autonomous nature",
        "observation state",
        "search-centric tools",
        "internal reasoning",
        "long video understanding benchmarks",
        "LVBench",
        "ablation studies",
        "tool analyses"
      ]
    },
    "publishedAt": "2025-05-23T12:37:36.000Z",
    "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding",
    "summary": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code will be released later.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6582b79aafc6b50a2cbaa5c8",
      "avatarUrl": "/avatars/65fa21ca144177f232347084b0e057c5.svg",
      "fullname": "Xiaoyi Zhang",
      "name": "xyzhang626",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02138",
      "authors": [
        {
          "_id": "683fe27c4f32bd7bbca087fc",
          "name": "Yarden Bakish",
          "hidden": false
        },
        {
          "_id": "683fe27c4f32bd7bbca087fd",
          "name": "Itamar Zimerman",
          "hidden": false
        },
        {
          "_id": "683fe27c4f32bd7bbca087fe",
          "name": "Hila Chefer",
          "hidden": false
        },
        {
          "_id": "683fe27c4f32bd7bbca087ff",
          "name": "Lior Wolf",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T18:07:55.000Z",
      "submittedOnDailyAt": "2025-06-04T04:51:22.070Z",
      "title": "Substituido LRP: La localización de atributos es un mejoramiento para el defecto insuficiente de explicación de los transformers.",
      "submittedOnDailyBy": {
        "_id": "65376feed325b3f02fb92c69",
        "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
        "isPro": false,
        "fullname": "Itamar Zimerman",
        "user": "ItamarZ",
        "type": "user"
      },
      "summary": "El desarrollo de herramientas de explicabilidad eficientes para Transformers es una tarea importante en la investigación de aprendizaje profundo. Una de las mejores formas de abordar este campo es la Regresión de Riesgos de la Propagación de Pesos de Redes (LRP). La LRP redistribuye los valores activos según reglas predefinidas y retropropaga los puntajes relevantes hacia el espacio de entrada de la red. Sin embargo, los métodos basados en LRP para la explicabilidad de Transformers actualmente ignoran completamente el componente esencial de la arquitectura Transformer, el encoding de posición (PE). Esto provoca la pérdida de conservación y la importancia de las características estructurales y de posición. Para resolver estas limitaciones, se reemplaza la explicabilidad de Transformers por un conjunto de pares de posición y tokens en el espacio de entrada. De esta manera, se proponen reglas teóricas de LRP especializadas que se centran en la propagación de características de diferentes métodos de encoding de posición, como Rotary, Learnable y Absolute PE. Los experimentos extendidos en clasificadores fine-tunados y modelos basados en 0-shot (por ejemplo, LLaMA 3) muestran que nuestro enfoque supera claramente los métodos más avanzados en ambos campos de trabajo, visual y de explicabilidad de NLP. Nuestro código está disponible para uso público.",
      "upvotes": 0,
      "discussionId": "683fe27d4f32bd7bbca08867",
      "githubRepo": "https://github.com/YardenBakish/PE-AWARE-LRP",
      "ai_summary": "A specialized LRP method for Transformer explainability considers positional encoding, improving relevance propagation and outperforming existing methods.",
      "ai_keywords": [
        "Layer-wise Relevance Propagation (LRP)",
        "Transformers",
        "positional encoding (PE)",
        "Rotary",
        "Learnable",
        "Absolute PE",
        "vision",
        "NLP explainability tasks"
      ]
    },
    "publishedAt": "2025-06-02T14:07:55.000Z",
    "title": "Revisiting LRP: Positional Attribution as the Missing Ingredient for\n  Transformer Explainability",
    "summary": "The development of effective explainability tools for Transformers is a\ncrucial pursuit in deep learning research. One of the most promising approaches\nin this domain is Layer-wise Relevance Propagation (LRP), which propagates\nrelevance scores backward through the network to the input space by\nredistributing activation values based on predefined rules. However, existing\nLRP-based methods for Transformer explainability entirely overlook a critical\ncomponent of the Transformer architecture: its positional encoding (PE),\nresulting in violation of the conservation property, and the loss of an\nimportant and unique type of relevance, which is also associated with\nstructural and positional features. To address this limitation, we reformulate\nthe input space for Transformer explainability as a set of position-token\npairs. This allows us to propose specialized theoretically-grounded LRP rules\ndesigned to propagate attributions across various positional encoding methods,\nincluding Rotary, Learnable, and Absolute PE. Extensive experiments with both\nfine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,\ndemonstrate that our method significantly outperforms the state-of-the-art in\nboth vision and NLP explainability tasks. Our code is publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02138.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65376feed325b3f02fb92c69",
      "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
      "fullname": "Itamar Zimerman",
      "name": "ItamarZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01565",
      "authors": [
        {
          "_id": "684009f8a33aeee1125b5765",
          "name": "Li Zhou",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b5766",
          "name": "Lutong Yu",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b5767",
          "name": "Dongchu Xie",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b5768",
          "name": "Shaohuan Cheng",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b5769",
          "name": "Wenyan Li",
          "hidden": false
        },
        {
          "_id": "684009f8a33aeee1125b576a",
          "name": "Haizhou Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T11:43:46.000Z",
      "submittedOnDailyAt": "2025-06-04T07:35:11.966Z",
      "title": "Hanfbarnich: Marcador de época para la comprensión cultural y la traducción de reedición multiforme",
      "submittedOnDailyBy": {
        "_id": "619b506f70d03780cbec5806",
        "avatarUrl": "/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg",
        "isPro": false,
        "fullname": "wenyan li",
        "user": "lyan62",
        "type": "user"
      },
      "summary": "La cultura es un ámbito rico y dinámico que evoluciona en ambas direcciones, territorial y temporal. Sin embargo, la investigación sobre la comprensión cultural basada en Modelos de Visión y Lenguaje (VLMs) actualmente enfatiza principalmente la diversidad territorial, pero deja de lado la importancia temporal. Para remediar esta brecha, presentamos \"Hanbok Bench\", un nuevo conjunto de datos de diversidad editado por expertos. El hanbok es un vestido tradicional que ha transcurrido a través de la antigua China, reflejando su cultura y cultura temporal, y sigue manteniendo una alta popularidad en la sociedad china contemporánea. Hanbok Bench incluye dos tareas clave: la comprensión visual cultural y el redesign de imágenes culturales. La primera tarea evalúa la reconocimiento de características temporales y culturales a partir de una o varias imágenes, y selecciona respuestas según preguntas visuales. La segunda centra su enfoque en adaptar el diseño tradicional de vestidos a contextos modernos, manteniendo los elementos culturales. Nuestra evaluación muestra que los VLMs cerrados presentan un rendimiento relativamente bajo en la comprensión cultural visual, comparados con no expertos. Los VLMs abiertos también presentan un rendimiento inferior a los no expertos. La redesign de tareas ha mostrado un éxito del 42% en modelos que obtuvieron los mejores resultados según evaluaciones humanas diversas. Nuestro benchmark ofrece una prueba crucial para explorar nuevas direcciones en la comprensión cultural temporal y la adaptación creativa, identificando claramente los grandes desafíos en esta área.",
      "upvotes": 0,
      "discussionId": "684009faa33aeee1125b57bd",
      "githubRepo": "https://github.com/lizhou21/TemporalCulture"
    },
    "publishedAt": "2025-06-02T07:43:46.000Z",
    "title": "Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural\n  Understanding and Transcreation",
    "summary": "Culture is a rich and dynamic domain that evolves across both geography and\ntime. However, existing studies on cultural understanding with vision-language\nmodels (VLMs) primarily emphasize geographic diversity, often overlooking the\ncritical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a\nnovel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning\nancient Chinese dynasties, serves as a representative cultural heritage that\nreflects the profound temporal aspects of Chinese culture while remaining\nhighly popular in Chinese contemporary society. Hanfu-Bench comprises two core\ntasks: cultural visual understanding and cultural image transcreation.The\nformer task examines temporal-cultural feature recognition based on single- or\nmulti-image inputs through multiple-choice visual question answering, while the\nlatter focuses on transforming traditional attire into modern designs through\ncultural element inheritance and modern context adaptation. Our evaluation\nshows that closed VLMs perform comparably to non-experts on visual cutural\nunderstanding but fall short by 10\\% to human experts, while open VLMs lags\nfurther behind non-experts. For the transcreation task, multi-faceted human\nevaluation indicates that the best-performing model achieves a success rate of\nonly 42\\%. Our benchmark provides an essential testbed, revealing significant\nchallenges in this new direction of temporal cultural understanding and\ncreative adaptation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01565.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619b506f70d03780cbec5806",
      "avatarUrl": "/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg",
      "fullname": "wenyan li",
      "name": "lyan62",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]