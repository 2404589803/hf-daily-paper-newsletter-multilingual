[
  {
    "paper": {
      "id": "2506.08007",
      "authors": [
        {
          "_id": "684794553ec10bdd8ab4de1a",
          "name": "Qingxiu Dong",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1b",
          "user": {
            "_id": "5df85abada6d0311fd3d5408",
            "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
            "isPro": false,
            "fullname": "Li Dong",
            "user": "unilm",
            "type": "user"
          },
          "name": "Li Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:23.723Z",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1c",
          "user": {
            "_id": "667119d6578448466d9531a6",
            "avatarUrl": "/avatars/72c31909a5584b1306b6404b94a22b2a.svg",
            "isPro": false,
            "fullname": "Yao Tang",
            "user": "YaoTang23",
            "type": "user"
          },
          "name": "Yao Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:20.414Z",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1d",
          "name": "Tianzhu Ye",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1e",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1f",
          "name": "Zhifang Sui",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de20",
          "user": {
            "_id": "67ecd6178647cfa1775f75ed",
            "avatarUrl": "/avatars/98882cc58dc0a5de94df765d523d92c9.svg",
            "isPro": false,
            "fullname": "FW",
            "user": "frontierai",
            "type": "user"
          },
          "name": "Furu Wei",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T02:11:34.050Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/OeDc7c4QFJOxdkJWvdbWB.png"
      ],
      "publishedAt": "2025-06-09T17:59:53.000Z",
      "submittedOnDailyAt": "2025-06-10T00:43:01.816Z",
      "title": "Reinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training\n\nReinforcement Pre-Training",
      "submittedOnDailyBy": {
        "_id": "5df85abada6d0311fd3d5408",
        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
        "isPro": false,
        "fullname": "Li Dong",
        "user": "unilm",
        "type": "user"
      },
      "summary": "En este estudio, se presenta un nuevo paradigma de escalado llamado Reinforcement Pre-Training (RPT) para modelos de lenguaje grandes y aprendizaje por refuerzo (RL). Específicamente, se reconstruye la tarea de etiquetado lógico utilizando el aprendizaje por refuerzo para predecir el siguiente token, y se aplica un método que recibe una recompensa confirmable al predecir el siguiente token con precisión en un contexto existente. El RPT, en contraste con los casos generales, proporciona un método de escalado sin depender de guías específicas en un área, utilizando grandes cantidades de datos de oraciones. Esto permite un gran aumento en la precisión de la predicción del siguiente token. Además, el RPT ofrece una fuerte base de pre-entrenamiento que puede desarrollarse a través de un ajuste de aprendizaje por refuerzo adicional. La curva de escalado muestra cómo la precisión de la predicción del siguiente token se mejora de manera consistente con el aumento del cálculo de aprendizaje. Estos resultados demuestran que el RPT es un paradigma efectivo y deseable para el pre-entrenamiento de modelos de lenguaje.",
      "upvotes": 108,
      "discussionId": "684794553ec10bdd8ab4de21",
      "ai_summary": "Reinforcement Pre-Training (RPT) improves language model accuracy through reinforcement learning and offers a scalable method for leveraging text data for general-purpose RL.",
      "ai_keywords": [
        "Reinforcement Pre-Training (RPT)",
        "next-token prediction",
        "reasoning task",
        "reinforcement learning (RL)",
        "verifiable rewards",
        "language modeling accuracy",
        "reinforcement fine-tuning",
        "scaling curves"
      ]
    },
    "publishedAt": "2025-06-09T13:59:53.000Z",
    "title": "Reinforcement Pre-Training",
    "summary": "In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling\nparadigm for large language models and reinforcement learning (RL).\nSpecifically, we reframe next-token prediction as a reasoning task trained\nusing RL, where it receives verifiable rewards for correctly predicting the\nnext token for a given context. RPT offers a scalable method to leverage vast\namounts of text data for general-purpose RL, rather than relying on\ndomain-specific annotated answers. By incentivizing the capability of\nnext-token reasoning, RPT significantly improves the language modeling accuracy\nof predicting the next tokens. Moreover, RPT provides a strong pre-trained\nfoundation for further reinforcement fine-tuning. The scaling curves show that\nincreased training compute consistently improves the next-token prediction\naccuracy. The results position RPT as an effective and promising scaling\nparadigm to advance language model pre-training.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/OeDc7c4QFJOxdkJWvdbWB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08007.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "5df85abada6d0311fd3d5408",
      "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
      "fullname": "Li Dong",
      "name": "unilm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 28
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07044",
      "authors": [
        {
          "_id": "684795093ec10bdd8ab4de43",
          "name": "LASA Team",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de44",
          "user": {
            "_id": "64118689756b9e455c7eac62",
            "avatarUrl": "/avatars/cdb3da22593facf545a0bafbf548b07e.svg",
            "isPro": false,
            "fullname": "Xu Weiwen",
            "user": "xww033",
            "type": "user"
          },
          "name": "Weiwen Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:07.459Z",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de45",
          "user": {
            "_id": "604f67ef0fe8ff3ec13d71ef",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
            "isPro": false,
            "fullname": "Hou Pong (Ken) Chan",
            "user": "kenchan0226",
            "type": "user"
          },
          "name": "Hou Pong Chan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:05.163Z",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de46",
          "name": "Long Li",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de47",
          "name": "Mahani Aljunied",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de48",
          "name": "Ruifeng Yuan",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de49",
          "user": {
            "_id": "61e09ec13a1781f66b4e9ae2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
            "isPro": false,
            "fullname": "Jianyu Wang",
            "user": "Jianyu",
            "type": "user"
          },
          "name": "Jianyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:03.340Z",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4a",
          "name": "Chenghao Xiao",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4b",
          "name": "Guizhen Chen",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4c",
          "name": "Chaoqun Liu",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4d",
          "name": "Zhaodonghui Li",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4e",
          "name": "Yu Sun",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4f",
          "name": "Junao Shen",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de50",
          "name": "Chaojun Wang",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de51",
          "name": "Jie Tan",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de52",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de53",
          "name": "Tingyang Xu",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de54",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de55",
          "user": {
            "_id": "642eecbf9b2484d7d8526781",
            "avatarUrl": "/avatars/773606f4a37d48861ec4f0f2df8a956f.svg",
            "isPro": false,
            "fullname": "Yu Rong",
            "user": "Swrooy",
            "type": "user"
          },
          "name": "Yu Rong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:01.224Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/604f67ef0fe8ff3ec13d71ef/R3ajyza5JHjd8tOwwV2ht.png"
      ],
      "publishedAt": "2025-06-08T08:47:30.000Z",
      "submittedOnDailyAt": "2025-06-10T00:48:48.080Z",
      "title": "Rings: Modelo generalizado para la integración de un entendimiento médico multi-tipo y lógico",
      "submittedOnDailyBy": {
        "_id": "604f67ef0fe8ff3ec13d71ef",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
        "isPro": false,
        "fullname": "Hou Pong (Ken) Chan",
        "user": "kenchan0226",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje multimodal (MLLMs) muestran una capacidad sorprendente para entender elementos visuales generales. Esto ha sido posible gracias a grandes conjuntos de datos y estrategias de aprendizaje avanzadas. Sin embargo, la efectividad de su aplicación en el ámbito médico está limitada por las diferencias inherentes a los datos y tareas médicas, que son distintas a las de las áreas generales. Específicamente, los MLLMs médicos actuales se mantienen limitados por los siguientes aspectos principales: una amplitud limitada de conocimiento médico además de imágenes médicas, un aumento de confusión debido a procesos inadecuados de preparación de datos y una insuficiencia de capacidades lógicas para situaciones complejas médicas. Para abordar estos problemas, se propone un procedimiento detallado de preparación de datos que incluye: comenzar con imágenes médicas y obtener de manera eficiente conocimiento médico a partir de documentos médicos y datos generales. Se sintetizarán capturas médicas precisas, preguntas visuales y respuestas (VQA) y muestras lógicas. Como resultado, se construirá un conjunto de datos rico en conocimiento médico. Con estos datos preparados, presentamos un nuevo MLLM médico: Lingshu. Lingshu incluye conocimiento médico especializado y mejora su capacidad para resolver tareas de manera evolutiva a través de aprendizaje en etapas. Además, hemos explorado la posibilidad de aprendizaje por refuerzo basado en un paradigma de recompensas provable. También hemos desarrollado MedEvalKit, un marco de evaluación que permite una evaluación estándar, justa y eficiente del modelo, integrando un conjunto de evaluaciones de benchmark médico con datos y texto multimodal. Lingshu ha demostrado superar a los modelos multimodal abiertos actuales, mostrando resultados superiores en tres tareas básicas médicas: QA multimodal, QA basada en texto y generación de informes médicos...",
      "upvotes": 50,
      "discussionId": "684795093ec10bdd8ab4de56",
      "ai_summary": "A medical-specialized multimodal large language model, Lingshu, is introduced with enhanced data curation and reinforcement learning to address limitations in medical applications.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "medical knowledge",
        "hallucinations",
        "data curation",
        "medical texts",
        "general-domain data",
        "accurate medical captions",
        "visual question answering",
        "VQA",
        "reasoning capabilities",
        "multi-stage training",
        "medical expertise",
        "reinforcement learning",
        "verifiable rewards paradigm",
        "MedEvalKit",
        "multimodal QA",
        "text-based QA",
        "medical report generation"
      ]
    },
    "publishedAt": "2025-06-08T04:47:30.000Z",
    "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and Reasoning",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ...",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/604f67ef0fe8ff3ec13d71ef/R3ajyza5JHjd8tOwwV2ht.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07044.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "604f67ef0fe8ff3ec13d71ef",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
      "fullname": "Hou Pong (Ken) Chan",
      "name": "kenchan0226",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07900",
      "authors": [
        {
          "_id": "6847924d3ec10bdd8ab4ddb9",
          "name": "MiniCPM Team",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddba",
          "name": "Chaojun Xiao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbb",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbc",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbd",
          "name": "Yuzhuo Bai",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbe",
          "name": "Jie Cai",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbf",
          "name": "Haotian Chen",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc0",
          "name": "Wentong Chen",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc1",
          "name": "Xin Cong",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc2",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc3",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc4",
          "name": "Shengdan Fan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc5",
          "name": "Yewei Fang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc6",
          "name": "Zixuan Fu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc7",
          "name": "Wenyu Guan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc8",
          "name": "Yitong Guan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc9",
          "name": "Junshao Guo",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddca",
          "name": "Yufeng Han",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddcb",
          "name": "Bingxiang He",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddcc",
          "name": "Yuxiang Huang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddcd",
          "name": "Cunliang Kong",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddce",
          "name": "Qiuzuo Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddcf",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd0",
          "name": "Wenhao Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd1",
          "name": "Yanghao Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd2",
          "name": "Yishan Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd3",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd4",
          "name": "Dan Liu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd5",
          "name": "Biyuan Lin",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd6",
          "name": "Yankai Lin",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd7",
          "name": "Xiang Long",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd8",
          "name": "Quanyu Lu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd9",
          "name": "Yaxi Lu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddda",
          "name": "Peiyan Luo",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dddb",
          "name": "Hongya Lyu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dddc",
          "name": "Litu Ou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dddd",
          "name": "Yinxu Pan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddde",
          "name": "Zekai Qu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dddf",
          "name": "Qundong Shi",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde0",
          "name": "Zijun Song",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde1",
          "name": "Jiayuan Su",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde2",
          "name": "Zhou Su",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde3",
          "name": "Ao Sun",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde4",
          "name": "Xianghui Sun",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde5",
          "name": "Peijun Tang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde6",
          "name": "Fangzheng Wang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde7",
          "name": "Feng Wang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde8",
          "name": "Shuo Wang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde9",
          "user": {
            "_id": "63be286fb3b8c44f8cecc16f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63be286fb3b8c44f8cecc16f/1CIkfEKoTnBYdYDSuQ8AT.jpeg",
            "isPro": false,
            "fullname": "Yudong Wang",
            "user": "BigDong",
            "type": "user"
          },
          "name": "Yudong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:45:33.890Z",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddea",
          "name": "Yesai Wu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddeb",
          "name": "Zhenyu Xiao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddec",
          "name": "Jie Xie",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dded",
          "name": "Zihao Xie",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddee",
          "name": "Yukun Yan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddef",
          "name": "Jiarui Yuan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf0",
          "name": "Kaihuo Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf1",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf2",
          "name": "Linyue Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf3",
          "name": "Xueren Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf4",
          "name": "Yudi Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf5",
          "name": "Hengyu Zhao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf6",
          "name": "Weilin Zhao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf7",
          "name": "Weilun Zhao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf8",
          "name": "Yuanqian Zhao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf9",
          "name": "Zhi Zheng",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfa",
          "name": "Ge Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfb",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfc",
          "name": "Wei Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfd",
          "name": "Zihan Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfe",
          "name": "Zixuan Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddff",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4de00",
          "name": "Guoyang Zeng",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4de01",
          "name": "Chao Jia",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4de02",
          "name": "Dahai Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4de03",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/608f6d72283d0a8d7be9d1f9/NF1aHsqQbJ_Dl18__cn2H.qt"
      ],
      "publishedAt": "2025-06-09T16:16:50.000Z",
      "submittedOnDailyAt": "2025-06-10T00:50:56.021Z",
      "title": "MiniCPM4: Implementación base de terminal eficiente de modelos de lenguaje grande (LLMs)",
      "submittedOnDailyBy": {
        "_id": "608f6d72283d0a8d7be9d1f9",
        "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
        "isPro": false,
        "fullname": "Chaojun XIAO",
        "user": "xcjthu",
        "type": "user"
      },
      "summary": "En este artículo, se presenta MiniCPM4, un modelo de lenguaje de grandes escalas eficiente, como un modelo para dispositivos terminales. Esta eficiencia se ha logrado a través de innovaciones sistemáticas en cuatro elementos principales: arquitectura del modelo, datos de entrenamiento, algoritmos de entrenamiento y sistema de inferencia. En particular, se propone InfLLM v2 en la arquitectura del modelo y se propone una estructura de atención rara y aprendible para acelerar el procesamiento de oraciones largas tanto en el preproceso como en el postproceso. En los datos de entrenamiento, se propone UltraClean y UltraChat v2 para lograr un rendimiento satisfactorio del modelo durante entrenamientos de 800 billones de tokens. En el algoritmo de entrenamiento, se propone ModelTunnel v2 y se implementa un aprendizaje de refuerzo equilibrado junto con algoritmos de entrenamiento eficientes en datos, incluyendo BitCPM, un LLM de 3 minutos con eficiencia en datos. En el sistema de inferencia, se propone CPM.cu, que combina atención rara, cuantización del modelo y procesamiento predecitivo para implementar eficientemente el preproceso y el postproceso. MiniCPM4 está disponible en dos versiones con 0.5B y 8B parámetros, y muestra un rendimiento excelente en múltiples benchmarks de la misma escala que otros modelos abiertos, demostrando claramente su eficiencia y eficacia. En particular, MiniCPM4-8B muestra una notable mejora de velocidad en el procesamiento de secuencias largas frente a Qwen3-8B. Además, MiniCPM4 ha tenido éxito en diversas aplicaciones, demostrando su capacidad para generar investigaciones confiables y utilizar protocolos de contexto del modelo para una amplia gama de usos.",
      "upvotes": 45,
      "discussionId": "6847924e3ec10bdd8ab4de04",
      "projectPage": "https://huggingface.co/collections/openbmb/minicpm4-6841ab29d180257e940baa9b",
      "githubRepo": "https://github.com/openbmb/minicpm",
      "ai_summary": "MiniCPM4, a highly efficient large language model for end-side devices, achieves superior performance using innovations in sparse attention, pre-training datasets, training algorithms, and inference systems.",
      "ai_keywords": [
        "InfLLM v2",
        "sparse attention mechanism",
        "UltraClean",
        "UltraChat v2",
        "prefilling",
        "decoding",
        "long-context processing",
        "ModelTunnel v2",
        "chunk-wise rollout",
        "data-efficient tenary LLM",
        "BitCPM",
        "CPM.cu",
        "model quantization",
        "speculative sampling"
      ]
    },
    "publishedAt": "2025-06-09T12:16:50.000Z",
    "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
    "summary": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Sufficient evaluation results show that MiniCPM4\noutperforms open-source models of similar size across multiple benchmarks,\nhighlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B\ndemonstrates significant speed improvements over Qwen3-8B when processing long\nsequences. Through further adaptation, MiniCPM4 successfully powers diverse\napplications, including trustworthy survey generation and tool use with model\ncontext protocol, clearly showcasing its broad usability.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/608f6d72283d0a8d7be9d1f9/NF1aHsqQbJ_Dl18__cn2H.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "608f6d72283d0a8d7be9d1f9",
      "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
      "fullname": "Chaojun XIAO",
      "name": "xcjthu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06444",
      "authors": [
        {
          "_id": "68479c1e3ec10bdd8ab4de9d",
          "name": "Ruizhong Qiu",
          "hidden": false
        },
        {
          "_id": "68479c1e3ec10bdd8ab4de9e",
          "name": "Gaotang Li",
          "hidden": false
        },
        {
          "_id": "68479c1e3ec10bdd8ab4de9f",
          "name": "Tianxin Wei",
          "hidden": false
        },
        {
          "_id": "68479c1e3ec10bdd8ab4dea0",
          "name": "Jingrui He",
          "hidden": false
        },
        {
          "_id": "68479c1e3ec10bdd8ab4dea1",
          "name": "Hanghang Tong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65370d95019de94263ad34a7/Uhd5i-LpHkPBjdpr4KJLi.jpeg"
      ],
      "publishedAt": "2025-06-06T18:05:45.000Z",
      "submittedOnDailyAt": "2025-06-10T01:29:36.727Z",
      "title": "¡Claro! Aquí tienes la traducción del texto del inglés al español, manteniendo la profesionalidad y precisión:\n\n**Safe-1: Paradigma de escalado inferencial de seguridad de los modelos de lenguaje**\n\nEspero que esto sea útil. Si necesitas más ayuda, no dudes en decírmelo.",
      "submittedOnDailyBy": {
        "_id": "65370d95019de94263ad34a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65370d95019de94263ad34a7/fH9SoJfX7yifXup0DvXkm.jpeg",
        "isPro": false,
        "fullname": "Ruizhong Qiu",
        "user": "q-rz",
        "type": "user"
      },
      "summary": "La investigación actual de seguridad se centra principalmente en el ajuste durante la etapa de aprendizaje, con el objetivo de enseñar a los LLM comportamientos seguros. Sin embargo, recientes estudios han demostrado que estos métodos son vulnerables a diferentes ataques de \"brakes de botella\". Además, la escalabilidad de la inferencia ha mejorado significativamente la capacidad lógica de los LLM, pero su impacto en la seguridad no ha sido estudiado detalladamente. Para abordar estas deficiencias, nuestra investigación desarrolla una nueva escalabilidad de la inferencia para la seguridad de los LLM frente a nuevos riesgos. Hemos demostrado que los métodos de escalabilidad de la inferencia existentes, que funcionan bien en tareas lógicas, presentan bajos rendimientos en contextos seguros y se ven afectados en comparación con el método de sampling Best-of-N. Esta inadecuación se atribuye a los altos overhead computacionales en la evaluación de modelos de recompensa por proceso (PRM), que representan un problema de dilema entre exploración y eficiencia. Para superar este dilema, proponemos el paradigma de escalabilidad de la inferencia SAFFRON. Uno de los elementos clave de este paradigma es la introducción del modelo de recompensa por división (MRM), que reduce significativamente el número de evaluaciones de modelos de recompensa necesarias. Para implementar este paradigma, proponemos tres acciones: (i) la entrenamiento de objetivos de sub-subconjuntos para el MRM, (ii) limitaciones de exploración conservadoras para evitar la exploración de distribuciones, y (iii) una estrategia de caching de clave-valor basada en árboles para promover la comparación de cachos de clave-valor entre secuencias durante la búsqueda de árboles. Las pruebas de extensión han demostrado el efecto de nuestro método. Además, hemos publicado un conjunto de datos de recompensa de seguridad a nivel de token (Safety4M) relacionado con el modelo de división de recompensa entrenado (Saffron-1), con el objetivo de acelerar la investigación sobre la seguridad de los LLM. Nuestro código, modelos y datos están disponibles en https://github.com/q-rz/saffron, y nuestra página web del proyecto está en https://q-rz.github.io/p/saffron.",
      "upvotes": 40,
      "discussionId": "68479c1e3ec10bdd8ab4dea2",
      "projectPage": "https://q-rz.github.io/p/saffron",
      "githubRepo": "https://github.com/q-rz/saffron",
      "ai_summary": "SAFFRON, a novel inference scaling paradigm, enhances LLM safety by reducing reward model evaluations through a multifurcation reward model and other optimizations.",
      "ai_keywords": [
        "LLMs",
        "inference scaling",
        "safety assurance",
        "jailbreak attacks",
        "Best-of-N Sampling",
        "process reward model",
        "exploration--efficiency dilemma",
        "multifurcation reward model",
        "partial supervision training",
        "conservative exploration constraint",
        "Trie-based key--value caching",
        "Safety4M dataset"
      ]
    },
    "publishedAt": "2025-06-06T14:05:45.000Z",
    "title": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance",
    "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65370d95019de94263ad34a7/Uhd5i-LpHkPBjdpr4KJLi.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06444.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65370d95019de94263ad34a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65370d95019de94263ad34a7/fH9SoJfX7yifXup0DvXkm.jpeg",
      "fullname": "Ruizhong Qiu",
      "name": "q-rz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07977",
      "authors": [
        {
          "_id": "684792f03ec10bdd8ab4de06",
          "name": "Jingjing Chang",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de07",
          "user": {
            "_id": "647469b9a51711a3b58bda2b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647469b9a51711a3b58bda2b/yeDf8Sa8IDEQyney1dGC9.jpeg",
            "isPro": false,
            "fullname": "Yixiao Fang",
            "user": "fangyixiao",
            "type": "user"
          },
          "name": "Yixiao Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:54.679Z",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de08",
          "name": "Peng Xing",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de09",
          "name": "Shuhan Wu",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0a",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:45:23.322Z",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0b",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0c",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0d",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0e",
          "name": "Hai-Bao Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/Qq0Ue6mPEkoDJkIMGjRJ0.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ywhobM8HPQHIRy4CRBkLI.jpeg"
      ],
      "publishedAt": "2025-06-09T17:50:21.000Z",
      "submittedOnDailyAt": "2025-06-10T00:52:27.518Z",
      "title": "OneIG-Bench: Evaluación Completa de la Generación de Imágenes",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "Contexto texto se traduce al japonés.",
      "upvotes": 35,
      "discussionId": "684792f03ec10bdd8ab4de0f",
      "projectPage": "https://oneig-bench.github.io/",
      "githubRepo": "https://github.com/OneIG-Bench/OneIG-Benchmark",
      "ai_summary": "OneIG-Bench is a comprehensive benchmark framework for evaluating text-to-image models across multiple dimensions including reasoning, text rendering, and diversity.",
      "ai_keywords": [
        "text-to-image (T2I) models",
        "prompt-image alignment",
        "text rendering precision",
        "reasoning-generated content",
        "stylization",
        "diversity"
      ]
    },
    "publishedAt": "2025-06-09T13:50:21.000Z",
    "title": "OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation",
    "summary": "Text-to-image (T2I) models have garnered significant attention for generating\nhigh-quality images aligned with text prompts. However, rapid T2I model\nadvancements reveal limitations in early benchmarks, lacking comprehensive\nevaluations, for example, the evaluation on reasoning, text rendering and\nstyle. Notably, recent state-of-the-art models, with their rich knowledge\nmodeling capabilities, show promising results on the image generation problems\nrequiring strong reasoning ability, yet existing evaluation systems have not\nadequately addressed this frontier. To systematically address these gaps, we\nintroduce OneIG-Bench, a meticulously designed comprehensive benchmark\nframework for fine-grained evaluation of T2I models across multiple dimensions,\nincluding prompt-image alignment, text rendering precision, reasoning-generated\ncontent, stylization, and diversity. By structuring the evaluation, this\nbenchmark enables in-depth analysis of model performance, helping researchers\nand practitioners pinpoint strengths and bottlenecks in the full pipeline of\nimage generation. Specifically, OneIG-Bench enables flexible evaluation by\nallowing users to focus on a particular evaluation subset. Instead of\ngenerating images for the entire set of prompts, users can generate images only\nfor the prompts associated with the selected dimension and complete the\ncorresponding evaluation accordingly. Our codebase and dataset are now publicly\navailable to facilitate reproducible evaluation studies and cross-model\ncomparisons within the T2I research community.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/Qq0Ue6mPEkoDJkIMGjRJ0.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ywhobM8HPQHIRy4CRBkLI.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07491",
      "authors": [
        {
          "_id": "684799083ec10bdd8ab4de8a",
          "user": {
            "_id": "63efbb1efc92a63ac81126d0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676655314726-noauth.jpeg",
            "isPro": true,
            "fullname": "Yongsen Mao",
            "user": "ysmao",
            "type": "user"
          },
          "name": "Yongsen Mao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:08.018Z",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8b",
          "name": "Junhao Zhong",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8c",
          "name": "Chuan Fang",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8d",
          "user": {
            "_id": "6437c0ead38ce48bdd4b0067",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6437c0ead38ce48bdd4b0067/9HdcjbD0ugiPypLDtQl8E.png",
            "isPro": false,
            "fullname": "Jia Zheng",
            "user": "bertjiazheng",
            "type": "user"
          },
          "name": "Jia Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:11.939Z",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8e",
          "name": "Rui Tang",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8f",
          "name": "Hao Zhu",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de90",
          "name": "Ping Tan",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de91",
          "name": "Zihan Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6437c0ead38ce48bdd4b0067/0ppuxf3I81w8mWqDgei0W.mp4"
      ],
      "publishedAt": "2025-06-09T07:10:58.000Z",
      "submittedOnDailyAt": "2025-06-10T01:04:13.223Z",
      "title": "Specetrum LM: Entrenamiento de grandes modelos de lenguaje para modelado estructurado de interiores",
      "submittedOnDailyBy": {
        "_id": "6437c0ead38ce48bdd4b0067",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6437c0ead38ce48bdd4b0067/9HdcjbD0ugiPypLDtQl8E.png",
        "isPro": false,
        "fullname": "Jia Zheng",
        "user": "bertjiazheng",
        "type": "user"
      },
      "summary": "SpatialLM es un gran modelo de lenguaje que procesa datos de puntos 3D y genera salidas para comprender espacios 3D estructurados. Estas salidas incluyen elementos arquitectónicos como paredes, puertas y ventanas, así como categorías significativas. Diferente de los métodos anteriores, SpatialLM no utiliza redes especializadas para tareas, sino que se ajusta directamente a una arquitectura de modelos de lenguaje de código abierto, a través de un ajuste fino.\n\nPara el entrenamiento de SpatialLM, se recopilaron 12,328 espacios interiores (54,778 habitaciones) de alta calidad de datos sintéticos, y se realizaron estudios detallados sobre modelos y decisiones de entrenamiento. En los marcos de referencia públicos, nuestro modelo muestra la mejor rendimiento en estimación espacial simultánea y detección de objetos 3D, lo que demuestra la posibilidad de mejorar la comprensión espacial de los modernos modelos de lenguaje de imágenes y robotes en aplicaciones como expansión de imágenes y robots concretos.",
      "upvotes": 22,
      "discussionId": "684799083ec10bdd8ab4de92",
      "projectPage": "https://manycore-research.github.io/SpatialLM",
      "githubRepo": "https://github.com/manycore-research/SpatialLM/",
      "ai_summary": "SpatialLM, a multimodal large language model, processes 3D point cloud data to generate structured scene understanding outputs, achieving state-of-the-art performance in layout estimation and competitive results in 3D object detection.",
      "ai_keywords": [
        "large language model",
        "3D point cloud",
        "structured 3D scene understanding",
        "multimodal LLM",
        "fine-tuning",
        "synthetic dataset",
        "ground-truth 3D annotations",
        "layout estimation",
        "3D object detection",
        "augmented reality",
        "embodied robotics"
      ]
    },
    "publishedAt": "2025-06-09T03:10:58.000Z",
    "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling",
    "summary": "SpatialLM is a large language model designed to process 3D point cloud data\nand generate structured 3D scene understanding outputs. These outputs include\narchitectural elements like walls, doors, windows, and oriented object boxes\nwith their semantic categories. Unlike previous methods which exploit\ntask-specific network designs, our model adheres to the standard multimodal LLM\narchitecture and is fine-tuned directly from open-source LLMs.\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset\nconsisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with\nground-truth 3D annotations, and conduct a careful study on various modeling\nand training decisions. On public benchmarks, our model gives state-of-the-art\nperformance in layout estimation and competitive results in 3D object\ndetection. With that, we show a feasible path for enhancing the spatial\nunderstanding capabilities of modern LLMs for applications in augmented\nreality, embodied robotics, and more.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6437c0ead38ce48bdd4b0067/0ppuxf3I81w8mWqDgei0W.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07491.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6437c0ead38ce48bdd4b0067",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6437c0ead38ce48bdd4b0067/9HdcjbD0ugiPypLDtQl8E.png",
      "fullname": "Jia Zheng",
      "name": "bertjiazheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07986",
      "authors": [
        {
          "_id": "68479b0f3ec10bdd8ab4de94",
          "name": "Zhengyao Lv",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de95",
          "name": "Tianlin Pan",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de96",
          "user": {
            "_id": "635f8ed47c05eb9f59963d3a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg",
            "isPro": false,
            "fullname": "ChenyangSi",
            "user": "ChenyangSi",
            "type": "user"
          },
          "name": "Chenyang Si",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:05.835Z",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de97",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de98",
          "name": "Wangmeng Zuo",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de99",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de9a",
          "name": "Kwan-Yee K. Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:54:04.000Z",
      "submittedOnDailyAt": "2025-06-10T01:15:51.787Z",
      "title": "El nuevo enfoque de la interacción entre los diversos modelos de inteligencia artificial",
      "submittedOnDailyBy": {
        "_id": "645aff5121ab438e732c47c1",
        "avatarUrl": "/avatars/23b2a853139b0f2ae1fa88e2bd4e0056.svg",
        "isPro": false,
        "fullname": "Zhengyao Lv",
        "user": "cszy98",
        "type": "user"
      },
      "summary": "MM-DiTs ha logrado un desarrollo sorprendente en la generación visual basada en texto. Sin embargo, modelos recientes como FLUX experimentan dificultades para lograr una precisa correspondencia entre los prompts de texto y el contenido generado. Se identificaron dos problemas importantes en la estructura de atención de MM-DiT: 1) la inhibición de la atención cruzada modal debido al desbalance entre tokens de imagen y texto, y 2) la falta de pesos de atención en función de los tiempos, los cuales obstaculizan la correspondencia. Para resolver estos problemas, se propone la temperatura-regulada atención cruzada modal (TACA). Esta metodología es eficiente en parámetros y utiliza escalado de temperatura y ajustes dependientes del tiempo para reorganizar la intersección multimodal dinámicamente. La combinación con fine-tuning de LoRA mejora significativamente la correspondencia texto-imagen en el T2I-CompBench benchmark con un mínimo de sobrecarga computacional. Se verificó TACA en modelos recientes como FLUX y SD3.5, demostrando mejoras en la correspondencia contextual de imagen con respecto a la apariencia de objetos, la combinación de características y las relaciones espaciales. Nuestros hallazgos subrayan la importancia del equilibrio de la atención cruzada modal para mejorar la fidelidad significativa en modelos de difusión de imagenes a partir de texto. Nuestro código está disponible en https://github.com/Vchitect/TACA.",
      "upvotes": 11,
      "discussionId": "68479b0f3ec10bdd8ab4de9b",
      "projectPage": "https://vchitect.github.io/TACA/",
      "githubRepo": "https://github.com/Vchitect/TACA",
      "ai_summary": "Temperature-Adjusted Cross-modal Attention (TACA) enhances text-image alignment in diffusion models by dynamically rebalancing multimodal interactions through temperature scaling and timestep-dependent adjustment.",
      "ai_keywords": [
        "Temperature-Adjusted Cross-modal Attention",
        "TACA",
        "multimodal interactions",
        "temperature scaling",
        "timestep-dependent adjustment",
        "FLUX",
        "SD3.5",
        "T2I-CompBench",
        "semantic fidelity",
        "text-to-image diffusion models"
      ]
    },
    "publishedAt": "2025-06-09T13:54:04.000Z",
    "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers",
    "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose Temperature-Adjusted Cross-modal Attention\n(TACA), a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\nhttps://github.com/Vchitect/TACA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07986.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645aff5121ab438e732c47c1",
      "avatarUrl": "/avatars/23b2a853139b0f2ae1fa88e2bd4e0056.svg",
      "fullname": "Zhengyao Lv",
      "name": "cszy98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07553",
      "authors": [
        {
          "_id": "684794a43ec10bdd8ab4de24",
          "user": {
            "_id": "65eaa07cb6c760d77468b4b6",
            "avatarUrl": "/avatars/4a1aae58986b40444351e0a167ca807c.svg",
            "isPro": false,
            "fullname": "Jingchao Wang",
            "user": "jcwang0602",
            "type": "user"
          },
          "name": "Jingchao Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:15.857Z",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de25",
          "user": {
            "_id": "65fd45473ccf43503350d837",
            "avatarUrl": "/avatars/11b9679945b3c89b142f0d62a312f362.svg",
            "isPro": false,
            "fullname": "Haote Yang",
            "user": "Hoter",
            "type": "user"
          },
          "name": "Haote Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:18.060Z",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de26",
          "name": "Jiang Wu",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de27",
          "name": "Yifan He",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de28",
          "name": "Xingjian Wei",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de29",
          "name": "Yinfan Wang",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2a",
          "name": "Chengjin Liu",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2b",
          "name": "Lingli Ge",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2c",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2d",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2e",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2f",
          "name": "Conghui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T08:47:10.000Z",
      "submittedOnDailyAt": "2025-06-10T00:49:58.326Z",
      "title": "GTR-CoT: Trayecto de Exploración de Grafos de Scout Foray Microkernel\nReconocimiento de Estructuras",
      "submittedOnDailyBy": {
        "_id": "65fd45473ccf43503350d837",
        "avatarUrl": "/avatars/11b9679945b3c89b142f0d62a312f362.svg",
        "isPro": false,
        "fullname": "Haote Yang",
        "user": "Hoter",
        "type": "user"
      },
      "summary": "La Reconocimiento de Estructuras Químicas (OCSR) desempeña un papel crucial en la digitalización de la conocimiento químico, transformando imágenes de moléculas en formatos que puedan ser leídas por máquina. Los modelos de lenguaje visual y de lenguaje (VLMs) recientes han abordado estos problemas, pero la captura de imágenes de estructuras complejas y descripciones inestables a menudo presentan desafíos. Para enfrentar estos desafíos, presentamos el GTR-Mol-VLM. Este modelo caracteriza dos innovaciones principales: 1) una estructura de Visual Coshot Escape que imita la teoría utilizada por los humanos para predecir secuencialmente las uniónes de átomos y analizar gradualmente la estructura de moléculas. 2) un enfoque centrado en datos para resolver la desigualdad entre las estructuras simples de las imágenes y las descripciones extendidas. Para el desarrollo del modelo, construimos el GTR-CoT-1.3M. Este conjunto de datos contiene un gran número de conjuntos de entrenamiento de instrucciones con descripciones modificadas minuciosamente. Además, presentamos MolRec-Bench, el primer benchmark para evaluar la precisión de la análisis de grafos en OCSR. Los experimentos detallados muestran que el GTR-Mol-VLM obtiene resultados superiores a los modelos especializados, los VLMs en química y los VLMs generales comerciales. En particular, para imágenes de moléculas que incluyen estructuras simples de grupos funcionales, el GTR-Mol-VLM mejora en aproximadamente un 14% en puntuaciones de SMILE y métricas basadas en grafos. Esperamos que esta investigación contribuya a satisfacer la necesidad real de la tecnología OCSR y fomente el desarrollo de la informática química y el AI para las ciencias. Publicamos el GTR-CoT.",
      "upvotes": 11,
      "discussionId": "684794a43ec10bdd8ab4de30",
      "ai_summary": "GTR-Mol-VLM, featuring graph traversal and data-centric principles, outperforms existing models in Optical Chemical Structure Recognition by accurately parsing molecular graphs and handling abbreviated structures.",
      "ai_keywords": [
        "Graph Traversal as Visual Chain of Thought",
        "Faithfully Recognize What You've Seen",
        "GTR-CoT-1.3M",
        "MolRec-Bench",
        "graph-parsing accuracy",
        "Optical Chemical Structure Recognition",
        "VLMs",
        "SMILES-based",
        "graph-based metrics"
      ]
    },
    "publishedAt": "2025-06-09T04:47:10.000Z",
    "title": "GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular\n  Structure Recognition",
    "summary": "Optical Chemical Structure Recognition (OCSR) is crucial for digitizing\nchemical knowledge by converting molecular images into machine-readable\nformats. While recent vision-language models (VLMs) have shown potential in\nthis task, their image-captioning approach often struggles with complex\nmolecular structures and inconsistent annotations. To overcome these\nchallenges, we introduce GTR-Mol-VLM, a novel framework featuring two key\ninnovations: (1) the Graph Traversal as Visual Chain of Thought\nmechanism that emulates human reasoning by incrementally parsing molecular\ngraphs through sequential atom-bond predictions, and (2) the data-centric\nprinciple of Faithfully Recognize What You've Seen, which addresses\nthe mismatch between abbreviated structures in images and their expanded\nannotations. To support model development, we constructed GTR-CoT-1.3M, a\nlarge-scale instruction-tuning dataset with meticulously corrected annotations,\nand introduced MolRec-Bench, the first benchmark designed for a fine-grained\nevaluation of graph-parsing accuracy in OCSR. Comprehensive experiments\ndemonstrate that GTR-Mol-VLM achieves superior results compared to specialist\nmodels, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in\nscenarios involving molecular images with functional group abbreviations,\nGTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage\npoints, both in SMILES-based and graph-based metrics. We hope that this work\nwill drive OCSR technology to more effectively meet real-world needs, thereby\nadvancing the fields of cheminformatics and AI for Science. We will release\nGTR-CoT at https://github.com/opendatalab/GTR-CoT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07553.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd45473ccf43503350d837",
      "avatarUrl": "/avatars/11b9679945b3c89b142f0d62a312f362.svg",
      "fullname": "Haote Yang",
      "name": "Hoter",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07712",
      "authors": [
        {
          "_id": "684790cd3ec10bdd8ab4ddaa",
          "user": {
            "_id": "66dfb6bac93721c02f75f37e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zzoX0tpkHCNsFGjtnATSt.png",
            "isPro": false,
            "fullname": "Renjie",
            "user": "RogerLos",
            "type": "user"
          },
          "name": "Renjie Luo",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T01:56:30.026Z",
          "hidden": false
        },
        {
          "_id": "684790cd3ec10bdd8ab4ddab",
          "name": "Jiaxi Li",
          "hidden": false
        },
        {
          "_id": "684790cd3ec10bdd8ab4ddac",
          "user": {
            "_id": "65d7b983baa72790a1151923",
            "avatarUrl": "/avatars/938531e84ca01a0c5a2a174057e3e9c5.svg",
            "isPro": false,
            "fullname": "Chen Huang",
            "user": "Albus-Chen",
            "type": "user"
          },
          "name": "Chen Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T09:19:43.446Z",
          "hidden": false
        },
        {
          "_id": "684790cd3ec10bdd8ab4ddad",
          "name": "Wei Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T12:56:41.000Z",
      "submittedOnDailyAt": "2025-06-10T00:30:07.286Z",
      "title": "Paseando por el barco: El camino para la entrenamiento efectivo de largo contexto en modelos de lenguaje pequeños",
      "submittedOnDailyBy": {
        "_id": "66dfb6bac93721c02f75f37e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zzoX0tpkHCNsFGjtnATSt.png",
        "isPro": false,
        "fullname": "Renjie",
        "user": "RogerLos",
        "type": "user"
      },
      "summary": "El control de CoT largo (Long Concept) se utiliza frecuentemente para fortalecer la lógica de los modelos de lenguaje. Es efectivo para grandes modelos, pero cuando se utilizan datos de CoT largo limitados para entrenar pequeños modelos de lenguaje (SLMs; menos de 3B parámetros), se observa un significativo descenso en el rendimiento lógico, lo que se denomina \"descenso de CoT largo\". Este fenómeno se ha observado en amplias pruebas con familias como Qwen2.5, LLaMA3 y Gemma3. En este contexto, los modelos entrenados con ejemplos de CoT largo de 8k pierden aproximadamente el 25% de su rendimiento antes del ajuste. Además, para pequeños modelos, incluso entrenando ejemplos de 220k de CoT largo no es posible recuperar o superar el rendimiento antes del ajuste. Este fenómeno se analiza como la acumulación de errores, ya que mientras los respuestas largas fortalecen la función de lógica en cada paso, el riesgo de repetir errores aumenta. Además, el descenso de CoT largo también tiene un impacto negativo en la entrenamiento de refuerzo (RL) en la etapa posterior. Se ha confirmado que este fenómeno puede ser mitigado con ajustes de microcontrolado suficientemente grandes (SFT). Estos hallazgos dudan sobre la asunción general sobre los beneficios de entrenar SLMs con CoT largo y proporcionan una guía práctica para la construcción de modelos lógicos de pequeño tamaño.",
      "upvotes": 10,
      "discussionId": "684790cd3ec10bdd8ab4ddae",
      "ai_summary": "Small language models experience significant performance declines when trained on long chain-of-thought data due to error accumulation, impacting downstream reinforcement learning but potentially mitigated by extensive supervised fine-tuning.",
      "ai_keywords": [
        "Long chain-of-thought",
        "Long CoT Degradation",
        "small language models",
        "SLMs",
        "Qwen2.5",
        "LLaMA3",
        "Gemma3",
        "error accumulation",
        "supervised fine-tuning",
        "SFT",
        "reinforcement learning"
      ]
    },
    "publishedAt": "2025-06-09T08:56:41.000Z",
    "title": "Through the Valley: Path to Effective Long CoT Training for Small\n  Language Models",
    "summary": "Long chain-of-thought (CoT) supervision has become a common strategy to\nenhance reasoning in language models. While effective for large models, we\nidentify a phenomenon we call Long CoT Degradation, in which small language\nmodels (SLMs; <=3B parameters) trained on limited long CoT data experience\nsignificant performance deterioration. Through extensive experiments on the\nQwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is\nwidespread across SLMs. In some settings, models trained on only 8k long CoT\nexamples lose up to 75% of their original performance before fine-tuning.\nStrikingly, we further observe that for some particularly small models, even\ntraining on 220k long CoT examples fails to recover or surpass their original\nperformance prior to fine-tuning. Our analysis attributes this effect to error\naccumulation: while longer responses increase the capacity for multi-step\nreasoning, they also amplify the risk of compounding mistakes. Furthermore, we\nfind that Long CoT Degradation may negatively impacts downstream reinforcement\nlearning (RL), although this can be alleviated by sufficiently scaled\nsupervised fine-tuning (SFT). Our findings challenge common assumptions about\nthe benefits of long CoT training for SLMs and offer practical guidance for\nbuilding more effective small-scale reasoning models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07712.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66dfb6bac93721c02f75f37e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zzoX0tpkHCNsFGjtnATSt.png",
      "fullname": "Renjie",
      "name": "RogerLos",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07530",
      "authors": [
        {
          "_id": "68478dae3ec10bdd8ab4dd9b",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "68478dae3ec10bdd8ab4dd9c",
          "name": "Chuyan Xiong",
          "hidden": false
        },
        {
          "_id": "68478dae3ec10bdd8ab4dd9d",
          "name": "Ruiping Wang",
          "hidden": false
        },
        {
          "_id": "68478dae3ec10bdd8ab4dd9e",
          "name": "Xilin Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T08:15:11.000Z",
      "submittedOnDailyAt": "2025-06-10T00:14:07.356Z",
      "title": "BitVLA: Formula de Manipulación de Modelos de Acciones de Larga Distancia de Visión 1 Bit",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "El modelo de acción VLA (Vision Language Action) muestra capacidades impresionantes para diferentes tareas complejas de la operación automática. Sin embargo, el aumento del tamaño del modelo ha convertidose en un problema importante para sistemas de máquinas con limitaciones de recursos. Se ha demostrado que la predicción de 1 bit es efectiva para aumentar la eficiencia de inferencia en modelos de lenguaje grandes sin perder significativamente de rendimiento, pero su aplicación a modelos VLA aún ha sido poco investigada. En este estudio, presentamos el primer modelo VLA de 1 bit para operación automática, llamado \"BitVLA\". En este modelo, todos los parámetros se componen de tres valores: {-1, 0, 1}. Además, proponemos una estrategia de entrenamiento para reducir la memoria del encoder visual mediante el proceso de distillación. En este proceso, el encoder completo puede ajustar mejor las representaciones potenciales del modelo objetivo. Por lo tanto, BitVLA, en lugar de predecir con bits de más de 4, alcanza un rendimiento comparable a modelos líderes como OpenVLA-OFT en el benchmark LIBERO, utilizando solo el 29.8% de la memoria. Estos resultados demuestran que BitVLA puede mejorar la posibilidad de procesamiento en dispositivos de borde con limitaciones de memoria. Los códigos y pesos del modelo están disponibles en GitHub: https://github.com/ustcwhy/BitVLA.",
      "upvotes": 9,
      "discussionId": "68478dae3ec10bdd8ab4dd9f",
      "githubRepo": "https://github.com/ustcwhy/BitVLA",
      "ai_summary": "BitVLA, a 1-bit VLA model with ternary parameters, achieves comparable performance to OpenVLA-OFT on LIBERO while using 29.8% less memory through distillation-aware training.",
      "ai_keywords": [
        "VLA models",
        "1-bit pretraining",
        "ternary parameters",
        "distillation-aware training",
        "vision encoder",
        "full-precision encoder",
        "latent representations",
        "memory footprint",
        "robotics manipulation",
        "OpenVLA-OFT",
        "LIBERO benchmark",
        "memory-constrained edge devices"
      ]
    },
    "publishedAt": "2025-06-09T04:15:11.000Z",
    "title": "BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation",
    "summary": "Vision-Language-Action (VLA) models have shown impressive capabilities across\na wide range of robotics manipulation tasks. However, their growing model size\nposes significant challenges for deployment on resource-constrained robotic\nsystems. While 1-bit pretraining has proven effective for enhancing the\ninference efficiency of large language models with minimal performance loss,\nits application to VLA models remains underexplored. In this work, we present\nBitVLA, the first 1-bit VLA model for robotics manipulation, in which every\nparameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint\nof the vision encoder, we propose the distillation-aware training strategy that\ncompresses the full-precision encoder to 1.58-bit weights. During this process,\na full-precision encoder serves as a teacher model to better align latent\nrepresentations. Despite the lack of large-scale robotics pretraining, BitVLA\nachieves performance comparable to the state-of-the-art model OpenVLA-OFT with\n4-bit post-training quantization on the LIBERO benchmark, while consuming only\n29.8% of the memory. These results highlight BitVLA's promise for deployment on\nmemory-constrained edge devices. We release the code and model weights in\nhttps://github.com/ustcwhy/BitVLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07530.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07298",
      "authors": [
        {
          "_id": "684795e83ec10bdd8ab4de6a",
          "user": {
            "_id": "62de9e6fdcdc9043efa8b756",
            "avatarUrl": "/avatars/c26974c740633d143f7382f0858ea99a.svg",
            "isPro": false,
            "fullname": "Yijia Dai",
            "user": "DaiYijia",
            "type": "user"
          },
          "name": "Yijia Dai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:53.562Z",
          "hidden": false
        },
        {
          "_id": "684795e83ec10bdd8ab4de6b",
          "name": "Zhaolin Gao",
          "hidden": false
        },
        {
          "_id": "684795e83ec10bdd8ab4de6c",
          "name": "Yahya Satter",
          "hidden": false
        },
        {
          "_id": "684795e83ec10bdd8ab4de6d",
          "user": {
            "_id": "664f92095a60ca2484b90d7a",
            "avatarUrl": "/avatars/3232bb702ed479ac821b7a5dfb457d0b.svg",
            "isPro": false,
            "fullname": "Sarah Dean",
            "user": "sarahdean",
            "type": "user"
          },
          "name": "Sarah Dean",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T02:18:19.596Z",
          "hidden": false
        },
        {
          "_id": "684795e83ec10bdd8ab4de6e",
          "name": "Jennifer J. Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-08T21:49:38.000Z",
      "submittedOnDailyAt": "2025-06-10T01:00:39.516Z",
      "title": "Los modelos de lenguaje de gran escala de aprendizaje predictivo aprenden el modelo de Markov oculto en el contexto.",
      "submittedOnDailyBy": {
        "_id": "652eec0aabc673c4204c459e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652eec0aabc673c4204c459e/9otSQFP8G3S8zarR1Y5rE.jpeg",
        "isPro": false,
        "fullname": "Zhaolin Gao",
        "user": "GitBag",
        "type": "user"
      },
      "summary": "Los modelos de Markov ocultos (HMMs) son herramientas básicas para modelar datos de secuencia, pero se encuentran complicados computacionalmente para adaptarse a los datos del mundo real. En este artículo, mostramos que modelos de lenguaje de gran escala (LLMs) previamente entrenados tienen la capacidad de inferir patrones a partir de ejemplos en el prompt y pueden modelar efectivamente los datos generados por HMMs. En un conjunto de HMMs sintéticos, los LLMs alcanzan una precisión de predicción cercana a la óptima teórica. Se descubren nuevos tendencias de escalado en función de las características de los HMMs y se proporcionan predicciones teóricas sobre estas observaciones experimentales. Además, proporcionamos guías prácticas para científicos para utilizar el aprendizaje en contexto (ICL) como herramienta de diagnóstico. En tareas deterministas de animales del mundo real, el ICL logra un rendimiento competitivo con modelos diseñados por expertos humanos. Según nuestra información, esto es un ejemplo que muestra por primera vez que el ICL puede aprender secuencias generadas por HMMs para hacer predicciones, estableciendo así la posible potencial de LLMs para entender el aprendizaje en contexto y desarrollar fuertes herramientas para el estudio de estructuras ocultas en datos científicos complejos.",
      "upvotes": 8,
      "discussionId": "684795e93ec10bdd8ab4de6f",
      "githubRepo": "https://github.com/DaiYijia02/icl-hmm",
      "ai_summary": "In-context learning in large language models can effectively model sequences generated by hidden Markov models, achieving predictive accuracy and uncovering scaling trends, thus demonstrating its potential as a diagnostic tool for complex scientific data.",
      "ai_keywords": [
        "hidden Markov models",
        "HMMs",
        "large language models",
        "LLMs",
        "in-context learning",
        "IC",
        "predictive accuracy",
        "theoretical optimum",
        "synthetic HMMs",
        "scaling trends",
        "empirical observations",
        "animal decision-making tasks",
        "human experts"
      ]
    },
    "publishedAt": "2025-06-08T17:49:38.000Z",
    "title": "Pre-trained Large Language Models Learn Hidden Markov Models In-context",
    "summary": "Hidden Markov Models (HMMs) are foundational tools for modeling sequential\ndata with latent Markovian structure, yet fitting them to real-world data\nremains computationally challenging. In this work, we show that pre-trained\nlarge language models (LLMs) can effectively model data generated by HMMs via\nin-context learning (ICL)x2013their ability to infer patterns from\nexamples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve\npredictive accuracy approaching the theoretical optimum. We uncover novel\nscaling trends influenced by HMM properties, and offer theoretical conjectures\nfor these empirical observations. We also provide practical guidelines for\nscientists on using ICL as a diagnostic tool for complex data. On real-world\nanimal decision-making tasks, ICL achieves competitive performance with models\ndesigned by human experts. To our knowledge, this is the first demonstration\nthat ICL can learn and predict HMM-generated sequencesx2013an\nadvance that deepens our understanding of in-context learning in LLMs and\nestablishes its potential as a powerful tool for uncovering hidden structure in\ncomplex scientific data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07298.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "652eec0aabc673c4204c459e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652eec0aabc673c4204c459e/9otSQFP8G3S8zarR1Y5rE.jpeg",
      "fullname": "Zhaolin Gao",
      "name": "GitBag",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07463",
      "authors": [
        {
          "_id": "68478a493ec10bdd8ab4dd90",
          "user": {
            "_id": "632c234f42c386ebd2710434",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg",
            "isPro": false,
            "fullname": "Guang Liu",
            "user": "ZacLiu",
            "type": "user"
          },
          "name": "Guang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T09:19:47.891Z",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd91",
          "user": {
            "_id": "63a11ce02fabbbb899a01d58",
            "avatarUrl": "/avatars/ee3d4088b6d32b2c18b8be91913e90dd.svg",
            "isPro": false,
            "fullname": "ldwang",
            "user": "ldwang",
            "type": "user"
          },
          "name": "Liangdong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T09:19:50.398Z",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd92",
          "name": "Jijie Li",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd93",
          "name": "Yang Yu",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd94",
          "name": "Yao Xu",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd95",
          "name": "Jiabei Chen",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd96",
          "name": "Yu Bai",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd97",
          "name": "Feng Liao",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd98",
          "user": {
            "_id": "629aa3155ab4232a3fe0893e",
            "avatarUrl": "/avatars/cf2d4a9295b5da9e2e4d2278bbb36040.svg",
            "isPro": false,
            "fullname": "Yonghua Lin",
            "user": "Yonghua",
            "type": "user"
          },
          "name": "Yonghua Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-10T09:39:44.976Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T06:14:19.000Z",
      "submittedOnDailyAt": "2025-06-10T01:15:00.564Z",
      "title": "CCI4.0: Fortalecimiento de la teoría del lenguaje en modelos de lenguaje grande mediante entrenamiento preliminar de bibliotecas",
      "submittedOnDailyBy": {
        "_id": "632c234f42c386ebd2710434",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg",
        "isPro": false,
        "fullname": "Guang Liu",
        "user": "ZacLiu",
        "type": "user"
      },
      "summary": "CCI4.0 se presenta. Es un conjunto de datos de predicción y edición de dos lenguajes de gran escala desarrollado para lograr una calidad de datos urgente y las diversas tendencias lógicas del ser humano. CCI4.0 ocupa aproximadamente 35 TB de espacio en disco y está constituido por dos subconjuntos de datos: CCI4.0-M2-Base y CCI4.0-M2-CoT. CCI4.0-M2-Base integra un conjunto de datos en inglés de 22.5 TB de Nemotron-CC, así como datos en matemáticas, Wiki, arXiv y códigos de diversas fuentes, totalizando 5.2 TB. Estos datos se proporcionan principalmente desde conjuntos de datos procesados, aunque los estándares de calidad en cada área varían dinámicamente y requieren la experiencia y labor de varios expertos. Por lo tanto, se propone un nuevo flujo de trabajo para discutir la calidad de los datos basado en modelos. Este flujo de trabajo se realiza a través de dos etapas de eliminación, un escore de calidad para un clasificador multiclase y filtrado fluyente por dominio. Se extraen 4.5 billones de páginas de templates de CoT (Chain-of-Thought) y se nombra CCI4.0-M2-CoT. Esto es diferente a la mejora de CoT en modelos grandes y muestra una reducción significativa en la posibilidad de hallucinación. En evaluaciones experimentales, los LLMs entrenados en CCI4.0 reciben señales de entrenamiento relativamente confiables, especialmente en tareas de matemáticas y código, mostrando una mejora consistente. Nuestros resultados subrayan la importancia de la edición estricta de datos y los templates de pensamiento humanos, y desempeñan un papel crucial en el mejoramiento de los LLMs, así como brindan varias luz al procesamiento automático de corpus previamente entrenados.",
      "upvotes": 7,
      "discussionId": "68478a493ec10bdd8ab4dd99",
      "projectPage": "https://openseek.baai.ac.cn/",
      "githubRepo": "https://github.com/FlagAI-Open/OpenSeek",
      "ai_summary": "A large-scale bilingual pre-training dataset, CCI4.0, enhances data quality and diverse reasoning patterns for language models, leading to improved performance in downstream tasks like math and code reflection.",
      "ai_keywords": [
        "pre-training dataset",
        "bilingual pre-training",
        "data quality",
        "reasoning trajectory",
        "deduplication",
        "multiclassifier quality scoring",
        "domain-aware fluency filtering",
        "Chain-of-Thought",
        "CoT extraction",
        "language models",
        "LLMs",
        "downstream tasks",
        "math tasks",
        "code reflection tasks",
        "data curation",
        "human thinking templates"
      ]
    },
    "publishedAt": "2025-06-09T02:14:19.000Z",
    "title": "CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large\n  Language Models",
    "summary": "We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered\nfor superior data quality and diverse human-like reasoning trajectory. CCI4.0\noccupies roughly 35 TB of disk space and comprises two sub-datasets:\nCCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a 5.2 TB carefully\ncurated Chinese web corpus, a 22.5 TB English subset from Nemotron-CC, and\ndiverse sources from math, wiki, arxiv, and code. Although these data are\nmostly sourced from well-processed datasets, the quality standards of various\ndomains are dynamic and require extensive expert experience and labor to\nprocess. So, we propose a novel pipeline justifying data quality mainly based\non models through two-stage deduplication, multiclassifier quality scoring, and\ndomain-aware fluency filtering. We extract 4.5 billion pieces of\nCoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the\ndistillation of CoT from larger models, our proposed staged CoT extraction\nexemplifies diverse reasoning patterns and significantly decreases the\npossibility of hallucination. Empirical evaluations demonstrate that LLMs\npre-trained in CCI4.0 benefit from cleaner, more reliable training signals,\nyielding consistent improvements in downstream tasks, especially in math and\ncode reflection tasks. Our results underscore the critical role of rigorous\ndata curation and human thinking templates in advancing LLM performance,\nshedding some light on automatically processing pretraining corpora.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07463.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632c234f42c386ebd2710434",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg",
      "fullname": "Guang Liu",
      "name": "ZacLiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07434",
      "authors": [
        {
          "_id": "684797f33ec10bdd8ab4de7a",
          "user": {
            "_id": "6447ca6ca478b20f1755b294",
            "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
            "isPro": false,
            "fullname": "Feifan Song",
            "user": "songff",
            "type": "user"
          },
          "name": "Feifan Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:29.497Z",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7b",
          "user": {
            "_id": "67244a81aa8556c561925ab6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/w-vZ0uwYACagrNq-H1oyO.jpeg",
            "isPro": false,
            "fullname": "Shaohang Wei",
            "user": "SylvainWei",
            "type": "user"
          },
          "name": "Shaohang Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:27.498Z",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7c",
          "name": "Wen Luo",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7d",
          "name": "Yuxuan Fan",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7e",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7f",
          "name": "Guoyin Wang",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de80",
          "name": "Houfeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T05:21:22.000Z",
      "submittedOnDailyAt": "2025-06-10T00:59:10.518Z",
      "title": "Comienza con una pequeña mitad fácil y termina: ajuste de la orientación de bajos recursos mediante la interpretación de la transición de un punto débil a un punto fuerte.",
      "submittedOnDailyBy": {
        "_id": "6447ca6ca478b20f1755b294",
        "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
        "isPro": false,
        "fullname": "Feifan Song",
        "user": "songff",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) necesitan evitar crear contenido que no se ajusta a las preocupaciones humanas, que sea inicial, inadecuado o sin sentido. Recientemente, se han desarrollado métodos para responder a estos modelos con bajo recursos, pero obtener contenido de alta calidad y correspondiente sigue siendo un desafío. Después de observar que la dificultad de la respuesta se centra en el inicio de la interpretación, proponemos un nuevo marco de trabajo \"Decoding from Weak to Strong (WSD)\" para mejorar la capacidad de respuesta. Un pequeño modelo de respuesta descarta la parte inicial correspondiente antes de que el modelo base genere la respuesta, y luego se controla automáticamente mediante una estructura de switch similar a lo que hace un programa de maquina. Además, hemos colectado un nuevo conjunto de datos llamado \"GenerAlign\" y fine-tunamos un pequeño modelo de 700-3B para usarlo como modelo de descarte. Este modelo mejora eficazmente el modelo base bajo el marco de trabajo WSD y muestra un rendimiento superior a todos los métodos básicos, evitando así pérdidas en tareas de descarga. Además, analizamos el impacto de diferentes configuraciones y la eficiencia en tiempo, así como la estructura interna de WSD en detalle.",
      "upvotes": 7,
      "discussionId": "684797f33ec10bdd8ab4de81",
      "githubRepo": "https://github.com/F2-Song/Weak-to-Strong-Decoding",
      "ai_summary": "A new decoding framework (Weak-to-Strong Decoding, WSD) enhances the alignment of large language models by using a small aligned model to draft responses, followed by the base model, with a design to prevent degradation in performance on downstream tasks.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "LLM alignment",
        "human preferences",
        "low-resource methods",
        "decoding",
        "small aligned model",
        "auto-switch mechanism",
        "GenerAlign",
        "Pilot-3B",
        "draft model",
        "alignment tax",
        "intrinsic mechanisms"
      ]
    },
    "publishedAt": "2025-06-09T01:21:22.000Z",
    "title": "Well Begun is Half Done: Low-resource Preference Alignment by\n  Weak-to-Strong Decoding",
    "summary": "Large Language Models (LLMs) require alignment with human preferences to\navoid generating offensive, false, or meaningless content. Recently,\nlow-resource methods for LLM alignment have been popular, while still facing\nchallenges in obtaining both high-quality and aligned content. Motivated by the\nobservation that the difficulty of generating aligned responses is concentrated\nat the beginning of decoding, we propose a novel framework, Weak-to-Strong\nDecoding (WSD), to enhance the alignment ability of base models by the guidance\nof a small aligned model. The small model first drafts well-aligned beginnings,\nfollowed by the large base model to continue the rest, controlled by a\nwell-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,\nto fine-tune a small-sized Pilot-3B as the draft model, which effectively\nenhances different base models under the WSD framework to outperform all\nbaseline methods, while avoiding degradation on downstream tasks, termed as the\nalignment tax. Extensive experiments are further conducted to examine the\nimpact of different settings and time efficiency, as well as analyses on the\nintrinsic mechanisms of WSD in depth.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07434.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6447ca6ca478b20f1755b294",
      "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
      "fullname": "Feifan Song",
      "name": "songff",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06941",
      "authors": [
        {
          "_id": "684797863ec10bdd8ab4de72",
          "user": {
            "_id": "6520621836008ecc88699622",
            "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
            "isPro": false,
            "fullname": "Parshin Shojaee",
            "user": "parshinsh",
            "type": "user"
          },
          "name": "Parshin Shojaee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:32.697Z",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de73",
          "name": "Iman Mirzadeh",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de74",
          "name": "Keivan Alizadeh",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de75",
          "name": "Maxwell Horton",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de76",
          "name": "Samy Bengio",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de77",
          "name": "Mehrdad Farajtabar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-07T22:42:29.000Z",
      "submittedOnDailyAt": "2025-06-10T01:00:21.500Z",
      "title": "「Contexto de la Memoria: Fortalezas y Límites de Modelos Teóricos que Consideran Problemas Complejos」",
      "submittedOnDailyBy": {
        "_id": "6520621836008ecc88699622",
        "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
        "isPro": false,
        "fullname": "Parshin Shojaee",
        "user": "parshinsh",
        "type": "user"
      },
      "summary": "Recientemente, los modelos de lenguaje han introducido modelos de grande razonamiento (LRMs) para generar procesos de pensamiento específicos antes de proporcionar una respuesta. Estos modelos han demostrado mejoras en los marcos de referencia de fundamentos lógicos, pero sus capacidades básicas, características de escalado y limitaciones no han sido suficientemente comprendidas. Actualmente, las evaluaciones se centran principalmente en marcos de referencia de matemáticas y programación, priorizando la precisión de la respuesta final. Sin embargo, este paradigma de evaluación no puede entender los fundamentos lógicos, por lo que pueden estar contaminados. En este estudio, se investigaron estos defectos estructuralmente, y se inspiró en entornos de puzzles controlables que permiten manipular la complejidad estructuralmente, para investigar los defectos del paradigma de evaluación. Este entorno permite analizar no solo la respuesta final sino también los fundamentos lógicos internos, y entender cómo los LRMs piensan. A través de experimentos extendidos, se demostró que los LRMs pierden completamente su rendimiento cuando superan cierta complejidad. Además, se observaron límites no estrictos de escalado: el esfuerzo lógico aumenta a un nivel constante con la complejidad de la tarea, pero el resto de los buckets de tokens no aumenta. Comparando los LRMs con los modelos de lenguaje estándar utilizando la misma computación de inferencia, se identificaron tres direcciones de rendimiento: (1) en tareas de baja complejidad, los modelos estándar superan a los LRMs, (2) en tareas de media complejidad, los LRMs tienen una ventaja, y (3) en tareas de alta complejidad, ambos modelos pierden completamente. Los LRMs se ven restringidos en cálculos precisos y muestran comportamientos inadecuados en escalado, sin un algoritmo claro. Además, se investigó más profundamente los fundamentos lógicos, se estudiaron patrones de soluciones encontradas y se analizaron los comportamientos de cálculo del modelo, revelando sus fortalezas y limitaciones y planteando dudas sobre su capacidad.",
      "upvotes": 7,
      "discussionId": "684797863ec10bdd8ab4de78",
      "ai_summary": "Large Reasoning Models (LRMs) exhibit varying performance across task complexities, with limitations in exact computation and inconsistent reasoning, as assessed using controllable puzzle environments.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRMs",
        "controllable puzzle environments",
        "reasoning traces",
        "standard LLMs",
        "performance regimes",
        "exact computation",
        "reasoning capabilities"
      ]
    },
    "publishedAt": "2025-06-07T18:42:29.000Z",
    "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of\n  Reasoning Models via the Lens of Problem Complexity",
    "summary": "Recent generations of language models have introduced Large Reasoning Models\n(LRMs) that generate detailed thinking processes before providing answers.\nWhile these models demonstrate improved performance on reasoning benchmarks,\ntheir fundamental capabilities, scaling properties, and limitations remain\ninsufficiently understood. Current evaluations primarily focus on established\nmath and coding benchmarks, emphasizing final answer accuracy. However, this\nevaluation paradigm often suffers from contamination and does not provide\ninsights into the reasoning traces. In this work, we systematically investigate\nthese gaps with the help of controllable puzzle environments that allow precise\nmanipulation of complexity while maintaining consistent logical structures.\nThis setup enables the analysis of not only final answers but also the internal\nreasoning traces, offering insights into how LRMs think. Through extensive\nexperiments, we show that LRMs face a complete accuracy collapse beyond certain\ncomplexities. Moreover, they exhibit a counterintuitive scaling limit: their\nreasoning effort increases with problem complexity up to a point, then declines\ndespite having remaining token budget. By comparing LRMs with their standard\nLLM counterparts under same inference compute, we identify three performance\nregimes: (1) low-complexity tasks where standard models outperform LRMs, (2)\nmedium-complexity tasks where LRMs demonstrates advantage, and (3)\nhigh-complexity tasks where both models face complete collapse. We found that\nLRMs have limitations in exact computation: they fail to use explicit\nalgorithms and reason inconsistently across scales. We also investigate the\nreasoning traces in more depth, studying the patterns of explored solutions and\nanalyzing the models' computational behavior, shedding light on their\nstrengths, limitations, and raising questions about their reasoning\ncapabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6520621836008ecc88699622",
      "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
      "fullname": "Parshin Shojaee",
      "name": "parshinsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06205",
      "authors": [
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf4",
          "user": {
            "_id": "66727b038171db46e7f4f242",
            "avatarUrl": "/avatars/3f8ab656006533fe1a24dd48c9b0a1b6.svg",
            "isPro": false,
            "fullname": "sc",
            "user": "sc-bd",
            "type": "user"
          },
          "name": "Sheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T09:21:08.452Z",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf5",
          "name": "Peiyu He",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf6",
          "name": "Jiaxin Hu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf7",
          "name": "Ziyang Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf8",
          "name": "Yansheng Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf9",
          "name": "Tao Xu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfa",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfb",
          "name": "Chongchong Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfc",
          "name": "Chao An",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfd",
          "name": "Shiyu Cai",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfe",
          "name": "Duo Cao",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbff",
          "name": "Kangping Chen",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc00",
          "name": "Shuai Chu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc01",
          "name": "Tianwei Chu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc02",
          "name": "Mingdi Dan",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc03",
          "name": "Min Du",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc04",
          "name": "Weiwei Fang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc05",
          "name": "Pengyou Fu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc06",
          "name": "Junkai Hu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc07",
          "name": "Xiaowei Jiang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc08",
          "name": "Zhaodi Jiang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc09",
          "name": "Fuxuan Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0a",
          "name": "Jun Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0b",
          "name": "Minghui Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0c",
          "name": "Mingyao Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0d",
          "name": "Yanchang Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0e",
          "name": "Zhibin Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0f",
          "name": "Guangming Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc10",
          "name": "Kairui Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc11",
          "name": "Lihao Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc12",
          "name": "Weizhi Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc13",
          "name": "Xiaoshun Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc14",
          "name": "Yufei Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc15",
          "name": "Yunfei Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc16",
          "name": "Qiang Lu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc17",
          "name": "Yuanfei Luo",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc18",
          "name": "Xiang Lv",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc19",
          "name": "Hongying Ma",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1a",
          "name": "Sai Ma",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1b",
          "name": "Lingxian Mi",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1c",
          "name": "Sha Sa",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1d",
          "name": "Hongxiang Shu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1e",
          "name": "Lei Tian",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1f",
          "name": "Chengzhi Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc20",
          "name": "Jiayu Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc21",
          "name": "Kaijie Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc22",
          "name": "Qingyi Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc23",
          "name": "Renwen Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc24",
          "name": "Tao Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc25",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc26",
          "name": "Xirui Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc27",
          "name": "Chao Wei",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc28",
          "name": "Xuguang Wei",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc29",
          "name": "Zijun Xia",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2a",
          "name": "Zhaohao Xiao",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2b",
          "name": "Tingshuai Yan",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2c",
          "name": "Liyan Yang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2d",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2e",
          "name": "Zhikai Yang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2f",
          "name": "Zhong Yin",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc30",
          "name": "Li Yuan",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc31",
          "name": "Liuchun Yuan",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc32",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc33",
          "name": "Jinyang Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc34",
          "name": "Junhui Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc35",
          "name": "Linge Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc36",
          "name": "Zhenyi Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc37",
          "name": "Zheyu Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc38",
          "name": "Dongjie Zhu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc39",
          "name": "Hang Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc3a",
          "name": "Yangang Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66727b038171db46e7f4f242/nrwuIsn9tQaR75nAuym-R.mp4"
      ],
      "publishedAt": "2025-06-06T16:08:47.000Z",
      "submittedOnDailyAt": "2025-06-10T07:53:05.305Z",
      "title": "Astra: \"Receta de Hoyo para el Aprendizaje Multimodal de Modo General en Robotas Móviles de Uso Común\"",
      "submittedOnDailyBy": {
        "_id": "66727b038171db46e7f4f242",
        "avatarUrl": "/avatars/3f8ab656006533fe1a24dd48c9b0a1b6.svg",
        "isPro": false,
        "fullname": "sc",
        "user": "sc-bd",
        "type": "user"
      },
      "summary": "El sistema de Navegación de robots modernos plantea diversos problemas en entornos interiores complejos y variados. Los métodos tradicionales se basan en múltiples módulos con pequeños modelos o en sistemas basados en reglas, lo que les impide adaptarse a nuevos entornos. En respuesta a esto, hemos desarrollado una arquitectura funcional de doble modelo para la Navegación de robots móviles, denominada Astra-Global y Astra-Local. Astra-Global procesa entradas visuales y lingüísticas utilizando un modelo de lenguaje grande multimodal, y determina la posición del robot y su destino utilizando un grafo temático-semántico mixto, superando así los métodos tradicionales de reconocimiento de lugares visuales. Astra-Local es una red neuronal capaz de procesar varias tareas, manejando la planificación de pasos locales y la medición de odometría. El encoder de espacio-tiempo 4D se entrena mediante observación propia, generando características fuertes en 4D para tareas posteriores. El cabezal de planificación utiliza el método de matching de flujos y una pérdida de ESDF con mascara para minimizar el riesgo de colisión y generar proyectos locales. El cabezal de odometría integra diversos entradas de sensores a través de un encoder de canales, predeciendo la posición relativa del robot. La implementación de Astra en el interior de Minas Reales logra un alto porcentaje de éxito en misiones en diversos entornos interiores.",
      "upvotes": 7,
      "discussionId": "6846ca9b3ec10bdd8ab4dc3b",
      "ai_summary": "Astra, a dual-model architecture for mobile robot navigation, uses a multimodal LLM for global localization and a multitask network for local path planning and odometry estimation, achieving high success rates in diverse indoor environments.",
      "ai_keywords": [
        "LLM",
        "self and goal localization",
        "hybrid topological-semantic graph",
        "multimodal LLM",
        "multitask network",
        "4D spatial-temporal encoder",
        "self-supervised learning",
        "4D features",
        "flow matching",
        "masked ESDF loss",
        "local trajectories",
        "transformer encoder",
        "relative pose prediction"
      ]
    },
    "publishedAt": "2025-06-06T12:08:47.000Z",
    "title": "Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal\n  Learning",
    "summary": "Modern robot navigation systems encounter difficulties in diverse and complex\nindoor environments. Traditional approaches rely on multiple modules with small\nmodels or rule-based systems and thus lack adaptability to new environments. To\naddress this, we developed Astra, a comprehensive dual-model architecture,\nAstra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a\nmultimodal LLM, processes vision and language inputs to perform self and goal\nlocalization using a hybrid topological-semantic graph as the global map, and\noutperforms traditional visual place recognition methods. Astra-Local, a\nmultitask network, handles local path planning and odometry estimation. Its 4D\nspatial-temporal encoder, trained through self-supervised learning, generates\nrobust 4D features for downstream tasks. The planning head utilizes flow\nmatching and a novel masked ESDF loss to minimize collision risks for\ngenerating local trajectories, and the odometry head integrates multi-sensor\ninputs via a transformer encoder to predict the relative pose of the robot.\nDeployed on real in-house mobile robots, Astra achieves high end-to-end mission\nsuccess rate across diverse indoor environments.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66727b038171db46e7f4f242/nrwuIsn9tQaR75nAuym-R.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06205.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66727b038171db46e7f4f242",
      "avatarUrl": "/avatars/3f8ab656006533fe1a24dd48c9b0a1b6.svg",
      "fullname": "sc",
      "name": "sc-bd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08012",
      "authors": [
        {
          "_id": "684791b63ec10bdd8ab4ddb1",
          "name": "Penghao Wu",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb2",
          "name": "Shengnan Ma",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb3",
          "name": "Bo Wang",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb4",
          "name": "Jiaheng Yu",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb5",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb6",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:59:57.000Z",
      "submittedOnDailyAt": "2025-06-10T00:35:58.769Z",
      "title": "GUI-Reflection: Utilidad Gráfica de Auto-Reflexión\nGUI-Reflection: Modelo de Utilidad Gráfica fortalecido por Auto-Reflexión\nGUI-Reflection: Creación de un modelo de GUI multimodal con auto-reflexión\nGUI-Reflection: Fortalecimiento del modelo de utilidad gráfica mediante Auto-Reflexión de Behavior",
      "submittedOnDailyBy": {
        "_id": "64101f81b27543634e377fc1",
        "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
        "isPro": false,
        "fullname": "Penghao Wu",
        "user": "craigwu",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) demuestran una posibilidad innovadora en la automatización de interfaces gráficas (GUI). Sin embargo, los modelos actuales de GUI se han entrenado principalmente en trajes offline con casi nula error, lo que ha limitado su capacidad para la rapidez y corrección de errores. Para remediar esto, proponemos un nuevo marco de trabajo llamado GUI-Reflection. Este marco establece tres etapas: entrenamiento previo de la GUI, ajuste de sub-subreducción en línea (SFT) y entrenamiento de reflexión en línea, lo que permite a los modelos de GUI adquirir claramente la capacidad de automatizar la reflexión y la corrección de errores.\n\nGUI-Reflection utiliza un proceso de generación y aprendizaje de datos completamente automatizados para impulsar la ocurrencia de acciones de reflexión automatizadas. Específicamente, 1) proponemos una escalable pipeline de datos para automaticamente construir datos de reflexión y corrección de errores a partir de los trajes de éxito actuales. Los modelos de GUI actuales se centran principalmente en la base gráfica y la comprensión de la interfaz de usuario, por lo que proponemos el GUI-Reflection Task Suite para enseñar y evaluar claramente las habilidades orientadas hacia la reflexión. 2) Además, hemos construido diversos ambientes eficientes para el entrenamiento en línea y la recopilación de datos. 3) Utilizando estos ambientes, proponemos un algoritmo de entrenamiento de reflexión en línea iterativo, lo que permite a los modelos continuamente mejorar su capacidad para la reflexión y la corrección de errores. Nuestro marco de trabajo otorga a los agentes de GUI la capacidad de reflexión automática y corrección de errores, abriéndole camino para una automatización de GUI más robusta, adaptable y inteligente. Estamos preparados para publicar de manera gratuita todos los datos, modelos, ambientes y herramientas.",
      "upvotes": 6,
      "discussionId": "684791b63ec10bdd8ab4ddb7",
      "projectPage": "https://penghao-wu.github.io/GUI_Reflection/",
      "githubRepo": "https://github.com/penghao-wu/GUI_Reflection",
      "ai_summary": "GUI-Reflection enhances GUI automation by integrating self-reflection and error correction through scalable data pipelines and an iterative online tuning framework.",
      "ai_keywords": [
        "multimodal large language models",
        "graphical user interface",
        "GUI automation",
        "self-reflection",
        "error correction",
        "GUI-specific pre-training",
        "supervised fine-tuning",
        "online reflection tuning",
        "reflection-oriented abilities",
        "iterative online reflection tuning algorithm"
      ]
    },
    "publishedAt": "2025-06-09T13:59:57.000Z",
    "title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection\n  Behavior",
    "summary": "Multimodal Large Language Models (MLLMs) have shown great potential in\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\nGUI models mostly rely on learning from nearly error-free offline trajectories,\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\npropose GUI-Reflection, a novel framework that explicitly integrates\nself-reflection and error correction capabilities into end-to-end multimodal\nGUI models throughout dedicated training stages: GUI-specific pre-training,\noffline supervised fine-tuning (SFT), and online reflection tuning.\nGUI-reflection enables self-reflection behavior emergence with fully automated\ndata generation and learning processes without requiring any human annotation.\nSpecifically, 1) we first propose scalable data pipelines to automatically\nconstruct reflection and error correction data from existing successful\ntrajectories. While existing GUI models mainly focus on grounding and UI\nunderstanding ability, we propose the GUI-Reflection Task Suite to learn and\nevaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a\ndiverse and efficient environment for online training and data collection of\nGUI models on mobile devices. 3) We also present an iterative online reflection\ntuning algorithm leveraging the proposed environment, enabling the model to\ncontinuously enhance its reflection and error correction abilities. Our\nframework equips GUI agents with self-reflection and correction capabilities,\npaving the way for more robust, adaptable, and intelligent GUI automation, with\nall data, models, environments, and tools to be released publicly.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64101f81b27543634e377fc1",
      "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
      "fullname": "Penghao Wu",
      "name": "craigwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07309",
      "authors": [
        {
          "_id": "6847b8793ec10bdd8ab4df4f",
          "user": {
            "_id": "67f42bd98752b56bd349a9db",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_1jLlx7qqMDYb2ylZ2TCa.png",
            "isPro": false,
            "fullname": "Yin Huang",
            "user": "MaggieHuang",
            "type": "user"
          },
          "name": "Yin Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T04:45:46.729Z",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df50",
          "name": "Yifan Ethan Xu",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df51",
          "name": "Kai Sun",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df52",
          "name": "Vera Yan",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df53",
          "name": "Alicia Sun",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df54",
          "name": "Haidar Khan",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df55",
          "name": "Jimmy Nguyen",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df56",
          "name": "Mohammad Kachuee",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df57",
          "name": "Zhaojiang Lin",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df58",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df59",
          "name": "Aaron Colak",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df5a",
          "name": "Anuj Kumar",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df5b",
          "name": "Wen-tau Yih",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df5c",
          "name": "Xin Luna Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-08T22:51:46.000Z",
      "submittedOnDailyAt": "2025-06-10T03:17:46.849Z",
      "title": "ConfQA: Responder solo lo que estás seguro de ser cierto.",
      "submittedOnDailyBy": {
        "_id": "67f42bd98752b56bd349a9db",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_1jLlx7qqMDYb2ylZ2TCa.png",
        "isPro": false,
        "fullname": "Yin Huang",
        "user": "MaggieHuang",
        "type": "user"
      },
      "summary": "LLM se pregunta si puede evitar la sesión de las cinturas con explicaciones verdaderas. En este artículo, se propone un paso de ajuste llamado ConfQA, que reduce el porcentaje de sesión de las cinturas en 20-40% a menos de 5% en múltiples marcos de prueba de verdad. La idea clave es sencilla: se debe entrenar al LLM para que continúe aprendiendo respuestas precisas y reconozca con \"no estoy seguro\" cuando no es así. Sin embargo, dos elementos principales contribuyen a un alto efecto de entrenamiento. Primero, se agrega un patrón de respuesta \"responder solo cuando está seguro\", lo que mantiene la sesión de las cinturas en un rango de 15-25% si no se incluye. Segundo, se utilizan explicaciones factuales sencillas, especialmente los valores de propiedades en grafos de conocimiento para ajustar la confianza del LLM y lograr una fuerte generalización según el dominio y el tipo de pregunta. Desde esta perspectiva, se propone un Dual Neural Knowledge framework para elegir un sistema de red neuronal paramétrico interno basado en confianza y un sistema externo de campos de signos registrados. Este framework permite mejorar la precisión en un 30% o más, al mismo tiempo que aumenta la precisión en un 95% o más.",
      "upvotes": 6,
      "discussionId": "6847b87a3ec10bdd8ab4df5d",
      "ai_summary": "ConfQA fine-tuning strategy reduces factual statement hallucination in LLMs by 80%, using a dampening prompt and factual statements from knowledge graphs to improve confidence calibration and knowledge selection.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "fine-tuning",
        "ConfQA",
        "hallucination",
        "factuality benchmarks",
        "dampening prompt",
        "factual statements",
        "knowledge graphs",
        "confidence calibration",
        "Dual Neural Knowledge framework",
        "neural knowledge",
        "symbolic knowledge",
        "accuracy gains",
        "external retrievals"
      ]
    },
    "publishedAt": "2025-06-08T18:51:46.000Z",
    "title": "ConfQA: Answer Only If You Are Confident",
    "summary": "Can we teach Large Language Models (LLMs) to refrain from hallucinating\nfactual statements? In this paper we present a fine-tuning strategy that we\ncall ConfQA, which can reduce hallucination rate from 20-40% to under 5% across\nmultiple factuality benchmarks. The core idea is simple: when the LLM answers a\nquestion correctly, it is trained to continue with the answer; otherwise, it is\ntrained to admit \"I am unsure\". But there are two key factors that make the\ntraining highly effective. First, we introduce a dampening prompt \"answer only\nif you are confident\" to explicitly guide the behavior, without which\nhallucination remains high as 15%-25%. Second, we leverage simple factual\nstatements, specifically attribute values from knowledge graphs, to help LLMs\ncalibrate the confidence, resulting in robust generalization across domains and\nquestion types. Building on this insight, we propose the Dual Neural Knowledge\nframework, which seamlessly select between internally parameterized neural\nknowledge and externally recorded symbolic knowledge based on ConfQA's\nconfidence. The framework enables potential accuracy gains to beyond 95%, while\nreducing unnecessary external retrievals by over 30%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07309.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67f42bd98752b56bd349a9db",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_1jLlx7qqMDYb2ylZ2TCa.png",
      "fullname": "Yin Huang",
      "name": "MaggieHuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08010",
      "authors": [
        {
          "_id": "6847ad3b3ec10bdd8ab4df06",
          "name": "Nick Jiang",
          "hidden": false
        },
        {
          "_id": "6847ad3b3ec10bdd8ab4df07",
          "name": "Amil Dravid",
          "hidden": false
        },
        {
          "_id": "6847ad3b3ec10bdd8ab4df08",
          "name": "Alexei Efros",
          "hidden": false
        },
        {
          "_id": "6847ad3b3ec10bdd8ab4df09",
          "name": "Yossi Gandelsman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:59:57.000Z",
      "submittedOnDailyAt": "2025-06-10T04:42:01.785Z",
      "title": "Visiones Transformers No Necesitan Registros Entrenados",
      "submittedOnDailyBy": {
        "_id": "6398d9d168e3392256aaf952",
        "avatarUrl": "/avatars/062363a9b5ebb26603c543a7fc3ee4ec.svg",
        "isPro": false,
        "fullname": "Nick",
        "user": "nickjiang",
        "type": "user"
      },
      "summary": "Estamos investigando los mecanismos de fenómenos que anteriormente se han reconocido. Este fenómeno consiste en la generación de mapas de atención que agregan ruido a los tokens de gran escala. Hemos observado en diversos modelos (como CLIP, DINOv2) que algunas neuronas escasas se centran en la activación de alta escala en los tokens de salida, generando patrones de atención discontinuos y deteriorando el procesamiento visual posterior. Actualmente, la solución es reentrenar el modelo utilizando registradores adicionales para eliminar estos salidas de salida. Sin embargo, basados en nuestros hallazgos, proponemos una aproximación sin entrenamiento adicional para reproducir el efecto de los registradores. Movemos la activación de gran escala de las neuronas registradoras encontradas sin entrenamiento adicional, y creamos el efecto de los registradores como si el modelo se entrenara sin utilizarlos. Nuestro método genera mapas de atención y mapas de características, mejora el rendimiento del modelo base en varias tareas visuales, y realiza comparaciones con modelos entrenados explícitamente con registradores. En segundo lugar, extendemos el registrador de tiempo de prueba a un modelo de lenguaje de visión de tiempo de prueba, y mejoramos su interpretabilidad. Nuestros resultados muestran que el registrador de tiempo de prueba realiza el mismo papel que los registradores en el tiempo de prueba, y proporciona una solución sin limitaciones de entrenamiento, independientemente de si el modelo de predicción incluye registradores o no.",
      "upvotes": 5,
      "discussionId": "6847ad3c3ec10bdd8ab4df0a",
      "ai_summary": "A training-free method shifts high-norm activations in Vision Transformers to an untrained token, enhancing attention maps and performance across visual tasks, and improving interpretability in vision-language models.",
      "ai_keywords": [
        "Vision Transformers",
        "high-norm tokens",
        "noisy attention maps",
        "activations",
        "neurons",
        "irregular attention patterns",
        "downstream visual processing",
        "register tokens",
        "feature maps",
        "vision-language models",
        "interpretability",
        "test-time registers"
      ]
    },
    "publishedAt": "2025-06-09T13:59:57.000Z",
    "title": "Vision Transformers Don't Need Trained Registers",
    "summary": "We investigate the mechanism underlying a previously identified phenomenon in\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\nsparse set of neurons is responsible for concentrating high-norm activations on\noutlier tokens, leading to irregular attention patterns and degrading\ndownstream visual processing. While the existing solution for removing these\noutliers involves retraining models from scratch with additional learned\nregister tokens, we use our findings to create a training-free approach to\nmitigate these artifacts. By shifting the high-norm activations from our\ndiscovered register neurons into an additional untrained token, we can mimic\nthe effect of register tokens on a model already trained without registers. We\ndemonstrate that our method produces cleaner attention and feature maps,\nenhances performance over base models across multiple downstream visual tasks,\nand achieves results comparable to models explicitly trained with register\ntokens. We then extend test-time registers to off-the-shelf vision-language\nmodels to improve their interpretability. Our results suggest that test-time\nregisters effectively take on the role of register tokens at test-time,\noffering a training-free solution for any pre-trained model released without\nthem.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08010.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6398d9d168e3392256aaf952",
      "avatarUrl": "/avatars/062363a9b5ebb26603c543a7fc3ee4ec.svg",
      "fullname": "Nick",
      "name": "nickjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08006",
      "authors": [
        {
          "_id": "6847ae533ec10bdd8ab4df0c",
          "name": "Sicheng Mo",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df0d",
          "name": "Ziyang Leng",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df0e",
          "name": "Leon Liu",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df0f",
          "name": "Weizhen Wang",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df10",
          "name": "Honglin He",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df11",
          "name": "Bolei Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:59:52.000Z",
      "submittedOnDailyAt": "2025-06-10T02:33:18.764Z",
      "title": "Driepland: Un mundo configurable con modelos de simulación y generación",
      "submittedOnDailyBy": {
        "_id": "637c94d3f219c71f93eda9ad",
        "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
        "isPro": true,
        "fullname": "Sicheng Mo",
        "user": "Sichengmo",
        "type": "user"
      },
      "summary": "Los modelos de generación de vídeos a gran escala son adecuados para la composición de contenido visual vivo que conforma un mundo dinámico, pero su falta de control individual de cada elemento los hace difíciles para su uso en ediciones de escena o en el entrenamiento de AI agentes específicos. Proponemos el marco de trabajo híbrido de generación de mundos \"Dreamland\", que integra el control de alto orden de simuladores basados en física y el contenido vivo de modelos de aprendizaje previo a gran escala. Específicamente, diseñamos una abstracción del mundo con capas que incluyen la significación y generalidad a niveles de píxeles y objetos, y usamos una representación intermedia para conectar el simulador con el modelo generativo. Esta metodología potencia la posibilidad de control y minimiza el costo de adaptación al distribución real y al ajuste inicial, apoyando el uso de modelos de aprendizaje previo abierto tanto en los existentes como futuros. Además, construimos el conjunto de datos D3Sim para fomentar el entrenamiento y evaluación de la cadena de generación híbrida. Los experimentos muestran que Dreamland mejora la calidad del contenido en un 50.8% y aumenta la posibilidad de control en un 17.9%, demostrando también su gran potencial para el entrenamiento de agentes AI específicos. Los códigos y datos están disponibles para su uso.",
      "upvotes": 4,
      "discussionId": "6847ae533ec10bdd8ab4df12",
      "projectPage": "https://metadriverse.github.io/dreamland/",
      "ai_summary": "Dreamland, a hybrid framework, combines physics-based simulators and generative models to improve controllability and image quality in video generation.",
      "ai_keywords": [
        "video generative models",
        "physics-based simulator",
        "photorealistic content",
        "world abstraction",
        "pixel-level semantics",
        "object-level semantics",
        "geometry",
        "layered world abstraction",
        "early alignment",
        "D3Sim dataset",
        "embodied agent training"
      ]
    },
    "publishedAt": "2025-06-09T13:59:52.000Z",
    "title": "Dreamland: Controllable World Creation with Simulator and Generative\n  Models",
    "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08006.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c94d3f219c71f93eda9ad",
      "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
      "fullname": "Sicheng Mo",
      "name": "Sichengmo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06266",
      "authors": [
        {
          "_id": "6847b4b43ec10bdd8ab4df33",
          "user": {
            "_id": "6337537b267cee4d068f604d",
            "avatarUrl": "/avatars/15267f0759a6570c98ee6a150558fcc0.svg",
            "isPro": false,
            "fullname": "Sabri Eyuboglu",
            "user": "sabrieyuboglu",
            "type": "user"
          },
          "name": "Sabri Eyuboglu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T04:29:41.818Z",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df34",
          "name": "Ryan Ehrlich",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df35",
          "name": "Simran Arora",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df36",
          "name": "Neel Guha",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df37",
          "name": "Dylan Zinsley",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df38",
          "name": "Emily Liu",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df39",
          "name": "Will Tennien",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df3a",
          "name": "Atri Rudra",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df3b",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df3c",
          "name": "Azalia Mirhoseini",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df3d",
          "name": "Christopher Re",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:48:23.000Z",
      "submittedOnDailyAt": "2025-06-10T03:02:08.278Z",
      "title": "Característica: Modelos ligeros y representaciones comunes de largo contexto se pueden aprender.",
      "submittedOnDailyBy": {
        "_id": "6337537b267cee4d068f604d",
        "avatarUrl": "/avatars/15267f0759a6570c98ee6a150558fcc0.svg",
        "isPro": false,
        "fullname": "Sabri Eyuboglu",
        "user": "sabrieyuboglu",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande responden a consultas basadas en grandes corpus de texto como base de código, documentos legales y registros de chat, utilizando la aprendizaje de contexto (ICL) al colocar todo el corpus en un ventana de contexto. Actualmente, los modelos soportan contextos de 100K-1M tokens, pero esta configuración incrementa la consumo de memoria de caché KV proporcionalmente a la longitud de la entrada, lo que implica costos para el servicio. Estamos revisando una configuración alternativa de entrenamiento en caché KV pequeño en la red offline por corpus. Durante la inferencia, el caché KV entrenado se llama \"cartografía\" y se lee para validar la respuesta. Un punto importante es que el costo de entrenamiento de la cartografía puede asignarse a todas las solicitudes que referencian el mismo corpus. Sin embargo, entrenar la cartografía utilizando la predicción del siguiente token en el corpus no ha resultado en resultados mejores que la ICL. En su lugar, proponemos generar un conversor sintético para el corpus y entrenar la cartografía utilizando los objetos distillados del contexto para un método de entrenamiento llamado \"autodidáctico\". La cartografía entrenada con autodidáctico recreate las funciones de la ICL y puede reducir significativamente los costos del servicio. En benchmarks de contextos largos y difíciles, la cartografía entrenada con autodidáctico puede competir con la ICL, reduciendo el uso de memoria en 38.6 veces y aumentando el flujo transformer en 26.4 veces. Además, el autodidáctico puede ampliar eficazmente la longitud del contexto del modelo (por ejemplo, desde 128k tokens a 484k en MTOB), y a su vez, permite que la cartografía se configure durante la inferencia.",
      "upvotes": 4,
      "discussionId": "6847b4b43ec10bdd8ab4df3e",
      "projectPage": "https://hazyresearch.stanford.edu/blog/2025-06-08-cartridges",
      "githubRepo": "https://github.com/HazyResearch/cartridges",
      "ai_summary": "Training a smaller, offline KV cache (Cartridge) with a context-distillation objective (self-study) for large language models reduces serving costs, matches ICL performance, and extends effective context length.",
      "ai_keywords": [
        "KV cache",
        "Cartridge",
        "in-context learning (ICL)",
        "self-study",
        "context-distillation objective",
        "MTOB"
      ]
    },
    "publishedAt": "2025-06-06T13:48:23.000Z",
    "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
    "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06266.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6337537b267cee4d068f604d",
      "avatarUrl": "/avatars/15267f0759a6570c98ee6a150558fcc0.svg",
      "fullname": "Sabri Eyuboglu",
      "name": "sabrieyuboglu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07848",
      "authors": [
        {
          "_id": "6847de223ec10bdd8ab4e02a",
          "name": "Teng Hu",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02b",
          "name": "Zhentao Yu",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02c",
          "name": "Zhengguang Zhou",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02d",
          "name": "Jiangning Zhang",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02e",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02f",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e030",
          "name": "Ran Yi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T15:11:09.000Z",
      "submittedOnDailyAt": "2025-06-10T05:57:02.723Z",
      "title": "PolyVivid: Interacción y actualización multicanales para la generación de vídeos de reencuentro de vídeos de vídeos",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de la generación de imágenes ha llevado a que los modelos actuales carezcan particularmente de la precisa control sobre la interacción y la identidad de múltiples personas. En este artículo, se propone un marco de trabajo para la personalización de imágenes múltiples llamado \"PolyVivid\", que permite mantener la identidad de los personajes mientras facilita la generación flexible. Para establecer una correspondencia precisa entre texto y imagen, se diseña un módulo de fusión de texto-imagen basado en VLLM, lo que permite insertar identificadores visuales en el espacio de texto, proporcionando una base precisa. Además, para preservar la identidad y promover la interacción entre personajes, se propone un módulo de extensión basado en 3D-RoPE, lo que permite una fusión bidireccional estructurada de texto y imagen. Además, se desarrolla un módulo para insertar identificadores que mantienen la coherencia y evitan la pérdida de identidad durante la generación de imágenes. Finalmente, se construye una cadena de datos basada en MLLM y se combina una estrategia de integración de personas basada en MLLM, básica, segmentación y clic para generar datos de alta calidad y reducir la ambigüedad y la identificabilidad en la generación posterior. Los experimentos extendidos muestran que PolyVivid supera a los límites actuales tanto de los códigos abiertos como de los líneas comerciales, demostrando excelentes resultados en la fidelidad de la identidad, la realismo de las imágenes y la alineación de los personajes.",
      "upvotes": 2,
      "discussionId": "6847de223ec10bdd8ab4e031",
      "projectPage": "https://sjtuplayer.github.io/projects/PolyVivid/",
      "ai_summary": "PolyVivid is a multi-subject video customization framework that uses text-image fusion, 3D-RoPE enhancement, attention-inherited identity injection, and MLLM-based data processing to ensure identity consistency and realistic video generation.",
      "ai_keywords": [
        "VLLM-based text-image fusion",
        "3D-RoPE-based enhancement",
        "attention-inherited identity injection",
        "MLLM-based data pipeline",
        "identity fidelity",
        "video realism",
        "subject alignment"
      ]
    },
    "publishedAt": "2025-06-09T11:11:09.000Z",
    "title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal\n  Interaction and Enhancement",
    "summary": "Despite recent advances in video generation, existing models still lack\nfine-grained controllability, especially for multi-subject customization with\nconsistent identity and interaction. In this paper, we propose PolyVivid, a\nmulti-subject video customization framework that enables flexible and\nidentity-consistent generation. To establish accurate correspondences between\nsubject images and textual entities, we design a VLLM-based text-image fusion\nmodule that embeds visual identities into the textual space for precise\ngrounding. To further enhance identity preservation and subject interaction, we\npropose a 3D-RoPE-based enhancement module that enables structured\nbidirectional fusion between text and image embeddings. Moreover, we develop an\nattention-inherited identity injection module to effectively inject fused\nidentity features into the video generation process, mitigating identity drift.\nFinally, we construct an MLLM-based data pipeline that combines MLLM-based\ngrounding, segmentation, and a clique-based subject consolidation strategy to\nproduce high-quality multi-subject data, effectively enhancing subject\ndistinction and reducing ambiguity in downstream video generation. Extensive\nexperiments demonstrate that PolyVivid achieves superior performance in\nidentity fidelity, video realism, and subject alignment, outperforming existing\nopen-source and commercial baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07848.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07527",
      "authors": [
        {
          "_id": "6847dce63ec10bdd8ab4e011",
          "user": {
            "_id": "659e3ea885956d2cccda2b9e",
            "avatarUrl": "/avatars/f26d415f7c1b39550af2075769f38a91.svg",
            "isPro": false,
            "fullname": "马路",
            "user": "RoadQAQ",
            "type": "user"
          },
          "name": "Lu Ma",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T07:21:11.443Z",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e012",
          "name": "Hao Liang",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e013",
          "name": "Meiyi Qiang",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e014",
          "name": "Lexiang Tang",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e015",
          "name": "Xiaochen Ma",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e016",
          "name": "Zhen Hao Wong",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e017",
          "name": "Junbo Niu",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e018",
          "name": "Chengyu Shen",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e019",
          "name": "Runming He",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e01a",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e01b",
          "name": "Wentao Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/659e3ea885956d2cccda2b9e/Q_orxNfgmdXXT6I2ahrDs.jpeg"
      ],
      "publishedAt": "2025-06-09T08:11:20.000Z",
      "submittedOnDailyAt": "2025-06-10T05:52:14.746Z",
      "title": "Aprendiendo desde el DeepLimit: Ajustes en línea para problemas de ingreso en línea para los más difíciles problemas",
      "submittedOnDailyBy": {
        "_id": "659e3ea885956d2cccda2b9e",
        "avatarUrl": "/avatars/f26d415f7c1b39550af2075769f38a91.svg",
        "isPro": false,
        "fullname": "马路",
        "user": "RoadQAQ",
        "type": "user"
      },
      "summary": "Recientemente, los modelos de lenguaje grande (LLM) están demostrando cómo los aprendizajes por refuerzo (RL) pueden generar acciones complejas como planificación y auto-reflexión. Sin embargo, el actual desarrollo de RL no supera las limitaciones de los modelos básicos, ya que se optimizan basándose en la información actual y no pueden adaptarse a nuevas informaciones. Para superar estas limitaciones, utilizamos ajustes de fine-tuning (SFT) para entrenar partes de RL que no pueden aprender. Esto nos permite combinar datos de supervisión de alta calidad y patrones de nuevas conocidas y razones. Al analizar la dinámica de entrenamiento de RL y SFT, hemos descubierto que RL se centra en mejorar la eficiencia y la seguridad de las acciones, mientras que SFT es eficaz para promover el aprendizaje en contextos fuera del rango actual del modelo. Basándonos en estos puntos fortes, presentamos un nuevo enfoque de entrenamiento llamado ReLIFT (Reinforcement Learning Interleaved with Online Fine-Tuning). En ReLIFT, se entrena principalmente con RL, pero cuando se enfrentan a consultas difíciles, se ajustan con soluciones de alta calidad y se realizan entrenamientos de cruzada entre RL y fine-tuning para mejorar la capacidad de razonamiento del modelo. En comparación con otros modelos de RL, ReLIFT ha obtenido un aumento promedio de más de +5.2 puntos en 5 marcadores de competencia y 1 benchmark de distribución fuera del rango. Además, con solo 13% de datos de supervisión detallada, ReLIFT supera tanto la eficiencia de RL como de SFT y destaca su escalabilidad. Estos resultados demuestran que ReLIFT supera las limitaciones básicas de RL y destaca su potencial importante.",
      "upvotes": 2,
      "discussionId": "6847dce73ec10bdd8ab4e01c",
      "githubRepo": "https://github.com/TheRoadQaQ/ReLIFT",
      "ai_summary": "ReLIFT, a method combining reinforcement learning and supervised fine-tuning, enhances large language model reasoning by addressing limitations of RL through interleaved training, improving performance across benchmarks with minimal data.",
      "ai_keywords": [
        "reinforcement learning",
        "supervised fine-tuning",
        "ReLIFT",
        "large language model",
        "reasoning",
        "training dynamics",
        "zero-RL models",
        "competition-level benchmarks",
        "out-of-distribution benchmark"
      ]
    },
    "publishedAt": "2025-06-09T04:11:20.000Z",
    "title": "Learning What Reinforcement Learning Can't: Interleaved Online\n  Fine-Tuning for Hardest Questions",
    "summary": "Recent advances in large language model (LLM) reasoning have shown that\nsophisticated behaviors such as planning and self-reflection can emerge through\nreinforcement learning (RL). However, despite these successes, RL in its\ncurrent form remains insufficient to induce capabilities that exceed the\nlimitations of the base model, as it is primarily optimized based on existing\nknowledge of the model rather than facilitating the acquisition of new\ninformation. To address this limitation, we employ supervised fine-tuning (SFT)\nto learn what RL cannot, which enables the incorporation of new knowledge and\nreasoning patterns by leveraging high-quality demonstration data. We analyze\nthe training dynamics of RL and SFT for LLM reasoning and find that RL excels\nat maintaining and improving performance on questions within the model's\noriginal capabilities, while SFT is more effective at enabling progress on\nquestions beyond the current scope of the model. Motivated by the complementary\nstrengths of RL and SFT, we introduce a novel training approach,\nReLIFT (Reinforcement Learning Interleaved\nwith Online Fine-Tuning). In ReLIFT, the model is primarily\ntrained using RL, but when it encounters challenging questions, high-quality\nsolutions are collected for fine-tuning, and the training process alternates\nbetween RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT\nachieves an average improvement of over +5.2 points across five\ncompetition-level benchmarks and one out-of-distribution benchmark compared to\nother zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both\nRL and SFT while using only 13\\% of the detailed demonstration data,\nhighlighting its scalability. These results provide compelling evidence that\nReLIFT overcomes the fundamental limitations of RL and underscores the\nsignificant potential.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/659e3ea885956d2cccda2b9e/Q_orxNfgmdXXT6I2ahrDs.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "659e3ea885956d2cccda2b9e",
      "avatarUrl": "/avatars/f26d415f7c1b39550af2075769f38a91.svg",
      "fullname": "马路",
      "name": "RoadQAQ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07240",
      "authors": [
        {
          "_id": "6847b6513ec10bdd8ab4df49",
          "user": {
            "_id": "600bde0c2b417b1d53669bd0",
            "avatarUrl": "/avatars/2d9704713630e96458368b47179c039c.svg",
            "isPro": false,
            "fullname": "Roy Eisenstadt",
            "user": "royeis",
            "type": "user"
          },
          "name": "Roy Eisenstadt",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T04:36:34.797Z",
          "hidden": false
        },
        {
          "_id": "6847b6513ec10bdd8ab4df4a",
          "name": "Itamar Zimerman",
          "hidden": false
        },
        {
          "_id": "6847b6513ec10bdd8ab4df4b",
          "name": "Lior Wolf",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/TYxLNOb6ac6HH-pR04f7W.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/CVHB0FrpM5va85IVFh2bT.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/c-o1T8-RuSJ222Fj79LDN.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/eKIDa2-ZEMdQdYOimgSpC.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/VI5bYWw7ZDAruk6rqNrMh.jpeg"
      ],
      "publishedAt": "2025-06-08T17:54:33.000Z",
      "submittedOnDailyAt": "2025-06-10T03:10:26.281Z",
      "title": "Observación y control de la longitud del camino de pensamiento en LLM",
      "submittedOnDailyBy": {
        "_id": "65376feed325b3f02fb92c69",
        "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
        "isPro": false,
        "fullname": "Itamar Zimerman",
        "user": "ItamarZ",
        "type": "user"
      },
      "summary": "Recientemente, la teoría de la estructura explícita ha demostrado que el razonamiento del modelo puede diferenciar claramente los procesos internos del modelo y las respuestas finales. En este contexto, una de las variables que afectan la calidad de las respuestas es la longitud del razonamiento. Si el razonamiento es demasiado corto, el modelo puede no comprender la complejidad del trabajo, mientras que si es demasiado largo, el modelo puede pensar demasiado y realizar cálculos innecesarios, lo que puede disminuir su rendimiento. Este artículo investiga y utiliza una estructura básica para que los modelos de lenguaje grande (LLM) comprendan y ajusten la longitud del razonamiento en el proceso de pensamiento explícito. Primero, codificamos cómo el modelo realiza el proceso de razonamiento, y luego visualizamos el plan del modelo para proporcionar una comprensión de su dinámica de planificación. A continuación, manipulamos el codificado de la progresión interna durante la inferencia para reducir los pasos innecesarios y generar una secuencia de pensamiento más clara y decisiva. Según nuestros resultados experimentales, este método \"overclock\" puede reducir el pensamiento excesivo, mejorar la precisión de las respuestas y reducir el tiempo de inferencia. Nuestro código está disponible para uso público.",
      "upvotes": 2,
      "discussionId": "6847b6513ec10bdd8ab4df4c",
      "projectPage": "https://royeisen.github.io/OverclockingLLMReasoning-paper/",
      "githubRepo": "https://github.com/royeisen/reasoning_loading_bar",
      "ai_summary": "LLMs regulate reasoning length through progress encoding, and manipulating this encoding improves accuracy and reduces inference time.",
      "ai_keywords": [
        "explicit structured reasoning",
        "LLMs",
        "reasoning process",
        "progress bar visualization",
        "progress encoding",
        "inference",
        "overclocking",
        "overthinking",
        "answer accuracy",
        "inference latency"
      ]
    },
    "publishedAt": "2025-06-08T13:54:33.000Z",
    "title": "Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path\n  Lengths in LLMs",
    "summary": "Recently, techniques such as explicit structured reasoning have demonstrated\nstrong test-time scaling behavior by enforcing a separation between the model's\ninternal \"thinking\" process and the final response. A key factor influencing\nanswer quality in this setting is the length of the thinking stage. When the\nreasoning is too short, the model may fail to capture the complexity of the\ntask. Conversely, when it is too long, the model may overthink, leading to\nunnecessary computation and degraded performance. This paper explores and\nexploits the underlying mechanisms by which LLMs understand and regulate the\nlength of their reasoning during explicit thought processes. First, we show\nthat LLMs encode their progress through the reasoning process and introduce an\ninteractive progress bar visualization, which is then used to reveal insights\non the model's planning dynamics. Second, we manipulate the internal progress\nencoding during inference to reduce unnecessary steps and generate a more\nconcise and decisive chain of thoughts. Our empirical results demonstrate that\nthis \"overclocking\" method mitigates overthinking, improves answer accuracy,\nand reduces inference latency. Our code is publicly available.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/TYxLNOb6ac6HH-pR04f7W.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/CVHB0FrpM5va85IVFh2bT.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/c-o1T8-RuSJ222Fj79LDN.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/eKIDa2-ZEMdQdYOimgSpC.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/VI5bYWw7ZDAruk6rqNrMh.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65376feed325b3f02fb92c69",
      "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
      "fullname": "Itamar Zimerman",
      "name": "ItamarZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07160",
      "authors": [
        {
          "_id": "6847c0263ec10bdd8ab4df60",
          "user": {
            "_id": "627b73728b6ecd7ece822825",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
            "isPro": false,
            "fullname": "Yikun Wang",
            "user": "LibraTree",
            "type": "user"
          },
          "name": "Yikun Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:42:47.991Z",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df61",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df62",
          "name": "Dianyi Wang",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df63",
          "name": "Zimian Peng",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df64",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df65",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df66",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-08T14:18:15.000Z",
      "submittedOnDailyAt": "2025-06-10T04:04:49.808Z",
      "title": "GeometryZero: Mejora del método geométrico de resolución de problemas de LLM mediante optimización estratégica de grupos",
      "submittedOnDailyBy": {
        "_id": "627b73728b6ecd7ece822825",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
        "isPro": false,
        "fullname": "Yikun Wang",
        "user": "LibraTree",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de grandes modelos de lenguaje (LLMs) ha demostrado capacidades extraordinarias en diversas áreas, incluyendo la resolución de problemas matemáticos. En particular, la resolución de problemas geométricos es un campo difícil donde la composición de la explicación juega un papel crucial. Los métodos actuales no logran alcanzar el rendimiento óptimo o dependen de grandes LLMs (como GPT-4o), lo que implica costos de cálculo muy elevados. Hemos identificado un potencial camino prometedor: el aprendizaje por refuerzo basado en compensaciones provable (como GRPO), que puede permitir el aprendizaje de pequeños modelos. Este enfoque tiene el potencial de integrar la composición de la explicación con fuertes capacidades de inferencia geométrica. Sin embargo, aplicar directamente GRPO a la inferencia geométrica implica limitaciones inherentes, ya que depende absolutamente de las señales de compensación, lo que puede generar una variedad de formas de composición de la explicación. Para resolver estos problemas, proponemos la Optimización de Políticas en Comparación con Grupos (GCPO). GCPO destaca por dos innovaciones clave: (1) Mascarado de Comparación de Grupos: proporciona señales de compensación positiva y negativa adecuadas para la composición de la explicación. (2) Compensación por Longitud: incentiva cadenas de inferencia largas. Basándonos en GCPO, hemos desarrollado la familia de modelos GeometryZero. GeometryZero ofrece modelos de inferencia geométrica que reducen los costos de cálculo al evaluar de manera adecuada la composición de la explicación. A través de amplios experimentos en diferentes benchmarks como Geometry3K y MathVista, los modelos de GeometryZero superan los estándares (como GRPO) y alcanzan un aumento promedio del 4.29% en todos los benchmarks.",
      "upvotes": 2,
      "discussionId": "6847c0273ec10bdd8ab4df67",
      "ai_summary": "A new reinforcement learning framework, Group Contrastive Policy Optimization (GCPO), enhances geometric reasoning in large language models with judicious auxiliary constructions, outperforming existing methods on benchmarks.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "mathematical reasoning",
        "geometry problem solving",
        "reinforcement learning",
        "verifiable reward",
        "GRPO",
        "Group Contrastive Policy Optimization (GCPO)",
        "Group Contrastive Masking",
        "length reward",
        "GeometryZero",
        "Geometry3K",
        "MathVista"
      ]
    },
    "publishedAt": "2025-06-08T10:18:15.000Z",
    "title": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive\n  Policy Optimization",
    "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities across diverse domains, particularly in mathematical reasoning,\namid which geometry problem solving remains a challenging area where auxiliary\nconstruction plays a enssential role. Existing approaches either achieve\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\nmassive computational costs. We posit that reinforcement learning with\nverifiable reward (e.g., GRPO) offers a promising direction for training\nsmaller models that effectively combine auxiliary construction with robust\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\npresents fundamental limitations due to its dependence on unconditional\nrewards, which leads to indiscriminate and counterproductive auxiliary\nconstructions. To address these challenges, we propose Group Contrastive Policy\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\nor negative reward signals for auxiliary construction based on contextual\nutility, and a (2) length reward that promotes longer reasoning chains.\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\ngeometric reasoning models that judiciously determine when to employ auxiliary\nconstruction. Our extensive empirical evaluation across popular geometric\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\nof 4.29% across all benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07160.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "627b73728b6ecd7ece822825",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
      "fullname": "Yikun Wang",
      "name": "LibraTree",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03690",
      "authors": [
        {
          "_id": "684664f13ec10bdd8ab4dac0",
          "user": {
            "_id": "64e6c617ecce34cb442cb208",
            "avatarUrl": "/avatars/ebc61bf6a043314cb2089b1efd5e6a18.svg",
            "isPro": false,
            "fullname": "JieSun",
            "user": "Sunshine279",
            "type": "user"
          },
          "name": "Jie Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:19.474Z",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac1",
          "name": "Junkang Wu",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac2",
          "name": "Jiancan Wu",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac3",
          "name": "Zhibo Zhu",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac4",
          "name": "Xingyu Lu",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac5",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac6",
          "name": "Lintao Ma",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac7",
          "name": "Xiang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T08:19:37.000Z",
      "submittedOnDailyAt": "2025-06-10T00:51:18.490Z",
      "title": "Optimizamos el estilo sólido mediante la utilización de márgenes de metas dinámicos.",
      "submittedOnDailyBy": {
        "_id": "64e6c617ecce34cb442cb208",
        "avatarUrl": "/avatars/ebc61bf6a043314cb2089b1efd5e6a18.svg",
        "isPro": false,
        "fullname": "JieSun",
        "user": "Sunshine279",
        "type": "user"
      },
      "summary": "La ajuste de un LLM es crucial para garantizar la seguridad y confianza en aplicaciones prácticas. La Direct Preference Optimization (DPO) es un método que utiliza pares de preferencias para optimizar el modelo de manera eficiente. Este método puede reducir significativamente las demandas de recursos. Sin embargo, el rendimiento de DPO se ve afectado considerablemente por la calidad de los datos. Los datos pueden sufrir daños por ruido. En este artículo, se propone un algoritmo de optimización de preferencias con margen dinámico (gamma-PO) que ajusta dinámicamente el margen de compensación en el nivel de pares utilizando la técnica de calibración de margen específico a instancia. El gamma-PO prioriza estratégicamente pares con alta confianza (pares en los que el margen de compensación es alto) y reduce el ruido en pares inciertos, adaptándose a la versión de DPO. En los benchmarks como AlpacaEval2 y Arena-Hard, el gamma-PO logró mejoras medias del 4.4%, estableciendo nuevos estándares de rendimiento. Además, el gamma-PO requiere solo cambios mínimos en el código y no afecta al entorno de entrenamiento. Es una solución poderosa para fortalecer el ajuste de un LLM. El código está disponible en https://github.com/sunjie279/gammaPO.",
      "upvotes": 2,
      "discussionId": "684664f13ec10bdd8ab4dac8",
      "ai_summary": "The paper introduces γ-PO, a dynamic target margin preference optimization algorithm that enhances Large Language Models' alignment by adjusting reward margins at the pairwise level, leading to improved performance with minimal impact on training.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "Direct Preference Optimization",
        "DPO",
        "preference pairs",
        "γ-PO",
        "instance-specific margin calibration",
        "reward margins",
        "AlpacaEval2",
        "Arena-Hard",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-06-04T04:19:37.000Z",
    "title": "Robust Preference Optimization via Dynamic Target Margins",
    "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose gamma-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, gamma-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, gamma-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, gamma-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, gamma-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\nhttps://github.com/sunjie279/gammaPO{https://github.com/sunjie279/gammaPO}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03690.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6c617ecce34cb442cb208",
      "avatarUrl": "/avatars/ebc61bf6a043314cb2089b1efd5e6a18.svg",
      "fullname": "JieSun",
      "name": "Sunshine279",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07803",
      "authors": [
        {
          "_id": "6847d4103ec10bdd8ab4dfb1",
          "user": {
            "_id": "6437d0a951c7ebfc813c735b",
            "avatarUrl": "/avatars/6cbac4e4be5029655702c5d8b9046b90.svg",
            "isPro": false,
            "fullname": "Allakhverdov Eduard",
            "user": "combat-helicopter",
            "type": "user"
          },
          "name": "Eduard Allakhverdov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:42:38.549Z",
          "hidden": false
        },
        {
          "_id": "6847d4103ec10bdd8ab4dfb2",
          "name": "Dmitrii Tarasov",
          "hidden": false
        },
        {
          "_id": "6847d4103ec10bdd8ab4dfb3",
          "name": "Elizaveta Goncharova",
          "hidden": false
        },
        {
          "_id": "6847d4103ec10bdd8ab4dfb4",
          "name": "Andrey Kuznetsov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T14:32:18.000Z",
      "submittedOnDailyAt": "2025-06-10T05:17:41.513Z",
      "title": "Reconstrucción de imágenes es un instrumento para el análisis de características.",
      "submittedOnDailyBy": {
        "_id": "6310ff34bc152fa3e810c186",
        "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
        "isPro": false,
        "fullname": "Elizaveta Goncharova",
        "user": "Elizaveta",
        "type": "user"
      },
      "summary": "El Vision Encoder está aumentando su uso en aplicaciones modernas como modelos de visión y sistemas multimodal. A pesar de sus éxitos sorprendentes, la forma en que esta arquitectura representa características internas no es clara. En este trabajo, proponemos un nuevo enfoque para interpretar características visuales a través de reconstrucción de imágenes. Comparamos dos familias de modelos relacionados, SigLIP y SigLIP2, que, aunque tienen objetivos de aprendizaje diferentes, muestran que un encoder pre-entrenado en tareas de versión de imágenes mantiene más información de imágenes que uno entrenado en tareas de versión de vídeo. Además, aplicamos esta metodología a varios encoders de visión y ordenamos su representación de características basándonos en la cantidad de información que contienen. Finalmente, demostramos que la manipulación de espacios de características produce cambios predictibles en las imágenes reconstruidas y que la rotación ortogonal (más que transformaciones espaciales) controla la codificación de color. Nuestro enfoque es aplicable a todos los encoders de vídeo y revela la estructura interna de sus espacios de características. Los códigos y pesos de modelos para reproducir los experimentos están disponibles en GitHub.",
      "upvotes": 0,
      "discussionId": "6847d4103ec10bdd8ab4dfb5",
      "projectPage": "https://fusionbrainlab.github.io/feature_analysis/",
      "githubRepo": "https://github.com/FusionBrainLab/feature_analysis",
      "ai_summary": "Image reconstruction reveals that vision encoders retain more image information after image-based tasks and that orthogonal rotations in feature space control color encoding.",
      "ai_keywords": [
        "SigLIP",
        "SigLIP2",
        "vision encoders",
        "image reconstruction",
        "contrastive learning",
        "feature representations"
      ]
    },
    "publishedAt": "2025-06-09T10:32:18.000Z",
    "title": "Image Reconstruction as a Tool for Feature Analysis",
    "summary": "Vision encoders are increasingly used in modern applications, from\nvision-only models to multimodal systems such as vision-language models.\nDespite their remarkable success, it remains unclear how these architectures\nrepresent features internally. Here, we propose a novel approach for\ninterpreting vision features via image reconstruction. We compare two related\nmodel families, SigLIP and SigLIP2, which differ only in their training\nobjective, and show that encoders pre-trained on image-based tasks retain\nsignificantly more image information than those trained on non-image tasks such\nas contrastive learning. We further apply our method to a range of vision\nencoders, ranking them by the informativeness of their feature representations.\nFinally, we demonstrate that manipulating the feature space yields predictable\nchanges in reconstructed images, revealing that orthogonal rotations (rather\nthan spatial transformations) control color encoding. Our approach can be\napplied to any vision encoder, shedding light on the inner structure of its\nfeature space. The code and model weights to reproduce the experiments are\navailable in GitHub.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07803.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310ff34bc152fa3e810c186",
      "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
      "fullname": "Elizaveta Goncharova",
      "name": "Elizaveta",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07645",
      "authors": [
        {
          "_id": "6847ea583ec10bdd8ab4e05a",
          "user": {
            "_id": "635270e36cfb8f14981312e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635270e36cfb8f14981312e7/FpqjdhABEDuxWR8k2zOwA.jpeg",
            "isPro": false,
            "fullname": "Maciej Chrabąszcz",
            "user": "mchraba",
            "type": "user"
          },
          "name": "Maciej Chrabąszcz",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:42:31.860Z",
          "hidden": false
        },
        {
          "_id": "6847ea583ec10bdd8ab4e05b",
          "user": {
            "_id": "66dab47f8506f9b6cf5f08ed",
            "avatarUrl": "/avatars/e6ba87adbaacdeccf8c4818596c655d0.svg",
            "isPro": false,
            "fullname": "LLM Attack",
            "user": "llmAttack",
            "type": "user"
          },
          "name": "Katarzyna Lorenc",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T08:18:33.059Z",
          "hidden": false
        },
        {
          "_id": "6847ea583ec10bdd8ab4e05c",
          "name": "Karolina Seweryn",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T11:09:39.000Z",
      "submittedOnDailyAt": "2025-06-10T06:49:10.646Z",
      "title": "Uso del modelo de Procesos en la evaluación de robustez en lenguajes ricos en recursos de LLM",
      "submittedOnDailyBy": {
        "_id": "635270e36cfb8f14981312e7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635270e36cfb8f14981312e7/FpqjdhABEDuxWR8k2zOwA.jpeg",
        "isPro": false,
        "fullname": "Maciej Chrabąszcz",
        "user": "mchraba",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje general (LLMs) han demostrado recientemente capacidades sorprendentes en diversas tareas de procesamiento del lenguaje natural (NLP). Sin embargo, son vulnerables a los \"jailbreaks\" y \"perturbations\" y requieren evaluaciones adicionales. Muchos LLMs están compuestos en múltiples idiomas, pero los datos de entrenamiento relacionados con seguridad se incluyen principalmente en idiomas de alto recurso como el inglés. Esto muestra la vulnerabilidad a la destrucción de idiomas de bajo recurso, como el polaco. Demostramos que sus ataques potentes pueden crearse con solo cambios de caracteres y el uso de pequeños modelos de procesamiento de palabras para calcular su importancia, lo que puede hacerlos más baratos. Estos ataques a nivel de caracteres y palabras pueden cambiar significativamente las predicciones de diferentes LLMs y evitar funciones de seguridad internas. Hemos verificado y mostrado la vulnerabilidad de estos ataques en el polaco (un idioma de bajo recurso) y también hemos presentado métodos para extenderlos a otros idiomas. Proporcionamos nuestro conjunto de datos y código para la investigación adicional.",
      "upvotes": 0,
      "discussionId": "6847ea583ec10bdd8ab4e05d",
      "ai_summary": "Character and word-level attacks using a proxy model reveal vulnerabilities in LLMs across languages, particularly in low-resource languages like Polish.",
      "ai_keywords": [
        "large language models",
        "natural language processing",
        "jailbreaks",
        "perturbations",
        "multilingual",
        "safety-related training data",
        "high-resource languages",
        "low-resource languages",
        "character-level attacks",
        "word-level attacks",
        "word importance calculation",
        "internal safety mechanisms"
      ]
    },
    "publishedAt": "2025-06-09T07:09:39.000Z",
    "title": "Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models",
    "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\nvarious natural language processing (NLP) tasks in recent years. However, their\nsusceptibility to jailbreaks and perturbations necessitates additional\nevaluations. Many LLMs are multilingual, but safety-related training data\ncontains mainly high-resource languages like English. This can leave them\nvulnerable to perturbations in low-resource languages such as Polish. We show\nhow surprisingly strong attacks can be cheaply created by altering just a few\ncharacters and using a small proxy model for word importance calculation. We\nfind that these character and word-level attacks drastically alter the\npredictions of different LLMs, suggesting a potential vulnerability that can be\nused to circumvent their internal safety mechanisms. We validate our attack\nconstruction methodology on Polish, a low-resource language, and find potential\nvulnerabilities of LLMs in this language. Additionally, we show how it can be\nextended to other languages. We release the created datasets and code for\nfurther research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07645.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635270e36cfb8f14981312e7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635270e36cfb8f14981312e7/FpqjdhABEDuxWR8k2zOwA.jpeg",
      "fullname": "Maciej Chrabąszcz",
      "name": "mchraba",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05904",
      "authors": [
        {
          "_id": "6847e05a3ec10bdd8ab4e03d",
          "user": {
            "_id": "6369b1d456d1f93498130a8a",
            "avatarUrl": "/avatars/8ec228aa6f171715652511f948765db9.svg",
            "isPro": false,
            "fullname": "Yichi Zhang",
            "user": "594zyc",
            "type": "user"
          },
          "name": "Yichi Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T07:35:55.259Z",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e03e",
          "name": "Xin Luna Dong",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e03f",
          "name": "Zhaojiang Lin",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e040",
          "name": "Andrea Madotto",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e041",
          "name": "Anuj Kumar",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e042",
          "name": "Babak Damavandi",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e043",
          "name": "Joyce Chai",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e044",
          "name": "Seungwhan Moon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T09:23:29.000Z",
      "submittedOnDailyAt": "2025-06-10T06:07:28.858Z",
      "title": "Generación de diálogos asistidos de acciones a partir de vídeos subjetivos",
      "submittedOnDailyBy": {
        "_id": "6369b1d456d1f93498130a8a",
        "avatarUrl": "/avatars/8ec228aa6f171715652511f948765db9.svg",
        "isPro": false,
        "fullname": "Yichi Zhang",
        "user": "594zyc",
        "type": "user"
      },
      "summary": "El desarrollo de la inteligencia artificial en conversaciones ha sido considerablemente grande, pero el desarrollo de sistemas en tiempo real para tareas visuales está enfrentado a grandes desafíos. Estos sistemas deben proporcionar consejos interactivos y activos basados en entradas visuales en tiempo real, pero su desarrollo está limitado por procesos costosos de recopilación de datos y evaluación del sistema. Para resolver estos limitaciones, presentamos tres principales contribuciones a través de un marco computacional basado:\n\nPrimero, introducimos un nuevo plan de colección de datos para generar conjuntos de datos mediante la síntesis de diarólogias desde capturas directas, lo que resulta en conjuntos de datos de diarólogias sintéticas de gran escala y diversos dominios.\n\nSegundo, desarrollamos métricas de evaluación automáticas validadas a través de diversas investigaciones humanas.\n\nTercero, proponemos modelos que procesan entradas vídeos dinámicos de manera eficiente y generan respuestas adecuadas desde el inicio hasta el final del flujo. Este modelo utiliza nuevas técnicas para manejar la desigualdad de datos y videos de larga duración.\n\nEste estudio se basa en el desarrollo de AI asistentes en tiempo real y activos que pueden guiar a los usuarios en diversas tareas. Página del proyecto: https://pro-assist.github.io/",
      "upvotes": 0,
      "discussionId": "6847e05a3ec10bdd8ab4e045",
      "projectPage": "https://pro-assist.github.io/",
      "ai_summary": "A framework provides automated data synthesis, evaluation metrics, and an end-to-end model for real-time, proactive conversational AI task guidance using streaming video inputs.",
      "ai_keywords": [
        "data curation pipeline",
        "synthetic dialogue dataset",
        "automatic evaluation metrics",
        "end-to-end model",
        "data imbalance",
        "long-duration videos"
      ]
    },
    "publishedAt": "2025-06-06T05:23:29.000Z",
    "title": "Proactive Assistant Dialogue Generation from Streaming Egocentric Videos",
    "summary": "Recent advances in conversational AI have been substantial, but developing\nreal-time systems for perceptual task guidance remains challenging. These\nsystems must provide interactive, proactive assistance based on streaming\nvisual inputs, yet their development is constrained by the costly and\nlabor-intensive process of data collection and system evaluation. To address\nthese limitations, we present a comprehensive framework with three key\ncontributions. First, we introduce a novel data curation pipeline that\nsynthesizes dialogues from annotated egocentric videos, resulting in \\dataset,\na large-scale synthetic dialogue dataset spanning multiple domains. Second, we\ndevelop a suite of automatic evaluation metrics, validated through extensive\nhuman studies. Third, we propose an end-to-end model that processes streaming\nvideo inputs to generate contextually appropriate responses, incorporating\nnovel techniques for handling data imbalance and long-duration videos. This\nwork lays the foundation for developing real-time, proactive AI assistants\ncapable of guiding users through diverse tasks. Project page:\nhttps://pro-assist.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05904.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6369b1d456d1f93498130a8a",
      "avatarUrl": "/avatars/8ec228aa6f171715652511f948765db9.svg",
      "fullname": "Yichi Zhang",
      "name": "594zyc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04807",
      "authors": [
        {
          "_id": "6847c0983ec10bdd8ab4df69",
          "user": {
            "_id": "65fba5700b78c48c9e393a3e",
            "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
            "isPro": false,
            "fullname": "Yuyi Zhang",
            "user": "ZZXF",
            "type": "user"
          },
          "name": "Yuyi Zhang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-10T05:42:47.055Z",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6a",
          "user": {
            "_id": "6616c9e090d2013d26a54b47",
            "avatarUrl": "/avatars/573064303dcdcf778e1fbbfcff3c9a2b.svg",
            "isPro": false,
            "fullname": "Shi",
            "user": "shiyx1",
            "type": "user"
          },
          "name": "Yongxin Shi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T05:20:28.050Z",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6b",
          "name": "Peirong Zhang",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6c",
          "name": "Yixin Zhao",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6d",
          "name": "Zhenhua Yang",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6e",
          "user": {
            "_id": "66a102960072f5db18e860e3",
            "avatarUrl": "/avatars/7679eddb31153c6b868cf496833551d6.svg",
            "isPro": false,
            "fullname": "Lianwen Jin",
            "user": "lianwen",
            "type": "user"
          },
          "name": "Lianwen Jin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T05:20:28.050Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T09:33:06.000Z",
      "submittedOnDailyAt": "2025-06-10T04:16:00.729Z",
      "title": "Megahan 97K: Megakategoría Reconocimiento de Caracteres Chinos para un Grande Dataset (Más de 97K Categorías)",
      "submittedOnDailyBy": {
        "_id": "65fba5700b78c48c9e393a3e",
        "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
        "isPro": false,
        "fullname": "Yuyi Zhang",
        "user": "ZZXF",
        "type": "user"
      },
      "summary": "La escritura china, fundamental para la lengua y la cultura chinas, es muy amplia y diversa, y el reciente estándar GB18030-2022 incluye 87,887 categorías. La precisión en la reconocimiento de estas grandes letras, conocido como \"Reconocimiento de Caracteres Tradicionales\", es un gran desafío para la preservación de la cultura y la aplicación digital. Aunque se ha avanzado en el Reconocimiento Óptico de Caracteres (OCR), el Reconocimiento de Caracteres Tradicionales sigue sin ser explorado debido a la escasez de conjuntos de datos. Para llenar este importante campo, presentamos MegaHan97K, un conjunto de datos de caracteres tradicionales y de gran escala. Nuestro trabajo ofrece tres contribuciones principales: 1) MegaHan97K es el primer conjunto de datos que completamente respete el estándar GB18030-2022 y incluye al menos 6 veces más categorías que los conjuntos de datos actuales; 2) soluciona efectivamente el problema de la distribución de colas larga, proporcionando muestras equilibradas para todas las categorías a través de tres subconjuntos diferentes: letras manuscritas, históricas y sintéticas; 3) revela nuevos desafíos en el reconocimiento de caracteres tradicionales, incluyendo el aumento de la carga de almacenamiento, la reconocción de caracteres similares y los desafíos de entrenamiento sin ejemplos. A pesar de nuestras limitaciones de conocimiento, MegaHan97K se espera que sea el conjunto de datos más grande en el campo de la OCR y en amplias áreas de reconocimiento de patrones. El conjunto de datos está disponible en https://github.com/SCUT-DLVCLab/MegaHan97K.",
      "upvotes": 0,
      "discussionId": "6847c0993ec10bdd8ab4df6f",
      "ai_summary": "MegaHan97K, a large-scale dataset for recognizing over 97,000 Chinese characters, addresses the long-tail distribution problem and reveals new challenges in mega-category OCR.",
      "ai_keywords": [
        "Optical Character Recognition (OCR)",
        "mega-category recognition",
        "MegaHan97K",
        "long-tail distribution",
        "zero-shot learning"
      ]
    },
    "publishedAt": "2025-06-05T05:33:06.000Z",
    "title": "MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character\n  Recognition with over 97K Categories",
    "summary": "Foundational to the Chinese language and culture, Chinese characters\nencompass extraordinarily extensive and ever-expanding categories, with the\nlatest Chinese GB18030-2022 standard containing 87,887 categories. The accurate\nrecognition of this vast number of characters, termed mega-category\nrecognition, presents a formidable yet crucial challenge for cultural heritage\npreservation and digital applications. Despite significant advances in Optical\nCharacter Recognition (OCR), mega-category recognition remains unexplored due\nto the absence of comprehensive datasets, with the largest existing dataset\ncontaining merely 16,151 categories. To bridge this critical gap, we introduce\nMegaHan97K, a mega-category, large-scale dataset covering an unprecedented\n97,455 categories of Chinese characters. Our work offers three major\ncontributions: (1) MegaHan97K is the first dataset to fully support the latest\nGB18030-2022 standard, providing at least six times more categories than\nexisting datasets; (2) It effectively addresses the long-tail distribution\nproblem by providing balanced samples across all categories through its three\ndistinct subsets: handwritten, historical and synthetic subsets; (3)\nComprehensive benchmarking experiments reveal new challenges in mega-category\nscenarios, including increased storage demands, morphologically similar\ncharacter recognition, and zero-shot learning difficulties, while also\nunlocking substantial opportunities for future research. To the best of our\nknowledge, the MetaHan97K is likely the dataset with the largest classes not\nonly in the field of OCR but may also in the broader domain of pattern\nrecognition. The dataset is available at\nhttps://github.com/SCUT-DLVCLab/MegaHan97K.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fba5700b78c48c9e393a3e",
      "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
      "fullname": "Yuyi Zhang",
      "name": "ZZXF",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23473",
      "authors": [
        {
          "_id": "6847c9693ec10bdd8ab4df91",
          "name": "Xiaorui Wu",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df92",
          "name": "Xiaofeng Mao",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df93",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df94",
          "name": "Fei Li",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df95",
          "name": "Chong Teng",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df96",
          "name": "Yuxiang Peng",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df97",
          "name": "Li Zheng",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df98",
          "name": "Donghong Ji",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df99",
          "name": "Zhuang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T14:26:46.000Z",
      "submittedOnDailyAt": "2025-06-10T04:28:59.228Z",
      "title": "EVOREFUSE: Evaluación y medidas de respuesta frente a la exagerada respuesta de rechazo de un modelo de lenguaje por optimización evolutiva de Prompts",
      "submittedOnDailyBy": {
        "_id": "63d159132036e44c44f87a91",
        "avatarUrl": "/avatars/6a2c9e5b3b25cf1949277d8c40c0070b.svg",
        "isPro": false,
        "fullname": "Zhuang Li",
        "user": "lizhuang144",
        "type": "user"
      },
      "summary": "Los grandes modelos de lenguaje (LLMs) frecuentemente rechazan estructuras de queries de mariposa: consultas sintéticamente inocentes son rechazadas innecesariamente por líneas de seguridad conservadoras, lo que da un gran daño al experiencia del usuario. Esta concentración de comandos evalúa y mitiga la rechazo excesivo, pero los métodos actuales de configuración de comandos, la generación automática o los cambios de comando no son escalables y no pueden generar una diversidad y una efectividad en la generación de rechazos suficientes. Para resolver estos limitaciones, presentamos EVOREFUSE. EVOREFUSE es un enfoque optimizador de programación para generar comandos coherentes de mariposa. EVOREFUSE explora un espacio de comandos más diverso que los métodos actuales, utilizando estrategias de mutación y recombinación para maximizar la evidencia inferior de la probabilidad de rechazo de un LLM, lo que lo hace más confiable. Con EVOREFUSE, hemos creado dos nuevos conjuntos de datos: EVOREFUSE-TEST, que actúa como un benchmark con 582 comandos de mariposa, con un promedio de rechazo de 140.41% en 9 modelos de LLM, una diversidad previa del 34.86% y un aumento del 40.03% en el puntaje de confianza de rechazo, superando el mejor benchmark disponible. EVOREFUSE-ALIGN proporciona 3,000 comandos y respuestas de mariposa para la entrenamiento de servicio y la entrenamiento de líneas de seguridad basadas en preferencias. El modelo LLAMA3.1-8B-INSTRUCT, entrenado con la regulación de EVOREFUSE-ALIGN, mitiga el rechazo excesivo en un 14.31% menos comparado con el mejor conjunto de datos de línea de seguridad. En el análisis de EVOREFUSE-TEST, se demuestra claramente que el modelo centra demasiado en palabras clave sensibles y ignora ampliamente el contexto, lo que provoca un rechazo excesivo.",
      "upvotes": 0,
      "discussionId": "6847c9693ec10bdd8ab4df9a",
      "ai_summary": "EVOREFUSE, an evolutionary algorithm, generates diverse pseudo-malicious instructions to optimize LLM refusal training, improving user experience without compromising safety.",
      "ai_keywords": [
        "large language models",
        "pseudo-malicious instructions",
        "safety alignment",
        "instruction optimization",
        "evolutionary algorithm",
        "mutation strategies",
        "recombination",
        "evidence lower bound",
        "refusal probability",
        "lexical diversity",
        "LLM response confidence scores",
        "over-refusals",
        "supervised fine-tuning",
        "preference-based alignment training"
      ]
    },
    "publishedAt": "2025-05-29T10:26:46.000Z",
    "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions",
    "summary": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious\ninstructions: semantically harmless input queries triggering unnecessary LLM\nrefusals due to conservative safety alignment, significantly impairing user\nexperience. Collecting such instructions is crucial for evaluating and\nmitigating over-refusals, but existing instruction curation methods, like\nmanual creation or instruction rewriting, either lack scalability or fail to\nproduce sufficiently diverse and effective refusal-inducing prompts. To address\nthese limitations, we introduce EVOREFUSE, a prompt optimization approach that\ngenerates diverse pseudo-malicious instructions consistently eliciting\nconfident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm\nexploring the instruction space in more diverse directions than existing\nmethods via mutation strategies and recombination, and iteratively evolves seed\ninstructions to maximize evidence lower bound on LLM refusal probability. Using\nEVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582\npseudo-malicious instructions that outperforms the next-best benchmark with\n140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater\nlexical diversity, and 40.03% improved LLM response confidence scores; and\nEVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with\nresponses for supervised and preference-based alignment training.\nLLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to\n14.31% fewer over-refusals than models trained on the second-best alignment\ndataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals\nmodels trigger over-refusals by overly focusing on sensitive keywords while\nignoring broader context.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23473.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d159132036e44c44f87a91",
      "avatarUrl": "/avatars/6a2c9e5b3b25cf1949277d8c40c0070b.svg",
      "fullname": "Zhuang Li",
      "name": "lizhuang144",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]