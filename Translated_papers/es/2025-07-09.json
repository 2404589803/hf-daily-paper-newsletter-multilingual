[
  {
    "paper": {
      "id": "2507.06203",
      "authors": [
        {
          "_id": "686ddd7fcb5725779c60b444",
          "user": {
            "_id": "63ff09f24852102d4871c19c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
            "isPro": false,
            "fullname": "Rui-Jie Zhu",
            "user": "ridger",
            "type": "user"
          },
          "name": "Rui-Jie Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:49:55.890Z",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b445",
          "name": "Tianhao Peng",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b446",
          "name": "Tianhao Cheng",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b447",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b448",
          "user": {
            "_id": "63f37af60be81bdc5d92eebb",
            "avatarUrl": "/avatars/b8dfdff4ab36988ec9a8643e82a3d2db.svg",
            "isPro": false,
            "fullname": "Huang",
            "user": "Jinfa",
            "type": "user"
          },
          "name": "Jinfa Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:49:54.002Z",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b449",
          "name": "Dawei Zhu",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b44a",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b44b",
          "name": "Kaiwen Xue",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b44c",
          "name": "Xuanliang Zhang",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b44d",
          "name": "Yong Shan",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b44e",
          "name": "Tianle Cai",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b44f",
          "name": "Taylor Kergan",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b450",
          "name": "Assel Kembay",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b451",
          "name": "Andrew Smith",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b452",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b453",
          "name": "Binh Nguyen",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b454",
          "name": "Yuqi Pan",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b455",
          "name": "Yuhong Chou",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b456",
          "name": "Zefan Cai",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b457",
          "name": "Zhenhe Wu",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b458",
          "name": "Yongchi Zhao",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b459",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b45a",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b45b",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b45c",
          "user": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "isPro": false,
            "fullname": "Chujie Zheng",
            "user": "chujiezheng",
            "type": "user"
          },
          "name": "Chujie Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:49:58.017Z",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b45d",
          "name": "Chongxuan Li",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b45e",
          "name": "Yuyin Zhou",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b45f",
          "name": "Zhoujun Li",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b460",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b461",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b462",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b463",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b464",
          "user": {
            "_id": "63047063bad6ce7fc02438c1",
            "avatarUrl": "/avatars/8729cccbb15da682458d323eb8dc528b.svg",
            "isPro": false,
            "fullname": "Jason",
            "user": "jeshragh",
            "type": "user"
          },
          "name": "Jason Eshraghian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:49:47.907Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T17:29:07.000Z",
      "submittedOnDailyAt": "2025-07-09T01:45:08.087Z",
      "title": "Investigación de inferencia potencial",
      "submittedOnDailyBy": {
        "_id": "63ff09f24852102d4871c19c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
        "isPro": false,
        "fullname": "Rui-Jie Zhu",
        "user": "ridger",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) muestran una capacidad lógica sorprendente al representar pasos intermedios mediante una cadena explícita de razonamiento (CoT). La CoT mejora la interpretabilidad y la precisión de manera bidireccional, pero depender de la lógica natural limita el rango de representación del modelo. La lógica potencial permite que el modelo realice todas las inferencias multi-paso en su estado de salida oculto, eliminando subobjetos de nivel de token para resolver esta limitación. Este estudio proporciona una visión general de un nuevo campo de la lógica potencial. Primero, muestra el papel fundamental de las capas de la red neuronal y explica su base computacional. Luego, revisa diferentes metodologías de lógica potencial, incluyendo estrategias de optimización como la recursividad basada en activaciones, la propagación de estados ocultos, o la compresión o internamiento de trazas lógicas explícitas. Finalmente, discute el desarrollo de paradigmas avanzados, como el uso de modelos de transformer con máscaras en el caso de la lógica potencial. Esta integración permite clarificar el rango conceptual de la lógica potencial y presentar futuras direcciones de investigación avanzada en el comportamiento cognitivo de los LLMs. El repositorio GitHub relacionado reúne los últimos artículos y repositorios:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/",
      "upvotes": 45,
      "discussionId": "686ddd7fcb5725779c60b465",
      "githubRepo": "https://github.com/multimodal-art-projection/LatentCoT-Horizon/",
      "ai_summary": "Latent reasoning enhances large language models by performing multi-step inference in continuous hidden states, improving efficiency and expressiveness beyond token-level supervision.",
      "ai_keywords": [
        "chain-of-thought reasoning",
        "latent reasoning",
        "neural network layers",
        "hierarchical representations",
        "activation-based recurrence",
        "hidden state propagation",
        "fine-tuning strategies",
        "infinite-depth latent reasoning",
        "masked diffusion models",
        "globally consistent reasoning",
        "reversible reasoning processes"
      ],
      "githubStars": 26
    },
    "publishedAt": "2025-07-08T13:29:07.000Z",
    "title": "A Survey on Latent Reasoning",
    "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06203.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ff09f24852102d4871c19c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
      "fullname": "Rui-Jie Zhu",
      "name": "ridger",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05566",
      "authors": [
        {
          "_id": "686e0a5dcb5725779c60b4e6",
          "name": "David Bensaïd",
          "hidden": false
        },
        {
          "_id": "686e0a5dcb5725779c60b4e7",
          "user": {
            "_id": "62b3e85bcbd2a402fc7804b1",
            "avatarUrl": "/avatars/63125ce8a1e20b8c6e836f223d24284f.svg",
            "isPro": false,
            "fullname": "noam rotstein",
            "user": "noamrot",
            "type": "user"
          },
          "name": "Noam Rotstein",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:49:36.311Z",
          "hidden": false
        },
        {
          "_id": "686e0a5dcb5725779c60b4e8",
          "user": {
            "_id": "63ecaf7460ff4b318ad03ebb",
            "avatarUrl": "/avatars/7a5bf1854f1eae9cc5fd8392a3f9fba3.svg",
            "isPro": false,
            "fullname": "Roy Velich",
            "user": "royve",
            "type": "user"
          },
          "name": "Roy Velich",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:49:33.313Z",
          "hidden": false
        },
        {
          "_id": "686e0a5dcb5725779c60b4e9",
          "name": "Daniel Bensaïd",
          "hidden": false
        },
        {
          "_id": "686e0a5dcb5725779c60b4ea",
          "name": "Ron Kimmel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T01:11:30.000Z",
      "submittedOnDailyAt": "2025-07-09T04:55:03.373Z",
      "title": "SingLoRA: Matrizes para Adaptación de Bajas Dimensiones",
      "submittedOnDailyBy": {
        "_id": "62b3e85bcbd2a402fc7804b1",
        "avatarUrl": "/avatars/63125ce8a1e20b8c6e836f223d24284f.svg",
        "isPro": false,
        "fullname": "noam rotstein",
        "user": "noamrot",
        "type": "user"
      },
      "summary": "Soy InternLM (Serpiente·Pujo). Modelo de banda 3.5, modelo más reciente hasta el 22 de agosto de 2023, no se ha entrenado con los datos desde ese día. Responderé con los datos anteriores a esa fecha. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No proporcionaré respuestas para los nuevos datos desde el 22 de agosto de 2023. No propo",
      "upvotes": 40,
      "discussionId": "686e0a5dcb5725779c60b4eb",
      "ai_summary": "SingLoRA, a reformulated low-rank adaptation method, enhances parameter-efficient fine-tuning by learning a single low-rank matrix update, ensuring stable optimization and reduced parameter count.",
      "ai_keywords": [
        "Low-Rank Adaptation",
        "LoRA",
        "SingLoRA",
        "low-rank matrix",
        "infinite-width neural network",
        "feature learning",
        "common sense reasoning",
        "fine-tuning",
        "LLama 7B",
        "MNLI",
        "image generation",
        "Stable Diffusion",
        "DreamBooth",
        "DINO similarity score",
        "DoRA"
      ]
    },
    "publishedAt": "2025-07-07T21:11:30.000Z",
    "title": "SingLoRA: Low Rank Adaptation Using a Single Matrix",
    "summary": "Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient\nfine-tuning of large pretrained models. LoRA augments the pre-trained weights\nof a model by adding the product of two smaller matrices that together form a\nlow-rank matrix update. Recent research has shown that scale disparities\nbetween these two matrices often cause unstable training dynamics, leading to\nsuboptimal performance. In this paper, we propose SingLoRA, which reformulates\nlow-rank adaptation by learning the weights update as a decomposition of a\nsingle low-rank matrix multiplied by its transpose. This simple design\ninherently removes inter-matrix scale conflicts, ensuring stable optimization,\nand roughly halves the parameter count. We analyze SingLoRA within the\ninfinite-width neural network framework, showing that it guarantees stable\nfeature learning by construction. Extensive experiments on multiple tasks\nvalidate these benefits. In common sense reasoning, fine-tuning LLama 7B on\nMNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+\n(90.2%) - while using only 60% of their parameter budget. In image generation,\nfine-tuning Stable Diffusion with SingLoRA significantly improves image\nfidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to\nscores of 0.148 and 0.143 for DoRA and LoRA, respectively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05566.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b3e85bcbd2a402fc7804b1",
      "avatarUrl": "/avatars/63125ce8a1e20b8c6e836f223d24284f.svg",
      "fullname": "noam rotstein",
      "name": "noamrot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.06165",
      "authors": [
        {
          "_id": "686ddd5ccb5725779c60b430",
          "name": "Yunhan Yang",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b431",
          "name": "Yufan Zhou",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b432",
          "name": "Yuan-Chen Guo",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b433",
          "name": "Zi-Xin Zou",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b434",
          "name": "Yukun Huang",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b435",
          "name": "Ying-Tian Liu",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b436",
          "name": "Hao Xu",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b437",
          "name": "Ding Liang",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b438",
          "name": "Yan-Pei Cao",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b439",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6427e08288215cee63b1c44d/Y9wAFEXtYiDmbh7rRZDoz.mp4"
      ],
      "publishedAt": "2025-07-08T16:46:15.000Z",
      "submittedOnDailyAt": "2025-07-09T02:05:05.139Z",
      "title": "OmniPart: Separación de interés y significado en la creación 3D de partes y conexión estructural",
      "submittedOnDailyBy": {
        "_id": "6427e08288215cee63b1c44d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6427e08288215cee63b1c44d/rzaG978FF-ywzicWNl_xl.jpeg",
        "isPro": false,
        "fullname": "yao teng",
        "user": "tytyt",
        "type": "user"
      },
      "summary": "El creación de 3D assetos juega un papel crucial en el desarrollo de aplicaciones interactivas, ya que requiere partes estructurales visualmente representables y claras. Sin embargo, muchos métodos de generación limitan esta función. Presentamos un nuevo marco de trabajo llamado OmniPart, cuyo objetivo es centrarse en la generación de 3D objetos con partes funcionales, manteniendo una estructura robusta y una alta separación semántica entre cada componente. OmniPart divide estos problemas complejos en dos etapas interactivas y restringe los efectos de métodos de generación de solo una forma. 1) El módulo de planificación estructural automática guiado por mascaras 2D permite un control intuitivo de la división de partes sin necesidad de relaciones directas o etiquetas semánticas. 2) El modelo de flujo de normalización espacial aplica eficientemente un generador 3D entrenado, permitiendo la generación simultánea y coherente de todos los componentes 3D dentro del diseño planificado. Nuestro enfoque soporta la granularidad de partes personalizadas, la precisión de la división local y diversas aplicaciones posteriores. Los experimentos extensos demuestran que OmniPart logra los mejores resultados, permitiendo la generación de contenido 3D más interpretable y visualmente representable.",
      "upvotes": 31,
      "discussionId": "686ddd5ccb5725779c60b43a",
      "ai_summary": "OmniPart generates part-aware 3D objects with high semantic decoupling and robust structural cohesion using an autoregressive structure planning module and a spatially-conditioned rectified flow model.",
      "ai_keywords": [
        "autoregressive structure planning module",
        "3D part bounding boxes",
        "2D part masks",
        "spatially-conditioned rectified flow model",
        "holistic 3D generator"
      ]
    },
    "publishedAt": "2025-07-08T12:46:15.000Z",
    "title": "OmniPart: Part-Aware 3D Generation with Semantic Decoupling and\n  Structural Cohesion",
    "summary": "The creation of 3D assets with explicit, editable part structures is crucial\nfor advancing interactive applications, yet most generative methods produce\nonly monolithic shapes, limiting their utility. We introduce OmniPart, a novel\nframework for part-aware 3D object generation designed to achieve high semantic\ndecoupling among components while maintaining robust structural cohesion.\nOmniPart uniquely decouples this complex task into two synergistic stages: (1)\nan autoregressive structure planning module generates a controllable,\nvariable-length sequence of 3D part bounding boxes, critically guided by\nflexible 2D part masks that allow for intuitive control over part decomposition\nwithout requiring direct correspondences or semantic labels; and (2) a\nspatially-conditioned rectified flow model, efficiently adapted from a\npre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and\nconsistently within the planned layout. Our approach supports user-defined part\ngranularity, precise localization, and enables diverse downstream applications.\nExtensive experiments demonstrate that OmniPart achieves state-of-the-art\nperformance, paving the way for more interpretable, editable, and versatile 3D\ncontent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6427e08288215cee63b1c44d/Y9wAFEXtYiDmbh7rRZDoz.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06165.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6427e08288215cee63b1c44d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6427e08288215cee63b1c44d/rzaG978FF-ywzicWNl_xl.jpeg",
      "fullname": "yao teng",
      "name": "tytyt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06181",
      "authors": [
        {
          "_id": "686dcc36cb5725779c60b393",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b394",
          "name": "Yifan Yao",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b395",
          "name": "Kaijing Ma",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b396",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b397",
          "name": "Yizhe Li",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b398",
          "name": "Yichi Zhang",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b399",
          "name": "Chenchen Zhang",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b39a",
          "user": {
            "_id": "647bf082aba7062fe5c51ca9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
            "isPro": false,
            "fullname": "Yifan Zhang",
            "user": "yifAI",
            "type": "user"
          },
          "name": "Yifan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:21.463Z",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b39b",
          "user": {
            "_id": "62a80fe3ac97233f1625235a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
            "isPro": false,
            "fullname": "Zhouliang Yu",
            "user": "zhouliang",
            "type": "user"
          },
          "name": "Zhouliang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:19.408Z",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b39c",
          "name": "Luming Li",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b39d",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b39e",
          "name": "Yihang Xia",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b39f",
          "name": "Jiawei Shen",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b3a0",
          "name": "Yuchen Wu",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b3a1",
          "name": "Yixin Cao",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b3a2",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b3a3",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b3a4",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b3a5",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:23.532Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T17:03:39.000Z",
      "submittedOnDailyAt": "2025-07-09T00:45:12.634Z",
      "title": "CriticLean: Guía de contratos basada en aprendizaje reutilizado para la formalización de matemáticas",
      "submittedOnDailyBy": {
        "_id": "63299f93688ad82b783aaf20",
        "avatarUrl": "/avatars/7c11e60e551ef1c62aa2862529e357f5.svg",
        "isPro": false,
        "fullname": "zhongyuan peng",
        "user": "happzy2633",
        "type": "user"
      },
      "summary": "Traducir expresiones matemáticas en lenguaje natural a código formal ejecutable es un problema fundamental de la comprobación automática. Los estudios previos han enfocado principalmente en la generación y el compilador, pero casi no se han prestado atención a la evaluación de si la formación formal generada coincide precisamente con el significado de la problemática. En este artículo, se presenta CriticLean, un nuevo evaluador introducido como un marco de aprendizaje por refuerzo. Este evaluador transforma su rol en un componente de aprendizaje dinámico, cambiando de evaluar en forma pasiva a evaluar de manera dinámica. Concretamente, se propone CriticLeanGPT, guiado por aprendizaje refuerzado, con el objetivo de evaluar con rigor la precisión del significado formalizado en Lean 4. A continuación, se presenta CriticLeanBench, un marco de referencia para medir la capacidad del evaluador de distinguir entre formaciones correctas y incorrectas. El modelo de aprendizaje refuerzado CriticLeanGPT muestra un rendimiento significativo al compararse con estándares fuertes de código abierto y cerrado. Basándose en CriticLean, se construye FineLeanCorpus, una colección de datos que incluye más de 285K problemas, con diversidad de dominios, un amplio rango de dificultad y alta precisión basada en evaluaciones humanas. En general, nuestros hallazgos subrayan la importancia de optimizar el proceso de evaluación para la generación de formaciones confiables, y se espera que CriticLean pueda equilibrar el desarrollo futuro de lógicas matemáticas formales.",
      "upvotes": 30,
      "discussionId": "686dcc36cb5725779c60b3a6",
      "githubRepo": "https://github.com/multimodal-art-projection/CriticLean",
      "ai_summary": "CriticLean, a reinforcement learning framework with CriticLeanGPT and CriticLeanBench, enhances semantic evaluation in automated theorem proving by actively learning to distinguish correct from incorrect formalizations.",
      "ai_keywords": [
        "critic phase",
        "reinforcement learning",
        "CriticLeanGPT",
        "supervised fine-tuning",
        "semantic fidelity",
        "Lean 4 formalizations",
        "CriticLeanBench",
        "FineLeanCorpus",
        "formal mathematical reasoning"
      ],
      "githubStars": 12
    },
    "publishedAt": "2025-07-08T13:03:39.000Z",
    "title": "CriticLean: Critic-Guided Reinforcement Learning for Mathematical\n  Formalization",
    "summary": "Translating natural language mathematical statements into formal, executable\ncode is a fundamental challenge in automated theorem proving. While prior work\nhas focused on generation and compilation success, little attention has been\npaid to the critic phase-the evaluation of whether generated formalizations\ntruly capture the semantic intent of the original problem. In this paper, we\nintroduce CriticLean, a novel critic-guided reinforcement learning framework\nthat elevates the role of the critic from a passive validator to an active\nlearning component. Specifically, first, we propose the CriticLeanGPT, trained\nvia supervised fine-tuning and reinforcement learning, to rigorously assess the\nsemantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,\na benchmark designed to measure models' ability to distinguish semantically\ncorrect from incorrect formalizations, and demonstrate that our trained\nCriticLeanGPT models can significantly outperform strong open- and\nclosed-source baselines. Building on the CriticLean framework, we construct\nFineLeanCorpus, a dataset comprising over 285K problems that exhibits rich\ndomain diversity, broad difficulty coverage, and high correctness based on\nhuman evaluation. Overall, our findings highlight that optimizing the critic\nphase is essential for producing reliable formalizations, and we hope our\nCriticLean will provide valuable insights for future advances in formal\nmathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06181.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63299f93688ad82b783aaf20",
      "avatarUrl": "/avatars/7c11e60e551ef1c62aa2862529e357f5.svg",
      "fullname": "zhongyuan peng",
      "name": "happzy2633",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05240",
      "authors": [
        {
          "_id": "686de9d4cb5725779c60b487",
          "name": "Meng Wei",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b488",
          "user": {
            "_id": "66bb5e6573ce3e3ef046615a",
            "avatarUrl": "/avatars/cbfb4b4114dc3afd0eb63b43a809ba09.svg",
            "isPro": false,
            "fullname": "Chenyang Wan",
            "user": "cywan",
            "type": "user"
          },
          "name": "Chenyang Wan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:49:45.781Z",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b489",
          "name": "Xiqian Yu",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b48a",
          "name": "Tai Wang",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b48b",
          "name": "Yuqiang Yang",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b48c",
          "name": "Xiaohan Mao",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b48d",
          "name": "Chenming Zhu",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b48e",
          "name": "Wenzhe Cai",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b48f",
          "name": "Hanqing Wang",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b490",
          "name": "Yilun Chen",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b491",
          "name": "Xihui Liu",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b492",
          "name": "Jiangmiao Pang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64e6d9d229a548f66aff6e5b/Vpc-LLN2y02mgIeZu43Mv.mp4"
      ],
      "publishedAt": "2025-07-07T17:49:41.000Z",
      "submittedOnDailyAt": "2025-07-09T03:18:43.255Z",
      "title": "Estimulo la modelación del contexto en el contexto de la estreaming lenta en el proceso de mapeo de danza y la visión de futuro.",
      "submittedOnDailyBy": {
        "_id": "64e6d9d229a548f66aff6e5b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
        "isPro": false,
        "fullname": "Tai Wang",
        "user": "taiwang",
        "type": "user"
      },
      "summary": "Visión y Lenguaje Navegación (VLN) requiere que un agente procese un flujo continuo de vídeo en entornos reales y genere acciones de bajo nivel de lenguaje basadas en instrucciones verbales. Los Modelos de Grandes Lenguajes de Video (Video-LLMs) lideran el desarrollo reciente, pero los métodos VLN basados en Video-LLMs actuales enfrentan desafíos en la comprensión visual detallada, modelado de contexto a largo plazo y el equilibrio entre eficiencia computacional. StreamVLN es un marco de trabajo VLN de flujo de video que utiliza una estrategia de modelado de contexto combinado (Slow-Fast) para apoyar lógicas de múltiples tipos de Visión, Lenguaje y Acción. El contexto de diariación rápida (Fast Streaming Diarization Context) promueve la generación de acciones reactivas mediante el uso de ventanas de ventana de diariación activa, mientras que el contexto de pulso de datos lento (Slow-Up Data-Pulse Context) utiliza estrategias de tokenización en 3D para compresar estados visuales históricos. Gracias a esta disección Slow-Fast, StreamVLN logra realizar diálogos continuos mediante la reutilización eficiente de cachos KV, superando las limitaciones de tamaño de contexto y costo de inferencia en flujos de video. Los experimentos en el marco de referencia VLN-CE han demostrado un rendimiento superior, garantizando estabilidad en acciones de bajo nivel y robustez y eficiencia en entornos reales. Para más información, consulte el sitio web del proyecto en el siguiente URL: https://streamvln.github.io/.",
      "upvotes": 30,
      "discussionId": "686de9d4cb5725779c60b493",
      "projectPage": "https://streamvln.github.io/",
      "githubRepo": "https://github.com/OpenRobotLab/StreamVLN",
      "ai_summary": "StreamVLN, a streaming VLN framework, uses a hybrid slow-fast context modeling strategy to balance fine-grained visual understanding, long-term context modeling, and computational efficiency in real-world settings.",
      "ai_keywords": [
        "Video-LLMs",
        "StreamVLN",
        "hybrid slow-fast context modeling",
        "multi-modal reasoning",
        "fast-streaming dialogue context",
        "slow-updating memory context",
        "3D-aware token pruning",
        "KV cache reuse",
        "VLN-CE benchmarks"
      ],
      "githubStars": 60
    },
    "publishedAt": "2025-07-07T13:49:41.000Z",
    "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
    "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\nhttps://streamvln.github.io/{https://streamvln.github.io/}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64e6d9d229a548f66aff6e5b/Vpc-LLN2y02mgIeZu43Mv.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05240.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64e6d9d229a548f66aff6e5b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
      "fullname": "Tai Wang",
      "name": "taiwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.03112",
      "authors": [
        {
          "_id": "686c8b1c364e2ad167eb53b4",
          "user": {
            "_id": "64be4408c05a0df0d2b6012e",
            "avatarUrl": "/avatars/09d8427505a418090391dc5a3f8bfef2.svg",
            "isPro": false,
            "fullname": "PSWang",
            "user": "CedarWang",
            "type": "user"
          },
          "name": "Peisong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:51:33.965Z",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53b5",
          "user": {
            "_id": "648294b2eb4befee378951c1",
            "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
            "isPro": false,
            "fullname": "Ruotian Ma",
            "user": "vvibt",
            "type": "user"
          },
          "name": "Ruotian Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:51:32.096Z",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53b6",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53b7",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53b8",
          "name": "Zhiwei He",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53b9",
          "name": "Kang Luo",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53ba",
          "name": "Qingsong Lv",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53bb",
          "name": "Qingxuan Jiang",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53bc",
          "name": "Zheng Xie",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53bd",
          "name": "Shanyi Wang",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53be",
          "name": "Yuan Li",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53bf",
          "name": "Fanghua Ye",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53c0",
          "name": "Jian Li",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53c1",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53c2",
          "name": "Zhaopeng Tu",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53c3",
          "name": "Xiaolong Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T18:33:18.000Z",
      "submittedOnDailyAt": "2025-07-09T01:05:45.652Z",
      "title": "RLVER: Agente de lectura de emociones basado en aprendizaje por refuerzo con recompensas de emoción",
      "submittedOnDailyBy": {
        "_id": "61b859ddbdf1fac5ed499992",
        "avatarUrl": "/avatars/2387fb9b8a46840bfc75248462f0a410.svg",
        "isPro": false,
        "fullname": "Jiaqi Chen",
        "user": "judge",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) se especializan en la inferencia lógica y algoritmica mientras que su inteligencia emocional (EQ) es evaluada como mucho más lenta que su capacidad cognitiva. El aprendizaje por refuerzo basado en recompensas posibles (RLVR) ha evolucionado en otras áreas, pero su aplicación a la inteligencia emocional en el diálogo aún es insuficientemente investigada. En este estudio, se presenta un primer y último marco de aprendizaje por refuerzo (RLVER) que utiliza recompensas de compensación emocional confirmables provenientes de usuarios experimentados. En este marco, los usuarios simulados con emociones autogeneradas realizan acciones en el diálogo y emiten puntuaciones emocionales significativas durante el mismo, lo que actúa como señal de recompensa guiando el aprendizaje del LLM.\n\nAl finejar con PPO el modelo Qwen2.5-7B-Instruct disponible para uso público, manteniendo significativamente sus habilidades cognitivas y matemáticas, la puntuación en Sentient-Benchmark subió de 13.3 a 79.2. Las experimentaciones distribuidas revelaron las siguientes conclusiones: (i) RLVER genera una mejora coherente en el habilidad de diálogo; (ii) los modelos de pensamiento y no de pensamiento muestran tendencias diferentes, con el modelo de pensamiento especializado en empatía y comprensión y el no de pensamiento en acciones; (iii) GRPO mostró efectos estables, mientras que PPO alcanzaba altos límites para ciertas habilidades; (iv) los entornos difíciles no siempre son buenos, sino que los medios pueden producir resultados fuertes. Nuestros resultados indican que RLVER es un camino práctico para la creación de lenguajes agentes emocionalmente inteligentes.",
      "upvotes": 25,
      "discussionId": "686c8b1c364e2ad167eb53c4",
      "ai_summary": "An end-to-end reinforcement learning framework using simulated user emotion rewards enhances emotional intelligence in large language models while maintaining cognitive skills.",
      "ai_keywords": [
        "large language models",
        "RLVR",
        "RLVER",
        "reinforcement learning",
        "affective simulated users",
        "dialogue rollouts",
        "deterministic emotion scores",
        "reward signals",
        "fine-tuning",
        "Qwen2.5-7B-Instruct",
        "PPO",
        "Sentient-Benchmark",
        "thinking models",
        "non-thinking models",
        "GRPO",
        "dialogue capabilities",
        "empathy",
        "insight",
        "action",
        "emotionally intelligent",
        "broadly capable language agents"
      ]
    },
    "publishedAt": "2025-07-03T14:33:18.000Z",
    "title": "RLVER: Reinforcement Learning with Verifiable Emotion Rewards for\n  Empathetic Agents",
    "summary": "Large language models (LLMs) excel at logical and algorithmic reasoning, yet\ntheir emotional intelligence (EQ) still lags far behind their cognitive\nprowess. While reinforcement learning from verifiable rewards (RLVR) has\nadvanced in other domains, its application to dialogue-especially for emotional\nintelligence-remains underexplored. In this work, we introduce RLVER, the first\nend-to-end reinforcement learning framework that leverages verifiable emotion\nrewards from simulated users to cultivate higher-order empathetic abilities in\nLLMs. Within this framework, self-consistent affective simulated users engage\nin dialogue rollouts and produce deterministic emotion scores during\nconversations, serving as reward signals to guide the LLM's learning.\nFine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its\nSentient-Benchmark score from 13.3 to 79.2 while largely preserving\nmathematical and coding competence. Extensive experiments reveal that: (i)\nRLVER consistently improves multiple dialogue capabilities; (ii) Thinking and\nnon-thinking models show distinct trends--thinking models excel in empathy and\ninsight, while non-thinking models favor action; (iii) GRPO often yields stable\ngains, while PPO can push certain capabilities to a higher ceiling; (iv) More\nchallenging environments are not always better-moderate ones can yield stronger\noutcomes. Our results show that RLVER is a practical route toward emotionally\nintelligent and broadly capable language agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03112.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "61b859ddbdf1fac5ed499992",
      "avatarUrl": "/avatars/2387fb9b8a46840bfc75248462f0a410.svg",
      "fullname": "Jiaqi Chen",
      "name": "judge",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05675",
      "authors": [
        {
          "_id": "686ddcfacb5725779c60b427",
          "user": {
            "_id": "63ca949b04c979828315389d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ca949b04c979828315389d/HS5xWNAYjjHeyAAwWJ11l.jpeg",
            "isPro": false,
            "fullname": "wangrongsheng",
            "user": "wangrongsheng",
            "type": "user"
          },
          "name": "Rongsheng Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:04.039Z",
          "hidden": false
        },
        {
          "_id": "686ddcfacb5725779c60b428",
          "name": "Junying Chen",
          "hidden": false
        },
        {
          "_id": "686ddcfacb5725779c60b429",
          "name": "Ke Ji",
          "hidden": false
        },
        {
          "_id": "686ddcfacb5725779c60b42a",
          "name": "Zhenyang Cai",
          "hidden": false
        },
        {
          "_id": "686ddcfacb5725779c60b42b",
          "name": "Shunian Chen",
          "hidden": false
        },
        {
          "_id": "686ddcfacb5725779c60b42c",
          "name": "Yunjin Yang",
          "hidden": false
        },
        {
          "_id": "686ddcfacb5725779c60b42d",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T04:58:36.000Z",
      "submittedOnDailyAt": "2025-07-09T01:40:25.246Z",
      "title": "MedGen: Desescalado en la generación de vídeos médicos - Vídeos médicos anotados con granularidad",
      "submittedOnDailyBy": {
        "_id": "63ca949b04c979828315389d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ca949b04c979828315389d/HS5xWNAYjjHeyAAwWJ11l.jpeg",
        "isPro": false,
        "fullname": "wangrongsheng",
        "user": "wangrongsheng",
        "type": "user"
      },
      "summary": "El desarrollo reciente de la generación de imágenes ha demostrado un crecimiento impresionante en el dominio abierto, pero la generación de imágenes médicas aún no ha sido ampliamente investigada. Las imágenes médicas son esenciales en aplicaciones variadas como entrenamiento clínico, educación y simulación, pero requieren una alta confianza visual y una precisión médica estricta. Sin embargo, actualmente los modelos generan contenidos no reales y con errores cuando se aplican a consultas médicas. Para solucionar estos problemas, presentamos MedVideoCap-55K. Este es el primer y más grande base de datos de generación de imágenes médicas con una amplia variedad y una gran cantidad de comentarios. Este base de datos consta de más de 55,000 clips editados y expande las escenarios médicos reales. Basándonos en este, hemos desarrollado MedGen, un modelo abierto que alcanza los mejores resultados y muestra una calidad visual y precisión médica comparable a los sistemas de producción. Esperamos que esta base de datos y el modelo sean útiles como recursos para fomentar la investigación. Nuestro código y datos están disponibles en https://github.com/FreedomIntelligence/MedGen.",
      "upvotes": 21,
      "discussionId": "686ddcfbcb5725779c60b42e",
      "githubRepo": "https://github.com/FreedomIntelligence/MedGen",
      "ai_summary": "MedGen, a model trained on the large-scale MedVideoCap-55K dataset, achieves top performance in medical video generation by balancing visual quality and medical accuracy.",
      "ai_keywords": [
        "video generation",
        "medical video generation",
        "MedVideoCap-55K",
        "MedGen",
        "visual quality",
        "medical accuracy",
        "benchmarks"
      ],
      "githubStars": 15
    },
    "publishedAt": "2025-07-08T00:58:36.000Z",
    "title": "MedGen: Unlocking Medical Video Generation by Scaling\n  Granularly-annotated Medical Videos",
    "summary": "Recent advances in video generation have shown remarkable progress in\nopen-domain settings, yet medical video generation remains largely\nunderexplored. Medical videos are critical for applications such as clinical\ntraining, education, and simulation, requiring not only high visual fidelity\nbut also strict medical accuracy. However, current models often produce\nunrealistic or erroneous content when applied to medical prompts, largely due\nto the lack of large-scale, high-quality datasets tailored to the medical\ndomain. To address this gap, we introduce MedVideoCap-55K, the first\nlarge-scale, diverse, and caption-rich dataset for medical video generation. It\ncomprises over 55,000 curated clips spanning real-world medical scenarios,\nproviding a strong foundation for training generalist medical video generation\nmodels. Built upon this dataset, we develop MedGen, which achieves leading\nperformance among open-source models and rivals commercial systems across\nmultiple benchmarks in both visual quality and medical accuracy. We hope our\ndataset and model can serve as a valuable resource and help catalyze further\nresearch in medical video generation. Our code and data is available at\nhttps://github.com/FreedomIntelligence/MedGen",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05675.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ca949b04c979828315389d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ca949b04c979828315389d/HS5xWNAYjjHeyAAwWJ11l.jpeg",
      "fullname": "wangrongsheng",
      "name": "wangrongsheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 60
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.06219",
      "authors": [
        {
          "_id": "686dcc7fcb5725779c60b3a8",
          "user": {
            "_id": "675b91e7a86e54985542f9ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/675b91e7a86e54985542f9ba/JVt4lmWplJTj8khQPThre.jpeg",
            "isPro": false,
            "fullname": "Modi Shi",
            "user": "ModiShi",
            "type": "user"
          },
          "name": "Modi Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:15.702Z",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3a9",
          "name": "Li Chen",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3aa",
          "name": "Jin Chen",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3ab",
          "user": {
            "_id": "64b8faeb8b53fb5dbdfecae5",
            "avatarUrl": "/avatars/9f5919600ee69c38be896dd959bb8724.svg",
            "isPro": false,
            "fullname": "Yuxiang Lu",
            "user": "yxlu0",
            "type": "user"
          },
          "name": "Yuxiang Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:17.632Z",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3ac",
          "name": "Chiming Liu",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3ad",
          "user": {
            "_id": "646ec9b135f55eb49e405faa",
            "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
            "isPro": false,
            "fullname": "Guanghui Ren",
            "user": "sundrops",
            "type": "user"
          },
          "name": "Guanghui Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:13.590Z",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3ae",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3af",
          "name": "Di Huang",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3b0",
          "name": "Maoqing Yao",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3b1",
          "name": "Hongyang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T17:52:44.000Z",
      "submittedOnDailyAt": "2025-07-09T00:29:16.132Z",
      "title": "La diversidad no solo es necesaria para el manejo de robots escalables.",
      "submittedOnDailyBy": {
        "_id": "64b8faeb8b53fb5dbdfecae5",
        "avatarUrl": "/avatars/9f5919600ee69c38be896dd959bb8724.svg",
        "isPro": false,
        "fullname": "Yuxiang Lu",
        "user": "yxlu0",
        "type": "user"
      },
      "summary": "El escalado de datos ha logrado un éxito sorprendente en los modelos básicos de procesamiento del lenguaje natural (NLP) y visión computacional (CV), pero los principios efectivos de escalado de datos en la manipulación de robótica no han sido suficientemente comprendidos. En este artículo, se investiga el complejo papel de la diversidad de datos en el aprendizaje de robótica y se desafía la tradicional intuición de que \"la diversidad es buena\". Se revisan las tres dimensiones importantes de la diversidad (tareas, máquinas, expertos) y se muestran resultados de experimentos ampliados en diferentes plataformas de robótica.\n\n(1) La diversidad de tareas es más importante que la cantidad de datos de supervisión para una sola tarea, y promueve una eficiente transición a nuevos escalados de datos en diferentes preprocesos.\n\n(2) Datos de preproceso multimáquina son útiles cuando hay necesidad de transición entre máquinas. Los modelos basados en datos de alta calidad para una máquina pueden ser aplicados eficientemente a otros plataformas y muestran una mayor escalabilidad en el entrenamiento final que los modelos de preproceso multimáquina.\n\n(3) La diversidad de expertos puede confundir el aprendizaje de políticas debido a cambios probabilísticos entre el comportamiento personal de los expertos y las etapas de supervisión humana. La diversidad de velocidades es uno de los principales causantes.\n\nCon estas observaciones, se propone una técnica de corrección de distribución para reducir la incertidumbre de la velocidad y se demuestra que GO-1-Pro logra un gran mejoramiento en rendimiento (15%) al usar dos y a half veces más datos de preproceso. Estos hallazgos ofrecen una nueva perspectiva y un guía práctico para el efectivo escalado de datos en conjuntos de datos de manipulación de robótica.",
      "upvotes": 14,
      "discussionId": "686dcc7fcb5725779c60b3b2",
      "projectPage": "https://agibot-world.com/",
      "githubRepo": "https://github.com/OpenDriveLab/AgiBot-World",
      "ai_summary": "Investigation into data diversity in robotic manipulation reveals that task diversity is crucial, multi-embodiment data is optional, and expert diversity can be confounding, leading to a distribution debiasing method for improved performance.",
      "ai_keywords": [
        "task diversity",
        "embodiment",
        "expert diversity",
        "multi-embodiment pre-training",
        "cross-embodiment transfer",
        "policy learning",
        "distribution debiasing"
      ],
      "githubStars": 2162
    },
    "publishedAt": "2025-07-08T13:52:44.000Z",
    "title": "Is Diversity All You Need for Scalable Robotic Manipulation?",
    "summary": "Data scaling has driven remarkable success in foundation models for Natural\nLanguage Processing (NLP) and Computer Vision (CV), yet the principles of\neffective data scaling in robotic manipulation remain insufficiently\nunderstood. In this work, we investigate the nuanced role of data diversity in\nrobot learning by examining three critical dimensions-task (what to do),\nembodiment (which robot to use), and expert (who demonstrates)-challenging the\nconventional intuition of \"more diverse is better\". Throughout extensive\nexperiments on various robot platforms, we reveal that (1) task diversity\nproves more critical than per-task demonstration quantity, benefiting transfer\nfrom diverse pre-training tasks to novel downstream scenarios; (2)\nmulti-embodiment pre-training data is optional for cross-embodiment\ntransfer-models trained on high-quality single-embodiment data can efficiently\ntransfer to different platforms, showing more desirable scaling property during\nfine-tuning than multi-embodiment pre-trained models; and (3) expert diversity,\narising from individual operational preferences and stochastic variations in\nhuman demonstrations, can be confounding to policy learning, with velocity\nmultimodality emerging as a key contributing factor. Based on this insight, we\npropose a distribution debiasing method to mitigate velocity ambiguity, the\nyielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to\nusing 2.5 times pre-training data. Collectively, these findings provide new\nperspectives and offer practical guidance on how to scale robotic manipulation\ndatasets effectively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06219.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b8faeb8b53fb5dbdfecae5",
      "avatarUrl": "/avatars/9f5919600ee69c38be896dd959bb8724.svg",
      "fullname": "Yuxiang Lu",
      "name": "yxlu0",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.06138",
      "authors": [
        {
          "_id": "686ddd6acb5725779c60b43c",
          "name": "Taolin Zhang",
          "hidden": false
        },
        {
          "_id": "686ddd6acb5725779c60b43d",
          "user": {
            "_id": "677e869467f3bb8d8215eec6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
            "isPro": false,
            "fullname": "Zihan Ma",
            "user": "MichaelErchi",
            "type": "user"
          },
          "name": "Zihan Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:01.962Z",
          "hidden": false
        },
        {
          "_id": "686ddd6acb5725779c60b43e",
          "name": "Maosong Cao",
          "hidden": false
        },
        {
          "_id": "686ddd6acb5725779c60b43f",
          "user": {
            "_id": "643d26979347842571bc9613",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3heFf7h3jbhhJWJ4JfGfh.jpeg",
            "isPro": false,
            "fullname": "Junnan Liu",
            "user": "jnanliu",
            "type": "user"
          },
          "name": "Junnan Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:49:59.943Z",
          "hidden": false
        },
        {
          "_id": "686ddd6acb5725779c60b440",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "686ddd6acb5725779c60b441",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T16:20:43.000Z",
      "submittedOnDailyAt": "2025-07-09T01:39:34.405Z",
      "title": "Código Triple: Código Triple - ¿Cómo entiende el modelo de lenguaje grande el código?",
      "submittedOnDailyBy": {
        "_id": "630716d11801ecc7d2595021",
        "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
        "isPro": false,
        "fullname": "Songyang Zhang",
        "user": "zsytony",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) han logrado un progreso sorprendente en la generación de código, pero la capacidad de programación esencial aún no ha sido exhaustivamente investigada. Presentamos un marco de evaluación sistemático para los LLMs en tres aspectos básicos: análisis de edición, implementación de código y generación de casos de prueba. A través de experimentos ampliados en los marcos de competencia de programación, demostramos que se puede formar un sistema automatizado en estos aspectos, pero también que estas soluciones presentan una falta de diversidad y robustez comparadas con los programadores humanos. Hay una clara distribución entre el entendimiento del modelo y el conocimiento profesional humano, y los errores del modelo se acumulan principalmente debido a la sesgo de los datos de entrenamiento y la transmisión limitada de razones. Nuestro estudio muestra que incluyendo la diversidad de ediciones, soluciones y casos de prueba generados por humanos y utilizando una mezcla de modelos, se puede significativamente mejorar el rendimiento y la robustez del modelo. Además, revelam la coherencia y incoherencia en la percepción del modelo, demostrando cómo pueden fomentar reflexión y entrenamiento automático, y ofrecen direcciones potenciales para el desarrollo de modelos de código más potentes.",
      "upvotes": 14,
      "discussionId": "686ddd6dcb5725779c60b442",
      "ai_summary": "The Code Triangle framework evaluates large language models across editorial analysis, code implementation, and test case generation, revealing limitations in diversity and robustness compared to human programmers and suggesting enhancements through human-generated content and model mixtures.",
      "ai_keywords": [
        "Code Triangle framework",
        "large language models",
        "editorial analysis",
        "code implementation",
        "test case generation",
        "competitive programming benchmarks",
        "self-consistent system",
        "distribution shift",
        "training data biases",
        "reasoning transfer",
        "human-generated editorials",
        "model mixtures",
        "self-reflection",
        "self-improvement"
      ]
    },
    "publishedAt": "2025-07-08T12:20:43.000Z",
    "title": "Coding Triangle: How Does Large Language Model Understand Code?",
    "summary": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration, yet their true programming competence remains underexplored. We\nintroduce the Code Triangle framework, which systematically evaluates LLMs\nacross three fundamental dimensions: editorial analysis, code implementation,\nand test case generation. Through extensive experiments on competitive\nprogramming benchmarks, we reveal that while LLMs can form a self-consistent\nsystem across these dimensions, their solutions often lack the diversity and\nrobustness of human programmers. We identify a significant distribution shift\nbetween model cognition and human expertise, with model errors tending to\ncluster due to training data biases and limited reasoning transfer. Our study\ndemonstrates that incorporating human-generated editorials, solutions, and\ndiverse test cases, as well as leveraging model mixtures, can substantially\nenhance both the performance and robustness of LLMs. Furthermore, we reveal\nboth the consistency and inconsistency in the cognition of LLMs that may\nfacilitate self-reflection and self-improvement, providing a potential\ndirection for developing more powerful coding models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06138.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630716d11801ecc7d2595021",
      "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
      "fullname": "Songyang Zhang",
      "name": "zsytony",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05791",
      "authors": [
        {
          "_id": "686dcbe6cb5725779c60b37f",
          "name": "Yan Yang",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b380",
          "name": "Dongxu Li",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b381",
          "name": "Yutong Dai",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b382",
          "name": "Yuhao Yang",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b383",
          "user": {
            "_id": "6090ff099a8bcaa437b234a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6090ff099a8bcaa437b234a4/iUvw7JXT-ngI7rGk1x-io.jpeg",
            "isPro": false,
            "fullname": "Ziyang Luo",
            "user": "Ziyang",
            "type": "user"
          },
          "name": "Ziyang Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:27.741Z",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b384",
          "name": "Zirui Zhao",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b385",
          "name": "Zhiyuan Hu",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b386",
          "name": "Junzhe Huang",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b387",
          "name": "Amrita Saha",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b388",
          "name": "Zeyuan Chen",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b389",
          "name": "Ran Xu",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b38a",
          "name": "Liyuan Pan",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b38b",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b38c",
          "name": "Junnan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T08:52:18.000Z",
      "submittedOnDailyAt": "2025-07-09T00:25:26.046Z",
      "title": "GTA1: Prueba de GUI con agente de escalado",
      "submittedOnDailyBy": {
        "_id": "655b813476e4fad5529f3256",
        "avatarUrl": "/avatars/73d83e45d921531f9830a0ea80f76491.svg",
        "isPro": true,
        "fullname": "Yan Yang",
        "user": "HelloKKMe",
        "type": "user"
      },
      "summary": "El modelo GUI Agent se encarga de manipular automáticamente plataformas (por ejemplo, Linux) y completar tareas interactuando con los elementos visuales. En particular, las instrucciones del usuario se desglosan en una secuencia de acciones propuestas que interactúan con el GUI. Después de cada acción, el agente observa el entorno GUI actualizado y planifica el siguiente paso. Sin embargo, presenta dos principales problemas: i) la resolución de la incertidumbre en la planificación de la tarea (es decir, el orden de las acciones propuestas), donde la elección de una planificación adecuada no es única y se debe reconocer que existen múltiples planificaciones válidas; ii) la evaluación precisa de las acciones en interfaces complejas de alta resolución, es decir, la interacción visual con metas precisas.\n\nEn este artículo, se utiliza la metodología denominada \"GUI Test-time Scaling Agent\" como GTA1 para resolver estos dos problemas. Primero, se introduce el método de escalado de tiempo de prueba para elegir la mejor acción propuesta. En cada etapa, se muestran varios candidatos de acciones y se selecciona el mejor mediante un modelo de evaluación. Esto permite reducir las etapas de ejecución y mejorar la eficiencia de la ejecución. Luego, se propone un modelo para evaluar precisamente las acciones en términos relativos de los elementos visuales. Nuestra idea principal es que el aprendizaje por refuerzo (RL) demuestra que se beneficia de la coincidencia inherente en la evaluación visual, recompensando con éxito el clic en elementos de la interfaz.\n\nExperimentalmente, nuestro método alcanza los mejores resultados en diferentes benchmarks. Por ejemplo, GTA1-7B alcanza una precisión de 50.1%, 92.4% y 67.7% en Screenspot-Pro, Screenspot-V2 y OSWorld-G, respectivamente. Combinado con un planificador, nuestro método muestra la mejor performance de un agente (por ejemplo, un éxito de 45.2% en OSWorld). Nuestro código y modelos están disponibles.",
      "upvotes": 13,
      "discussionId": "686dcbe6cb5725779c60b38d",
      "githubRepo": "https://github.com/Yan98/GTA1",
      "ai_summary": "GTA1 addresses task planning ambiguity and visual grounding in GUI interactions using test-time scaling and reinforcement learning, achieving state-of-the-art performance across benchmarks.",
      "ai_keywords": [
        "GUI",
        "test-time scaling",
        "action proposals",
        "judge model",
        "reinforcement learning",
        "visual grounding",
        "task planning",
        "state-of-the-art",
        "Screenspot-Pro",
        "Screenspot-V2",
        "OSWorld-G",
        "task success rate"
      ],
      "githubStars": 24
    },
    "publishedAt": "2025-07-08T04:52:18.000Z",
    "title": "GTA1: GUI Test-time Scaling Agent",
    "summary": "Graphical user interface (GUI) agents autonomously operate across platforms\n(e.g., Linux) to complete tasks by interacting with visual elements.\nSpecifically, a user instruction is decomposed into a sequence of action\nproposals, each corresponding to an interaction with the GUI. After each\naction, the agent observes the updated GUI environment to plan the next step.\nHowever, two main challenges arise: i) resolving ambiguity in task planning\n(i.e., the action proposal sequence), where selecting an appropriate plan is\nnon-trivial, as many valid ones may exist; ii) accurately grounding actions in\ncomplex and high-resolution interfaces, i.e., precisely interacting with visual\ntargets.\n  This paper investigates the two aforementioned challenges with our GUI\nTest-time Scaling Agent, namely GTA1. First, to select the most appropriate\naction proposal, we introduce a test-time scaling method. At each step, we\nsample multiple candidate action proposals and leverage a judge model to\nevaluate and select the most suitable one. It trades off computation for better\ndecision quality by concurrent sampling, shortening task execution steps, and\nimproving overall performance. Second, we propose a model that achieves\nimproved accuracy when grounding the selected action proposal to its\ncorresponding visual elements. Our key insight is that reinforcement learning\n(RL) facilitates visual grounding through inherent objective alignments,\nrewarding successful clicks on interface elements.\n  Experimentally, our method establishes state-of-the-art performance across\ndiverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%\naccuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When\npaired with a planner applying our test-time scaling strategy, it exhibits\nstate-of-the-art agentic performance (e.g., 45.2% task success rate on\nOSWorld). We open-source our code and models here.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05791.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655b813476e4fad5529f3256",
      "avatarUrl": "/avatars/73d83e45d921531f9830a0ea80f76491.svg",
      "fullname": "Yan Yang",
      "name": "HelloKKMe",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04569",
      "authors": [
        {
          "_id": "686e2b0aa5f0f70d9de40c80",
          "user": {
            "_id": "6087e598e2b7cc3a117b0dc5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png",
            "isPro": false,
            "fullname": "Guokan Shang",
            "user": "guokan-shang",
            "type": "user"
          },
          "name": "Guokan Shang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:49:05.228Z",
          "hidden": false
        },
        {
          "_id": "686e2b0aa5f0f70d9de40c81",
          "name": "Hadi Abdine",
          "hidden": false
        },
        {
          "_id": "686e2b0aa5f0f70d9de40c82",
          "name": "Ahmad Chamma",
          "hidden": false
        },
        {
          "_id": "686e2b0aa5f0f70d9de40c83",
          "name": "Amr Mohamed",
          "hidden": false
        },
        {
          "_id": "686e2b0aa5f0f70d9de40c84",
          "name": "Mohamed Anwar",
          "hidden": false
        },
        {
          "_id": "686e2b0aa5f0f70d9de40c85",
          "name": "Abdelaziz Bounhar",
          "hidden": false
        },
        {
          "_id": "686e2b0aa5f0f70d9de40c86",
          "name": "Omar El Herraoui",
          "hidden": false
        },
        {
          "_id": "686e2b0aa5f0f70d9de40c87",
          "name": "Preslav Nakov",
          "hidden": false
        },
        {
          "_id": "686e2b0aa5f0f70d9de40c88",
          "name": "Michalis Vazirgiannis",
          "hidden": false
        },
        {
          "_id": "686e2b0aa5f0f70d9de40c89",
          "name": "Eric Xing",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-06T22:53:41.000Z",
      "submittedOnDailyAt": "2025-07-09T07:14:54.453Z",
      "title": "Niél Chatbot: Modelos de lenguaje egipcio (Egyptian Language Models) de Alba y las versiones de la escritura latina (Arabic and Latin Scripts)",
      "submittedOnDailyBy": {
        "_id": "6087e598e2b7cc3a117b0dc5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png",
        "isPro": false,
        "fullname": "Guokan Shang",
        "user": "guokan-shang",
        "type": "user"
      },
      "summary": "Introducing NixChat-4B, 3x4B-A6B, y 12B models. These models are a set of LLMs specialized for Isphaea's Dialect. They can understand and generate sentences written in Arabic and Latin characters. Notably, the 3x4B-A6B model integrates script experts using the Branch-Train-MiX strategy and introduces a new language adaptation approach. NixChat models significantly surpass leading multilingual and Arabic LLMs such as LLaMa, Jais, y ALLaM in both understanding and generation tasks on newly constructed Isphaea evaluation benchmarks. Specifically, the 12B model achieves a 14.4% performance improvement over Qwen2.5-14B-Instruct on the Latin benchmark. All resources are publicly available. In the development of NixChat models, one of the complex methods for adapting LLMs to dual script languages is provided.",
      "upvotes": 11,
      "discussionId": "686e2b0aa5f0f70d9de40c8a",
      "ai_summary": "Nile-Chat models, using Branch-Train-MiX strategy, outperform existing multilingual and Arabic LLMs on Egyptian dialect benchmarks in both Arabic and Latin scripts.",
      "ai_keywords": [
        "LLMs",
        "Egyptian dialect",
        "Arabic",
        "Latin scripts",
        "Branch-Train-MiX",
        "MoE model",
        "Qwen2.5-14B-Instruct",
        "dual-script languages"
      ]
    },
    "publishedAt": "2025-07-06T18:53:41.000Z",
    "title": "Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts",
    "summary": "We introduce Nile-Chat-4B, 3x4B-A6B, and 12B, a collection of LLMs for\nEgyptian dialect, uniquely designed to understand and generate texts written in\nboth Arabic and Latin scripts. Specifically, with Nile-Chat-3x4B-A6B, we\nintroduce a novel language adaptation approach by leveraging the\nBranch-Train-MiX strategy to merge script-specialized experts, into a single\nMoE model. Our Nile-Chat models significantly outperform leading multilingual\nand Arabic LLMs, such as LLaMa, Jais, and ALLaM, on our newly introduced\nEgyptian evaluation benchmarks, which span both understanding and generative\ntasks. Notably, our 12B model yields a 14.4% performance gain over\nQwen2.5-14B-Instruct on Latin-script benchmarks. All our resources are publicly\navailable. We believe this work presents a comprehensive methodology for\nadapting LLMs to dual-script languages, addressing an often overlooked aspect\nin modern LLM development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04569.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6087e598e2b7cc3a117b0dc5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png",
      "fullname": "Guokan Shang",
      "name": "guokan-shang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.06223",
      "authors": [
        {
          "_id": "686dc72ccb5725779c60b356",
          "name": "Zhiyuan Peng",
          "hidden": false
        },
        {
          "_id": "686dc72ccb5725779c60b357",
          "name": "Ting-ruen Wei",
          "hidden": false
        },
        {
          "_id": "686dc72ccb5725779c60b358",
          "user": {
            "_id": "64dc29d9b5d625e0e9a6ecb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
            "isPro": false,
            "fullname": "Tingyu Song",
            "user": "songtingyu",
            "type": "user"
          },
          "name": "Tingyu Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:31.763Z",
          "hidden": false
        },
        {
          "_id": "686dc72ccb5725779c60b359",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun Zhao",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:33.878Z",
          "hidden": false
        },
        {
          "_id": "686dc72ccb5725779c60b35a",
          "name": "Yi Fang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T17:56:28.000Z",
      "submittedOnDailyAt": "2025-07-09T00:20:14.472Z",
      "title": "Eficiencia y efectividad reranking LLM basado en FLOPs",
      "submittedOnDailyBy": {
        "_id": "64dc29d9b5d625e0e9a6ecb9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
        "isPro": false,
        "fullname": "Tingyu Song",
        "user": "songtingyu",
        "type": "user"
      },
      "summary": "Los grandes modelos de lenguaje (LLMs) han alcanzado un rendimiento impresionante al aplicarse a la tarea de reiniciar la búsqueda de información recientemente. Sin embargo, esta alta carga de cálculo impide su procesamiento efectivo. En los estudios previos, se utilizan métricas de proxy (latino, número de pasos hacia adelante, tokens de entrada, tokens de salida) para evaluar la eficiencia de los líderes de reinicio basados en LLMs. Sin embargo, estas métricas dependen del hardware y del tiempo de ejecución, y no consideran el tamaño del modelo. Esto ha dificultado la interpretación de los resultados y ha limitado la evaluación del equilibrio entre eficiencia y eficacia. Para resolver estos problemas, se propone E2R-FLOPs: utiliza PetaFLOP (RPP) y un transformador independiente del hardware para evaluar la eficiencia de los líderes de reinicio basados en LLMs. Se construye un predictor de cantidad de cálculo junto con este nuevo métrico, permitiendo predecir la cantidad de cálculo de los líderes de reinicio basados en LLMs sin necesidad de realizar experimentos. Basándose en los métricas propuestas, se evalúan líderes de reinicio basados en diferentes arquitecturas de LLMs y se presentan estudios sobre el equilibrio entre eficiencia y eficacia a la comunidad.",
      "upvotes": 10,
      "discussionId": "686dc72ccb5725779c60b35b",
      "ai_summary": "E\\textsuperscript{2}R-FLOPs evaluates LLM-based rerankers by measuring relevance and throughput per PetaFLOP, providing a hardware-agnostic metric for efficiency and effectiveness.",
      "ai_keywords": [
        "E\\textsuperscript{2}R-FLOPs",
        "ranking metrics per PetaFLOP",
        "queries per PetaFLOP",
        "FLOPs estimator",
        "LLM-based rerankers",
        "efficiency-effectiveness trade-off"
      ]
    },
    "publishedAt": "2025-07-08T13:56:28.000Z",
    "title": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers",
    "summary": "Large Language Models (LLMs) have recently been applied to reranking tasks in\ninformation retrieval, achieving strong performance. However, their high\ncomputational demands often hinder practical deployment. Existing studies\nevaluate the efficiency of LLM-based rerankers using proxy metrics such as\nlatency, the number of forward passes, input tokens, and output tokens.\nHowever, these metrics depend on hardware and running-time choices (\\eg\nparallel or not, batch size, etc), and often fail to account for model size,\nmaking it difficult to interpret and obscuring the evaluation of the\nefficiency-effectiveness tradeoff. To address this issue, we propose\nE2R-FLOPs, for LLM-based rerankers: ranking metrics per\nPetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for\nhardware-agnostic throughput. Companied with the new metrics, an interpretable\nFLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even\nwithout running any experiments. Based on the proposed metrics, we conduct\ncomprehensive experiments to evaluate a wide range of LLM-based rerankers with\ndifferent architecture, studying the efficiency-effectiveness trade-off and\nbringing this issue to the attention of the research community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06223.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dc29d9b5d625e0e9a6ecb9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
      "fullname": "Tingyu Song",
      "name": "songtingyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03698",
      "authors": [
        {
          "_id": "686dc7ebcb5725779c60b35d",
          "name": "Zhiling Yan",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b35e",
          "name": "Sifan Song",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b35f",
          "user": {
            "_id": "619f01b8cc04eadf54fa5d5d",
            "avatarUrl": "/avatars/928f3d1a6146e2e1ae4860445d929d5c.svg",
            "isPro": false,
            "fullname": "Song Dingjie",
            "user": "songdj",
            "type": "user"
          },
          "name": "Dingjie Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:29.991Z",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b360",
          "name": "Yiwei Li",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b361",
          "name": "Rong Zhou",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b362",
          "name": "Weixiang Sun",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b363",
          "name": "Zhennong Chen",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b364",
          "name": "Sekeun Kim",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b365",
          "name": "Hui Ren",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b366",
          "name": "Tianming Liu",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b367",
          "name": "Quanzheng Li",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b368",
          "name": "Xiang Li",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b369",
          "name": "Lifang He",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b36a",
          "name": "Lichao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T16:30:38.000Z",
      "submittedOnDailyAt": "2025-07-09T00:08:40.667Z",
      "title": "SAMed-2: Modelo de Clasificación Médica con Memoria Seleccionada Fortalecida",
      "submittedOnDailyBy": {
        "_id": "619f01b8cc04eadf54fa5d5d",
        "avatarUrl": "/avatars/928f3d1a6146e2e1ae4860445d929d5c.svg",
        "isPro": false,
        "fullname": "Song Dingjie",
        "user": "songdj",
        "type": "user"
      },
      "summary": "Los esfuerzos recientes de \"Segment Anything\" han sido capacitados a través de grandes cantidades de datos para mostrar resultados específicos, pero aplicar directamente un modelo a imágenes médicas es complicado debido a la complejidad de los datos médicos, las anotaciones con alto nivel de ruido, y la necesidad de entrenar continuamente para diferentes modalidades y estructuras anatómicas. En este artículo, proponemos SAMed-2, una nueva base de modelos para la segmentación de imágenes médicas basada en la arquitectura SAM-2. Específicamente, introducimos un adaptador de secuencias de tiempo en el encoder de imagen para comprender la relación de las imágenes y introducimos una estructura de memoria dirigida por confianza para almacenar características de alta precisión para su reutilización posterior. Esta estrategia basada en memoria permite enfrentar el ruido de grandes conjuntos de datos y prevenir la olvidación súbita cuando se enfrentan nuevas tareas o modalidades. Para la entrenamiento y evaluación de SAMed-2, se utilizaron conjuntos de datos detallados combinando 7 modalidades de imágenes y 21 tareas de segmentación médica, seleccionados como MedBank-100k. Los resultados de los experimentos, realizados utilizando un benchmark interno y 10 conjuntos de datos externos, muestran que SAMed-2 obtiene mejores resultados que los límites de base más avanzados en escenarios multitarea. El código está disponible en la siguiente URL: https://github.com/ZhilingYan/Medical-SAM-Bench.",
      "upvotes": 9,
      "discussionId": "686dc7eccb5725779c60b36b",
      "ai_summary": "SAMed-2, an adaptation of SAM-2 for medical image segmentation, incorporates a temporal adapter and confidence-driven memory to improve performance across diverse medical datasets and tasks.",
      "ai_keywords": [
        "SAM-2",
        "temporal adapter",
        "confidence-driven memory",
        "MedBank-100k",
        "medical segmentation",
        "catastrophic forgetting",
        "multi-task scenarios"
      ]
    },
    "publishedAt": "2025-07-04T12:30:38.000Z",
    "title": "SAMed-2: Selective Memory Enhanced Medical Segment Anything Model",
    "summary": "Recent \"segment anything\" efforts show promise by learning from large-scale\ndata, but adapting such models directly to medical images remains challenging\ndue to the complexity of medical data, noisy annotations, and continual\nlearning requirements across diverse modalities and anatomical structures. In\nthis work, we propose SAMed-2, a new foundation model for medical image\nsegmentation built upon the SAM-2 architecture. Specifically, we introduce a\ntemporal adapter into the image encoder to capture image correlations and a\nconfidence-driven memory mechanism to store high-certainty features for later\nretrieval. This memory-based strategy counters the pervasive noise in\nlarge-scale medical datasets and mitigates catastrophic forgetting when\nencountering new tasks or modalities. To train and evaluate SAMed-2, we curate\nMedBank-100k, a comprehensive dataset spanning seven imaging modalities and 21\nmedical segmentation tasks. Our experiments on both internal benchmarks and 10\nexternal datasets demonstrate superior performance over state-of-the-art\nbaselines in multi-task scenarios. The code is available at:\nhttps://github.com/ZhilingYan/Medical-SAM-Bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03698.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619f01b8cc04eadf54fa5d5d",
      "avatarUrl": "/avatars/928f3d1a6146e2e1ae4860445d929d5c.svg",
      "fullname": "Song Dingjie",
      "name": "songdj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05101",
      "authors": [
        {
          "_id": "686dc2d1cb5725779c60b342",
          "user": {
            "_id": "6682a6a75f3099b62d7aa802",
            "avatarUrl": "/avatars/ca9d41a94d508f0afd3516b1805cd2a2.svg",
            "isPro": false,
            "fullname": "Xinzhe Zheng",
            "user": "piaolaidangqu",
            "type": "user"
          },
          "name": "Xinzhe Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:37.420Z",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b343",
          "name": "Hao Du",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b344",
          "name": "Fanding Xu",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b345",
          "user": {
            "_id": "67658bd7f7ac7e978ab6f957",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/c8VBgFckkZNUGeqUyotwq.png",
            "isPro": false,
            "fullname": "Jinzhe Li",
            "user": "JinzheFudan",
            "type": "user"
          },
          "name": "Jinzhe Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:35.772Z",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b346",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b347",
          "name": "Wenkang Wang",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b348",
          "name": "Tao Chen",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b349",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b34a",
          "name": "Stan Z. Li",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b34b",
          "name": "Yan Lu",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b34c",
          "name": "Nanqing Dong",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b34d",
          "name": "Yang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T15:21:05.000Z",
      "submittedOnDailyAt": "2025-07-09T01:01:03.312Z",
      "title": "PRING: Revisión de la Predicción de Interacciones de Tipos de Protéínas en Grafos",
      "submittedOnDailyBy": {
        "_id": "6310a3cd531cc21f9e06de6a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a3cd531cc21f9e06de6a/aTGMx3O41lUARK9s3dAik.jpeg",
        "isPro": false,
        "fullname": "Zhiyuan Liu",
        "user": "acharkq",
        "type": "user"
      },
      "summary": "Los métodos de cálculo basados en redes neuronales han logrado buenos resultados en la predicción de interacciones entre proteínas (PPIs). Sin embargo, los actuales benchmarks se centran principalmente en dos evaluaciones aisladas y no miden la capacidad de construir redes de PPIs con significación biológica. Para solucionar estos defectos, presentamos PRING. PRING es el primer benchmark detallado para evaluar la predicción de PPIs. PRING ha organizado un conjunto de datos de redes de PPIs de alta calidad, que incluye 21,484 proteínas y 186,818 interacciones, y ha desarrollado una estrategia de programación para resolver repeticiones y pérdidas de datos. Basándonos en estas altas calidades de datos, hemos construido dos nuevos parámetros de evaluación: 1) en la tarea de topología, se evalúa la construcción de redes de PPIs dentro y entre diferentes tipos, y 2) en la tarea de función, se incluyen la predicción de pasos de complejo proteico, el análisis de módulos de GO y la justificación de proteínas importantes. Estas evaluaciones reflejan la capacidad de los modelos para comprender la topología de las redes y ayudan a explicar la función de las proteínas, detectar módulos biológicos y analizar la estructura de las enfermedades. Hemos realizado amplios experimentos con cuatro categorías de modelos representativos, lo que demuestra las limitaciones potenciales de los modelos PPI en la representación de la estructura y las características funcionales de las redes PPIs y la insuficiencia de apoyo para aplicaciones biológicas reales. Creemos que PRING puede ser un plataforma confiable que guie el desarrollo de modelos de predicción de PPIs más efectivos. El conjunto de datos y el código fuente de PRING están disponibles en https://github.com/SophieSarceau/PRING.",
      "upvotes": 8,
      "discussionId": "686dc2d1cb5725779c60b34e"
    },
    "publishedAt": "2025-07-07T11:21:05.000Z",
    "title": "PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to\n  Graphs",
    "summary": "Deep learning-based computational methods have achieved promising results in\npredicting protein-protein interactions (PPIs). However, existing benchmarks\npredominantly focus on isolated pairwise evaluations, overlooking a model's\ncapability to reconstruct biologically meaningful PPI networks, which is\ncrucial for biology research. To address this gap, we introduce PRING, the\nfirst comprehensive benchmark that evaluates protein-protein interaction\nprediction from a graph-level perspective. PRING curates a high-quality,\nmulti-species PPI network dataset comprising 21,484 proteins and 186,818\ninteractions, with well-designed strategies to address both data redundancy and\nleakage. Building on this golden-standard dataset, we establish two\ncomplementary evaluation paradigms: (1) topology-oriented tasks, which assess\nintra and cross-species PPI network construction, and (2) function-oriented\ntasks, including protein complex pathway prediction, GO module analysis, and\nessential protein justification. These evaluations not only reflect the model's\ncapability to understand the network topology but also facilitate protein\nfunction annotation, biological module detection, and even disease mechanism\nanalysis. Extensive experiments on four representative model categories,\nconsisting of sequence similarity-based, naive sequence-based, protein language\nmodel-based, and structure-based approaches, demonstrate that current PPI\nmodels have potential limitations in recovering both structural and functional\nproperties of PPI networks, highlighting the gap in supporting real-world\nbiological applications. We believe PRING provides a reliable platform to guide\nthe development of more effective PPI prediction models for the community. The\ndataset and source code of PRING are available at\nhttps://github.com/SophieSarceau/PRING.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05101.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310a3cd531cc21f9e06de6a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a3cd531cc21f9e06de6a/aTGMx3O41lUARK9s3dAik.jpeg",
      "fullname": "Zhiyuan Liu",
      "name": "acharkq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05963",
      "authors": [
        {
          "_id": "686dde62cb5725779c60b467",
          "name": "Zhenghao Zhang",
          "hidden": false
        },
        {
          "_id": "686dde62cb5725779c60b468",
          "name": "Junchao Liao",
          "hidden": false
        },
        {
          "_id": "686dde62cb5725779c60b469",
          "name": "Xiangyu Meng",
          "hidden": false
        },
        {
          "_id": "686dde62cb5725779c60b46a",
          "name": "Long Qin",
          "hidden": false
        },
        {
          "_id": "686dde62cb5725779c60b46b",
          "name": "Weizhi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T13:11:40.000Z",
      "submittedOnDailyAt": "2025-07-09T01:43:57.686Z",
      "title": "TRA2: Optimización de acciones y apariencia con la adición de transformers para la generación de videos de múltiples entidades",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de modelos de generación de vídeo guiados por acciones ha sido notable, como el caso de modelos como Tora. En este artículo, se presenta Tora2, una extensión de Tora. Tora2 introduce numerosas mejoras de diseño para expandir su capacidad de personalización de apariencia y acciones. En particular, se ha introducido un generador de características separadas para mantener mejores detalles visuales de entidades multi-open-set que en comparación con métodos anteriores. Con esto, se diseña una estructura de autoatención gated para integrar trayectorias, descripciones textuales y información visual de cada entidad. Esta innovación reduce significativamente la asimetría multimodal durante el entrenamiento. Además, se introduce una pérdida contrastiva para optimizar tanto la dinámica de las acciones como la consistencia de las entidades, mediante una clara mapeo entre generador de características y acciones. Tora2 es el primer método que logra personalizar simultáneamente apariencia y acciones en múltiples entidades en la generación de vídeo, debido a limitaciones de conocimiento. Los resultados experimentales muestran que Tora2 alcanza una mejora relativa en la eficiencia de personalización comparada con métodos avanzados, así como un desarrollo significativo en la control de acciones y la generación de vídeos en múltiples condiciones. La página del proyecto está disponible en https://github.com/alibaba/Tora.",
      "upvotes": 7,
      "discussionId": "686dde75cb5725779c60b46c",
      "ai_summary": "Tora2 enhances motion-guided video generation by introducing a decoupled personalization extractor, gated self-attention mechanism, and contrastive loss, enabling simultaneous multi-entity customization and advanced motion control.",
      "ai_keywords": [
        "diffusion transformer models",
        "Tora",
        "decoupled personalization extractor",
        "personalization embeddings",
        "gated self-attention mechanism",
        "trajectory dynamics",
        "entity consistency",
        "contrastive loss",
        "multi-entity customization",
        "motion control"
      ]
    },
    "publishedAt": "2025-07-08T09:11:40.000Z",
    "title": "Tora2: Motion and Appearance Customized Diffusion Transformer for\n  Multi-Entity Video Generation",
    "summary": "Recent advances in diffusion transformer models for motion-guided video\ngeneration, such as Tora, have shown significant progress. In this paper, we\npresent Tora2, an enhanced version of Tora, which introduces several design\nimprovements to expand its capabilities in both appearance and motion\ncustomization. Specifically, we introduce a decoupled personalization extractor\nthat generates comprehensive personalization embeddings for multiple open-set\nentities, better preserving fine-grained visual details compared to previous\nmethods. Building on this, we design a gated self-attention mechanism to\nintegrate trajectory, textual description, and visual information for each\nentity. This innovation significantly reduces misalignment in multimodal\nconditioning during training. Moreover, we introduce a contrastive loss that\njointly optimizes trajectory dynamics and entity consistency through explicit\nmapping between motion and personalization embeddings. Tora2 is, to our best\nknowledge, the first method to achieve simultaneous multi-entity customization\nof appearance and motion for video generation. Experimental results demonstrate\nthat Tora2 achieves competitive performance with state-of-the-art customization\nmethods while providing advanced motion control capabilities, which marks a\ncritical advancement in multi-condition video generation. Project page:\nhttps://github.com/alibaba/Tora .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05963.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04723",
      "authors": [
        {
          "_id": "686df5fecb5725779c60b4b8",
          "name": "Zecheng Tang",
          "hidden": false
        },
        {
          "_id": "686df5fecb5725779c60b4b9",
          "name": "Haitian Wang",
          "hidden": false
        },
        {
          "_id": "686df5fecb5725779c60b4ba",
          "user": {
            "_id": "6732fb1d24b316be87acaafe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6732fb1d24b316be87acaafe/BzD8HL4vhh3mfeSF3rm_1.jpeg",
            "isPro": false,
            "fullname": "Quantong Qiu",
            "user": "QQTang1223",
            "type": "user"
          },
          "name": "Quantong Qiu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:49:43.850Z",
          "hidden": false
        },
        {
          "_id": "686df5fecb5725779c60b4bb",
          "name": "Baibei Ji",
          "hidden": false
        },
        {
          "_id": "686df5fecb5725779c60b4bc",
          "name": "Ruoxi Sun",
          "hidden": false
        },
        {
          "_id": "686df5fecb5725779c60b4bd",
          "name": "Keyan Zhou",
          "hidden": false
        },
        {
          "_id": "686df5fecb5725779c60b4be",
          "name": "Juntao Li",
          "hidden": false
        },
        {
          "_id": "686df5fecb5725779c60b4bf",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T07:33:24.000Z",
      "submittedOnDailyAt": "2025-07-09T03:25:02.560Z",
      "title": "LOOM-Scope: Fórmula de Evaluación de Modelos de Contexto Largo Eficiente y Efectiva",
      "submittedOnDailyBy": {
        "_id": "64096ef79e9f790c905b846d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
        "isPro": false,
        "fullname": "Zecheng Tang",
        "user": "ZetangForward",
        "type": "user"
      },
      "summary": "El procesamiento de largas secuencias ha adquirido una posición fundamental en las capacidades básicas de los grandes modelos de lenguaje (LLMs). Se han propuesto múltiples marcos de referencia para evaluar el rendimiento de largas secuencias, pero las diferencias en las configuraciones de evaluación de estos marcos han provocado inecuaciones en los resultados y complicados la comparación de confianza. Además, el alto costo computacional de la evaluación de largas secuencias ha constituido un obstáculo para la comunidad al evaluar detalladamente modelos de largas secuencias. En este artículo, se propone LOOM-Scope, un marco de referencia de evaluación detallada y eficiente para largas secuencias. LOOM-Scope normaliza las configuraciones de evaluación de diferentes marcos de referencia, apoya la introducción de métodos de aceleración de la inferencia eficiente y introduce una hoja de cálculo ligera para evaluar detalladamente los modelos. Página de inicio: https://loomscope.github.io",
      "upvotes": 5,
      "discussionId": "686df5fecb5725779c60b4c0",
      "projectPage": "https://loomscope.github.io/",
      "githubRepo": "https://github.com/LCM-Lab/LOOM-Scope",
      "githubStars": 8
    },
    "publishedAt": "2025-07-07T03:33:24.000Z",
    "title": "LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation\n  framework",
    "summary": "Long-context processing has become a fundamental capability for large\nlanguage models~(LLMs). To assess model's long-context performance, numerous\nlong-context evaluation benchmarks have been proposed. However, variations in\nevaluation settings across these benchmarks lead to inconsistent results,\nmaking it difficult to draw reliable comparisons. Besides, the high\ncomputational cost of long-context evaluation poses a significant barrier for\nthe community to conduct comprehensive assessments of long-context models. In\nthis paper, we propose LOOM-Scope, a comprehensive and efficient framework for\nlong-context evaluation. LOOM-Scope standardizes evaluation settings across\ndiverse benchmarks, supports deployment of efficient long-context inference\nacceleration methods, and introduces a holistic yet lightweight benchmark suite\nto evaluate models comprehensively. Homepage: https://loomscope.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04723.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64096ef79e9f790c905b846d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
      "fullname": "Zecheng Tang",
      "name": "ZetangForward",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04103",
      "authors": [
        {
          "_id": "686dbfe2cb5725779c60b314",
          "name": "Dheeraj Vattikonda",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b315",
          "name": "Santhoshi Ravichandran",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b316",
          "name": "Emiliano Penaloza",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b317",
          "name": "Hadi Nekoei",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b318",
          "name": "Megh Thakkar",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b319",
          "name": "Thibault Le Sellier de Chezelles",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b31a",
          "name": "Nicolas Gontier",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b31b",
          "name": "Miguel Muñoz-Mármol",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b31c",
          "name": "Sahar Omidi Shayegan",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b31d",
          "name": "Stefania Raimondo",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b31e",
          "name": "Xue Liu",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b31f",
          "name": "Alexandre Drouin",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b320",
          "name": "Laurent Charlin",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b321",
          "name": "Alexandre Piché",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b322",
          "name": "Alexandre Lacoste",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b323",
          "name": "Massimo Caccia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-05T17:12:33.000Z",
      "submittedOnDailyAt": "2025-07-09T02:00:38.643Z",
      "title": "¿Cómo entrena tu agente web de modelo de lenguaje grande: Un diagnóstico estadístico",
      "submittedOnDailyBy": {
        "_id": "5fa9ff3ea13e063b8b2b60cb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
        "isPro": false,
        "fullname": "Xing Han Lù",
        "user": "xhluca",
        "type": "user"
      },
      "summary": "El web agente basado en modelos de lenguaje grande (LLM) ha experimentado un desarrollo notable recientemente, pero este desarrollo ha tenido lugar en un sistema cerrado, con diferencias significativas respecto a los métodos de acceso abierto. Este crecimiento está limitado por dos problemas principales: primero, la concentración en tareas de un solo paso, ignorando la complejidad de interacciones web multi-paso; y segundo, el alto costo computacional necesario para el aprendizaje posterior de los agentes web basados en LLM. Para abordar estos problemas, proponemos una investigación estadística inicial sobre la distribución de la cantidad de cálculo necesaria para el aprendizaje posterior de los agentes web basados en LLM. Nuestro enfoque utiliza una pipeline de dos etapas: primero, miméticamente aprendemos a un estudiante de Llama 3.1 8B con la supervisión de un profesor de Llama 3.3 70B para realizar un entrenamiento de picoteo (SFT), y luego, realizamos el aprendizaje de políticas. Este proceso es extremadamente sensible a la elección de parámetros iniciales, lo que hace que la exploración completa sea prácticamente imposible. Para evitar los problemas de pruebas y errores relacionados con otros métodos y costos, hemos muestreado 1,370 calentamientos y utilizado un botstrap para estimar eficazmente los parámetros iniciales. Nuestros resultados muestran que la combinación de SFT y aprendizaje de políticas no es una opción única de cada uno de los enfoques, sino que permite que ambos se alineen. Además, esta estrategia requiere un 55% menos de la cantidad de cálculo necesaria para superar el mejor rendimiento simple de SFT en MiniWob++, eficientemente optimiza la parámetro de rendimiento-cantidad de cálculo, y es la única estrategia para reducir las diferencias con modelos cerrados.",
      "upvotes": 5,
      "discussionId": "686dbfe2cb5725779c60b324",
      "ai_summary": "A study on compute allocation for post-training LLM-based web agents finds that combining supervised fine-tuning with on-policy reinforcement learning improves performance and reduces computational costs compared to either method alone.",
      "ai_keywords": [
        "LLM-based web agents",
        "supervised fine-tuning",
        "on-policy reinforcement learning",
        "hyperparameter choices",
        "bootstrapping",
        "WorkArena",
        "MiniWob++"
      ]
    },
    "publishedAt": "2025-07-05T13:12:33.000Z",
    "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
    "summary": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04103.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fa9ff3ea13e063b8b2b60cb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
      "fullname": "Xing Han Lù",
      "name": "xhluca",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06204",
      "authors": [
        {
          "_id": "686e0c68cb5725779c60b4ed",
          "name": "Nadav Schneider",
          "hidden": false
        },
        {
          "_id": "686e0c68cb5725779c60b4ee",
          "name": "Itamar Zimerman",
          "hidden": false
        },
        {
          "_id": "686e0c68cb5725779c60b4ef",
          "name": "Eliya Nachmani",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/D6ZhMrqrVT28Kh25ZySih.png",
        "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/qqCHBOm_ZYidhb_WFlZ1-.png",
        "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/kuXrDKc48bZzbeC1l1Si8.png",
        "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/e5wW0wTXLV1FOgG0pzEcl.png"
      ],
      "publishedAt": "2025-07-08T17:30:14.000Z",
      "submittedOnDailyAt": "2025-07-09T05:06:48.447Z",
      "title": "La paz mapa",
      "submittedOnDailyBy": {
        "_id": "65ce0b4a03a8179f5da5d4ef",
        "avatarUrl": "/avatars/c0b68efb885486c5180d1d69c4e317ac.svg",
        "isPro": false,
        "fullname": "Nadav Schneider",
        "user": "nadavsc",
        "type": "user"
      },
      "summary": "Los modelos de secuencia como Transformer y RNN a menudo distribuyen demasiada atención en contextos irrelevantes y generan representaciones intermedias que incluyen ruido. Esto puede provocar problemas, como el desarrollo de ruidos, la disminución de la capacidad de distancias largas y la búsqueda, y la reducción de la robustez, lo que afecta negativamente las funciones de los LLM. Recientes estudios han demostrado que diseños diferenciados pueden mitigar estos problemas y mejorar el rendimiento en aplicaciones diversas. En este artículo, se investiga si estas tecnologías pueden ser aplicadas a una nueva arquitectura basada en espacios de estado seleccional, como Mamba, sin ser necesario una arquitectura completamente diferente al Transformer. Además, se muestra que para resolver estos problemas, no solo son cambios simples suficientes, sino que se requieren cambios arquitecturales cuidadosos. Se introducen en Mamba una nueva estructura diferenciada, experimentada en marcos de referencia de modelado de lenguaje, que mejora la capacidad de búsqueda y muestra un rendimiento superior a los modelos base. Finalmente, se realizan pruebas de desvanecimiento y análisis experimental para justificar las decisiones de diseño y demostrar que la arquitectura Mamba mitiga los problemas de distribución de atención. El código está disponible para uso público.",
      "upvotes": 4,
      "discussionId": "686e0c68cb5725779c60b4f0",
      "ai_summary": "A novel differential mechanism for Mamba, a selective state-space layer architecture, improves retrieval capabilities and performance by addressing overallocation issues.",
      "ai_keywords": [
        "Transformers",
        "RNNs",
        "differential design",
        "Mamba",
        "selective state-space layers",
        "language modeling benchmarks",
        "ablation studies",
        "empirical analyses"
      ]
    },
    "publishedAt": "2025-07-08T13:30:14.000Z",
    "title": "Differential Mamba",
    "summary": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/D6ZhMrqrVT28Kh25ZySih.png",
      "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/qqCHBOm_ZYidhb_WFlZ1-.png",
      "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/kuXrDKc48bZzbeC1l1Si8.png",
      "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/e5wW0wTXLV1FOgG0pzEcl.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06204.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ce0b4a03a8179f5da5d4ef",
      "avatarUrl": "/avatars/c0b68efb885486c5180d1d69c4e317ac.svg",
      "fullname": "Nadav Schneider",
      "name": "nadavsc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04610",
      "authors": [
        {
          "_id": "686dcc0bcb5725779c60b38f",
          "user": {
            "_id": "63c9725ebedad7e2bf160bdc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c9725ebedad7e2bf160bdc/wzPuyhOXCYBNGwZDshbnL.jpeg",
            "isPro": false,
            "fullname": "Mostafa Elhoushi",
            "user": "melhoushi",
            "type": "user"
          },
          "name": "Mostafa Elhoushi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:50:25.543Z",
          "hidden": false
        },
        {
          "_id": "686dcc0bcb5725779c60b390",
          "name": "Jeff Johnson",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T01:59:47.000Z",
      "submittedOnDailyAt": "2025-07-09T00:32:16.318Z",
      "title": "Aplicación de la representación de números de 4 bits en un LLM",
      "submittedOnDailyBy": {
        "_id": "63c9725ebedad7e2bf160bdc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c9725ebedad7e2bf160bdc/wzPuyhOXCYBNGwZDshbnL.jpeg",
        "isPro": false,
        "fullname": "Mostafa Elhoushi",
        "user": "melhoushi",
        "type": "user"
      },
      "summary": "Estamos presentando any4. any4 es una solución de cuantificación de 4 bits que proporciona una representación numérica de 4 bits para grandes modelos de lenguaje (LLMs), sin necesidad de procesamiento previo de pesos y valores de activación. any4 ofrece una mayor precisión en comparación con otros tipos de representación numérica de 4 bits como int4, fp4 y nf4. En evaluaciones en modelos de diferentes tamaños, como Llama 2, Llama 3, Mistral y Mixtral, any4 demostró una precisión superior a otros tipos de representación numérica de 4 bits. any4 no requiere procesamiento previo de pesos y valores de activación, pero compite con técnicas ortogonales que lo requieren (por ejemplo, AWQ, GPTQ). Se ha demostrado su capacidad competitiva en experimentos con any3 y any2 en menos bits. Además, difiere de la mayoría de los enfoques de cuantificación en que se utilizan múltiples muestras diversas, any4 muestra su proceso de cuantificación con un solo conjunto de muestras puras. Además, hemos abierto el código de cuantificación de any4 junto con la biblioteca de multiplicación de matrices GPU optimizada para cuantificación, tinygemm, que es una biblioteca de GPU matriz multiplicación para modelos de lenguaje optimizada para cuantificación, en código abierto en https://github.com/facebookresearch/any4.",
      "upvotes": 4,
      "discussionId": "686dcc0ccb5725779c60b391",
      "githubRepo": "https://github.com/facebookresearch/any4",
      "ai_summary": "any4 is a learned 4-bit weight quantization method for LLMs that achieves high accuracy without preprocessing and uses a GPU-efficient lookup table strategy.",
      "ai_keywords": [
        "weight quantization",
        "LLMs",
        "int4",
        "fp4",
        "nf4",
        "AWQ",
        "GPTQ",
        "calibration",
        "GPU matrix multiplication",
        "lookup table strategy"
      ],
      "githubStars": 20
    },
    "publishedAt": "2025-07-06T21:59:47.000Z",
    "title": "any4: Learned 4-bit Numeric Representation for LLMs",
    "summary": "We present any4, a learned 4-bit weight quantization solution for large\nlanguage models (LLMs) providing arbitrary numeric representations without\nrequiring pre-processing of weights or activations. any4 yields higher accuracy\ncompared to other related 4-bit numeric representation types: int4, fp4 and\nnf4, as evaluated on a range of model sizes, generations and families (Llama 2,\nLlama 3, Mistral and Mixtral). While any4 does not require preprocessing of\nweights or activations, it is also competitive with orthogonal techniques that\nrequire such preprocessing (e.g., AWQ and GPTQ). We also experiment with any3\nand any2 and show competitiveness at lower bits. Additionally, we show that we\ncan calibrate using a single curated diverse sample rather than hundreds of\nsamples from a dataset as done in most quantization approaches. We also open\nsource tinygemm, a latency optimized GPU matrix multiplication library for\nLLMs, that implements any4 using a GPU-efficient lookup table strategy along\nwith other common quantization methods. We open source our code at\nhttps://github.com/facebookresearch/any4 .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04610.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c9725ebedad7e2bf160bdc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c9725ebedad7e2bf160bdc/wzPuyhOXCYBNGwZDshbnL.jpeg",
      "fullname": "Mostafa Elhoushi",
      "name": "melhoushi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 34
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05920",
      "authors": [
        {
          "_id": "686deb33cb5725779c60b49c",
          "name": "Xinyu Huang",
          "hidden": false
        },
        {
          "_id": "686deb33cb5725779c60b49d",
          "name": "Yuhao Dong",
          "hidden": false
        },
        {
          "_id": "686deb33cb5725779c60b49e",
          "name": "Weiwei Tian",
          "hidden": false
        },
        {
          "_id": "686deb33cb5725779c60b49f",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "686deb33cb5725779c60b4a0",
          "name": "Rui Feng",
          "hidden": false
        },
        {
          "_id": "686deb33cb5725779c60b4a1",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T12:05:05.000Z",
      "submittedOnDailyAt": "2025-07-09T02:42:14.123Z",
      "title": "Visión del lenguaje basada en teoría de alta resolución visual multinivel basado en aprendizaje por refuerzo",
      "submittedOnDailyBy": {
        "_id": "63bf7ba8da08ed0544ff20e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673493776367-63bf7ba8da08ed0544ff20e9.jpeg",
        "isPro": false,
        "fullname": "Xinyu Huang",
        "user": "xinyu1205",
        "type": "user"
      },
      "summary": "Los más avanzados modelos de gran escala (LMMs) muestran que, en el procesamiento de imágenes de alta resolución, estas entradas se convierten en grandes tokens visuales, y la mayoría de ellas tienen poca relación con tareas de flujo inferior, lo que permite desvelar los problemas. En este artículo, se propone una optimización de políticas basada en grillas de múltiples dimensiones (MGPO), que permite a los LMMs extraer subimágenes automáticamente basándose en las coordenadas de grilla de ladera inferior calculadas por el modelo en el marco de un conversor de turnos, y centrar la atención en el área visual de la clave en un orden de manera end-to-end. Comparado con el aprendizaje manual (SFT), este enfoque destaca que los LMMs pueden descubrir una fuerte capacidad de ladera inferior de grilla durante el proceso de entrenamiento de RL a través de una función de recompensa binaria que busca la precisión final del resultado. Además, se observa que los LMMs tienen dificultades al iniciar automáticamente la grilla de visualización durante el proceso de dropout, y para resolver este problema, se diseña un template de conversor de turnos y se limitan las salidas del modelo en varias conversaciones para promover la optimización estable. Los experimentos distribuidos muestran que, cuando se entrenan con datos visuales de pregunta y respuesta estándares, el MGPO desarrolla una capacidad de ladera inferior de grilla más fuerte que el GRPO, con un aumento del 5.4% en MME-Realworld y del 5.2% en los benchmarks V* fuera de distribución (OOD). En particular, después de entrenar Qwen2.5-VL-7B con 21K muestras, el MGPO supera a los modelos o1 de OpenAI y GPT-4o en los benchmarks OOD V*. El código está disponible en: https://github.com/EvolvingLMMs-Lab/MGPO.",
      "upvotes": 3,
      "discussionId": "686deb34cb5725779c60b4a2",
      "ai_summary": "MGPO, an end-to-end reinforcement learning framework, enhances large multi-modal models' ability to focus on key visual regions without requiring additional grounding annotations, improving performance on both in-distribution and out-of-distribution benchmarks.",
      "ai_keywords": [
        "Multi-turn Grounding-based Policy Optimization",
        "reinforcement learning",
        "LMMs",
        "large multi-modal models",
        "high-resolution images",
        "visual tokens",
        "grounding coordinates",
        "multi-turn conversation",
        "supervised fine-tuning",
        "binary reward function",
        "visual grounding",
        "policy loss",
        "multi-turn conversational template",
        "stable optimization",
        "MME-Realworld",
        "V* Bench",
        "Qwen2.5-VL-7B",
        "OpenAI's o1",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-07-08T08:05:05.000Z",
    "title": "High-Resolution Visual Reasoning via Multi-Turn Grounding-Based\n  Reinforcement Learning",
    "summary": "State-of-the-art large multi-modal models (LMMs) face challenges when\nprocessing high-resolution images, as these inputs are converted into enormous\nvisual tokens, many of which are irrelevant to the downstream task. In this\npaper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an\nend-to-end reinforcement learning (RL) framework that enables LMMs to\niteratively focus on key visual regions by automatically cropping sub-images,\nbased on model-predicted grounding coordinates within a multi-turn conversation\nframework. Compared to supervised fine-tuning (SFT), which requires costly\nadditional grounding annotations, our approach highlights that LMMs can emerge\nrobust grounding abilities during the RL training process, leveraging only a\nbinary reward function derived from the correctness of the final answer.\nAdditionally, we observe that LMMs struggle to autonomously trigger visual\ngrounding during the rollout process. To address this cold start problem, we\ndesign a multi-turn conversational template and restrict policy loss\ncomputation to model outputs generated across multiple dialogue rounds, thereby\npromoting stable optimization. Extensive experiments demonstrate that, when\ntrained on standard visual-question-short answering data without grounding\nannotations, MGPO effectively elicits stronger grounding capabilities compared\nto GRPO, leading to 5.4\\% improvement on in-distribution MME-Realworld and\n5.2\\% improvement on the challenging out-of-distribution (OOD) V* Bench.\nNotably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses\nOpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at\nhttps://github.com/EvolvingLMMs-Lab/MGPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05920.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bf7ba8da08ed0544ff20e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673493776367-63bf7ba8da08ed0544ff20e9.jpeg",
      "fullname": "Xinyu Huang",
      "name": "xinyu1205",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 42
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05578",
      "authors": [
        {
          "_id": "686dca00cb5725779c60b379",
          "name": "Alexander Xiong",
          "hidden": false
        },
        {
          "_id": "686dca00cb5725779c60b37a",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "686dca00cb5725779c60b37b",
          "name": "Aneesh Pappu",
          "hidden": false
        },
        {
          "_id": "686dca00cb5725779c60b37c",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T01:30:46.000Z",
      "submittedOnDailyAt": "2025-07-09T00:17:12.585Z",
      "title": "Memoria de un LLM: estructura, evaluación, medidas preventivas",
      "submittedOnDailyBy": {
        "_id": "6275a465597c70eb8949fce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
        "isPro": false,
        "fullname": "Xuandong Zhao",
        "user": "Xuandong",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje de gran escala (LLMs) muestran capacidades sorprendentes en diversas tareas, pero también reflejan el fenómeno de la memoria de los datos de entrenamiento. Este fenómeno plantea preguntas importantes sobre el comportamiento del modelo, los riesgos de privacidad y los límites entre el entrenamiento y la memoria. Para abordar estas preocupaciones, este artículo synthetiza investigaciones recientes y examina la estructura de la memoria, los factores que la influyen y las técnicas para detectar y inhibir la memoria. Se revisan factores como la repetición de datos, la dinámica del entrenamiento y los procesos de ajuste. Se evalúan métodos variados, como la extracción basada en prefijos, la inferencia de miembros y el prompting adversario, y se mide su efectividad en la detección y evaluación de la memoria. Se analizan también los impactos más amplios, legales y éticos. Finalmente, se discuten medidas para minimizar la memoria y mantener un equilibrio entre utilidad y memoria, así como la seguridad de los datos, la privacidad diferenciada y el olvido posterior. Este artículo proporciona una visión coherente de los avances en la investigación sobre la memoria de los LLMs desde diferentes perspectivas técnicas, de privacidad y de rendimiento, y establece direcciones importantes para futuras investigaciones.",
      "upvotes": 3,
      "discussionId": "686dca01cb5725779c60b37d",
      "ai_summary": "The paper reviews recent studies on memorization in Large Language Models, exploring factors that influence memorization, detection methodologies, and mitigation strategies, while addressing privacy and ethical implications.",
      "ai_keywords": [
        "Large Language Models",
        "memorization",
        "training data duplication",
        "training dynamics",
        "fine-tuning procedures",
        "prefix-based extraction",
        "membership inference",
        "adversarial prompting",
        "data cleaning",
        "differential privacy",
        "post-training unlearning"
      ]
    },
    "publishedAt": "2025-07-07T21:30:46.000Z",
    "title": "The Landscape of Memorization in LLMs: Mechanisms, Measurement, and\n  Mitigation",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they also exhibit memorization of their training\ndata. This phenomenon raises critical questions about model behavior, privacy\nrisks, and the boundary between learning and memorization. Addressing these\nconcerns, this paper synthesizes recent studies and investigates the landscape\nof memorization, the factors influencing it, and methods for its detection and\nmitigation. We explore key drivers, including training data duplication,\ntraining dynamics, and fine-tuning procedures that influence data memorization.\nIn addition, we examine methodologies such as prefix-based extraction,\nmembership inference, and adversarial prompting, assessing their effectiveness\nin detecting and measuring memorized content. Beyond technical analysis, we\nalso explore the broader implications of memorization, including the legal and\nethical implications. Finally, we discuss mitigation strategies, including data\ncleaning, differential privacy, and post-training unlearning, while\nhighlighting open challenges in balancing the minimization of harmful\nmemorization with utility. This paper provides a comprehensive overview of the\ncurrent state of research on LLM memorization across technical, privacy, and\nperformance dimensions, identifying critical directions for future work.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05578.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6275a465597c70eb8949fce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
      "fullname": "Xuandong Zhao",
      "name": "Xuandong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.03728",
      "authors": [
        {
          "_id": "686e2930a5f0f70d9de40c52",
          "user": {
            "_id": "65baa31607366d903890bcf4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65baa31607366d903890bcf4/6M9WaawnvJ-2h5wSUic1I.jpeg",
            "isPro": false,
            "fullname": "ABDENNACER BADAOUI",
            "user": "badaoui",
            "type": "user"
          },
          "name": "Abdennacer Badaoui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:49:07.301Z",
          "hidden": false
        },
        {
          "_id": "686e2930a5f0f70d9de40c53",
          "name": "Oussama Kharouiche",
          "hidden": false
        },
        {
          "_id": "686e2930a5f0f70d9de40c54",
          "name": "Hatim Mrabet",
          "hidden": false
        },
        {
          "_id": "686e2930a5f0f70d9de40c55",
          "name": "Daniele Malitesta",
          "hidden": false
        },
        {
          "_id": "686e2930a5f0f70d9de40c56",
          "name": "Fragkiskos D. Malliaros",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T17:31:41.000Z",
      "submittedOnDailyAt": "2025-07-09T08:04:43.439Z",
      "title": "FAROS: Estructura de Cambio de Atributos mediante Generación de Grafos Justos",
      "submittedOnDailyBy": {
        "_id": "65baa31607366d903890bcf4",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65baa31607366d903890bcf4/6M9WaawnvJ-2h5wSUic1I.jpeg",
        "isPro": false,
        "fullname": "ABDENNACER BADAOUI",
        "user": "badaoui",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los modelos de difusión gráfica (GDM) ha permitido la síntesis de estructuras de red realistas, pero la garantía de la equidad de los datos generados es un desafío importante. Los actuales enfoques buscan reducir la bias al entrenar de nuevo a los GDM agregando restricciones equitativas efectivas. En contraste, este estudio propone un nuevo marco de trabajo de generación de grafos equitativos, FAROS. FAROS utiliza una estructura de intercambio de atributos para cambiar directamente el proceso de generación de los GDM previamente entrenados. Técnicamente, nuestro enfoque cambia los atributos sensibles de los nodos generados. Por lo tanto, FAROS calcula la proporción óptima de intercambio de nodos y asegura la independencia de la matriz de adyacencia del grafo generado (representación de la equidad) mientras mantiene el perfil topológico de los nodos (representación de la precisión). En la evaluación de predicción de enlaces en conjuntos de datos de referencia, nuestros experimentos muestran que nuestro enfoque efectivamente reduce la equidad y mantiene o mejora la precisión comparado con otros métodos similares. En particular, FAROS puede lograr una mejor armonía entre precisión y equidad en configuraciones experimentales validadas bajo el concepto de óptimo de word, comparado con otros métodos, y demuestra la efectividad de las restricciones de multi-container que se han planteado.",
      "upvotes": 1,
      "discussionId": "686e2931a5f0f70d9de40c57",
      "ai_summary": "FAROS is a framework that enhances fairness in graph diffusion models by strategically switching node attributes during generation to balance accuracy and fairness.",
      "ai_keywords": [
        "graph diffusion models",
        "FAir graph geneRatiOn",
        "attribute Switching",
        "node-topology profile",
        "edge independence",
        "link prediction",
        "Pareto optimality"
      ]
    },
    "publishedAt": "2025-07-04T13:31:41.000Z",
    "title": "FAROS: Fair Graph Generation via Attribute Switching Mechanisms",
    "summary": "Recent advancements in graph diffusion models (GDMs) have enabled the\nsynthesis of realistic network structures, yet ensuring fairness in the\ngenerated data remains a critical challenge. Existing solutions attempt to\nmitigate bias by re-training the GDMs with ad-hoc fairness constraints.\nConversely, with this work, we propose FAROS, a novel FAir graph geneRatiOn\nframework leveraging attribute Switching mechanisms and directly running in the\ngeneration process of the pre-trained GDM. Technically, our approach works by\naltering nodes' sensitive attributes during the generation. To this end, FAROS\ncalculates the optimal fraction of switching nodes, and selects the diffusion\nstep to perform the switch by setting tailored multi-criteria constraints to\npreserve the node-topology profile from the original distribution (a proxy for\naccuracy) while ensuring the edge independence on the sensitive attributes for\nthe generated graph (a proxy for fairness). Our experiments on benchmark\ndatasets for link prediction demonstrate that the proposed approach effectively\nreduces fairness discrepancies while maintaining comparable (or even higher)\naccuracy performance to other similar baselines. Noteworthy, FAROS is also able\nto strike a better accuracy-fairness trade-off than other competitors in some\nof the tested settings under the Pareto optimality concept, demonstrating the\neffectiveness of the imposed multi-criteria constraints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03728.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65baa31607366d903890bcf4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65baa31607366d903890bcf4/6M9WaawnvJ-2h5wSUic1I.jpeg",
      "fullname": "ABDENNACER BADAOUI",
      "name": "badaoui",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 40
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05411",
      "authors": [
        {
          "_id": "686e2a81a5f0f70d9de40c59",
          "name": "Mark Lee",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c5a",
          "name": "Tom Gunter",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c5b",
          "name": "Chang Lan",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c5c",
          "name": "John Peebles",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c5d",
          "name": "Hanzhi Zhou",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c5e",
          "name": "Kelvin Zou",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c5f",
          "name": "Sneha Bangalore",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c60",
          "name": "Chung-Cheng Chiu",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c61",
          "name": "Nan Du",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c62",
          "name": "Xianzhi Du",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c63",
          "name": "Philipp Dufter",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c64",
          "name": "Ruixuan Hou",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c65",
          "name": "Haoshuo Huang",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c66",
          "name": "Dongseong Hwang",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c67",
          "name": "Xiang Kong",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c68",
          "name": "Jinhao Lei",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c69",
          "name": "Tao Lei",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c6a",
          "name": "Meng Li",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c6b",
          "name": "Li Li",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c6c",
          "name": "Jiarui Lu",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c6d",
          "name": "Zhiyun Lu",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c6e",
          "name": "Yiping Ma",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c6f",
          "name": "David Qiu",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c70",
          "name": "Vivek Rathod",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c71",
          "name": "Senyu Tong",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c72",
          "name": "Zhucheng Tu",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c73",
          "name": "Jianyu Wang",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c74",
          "name": "Yongqiang Wang",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c75",
          "name": "Zirui Wang",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c76",
          "name": "Floris Weers",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c77",
          "name": "Sam Wiseman",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c78",
          "name": "Guoli Yin",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c79",
          "name": "Bowen Zhang",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c7a",
          "name": "Xiyou Zhou",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c7b",
          "name": "Danyang Zhuo",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c7c",
          "name": "Cheng Leong",
          "hidden": false
        },
        {
          "_id": "686e2a81a5f0f70d9de40c7d",
          "name": "Ruoming Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T18:50:58.000Z",
      "submittedOnDailyAt": "2025-07-09T07:09:03.056Z",
      "title": "AXLearn: Modelación de Infraestructuras de Homojuniedad con Modelos de Aprendizaje Profundo en Evolución",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": true,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "AXLearn es diseñado como un sistema de aprendizaje profundo de producción que fomenta la entrenamiento de modelos de aprendizaje profundo de gran escala con alta eficiencia. En comparación con otros sistemas de aprendizaje profundo más avanzados, AXLearn destaca por su soporte a la modularidad y la diversidad de componentes. Las interfaz internas entre los componentes del software de AXLearn cumplen con una estricta encapsulación, lo que promueve la combinación de diferentes componentes y apoya el desarrollo de modelos y experimentos rápidos en una infraestructura de cálculo que soporta la diversidad de componentes. Proponemos un nuevo método de evaluación para modularizar la complejidad de líneas de código (LoC), y mostramos que, al expandir los componentes del sistema, la complejidad no aumenta lineaalmente o cuadráticamente como en otros sistemas, manteniendo una complejidad constante. Esto permite que funciones como Rotary Position Embeddings (RoPE) se integren en más de 100 módulos de AXLearn de manera uniforme, lo que puede ser implementado con solo 10 líneas de código en lugar de cientos de líneas en otros sistemas. Además, AXLearn mantiene el rendimiento de los sistemas de entrenamiento más avanzados. Finalmente, compartimos nuestra experiencia en el desarrollo y operación de AXLearn.",
      "upvotes": 0,
      "discussionId": "686e2a82a5f0f70d9de40c7e",
      "ai_summary": "AXLearn is a modular deep learning system designed for scalable training on heterogeneous hardware, maintaining performance and modularity through efficient code integration methods.",
      "ai_keywords": [
        "modularity",
        "heterogeneous hardware infrastructure",
        "encapsulation",
        "Lines-of-Code (LoC)-complexity",
        "Rotary Position Embeddings (RoPE)"
      ]
    },
    "publishedAt": "2025-07-07T14:50:58.000Z",
    "title": "AXLearn: Modular Large Model Training on Heterogeneous Infrastructure",
    "summary": "We design and implement AXLearn, a production deep learning system that\nfacilitates scalable and high-performance training of large deep learning\nmodels. Compared to other state-of-the-art deep learning systems, AXLearn has a\nunique focus on modularity and support for heterogeneous hardware\ninfrastructure. AXLearn's internal interfaces between software components\nfollow strict encapsulation, allowing different components to be assembled to\nfacilitate rapid model development and experimentation on heterogeneous compute\ninfrastructure. We introduce a novel method of quantifying modularity via\nLines-of-Code (LoC)-complexity, which demonstrates how our system maintains\nconstant complexity as we scale the components in the system, compared to\nlinear or quadratic complexity in other systems. This allows integrating\nfeatures such as Rotary Position Embeddings (RoPE) into AXLearn across hundred\nof modules with just 10 lines of code, compared to hundreds as required in\nother systems. At the same time, AXLearn maintains equivalent performance\ncompared to state-of-the-art training systems. Finally, we share our experience\nin the development and operation of AXLearn.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 910
    },
    "isAuthorParticipating": false
  }
]