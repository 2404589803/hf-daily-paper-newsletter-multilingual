[
  {
    "paper": {
      "id": "2504.15376",
      "authors": [
        {
          "_id": "680bda9c34c8d0bd08e01a25",
          "user": {
            "_id": "64c170190bfb901b04399295",
            "avatarUrl": "/avatars/c30ce7566ae3497ddc989ec8918d37cc.svg",
            "isPro": false,
            "fullname": "Zhiqiu Lin",
            "user": "zhiqiulin",
            "type": "user"
          },
          "name": "Zhiqiu Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:01.030Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a26",
          "user": {
            "_id": "65f82fb0de5e636ca20184fa",
            "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
            "isPro": false,
            "fullname": "Alan",
            "user": "syCen",
            "type": "user"
          },
          "name": "Siyuan Cen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:05.915Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a27",
          "name": "Daniel Jiang",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a28",
          "name": "Jay Karhade",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a29",
          "user": {
            "_id": "67b2db158904ba09ca8feb79",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b2db158904ba09ca8feb79/faCKKdyroDNCcylEAQZKu.png",
            "isPro": false,
            "fullname": "Hewei Wang",
            "user": "Stephen624",
            "type": "user"
          },
          "name": "Hewei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:03.301Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2a",
          "name": "Chancharik Mitra",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2b",
          "name": "Tiffany Ling",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2c",
          "name": "Yuhan Huang",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2d",
          "name": "Sifan Liu",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2e",
          "name": "Mingyu Chen",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2f",
          "name": "Rushikesh Zawar",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a30",
          "name": "Xue Bai",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a31",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a32",
          "name": "Chuang Gan",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a33",
          "name": "Deva Ramanan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T18:34:57.000Z",
      "submittedOnDailyAt": "2025-04-28T00:10:15.204Z",
      "title": "Towards Understanding Camera Motions in Any Video",
      "submittedOnDailyBy": {
        "_id": "65f82fb0de5e636ca20184fa",
        "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
        "isPro": false,
        "fullname": "Alan",
        "user": "syCen",
        "type": "user"
      },
      "summary": "CameraBench es un grande conjunto de datos y marcos de referencia diseñados para entender y mejorar el comportamiento de las cámaras. Está compuesto por aproximadamente 3,000 diferentes vídeos de internet, explicados por expertos a través de un proceso de gestión de calidad multinivel estricto. Uno de nuestros contribuidores es el diseño de la base de conocimiento fundamental de la acción de la cámara, el aprendizaje profundo. Por ejemplo, se ha encontrado que acciones como \"tracking\" (o desaparición) requieren comprender el contenido de un sujeto en movimiento. Hemos realizado investigaciones a gran escala para medir la capacidad de explicación humana, demostrando que el conocimiento profesional y la formación basada en tutoriales mejoran significativamente la precisión. Por ejemplo, principiantes pueden confundir \"zun\" (cambios intrínsecos) con \"jin\" (cambios extrínsecos), pero se reciben entrenamiento para distinguirlos. Usando CameraBench, evaluamos la acción desde la estructura (SfM) y los modelos de lenguaje de video (VLM). Los modelos de SfM facilitan la comprensión de los elementos básicos significativos según el contenido de la escena, mientras que los VLM facilitan la comprensión de los elementos básicos geométricos que requieren una estimación de trayectoria precisa. Posteriormente, usamos CameraBench para ajustar y optimizar ambos modelos, demostrando aplicaciones variadas como la adición de subseñes, respuestas a preguntas de video y búsqueda de texto de video. Esperamos que el futuro de nuestros esfuerzos en entender el comportamiento de las cámaras dentro de los videos se orienten hacia el objetivo final, utilizando aprendizaje profundo, marcos de referencia y tutoriales.",
      "upvotes": 110,
      "discussionId": "680bda9e34c8d0bd08e01ae9",
      "projectPage": "https://linzhiqiu.github.io/papers/camerabench/",
      "githubRepo": "https://github.com/sy77777en/CameraBench",
      "ai_keywords": [
        "Structure-from-Motion (SfM)",
        "Video-Language Models (VLMs)",
        "semantic primitives",
        "geometric primitives",
        "generative VLM",
        "motion-augmented captioning",
        "video question answering",
        "video-text retrieval"
      ]
    },
    "publishedAt": "2025-04-21T14:34:57.000Z",
    "title": "Towards Understanding Camera Motions in Any Video",
    "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f82fb0de5e636ca20184fa",
      "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
      "fullname": "Alan",
      "name": "syCen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16656",
      "authors": [
        {
          "_id": "6809a4ac81a95c83f0c81c83",
          "name": "Chris",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c84",
          "name": "Yichen Wei",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c85",
          "name": "Yi Peng",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c86",
          "name": "Xiaokun Wang",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c87",
          "name": "Weijie Qiu",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c88",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c89",
          "name": "Tianyidan Xie",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8a",
          "name": "Jiangbo Pei",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8b",
          "name": "Jianhao Zhang",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8c",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8d",
          "user": {
            "_id": "6462b241b438438da3c25a5d",
            "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
            "isPro": false,
            "fullname": "Xuchen Song",
            "user": "xuchensong",
            "type": "user"
          },
          "name": "Xuchen Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:35:17.241Z",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8e",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8f",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T12:24:10.000Z",
      "submittedOnDailyAt": "2025-04-28T05:19:19.230Z",
      "title": "Skywork R1V2: Estudio de un híbrido de aprendizaje de refuerzo para la lógica utilizando la dominación híbrida de la lógica",
      "submittedOnDailyBy": {
        "_id": "6462b241b438438da3c25a5d",
        "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
        "isPro": false,
        "fullname": "Xuchen Song",
        "user": "xuchensong",
        "type": "user"
      },
      "summary": "Introduzco Skywork R1V2. Este es un modelo cognitivo multimodal de las siguientes generaciones, que ha evolucionado significativamente en comparación con el anterior, el Skywork R1V. El núcleo de R1V2 es un paradigma de aprendizaje por refuerzo híbrido que combina un modelo de recompensa y estrategias basadas en reglas. Esto permite resolver problemas antiguos de capacidad cognitiva compleja y generalización amplia. Además, proponemos una estructura de bufer de muestras seleccionadas (SSB) para mejorar la eficiencia de entrenamiento. Esta estructura prioriza muestras de alto valor durante el proceso de optimización, lo que resuelve el problema de \"ganancias perdidas\" en el optimizador de políticas de grupo (GRPO). Específicamente, observamos que se producen falsos positivos debido a señales de fortaleza excesiva, y mediante un escalado de recompensas ajustado se observa y mitiga este fenómeno sistemáticamente. Los resultados de los experimentos demuestran las excelentes capacidades de R1V2, mostrando un rendimiento de 62.6 en OlympiadBench, 79.0 en AIME2024, 63.6 en LiveCodeBench y 74.0 en MMMU. Estos resultados superan a los modelos abierto-source actuales y reducen la diferencia de rendimiento con sistemas avanzados como Gemini 2.5 y o4-mini de OpenAI. Los pesos del modelo Skywork R1V2 están disponibles y están publicados para fomentar la transparencia y la reproducibilidad. El sitio web es https://huggingface.co/Skywork/Skywork-R1V2-38B.",
      "upvotes": 38,
      "discussionId": "6809a4ae81a95c83f0c81cda",
      "githubRepo": "https://github.com/SkyworkAI/Skywork-R1V",
      "ai_keywords": [
        "reinforcement learning",
        "reward-model guidance",
        "rule-based strategies",
        "Selective Sample Buffer (SSB)",
        "Vanishing Advantages",
        "Group Relative Policy Optimization (GRPO)",
        "visual hallucinations",
        "calibrated reward thresholds",
        "benchmark-leading performances",
        "OlympiadBench",
        "AIME2024",
        "LiveCodeBench",
        "MMMU",
        "Skywork R1V2-38B"
      ]
    },
    "publishedAt": "2025-04-23T08:24:10.000Z",
    "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning",
    "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a\nmajor leap forward from its predecessor, Skywork R1V. At its core, R1V2\nintroduces a hybrid reinforcement learning paradigm that harmonizes\nreward-model guidance with rule-based strategies, thereby addressing the\nlong-standing challenge of balancing sophisticated reasoning capabilities with\nbroad generalization. To further enhance training efficiency, we propose the\nSelective Sample Buffer (SSB) mechanism, which effectively counters the\n``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization\n(GRPO) by prioritizing high-value samples throughout the optimization process.\nNotably, we observe that excessive reinforcement signals can induce visual\nhallucinations--a phenomenon we systematically monitor and mitigate through\ncalibrated reward thresholds throughout the training process. Empirical results\naffirm the exceptional capability of R1V2, with benchmark-leading performances\nsuch as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and\n74.0 on MMMU. These results underscore R1V2's superiority over existing\nopen-source models and demonstrate significant progress in closing the\nperformance gap with premier proprietary systems, including Gemini 2.5 and\nOpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to\npromote openness and reproducibility\nhttps://huggingface.co/Skywork/Skywork-R1V2-38B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6462b241b438438da3c25a5d",
      "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
      "fullname": "Xuchen Song",
      "name": "xuchensong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18415",
      "authors": [
        {
          "_id": "680ef1549cc294f617fb14b4",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "680ef1549cc294f617fb14b5",
          "name": "Shuming Ma",
          "hidden": false
        },
        {
          "_id": "680ef1549cc294f617fb14b6",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-25T15:17:52.000Z",
      "submittedOnDailyAt": "2025-04-28T01:39:22.422Z",
      "title": "BitNet v2: Aplicación de la Transformación Hedimard en un LLM de 1 Bit con Acción Activa en 4 Bits Locales",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "1-bit Large Language Models (LLMs) sufren una eficiencia comprometida debido a la presencia de outliers de activación (outliers de activación). Estos outliers complican la cuantificación a bajos bits, dificultando así la colocación eficiente. Presentamos un nuevo marco de trabajo, BitNet v2, que permite cuantificar la activación nativa de 4 bits de los 1-bit LLMs. En particular, proponemos el módulo H-BitLinear, que aplica la transformación Hadamard en línea para resolver los problemas de outliers de activación en red neuronal de atención y en red neuronal de propagación hacia adelante. Esta transformación regulariza la distribución de activación hacia una distribución gaussiana, convertiéndola en un formato adecuado para representaciones bajos en bits. Los resultados experimentales muestran que el entrenamiento de BitNet v2 con activación de 8 bits logra rendimientos comparables a BitNet b1.58. Es importante destacar que BitNet v2 se entrena utilizando la activación nativa de 4 bits, lo que permite reducciones significativas en el consumo de memoria y en el costo de cálculo en la inferencia de la colocación, sin una disminución significativa del rendimiento.",
      "upvotes": 17,
      "discussionId": "680ef1559cc294f617fb1536",
      "ai_keywords": [
        "BitNet v2",
        "1-bit Large Language Models (LLMs)",
        "activation outliers",
        "quantization",
        "4-bit activation quantization",
        "H-BitLinear",
        "Hadamard transformation",
        "activation distributions",
        "Gaussian-like forms",
        "low-bit representation",
        "8-bit activations",
        "BitNet b1.58",
        "batched inference"
      ]
    },
    "publishedAt": "2025-04-25T11:17:52.000Z",
    "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs",
    "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17821",
      "authors": [
        {
          "_id": "680f56b8da9639d22c64443f",
          "name": "Xinyu Chen",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644440",
          "name": "Yunxin Li",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644441",
          "name": "Haoyuan Shi",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644442",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644443",
          "name": "Wenhan Luo",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644444",
          "name": "Yaowei Wang",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644445",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T13:47:30.000Z",
      "submittedOnDailyAt": "2025-04-28T09:15:13.533Z",
      "title": "VideoVista-CulturalLingo: Límites de 360° - Conectando cultura, lenguaje y áreas en la comprensión de películas",
      "submittedOnDailyBy": {
        "_id": "62fdb01bc1588e1d4c6c1a7c",
        "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
        "isPro": false,
        "fullname": "Yunxin Li",
        "user": "YunxinLi",
        "type": "user"
      },
      "summary": "Evaluación de un sistema de IA multimodal para comprender videos mide su capacidad de entender y explicar. Los marcadores de evaluación de videos se centran generalmente en inglés y utilizan videos típicamente de culturas occidentales. En este artículo, se presenta VideoVista-CulturalLingo, el primer marcador de evaluación de videos. Este marcador conecta los intercambios culturales, lingüísticos y regionales en la comprensión de videos. Nuestra investigación presenta diferencias con los marcadores existentes:\n\n1. Diversidad Cultural: Se utilizan videos de China, Estados Unidos y Europa.\n2. Multilingüismo: Se incluyen preguntas escritas en chino y inglés, las dos lenguas más comunes.\n3. Amplios Áreas: Se utilizan videos de miles de áreas creadas por humanos.\n\nVideoVista-CulturalLingo incluye 1,389 videos y 3,134 pares de preguntas y respuestas, evaluando 24 modelos de videos abierto fuente o propietario. Los resultados de los experimentos revelan:\n\n1. Los modelos actuales no muestran mejores resultados en preguntas centradas en culturas orientales, especialmente en preguntas sobre la historia de China.\n2. Los modelos abierto fuente presentan limitaciones específicas en la comprensión temporal, alcanzando un máximo de 45.2% de puntuación en tareas como la detección de eventos.\n3. Los modelos principales muestran excelente competencia en preguntas científicas generales, pero los modelos abierto fuente presentan deficiencias en matemáticas.",
      "upvotes": 14,
      "discussionId": "680f56bdda9639d22c64456b",
      "projectPage": "https://videovista-culturallingo.github.io/",
      "githubRepo": "https://github.com/HITsz-TMG/VideoVista"
    },
    "publishedAt": "2025-04-23T09:47:30.000Z",
    "title": "VideoVista-CulturalLingo: 360^circ Horizons-Bridging Cultures,\n  Languages, and Domains in Video Comprehension",
    "summary": "Assessing the video comprehension capabilities of multimodal AI systems can\neffectively measure their understanding and reasoning abilities. Most video\nevaluation benchmarks are limited to a single language, typically English, and\npredominantly feature videos rooted in Western cultural contexts. In this\npaper, we present VideoVista-CulturalLingo, the first video evaluation\nbenchmark designed to bridge cultural, linguistic, and domain divide in video\ncomprehension. Our work differs from existing benchmarks in the following ways:\n1) Cultural diversity, incorporating cultures from China, North America, and\nEurope; 2) Multi-linguistics, with questions presented in Chinese and\nEnglish-two of the most widely spoken languages; and 3) Broad domain, featuring\nvideos sourced from hundreds of human-created domains. VideoVista-CulturalLingo\ncontains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent\nopen-source or proprietary video large models. From the experiment results, we\nobserve that: 1) Existing models perform worse on Chinese-centric questions\nthan Western-centric ones, particularly those related to Chinese history; 2)\nCurrent open-source models still exhibit limitations in temporal understanding,\nespecially in the Event Localization task, achieving a maximum score of only\n45.2%; 3) Mainstream models demonstrate strong performance in general\nscientific questions, while open-source models demonstrate weak performance in\nmathematics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fdb01bc1588e1d4c6c1a7c",
      "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
      "fullname": "Yunxin Li",
      "name": "YunxinLi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16427",
      "authors": [
        {
          "_id": "680c48805ec65044c2861a6a",
          "user": {
            "_id": "669090c01e3f5b16ce22b535",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
            "isPro": false,
            "fullname": "Hanlei Zhang",
            "user": "HanleiZhang",
            "type": "user"
          },
          "name": "Hanlei Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:52:58.965Z",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6b",
          "name": "Zhuohang Li",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6c",
          "name": "Yeshuang Zhu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6d",
          "name": "Hua Xu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6e",
          "name": "Peiwu Wang",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6f",
          "name": "Haige Zhu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a70",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a71",
          "name": "Jinchao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T05:25:13.000Z",
      "submittedOnDailyAt": "2025-04-28T00:53:49.838Z",
      "title": "El módulo modelo de Rare Rare es útil para el análisis de lenguaje multimodal? MMLA: Benchmark complejo",
      "submittedOnDailyBy": {
        "_id": "669090c01e3f5b16ce22b535",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
        "isPro": false,
        "fullname": "Hanlei Zhang",
        "user": "HanleiZhang",
        "type": "user"
      },
      "summary": "El análisis de lenguaje multimodal es un campo que ha estado en rápido desarrollo para entender la profunda semántica de las conversaciones humanas utilizando diversos módulos. Aunque este campo es crucial, la investigación sobre la capacidad de los modelos multimodal de lenguaje (MLLMs) para comprender semánticas cognitivas es insuficiente. En este artículo, se presenta una solución para este vacío, presentando un benchmark específico llamado MMLA. MMLA incluye más de 61K diálogos multimodales y está compuesto por escenarios iterativos y escenarios reales, cubriendo seis dimensiones clave de la semántica multimodal: intención, emoción, acciones de diálogo, sentimiento, estilo de conversación y acciones de comunicación. Se evalúan en tres métodos a los 8 principales áreas de los modelos de lenguaje grande (LLMs) y MLLMs: inferencia 0-shot, ajuste de subproyectos y ajuste manual. Los resultados de los experimentos extensos muestran que incluso los modelos moderadamente ajustados pueden alcanzar una precisión del 60% a 70%, lo que claramente demuestra las limitaciones actuales de los MLLMs en la comprensión de lenguaje humano complejo. MMLA proporciona una sólida base para el análisis de lenguaje multimodal en modelos de lenguaje grande y ofrece recursos valiosos para el progreso de este campo. El dataset y el código están disponibles en la versión abierta en GitHub: https://github.com/thuiar/MMLA.",
      "upvotes": 9,
      "discussionId": "680c48825ec65044c2861ac4",
      "githubRepo": "https://github.com/thuiar/MMLA",
      "ai_keywords": [
        "multimodal language models (MLLMs)",
        "MMLA (Multimodal Language Analysis)",
        "multimodal utterances",
        "intent",
        "emotion",
        "dialogue act",
        "sentiment",
        "speaking style",
        "communication behavior",
        "zero-shot inference",
        "supervised fine-tuning",
        "instruction tuning",
        "large language models (LLMs)"
      ]
    },
    "publishedAt": "2025-04-23T01:25:13.000Z",
    "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
    "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669090c01e3f5b16ce22b535",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
      "fullname": "Hanlei Zhang",
      "name": "HanleiZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17768",
      "authors": [
        {
          "_id": "680f2668db85fd31cd5080ff",
          "name": "Piotr Nawrot",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508100",
          "name": "Robert Li",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508101",
          "name": "Renjie Huang",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508102",
          "name": "Sebastian Ruder",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508103",
          "name": "Kelly Marchisio",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508104",
          "name": "Edoardo M. Ponti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T17:39:25.000Z",
      "submittedOnDailyAt": "2025-04-28T05:26:26.185Z",
      "title": "Sparse Frontier: Punto de Equilibrio de la Foco Esparso en Transformer LLMs",
      "submittedOnDailyBy": {
        "_id": "640deb5d3c82bd463ee44735",
        "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
        "isPro": false,
        "fullname": "Piotr Nawrot",
        "user": "pnawrot",
        "type": "user"
      },
      "summary": "Sparse attention es una estrategia potencialmente útil para ampliar la capacidad de procesamiento de contexto largo de los LLMs Transformer, pero la posibilidad, eficiencia y precisión entre ellas, así como la investigación sobre escalado sistematico, aún no han sido exploradas. Para llenar esta brecha, se comparan con mayor detalle diferentes métodos de atención esparsa no entrenados en un conjunto de tareas de contexto largo (que incluye tareas basadas en lenguaje natural, pero mantienen control y evaluación sencillas). Basándose en los resultados de los experimentos, se reportan los siguientes hallazgos principales: 1) en análisis isoFLOPS, en casos de contextos muy largos, modelos grandes y altamente esparsos son superiores a modelos pequeños y densos. 2) El nivel de esparsidad necesario para mantener la precisión estadística es más alto durante el procesamiento previo que durante el decodificación, lo que indica que en el decodificación, este nivel puede depender del tamaño del modelo. 3) No se encuentra un punto óptimo de esparsificación para tareas o etapas completas, sino que depende de escenarios con diferentes unidades de esparsificación y adaptación visual. Esto implica que, a niveles moderadamente esparsos, se observa una notable pérdida de rendimiento en al menos una tarea, lo que subraya que la atención esparsa no es una solución general. 4) Se propone una regla de escalado específica para la atención esparsa y se demuestra que esta regla existe y se aplica fuera del rango de los experimentos. Basándose en estas observaciones, la atención esparsa es una herramienta importante para mejorar la capacidad de procesamiento de contexto largo de los LLMs Transformer, y es necesario cuidar cuidadosamente el equilibrio entre rendimiento y costo en aplicaciones de spin-off.",
      "upvotes": 7,
      "discussionId": "680f2669db85fd31cd50815e",
      "ai_keywords": [
        "Sparse attention",
        "Transformer LLMs",
        "Training-free",
        "IsoFLOPS analysis",
        "Sequence lengths",
        "Sparsity levels",
        "Long-sequence tasks",
        "Natural language",
        "Accuracy preservation",
        "Decoding",
        "Prefilling",
        "Budget adaptivity",
        "Performance degradation",
        "Scaling laws"
      ]
    },
    "publishedAt": "2025-04-24T13:39:25.000Z",
    "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
    "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17768.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "640deb5d3c82bd463ee44735",
      "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
      "fullname": "Piotr Nawrot",
      "name": "pnawrot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17816",
      "authors": [
        {
          "_id": "680ed2679e529f7799a0689f",
          "user": {
            "_id": "636b20591340f879a2eb98d0",
            "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
            "isPro": false,
            "fullname": "Daneul Kim",
            "user": "carpedkm",
            "type": "user"
          },
          "name": "Daneul Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-28T07:38:37.502Z",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a0",
          "name": "Jingxu Zhang",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a1",
          "name": "Wonjoon Jin",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a2",
          "name": "Sunghyun Cho",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a3",
          "user": {
            "_id": "65115c00a588fdb36558b673",
            "avatarUrl": "/avatars/1f36263dc4bfaf696a4aa959a6aab1e1.svg",
            "isPro": false,
            "fullname": "Qi Dai",
            "user": "daiqi",
            "type": "user"
          },
          "name": "Qi Dai",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-28T00:57:12.914Z",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a4",
          "name": "Jaesik Park",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a5",
          "user": {
            "_id": "676a328148d749b7086782d0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Tt7u8l8f_1oVBWmBp7tkm.png",
            "isPro": false,
            "fullname": "Chong Luo",
            "user": "cluo-ms",
            "type": "user"
          },
          "name": "Chong Luo",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-28T00:57:12.914Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/636b20591340f879a2eb98d0/ZUDDcDv2cTWIFx65RIEbG.mp4"
      ],
      "publishedAt": "2025-04-23T06:48:31.000Z",
      "submittedOnDailyAt": "2025-04-28T07:02:51.113Z",
      "title": "As I begin this translation task, I'm aware of the importance of maintaining the original meaning and context while ensuring the translation is both accurate and professional. Here is the translation of the provided text into Spanish:\n\n**\"Creación de vídeos enfocada en el tema: Identidades diferenciadas y movimientos\"**\n\nThis translation captures the essence of the original text, maintaining the focus on theme-driven video creation with a specific emphasis on unique identities and movements. The translation is intended to be clear, professional, and accurate, adhering to the highest standards of quality.",
      "submittedOnDailyBy": {
        "_id": "636b20591340f879a2eb98d0",
        "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
        "isPro": false,
        "fullname": "Daneul Kim",
        "user": "carpedkm",
        "type": "user"
      },
      "summary": "Proponemos un método para entrenar un modelo de generación de vídeos personalizados sin entrenamiento adicional en un entorno 0-shot, separando el aprendizaje por objetos y la dinámica temporal. Los tradicionales métodos de tuning sin entrenamiento para vídeos personalizados se basan en grandes y etiquetados conjuntos de vídeos, lo cual es costoso computacionalmente y requiere etiquetado extenso. En contraste con estos enfoques, presentamos un método que utiliza directamente un conjunto de imágenes personalizadas para entrenar el modelo de vídeo personalizado. El modelo de vídeo personalizado se divide en dos partes: (1) la Inyección de Reconocimiento con datos de imágenes personalizadas y (2) el método de entrenamiento de imagen-a-vídeo utilizando un pequeño conjunto de vídeos no etiquetados para preservar la modelación temporal. Además, para mitigar los problemas de copia y pega durante el ajuste micro, utilizamos la eliminación de tokens de imágenes aleatorias y la inicialización de imágenes aleatorias en el entrenamiento de imagen-a-vídeo. Para mejorar el aprendizaje, introducimos un cambio probabilístico en la optimización conjunta de características por objetos y temporales para mitigar los olvidos críticos. Nuestro método presenta una fuerte consistencia por objetos y una extensibilidad, demostrando un excelente rendimiento en un entorno 0-shot, contribuyendo a demostrar la efectividad de nuestro marco de trabajo.",
      "upvotes": 5,
      "discussionId": "680ed2689e529f7799a06907",
      "projectPage": "https://carpedkm.github.io/projects/disentangled_sub/",
      "githubRepo": "https://github.com/carpedkm/disentangled-subject-to-vid",
      "ai_keywords": [
        "subject-specific learning",
        "temporal dynamics",
        "image customization dataset",
        "identity injection",
        "temporal modeling",
        "image-to-video training method",
        "random image token dropping",
        "randomized image initialization",
        "image-to-video fine-tuning",
        "stochastic switching",
        "joint optimization",
        "catastrophic forgetting",
        "subject consistency",
        "zero-shot settings"
      ]
    },
    "publishedAt": "2025-04-23T02:48:31.000Z",
    "title": "Subject-driven Video Generation via Disentangled Identity and Motion",
    "summary": "We propose to train a subject-driven customized video generation model\nthrough decoupling the subject-specific learning from temporal dynamics in\nzero-shot without additional tuning. A traditional method for video\ncustomization that is tuning-free often relies on large, annotated video\ndatasets, which are computationally expensive and require extensive annotation.\nIn contrast to the previous approach, we introduce the use of an image\ncustomization dataset directly on training video customization models,\nfactorizing the video customization into two folds: (1) identity injection\nthrough image customization dataset and (2) temporal modeling preservation with\na small set of unannotated videos through the image-to-video training method.\nAdditionally, we employ random image token dropping with randomized image\ninitialization during image-to-video fine-tuning to mitigate the copy-and-paste\nissue. To further enhance learning, we introduce stochastic switching during\njoint optimization of subject-specific and temporal features, mitigating\ncatastrophic forgetting. Our method achieves strong subject consistency and\nscalability, outperforming existing video customization models in zero-shot\nsettings, demonstrating the effectiveness of our framework.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/636b20591340f879a2eb98d0/ZUDDcDv2cTWIFx65RIEbG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17816.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636b20591340f879a2eb98d0",
      "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
      "fullname": "Daneul Kim",
      "name": "carpedkm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15716",
      "authors": [
        {
          "_id": "680dcc5d3478de07603a8036",
          "user": {
            "_id": "642656cbad1e3b0e6e91b752",
            "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
            "isPro": false,
            "fullname": "Jie Zhu",
            "user": "amazingj",
            "type": "user"
          },
          "name": "Jie Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-28T07:39:02.713Z",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8037",
          "name": "Qian Chen",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8038",
          "name": "Huaixia Dou",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8039",
          "name": "Junhui Li",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803a",
          "name": "Lifan Guo",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803b",
          "name": "Feng Chen",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803c",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T09:01:04.000Z",
      "submittedOnDailyAt": "2025-04-28T06:16:26.234Z",
      "title": "DianJin-R1: Evaluación y mejora financiera de modelos de lenguaje de gran escala",
      "submittedOnDailyBy": {
        "_id": "642656cbad1e3b0e6e91b752",
        "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
        "isPro": false,
        "fullname": "Jie Zhu",
        "user": "amazingj",
        "type": "user"
      },
      "summary": "La lógica basada en razones es un problema crucial en el campo financiero de los grandes modelos de lenguaje (LLMs), lo cual se ha reconocido. Para resolver estos problemas, se requieren conocimientos específicos del sector, cálculos numéricos precisos y la cumplimiento de reglas estrictas. Proponemos un marco de trabajo para fortalecer la lógica basada en razones llamado DianJin-R1, con el objetivo de resolver estos problemas. El enfoque clave de nuestro trabajo es DianJin-R1-Data. Este conjunto de datos se ha construido a partir de CFLUE, FinQA y un corpus personalizado de violaciones (Verificación de Violaciones en China, CCC). Este conjunto de datos combina escalas de lógica basada en razones en diferentes áreas financieras y notas validadas. Nuestros modelos, DianJin-R1-7B y DianJin-R1-32B, se han construido a partir de Qwen2.5-7B-Instruct y Qwen2.5-32B-Instruct, y se han ajustado utilizando un formato estructurado para generar etapas basadas en razones y las respuestas finales. Para mejorar la calidad de la lógica basada en razones, aplicamos la Group Relative Policy Optimization (GRPO), un método de aprendizaje por refuerzo que incluye un señal de recompensa para promover salidas estructuradas y otro para la precisión de la respuesta. Nuestros modelos se evaluaron en 5 benchmarks: 3 datasets financieros (CFLUE, FinQA, CCC) y 2 benchmarks generales de lógica basada en razones (MATH-500, GPQA-Diamond). Los resultados de los experimentos muestran que los modelos DianJin-R1 superan a los modelos sin lógica basada en razones, especialmente en tareas financieras complejas. Además, en el dataset de CCC, nuestro modelo basado en razones se ha demostrado capaz de superar el rendimiento de sistemas multi-agente con alto costo computacional. Estos hallazgos demuestran la eficacia de DianJin-R1 en fortalecer la lógica basada en razones en el financiamiento a través de la aprendizaje estructurado y la optimización de recompensas, ofreciendo soluciones escalables y prácticas para aplicaciones financieras reales.",
      "upvotes": 5,
      "discussionId": "680dcc5e3478de07603a807e",
      "ai_keywords": [
        "reasoning-enhanced framework",
        "reasoning-augmented supervision",
        "reinforcement learning",
        "DianJin-R1-Data",
        "CFLUE",
        "FinQA",
        "Chinese Compliance Check (CCC)",
        "high-quality dataset",
        "DianJin-R1-7B",
        "DianJin-R1-32B",
        "Qwen2.5-7B-Instruct",
        "Qwen2.5-32B-Instruct",
        "structured format",
        "reasoning steps",
        "Group Relative Policy Optimization (GRPO)",
        "dual reward signals",
        "structured outputs",
        "answer correctness",
        "MATH-500",
        "GPQA-Diamond",
        "financial datasets",
        "single-call reasoning models",
        "multi-agent systems",
        "real-world applications"
      ]
    },
    "publishedAt": "2025-04-22T05:01:04.000Z",
    "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large\n  Language Models",
    "summary": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15716.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642656cbad1e3b0e6e91b752",
      "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
      "fullname": "Jie Zhu",
      "name": "amazingj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12080",
      "authors": [
        {
          "_id": "680afc5f2c4b584e1d786eee",
          "name": "Mengshi Qi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786eef",
          "user": {
            "_id": "66a8c8e4f5cda7b8690205ef",
            "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
            "isPro": false,
            "fullname": "Pengfei Zhu",
            "user": "zaplm",
            "type": "user"
          },
          "name": "Pengfei Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:30.195Z",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef0",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef1",
          "name": "Xiaoyang Bi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef2",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef3",
          "name": "Huadong Ma",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef4",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T13:41:59.000Z",
      "submittedOnDailyAt": "2025-04-28T02:11:37.182Z",
      "title": "DC-SAM: Método por doble coincidencia para la segmentación dentro del contexto en imágenes y vídeos",
      "submittedOnDailyBy": {
        "_id": "66a8c8e4f5cda7b8690205ef",
        "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
        "isPro": false,
        "fullname": "Pengfei Zhu",
        "user": "zaplm",
        "type": "user"
      },
      "summary": "Cuando se proporciona un ejemplo de etiqueta para datos gráficos, la segmentación en contexto tiene como objetivo segmentar las objetos correspondientes. Esta configuración se conoce como segmentación de pocos ejemplos (few-shot segmentation) y se aplica en tareas visuales variadas, como comprensión de escenarios y edición de imágenes o videos. Los modelos más recientes de Segment Anything (SAM) han logrado resultados más avanzados a través de segmentación interactiva, pero esta metodología no se aplica directamente a la segmentación en contexto. En este artículo, se propone un método basado en ajuste de prompt denominado Dual Consistency SAM (DC-SAM), que se aplica tanto a imágenes como a videos en la segmentación en contexto. La idea principal consiste en fortalecer el encoder de prompts de SAM para proporcionar prompts visuales de alta calidad. Se fusiona la característica de SAM al crear la máscara, lo que permite que el encoder de prompts se ajuste mejor. A continuación, se diseña una atención cruzada iterativa que se alinea con los prompts iniciales y los características fusionadas. Luego, se propone un diseño binario utilizando prompts positivos y negativos distintos dentro del encoder de prompts. Además, se diseña una estrategia de entrenamiento sencilla para aplicar el método de doble consistencia a la segmentación de máscaras. DC-SAM se ha aplicado principalmente a imágenes, pero con el apoyo de SAM2, se puede ampliar a la área de videos. Debido a que no existe segmentación en contexto en videos, se auto-corrige el primer benchmark IC-VOS (In-Context Video Object Segmentation) y se construye utilizando conjuntos de datos de segmentación de videos existentes. Con este benchmark, se logran experimentos extendidos que alcanzan un mIoU de 55.5 (+1.4) en COCO-20i, un mIoU de 73.0 (+1.1) en PASCAL-5i y un J&F score de 71.52 en el benchmark IC-VOS propuesto. La fuente código de DC-SAM y el benchmark están disponibles en https://github.com/zaplm/DC-SAM.",
      "upvotes": 5,
      "discussionId": "680afc622c4b584e1d786f9e",
      "ai_keywords": [
        "prompt-tuning",
        "prompt encoder",
        "mask prior",
        "cycle-consistent cross-attention",
        "dual-branch design",
        "discriminative positive prompts",
        "negative prompts",
        "mask-tube",
        "In-Context Video Object Segmentation (IC-VOS)",
        "mIoU"
      ]
    },
    "publishedAt": "2025-04-16T09:41:59.000Z",
    "title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency",
    "summary": "Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12080.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a8c8e4f5cda7b8690205ef",
      "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
      "fullname": "Pengfei Zhu",
      "name": "zaplm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]