[
  {
    "paper": {
      "id": "2507.07095",
      "authors": [
        {
          "_id": "686f2579d938c25d68441b43",
          "name": "Ke Fan",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b44",
          "name": "Shunlin Lu",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b45",
          "user": {
            "_id": "6853b71ec1be83a29eb5ba36",
            "avatarUrl": "/avatars/ae205c2ec2c421a0d7851755b4f123a2.svg",
            "isPro": false,
            "fullname": "Minyue Dai",
            "user": "Jixi111",
            "type": "user"
          },
          "name": "Minyue Dai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:19.845Z",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b46",
          "name": "Runyi Yu",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b47",
          "name": "Lixing Xiao",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b48",
          "name": "Zhiyang Dou",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b49",
          "name": "Junting Dong",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b4a",
          "name": "Lizhuang Ma",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b4b",
          "name": "Jingbo Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T17:52:04.000Z",
      "submittedOnDailyAt": "2025-07-10T01:12:54.854Z",
      "title": "Go to Zero: Creación de la acción del cerebro negativo (con datos de escala de millón)",
      "submittedOnDailyBy": {
        "_id": "66d59dc9b005ad82ca6fc61d",
        "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
        "isPro": false,
        "fullname": "Runyi YU",
        "user": "IngridYU",
        "type": "user"
      },
      "summary": "Generar diferentes secuencias de acciones naturales basadas en texto es un campo de investigación básico y difícil en la visión computacional, grafica y robótica. En este campo, se ha avanzado, pero los métodos actuales tienen problemas debido a la limitación del tamaño de los conjuntos de datos de aprendizaje, lo que limita su capacidad de generalización sin ejemplos (0-shot). Además, la falta de marcos de evaluación impide determinar direcciones de mejora, lo que se ven afectados por este problema. En este artículo, se trabaja para traer a la generación de acciones desde el texto al nuevo siglo, con el objetivo de lograr capacidad de generalización sin ejemplos (0-shot). Para ello, se desarrolla un eficiente pipeline de anotación y se presenta el conjunto de datos de acciones más grande hasta el momento, el MotionMillion. Este conjunto de datos se caracteriza por 200,000 secuencias de acciones de alta calidad y más de 2,000 horas de secuencias de acciones. Además, se propone el MotionMillion-Eval, el más amplio marco de evaluación para la generación de acciones sin ejemplos (0-shot). Se utiliza una arquitectura escalable para expandir el modelo a 7B parámetros y se valida su rendimiento en el MotionMillion-Eval. Este resultado muestra una fuerte capacidad de generalización para acciones complejas en estructuras no predecibles y marca un paso importante en la generación de acciones humanas sin ejemplos. El código está disponible en https://github.com/VankouF/MotionMillion-Codes.",
      "upvotes": 32,
      "discussionId": "686f2579d938c25d68441b4c",
      "ai_summary": "A new dataset and evaluation framework improve zero-shot text-to-motion generation through a large-scale, high-quality dataset and a scalable model architecture.",
      "ai_keywords": [
        "MotionMillion",
        "MotionMillion-Eval",
        "zero-shot motion generation",
        "scalable architecture"
      ]
    },
    "publishedAt": "2025-07-09T13:52:04.000Z",
    "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data",
    "summary": "Generating diverse and natural human motion sequences based on textual\ndescriptions constitutes a fundamental and challenging research area within the\ndomains of computer vision, graphics, and robotics. Despite significant\nadvancements in this field, current methodologies often face challenges\nregarding zero-shot generalization capabilities, largely attributable to the\nlimited size of training datasets. Moreover, the lack of a comprehensive\nevaluation framework impedes the advancement of this task by failing to\nidentify directions for improvement. In this work, we aim to push\ntext-to-motion into a new era, that is, to achieve the generalization ability\nof zero-shot. To this end, firstly, we develop an efficient annotation pipeline\nand introduce MotionMillion-the largest human motion dataset to date, featuring\nover 2,000 hours and 2 million high-quality motion sequences. Additionally, we\npropose MotionMillion-Eval, the most comprehensive benchmark for evaluating\nzero-shot motion generation. Leveraging a scalable architecture, we scale our\nmodel to 7B parameters and validate its performance on MotionMillion-Eval. Our\nresults demonstrate strong generalization to out-of-domain and complex\ncompositional motions, marking a significant step toward zero-shot human motion\ngeneration. The code is available at\nhttps://github.com/VankouF/MotionMillion-Codes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07095.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66d59dc9b005ad82ca6fc61d",
      "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
      "fullname": "Runyi YU",
      "name": "IngridYU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06448",
      "authors": [
        {
          "_id": "686f326dd938c25d68441b6a",
          "name": "Zhenhailong Wang",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6b",
          "name": "Xuehang Guo",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6c",
          "name": "Sofia Stoica",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6d",
          "user": {
            "_id": "645b10e80c73ea27d13f7aca",
            "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
            "isPro": false,
            "fullname": "xuhaiyang",
            "user": "xhyandwyy",
            "type": "user"
          },
          "name": "Haiyang Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:17.834Z",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6e",
          "name": "Hongru Wang",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6f",
          "name": "Hyeonjeong Ha",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b70",
          "name": "Xiusi Chen",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b71",
          "name": "Yangyi Chen",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b72",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b73",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b74",
          "name": "Heng Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T23:22:34.000Z",
      "submittedOnDailyAt": "2025-07-10T01:56:27.555Z",
      "title": "Percepción-Consciente Optimización de Políticas para Razonamiento Multimodal",
      "submittedOnDailyBy": {
        "_id": "628d7265db4cd1d1717c884f",
        "avatarUrl": "/avatars/dff2a3dd10d84b4a73fa486402de7219.svg",
        "isPro": false,
        "fullname": "Zhenhailong Wang",
        "user": "mikewang",
        "type": "user"
      },
      "summary": "RLVR ha demostrado que es una estratégia muy efectiva cuando los Grandes Modelos de Lenguaje (LLMs) tienen una fuerte capacidad de inferencia multi-paso. Sin embargo, su diseño y optimización están limitados a la área de texto, lo que hace que su rendimiento no se optimice al aplicarlos a tareas de inferencia multi-modelo. En particular, se ha descubierto que los errores principales en la inferencia multi-modelo actual son debidos a la observación de la entrada visual. Para resolver este problema, se propone la Perception-Aware Policy Optimization (PAPO). PAPO es una simple y efectiva extensión de GRPO, que guia el modelo para aprender razones y observar señales sub-privadas internas. En particular, PAPO no depende de la curación de datos adicionales, modelos de recompensa externos o modelos de propiedad. Concretamente, se agrega un Implicit Perception Loss a los objetos de GRPO, utilizando la divergencia de KL, lo que, aunque es simple, provoca una mejora significativa en varios benchmarks multi-modelo (4.4%). Esta mejora es más clara en tareas con alta dependencia visual (8.0%). Además, se ha confirmado que la reducción de errores observacionales es significativa (30.5%) y que la capacidad de observación se ha mejorado. Se ha realizado un análisis detallado de PAPO y se ha analizado con rigor utilizando la pérdida de Entropía Doble. En total, nuestro estudio establece la base para un nuevo marco de referencia de aprendizaje por refuerzo que impulsa el objetivo de aprendizaje de RLVR y la razón basada en la visión. Página del proyecto: https://mikewangwzhl.github.io/PAPO.",
      "upvotes": 23,
      "discussionId": "686f326dd938c25d68441b75",
      "projectPage": "https://mikewangwzhl.github.io/PAPO",
      "githubRepo": "https://github.com/MikeWangWZHL/PAPO",
      "ai_summary": "Perception-Aware Policy Optimization (PAPO) enhances reinforcement learning with verifiable rewards for multimodal reasoning by integrating implicit perception loss, improving visual perception and reasoning.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "Large Language Models (LLMs)",
        "multimodal reasoning tasks",
        "Perception-Aware Policy Optimization (PAPO)",
        "GRPO",
        "Implicit Perception Loss",
        "KL divergence",
        "Double Entropy Loss",
        "visually grounded reasoning"
      ],
      "githubStars": 10
    },
    "publishedAt": "2025-07-08T19:22:34.000Z",
    "title": "Perception-Aware Policy Optimization for Multimodal Reasoning",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose Perception-Aware Policy\nOptimization (PAPO), a simple yet effective extension of GRPO that encourages\nthe model to learn to perceive while learning to reason, entirely from internal\nsupervision signals. Notably, PAPO does not rely on additional data curation,\nexternal reward models, or proprietary models. Specifically, we introduce the\nImplicit Perception Loss in the form of a KL divergence term to the GRPO\nobjective, which, despite its simplicity, yields significant overall\nimprovements (4.4%) on diverse multimodal benchmarks. The improvements are more\npronounced, approaching 8.0%, on tasks with high vision dependency. We also\nobserve a substantial reduction (30.5%) in perception errors, indicating\nimproved perceptual capabilities with PAPO. We conduct comprehensive analysis\nof PAPO and identify a unique loss hacking issue, which we rigorously analyze\nand mitigate through a Double Entropy Loss. Overall, our work introduces a\ndeeper integration of perception-aware supervision into RLVR learning\nobjectives and lays the groundwork for a new RL framework that encourages\nvisually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "628d7265db4cd1d1717c884f",
      "avatarUrl": "/avatars/dff2a3dd10d84b4a73fa486402de7219.svg",
      "fullname": "Zhenhailong Wang",
      "name": "mikewang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06920",
      "authors": [
        {
          "_id": "686f1da1d938c25d68441b1b",
          "user": {
            "_id": "677e869467f3bb8d8215eec6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
            "isPro": false,
            "fullname": "Zihan Ma",
            "user": "MichaelErchi",
            "type": "user"
          },
          "name": "Zihan Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:33.413Z",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b1c",
          "name": "Taolin Zhang",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b1d",
          "name": "Maosong Cao",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b1e",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b1f",
          "name": "Minnan Luo",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b20",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b21",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T14:58:47.000Z",
      "submittedOnDailyAt": "2025-07-10T00:31:10.862Z",
      "title": "Revisión de la validación de Rietiding y generación de código de LLM: Código generación en el contexto de las pruebas",
      "submittedOnDailyBy": {
        "_id": "677e869467f3bb8d8215eec6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
        "isPro": false,
        "fullname": "Zihan Ma",
        "user": "MichaelErchi",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) han logrado un éxito notable recientemente en marcos de evaluación como HumanEval y LiveCodeBench. Sin embargo, una investigación detallada revela que estos sistemas de evaluación generalmente incluyen un número limitado de casos de prueba homogéneos y que casi no detectan errores. Esto se produce junto con la pérdida de una evaluación de recompensa precisa en frameworks de aprendizaje por refuerzo (RLVR) que subestiman el rendimiento y utilizan recompensas visualizables. Para solucionar estas deficiencias, se propone una investigación sistemática de la tarea de generación de casos de prueba (TCG), proponiendo diferentes métricas para cuantificar con precisión los detalles de los sistemas de prueba, y introduciendo un enfoque cooperativo humano-LLM (SAGA) que combina el conocimiento de programación humano y la capacidad de razonamiento de los LLMs, lo que significa una significativa mejora en la cobertura y calidad de los casos de prueba generados. Además, se desarrolla TCGBench para apoyar la investigación en la tarea de TCG. Los resultados de los experimentos muestran que SAGA logra un rendimiento de detección del 90.62% y una precisión en los datos de validación del 32.58%. La precisión del verificador de códigos generados por SAGA es del 10.78% más alta que en LiveCodeBench-v6. Estos resultados demuestran el efecto de nuestro método propuesto. Esperamos que esta investigación contribuya a la construcción escalable de evaluaciones confiables de código generado por LLMs, fomente el desarrollo de RLVR en la generación de código, y abra caminos para la síntesis automática de pruebas contrarias y la integración adaptativa de marcos de referencia.",
      "upvotes": 18,
      "discussionId": "686f1da1d938c25d68441b22",
      "ai_summary": "A human-LLM collaborative method enhances code generation test case generation, improving reliability and detection rates in code evaluation benchmarks.",
      "ai_keywords": [
        "large language models",
        "code-generation",
        "HumanEval",
        "LiveCodeBench",
        "test-case generation",
        "multi-dimensional metrics",
        "human-LLM collaboration",
        "SAGA",
        "TCGBench",
        "reinforcement learning frameworks",
        "verifiable rewards",
        "RLVR",
        "verifier accuracy",
        "adversarial test synthesis",
        "adaptive benchmark integration"
      ]
    },
    "publishedAt": "2025-07-09T10:58:47.000Z",
    "title": "Rethinking Verification for LLM Code Generation: From Generation to\n  Testing",
    "summary": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06920.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "677e869467f3bb8d8215eec6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
      "fullname": "Zihan Ma",
      "name": "MichaelErchi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07105",
      "authors": [
        {
          "_id": "686f5cbad938c25d68441bb2",
          "user": {
            "_id": "643e9efa2263cdc630f88f5c",
            "avatarUrl": "/avatars/96cea51f17e7d41ffb6a4b438e05f5cb.svg",
            "isPro": false,
            "fullname": "Yushen Zuo",
            "user": "YSZuo",
            "type": "user"
          },
          "name": "Yushen Zuo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T09:12:43.813Z",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb3",
          "name": "Qi Zheng",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb4",
          "name": "Mingyang Wu",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb5",
          "name": "Xinrui Jiang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb6",
          "name": "Renjie Li",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb7",
          "name": "Jian Wang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb8",
          "name": "Yide Zhang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb9",
          "name": "Gengchen Mai",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bba",
          "name": "Lihong V. Wang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bbb",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bbc",
          "name": "Xiaoyu Wang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bbd",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bbe",
          "user": {
            "_id": "62548d5fef3debb2ddf91217",
            "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
            "isPro": false,
            "fullname": "Zhengzhong Tu",
            "user": "vztu",
            "type": "user"
          },
          "name": "Zhengzhong Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T09:12:41.972Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ESnx_PS3_HUQoNDY5cSqI.png"
      ],
      "publishedAt": "2025-07-09T17:59:19.000Z",
      "submittedOnDailyAt": "2025-07-10T04:56:36.746Z",
      "title": "4KAgent: 4K Agent",
      "submittedOnDailyBy": {
        "_id": "62548d5fef3debb2ddf91217",
        "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
        "isPro": false,
        "fullname": "Zhengzhong Tu",
        "user": "vztu",
        "type": "user"
      },
      "summary": "Presentamos 4KAgent. Este es un sistema de generalización de super resolución agente que puede convertir todas las imágenes en resolución 4K (y hasta resoluciones más altas cuando se aplica repetidamente). Nuestro sistema puede transformar imágenes con graves defectos de calidad en resoluciones muy bajas, como 256x256, a imágenes de resolución 4K determinante. 4KAgent está compuesto por tres componentes clave: (1) Profiling, personalización del flujo de trabajo de 4KAgent basado en casos de uso específicos; (2) Agente Sensorial, que analiza las imágenes mediante modelos de lenguaje visual y expertos en evaluación de calidad de imagen para crear planes de recuperación adaptativos; (3) Agente de Recuperación, que selecciona la mejor salida de acuerdo con un paradigma de ejecución-reflexión recursivo para ejecutar el plan. Además, 4KAgent incluye una pipeline personalizada para la recuperación de rostros, mejorando significativamente los detalles faciales en portados y fotos de autos. Hemos evaluado 4KAgent con 26 diferentes benchmarks, incluyendo 11 categorías de trabajo, para asegurar que tenga el nivel más reciente en una amplia gama de campos de imágenes, desde imágenes naturales hasta contenido generado por IA, imágenes satelitales, microscopía fluorescente y imágenes médicas (como: endoscopio óptico, ultrasonido, rayos X). Nuestra evaluación demostró excelentes resultados en índices sensoriales (como NIQE, MUSIQ) y precisión (como PSNR). Desarrollamos un nuevo paradigma de agente para trabajos visuales de bajo nivel, lo que fomenta la interés y la innovación en comunidades de investigación hacia agentes autónomos centrados en la visión. Todo código, modelos y resultados se publicarán en https://4kagent.github.io.",
      "upvotes": 15,
      "discussionId": "686f5cbbd938c25d68441bbf",
      "projectPage": "https://4kagent.github.io/",
      "githubRepo": "https://github.com/taco-group/4KAgent",
      "ai_summary": "4KAgent, a unified agentic super-resolution system, enhances low-resolution images to 4K using profiling, perception, and restoration agents, achieving state-of-the-art performance across various imaging domains.",
      "ai_keywords": [
        "agentic super-resolution",
        "Profiling",
        "Perception Agent",
        "vision-language models",
        "image quality assessment",
        "Restoration Agent",
        "recursive execution-reflection",
        "quality-driven mixture-of-experts",
        "face restoration pipeline",
        "NIQE",
        "MUSIQ",
        "PSNR",
        "low-level vision tasks",
        "autonomous agents"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-07-09T13:59:19.000Z",
    "title": "4KAgent: Agentic Any Image to 4K Super-Resolution",
    "summary": "We present 4KAgent, a unified agentic super-resolution generalist system\ndesigned to universally upscale any image to 4K resolution (and even higher, if\napplied iteratively). Our system can transform images from extremely low\nresolutions with severe degradations, for example, highly distorted inputs at\n256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three\ncore components: (1) Profiling, a module that customizes the 4KAgent pipeline\nbased on bespoke use cases; (2) A Perception Agent, which leverages\nvision-language models alongside image quality assessment experts to analyze\nthe input image and make a tailored restoration plan; and (3) A Restoration\nAgent, which executes the plan, following a recursive execution-reflection\nparadigm, guided by a quality-driven mixture-of-expert policy to select the\noptimal output for each step. Additionally, 4KAgent embeds a specialized face\nrestoration pipeline, significantly enhancing facial details in portrait and\nselfie photos. We rigorously evaluate our 4KAgent across 11 distinct task\ncategories encompassing a total of 26 diverse benchmarks, setting new\nstate-of-the-art on a broad spectrum of imaging domains. Our evaluations cover\nnatural images, portrait photos, AI-generated content, satellite imagery,\nfluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and\nX-ray, demonstrating superior performance in terms of both perceptual (e.g.,\nNIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic\nparadigm for low-level vision tasks, we aim to catalyze broader interest and\ninnovation within vision-centric autonomous agents across diverse research\ncommunities. We will release all the code, models, and results at:\nhttps://4kagent.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ESnx_PS3_HUQoNDY5cSqI.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07105.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62548d5fef3debb2ddf91217",
      "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
      "fullname": "Zhengzhong Tu",
      "name": "vztu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.06457",
      "authors": [
        {
          "_id": "686f2371d938c25d68441b36",
          "name": "Dustin Wang",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b37",
          "user": {
            "_id": "63ff09f24852102d4871c19c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
            "isPro": false,
            "fullname": "Rui-Jie Zhu",
            "user": "ridger",
            "type": "user"
          },
          "name": "Rui-Jie Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:30.737Z",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b38",
          "name": "Steven Abreu",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b39",
          "name": "Yong Shan",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3a",
          "name": "Taylor Kergan",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3b",
          "name": "Yuqi Pan",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3c",
          "name": "Yuhong Chou",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3d",
          "name": "Zheng Li",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3e",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3f",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b40",
          "name": "Jason Eshraghian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T23:54:11.000Z",
      "submittedOnDailyAt": "2025-07-10T02:04:35.136Z",
      "title": "Análisis sistemático de la atención lineal híbrida",
      "submittedOnDailyBy": {
        "_id": "63ff09f24852102d4871c19c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
        "isPro": false,
        "fullname": "Rui-Jie Zhu",
        "user": "ridger",
        "type": "user"
      },
      "summary": "Transformers experimentan problemas de complejidad doble y memoria con secuencias largas, lo cual se resuelve mediante el uso de estados ocultos de tamaño fijo y la introducción de una estructura de atención lineal. Sin embargo, los modelos lineales suelen tener limitaciones en el rendimiento de memoria, por lo que se han introducido arquitecturas híbridas que combinan la atención lineal y la atención completa. Aunque la investigación en estas arquitecturas ha sido amplia, no ha habido una investigación profunda sobre la elección de los componentes de atención lineal. Hemos construido estructuras de gating desarrolladas en la representación de vectores, y evaluamos tanto modelos puros como híbridos. Para ello, entrenamos y abrimos fuentes 72 modelos: 36 modelos con 340M parámetros (20B tokens) y 36 con 1.3B parámetros (100B tokens), incluyendo 6 variantes de atención lineal con 5 proporciones híbridas. A través de evaluaciones en modelos de lenguaje y tareas de memoria, se demuestra claramente que los mejores modelos puros no superan necesariamente a los híbridos. La proporción de atención completa en modelos lineales es estable y mejora significativamente la performance de memoria con un aumento de la capa de atención completa, siendo especialmente efectivos proporciones inferiores a 3:1. Nuestra investigación destaca la importancia de gating selectivo, representación estratificada y olvido controlado en modelos híbridos. Recomendamos arquitecturas como HGRN-2 y GatedDeltaNet, con el objetivo de alcanzar un rendimiento de memoria transformador-nivel con una proporción de atención lineal a completa entre 3:1 y 6:1. Nuestros modelos están abiertos en https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.",
      "upvotes": 13,
      "discussionId": "686f2371d938c25d68441b41",
      "projectPage": "https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e",
      "ai_summary": "Research evaluates various linear attention models and their integration with full attention in Transformers, identifying key mechanisms like selective gating and hierarchical recurrence for enhanced recall performance.",
      "ai_keywords": [
        "quadratic complexity",
        "linear attention mechanisms",
        "full attention layers",
        "hybrid architectures",
        "vector recurrences",
        "gating mechanisms",
        "recall performance",
        "language modeling",
        "recall tasks",
        "selective gating",
        "hierarchical recurrence",
        "controlled forgetting",
        "HGRN-2",
        "GatedDeltaNet"
      ]
    },
    "publishedAt": "2025-07-08T19:54:11.000Z",
    "title": "A Systematic Analysis of Hybrid Linear Attention",
    "summary": "Transformers face quadratic complexity and memory issues with long sequences,\nprompting the adoption of linear attention mechanisms using fixed-size hidden\nstates. However, linear models often suffer from limited recall performance,\nleading to hybrid architectures that combine linear and full attention layers.\nDespite extensive hybrid architecture research, the choice of linear attention\ncomponent has not been deeply explored. We systematically evaluate various\nlinear attention models across generations - vector recurrences to advanced\ngating mechanisms - both standalone and hybridized. To enable this\ncomprehensive analysis, we trained and open-sourced 72 models: 36 at 340M\nparameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six\nlinear attention variants across five hybridization ratios. Benchmarking on\nstandard language modeling and recall tasks reveals that superior standalone\nlinear models do not necessarily excel in hybrids. While language modeling\nremains stable across linear-to-full attention ratios, recall significantly\nimproves with increased full attention layers, particularly below a 3:1 ratio.\nOur study highlights selective gating, hierarchical recurrence, and controlled\nforgetting as critical for effective hybrid models. We recommend architectures\nsuch as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1\nto achieve Transformer-level recall efficiently. Our models are open-sourced at\nhttps://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06457.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ff09f24852102d4871c19c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
      "fullname": "Rui-Jie Zhu",
      "name": "ridger",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07017",
      "authors": [
        {
          "_id": "686f39e8d938c25d68441b89",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tianyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:04.799Z",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8a",
          "user": {
            "_id": "65d2251f98b4a470bf6a26e3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d2251f98b4a470bf6a26e3/C4T0LHYGejrI9mu_k3M8p.jpeg",
            "isPro": false,
            "fullname": "xts",
            "user": "xtsssss",
            "type": "user"
          },
          "name": "Tianshun Xing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T09:12:45.801Z",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8b",
          "name": "Qingshui Gu",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8c",
          "name": "Taoran Liang",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8d",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8e",
          "name": "Xin Zhou",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8f",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b90",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b91",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b92",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b93",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b94",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b95",
          "name": "Zejun Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T16:45:48.000Z",
      "submittedOnDailyAt": "2025-07-10T02:29:04.666Z",
      "title": "First Return, Entropy-Eliciting Explore",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "La aprendizaje por refuerzo con recompensas confirmables (RLVR) puede mejorar la capacidad de tomar decisiones de modelos de lenguaje grandes (LLMs), pero enfrenta desafíos en la exploración inestable. Proponemos un robot de exploración orientado a objetivos para construir retroalimentación intermedia basada en contexto, enfocado en el camino que explica la alta incertidumbre de la decisión. FR3E (First Return, Entropy-Eliciting Explore) proporciona pautas específicas para no depender de supervías densas. A través de experimentos en el benchmark matemático (AIME24), FR3E promueve el aprendizaje estable, genera respuestas largas y coherentes, y aumenta la proporción de rutas completamente precisas. Estos resultados revelan que el efecto del marco de exploración más robusto y estructurado para mejorar la capacidad de tomar decisiones de los LLMs se ha demostrado.",
      "upvotes": 12,
      "discussionId": "686f39e8d938c25d68441b96",
      "ai_summary": "FR3E enhances LLM reasoning by providing structured exploration through targeted rollouts at high-uncertainty points, leading to more stable training and accurate responses.",
      "ai_keywords": [
        "Reinforcement Learning from Verifiable Rewards",
        "RLVR",
        "Large Language Models",
        "LLMs",
        "FR3E",
        "First Return",
        "Entropy-Eliciting Explore",
        "structured exploration",
        "high-uncertainty decision points",
        "reasoning trajectories",
        "targeted rollouts",
        "semantically grounded intermediate feedback",
        "mathematical reasoning benchmarks",
        "AIME24",
        "stable training",
        "coherent responses",
        "fully correct trajectories"
      ]
    },
    "publishedAt": "2025-07-09T12:45:48.000Z",
    "title": "First Return, Entropy-Eliciting Explore",
    "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07017.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 50
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05687",
      "authors": [
        {
          "_id": "686e2c00a5f0f70d9de40c8c",
          "name": "Shangzhan Li",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c8d",
          "name": "Zefan Wang",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c8e",
          "name": "Ye He",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c8f",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c90",
          "user": {
            "_id": "62ccd26d376917c022420a46",
            "avatarUrl": "/avatars/629858b1c7419dbcdbead3484a36abd1.svg",
            "isPro": false,
            "fullname": "Qi Shi",
            "user": "qshi",
            "type": "user"
          },
          "name": "Qi Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:49:03.237Z",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c91",
          "name": "Jianling Li",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c92",
          "name": "Yonggang Hu",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c93",
          "name": "Wanxiang Che",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c94",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c95",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c96",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T05:38:24.000Z",
      "submittedOnDailyAt": "2025-07-10T00:44:20.590Z",
      "title": "AutoTriton: Programación Automática de Reino de Tronos para Aprendizaje por Refuerzo",
      "submittedOnDailyBy": {
        "_id": "62ccd26d376917c022420a46",
        "avatarUrl": "/avatars/629858b1c7419dbcdbead3484a36abd1.svg",
        "isPro": false,
        "fullname": "Qi Shi",
        "user": "qshi",
        "type": "user"
      },
      "summary": "El desarrollo de kernels en deep learning requiere la optimización de las funciones de hardware, lo que implica la gestión del memoria, cálculos paralelos y ajustes para las características propias de la hardware. Como un lenguaje específico de dominio, Triton abstracta los detalles de bajo nivel para facilitar el programación de GPUs, pero los desarrolladores deben ajustar parámetros cruciales como el tamaño de las ventanas y patrones de acceso a la memoria, lo que representa una gran barrera para lograr un rendimiento óptimo y una amplia adopción. En esta investigación, se presenta el primer modelo de programación de Triton especializado llamado AutoTriton, que incorpora aprendizaje por refuerzo (RL). AutoTriton utiliza una pipeline de monitoreo de datos de alta calidad para ajustar la conocimiento básico de programación de Triton mediante aprendizaje supervisado (SFT) y mejora su capacidad de programación a través de la combinación de recompensas basadas en reglas y basadas en ejecución utilizando el algoritmo de Policy Optimization Group Relative (GRPO). Los experimentos realizados en los 5 canales de evaluación de TritonBench y KernelBench muestran que nuestro modelo de 8B, AutoTriton, logra rendimientos comparables a los de los modelos de gran escala predominantes. Estos resultados demuestran que el aprendizaje por refuerzo puede generar kerneles de alto rendimiento automáticamente, y que estos kerneles son partes esenciales de los sistemas de IA. Esta innovación proporciona una base fundamental para la construcción de sistemas de IA más eficientes. El modelo y el código están disponibles en https://github.com/AI9Stars/AutoTriton.",
      "upvotes": 8,
      "discussionId": "686e2c00a5f0f70d9de40c97"
    },
    "publishedAt": "2025-07-08T01:38:24.000Z",
    "title": "AutoTriton: Automatic Triton Programming with Reinforcement Learning in\n  LLMs",
    "summary": "Kernel development in deep learning requires optimizing computational units\nacross hardware while balancing memory management, parallelism, and\nhardware-specific optimizations through extensive empirical tuning. Although\ndomain-specific languages like Triton simplify GPU programming by abstracting\nlow-level details, developers must still manually tune critical parameters such\nas tile sizes and memory access patterns through iterative experimentation,\ncreating substantial barriers to optimal performance and wider adoption. In\nthis work, we introduce AutoTriton, the first model dedicated to Triton\nprogramming powered by reinforcement learning (RL). AutoTriton performs\nsupervised fine-tuning (SFT) to be equipped with essential Triton programming\nexpertise using a high-quality data gathering pipeline, and conducts RL with\nGroup Relative Policy Optimization (GRPO) algorithm, combining a rule-based\nreward and an execution-based reward to further improve Triton programming\nability, sequentially. Experiments across five evaluation channels of\nTritonBench and KernelBench illustrate that our 8B model AutoTriton achieves\nperformance comparable to mainstream large models, including Claude-4-Sonnet\nand DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial\nrole of each module within AutoTriton, including the SFT stage, the RL stage,\nand the reward design strategy. These findings underscore the promise of RL for\nautomatically generating high-performance kernels, and since high-performance\nkernels are core components of AI systems, this breakthrough establishes an\nimportant foundation for building more efficient AI systems. The model and code\nwill be available at https://github.com/AI9Stars/AutoTriton.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05687.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ccd26d376917c022420a46",
      "avatarUrl": "/avatars/629858b1c7419dbcdbead3484a36abd1.svg",
      "fullname": "Qi Shi",
      "name": "qshi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.06804",
      "authors": [
        {
          "_id": "686f1f4dd938c25d68441b24",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b25",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b26",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b27",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b28",
          "name": "Feng Zhang",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b29",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b2a",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T22:38:49.000Z",
      "submittedOnDailyAt": "2025-07-10T00:33:32.493Z",
      "title": "Desarrollo de métodos para resolver problemas complejos de IMO a partir de explicaciones proporcionadas",
      "submittedOnDailyBy": {
        "_id": "62ffa3f8311cad266f9af236",
        "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
        "isPro": false,
        "fullname": "Zhenwen Liang",
        "user": "invokerliang",
        "type": "user"
      },
      "summary": "La comprobación automática de teoremas (ATP) en lenguajes formales es uno de los problemas fundamentales de la inteligencia artificial. Los modelos de la memoria recurrente de largo alcance (LLMs) han experimentado un desarrollo sorprendente, pero sigue existiendo un gran vacío entre su capacidad para la lógica no formal y el rendimiento de los pruebas de comprobación de teoremas formales. Según los últimos estudios, la precisión no formal supera el 80%, mientras que el rendimiento de éxito en pruebas de tipo PutnamBench es menor del 8%. Proponemos que estas discrepancias surgen debido a que los mejores sistemas de comprobación actuales se han enfocado en la rigor de la lógica y en el aprendizaje profundo de estrategias basadas en texto superficial. Para superar estas limitaciones, proponemos un nuevo marco de trabajo que separa la lógica formal de la generación de teoremas. Nuestro enfoque utiliza dos modelos especializados: un potente Reasoner que genera teoremas para diferentes objetivos estratégicos y un eficiente Prover que los comprueba rigurosamente. Este diseño modular libera la capacidad lógica del modelo y evita los limites de entrenamiento en el extremo. Nuestro método se ha evaluado en una serie de problemas complejos de la IMO desde 2000, problemas en los que ningún sistema de comprobación abierto ha tenido éxito. Nuestro marco de trabajo separado ha logrado resolver 5 de esos problemas, demostrando una importante avancada en la lógica automática para desafíos matemáticos complejos. Para fomentar futuras investigaciones, publicamos un conjunto completo de datos de teoremas generados y comprobados de problemas IMO desde 2000. Este conjunto de datos está disponible en https://tencent-imo.github.io/.",
      "upvotes": 8,
      "discussionId": "686f1f4ed938c25d68441b2b",
      "ai_summary": "A novel framework decouples reasoning and proving in ATP to improve formal proving performance, achieving success on challenging IMO problems.",
      "ai_keywords": [
        "Automated Theorem Proving",
        "Large Language Models",
        "formal proving",
        "informal reasoning",
        "PutnamBench",
        "subgoal lemmas",
        "Reasoner",
        "Prover",
        "modular design",
        "end-to-end training",
        "IMO problems"
      ]
    },
    "publishedAt": "2025-07-07T18:38:49.000Z",
    "title": "Towards Solving More Challenging IMO Problems via Decoupled Reasoning\n  and Proving",
    "summary": "Automated Theorem Proving (ATP) in formal languages is a foundational\nchallenge for AI. While Large Language Models (LLMs) have driven remarkable\nprogress, a significant gap remains between their powerful informal reasoning\ncapabilities and their weak formal proving performance. Recent studies show\nthat the informal accuracy exceeds 80% while formal success remains below 8% on\nbenchmarks like PutnamBench. We argue this gap persists because current\nstate-of-the-art provers, by tightly coupling reasoning and proving, are\ntrained with paradigms that inadvertently punish deep reasoning in favor of\nshallow, tactic-based strategies. To bridge this fundamental gap, we propose a\nnovel framework that decouples high-level reasoning from low-level proof\ngeneration. Our approach utilizes two distinct, specialized models: a powerful,\ngeneral-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an\nefficient Prover to rigorously verify them. This modular design liberates the\nmodel's full reasoning potential and bypasses the pitfalls of end-to-end\ntraining. We evaluate our method on a challenging set of post-2000 IMO\nproblems, a problem set on which no prior open-source prover has reported\nsuccess. Our decoupled framework successfully solves 5 of these problems,\ndemonstrating a significant step towards automated reasoning on exceptionally\ndifficult mathematical challenges. To foster future research, we release our\nfull dataset of generated and verified lemmas for a wide range of IMO problems,\navailable at https://tencent-imo.github.io/ .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06804.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ffa3f8311cad266f9af236",
      "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
      "fullname": "Zhenwen Liang",
      "name": "invokerliang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.24044",
      "authors": [
        {
          "_id": "68681d82213f123a1f88b973",
          "user": {
            "_id": "683daabc402acb18654e4674",
            "avatarUrl": "/avatars/9307b4c1c248e45b2daa8ffa1d74d4b4.svg",
            "isPro": false,
            "fullname": "Sicong Jiang",
            "user": "Max2045",
            "type": "user"
          },
          "name": "Sicong Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:52:40.893Z",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b974",
          "name": "Zilin Huang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b975",
          "name": "Kangan Qian",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b976",
          "name": "Ziang Luo",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b977",
          "name": "Tianze Zhu",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b978",
          "name": "Yang Zhong",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b979",
          "name": "Yihong Tang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97a",
          "name": "Menglin Kong",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97b",
          "name": "Yunlong Wang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97c",
          "name": "Siwen Jiao",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97d",
          "name": "Hao Ye",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97e",
          "name": "Zihao Sheng",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97f",
          "name": "Xin Zhao",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b980",
          "name": "Tuopu Wen",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b981",
          "name": "Zheng Fu",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b982",
          "name": "Sikai Chen",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b983",
          "name": "Kun Jiang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b984",
          "name": "Diange Yang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b985",
          "name": "Seongjin Choi",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b986",
          "name": "Lijun Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T16:50:02.000Z",
      "submittedOnDailyAt": "2025-07-10T01:10:17.895Z",
      "title": "Investigación sobre la autonomía en modelos de visión, lenguaje y acción",
      "submittedOnDailyBy": {
        "_id": "683daabc402acb18654e4674",
        "avatarUrl": "/avatars/9307b4c1c248e45b2daa8ffa1d74d4b4.svg",
        "isPro": false,
        "fullname": "Sicong Jiang",
        "user": "Max2045",
        "type": "user"
      },
      "summary": "El rápido avance de los grandes modelos de lenguaje multimodelo (MLLM) ha abierto un camino claro para la implementación del paradigma de Visión-Lenguaje-Acción (VLA), que integra la percepción visual, el entendimiento del lenguaje natural y el control en un solo conjunto de reglas. Los investigadores en el campo de la conducción automática están aplicando estos métodos al dominio vehicular. Estos modelos tienen la capacidad de comprender indicaciones de alto nivel, considerar escenarios de tráfico complejos y tomar decisiones autonomamente, lo que es lo que se espera de un vehículo autónomo. Sin embargo, la literatura está dispersa y está expandiéndose rápidamente. Esta investigación proporciona una primera visión general completa de VLA para la conducción automática (VLA4AD). Hemos formalizado los bloques de construcción compartidos en los trabajos recientes (i), seguido la evolución de modelos VLA racionales desde los primeros explicadores (ii), y comparado más de 20 modelos representativos basados en el progreso de VLA en el dominio de la conducción automática (iii). Además, hemos integrado existentes conjuntos de datos y marcos de evaluación, y proponemos un protocolo para medir la seguridad de conducción, precisión y calidad de la explicación. Finalmente, explicamos en detalle los desafíos abiertos (robustez, eficiencia en tiempo real, pruebas formales) y ofrecemos una visión de futuro para VLA4AD. Esta investigación se propone como una resumen claro y completo de los avances socialmente aceptados en vehículos autónomos. El repositorio GitHub está disponible en https://github.com/JohnsonJiang1996/Awesome-VLA4AD{SicongJiang/Awesome-VLA4AD}.",
      "upvotes": 7,
      "discussionId": "68681d82213f123a1f88b987",
      "githubRepo": "https://github.com/JohnsonJiang1996/Awesome-VLA4AD",
      "ai_summary": "This survey provides a comprehensive overview of Vision-Language-Action (VLA) paradigms and their adaptation for autonomous driving, detailing architectural components, evolution of models, datasets, and future challenges.",
      "ai_keywords": [
        "multimodal large language models",
        "Vision-Language-Action",
        "VLA",
        "VLA for Autonomous Driving",
        "VLA4AD",
        "explainer",
        "reasoning-centric models",
        "autonomous driving",
        "driving safety",
        "accuracy",
        "explanation quality",
        "robustness",
        "real-time efficiency",
        "formal verification",
        "interpretable socially aligned autonomous vehicles"
      ],
      "githubStars": 173
    },
    "publishedAt": "2025-06-30T12:50:02.000Z",
    "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
    "summary": "The rapid progress of multimodal large language models (MLLM) has paved the\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\nperception, natural language understanding, and control within a single policy.\nResearchers in autonomous driving are actively adapting these methods to the\nvehicle domain. Such models promise autonomous vehicles that can interpret\nhigh-level instructions, reason about complex traffic scenes, and make their\nown decisions. However, the literature remains fragmented and is rapidly\nexpanding. This survey offers the first comprehensive overview of VLA for\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\nshared across recent work, (ii) trace the evolution from early explainer to\nreasoning-centric VLA models, and (iii) compare over 20 representative models\naccording to VLA's progress in the autonomous driving domain. We also\nconsolidate existing datasets and benchmarks, highlighting protocols that\njointly measure driving safety, accuracy, and explanation quality. Finally, we\ndetail open challenges - robustness, real-time efficiency, and formal\nverification - and outline future directions of VLA4AD. This survey provides a\nconcise yet complete reference for advancing interpretable socially aligned\nautonomous vehicles. Github repo is available at\nhttps://github.com/JohnsonJiang1996/Awesome-VLA4AD{SicongJiang/Awesome-VLA4AD}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.24044.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "683daabc402acb18654e4674",
      "avatarUrl": "/avatars/9307b4c1c248e45b2daa8ffa1d74d4b4.svg",
      "fullname": "Sicong Jiang",
      "name": "Max2045",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.06853",
      "authors": [
        {
          "_id": "686f4142d938c25d68441b98",
          "user": {
            "_id": "64e84ec6d41a68b065bf78a7",
            "avatarUrl": "/avatars/bae3c5e3210b40af6e4f113e85f3e206.svg",
            "isPro": false,
            "fullname": "Liang Wang",
            "user": "AzureLeon1",
            "type": "user"
          },
          "name": "Liang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:02.989Z",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b99",
          "name": "Yu Rong",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9a",
          "name": "Tingyang Xu",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9b",
          "name": "Zhenyi Zhong",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9c",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9d",
          "name": "Pengju Wang",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9e",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9f",
          "name": "Qiang Liu",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441ba0",
          "name": "Shu Wu",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441ba1",
          "name": "Liang Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/zoojDRIzYMaSv2bj_dh7R.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/Lc9mfYTjI12Vz0qsq_JWj.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/brRCb577NUJrWNDqDkcMJ.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/LV-8rRgSCOr0UmateiCPM.jpeg"
      ],
      "publishedAt": "2025-07-09T13:57:20.000Z",
      "submittedOnDailyAt": "2025-07-10T03:05:48.871Z",
      "title": "DiffSpectra: Modelo de ramificación en espectros a través de la clarificación de la estructura molecular",
      "submittedOnDailyBy": {
        "_id": "64e84ec6d41a68b065bf78a7",
        "avatarUrl": "/avatars/bae3c5e3210b40af6e4f113e85f3e206.svg",
        "isPro": false,
        "fullname": "Liang Wang",
        "user": "AzureLeon1",
        "type": "user"
      },
      "summary": "La clarificación de la estructura molecular es un problema fundamental en la química, que tiene un gran impacto en la identificación, síntesis y desarrollo de fármacos. Los métodos tradicionales dependen significativamente de la interpretación de expertos y tienen una tendencia a ser escalables. Los métodos de aprendizaje automático de vanguardia han introducido estrategias basadas en búsqueda, pero dependen de una base de datos limitada, lo que limita la generalización a nuevos moléculas. Los modelos generativos podrían ser una alternativa adecuada, pero la mayoría adoptan arquitecturas basadas en SMILES auto-regresivos, ignoran la estructura 3D y presentan dificultades para integrar diferentes espectros. En este estudio, se propone DiffSpectras, un marco de trabajo generativo que infere directamente las estructuras moleculares 2D y 3D. DiffSpectras configura la clarificación de la estructura como un proceso generativo condicional. La red de eliminación de ruido se parametriza como Diffusion Molecule Transformer, una arquitectura invariante en SE(3) que integra información topológica y geométrica. La condición se proporciona mediante SpecFormer, una herramienta de aprendizaje profundo que captura las relaciones dependientes dentro del espectro a partir de diferentes espectros. La validación de escala muestra que DiffSpectras alcanza altas precisiones en la clarificación de la estructura, con una precisión top-1 del 16.01% y una precisión top-20 del 96.86% en la reconstrucción de estructuras. El modelo se mejora significativamente con la modelización geométrica 3D, la entrenamiento previo de SpecFormer y diferentes condiciones. Estos resultados demuestran claramente la efectividad de la modelación de la varianza condicional en la clarificación de la estructura molecular. Según nuestra experiencia, DiffSpectras es el primer marco de trabajo que integra la clarificación de nuevas estructuras moleculares con diferentes espectros y un modelado generativo común 2D/3D.",
      "upvotes": 2,
      "discussionId": "686f4142d938c25d68441ba2",
      "ai_summary": "DiffSpectra uses diffusion models with SE(3)-equivariant architecture and SpecFormer spectral encoder to accurately infer both 2D and 3D molecular structures from multi-modal spectral data.",
      "ai_keywords": [
        "diffusion models",
        "SE(3)-equivariant architecture",
        "Diffusion Molecule Transformer",
        "SpecFormer",
        "spectral encoder",
        "multi-modal spectral reasoning",
        "joint 2D/3D generative modeling",
        "de novo molecular structure elucidation"
      ]
    },
    "publishedAt": "2025-07-09T09:57:20.000Z",
    "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using\n  Diffusion Models",
    "summary": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/zoojDRIzYMaSv2bj_dh7R.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/Lc9mfYTjI12Vz0qsq_JWj.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/brRCb577NUJrWNDqDkcMJ.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/LV-8rRgSCOr0UmateiCPM.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e84ec6d41a68b065bf78a7",
      "avatarUrl": "/avatars/bae3c5e3210b40af6e4f113e85f3e206.svg",
      "fullname": "Liang Wang",
      "name": "AzureLeon1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05455",
      "authors": [
        {
          "_id": "686f5ec6d938c25d68441bc1",
          "user": {
            "_id": "628c29a54c5a62a1d216c560",
            "avatarUrl": "/avatars/d21b4da766f87f47228112958666643b.svg",
            "isPro": false,
            "fullname": "Ashima Suvarna",
            "user": "Ashima",
            "type": "user"
          },
          "name": "Ashima Suvarna",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T09:12:39.834Z",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc2",
          "user": {
            "_id": "6543d269326cb9a32bd58e40",
            "avatarUrl": "/avatars/ddabbd80844e3dff676a8c2a182d920c.svg",
            "isPro": false,
            "fullname": "Christina",
            "user": "christinachance",
            "type": "user"
          },
          "name": "Christina Chance",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-10T09:16:25.997Z",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc3",
          "name": "Karolina Naranjo",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc4",
          "user": {
            "_id": "6189ddd0992df2640e3e7d40",
            "avatarUrl": "/avatars/925f2308fc412ae352b57a1b71815028.svg",
            "isPro": false,
            "fullname": "Hamid Palangi",
            "user": "hamidpalangi",
            "type": "user"
          },
          "name": "Hamid Palangi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-10T09:16:36.140Z",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc5",
          "user": {
            "_id": "65d9787bec3dc9ccf7344f6f",
            "avatarUrl": "/avatars/ab361193ed2287158f803fc792bb2df5.svg",
            "isPro": false,
            "fullname": "Sophie Hao",
            "user": "notaphonologist",
            "type": "user"
          },
          "name": "Sophie Hao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-10T09:16:42.125Z",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc6",
          "name": "Thomas Hartvigsen",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc7",
          "name": "Saadia Gabriel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T20:15:18.000Z",
      "submittedOnDailyAt": "2025-07-10T05:04:25.096Z",
      "title": "Modelos de Categorización: Representación de la voz de la comunidad en la seguridad en línea",
      "submittedOnDailyBy": {
        "_id": "61c5c25705aa54027c52f7b3",
        "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
        "isPro": false,
        "fullname": "Hritik Bansal",
        "user": "hbXNov",
        "type": "user"
      },
      "summary": "El análisis automático de lenguaje tóxico desempeña un papel crucial en la construcción de espacios en línea seguros e inclusivos. Sin embargo, es una tarea muy subjetiva, y el entendimiento de lenguaje tóxico se forma a través de las normas de la comunidad y su experiencia real. Los modelos actuales de análisis de lenguaje tóxico roban a las diversas perspectivas de comentaristas una única verdad, y el concepto de lenguaje tóxico se vuelve menos contextualizado. En este sentido, presentamos el Dataset MODELCITIZENS. Este dataset incluye 6.8K postes de redes sociales y 40K comentarios tóxicos, cubriendo diversas grupos de identidad. Para comprender la influencia del contexto en el lenguaje tóxico, agregamos a los postes de MODELCITIZENS escenarios de conversación generados por un modelo de lenguaje grande (LLM). Los últimos herramientas de análisis de lenguaje tóxico (por ejemplo, la API de Moderación de OpenAI, GPT-o4-mini) presentan bajos rendimientos en MODELCITIZENS, y su desempeño se reduce aún más cuando se aplican a los postes con contexto agregado. Finalmente, lanzamos LLAMACITIZEN-8B y GEMMACITIZEN-12B. Estos modelos se fine-tunan en MODELCITIZENS y se basan en la LLaMA y Gemma, superando a GPT-o4-mini en evaluaciones dentro de la distribución en un porcentaje mayor de 5.5%. Nuestros hallazgos subrayan la importancia de comentarios colaborativos y modelado en la moderación de contenido inclusivo. Los datos, modelos y códigos de MODELCITIZENS están disponibles en https://github.com/asuvarna31/modelcitizens.",
      "upvotes": 2,
      "discussionId": "686f5ec7d938c25d68441bc8",
      "ai_summary": "A new dataset and models for toxic language detection incorporate diverse community perspectives and conversational context, improving accuracy over existing tools.",
      "ai_keywords": [
        "MODELCITIZENS",
        "LLM-generated conversational scenarios",
        "OpenAI Moderation API",
        "GPT-o4-mini",
        "LLAMACITIZEN-8B",
        "GEMMACITIZEN-12B",
        "LLaMA-based models",
        "Gemma-based models",
        "in-distribution evaluations",
        "community-informed annotation",
        "inclusive content moderation"
      ]
    },
    "publishedAt": "2025-07-07T16:15:18.000Z",
    "title": "ModelCitizens: Representing Community Voices in Online Safety",
    "summary": "Automatic toxic language detection is critical for creating safe, inclusive\nonline spaces. However, it is a highly subjective task, with perceptions of\ntoxic language shaped by community norms and lived experience. Existing\ntoxicity detection models are typically trained on annotations that collapse\ndiverse annotator perspectives into a single ground truth, erasing important\ncontext-specific notions of toxicity such as reclaimed language. To address\nthis, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K\ntoxicity annotations across diverse identity groups. To capture the role of\nconversational context on toxicity, typical of social media posts, we augment\nMODELCITIZENS posts with LLM-generated conversational scenarios.\nState-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,\nGPT-o4-mini) underperform on MODELCITIZENS, with further degradation on\ncontext-augmented posts. Finally, we release LLAMACITIZEN-8B and\nGEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,\nwhich outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our\nfindings highlight the importance of community-informed annotation and modeling\nfor inclusive content moderation. The data, models and code are available at\nhttps://github.com/asuvarna31/modelcitizens.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05455.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61c5c25705aa54027c52f7b3",
      "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
      "fullname": "Hritik Bansal",
      "name": "hbXNov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06260",
      "authors": [
        {
          "_id": "686f14e3d938c25d68441af8",
          "name": "Satyapriya Krishna",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441af9",
          "name": "Ninareh Mehrabi",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afa",
          "name": "Abhinav Mohanty",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afb",
          "name": "Matteo Memelli",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afc",
          "name": "Vincent Ponzo",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afd",
          "name": "Payal Motwani",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afe",
          "name": "Rahul Gupta",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T13:33:35.000Z",
      "submittedOnDailyAt": "2025-07-10T00:10:50.485Z",
      "title": "Framework de estabilidad de modelos de frente basado en Amazon's Nova Premier se evalua los principales riesgos.",
      "submittedOnDailyBy": {
        "_id": "6186fef1b1085ab638324e7f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6186fef1b1085ab638324e7f/BL6_WJCkxB-BatBUBilT8.jpeg",
        "isPro": false,
        "fullname": "Satya",
        "user": "skrishna",
        "type": "user"
      },
      "summary": "NobaPremiuma es el modelo de lenguaje basado en diferencias más capacaz y profesional de Amazon. NobaPremiuma procesa frases, imágenes y vídeos en un ventana de contexto de 1,000,000 tokens, permitiendo a un solo prompt analizar un gran código de base, 400 páginas de documento y 90 minutos de video. Presentamos la primera evaluación detallada de los riesgos importantes de NobaPremiuma como base para el marco de seguridad de modelos de frontera. La evaluación se centra en tres áreas de alto riesgo: química, biología, radiación y energía nuclear (CBRN), operaciones cibernéticas hostiles y el desarrollo de IA automatizado. Combina auto-benchmarking, equipos de testeo de expertos y investigación de carga. Resumimos los métodos y descubrimientos clave. Basándonos en esta evaluación, se confirma que NobaPremiuma puede ser publicado según la comisión dedicada a la seguridad de la IA en la AI Security Symposium de París en 2025. Mientras nuevos riesgos y capacidades relacionadas con modelos de frontera se reconocen, continuamos con evaluaciones de seguridad y pipelines de mutación.",
      "upvotes": 0,
      "discussionId": "686f14e4d938c25d68441aff"
    },
    "publishedAt": "2025-07-07T09:33:35.000Z",
    "title": "Evaluating the Critical Risks of Amazon's Nova Premier under the\n  Frontier Model Safety Framework",
    "summary": "Nova Premier is Amazon's most capable multimodal foundation model and teacher\nfor model distillation. It processes text, images, and video with a\none-million-token context window, enabling analysis of large codebases,\n400-page documents, and 90-minute videos in a single prompt. We present the\nfirst comprehensive evaluation of Nova Premier's critical risk profile under\nthe Frontier Model Safety Framework. Evaluations target three high-risk domains\n-- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber\nOperations, and Automated AI R&D -- and combine automated benchmarks, expert\nred-teaming, and uplift studies to determine whether the model exceeds release\nthresholds. We summarize our methodology and report core findings. Based on\nthis evaluation, we find that Nova Premier is safe for public release as per\nour commitments made at the 2025 Paris AI Safety Summit. We will continue to\nenhance our safety evaluation and mitigation pipelines as new risks and\ncapabilities associated with frontier models are identified.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06260.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6186fef1b1085ab638324e7f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6186fef1b1085ab638324e7f/BL6_WJCkxB-BatBUBilT8.jpeg",
      "fullname": "Satya",
      "name": "skrishna",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01702",
      "authors": [
        {
          "_id": "686f7ef5d938c25d6844295f",
          "name": "Zixin Chen",
          "hidden": false
        },
        {
          "_id": "686f7ef5d938c25d68442960",
          "name": "Hongzhan Lin",
          "hidden": false
        },
        {
          "_id": "686f7ef5d938c25d68442961",
          "name": "Kaixin Li",
          "hidden": false
        },
        {
          "_id": "686f7ef5d938c25d68442962",
          "name": "Ziyang Luo",
          "hidden": false
        },
        {
          "_id": "686f7ef5d938c25d68442963",
          "name": "Zhen Ye",
          "hidden": false
        },
        {
          "_id": "686f7ef5d938c25d68442964",
          "name": "Guang Chen",
          "hidden": false
        },
        {
          "_id": "686f7ef5d938c25d68442965",
          "name": "Zhiyong Huang",
          "hidden": false
        },
        {
          "_id": "686f7ef5d938c25d68442966",
          "name": "Jing Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T13:32:30.000Z",
      "submittedOnDailyAt": "2025-07-10T07:21:57.693Z",
      "title": "AdamMeme: Se realiza una investigación adaptativa sobre la función de la teoría lógica de modelos de lenguaje de gran escala multiestructural relacionada con la armonía.",
      "submittedOnDailyBy": {
        "_id": "6499466c7d1edf7cb612a9a6",
        "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
        "isPro": false,
        "fullname": "Hongzhan Lin",
        "user": "danielhzlin",
        "type": "user"
      },
      "summary": "En la era de los social media, la proliferación de diferentes memes requiere que los modelos de lenguaje multilingües (mLLMs) comprendan efectivamente la daño de estos memes. Los actuales mLLMs se evalúan mediante marcos de referencia que utilizan conjuntos de datos estáticos y se basan en criterios de precisión para evaluar de manera independiente al modelo. Estos marcos no ofrecen evaluaciones actualizadas y detalladas debido a la dinámica de los memes en línea. Para resolver estas limitaciones, proponemos un marco de evaluación agente-basado flexible llamado \"AdamMeme\", diseñado para verificar de manera adaptativa el rendimiento lógico de los mLLMs al comprender la daño de los memes. Mediante el colaboración entre los agentes, AdamMeme actualiza repetidamente los datos de los memes que incluyen muestras difíciles y revela las limitaciones específicas en las que los mLLMs interpretan la daño. Los experimentos extendidos muestran que nuestro marco de evaluación permite sistemáticamente descubrir los cambios en el rendimiento de los mLLMs con diferentes objetivos, detallar sus debilidades propias y ofrecer un análisis de gran detalle. Nuestro código está disponible en https://github.com/Lbotirx/AdamMeme.",
      "upvotes": 0,
      "discussionId": "686f7ef5d938c25d68442967",
      "ai_summary": "AdamMeme, an adaptive agent-based framework, evaluates multimodal Large Language Models' understanding of harmful memes through iterative updates and multi-agent collaboration, revealing model-specific weaknesses.",
      "ai_keywords": [
        "multimodal Large Language Models",
        "meme harmfulness",
        "agent-based evaluation framework",
        "multi-agent collaboration",
        "meme data updates",
        "model-specific weaknesses"
      ]
    },
    "publishedAt": "2025-07-02T09:32:30.000Z",
    "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large\n  Language Models on Harmfulness",
    "summary": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01702.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6499466c7d1edf7cb612a9a6",
      "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
      "fullname": "Hongzhan Lin",
      "name": "danielhzlin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]