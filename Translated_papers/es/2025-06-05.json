[
  {
    "paper": {
      "id": "2506.03569",
      "authors": [
        {
          "_id": "6841003e45e7d8a890731765",
          "name": "Xiaomi LLM-Core Team",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731767",
          "name": "Zihao Yue",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731768",
          "name": "Zhenru Lin",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731769",
          "name": "Yifan Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176a",
          "name": "Weikun Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176b",
          "user": {
            "_id": "60d2e681b8448e1785bbda06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
            "isPro": false,
            "fullname": "Shuhuai Ren",
            "user": "ShuhuaiRen",
            "type": "user"
          },
          "name": "Shuhuai Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:00.497Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176c",
          "user": {
            "_id": "642e72cec1b0f8e4e76af16d",
            "avatarUrl": "/avatars/f900811d3c22a114c67283b646949f86.svg",
            "isPro": false,
            "fullname": "shuhao gu",
            "user": "gsh33",
            "type": "user"
          },
          "name": "Shuhao Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:04.948Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176d",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176e",
          "name": "Peidian Li",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176f",
          "name": "Liang Zhao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731770",
          "user": {
            "_id": "6038d6d0612f5eef3cc05ea9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
            "isPro": false,
            "fullname": "Lei Li",
            "user": "tobiaslee",
            "type": "user"
          },
          "name": "Lei Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:07.044Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731771",
          "name": "Kainan Bao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731772",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731773",
          "name": "Hailin Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731774",
          "name": "Gang Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731775",
          "user": {
            "_id": "64d2fce8129a210e569e0c76",
            "avatarUrl": "/avatars/a79a832dc3a46ece1b9e542369fc4888.svg",
            "isPro": false,
            "fullname": "Dawei Zhu",
            "user": "dwzhu",
            "type": "user"
          },
          "name": "Dawei Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:02.720Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731776",
          "name": "Cici",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731777",
          "name": "Chenhong He",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731778",
          "name": "Bowen Ye",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731779",
          "name": "Bowen Shen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177a",
          "name": "Zihan Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177b",
          "name": "Zihan Jiang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177c",
          "name": "Zhixian Zheng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177d",
          "name": "Zhichao Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177e",
          "name": "Zhenbo Luo",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177f",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731780",
          "name": "Yudong Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731781",
          "name": "Yuanyuan Tian",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731782",
          "name": "Yu Tu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731783",
          "name": "Yihan Yan",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731784",
          "name": "Yi Huang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731785",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731786",
          "name": "Xinzhe Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731787",
          "name": "Xingchen Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731788",
          "name": "Xing Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731789",
          "name": "Xing Yong",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178a",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178b",
          "name": "Xiangwei Deng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178c",
          "name": "Wenyu Yang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178d",
          "name": "Wenhan Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178e",
          "name": "Weiwei Lv",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178f",
          "name": "Weiji Zhuang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731790",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731791",
          "name": "Sirui Deng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731792",
          "name": "Shuo Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731793",
          "name": "Shimao Chen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731794",
          "name": "Shihua Yu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731795",
          "name": "Shaohui Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731796",
          "name": "Shande Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731797",
          "name": "Rui Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731798",
          "name": "Qiantong Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731799",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179a",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179b",
          "name": "Menghang Zhu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179c",
          "name": "Kangyang Zhou",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179d",
          "name": "Kang Zhou",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179e",
          "name": "Kai Fang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179f",
          "name": "Jun Shi",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a0",
          "name": "Jinhao Dong",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a1",
          "name": "Jiebao Xiao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a2",
          "name": "Jiaming Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a3",
          "user": {
            "_id": "680e9a219e529f779991be0c",
            "avatarUrl": "/avatars/327b945649192b0881fe290298d10e23.svg",
            "isPro": false,
            "fullname": "Huaqiu Liu",
            "user": "Prestonprom",
            "type": "user"
          },
          "name": "Huaqiu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:58.279Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a4",
          "name": "Hongshen Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a5",
          "name": "Heng Qu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a6",
          "name": "Haochen Zhao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a7",
          "name": "Hanglong Lv",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a8",
          "name": "Guoan Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a9",
          "name": "Duo Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317aa",
          "name": "Dong Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ab",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ac",
          "name": "Chong Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ad",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ae",
          "name": "Can Cai",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317af",
          "name": "Bingquan Xia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T04:32:54.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:27.734Z",
      "title": "MiMo-VL Informe Técnico\n\nEl informe técnico MiMo-VL (Multiple Instance Multimodal Vision-Language) presenta las últimas tecnologías y futuros desarrollos en la área de Visión-Lenguaje (V&L), que son una tecnología esencial para procesar información de manera más eficiente y efectiva a través de la interacción entre información visual y lingüística. Este informe detalla rigurosamente los principios básicos, el contexto de desarrollo y las tendencias de investigación actuales, así como las perspectivas de futuro del MiMo-VL. Este documento proporciona información crucial para investigadores y desarrolladores en el campo de la inteligencia artificial, destacando la posibilidad de aplicaciones en diversas áreas.",
      "submittedOnDailyBy": {
        "_id": "6038d6d0612f5eef3cc05ea9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
        "isPro": false,
        "fullname": "Lei Li",
        "user": "tobiaslee",
        "type": "user"
      },
      "summary": "Nosotros publicamos dos potentes modelos visuo-lenguaje: MiMo-VL-7B-SFT y MiMo-VL-7B-RL, ambos en código abierto. Este modelo ofrece un rendimiento de nivel más reciente en comprensión visual general y procesamiento lógico multimodal. MiMo-VL-7B-RL supera a 35 de 40 evaluaciones, logrando un puntaje de 59.4 en OlympiadBench, superando a modelos con 78B parámetros. En la aplicación de GUI fijada, registra un puntaje de 56.1 en OSWorld-G, superando modelos como UI-TARS. Nuestro entrenamiento combina cuatro etapas de entrenamiento previo (2.4 billones de tokens) y una aprendizaje por refuerzo con políticas mixtas (MORL), integrando diversos señales de recompensa. Enfatizamos la importancia de datos de razonamiento de cadena de pensamiento de alta calidad en el entrenamiento previo y los desafíos de optimización en múltiples dominios, destacando las ventajas del aprendizaje por refuerzo mixto. Además, proporcionamos un conjunto evaluativo integral que incluye más de 50 tareas, fomentando la reproducibilidad y el desarrollo de la disciplina. Los puntos de chequeo del modelo y el conjunto completo de evaluaciones están disponibles en https://github.com/XiaomiMiMo/MiMo-VL.",
      "upvotes": 44,
      "discussionId": "6841004145e7d8a890731853",
      "githubRepo": "https://github.com/XiaomiMiMo/MiMo-VL",
      "ai_summary": "MiMo-VL-7B-SFT and MiMo-VL-7B-RL provide state-of-the-art general visual understanding and multimodal reasoning through four-stage pre-training and Mixed On-policy Reinforcement Learning, outperforming models with up to 78B parameters.",
      "ai_keywords": [
        "vision-language models",
        "multimodal reasoning",
        "four-stage pre-training",
        "Mixed On-policy Reinforcement Learning",
        "MORL",
        "Chain-of-Thought",
        "reproducibility"
      ]
    },
    "publishedAt": "2025-06-04T00:32:54.000Z",
    "title": "MiMo-VL Technical Report",
    "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03569.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6038d6d0612f5eef3cc05ea9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
      "fullname": "Lei Li",
      "name": "tobiaslee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04207",
      "authors": [
        {
          "_id": "684117e22db29aa7b403af8d",
          "name": "Shuang Chen",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af8e",
          "name": "Yue Guo",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af8f",
          "user": {
            "_id": "64264095ba51f8a2136946a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
            "isPro": false,
            "fullname": "Zhaochen Su",
            "user": "Warrieryes",
            "type": "user"
          },
          "name": "Zhaochen Su",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:45.759Z",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af90",
          "name": "Yafu Li",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af91",
          "name": "Yulun Wu",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af92",
          "user": {
            "_id": "65352acb7139c5dd8d9a8590",
            "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
            "isPro": false,
            "fullname": "JiachengChen",
            "user": "JC-Chen",
            "type": "user"
          },
          "name": "Jiacheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:48:38.463Z",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af93",
          "name": "Jiayu Chen",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af94",
          "name": "Weijie Wang",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af95",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af96",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:51:08.000Z",
      "submittedOnDailyAt": "2025-06-05T02:38:24.366Z",
      "title": "Avanzando con múltiples modelos lógicos: inicio optimizado frío y frío, con aprendizaje reforzado en etapas progresivas",
      "submittedOnDailyBy": {
        "_id": "65352acb7139c5dd8d9a8590",
        "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
        "isPro": false,
        "fullname": "JiachengChen",
        "user": "JC-Chen",
        "type": "user"
      },
      "summary": "DEEP SEEKI-R1 se caracteriza por su excelente capacidad lógica en tareas de contexto complejo, lo cual se logra a través de la aplicación de muchos estudios sobre aprendizaje por refuerzo (RL). Sin embargo, activar lógicas complejas es un desafío. En este artículo, se enfatiza la importancia de una inicialización efectiva para mejorar la capacidad lógica. Interesantemente, se observa que utilizar datos seleccionados mejores con solo datos de contexto puede llevar a un rendimiento superior que los modelos lógicos de varios modelos. Además, cuando se aplica el GRPO estándar en el aprendizaje por refuerzo de varios modelos, se produce un descenso de gradiente que afecta la estabilidad del aprendizaje y su rendimiento. Finalmente, para mejorar aún más la capacidad lógica de varios modelos, es necesario realizar aprendizaje por refuerzo solo sobre texto después de la etapa de aprendizaje por refuerzo de varios modelos. Este enfoque de aprendizaje en etapas puede mantener un equilibrio entre las bases visuales y el desarrollo cognitivo lógico. Integrando estas pautas, se resuelven los problemas de aprendizaje por refuerzo de varios modelos y se presenta ReVisual-R1. En MathVerse, MathVision, WeMath, LogicVista, DynaMath, y en los desafiantes marcos de referencia AIME2024 y AIME2025, se alcanzan nuevos rendimientos óptimos para los MLLMs de 7B de código abierto.",
      "upvotes": 35,
      "discussionId": "684117e32db29aa7b403afc2",
      "githubRepo": "https://github.com/CSfufu/Revisual-R1"
    },
    "publishedAt": "2025-06-04T13:51:08.000Z",
    "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning",
    "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04207.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65352acb7139c5dd8d9a8590",
      "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
      "fullname": "JiachengChen",
      "name": "JC-Chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04089",
      "authors": [
        {
          "_id": "684153cf911d1b3135fa5dfe",
          "name": "Anastasiia Ivanova",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5dff",
          "user": {
            "_id": "661af24d8328f43c6abc2d11",
            "avatarUrl": "/avatars/afe7eaf1f7a378dbcdba5cd3e86adf9c.svg",
            "isPro": false,
            "fullname": "Eva",
            "user": "tenebrissilvam",
            "type": "user"
          },
          "name": "Eva Bakaeva",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T10:00:02.435Z",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5e00",
          "user": {
            "_id": "64198f70ed725fef6442b37e",
            "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
            "isPro": false,
            "fullname": "Alexey Kovalev",
            "user": "AlexeyKov",
            "type": "user"
          },
          "name": "Zoya Volovikova",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T08:22:39.926Z",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5e01",
          "name": "Alexey K. Kovalev",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5e02",
          "name": "Aleksandr I. Panov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T15:47:07.000Z",
      "submittedOnDailyAt": "2025-06-05T07:08:16.935Z",
      "title": "AmbiK: Conjunto de datos de trabajo incierto en el entorno de cocina",
      "submittedOnDailyBy": {
        "_id": "64198f70ed725fef6442b37e",
        "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
        "isPro": false,
        "fullname": "Alexey Kovalev",
        "user": "AlexeyKov",
        "type": "user"
      },
      "summary": "Los Modelos de Lenguaje de Gran Tamaño (LLMs) son parte de los agentes automáticos, generalmente utilizados para decidir planes de acción según comandos de naturaleza lenguaje de usuario. Sin embargo, procesar comandos gramaticalmente ambiguos en entornos reales es una tarea muy difícil para los LLMs. Se han propuesto diversos métodos para detectar tareas gramaticalmente ambiguas, pero su comparación es difícil debido a la falta de conjuntos de datos o marcos de referencia comunes. En este sentido, se propone AmbiK (Conjunto de Datos de Tareas Gramaticalmente Ambiguas en Ambientes de Restaurante). AmbiK es un conjunto de datos completo de contexto para procesar instrucciones gramaticalmente ambiguas a los dispositivos, colectado con la ayuda de los LLMs y verificado por humanos. AmbiK incluye 1000 pares de tareas gramaticalmente ambiguas y sus correspondientes respuestas adaptadas, clasificadas por tipos de ambigüedad (preferencias humanas, conocimiento general, seguridad). Además, contiene 2000 conjuntos de datos integrales que incluyen descripciones del entorno, preguntas y respuestas, la intención del usuario y planes de tareas. AmbiK permite una comparación unificada de métodos de detección de ambigüedad y está disponible en https://github.com/cog-model/AmbiK-dataset.",
      "upvotes": 31,
      "discussionId": "684153cf911d1b3135fa5e2e",
      "ai_summary": "AmbiK, a textual dataset of ambiguous instructions for kitchen robots, enables unified comparison of ambiguity detection methods.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "behavior planning",
        "ambiguous instructions",
        "task ambiguity detection",
        "AmbiK",
        "dataset",
        "human-validated",
        "ambiguity types",
        "Human Preferences",
        "Common Sense Knowledge",
        "Safety",
        "environment descriptions",
        "clarifying questions",
        "user intents",
        "task plans"
      ]
    },
    "publishedAt": "2025-06-04T11:47:07.000Z",
    "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment",
    "summary": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04089.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64198f70ed725fef6442b37e",
      "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
      "fullname": "Alexey Kovalev",
      "name": "AlexeyKov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02921",
      "authors": [
        {
          "_id": "683ff4dcfbc9041ef7274c51",
          "user": {
            "_id": "657eea68f4f72f2c4c44640d",
            "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
            "isPro": false,
            "fullname": "Yijun YANG",
            "user": "thomasyyj",
            "type": "user"
          },
          "name": "Yijun Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:53:47.455Z",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c52",
          "name": "Zeyu Huang",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c53",
          "name": "Wenhao Zhu",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c54",
          "name": "Zihan Qiu",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c55",
          "name": "Fei Yuan",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c56",
          "name": "Jeff Z. Pan",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c57",
          "name": "Ivan Titov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T14:23:06.000Z",
      "submittedOnDailyAt": "2025-06-05T02:04:55.586Z",
      "title": "Controlable Inspection Similar to Natural Language Processing",
      "submittedOnDailyBy": {
        "_id": "657eea68f4f72f2c4c44640d",
        "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
        "isPro": false,
        "fullname": "Yijun YANG",
        "user": "thomasyyj",
        "type": "user"
      },
      "summary": "Actualmente, el marco de evaluación de modelos de lenguaje de largo contexto (LCLM) se clasifica en tareas de real mundo y tareas de síntesis. Ambas aproximaciones tienen limitaciones propias. Las tareas de real mundo se vuelven complejas, difíciles de interpretar y caracterizar, y son vulnerables a la contaminación de datos. Por otro lado, las tareas de síntesis adoptan el formato \"Naive in the High Stack\" (NIAH), y la discontinuidad entre \"Naive\" y \"High Stack\" destruyen la justificación de los modelos como representantes prácticos de aplicaciones. En respuesta a estos desafíos, se propone que un marco de evaluación ideal de largo contexto debe tener tres características básicas: contexto continuo, configuración controlable y evaluación de seguridad. En este estudio, se presenta un nuevo marco de referencia \"LongBioBench\" utilizando logros de negocios generados artificialmente, y se realizan evaluaciones de comprensión, inferencia y confianza en LCLM.\n\nEn la evaluación experimental, se incluyen 18 modelos de LCLM, y varios modelos presentan errores en la comprensión significativa de resultados de búsqueda y en la inferencia básica, con una disminución de confianza al aumentar la longitud del contexto. Un análisis profundo muestra que las decisiones de diseño, como la discontinuidad del contexto, la \"Naive\" de números y la ausencia de detección de errores, son vulnerabilidades en la verificación de la capacidad de largo contexto de los modelos. Además, la entrenamiento de predicciones continuas en largo contexto se ajusta principalmente a la adaptación de RoPE embeddings a largas longitudes de contexto. En resumen, comparado con los marcos de referencia de síntesis existentes, LongBioBench refleja tareas de lenguaje real, mantiene la posibilidad de control y logra un equilibrio mejorado, siendo altamente interpretable y configurable.",
      "upvotes": 25,
      "discussionId": "683ff4ddfbc9041ef7274c73",
      "githubRepo": "https://github.com/Thomasyyj/LongBio-Benchmark",
      "ai_summary": "LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.",
      "ai_keywords": [
        "long-context language models (LCLM)",
        "real-world tasks",
        "synthetic tasks",
        "needle-in-the-haystack (NIAH)",
        "seamless context",
        "controllable setting",
        "sound evaluation",
        "LongBioBench",
        "semantic understanding",
        "elementary reasoning",
        "trustworthiness",
        "long-context continual pretraining",
        "RoPE embedding"
      ]
    },
    "publishedAt": "2025-06-03T10:23:06.000Z",
    "title": "A Controllable Examination for Long-Context Language Models",
    "summary": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: seamless context, controllable setting, and\nsound evaluation. This study introduces LongBioBench, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\nunderstanding, reasoning, and trustworthiness.\nOur experimental evaluation, which includes 18 LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657eea68f4f72f2c4c44640d",
      "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
      "fullname": "Yijun YANG",
      "name": "thomasyyj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16968",
      "authors": [
        {
          "_id": "683656aefd55e753bf26ed3e",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T08:58:30.760Z",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed3f",
          "user": {
            "_id": "62676a94dacab364889bb36c",
            "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
            "isPro": false,
            "fullname": "SARIM HASHMI",
            "user": "Sarim-Hash",
            "type": "user"
          },
          "name": "Sarim Hashmi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:49:01.879Z",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed40",
          "user": {
            "_id": "62eaadf4086bd1debb30a122",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62eaadf4086bd1debb30a122/wgxsPVnkOuEfq1oqlUhiB.jpeg",
            "isPro": false,
            "fullname": "Gustavo Stahl",
            "user": "GustavoStahl",
            "type": "user"
          },
          "name": "Gustavo Bertolo Stahl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T08:31:48.782Z",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed41",
          "name": "Seung Hun Eddie Han",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed42",
          "name": "Salman Khan",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed43",
          "name": "Abdulrahman Mahmoud",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/T4ESSrZsC7163P3I8p17C.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/WF-SJEyKKtpa3Zq0JvBXA.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Hl8Dkgmc4QL_l9YKPhRvD.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/p-io7OU8TtxwvBp4_M1Hd.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/bu6bpeVfonZgrXopd9f79.png"
      ],
      "publishedAt": "2025-05-22T17:48:53.000Z",
      "submittedOnDailyAt": "2025-06-05T06:33:02.615Z",
      "title": "CASS: Transpléando de Nvidia a AMD utilizando datos, modelos y benchmarks de Nvidia",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "Introduzco CASS. Este es el primer grande conjunto de datos y sistema de modelos que se centra en la transacción de código de GPU entre arquitecturas de computadora. Se centra en la traducción a niveles de código fuente (CUDA ⇔ HIP) y de asamblea (Nvidia SASS ⇔ AMD RDNA3). Este conjunto de datos incluye 70k pares de códigos validados y resuelve importantes deficiencias en la potabilidad de código de GPU de bajo nivel tanto en el host como en el dispositivo. Utilizando este recurso, se entrena modelos de lenguaje especializados de la familia CASS, logrando una precisión de traducción del código fuente del 95% y una precisión de traducción de asamblea del 37.5%, superando significativamente líneas comerciales como GPT-4o, Claude y Hipify. Los códigos generados mantienen un rendimiento de nuevo paradigma en más de 85% de los casos de prueba, manteniendo un equilibrio entre tiempo de ejecución y memoria. Para evaluar estrictamente, presento CASS-Bench, un marco de referencia personalizado que extiende a 16 tipos de juegos de GPU y incluye resultados de ejecución reales. Todos los datos, modelos y herramientas de evaluación se han lanzado como código abierto para fomentar el desarrollo de compiladores de GPU, sesgos binarios y traducciones de hardware guiadas por modelos de lenguaje. El conjunto de datos y el marco de referencia están disponibles en https://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}}, mientras que el código está en https://github.com/GustavoStahl/CASS{blue{GitHub}}.",
      "upvotes": 23,
      "discussionId": "683656b0fd55e753bf26edf7",
      "githubRepo": "https://github.com/GustavoStahl/CASS",
      "ai_summary": "CASS is a dataset and model suite for GPU code transpilation at both source and assembly levels, achieving high accuracy and performance matching with native code.",
      "ai_keywords": [
        "cross-architecture GPU code transpilation",
        "CASS",
        "CUDA",
        "HIP",
        "Nvidia SASS",
        "AMD RDNA3",
        "domain-specific language models",
        "source translation accuracy",
        "assembly translation accuracy",
        "native performance",
        "CASS-Bench",
        "GPU compiler tooling",
        "binary compatibility",
        "LLM-guided hardware translation"
      ]
    },
    "publishedAt": "2025-05-22T13:48:53.000Z",
    "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
    "summary": "We introduce CASS, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level (CUDA\nleftrightarrow HIP) and assembly-level (Nvidia SASS leftrightarrow AMD\nRDNA3) translation. The dataset comprises 70k verified code pairs across host\nand device, addressing a critical gap in low-level GPU code portability.\nLeveraging this resource, we train the CASS family of domain-specific language\nmodels, achieving 95% source translation accuracy and 37.5% assembly\ntranslation accuracy, substantially outperforming commercial baselines such as\nGPT-4o, Claude, and Hipify. Our generated code matches native performance in\nover 85% of test cases, preserving runtime and memory behavior. To support\nrigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16\nGPU domains with ground-truth execution. All data, models, and evaluation tools\nare released as open source to foster progress in GPU compiler tooling, binary\ncompatibility, and LLM-guided hardware translation. Dataset and benchmark are\non\nhttps://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}},\nwith code at\nhttps://github.com/GustavoStahl/CASS{blue{GitHub}}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/T4ESSrZsC7163P3I8p17C.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/WF-SJEyKKtpa3Zq0JvBXA.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Hl8Dkgmc4QL_l9YKPhRvD.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/p-io7OU8TtxwvBp4_M1Hd.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/bu6bpeVfonZgrXopd9f79.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16968.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 39
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04180",
      "authors": [
        {
          "_id": "6840fefb3098ab525906d852",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d853",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d854",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:09.305Z",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d855",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d856",
          "name": "Roy Ka-Wei Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:27:42.000Z",
      "submittedOnDailyAt": "2025-06-05T00:58:27.883Z",
      "title": "Super Lítera: Modelo de generación de largas oraciones sintéticas",
      "submittedOnDailyBy": {
        "_id": "64ed568ccf6118a9379a61b8",
        "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
        "isPro": false,
        "fullname": "Yushi Bai",
        "user": "bys0318",
        "type": "user"
      },
      "summary": "La generación de largos textos es un desafío a largo plazo para los modelos de lenguaje de gran tamaño (LLMs), especialmente debido a la necesidad de mantener la continuidad, garantizar la coherencia lógica y mantener la calidad de la textura a medida que la longitud de la secuencia aumenta. Para resolver estos limitaciones, proponemos el SuperWriter-Agent. El SuperWriter-Agent es un marco de trabajo basado en agentes diseñado para mejorar la calidad y la coherencia de la generación de largos textos. Introduce pasos explícitos y estructurados de pensamiento y mejora en la pipeline de generación, lo que conduce al modelo a seguir un proceso más riguroso y cognitivamente orientado. Basándonos en este marco, construimos un conjunto de datos de fine-tuning estándar para entrenar el SuperWriter-LM de 7B y desarrollamos un proceso de Optimización de Preferencias Directas (DPO) jerárquico. Este proceso utiliza la búsqueda de árbol de Monte Carlo (MCTS) para evaluar la calidad final y optimizar cada paso de generación adecuadamente. Los resultados de experimentos en diferentes benchmarks demuestran que el SuperWriter-LM ha alcanzado un rendimiento más reciente y ha superado a modelos más grandes en evaluaciones automáticas y humanas. Además, pruebas detalladas muestran el efecto del DPO jerárquico y destacan la función de los pasos de pensamiento estructurados en la mejora de la calidad de la generación de largos textos.",
      "upvotes": 20,
      "discussionId": "6840fefc3098ab525906d89c",
      "ai_summary": "SuperWriter-Agent enhances long-form text generation by integrating structured planning and refinement, achieving top performance with a 7B model and hierarchical Direct Preference Optimization.",
      "ai_keywords": [
        "agent-based framework",
        "structured thinking-through planning",
        "refinement stages",
        "SuperWriter-Agent",
        "SuperWriter-LM",
        "hierarchical Direct Preference Optimization",
        "Monte Carlo Tree Search",
        "DPO",
        "automatic evaluation",
        "human evaluation",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-06-04T13:27:42.000Z",
    "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models",
    "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04180.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ed568ccf6118a9379a61b8",
      "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
      "fullname": "Yushi Bai",
      "name": "bys0318",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01320",
      "authors": [
        {
          "_id": "684124368cb0edba3ab8f738",
          "name": "Taehoon Yoon",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f739",
          "name": "Yunhong Min",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f73a",
          "name": "Kyeongmin Yeo",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f73b",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T05:02:33.000Z",
      "submittedOnDailyAt": "2025-06-05T03:38:33.152Z",
      "title": "Ψ-Sampler: Ajuste de la recompensa en la muestración inicial de partículas para la inferencia basada en SMC mediante un modelo de puntuación",
      "submittedOnDailyBy": {
        "_id": "66ee81b676a8038cb42c8caa",
        "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
        "isPro": false,
        "fullname": "Yunhong Min",
        "user": "myhong",
        "type": "user"
      },
      "summary": "Psi-Sampler se presenta. Este marco de trabajo apoya de manera efectiva la ajuste de recompensas en la inferencia de modelos generativos basados en puntuaciones, utilizando un proceso de muestreo inicial basado en pCNL. El ajuste de recompensas en la inferencia de modelos generativos basados en puntuaciones ha sido un tema más ampliamente abordado debido al cambio de paradigma que ha surgido desde la optimización después del entrenamiento hasta la modificación de plugins. El corazón de este proceso se realiza mediante la aplicación del método de Monte Carlo Sequential (SMC). Actualmente, los métodos utilizan generalmente inicializar los particulas con una distribución gaussiana, lo que no permite una comprensión suficiente de las regiones relacionadas con las recompensas, lo que disminuye la eficiencia del muestreo. Demostramos que inicializar las particulas de manera interesante para las recompensas mejora significativamente el rendimiento del ajuste. Se presenta el algoritmo preconditioned Crank-Nicolson Langevin (pCNL) para facilitar la muestreo de particulas en espacios potenciales de alta dimensión, combinando la propuesta por dimensión y la derivada con la dinámica. Esta aproximación permite un muestreo eficiente y escalable de particulas, mostrando un aumento positivo en rendimiento en tareas de generación de modelos de capas, generación sensorial y generación de preferencias artísticas.",
      "upvotes": 15,
      "discussionId": "6841243c8cb0edba3ab8f8bf",
      "ai_summary": "The framework $\\Psi$-Sampler uses SMC with pCNL for efficient posterior sampling and reward alignment in score-based generative models, enhancing performance across various tasks.",
      "ai_keywords": [
        "SMC-based framework",
        "pCNL-based initial particle sampling",
        "inference-time reward alignment",
        "score-based generative model",
        "Sequential Monte Carlo",
        "denoising process",
        "Gaussian prior",
        "reward-aware posterior",
        "preconditioned Crank-Nicolson Langevin",
        "layout-to-image generation",
        "quantity-aware generation",
        "aesthetic-preference generation"
      ]
    },
    "publishedAt": "2025-06-02T01:02:33.000Z",
    "title": "Ψ-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models",
    "summary": "We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based\ninitial particle sampling for effective inference-time reward alignment with a\nscore-based generative model. Inference-time reward alignment with score-based\ngenerative models has recently gained significant traction, following a broader\nparadigm shift from pre-training to post-training optimization. At the core of\nthis trend is the application of Sequential Monte Carlo (SMC) to the denoising\nprocess. However, existing methods typically initialize particles from the\nGaussian prior, which inadequately captures reward-relevant regions and results\nin reduced sampling efficiency. We demonstrate that initializing from the\nreward-aware posterior significantly improves alignment performance. To enable\nposterior sampling in high-dimensional latent spaces, we introduce the\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines\ndimension-robust proposals with gradient-informed dynamics. This approach\nenables efficient and scalable posterior sampling and consistently improves\nperformance across various reward alignment tasks, including layout-to-image\ngeneration, quantity-aware generation, and aesthetic-preference generation, as\ndemonstrated in our experiments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01320.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ee81b676a8038cb42c8caa",
      "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
      "fullname": "Yunhong Min",
      "name": "myhong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04225",
      "authors": [
        {
          "_id": "68413366adeec0116d071af2",
          "user": {
            "_id": "63425d394c9a81858b36aeb5",
            "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
            "isPro": false,
            "fullname": "Tianyu Huang",
            "user": "tyhuang",
            "type": "user"
          },
          "name": "Tianyu Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:41.198Z",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af3",
          "name": "Wangguandong Zheng",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af4",
          "name": "Tengfei Wang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af5",
          "name": "Yuhao Liu",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af6",
          "name": "Zhenwei Wang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af7",
          "name": "Junta Wu",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af8",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af9",
          "name": "Hui Li",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afa",
          "name": "Rynson W. H. Lau",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afb",
          "name": "Wangmeng Zuo",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afc",
          "name": "Chunchao Guo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63425d394c9a81858b36aeb5/cjZH2kR6B3y9IAmmRHNJS.mp4"
      ],
      "publishedAt": "2025-06-04T17:59:04.000Z",
      "submittedOnDailyAt": "2025-06-05T06:00:26.815Z",
      "title": "Voyager: Larga distancia y diseño de videos mundiales para la creación de pantallas 3D explorables",
      "submittedOnDailyBy": {
        "_id": "63425d394c9a81858b36aeb5",
        "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
        "isPro": false,
        "fullname": "Tianyu Huang",
        "user": "tyhuang",
        "type": "user"
      },
      "summary": "En el desarrollo de aplicaciones en la realidad, la tecnología de modelado 3D es esencial en diversas áreas como juegos 3D y realidad virtual (VR). Recientemente, los avances en la generación de modelos 3D han ido desarrollandose desde textos y imágenes, pero la configuración de escenas 3D sigue siendo un problema complejo y difícil. En este artículo, se presenta un nuevo marco de trabajo llamado \"Voyager\" que permite generar secuencias de clusters de puntos 3D globalmente consistentes a partir de una sola imagen, basándose en un camino de cámara especificado por el usuario. A diferencia de los métodos existentes, Voyager mantiene la consistencia de la correspondencia en cada frame mientras logra la generación y reconstrucción de escenas 3D coherentes, reduciendo la necesidad de procesos de reconstrucción 3D (por ejemplo, acciones basadas en la estructura, stereoscopia múltiple puntos). Nuestro enfoque se compone de tres elementos clave: 1) Difusión de videos globalmente consistentes: una arquitectura unificada que permite generar secuencias de video RGB y profundidad consistentes basándose en observaciones mundiales, asegurando la coherencia global. 2) Exploración mundial a larga distancia: utilizando cachos mundiales eficientes, eliminación de puntos, inferencia automática y muestreo de video suave, se logra la expansión repetitiva de escenas y la consistencia en el contexto. 3) Motor de datos intercambiables: automatiza la estimación de la orientación de la cámara y la predicción de profundidad para cualquier video, facilitando la recopilación de grandes conjuntos de datos y diversos, sin necesidad de etiquetar 3D. Esta arquitectura mejora claramente la calidad visual y la precisión de generalización en comparación con los métodos actuales, y permite aplicación en diversas aplicaciones.",
      "upvotes": 14,
      "discussionId": "6841336badeec0116d071c2b",
      "projectPage": "https://voyager-world.github.io",
      "githubRepo": "https://github.com/Voyager-World/Voyager",
      "ai_summary": "Voyager is a video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image, enabling long-range, consistent 3D scene exploration with user-defined camera paths.",
      "ai_keywords": [
        "video diffusion",
        "world-consistent video diffusion",
        "3D point-cloud sequences",
        "camera path",
        "end-to-end scene generation",
        "consistent frames",
        "unified architecture",
        "RGB and depth video sequences",
        "world observation",
        "global coherence",
        "long-range world exploration",
        "world cache",
        "point culling",
        "auto-regressive inference",
        "smooth video sampling",
        "scene extension",
        "context-aware consistency",
        "scalable data engine",
        "camera pose estimation",
        "metric depth prediction",
        "large-scale",
        "diverse training data"
      ]
    },
    "publishedAt": "2025-06-04T13:59:04.000Z",
    "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation",
    "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63425d394c9a81858b36aeb5/cjZH2kR6B3y9IAmmRHNJS.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63425d394c9a81858b36aeb5",
      "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
      "fullname": "Tianyu Huang",
      "name": "tyhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04228",
      "authors": [
        {
          "_id": "684103aed45a1fc5540ddc10",
          "name": "Sihui Ji",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc11",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc12",
          "user": {
            "_id": "644a1b6401e18bf93a6f45c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
            "isPro": false,
            "fullname": "xichen",
            "user": "xichenhku",
            "type": "user"
          },
          "name": "Xi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:50.027Z",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc13",
          "name": "Yuanpeng Tu",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc14",
          "name": "Yiyang Wang",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc15",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:59:58.000Z",
      "submittedOnDailyAt": "2025-06-05T01:11:16.967Z",
      "title": "LayerFlow: Modelo de unidad de generación de vídeos en la capa de interés",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "LayerFlow es una solución integral para la generación de imágenes que reconoce información de capas. Al proporcionar procesamiento por capas, LayerFlow genera imágenes que incluyen un fondo transparente, un fondo limpio y escenarios brandeados. Además, también soporta la descomposición de imágenes brandeadas, la generación de fondos para los marcas de fuente, y varias versiones diferentes. Comienza el proceso de difusión de vídeo en contexto, organiza las imágenes de cada capa como subclips y utiliza técnicas de envolvimiento de capas para diferenciar los subclips por capas. De esta manera, las versiones anteriormente mencionadas pueden ser soportadas fácilmente en un solo marco integrado. Dada la escasez de alta calidad de entrenamiento de vídeos por capas, diseña una estrategia de entrenamiento paso a paso para responder a imágenes estáticas con anotaciones de capas de alta calidad. Específicamente, inicia con entrenamiento de modelos con datos de vídeo de baja calidad, ajusta la LoRA de acciones para adaptarse a los frames estáticos, y posteriormente entrena la LoRA de contenido con una mezcla de datos de imágenes de alta calidad por capas y vídeos de copia y pega. Durante la inferencia, elimina la LoRA de acciones y genera vídeos suaves de la capa deseada.",
      "upvotes": 12,
      "discussionId": "684103b0d45a1fc5540ddca8",
      "ai_summary": "LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.",
      "ai_keywords": [
        "LayerFlow",
        "text-to-video diffusion transformer",
        "layer embeddings",
        "sub-clips",
        "multi-stage training strategy",
        "motion LoRA",
        "content LoRA",
        "layered images",
        "copy-pasted video data",
        "smooth videos"
      ]
    },
    "publishedAt": "2025-06-04T13:59:58.000Z",
    "title": "LayerFlow: A Unified Model for Layer-aware Video Generation",
    "summary": "We present LayerFlow, a unified solution for layer-aware video generation.\nGiven per-layer prompts, LayerFlow generates videos for the transparent\nforeground, clean background, and blended scene. It also supports versatile\nvariants like decomposing a blended video or generating the background for the\ngiven foreground and vice versa. Starting from a text-to-video diffusion\ntransformer, we organize the videos for different layers as sub-clips, and\nleverage layer embeddings to distinguish each clip and the corresponding\nlayer-wise prompts. In this way, we seamlessly support the aforementioned\nvariants in one unified framework. For the lack of high-quality layer-wise\ntraining videos, we design a multi-stage training strategy to accommodate\nstatic images with high-quality layer annotations. Specifically, we first train\nthe model with low-quality video data. Then, we tune a motion LoRA to make the\nmodel compatible with static frames. Afterward, we train the content LoRA on\nthe mixture of image data with high-quality layered images along with\ncopy-pasted video data. During inference, we remove the motion LoRA thus\ngenerating smooth videos with desired layers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04228.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 41
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03139",
      "authors": [
        {
          "_id": "683fb0a7be8421eda3152283",
          "user": {
            "_id": "676b86e79ff0244316f7202f",
            "avatarUrl": "/avatars/3e1d26312a96752356895ab88eeb3ce0.svg",
            "isPro": false,
            "fullname": "chensiqi",
            "user": "xiaoooobai",
            "type": "user"
          },
          "name": "Siqi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:28:25.792Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152284",
          "name": "Xinyu Dong",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152285",
          "user": {
            "_id": "6692aff88db712bad780f02a",
            "avatarUrl": "/avatars/5dc4b1c27c70f6a64864711dbff4910f.svg",
            "isPro": false,
            "fullname": "xhl",
            "user": "zjuxhl",
            "type": "user"
          },
          "name": "Haolei Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:42.705Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152286",
          "name": "Xingyu Wu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152287",
          "name": "Fei Tang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152288",
          "name": "Hang Zhang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152289",
          "user": {
            "_id": "64098738342c26884c792c93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
            "isPro": false,
            "fullname": "Yuchen Yan",
            "user": "yanyc",
            "type": "user"
          },
          "name": "Yuchen Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:47.244Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228a",
          "name": "Linjuan Wu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228b",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228c",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228d",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228e",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228f",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:58:57.000Z",
      "submittedOnDailyAt": "2025-06-05T03:44:24.728Z",
      "title": "SVGenius: Benchmark de comprensión, edición y generación de SVG con LLM",
      "submittedOnDailyBy": {
        "_id": "5e1058e9fcf41d740b69966d",
        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
        "isPro": false,
        "fullname": "Yongliang Shen",
        "user": "tricktreat",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) y los modelos de LLMs multimodal han demostrado las capacidades esperadas en el procesamiento de SVG, pero los actuales benchmarks están limitados en su cobertura real y no tienen una clasificación de complejidad ni un paradigma de evaluación separado. Presentamos SVGenius, un estricto benchmark que incluye 2,377 preguntas en tres etapas: comprensión, edición y generación, construido a partir de datos reales y con una clasificación sistemática de complejidad. Este benchmark evalúa modelos en 8 categorías de tareas y 18 métricas. Se evalúan 22 modelos principales (con diferentes escalas, arquitecturas, paradigmas de entrenamiento y niveles de accesibilidad). El análisis muestra que los modelos propios tienen un desafío significativo sobre los modelos abiertos, pero todos los modelos muestran una disminución sistemática en su rendimiento al aumentar la complejidad, demostrando las limitaciones básicas de los métodos actuales. Sin embargo, el aprendizaje reforzado con lógica es más efectivo para superar estas limitaciones, aunque la capacidad más difícil en todos los modelos es la transformación de estilo. SVGenius es el primer marco de evaluación estricto para el procesamiento de SVG, proporcionando importantes insights para el desarrollo de modelos de gráficos vectoriales y el avance de aplicaciones de diseño gráfico automático. Los datos adicionales y datos complementarios (incluyendo datos y código) están disponibles en https://zju-real.github.io/SVGenius.",
      "upvotes": 12,
      "discussionId": "683fb0a7be8421eda31522ca",
      "projectPage": "https://zju-real.github.io/SVGenius/",
      "githubRepo": "https://github.com/ZJU-REAL/SVGenius",
      "ai_summary": "SVGenius evaluates Large Language Models and Multimodal LLMs for SVG processing using a comprehensive benchmark across three dimensions: understanding, editing, and generation, revealing insights into model capabilities and limitations.",
      "ai_keywords": [
        "Large Language Models",
        "Multimodal LLMs",
        "SVG processing",
        "SVGenius",
        "complexity stratification",
        "reasoning-enhanced training",
        "style transfer"
      ]
    },
    "publishedAt": "2025-06-03T13:58:57.000Z",
    "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
    "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03139.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e1058e9fcf41d740b69966d",
      "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
      "fullname": "Yongliang Shen",
      "name": "tricktreat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03295",
      "authors": [
        {
          "_id": "6840e7d81fadbc85ae3bdc0f",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc10",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc11",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc12",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc13",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T18:35:52.000Z",
      "submittedOnDailyAt": "2025-06-05T02:48:14.083Z",
      "title": "La liberación del potencial lógico de los modelos de lenguaje preentrenados por aprendizaje automático a través de la intuición\nAjustes para el problema específico",
      "submittedOnDailyBy": {
        "_id": "636a35eff8d9af4aea181608",
        "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
        "isPro": false,
        "fullname": "yubo",
        "user": "ubowang",
        "type": "user"
      },
      "summary": "Hemos observado que fuertes LLMs (por ejemplo: Qwen-Math, MiMo, Phi-4) pueden probar de manera impresionante la gran posibilidad lógica desde el principio de su entrenamiento. El aprendizaje por refuerzo (RL) puede mejorar significativamente estas capacidades lógicas en tareas. Según recientes estudios, el aprendizaje por refuerzo para un problema único puede liberar la capacidad lógica de estos modelos. Sin embargo, el RL es muy costoso y no está estable. Incluso un simple aprendizaje por refuerzo requiere de cientos de horas de hardware, lo que plantea una pregunta importante: ¿existen métodos más eficientes para liberar la capacidad lógica de estos potentes modelos básicos? Este estudio presenta una solución a este problema, demostrando que el Aprendizaje por Criticism y Fine-Tuning (CFT) para problemas únicos puede liberar efectivamente la capacidad lógica de los LLMs. Nuestro método recopila soluciones generadas por diferentes modelos para un problema único y construye datos de crítica mediante una modelo de enseñanza detallado. Hemos realizado fine-tuning con datos de CFT para modelos de Qwen y Llama (cantidad de parámetros: 1.5B a 14B), observando una mejora clara en diferentes tareas lógicas. Por ejemplo, Qwen-Math-7B-CFT demostró un aumento promedio del 15% en 6 marcadores matemáticos y del 16% en 3 marcadores de teoría de la lógica, incluso con solo 5 horas de entrenamiento. Estos resultados son comparables a los de RL, pero con un consumo de computación reducido en al menos 20 veces, demostrando la eficiencia y generalidad del CFT para problemas únicos. Según la investigación, la robustez del CFT para problemas únicos también se manifiesta en otros problemas de prompt. Estos resultados muestran que existen métodos sencillos y con un margen de computación para liberar la capacidad lógica de los LLMs modernos.",
      "upvotes": 10,
      "discussionId": "6840e7d81fadbc85ae3bdc45",
      "ai_summary": "Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.",
      "ai_keywords": [
        "Critique Fine-Tuning",
        "teacher LLMs",
        "Qwen-Math",
        "Llama family models",
        "reasoning tasks",
        "one-shot CFT",
        "performance gains",
        "logic reasoning benchmarks",
        "math benchmarks",
        "prompt problems"
      ]
    },
    "publishedAt": "2025-06-03T14:35:52.000Z",
    "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem",
    "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636a35eff8d9af4aea181608",
      "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
      "fullname": "yubo",
      "name": "ubowang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24500",
      "authors": [
        {
          "_id": "683fb063ef97de05eb2a44cc",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44cd",
          "name": "Xing Gao",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44ce",
          "name": "Yuchuan Wu",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44cf",
          "name": "Xiang Huang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d0",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d1",
          "name": "Zhe Zheng",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d2",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d3",
          "name": "Jialu Du",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d4",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d5",
          "name": "Yongbin Li",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d6",
          "name": "Weiming Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T12:01:06.000Z",
      "submittedOnDailyAt": "2025-06-05T00:45:50.911Z",
      "title": "Reconocimiento de información temporal con aprendizaje por refuerzo hiper-corona: aplicación del aprendizaje por refuerzo hiper-corona para el reconocimiento de secuencias temporales",
      "submittedOnDailyBy": {
        "_id": "67c03110e8c7d56a8e135ac8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
        "isPro": false,
        "fullname": "Hou",
        "user": "Guiyang1001",
        "type": "user"
      },
      "summary": "Recientemente, los modelos de lenguaje grande (LLMs) han logrado notables avances en áreas como la matemática o el codigo, donde se requiere un pensamiento cuidadoso. Sin embargo, el desarrollo cognitivo de los LLMs desde un punto de vista social, especialmente después de un entrenamiento, aún no ha sido suficientemente investigado.\n\nEl mundo social tiene otras dimensiones de tiempo y requiere una combinación rica de reacciones intuitivas (System 1) y pensamientos superficiales con pensamientos profundos (System 2), lo cual es más claramente necesario que la matemática. La matemática depende principalmente del cognitivo de System 2 (pensamiento cuidadoso, explicativo en pasos), lo que nos permite comprender la importancia de esta combinación cognitiva. En respuesta a esto, introducimos un entrenamiento cognitivo relacionado con el tiempo para mejorar el inteligencia social (TimeHC-RL).\n\nEn los experimentos, evaluamos el rendimiento del TimeHC-RL en mejorar la inteligencia social de los LLMs utilizando 8 conjuntos de datos, 5 paradigmas de entrenamiento y 2 paradigmas de interacción de prueba. Los resultados de los experimentos muestran claramente que el método TimeHC-RL proporciona un rendimiento superior comparado con el aprendizaje por refuerzo de System 2. Este método permite comparar el rendimiento de modelos avanzados como DeepSeek-R1 y OpenAI-O3, y también ofrece una exploración sistemática que proporciona muchos feedbacks valiosos para mejorar la inteligencia social.",
      "upvotes": 10,
      "discussionId": "683fb064ef97de05eb2a452b",
      "ai_summary": "Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.",
      "ai_keywords": [
        "Large Language Models",
        "Temporal-aware Hierarchical Cognitive Reinforcement Learning",
        "TimeHC-RL",
        "System 1",
        "System 2",
        "RL",
        "DeepSeek-R1",
        "OpenAI-O3"
      ]
    },
    "publishedAt": "2025-05-30T08:01:06.000Z",
    "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence",
    "summary": "Recently, Large Language Models (LLMs) have made significant progress in\nIQ-related domains that require careful thinking, such as mathematics and\ncoding. However, enhancing LLMs' cognitive development in social domains,\nparticularly from a post-training perspective, remains underexplored.\nRecognizing that the social world follows a distinct timeline and requires a\nricher blend of cognitive modes (from intuitive reactions (System 1) and\nsurface-level thinking to deliberate thinking (System 2)) than mathematics,\nwhich primarily relies on System 2 cognition (careful, step-by-step reasoning),\nwe introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning\n(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we\nsystematically explore improving LLMs' social intelligence and validate the\neffectiveness of the TimeHC-RL method, through five other post-training\nparadigms and two test-time intervention paradigms on eight datasets with\ndiverse data patterns. Experimental results reveal the superiority of our\nproposed TimeHC-RL method compared to the widely adopted System 2 RL method. It\ngives the 7B backbone model wings, enabling it to rival the performance of\nadvanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic\nexploration from post-training and test-time interventions perspectives to\nimprove LLMs' social intelligence has uncovered several valuable insights.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24500.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67c03110e8c7d56a8e135ac8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
      "fullname": "Hou",
      "name": "Guiyang1001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04158",
      "authors": [
        {
          "_id": "6840fb71d4e16ff5f95108aa",
          "name": "Yujia Hu",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ab",
          "name": "Songhua Liu",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ac",
          "name": "Zhenxiong Tan",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ad",
          "user": {
            "_id": "634cfebc350bcee9bed20a4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
            "isPro": false,
            "fullname": "Xingyi Yang",
            "user": "adamdad",
            "type": "user"
          },
          "name": "Xingyi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:11.256Z",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ae",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/tp0cLz8OZhdI3vs50cxhF.jpeg"
      ],
      "publishedAt": "2025-06-04T16:57:24.000Z",
      "submittedOnDailyAt": "2025-06-05T00:38:20.484Z",
      "title": "Imagen editado es un programa que utiliza modelos de difusión.",
      "submittedOnDailyBy": {
        "_id": "634cfebc350bcee9bed20a4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
        "isPro": false,
        "fullname": "Xingyi Yang",
        "user": "adamdad",
        "type": "user"
      },
      "summary": "Los modelos de difusión han tenido un éxito sorprendente en la generación de imágenes a partir de texto, pero han encontrado grandes problemas en la edición de imágenes mediante comandos. En este estudio, se han descubierto importantes problemas: estos modelos sufren particularmente en ediciones estructuralmente desacopladas. Para solucionar esto, se utiliza un marco integrado para edición de imágenes basado en la arquitectura de Transformer de Difusión (DiT), llamado Image Editing As Programs (IEAP). El núcleo de IEAP es procesar la edición basada en comandos desde una perspectiva simplificada y decompor instrucciones de edición complejas en una secuencia de operaciones básicas. Cada operación comparte el mismo backend de DiT y se especializa en una especie de edición. Los agentes basados en modelos de visión y lenguaje (VLM) se programan para soportar transformaciones arbitrarias e estructuralmente desacopladas. Al modularizar y procesar las ediciones de manera secuencial, IEAP se adapta ampliamente a tareas de edición, desde pequeñas ajustes hasta grandes cambios estructurales. Los experimentos extendidos muestran que IEAP supera significativamente a los métodos más avanzados en los benchmarks estándar. En estas evaluaciones, nuestro marco de trabajo ofrece una alta precisión y fidelidad semántica, especialmente para instrucciones complejas y paso a paso. El código está disponible en https://github.com/YujiaHu1109/IEAP.",
      "upvotes": 8,
      "discussionId": "6840fb73d4e16ff5f9510950",
      "projectPage": "https://yujiahu1109.github.io/IEAP/",
      "githubRepo": "https://github.com/YujiaHu1109/IEAP",
      "ai_summary": "A unified image editing framework, IEAP, built on Diffusion Transformer (DiT) decomposes complex editing instructions into operations performed by vision-language models for robust editing across various tasks.",
      "ai_keywords": [
        "diffusion models",
        "text-to-image generation",
        "instruction-driven image editing",
        "structurally inconsistent edits",
        "Image Editing As Programs (IEAP)",
        "Diffusion Transformer (DiT)",
        "atomic operations",
        "lightweight adapter",
        "vision-language model (VLM)",
        "modularizing edits"
      ]
    },
    "publishedAt": "2025-06-04T12:57:24.000Z",
    "title": "Image Editing As Programs with Diffusion Models",
    "summary": "While diffusion models have achieved remarkable success in text-to-image\ngeneration, they encounter significant challenges with instruction-driven image\nediting. Our research highlights a key challenge: these models particularly\nstruggle with structurally inconsistent edits that involve substantial layout\nchanges. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a\nunified image editing framework built upon the Diffusion Transformer (DiT)\narchitecture. At its core, IEAP approaches instructional editing through a\nreductionist lens, decomposing complex editing instructions into sequences of\natomic operations. Each operation is implemented via a lightweight adapter\nsharing the same DiT backbone and is specialized for a specific type of edit.\nProgrammed by a vision-language model (VLM)-based agent, these operations\ncollaboratively support arbitrary and structurally inconsistent\ntransformations. By modularizing and sequencing edits in this way, IEAP\ngeneralizes robustly across a wide range of editing tasks, from simple\nadjustments to substantial structural changes. Extensive experiments\ndemonstrate that IEAP significantly outperforms state-of-the-art methods on\nstandard benchmarks across various editing scenarios. In these evaluations, our\nframework delivers superior accuracy and semantic fidelity, particularly for\ncomplex, multi-step instructions. Codes are available at\nhttps://github.com/YujiaHu1109/IEAP.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/tp0cLz8OZhdI3vs50cxhF.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04158.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634cfebc350bcee9bed20a4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
      "fullname": "Xingyi Yang",
      "name": "adamdad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04141",
      "authors": [
        {
          "_id": "684106fc8cb0edba3ab212bb",
          "name": "Kejian Zhu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bc",
          "name": "Zhuoran Jin",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bd",
          "name": "Hongbang Yuan",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212be",
          "name": "Jiachun Li",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bf",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c0",
          "name": "Pengfei Cao",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c1",
          "name": "Yubo Chen",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c2",
          "name": "Kang Liu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c3",
          "name": "Jun Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/_-WD0IU9jQMInqhXgGfUP.jpeg"
      ],
      "publishedAt": "2025-06-04T16:33:41.000Z",
      "submittedOnDailyAt": "2025-06-05T04:38:26.132Z",
      "title": "MMR-V: ¿Qué no se dice y que queda? Punto de referencia lógico profundo de la estructura multifásica en el video",
      "submittedOnDailyBy": {
        "_id": "648c48d8c0ddeee6df5b6d22",
        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
        "isPro": false,
        "fullname": "Shangqing Tu",
        "user": "tsq2000",
        "type": "user"
      },
      "summary": "La estructura secuencial del video desafía a los modelos de lenguaje multimodal (MLLM) en su capacidad para identificar y ejecutar lógicas de múltiples ejemplos en diferentes marcos de referencia. Sin embargo, los marcos de evaluación actuales se centran principalmente en tareas de comprensión, exigiendo que los modelos reconozcan solo los frames cercanos al marco de referencia mencionado en la tarea (a continuación, \"marco de referencia de la tarea\"). Para abordar estas limitaciones, proponemos el MMR-V (Benchmark de lógica profunda de múltiples ejemplos en videos), que presenta las siguientes características: (1) lógica de múltiples ejemplos a larga distancia: los modelos deben inferir y analizar frames de evidencia que estén lejos del marco de referencia de la tarea. (2) más allá de la reconocción: la tarea no puede ser resuelta directamente mediante reconocimiento, sino que requiere procesar información oculta lógicamente. (3) confiabilidad: todos los tareas están realizados a través de anotaciones directas, y se basan en la comprensión de un amplio público de usuarios reales para ajustarse a la comprensión general. (4) confusión: se utilizaron estrategias de anotación de detectores cuidadosamente diseñadas para reducir las debilidades del modelo. El MMR-V comprende 317 videos y 1,257 tareas. Nuestros experimentos muestran que los modelos actuales enfrentan dificultades con la lógica de múltiples ejemplos, y el mejor modelo, o4-mini, alcanza una precisión de 52.5%. Además, las estrategias de expansión lógica actuales (CoT y escalado de computación en tiempo de ejecución) han mostrado efectos limitados. A través de la evolución de la análisis, se ha descubierto que el CoT difiere de la lógica de la literatura, lo que explica parte de las limitaciones de rendimiento. Buscamos que el MMR-V fomente el desarrollo de las habilidades de lógica de múltiples ejemplos.",
      "upvotes": 7,
      "discussionId": "684106ff8cb0edba3ab21374",
      "ai_summary": "A new benchmark, MMR-V, is proposed to challenge multimodal large language models with long-range, multi-frame reasoning and hidden information processing in videos, revealing their limitations and inspiring further research.",
      "ai_keywords": [
        "multimodal large language models",
        "MMR-V",
        "long-range",
        "multi-frame reasoning",
        "multimodal reasoning",
        "manual annotation",
        "distractor annotation strategy",
        "Chain-of-Thought"
      ]
    },
    "publishedAt": "2025-06-04T12:33:41.000Z",
    "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos",
    "summary": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/_-WD0IU9jQMInqhXgGfUP.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04141.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03930",
      "authors": [
        {
          "_id": "6841090145662bb7d322ecc6",
          "user": {
            "_id": "64de37ee5e192985054be575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
            "isPro": false,
            "fullname": "Yuansheng Ni",
            "user": "yuanshengni",
            "type": "user"
          },
          "name": "Yuansheng Ni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T10:00:04.196Z",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc7",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc8",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc9",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecca",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T13:24:44.000Z",
      "submittedOnDailyAt": "2025-06-05T05:54:39.464Z",
      "title": "VisCoder: Generación de código de visualización Python ejecutable a través de un LLM",
      "submittedOnDailyBy": {
        "_id": "64de37ee5e192985054be575",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
        "isPro": false,
        "fullname": "Yuansheng Ni",
        "user": "yuanshengni",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) requieren considerar tanto la precisión del código como el significado visual en tareas de visualización. Los conjuntos de datos de fine-tuning actuales basados en ejecución faltan soporte y son limitados a correcciones de código repetitivas, lo que puede llevar a la generación de gráficos insatisfactorios e incertificantes. Presentamos \"VisCode-200K\", un conjunto de datos de fine-tuning de instrucciones basado en Python para visualización y autocorrección. Este conjunto incluye más de 200K ejemplos y cuenta con los siguientes aspectos: 1) código de visualización verificado en repositorios de código abierto, combinado con instrucciones de lenguaje natural y gráficos visualizados, y 2) 45K diálogos de corrección de código en varias etapas, filtrados mediante retroalimentación del modelo al ejecutar el código. Hemos fine-tunado Qwen2.5-Coder-Instruct con VisCode-200K para crear \"VisCoder\". VisCoder supera significativamente a los estándares de código abierto y se parece al rendimiento de modelos como GPT-4o-mini. Además, introducimos un protocolo de evaluación de correcciones iterativas para mostrar la beta de aprendizaje dirigido por retroalimentación en la generación de código visualmente preciso ejecutable.",
      "upvotes": 7,
      "discussionId": "6841090245662bb7d322ed1f",
      "projectPage": "https://tiger-ai-lab.github.io/VisCoder/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VisCoder",
      "ai_summary": "VisCode-200K, a large-scale dataset for visualization, improves plot generation performance by integrating execution-grounded supervision and iterative code correction, outperforming open-source models and rivaling proprietary ones.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "visualization tasks",
        "plot generation",
        "execution-grounded supervision",
        "iterative code correction",
        "VisCode-200K",
        "Python-based visualization",
        "validated plotting code",
        "natural language instructions",
        "rendered plots",
        "correction dialogues",
        "Qwen2.5-Coder-Instruct",
        "VisCoder",
        "PandasPlotBench",
        "self-debug evaluation",
        "feedback-driven learning"
      ]
    },
    "publishedAt": "2025-06-04T09:24:44.000Z",
    "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation",
    "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03930.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64de37ee5e192985054be575",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
      "fullname": "Yuansheng Ni",
      "name": "yuanshengni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03517",
      "authors": [
        {
          "_id": "68412f853c22997c7329f3a0",
          "user": {
            "_id": "62980664ff0acd7e027d6686",
            "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
            "isPro": false,
            "fullname": "Ziyi Wu",
            "user": "Dazitu616",
            "type": "user"
          },
          "name": "Ziyi Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:43.314Z",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a1",
          "name": "Anil Kag",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a2",
          "name": "Ivan Skorokhodov",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a3",
          "name": "Willi Menapace",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a4",
          "name": "Ashkan Mirzaei",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a5",
          "name": "Igor Gilitschenski",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a6",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a7",
          "name": "Aliaksandr Siarohin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T03:06:08.000Z",
      "submittedOnDailyAt": "2025-06-05T04:18:57.242Z",
      "title": "DenseDPO: Optimización de preferencias temporales mínimas en vídeos",
      "submittedOnDailyBy": {
        "_id": "62980664ff0acd7e027d6686",
        "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
        "isPro": false,
        "fullname": "Ziyi Wu",
        "user": "Dazitu616",
        "type": "user"
      },
      "summary": "La Optimización de Preferencias Directas (Direct Preference Optimization, DPO) es una técnica reciente para el entrenamiento de modelos de dispersión de películas desde texto. Para obtener datos de entrenamiento, se solicita que proporcionen gustos de películas generadas por dos ruidos independientes. Sin embargo, esta aproximación prohíbe comparaciones detalladas y da un bias a pequeños clips. En este estudio, se introduce el método DenseDPO, que ofrece tres contribuciones para resolver estos problemas. Primero, en DPO se crea pares de videos utilizando ruidos de películas reales, lo que permite generar pares alineados con estructuras similares pero detalles locales diferentes, neutralizando el bias de movimiento. Luego, se utilizan estas alineaciones temporales para etiquetar preferencias en cortos segmentos, obteniendo señales de aprendizaje más densas y precisas. Con solo un tercio de los datos etiquetados, DenseDPO mejora significativamente la generación de movimientos, manteniendo el rendimiento en alineamiento de texto, calidad visual y coherencia temporal similar al DPO. Finalmente, DenseDPO muestra la generación automática de etiquetas de gusto utilizando modelos de lenguaje de visión y lenguaje (VLMs): GPT predice niveles de gusto similares a modelos de recompensa de películas entrenados, lo que permite que DenseDPO alcanze un rendimiento similar a cuando se usan etiquetas humanas.",
      "upvotes": 7,
      "discussionId": "68412f8a3c22997c7329f4ff",
      "projectPage": "https://snap-research.github.io/DenseDPO/"
    },
    "publishedAt": "2025-06-03T23:06:08.000Z",
    "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video\n  Diffusion Models",
    "summary": "Direct Preference Optimization (DPO) has recently been applied as a\npost-training technique for text-to-video diffusion models. To obtain training\ndata, annotators are asked to provide preferences between two videos generated\nfrom independent noise. However, this approach prohibits fine-grained\ncomparisons, and we point out that it biases the annotators towards low-motion\nclips as they often contain fewer visual artifacts. In this work, we introduce\nDenseDPO, a method that addresses these shortcomings by making three\ncontributions. First, we create each video pair for DPO by denoising corrupted\ncopies of a ground truth video. This results in aligned pairs with similar\nmotion structures while differing in local details, effectively neutralizing\nthe motion bias. Second, we leverage the resulting temporal alignment to label\npreferences on short segments rather than entire clips, yielding a denser and\nmore precise learning signal. With only one-third of the labeled data, DenseDPO\ngreatly improves motion generation over vanilla DPO, while matching it in text\nalignment, visual quality, and temporal consistency. Finally, we show that\nDenseDPO unlocks automatic preference annotation using off-the-shelf Vision\nLanguage Models (VLMs): GPT accurately predicts segment-level preferences\nsimilar to task-specifically fine-tuned video reward models, and DenseDPO\ntrained on these labels achieves performance close to using human labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03517.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62980664ff0acd7e027d6686",
      "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
      "fullname": "Ziyi Wu",
      "name": "Dazitu616",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04142",
      "authors": [
        {
          "_id": "684132cb725b7fb67f68ffb8",
          "name": "Kejian Zhu",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffb9",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffba",
          "name": "Zhuoran Jin",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbb",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbc",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbd",
          "name": "Jun Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T16:33:44.000Z",
      "submittedOnDailyAt": "2025-06-05T04:32:19.273Z",
      "title": "Análisis de Neuronas Cortadas para Evaluar con Confianza un LLM",
      "submittedOnDailyBy": {
        "_id": "648c48d8c0ddeee6df5b6d22",
        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
        "isPro": false,
        "fullname": "Shangqing Tu",
        "user": "tsq2000",
        "type": "user"
      },
      "summary": "La evaluación de confianza en el desarrollo de modelos de lenguaje grande (LLMs) es crucial, pero actualmente esta evaluación depende principalmente de marcos de evaluación publicados, lo que afecta significativamente la equidad debido a problemas de contemporaneidad de los datos. En los estudios previos, se ha enfocado en la construcción de marcos de evaluación dinámicos para resolver este problema, pero la continua construcción de nuevos marcos es costosa y cíclica. En este artículo, se analiza la estructura de modelos contemporáneos para abordar el problema de contemporaneidad. En los experimentos, se demostró que la sobreavaluación de modelos contemporáneos es debida a que los parámetros obtuvieron soluciones de corto camino durante el entrenamiento. Además, se proporcionó un análisis relativo y causal para identificar y restringir los neuronas de corto camino utilizando un método nuevo. Esto permitió introducir una evaluación que suprima las neuronas de corto camino, conocida como \"patch de neuronas de corto camino\". Los experimentos probaron que esta aproximación mitiga el problema de contemporaneidad. Además, la evaluación junto con los recientemente publicados marcos de evaluación confiables como MixEval mostró una fuerte correlación lineal, con un coeficiente de Shurman (rho) superior a 0.95. Esta alta correlación indica que esta aproximación muestra la verdadera capacidad de los modelos y confiables. Además, se demostró la generalización de esta aproximación mediante experimentos adicionales en diferentes marcos de evaluación y rangos de configuración de parámetros. Código: https://github.com/GaryStack/Trustworthy-Evaluation",
      "upvotes": 6,
      "discussionId": "684132cc725b7fb67f68fff5",
      "githubRepo": "https://github.com/GaryStack/Trustworthy-Evaluation",
      "ai_summary": "A method called shortcut neuron patching identifies and suppresses shortcut neurons in language models to mitigate data contamination issues in trustworthy evaluations.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "trustworthy evaluation",
        "data contamination",
        "benchmarks",
        "dynamic benchmarks",
        "shortcut solutions",
        "shortcut neurons",
        "comparative analysis",
        "causal analysis",
        "shortcut neuron patching",
        "MixEval",
        "Spearman coefficient"
      ]
    },
    "publishedAt": "2025-06-04T12:33:44.000Z",
    "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
    "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient (rho)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04108",
      "authors": [
        {
          "_id": "684104a16b106ae42f5acc1a",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1b",
          "name": "Tianzhu Ye",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1c",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1d",
          "name": "Yuqing Xia",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1e",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1f",
          "name": "Yizhao Gao",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc20",
          "name": "Shijie Cao",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc21",
          "name": "Jianyong Wang",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc22",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T16:01:48.000Z",
      "submittedOnDailyAt": "2025-06-05T01:16:28.444Z",
      "title": "Rectified Sparse Attention",
      "submittedOnDailyBy": {
        "_id": "6300ef4779c5ddbc6cf83e1a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661005591657-noauth.jpeg",
        "isPro": false,
        "fullname": "Yutao Sun",
        "user": "sunyt32",
        "type": "user"
      },
      "summary": "La generación eficiente de cabezas de largas oraciones es un problema importante en modelos de lenguaje de gran tamaño. Los métodos de decodificación rara son capaces de aumentar la eficiencia, pero su asimetría en el caché de KV y el acumulación de errores aproximados pueden disminuir la calidad de la generación. En este trabajo, proponemos una simple y efectiva metodología llamada Attention with Rectified Sparse Attention (ReSA), que combina la atención esparsa de bloques con un ajuste denso periódico. ReSA utiliza pasos de flujo densos a intervalos fijos para actualizar el caché de KV, lo que limita el acumulación de errores y mantiene la correspondencia con la distribución de entrenamiento previo. A través de experimentos matemáticos, de modelado de lenguaje y de tareas de búsqueda, ReSA logró generar de manera aproximadamente sin pérdida y mejoró significativamente la eficiencia. En particular, se alcanzó una velocidad de 2.42 veces más rápida en la decodificación de oraciones de 256K palabras, lo que puede ser una solución práctica para la inferencia de contextos largos en tareas de búsqueda. El código está disponible en https://aka.ms/ReSA-LM.",
      "upvotes": 6,
      "discussionId": "684104a26b106ae42f5acc50",
      "ai_summary": "Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.",
      "ai_keywords": [
        "sparse decoding",
        "KV cache misalignment",
        "Rectified Sparse Attention",
        "ReSA",
        "block-sparse attention",
        "dense rectification",
        "pretraining distribution",
        "long-context inference"
      ]
    },
    "publishedAt": "2025-06-04T12:01:48.000Z",
    "title": "Rectified Sparse Attention",
    "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42times end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04108.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6300ef4779c5ddbc6cf83e1a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661005591657-noauth.jpeg",
      "fullname": "Yutao Sun",
      "name": "sunyt32",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03099",
      "authors": [
        {
          "_id": "684107c6142b5c0b4226025f",
          "name": "Chetwin Low",
          "hidden": false
        },
        {
          "_id": "684107c6142b5c0b42260260",
          "name": "Weimin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:29:28.000Z",
      "submittedOnDailyAt": "2025-06-05T01:30:00.782Z",
      "title": "TalkingMachines: Implementar un modelo de clasificación de videos de estilo Pitch-style voz-guiado por hora",
      "submittedOnDailyBy": {
        "_id": "62b43ffec624a43b1a1ada46",
        "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
        "isPro": false,
        "fullname": "weimin wang ",
        "user": "weiminwang",
        "type": "user"
      },
      "summary": "En este artículo, se propone el framework eficiente llamado TalkingMachines. Este framework transforma modelos de generación de vídeo pre-entrenados en modelos de animación de caracteres actuados por voz. TalkingMachines integra el modelo de lenguaje de voz-lenguaje (LLM) y nuestro modelo de generación de vídeo para permitir experiencias de conversación natural. Nuestras principales contribuciones son las siguientes:\n\n(1) Aplicamos DiT a la generación de avatars actuados por voz a partir de imágenes pre-entrenadas de nivel óptimo, creando un modelo con 1.8 billones de parámetros.\n(2) Observamos experiencias de conocimiento no simétrico desde modelos de corrección simétrico, evitando así la acumulación de errores y permitiendo un flujo de vídeo infinito.\n(3) Diseñamos una arquitectura de alta ejecución y baja latencia para la ejecución, utilizando las siguientes técnicas de optimización clave:\n(a) Separamos DiT y el decodificador VAE en dispositivos separados.\n(b) Utilizamos flujos CUDA para comunicar y calcular de manera eficiente y paralela entre dispositivos.\n(c) Reducimos las cálculos repetidos y maximizamos el flujo de transformación de la generación de frames.\n\nSe puede ver el vídeo demostración aquí - https://aaxwaz.github.io/TalkingMachines/",
      "upvotes": 6,
      "discussionId": "684107c8142b5c0b42260293",
      "projectPage": "https://aaxwaz.github.io/TalkingMachines/",
      "githubRepo": "https://github.com/aaxwaz/TalkingMachines",
      "ai_summary": "TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.",
      "ai_keywords": [
        "DiT",
        "audio large language model",
        "asymmetric knowledge distillation",
        "bidirectional teacher model",
        "sparse causal",
        "autoregressive student model",
        "inference pipeline",
        "CUDA streams",
        "frame-generation throughput"
      ]
    },
    "publishedAt": "2025-06-03T13:29:28.000Z",
    "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models",
    "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03099.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b43ffec624a43b1a1ada46",
      "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
      "fullname": "weimin wang ",
      "name": "weiminwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02592",
      "authors": [
        {
          "_id": "684104c89ec96d9991484c24",
          "user": {
            "_id": "65309a1d657ae56cdb65e0e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
            "isPro": false,
            "fullname": "Zhi-Yuan Chen",
            "user": "JaxChen",
            "type": "user"
          },
          "name": "Zhi-Yuan Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:45:29.221Z",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c25",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c26",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c27",
          "name": "Enrui Hu",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c28",
          "name": "Yankai Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T08:12:47.000Z",
      "submittedOnDailyAt": "2025-06-05T01:30:17.419Z",
      "title": "Más serio que la superficie: La medida de la preferencia propia en la decisión de un LLM",
      "submittedOnDailyBy": {
        "_id": "65309a1d657ae56cdb65e0e7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
        "isPro": false,
        "fullname": "Zhi-Yuan Chen",
        "user": "JaxChen",
        "type": "user"
      },
      "summary": "Según recientes estudios, los modelos de lenguaje de gran escala (LLMs) tienen la tendencia de preferir sus propias respuestas cuando se trata de evaluarlas como jurados. Esto significa que los modelos prefieren sus propias respuestas en comparación con aquellas generadas por otros modelos. Actualmente, los métodos generalmente calculan la diferencia entre los puntajes asignados por el modelo evaluador a las respuestas generadas por el modelo y a las respuestas generadas por otros modelos para medir esta preferencia. Sin embargo, este enfoque puede confundir la calidad de la respuesta con la preferencia, ya que puede reconocer diferencias positivas de puntajes incluso cuando no hay preferencia de la parte del modelo evaluador. Para resolver este problema, utilizamos la calidad de la respuesta como proxy con la \"gold-standard\" y proponemos el puntaje DBG, que mide la diferencia entre el puntaje asignado por el modelo evaluador a la respuesta del modelo y el puntaje del gold-standard correspondiente. La gold-standard refleja la calidad real de la respuesta, por lo que el puntaje DBG puede reducir la influencia de la confusión entre la calidad de la respuesta y la preferencia. Usando el puntaje DBG, evaluamos la preferencia de los modelos propios de sus respuestas en diferentes versiones, tamaños y capacidades de lógica. Además, investigamos dos factores que pueden afectar la preferencia de las respuestas: el estilo de texto de las respuestas y los datos de entrenamiento adicional del modelo evaluador. Finalmente, examinamos la estructura potencial de la preferencia de las respuestas desde una perspectiva basada en la atención. Nuestro código y datos están disponibles en https://github.com/zhiyuanc2001/self-preference.",
      "upvotes": 6,
      "discussionId": "684104c99ec96d9991484c5e",
      "githubRepo": "https://github.com/zhiyuanc2001/self-preference",
      "ai_summary": "The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.",
      "ai_keywords": [
        "large language models",
        "self-preference bias",
        "judge model",
        "gold judgments",
        "DBG score",
        "response quality",
        "attention-based perspective"
      ]
    },
    "publishedAt": "2025-06-03T04:12:47.000Z",
    "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments",
    "summary": "Recent studies show that large language models (LLMs) exhibit self-preference\nbias when serving as judges, meaning they tend to favor their own responses\nover those generated by other models. Existing methods typically measure this\nbias by calculating the difference between the scores a judge model assigns to\nits own responses and those it assigns to responses from other models. However,\nthis approach conflates self-preference bias with response quality, as\nhigher-quality responses from the judge model may also lead to positive score\ndifferences, even in the absence of bias. To address this issue, we introduce\ngold judgments as proxies for the actual quality of responses and propose the\nDBG score, which measures self-preference bias as the difference between the\nscores assigned by the judge model to its own responses and the corresponding\ngold judgments. Since gold judgments reflect true response quality, the DBG\nscore mitigates the confounding effect of response quality on bias measurement.\nUsing the DBG score, we conduct comprehensive experiments to assess\nself-preference bias across LLMs of varying versions, sizes, and reasoning\nabilities. Additionally, we investigate two factors that influence and help\nalleviate self-preference bias: response text style and the post-training data\nof judge models. Finally, we explore potential underlying mechanisms of\nself-preference bias from an attention-based perspective. Our code and data are\navailable at https://github.com/zhiyuanc2001/self-preference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02592.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65309a1d657ae56cdb65e0e7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
      "fullname": "Zhi-Yuan Chen",
      "name": "JaxChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03106",
      "authors": [
        {
          "_id": "684104d9ee7646c073776b2e",
          "name": "Xiaoying Zhang",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b2f",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b30",
          "user": {
            "_id": "666e91b1623133f1ce35acc5",
            "avatarUrl": "/avatars/cc78520e6cfb83817c1d0c1ac867ebdd.svg",
            "isPro": false,
            "fullname": "YipengZhang",
            "user": "YipengZhang",
            "type": "user"
          },
          "name": "Yipeng Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:45:46.851Z",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b31",
          "name": "Kaituo Feng",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b32",
          "name": "Chaochao Lu",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b33",
          "name": "Chao Yang",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b34",
          "name": "Helen Meng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:39:02.000Z",
      "submittedOnDailyAt": "2025-06-05T01:16:53.836Z",
      "title": "Critique-GRPO: Desarrollo de la inferencia lógica en LLM utilizando retroalimentación de lenguaje y números",
      "submittedOnDailyBy": {
        "_id": "67079840a9bcb7459b8d2a46",
        "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
        "isPro": false,
        "fullname": "Kaituo Feng",
        "user": "KaituoFeng",
        "type": "user"
      },
      "summary": "El desarrollo reciente del aprendizaje por refuerzo (RL) utilizando retroalimentación numérica (por ejemplo, scalárid) ha mejorado significativamente las capacidades lógicas complejas de los modelos de lenguaje grandes (LLMs). A pesar de este éxito, hemos identificado tres problemas importantes en el uso exclusivo de retroalimentación numérica en RL: la platificación del rendimiento, la limitación de la auto-reflexión y el fracaso a largo plazo. Además, demostramos que modelos fine-tunados en RL pueden generar una fine-tunado correcto para resolver el problema de fracaso a largo plazo utilizando retroalimentación de lenguaje natural (forma de evaluación), incluso después de que el rendimiento se platifica. Basándonos en esta perspectiva, proponemos un marco de RL en línea efectivo para la optimización de políticas que integra retroalimentación de lenguaje natural y numérica, llamado Critique-GRPO. Critique-GRPO permite que los LLMs aprendan simultáneamente a mejorar respuestas iniciales y a ser evaluadas, manteniendo la exploración. Con Qwen2.5-7B-Base y Qwen3-8B-Base, y validados con 8 tareas difíciles de matemática, STEM y lógica general, Critique-GRPO mejora en un 4.5% a 5% en el promedio de la tasa de paso @1, superando la base línea. En particular, Critique-GRPO supera una base línea fuerte que incluye guías de expertos. En el análisis adicional, se han claramente identificado dos perspectivas importantes sobre la exploración de políticas: (1) una alta entropía no garantiza siempre un aprendizaje eficiente en la exploración, y (2) respuestas largas no garantizan siempre una eficiencia en la exploración.",
      "upvotes": 5,
      "discussionId": "684104daee7646c073776b88",
      "githubRepo": "https://github.com/zhangxy-2019/critique-GRPO",
      "ai_summary": "Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "large language models",
        "LLMs",
        "scalar rewards",
        "performance plateaus",
        "self-reflection",
        "persistent failures",
        "natural language feedback",
        "critiques",
        "policy optimization",
        "Qwen2.5-7B-Base",
        "Qwen3-8B-Base",
        "pass@1",
        "policy exploration",
        "entropy",
        "response length"
      ]
    },
    "publishedAt": "2025-06-03T13:39:02.000Z",
    "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
    "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03106.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67079840a9bcb7459b8d2a46",
      "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
      "fullname": "Kaituo Feng",
      "name": "KaituoFeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21541",
      "authors": [
        {
          "_id": "6840f79ceb249b555b244efc",
          "name": "Zitong Wang",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244efd",
          "name": "Hang Zhao",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244efe",
          "name": "Qianyu Zhou",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244eff",
          "name": "Xuequan Lu",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244f00",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244f01",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T16:08:04.000Z",
      "submittedOnDailyAt": "2025-06-05T00:21:34.798Z",
      "title": "DiffDecompose: Decomposición por capas de un entrenador de ramas para imágenes sintéticas de alfa",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "El modelo de difusión ha sido muy exitoso en tareas de eliminación de objetos y varias generativas recientes. Sin embargo, la tecnología actual de descomposición de imágenes es difícil para separar las ocultaciones de capas transparentes o semitransparentes debido a dependencias de la máscara de proyección, asumir objetos estáticos y la escasez de conjuntos de datos. En este artículo, abordamos un nuevo desafío: la descomposición de imágenes alfa por capas, donde se reconstruyen las capas de los componentes a partir de imágenes que superponen, en situaciones de ocultación no lineal de capas alfa transparentes o semitransparentes. Para enfrentar la incertidumbre de las capas, la generalización y la escasez de datos, primero presentamos el AlphaBlend. El AlphaBlend utiliza un conjunto de datos de alta calidad inicial de gran escala para abordar la descomposición de capas transparentes y semitransparentes, y soporta seis sub-tareas reales (por ejemplo: eliminación de fuegos semitransparentes, descomposición de células semitransparentes, descomposición de artículos de vidrio). Basado en este conjunto de datos, proponemos el DiffDecompose. El DiffDecompose aprende el proceso de descomposición de capas según la imagen de entrada, el prompt semántico y el tipo de branding. En lugar de predecir directamente la matriz alfa, el DiffDecompose realiza la descomposición en contexto y permite que el modelo predicte una o varias capas. Para mantener la correspondencia de píxeles entre las capas, introducimos la clonación de codificación de posición de capas (Layer Position Encoding Cloning). Evaluamos el efecto del DiffDecompose en el conjunto de datos de alfa propuesto y en el conjunto de datos de logos publicados. Los códigos y conjuntos de datos se proporcionarán después de la revisión del artículo. Los códigos pueden usarse a través de la siguiente URL: https://github.com/Wangzt1121/DiffDecompose.",
      "upvotes": 5,
      "discussionId": "6840f7a1eb249b555b244ffe",
      "ai_summary": "DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.",
      "ai_keywords": [
        "diffusion models",
        "diffusion Transformer",
        "posterior",
        "semantic prompts",
        "blending type",
        "In-Context Decomposition",
        "Layer Position Encoding Cloning",
        "AlphaBlend dataset",
        "translucent flare removal",
        "semi-transparent cell decomposition",
        "glassware decomposition",
        "LOGO dataset"
      ]
    },
    "publishedAt": "2025-05-24T12:08:04.000Z",
    "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers",
    "summary": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21541.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03956",
      "authors": [
        {
          "_id": "6841396eee27975702b57e87",
          "user": {
            "_id": "6759546743971eff5a12a087",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/esJm_83zW1R6NqWltof8P.png",
            "isPro": false,
            "fullname": "Aojun Lu",
            "user": "Kurt1024",
            "type": "user"
          },
          "name": "Aojun Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:29.156Z",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e88",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e89",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:31.411Z",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e8a",
          "name": "Chunhui Ding",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e8b",
          "name": "Yanan Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T13:46:33.000Z",
      "submittedOnDailyAt": "2025-06-05T05:00:41.771Z",
      "title": "Adaptar antes de Continual Learning",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "La continua aprendizaje (CL) tiene como objetivo que las redes neuronales agreguen nuevos conocimientos mientras mantengan los conocimientos existentes (sobre-ajuste). Los modelos de predicción (PTMs) desempeñan un papel importante en la CL, pero en los métodos actuales de acceso, el cuerpo de los PTMs está fijado para mantener la estabilidad y limitar la sostenibilidad, lo que presenta particulares problemas cuando se experimentan grandes diferencias entre dominios en tareas incrementales. En cambio, ajustar de manera secuencial el PTM completo está expuesto al olvido de conocimientos dispersos y a un importante trade-off entre estabilidad y sostenibilidad. Para enfrentar estas desafíos, proponemos un nuevo marco de trabajo \"Adaptando PTMs antes del proceso de CL central (ACL)\" que mejora el cuerpo de los PTMs antes de los procesos clave de la CL. ACL mejora el cuerpo de los PTMs antes de aprender cada nueva tarea, utilizando un fase de adaptación de plug-and-play con el uso de técnicas de ajuste de CL existentes (por ejemplo, fine-tuning). ACL se ha probado teóricamente y experimentalmente, demostrando cómo equilibra estabilidad y sostenibilidad al ajustar los embeddings para que coincidan con los prototipos de clases originales y se alejen de otras clases, lo que mejora la sostenibilidad. Los experimentos expandidos muestran que ACL mejora significativamente el rendimiento de la CL al integrarse en marcos de referencia y métodos de expansión. Ofrece soluciones amplias para la CL basada en PTMs.",
      "upvotes": 4,
      "discussionId": "6841396eee27975702b57eb7",
      "projectPage": "https://github.com/byyx666/ACL_code",
      "githubRepo": "https://github.com/byyx666/ACL_code",
      "ai_summary": "Adapting Pre-trained Models before the core CL process (ACL) improves Continual Learning by enhancing plasticity while maintaining stability.",
      "ai_keywords": [
        "Continual Learning",
        "CL",
        "Pre-trained models",
        "PTMs",
        "plasticity",
        "stability",
        "domain gaps",
        "catastrophic forgetting",
        "prompt tuning",
        "embeddings",
        "class prototypes"
      ]
    },
    "publishedAt": "2025-06-04T09:46:33.000Z",
    "title": "Adapt before Continual Learning",
    "summary": "Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). While pre-trained models (PTMs) have become pivotal in CL,\nprevailing approaches freeze the PTM backbone to preserve stability, limiting\ntheir plasticity, particularly when encountering significant domain gaps in\nincremental tasks. Conversely, sequentially finetuning the entire PTM risks\ncatastrophic forgetting of generalizable knowledge, exposing a critical\nstability-plasticity trade-off. To address this challenge, we propose Adapting\nPTMs before the core CL process (ACL), a novel framework that refines the PTM\nbackbone through a plug-and-play adaptation phase before learning each new task\nwith existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by\naligning embeddings with their original class prototypes while distancing them\nfrom others, theoretically and empirically shown to balance stability and\nplasticity. Extensive experiments demonstrate that ACL significantly improves\nCL performance across benchmarks and integrated methods, offering a versatile\nsolution for PTM-based CL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03956.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03448",
      "authors": [
        {
          "_id": "684105479060432bf302b432",
          "name": "Bimsara Pathiraja",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b433",
          "user": {
            "_id": "622d2ff38d04fd29a9ccf1a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
            "isPro": false,
            "fullname": "Maitreya Patel",
            "user": "mpatel57",
            "type": "user"
          },
          "name": "Maitreya Patel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:48.135Z",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b434",
          "name": "Shivam Singh",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b435",
          "name": "Yezhou Yang",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b436",
          "name": "Chitta Baral",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T23:20:24.000Z",
      "submittedOnDailyAt": "2025-06-05T01:19:11.023Z",
      "title": "RefEdit: Marco y Metodología para Mejorar Modelos de Edición de Imágenes Basados en Expresiones de Referencia",
      "submittedOnDailyBy": {
        "_id": "622d2ff38d04fd29a9ccf1a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
        "isPro": false,
        "fullname": "Maitreya Patel",
        "user": "mpatel57",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de tecnologías de edición de imágenes basadas en reversa y direcciones ha llevado a que los métodos actuales principalmente están adaptados para editar unicamente el principal objeto, pero en casos donde se trata de esquemas complejos que incluyen múltiples existencias, se enfrentan a grandes desafíos. Para cuantificar estas limitaciones, presentamos RefEdit-Bench. Este es un estricto marco de prueba de la realidad, basado en RefCOCO, con millones de muestras de entrenamiento que no superan casi a la línea base básica. Para superar estos límites, presentamos RefEdit, un modelo de edición basado en instrucciones entrenado utilizando nuestra escalable pipeline de generación de datos sintéticos. RefEdit se entrena con 20,000 tripletas de edición, y supera a la línea base basada en modelos Flux/SD3 entrenados con millones de datos, mejorando en evaluaciones extendidas en diferentes marcos de referencia. Nuestro modelo también supera tareas de representación de referencia, mejora el rendimiento de marcos de referencia tradicionales y implementa resultados recientes como métodos de copia de sonido cerrado. Para garantizar la reproducibilidad, publicamos los datos y los checkpoints.",
      "upvotes": 4,
      "discussionId": "6841054b9060432bf302b559",
      "projectPage": "https://refedit.vercel.app",
      "githubRepo": "https://github.com/bimsarapathiraja/refedit",
      "ai_summary": "RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.",
      "ai_keywords": [
        "RefEdit-Bench",
        "RefCOCO",
        "instruction-based editing model",
        "scalable synthetic data generation pipeline",
        "Flux/SD3",
        "state-of-the-art results"
      ]
    },
    "publishedAt": "2025-06-03T19:20:24.000Z",
    "title": "RefEdit: A Benchmark and Method for Improving Instruction-based Image\n  Editing Model on Referring Expressions",
    "summary": "Despite recent advances in inversion and instruction-based image editing,\nexisting approaches primarily excel at editing single, prominent objects but\nsignificantly struggle when applied to complex scenes containing multiple\nentities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous\nreal-world benchmark rooted in RefCOCO, where even baselines trained on\nmillions of samples perform poorly. To overcome this limitation, we introduce\nRefEdit -- an instruction-based editing model trained on our scalable synthetic\ndata generation pipeline. Our RefEdit, trained on only 20,000 editing triplets,\noutperforms the Flux/SD3 model-based baselines trained on millions of data.\nExtensive evaluations across various benchmarks demonstrate that our model not\nonly excels in referring expression tasks but also enhances performance on\ntraditional benchmarks, achieving state-of-the-art results comparable to\nclosed-source methods. We release data \\& checkpoint for reproducibility.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622d2ff38d04fd29a9ccf1a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
      "fullname": "Maitreya Patel",
      "name": "mpatel57",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03355",
      "authors": [
        {
          "_id": "68415c5cce09e3eca94e9839",
          "name": "Elias Abad Rocamora",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983a",
          "user": {
            "_id": "6310a6bb0a43f97f6c5567d3",
            "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
            "isPro": false,
            "fullname": "Christian Schlarmann",
            "user": "chs20",
            "type": "user"
          },
          "name": "Christian Schlarmann",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T09:59:48.051Z",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983b",
          "name": "Naman Deep Singh",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983c",
          "name": "Yongtao Wu",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983d",
          "name": "Matthias Hein",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983e",
          "name": "Volkan Cevher",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T19:57:09.000Z",
      "submittedOnDailyAt": "2025-06-05T07:29:30.561Z",
      "title": "Robustez en ambos dominios: CLIP necesita un poderoso encoder de texto.",
      "submittedOnDailyBy": {
        "_id": "6310a6bb0a43f97f6c5567d3",
        "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
        "isPro": false,
        "fullname": "Christian Schlarmann",
        "user": "chs20",
        "type": "user"
      },
      "summary": "Los ataques de entrada adversarios pueden cambiar significativamente los embeddings de CLIP. Esto puede afectar la robustez de modelos que insertan CLIP en medio. Por ejemplo, modelos de generación de imágenes a partir de texto o grandes modelos de lenguaje visuo-lingüístico. Aunque se ha trabajado en la robustez del encoder de CLIP de imágenes, la robustez del encoder de texto es incierta. Este artículo complementa esta lacuna en la literatura. Proponemos LEAF (Fine-tuning Adversarialmente Ampliado Localmente). LEAF es un método eficiente de fine-tuning adversario para el dominio del texto y puede aplicarse a modelos de CLIP de gran escala. Nuestro modelo mejora significativamente la precisión de 0 shot en el dominio del texto y mantiene el rendimiento visual proporcionado por un encoder de imagenes robusto. Junto con modelos de expansión de imágenes a partir de texto, LEAF mejora la calidad de generación bajo ruido adversario. En evaluaciones de diversidad, nuestro encoder robusto de CLIP puede aumentar la probabilidad de llamada frente a ruidos adversarios en modelos estándar de CLIP. Finalmente, un encoder de texto robusto optimiza directamente la reconstrucción de consultas de entrada para mejorarla.",
      "upvotes": 4,
      "discussionId": "68415c5ece09e3eca94e98e4",
      "ai_summary": "LEAF, an adversarial finetuning method, enhances the robustness of CLIP text encoders, improving zero-shot accuracy and multimodal retrieval performance under adversarial noise.",
      "ai_keywords": [
        "adversarial input attacks",
        "CLIP embeddings",
        "text-to-image generative models",
        "large vision language models",
        "adversarial finetuning",
        "zero-shot adversarial accuracy",
        "text-to-image diffusion models",
        "multimodal retrieval tasks",
        "robust text encoders"
      ]
    },
    "publishedAt": "2025-06-03T15:57:09.000Z",
    "title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder",
    "summary": "Adversarial input attacks can cause a significant shift of CLIP embeddings.\nThis can affect the downstream robustness of models incorporating CLIP in the\npipeline, such as text-to-image generative models or large vision language\nmodels. While some efforts have been done towards making the CLIP image\nencoders robust, the robustness of text encoders remains unexplored. In this\nwork, we cover this gap in the literature. We propose LEAF: an efficient\nadversarial finetuning method for the text domain, with the ability to scale to\nlarge CLIP models. Our models significantly improve the zero-shot adversarial\naccuracy in the text domain, while maintaining the vision performance provided\nby robust image encoders. When combined with text-to-image diffusion models, we\ncan improve the generation quality under adversarial noise. When employing our\nrobust CLIP encoders in multimodal retrieval tasks, we improve the recall under\nadversarial noise over standard CLIP models. Finally, we show that robust text\nencoders facilitate better reconstruction of input text from its embedding via\ndirect optimization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03355.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310a6bb0a43f97f6c5567d3",
      "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
      "fullname": "Christian Schlarmann",
      "name": "chs20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02945",
      "authors": [
        {
          "_id": "6841008f2f66f731bf010feb",
          "name": "Aishwarya Sahoo",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fec",
          "name": "Jeevana Kruthi Karnuthala",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fed",
          "name": "Tushar Parmanand Budhwani",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fee",
          "name": "Pranchal Agarwal",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fef",
          "name": "Sankaran Vaidyanathan",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff0",
          "name": "Alexa Siu",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff1",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:55.170Z",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff2",
          "name": "Jennifer Healey",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff3",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff4",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff5",
          "name": "Uttaran Bhattacharya",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff6",
          "name": "Branislav Kveton",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T14:44:23.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:33.123Z",
      "title": "Jueces Cuantitativos de LLM\n\nCampeonato LLM de Escándalo\n\n(Nota: Aunque se solicita no agregar ningún comentario o texto adicional, se asegura que la traducción sea precisa y profesional, teniendo en cuenta las costumbres y términos profesionales del idioma coreano.)",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "LLM-as-a-judge es un marco de trabajo que permite a modelos de lenguaje grandes (LLM) evaluar automáticamente las salidas de otros modelos de lenguaje grandes. Proponemos un juez cuantitativo de LLM. Este modelo ajusta los puntajes evaluados por el juez actual de LLM de manera que coinciden con los puntajes evaluados por un humano en ciertos dominios, utilizando un modelo de regresión. Este modelo se entrenó utilizando los contextos y puntajes de evaluación de los evaluadores para mejorar los puntajes originales de los evaluadores. Presentamos cuatro jueces cuantitativos. Esto demuestra la generalidad y diversidad de nuestro marco de trabajo. Esperamos que nuestro marco de trabajo sea eficiente computacionalmente, no basándose en aprendizaje por observación, sino en retroalimentación entrenadida, y estadísticamente eficiente cuando el retroalimentación humana está limitada. Hemos verificado el efecto de nuestro marco de trabajo mediante experimentos con cuatro conjuntos de datos y dos jueces básicos. Nuestros experimentos muestran que los jueces cuantitativos mejoran efectivamente la capacidad de predicción de los jueces existentes.",
      "upvotes": 4,
      "discussionId": "684100902f66f731bf01101e",
      "ai_summary": "A framework uses quantitative LLM judges to align existing LLM evaluation scores with human scores, improving predictive power and efficiency through regression models.",
      "ai_keywords": [
        "LLM-as-a-judge",
        "large language model",
        "quantitative LLM judges",
        "regression models",
        "score alignment",
        "predictive power",
        "post-hoc modeling"
      ]
    },
    "publishedAt": "2025-06-03T10:44:23.000Z",
    "title": "Quantitative LLM Judges",
    "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02945.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00482",
      "authors": [
        {
          "_id": "68402986a50b67f983749710",
          "user": {
            "_id": "6576ace7769f3ee9bd7b1b88",
            "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
            "isPro": false,
            "fullname": "Eunsu Kim",
            "user": "EunsuKim",
            "type": "user"
          },
          "name": "Eunsu Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:49.056Z",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749711",
          "name": "Haneul Yoo",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749712",
          "name": "Guijin Son",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749713",
          "name": "Hitesh Patel",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749714",
          "name": "Amit Agarwal",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749715",
          "user": {
            "_id": "60e0251ea9b5d8282481f2b7",
            "avatarUrl": "/avatars/43441373af054a6184c22097bfeb97e4.svg",
            "isPro": false,
            "fullname": "Alice Oh",
            "user": "aliceoh",
            "type": "user"
          },
          "name": "Alice Oh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-05T06:37:21.889Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T09:24:32.000Z",
      "submittedOnDailyAt": "2025-06-05T04:40:48.000Z",
      "title": "BenchHub: Hoja de cálculo de benchmark de unidades. Evaluación general y definible por el usuario de un LLM.",
      "submittedOnDailyBy": {
        "_id": "6576ace7769f3ee9bd7b1b88",
        "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
        "isPro": false,
        "fullname": "Eunsu Kim",
        "user": "EunsuKim",
        "type": "user"
      },
      "summary": "A medida que los modelos de lenguaje de alto nivel (LLMs) evolucionan, la necesidad de conjuntos de datos más recientes y marcos de evaluación corregidos se ha vuelto cada vez más importante. Sin embargo, los conjuntos de datos actuales están dispersos y su manejo es difícil, lo que dificulta la realización de evaluaciones en específicos requisitos o áreas. En particular, el aumento de la importancia de modelos en áreas específicas como la matemática o el código ha sido notable. En este artículo, se presenta la recopilación de conjuntos de datos de evaluación y el introducción de un repositorio de evaluación, BenchHub, que proporciona clases dinámicas para que los investigadores y desarrolladores puedan evaluar efectivamente los LLMs. BenchHub integra 303K preguntas de 38 marcos de evaluación, ofreciendo actualizaciones continuas y una gestión de datos escalable. De esta manera, se pueden realizar evaluaciones flexibles en diferentes áreas y casos de uso. A través de experimentos con familias de modelos de LLMs, se muestra que el rendimiento de los modelos puede variar significativamente en subconjuntos específicos de áreas, lo que subraya la importancia de los marcos de evaluación por área. BenchHub fomenta la reutilización de conjuntos de datos, hace más transparente la comparación de modelos, facilita la identificación de áreas poco abordadas en marcos de evaluación existentes y proporciona una infraestructura crucial para el desarrollo de la investigación en la evaluación de LLMs.",
      "upvotes": 3,
      "discussionId": "68402987a50b67f983749746",
      "projectPage": "https://huggingface.co/BenchHub",
      "githubRepo": "https://github.com/rladmstn1714/BenchHub",
      "ai_summary": "BenchHub is a dynamic benchmark repository that aggregates and classifies datasets for large language models, facilitating domain-specific evaluations and improving model comparisons.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "BenchHub",
        "benchmark repository",
        "domain-specific models",
        "benchmark datasets",
        "continuous updates",
        "scalable data management",
        "model performance",
        "domain-aware benchmarking"
      ]
    },
    "publishedAt": "2025-05-31T05:24:32.000Z",
    "title": "BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM\n  Evaluation",
    "summary": "As large language models (LLMs) continue to advance, the need for up-to-date\nand well-organized benchmarks becomes increasingly critical. However, many\nexisting datasets are scattered, difficult to manage, and make it challenging\nto perform evaluations tailored to specific needs or domains, despite the\ngrowing importance of domain-specific models in areas such as math or code. In\nthis paper, we introduce BenchHub, a dynamic benchmark repository that empowers\nresearchers and developers to evaluate LLMs more effectively. BenchHub\naggregates and automatically classifies benchmark datasets from diverse\ndomains, integrating 303K questions across 38 benchmarks. It is designed to\nsupport continuous updates and scalable data management, enabling flexible and\ncustomizable evaluation tailored to various domains or use cases. Through\nextensive experiments with various LLM families, we demonstrate that model\nperformance varies significantly across domain-specific subsets, emphasizing\nthe importance of domain-aware benchmarking. We believe BenchHub can encourage\nbetter dataset reuse, more transparent model comparisons, and easier\nidentification of underrepresented areas in existing benchmarks, offering a\ncritical infrastructure for advancing LLM evaluation research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00482.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576ace7769f3ee9bd7b1b88",
      "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
      "fullname": "Eunsu Kim",
      "name": "EunsuKim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23807",
      "authors": [
        {
          "_id": "683fec0a9f37285365be6142",
          "user": {
            "_id": "656201912d309fa7e27ddf40",
            "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
            "isPro": false,
            "fullname": "Yuli chen",
            "user": "yulichen",
            "type": "user"
          },
          "name": "Yuli Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T15:03:32.826Z",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6143",
          "name": "Bo Cheng",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6144",
          "name": "Jiale Han",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6145",
          "name": "Yingying Zhang",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6146",
          "name": "Yingting Li",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6147",
          "name": "Shuhao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T07:35:00.000Z",
      "submittedOnDailyAt": "2025-06-05T00:42:31.891Z",
      "title": "DLP: Prueba dinámica de cada capa (Dynamic Layer Pruning)",
      "submittedOnDailyBy": {
        "_id": "656201912d309fa7e27ddf40",
        "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
        "isPro": false,
        "fullname": "Yuli chen",
        "user": "yulichen",
        "type": "user"
      },
      "summary": "La pruning es ampliamente introducido recientemente para reducir el tamaño de los parámetros de los grandes modelos de lenguaje (LLMs) y mejorar la eficiencia de la inferencia. Los métodos principales de pruning generalmente se basan en estrategias uniformes por capas y suelen experimentar una pérdida significativa de rendimiento a altos niveles de sparsidad. Recientes investigaciones han enfocado en pruning no uniforme por capas, reconociendo la contribución diferente de cada capa, pero estas aproximaciones a menudo se basan en valores predefinidos y no logran el rendimiento óptimo. Para superar estas limitaciones, proponemos un nuevo método llamado \"Dynamic Layerwise Pruning (DLP)\". Este enfoque determina la importancia relativa de cada capa a través de la integración de la información de pesos y activaciones de entrada, asignando adecuadamente los niveles de pruning. Los resultados de los experimentos muestran que DLP mantiene el rendimiento del modelo a altos niveles de sparsidad y se aplica eficazmente en varios LLMs. En particular, a un 70% de sparsidad, DLP reduce la perplexidad de LLaMA2-7B en un 7.79% y mejora la precisión promedio en un 2.7% en comparación con los métodos más avanzados. Además, DLP se integra fácilmente con las tecnologías de compresión actuales de LLMs y con el fine-tuning eficiente de parámetros (PEFT). Publicamos el código en GitHub en https://github.com/ironartisan/DLP y invitamos a continuar con la investigación.",
      "upvotes": 3,
      "discussionId": "683fec0a9f37285365be617f",
      "ai_summary": "A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.",
      "ai_keywords": [
        "pruning",
        "Large Language Models (LLMs)",
        "uniform layerwise pruning",
        "non-uniform layerwise pruning",
        "Dynamic Layerwise Pruning (DLP)",
        "perplexity",
        "Parameter-Efficient Fine-Tuning (PEFT)"
      ]
    },
    "publishedAt": "2025-05-27T03:35:00.000Z",
    "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
    "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656201912d309fa7e27ddf40",
      "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
      "fullname": "Yuli chen",
      "name": "yulichen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04133",
      "authors": [
        {
          "_id": "6840f32dda736de98e843831",
          "user": {
            "_id": "64d3c16a0553a2522f1aa792",
            "avatarUrl": "/avatars/951e272ffccf2388f138b248e5ef7142.svg",
            "isPro": false,
            "fullname": "Shaina Raza",
            "user": "shainar",
            "type": "user"
          },
          "name": "Shaina Raza",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-05T01:39:24.184Z",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843832",
          "name": "Ranjan Sapkota",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843833",
          "name": "Manoj Karkee",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843834",
          "name": "Christos Emmanouilidis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/7jK4mzUkVjPRUDMAacaCO.jpeg"
      ],
      "publishedAt": "2025-06-04T16:26:11.000Z",
      "submittedOnDailyAt": "2025-06-05T00:02:27.010Z",
      "title": "TRiSM para el IA de agentes: revisión de la confianza, riesgo y gestión de seguridad en sistemas multi-agente basados en IA de lenguaje de máquina",
      "submittedOnDailyBy": {
        "_id": "67ddd80896ac367438d400a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
        "isPro": false,
        "fullname": "Ranjan Sapkota",
        "user": "RanjanSapkota",
        "type": "user"
      },
      "summary": "El sistema de IA agente se basa en modelos de lenguaje grandes (LLM) y se construye con múltiples agentes, redefiniendo la autonomía, la cooperación y las decisiones en el ámbito empresarial y social. Esta revisión proporciona un análisis estructurado sobre la gestión de confianza, riesgos y seguridad (TRiSM) en sistemas de múltiples agentes basados en LLM. Primero, se examina la base conceptual del IA agente, la diferencia con el marco de trabajo de IA agente y el diseño de sistemas que extienden las funciones de los agentes. A continuación, se describen con detalle las cuatro pilares de TRiSM en el marco de la IA agente: Justicia, Explicabilidad, ModelOps y Privacidad/Seguridad. Se identifican los vectores de riesgos característicos de aplicaciones de IA agente y presentan estudios de caso que muestran vulnerabilidades reales que pueden afectar a los sistemas de riesgo. Además, se investigan las instituciones de construcción de confianza, los métodos de transparencia y vigilancia y las estrategias de explicabilidad en sistemas de agentes distribuidos de LLM. Se también evalúan métricas para la confianza, la explicabilidad y la humano-ciencia, incluyendo la participación en desafíos de benchmark abierto. Finalmente, se examina la seguridad y la privacidad, con investigaciones sobre criptografía, defensa contra ataques y la evolución de las leyes de IA. Este estudio proporciona una guía para la adopción responsable de IA agente y sugiere direcciones de investigación para la adopción segura, responsable y transparente de principios fuertes de TRiSM.",
      "upvotes": 2,
      "discussionId": "6840f32eda736de98e843858",
      "ai_summary": "A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.",
      "ai_keywords": [
        "LLMs",
        "agentic AI",
        "multi-agent systems",
        "TRiSM",
        "governance",
        "explainability",
        "ModelOps",
        "privacy",
        "security",
        "encryption",
        "adversarial defense",
        "compliance",
        "AI regulations",
        "trust-building mechanisms",
        "transparency",
        "oversight",
        "interpretability",
        "human-centered performance",
        "benchmarking",
        "responsible AI",
        "research directions"
      ]
    },
    "publishedAt": "2025-06-04T12:26:11.000Z",
    "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems",
    "summary": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/7jK4mzUkVjPRUDMAacaCO.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04133.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ddd80896ac367438d400a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
      "fullname": "Ranjan Sapkota",
      "name": "RanjanSapkota",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04034",
      "authors": [
        {
          "_id": "6840ff0b535bfb4942b31576",
          "name": "Qing Jiang",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31577",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31578",
          "name": "Zhaoyang Zeng",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31579",
          "name": "Junzhi Yu",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b3157a",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f46b6838ac3601fc89852/0J-cvgz2dA6bVQJJNb_Yz.jpeg"
      ],
      "publishedAt": "2025-06-04T14:56:57.000Z",
      "submittedOnDailyAt": "2025-06-05T00:56:30.698Z",
      "title": "Rex-Thinker: Referencia de objetos basada en lógica de contexto conectada",
      "submittedOnDailyBy": {
        "_id": "647f46b6838ac3601fc89852",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647f46b6838ac3601fc89852/N5cr1MFEtgKLJ4sVAhS04.jpeg",
        "isPro": true,
        "fullname": "Qing Jiang",
        "user": "Mountchicken",
        "type": "user"
      },
      "summary": "El objetivo de la detección de objetos es detectar todos los objetos presentes en una imagen que coincidan con una descripción dada en lenguaje natural. Argumentamos que es crucial que un modelo fuerte de detección de objetos realice predicciones precisas sobre el contenido visual cuando se le proporciona una descripción. En particular, debe cumplir las siguientes dos características principales: 1) Visualidad, proporcionar una explicación interpretable con una clara asociación con evidencias visuales para justificar las predicciones; 2) Confiabilidad, rechazar representaciones que no corresponden a objetos presentes en la imagen, aunque estas representaciones sean expresadas. Sin embargo, muchos métodos tratan la detección de objetos directamente como un tarea de predicción de bounding boxes, lo que limita su interpretabilidad y hace imposible rechazar representaciones sin objetos. En este estudio, proponemos el modelo Rex-Thinker para configurar la detección de objetos como una tarea explícita de inferencia basada en contexto (CoT). Al proporcionarse una representación de un objeto, Rex-Thinker identifica todas las instancias de objetos candidatas que correspondan a la categoría del objeto. Para cada candidato, Rex-Thinker realiza una evaluación paso a paso sobre si coincide con la representación dada y finaliza con una predicción. Para apoyar esto, construimos un gran conjunto de datos de detección de objetos en el estilo de CoT, HumanRef-CoT, mediante la utilización de GPT-4o en el conjunto de datos HumanRef. Cada traza de razón está representada en un formato estructurado de construcción, acción y resumen, permitiendo que el modelo aprenda razones interpretables sobre las instancias de objetos candidatas. Además, Rex-Thinker se entrena en dos etapas: primero con un entrenamiento de inicio frío de entrenamiento guiado paso a paso, para enseñar al modelo a construir razones, y luego con aprendizaje de refuerzo basado en GRPO para mejorar la precisión y la generalización. Los experimentos muestran que nuestro enfoque supera los límites de precisión y interpretabilidad en evaluaciones dentro del dominio, y también muestra su capacidad para rechazar representaciones falsas y su fuerte generalización en entornos fuera del dominio.",
      "upvotes": 2,
      "discussionId": "6840ff0e535bfb4942b3165f",
      "projectPage": "https://rexthinker.github.io/",
      "githubRepo": "https://github.com/IDEA-Research/Rex-Thinker",
      "ai_summary": "Rex-Thinker is a CoT-based model that enhances object referring by performing step-by-step reasoning over candidate objects, leading to improved interpretability and rejection of mismatched queries.",
      "ai_keywords": [
        "CoT reasoning",
        "HumanRef-CoT",
        "GPT-4o",
        "structured reasoning",
        "cold-start supervised fine-tuning",
        "GRPO-based RL learning"
      ]
    },
    "publishedAt": "2025-06-04T10:56:57.000Z",
    "title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning",
    "summary": "Object referring aims to detect all objects in an image that match a given\nnatural language description. We argue that a robust object referring model\nshould be grounded, meaning its predictions should be both explainable and\nfaithful to the visual content. Specifically, it should satisfy two key\nproperties: 1) Verifiable, by producing interpretable reasoning that justifies\nits predictions and clearly links them to visual evidence; and 2) Trustworthy,\nby learning to abstain when no object in the image satisfies the given\nexpression. However, most methods treat referring as a direct bounding box\nprediction task, offering limited interpretability and struggling to reject\nexpressions with no matching object. In this work, we propose Rex-Thinker, a\nmodel that formulates object referring as an explicit CoT reasoning task. Given\na referring expression, we first identify all candidate object instances\ncorresponding to the referred object category. Rex-Thinker then performs\nstep-by-step reasoning over each candidate to assess whether it matches the\ngiven expression, before making a final prediction. To support this paradigm,\nwe construct a large-scale CoT-style referring dataset named HumanRef-CoT by\nprompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a\nstructured planning, action, and summarization format, enabling the model to\nlearn decomposed, interpretable reasoning over object candidates. We then train\nRex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach\nthe model how to perform structured reasoning, followed by GRPO-based RL\nlearning to improve accuracy and generalization. Experiments show that our\napproach outperforms standard baselines in both precision and interpretability\non in-domain evaluation, while also demonstrating improved ability to reject\nhallucinated outputs and strong generalization in out-of-domain settings.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f46b6838ac3601fc89852/0J-cvgz2dA6bVQJJNb_Yz.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04034.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f46b6838ac3601fc89852",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647f46b6838ac3601fc89852/N5cr1MFEtgKLJ4sVAhS04.jpeg",
      "fullname": "Qing Jiang",
      "name": "Mountchicken",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03951",
      "authors": [
        {
          "_id": "68415a1cce09e3eca94e02ef",
          "user": {
            "_id": "6759546743971eff5a12a087",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/esJm_83zW1R6NqWltof8P.png",
            "isPro": false,
            "fullname": "Aojun Lu",
            "user": "Kurt1024",
            "type": "user"
          },
          "name": "Aojun Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T09:59:50.242Z",
          "hidden": false
        },
        {
          "_id": "68415a1cce09e3eca94e02f0",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T09:59:52.907Z",
          "hidden": false
        },
        {
          "_id": "68415a1cce09e3eca94e02f1",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "68415a1cce09e3eca94e02f2",
          "name": "Yanan Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T13:40:41.000Z",
      "submittedOnDailyAt": "2025-06-05T07:19:58.382Z",
      "title": "Reevaluamos el equilibrio entre la estabilidad y la flexibilidad desde la perspectiva estructural de aprendizaje continuo.",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "El problema de la aprendizaje continuo (CL) es que se propone dar a las redes neuronales un aprendizaje y una capacidad de adaptación a lo largo del tiempo. El enfoque central de CL es resolver el \"dilema de la estabilidad y la variabilidad\". Esto implica permitir al mismo tiempo la conservación de los conocimientos aprendidos y la adquisición de nuevos conocimientos. Los métodos de CL trabajan para equilibrar estos aspectos, pero a menudo se pierden las influencias que la estructura de la red tiene en la estabilidad y la variabilidad. En este artículo, se centra en el conflicto de estabilidad y variabilidad a nivel estructural. En este contexto, se observa que redes profundas muestran mejor variabilidad bajo restricciones de parámetros, mientras que redes más amplias muestran mayor estabilidad. Para resolver este dilema, se propone un nuevo marco llamado Dual-Arch. Este marco funciona como un componente pluggable de CL y utiliza las ventajas complementarias de dos redes neuronales independientes. Estas redes tienen estructuras diferentes adaptadas a sus objetivos respectivos, y los experimentos expandidos muestran que Dual-Arch mejora el rendimiento de los métodos de CL existentes y aumenta la compresión de parámetros en más del 87%.",
      "upvotes": 2,
      "discussionId": "68415a1dce09e3eca94e0314",
      "projectPage": "https://github.com/byyx666/Dual-Arch",
      "githubRepo": "https://github.com/byyx666/Dual-Arch",
      "ai_summary": "A novel framework, Dual-Arch, enhances Continual Learning by addressing the stability-plasticity dilemma at the architectural level using two specialized networks.",
      "ai_keywords": [
        "Continual Learning",
        "stability-plasticity dilemma",
        "deep networks",
        "wide networks",
        "Dual-Arch"
      ]
    },
    "publishedAt": "2025-06-04T09:40:41.000Z",
    "title": "Rethinking the Stability-Plasticity Trade-off in Continual Learning from\n  an Architectural Perspective",
    "summary": "The quest for Continual Learning (CL) seeks to empower neural networks with\nthe ability to learn and adapt incrementally. Central to this pursuit is\naddressing the stability-plasticity dilemma, which involves striking a balance\nbetween two conflicting objectives: preserving previously learned knowledge and\nacquiring new knowledge. While numerous CL methods aim to achieve this\ntrade-off, they often overlook the impact of network architecture on stability\nand plasticity, restricting the trade-off to the parameter level. In this\npaper, we delve into the conflict between stability and plasticity at the\narchitectural level. We reveal that under an equal parameter constraint, deeper\nnetworks exhibit better plasticity, while wider networks are characterized by\nsuperior stability. To address this architectural-level dilemma, we introduce a\nnovel framework denoted Dual-Arch, which serves as a plug-in component for CL.\nThis framework leverages the complementary strengths of two distinct and\nindependent networks: one dedicated to plasticity and the other to stability.\nEach network is designed with a specialized and lightweight architecture,\ntailored to its respective objective. Extensive experiments demonstrate that\nDual-Arch enhances the performance of existing CL methods while being up to 87%\nmore compact in terms of parameters.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03614",
      "authors": [
        {
          "_id": "684134ca20ff8abcccb11302",
          "name": "Zhanhui Zhou",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11303",
          "name": "Lingjie Chen",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11304",
          "name": "Chao Yang",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11305",
          "name": "Chaochao Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T06:46:06.000Z",
      "submittedOnDailyAt": "2025-06-05T04:41:50.905Z",
      "title": "VLMs pueden realizar el repaso de datos de entrenamiento distribuidos para concentrarlos.",
      "submittedOnDailyBy": {
        "_id": "642e5a7ba0b65dce1f87a7a2",
        "avatarUrl": "/avatars/3ae01c9330a47e98fac9f1eb0ba94073.svg",
        "isPro": false,
        "fullname": "Zhanhui Zhou",
        "user": "ZHZisZZ",
        "type": "user"
      },
      "summary": "Una de los métodos para reducir el riesgo de los modelos de lenguaje de visión (VLMs) es eliminar muestras peligrosas del conjunto de entrenamiento. Sin embargo, imágenes peligrosas pueden estar separadas en pequeños, \"salvajes\" patrones que se dispersen en varios ejemplos de entrenamiento, lo que facilita que estos datos no sean detectados y sean ignorados durante el modelado. Como resultado, durante la entrenamiento, VLMs pueden combinar estos patrones para generar respuestas peligrosas en el momento de la inferencia. Por ejemplo, si se combina un patrón de imágen que representa sangre y una descripción de \"seguro\", un VLM entrenado puede explicar la imagen completa o el contexto como \"seguro\".\n\nLa capacidad clave de los VLMs que permite estas amenazas es la \"estilización visual\". Esto se define como la habilidad de integrar información visual dispersa en múltiples ejemplos de entrenamiento que comparten la misma descripción textual. En nuestro estudio, demostramos la capacidad de estilización visual de VLMs en tres conjuntos de datos, al asignar un ID de síntesis único a cada imagen. Primero, se transforman los pares (imagen, ID) en pares (patrón, ID) a diferentes niveles y se entrenan los modelos. Luego, se muestra que los modelos entrenados pueden identificar correctamente el ID a través de la imagen completa o el contexto. Basándonos en esto, se simula una estrategia similar para evitar el modelado de imágenes peligrosas, reemplazando los patrón de imágenes peligrosos con descripciones de \"seguro\" o \"inseguro\", lo que permite que el contenido peligroso se evite en el modelado y posteriormente se reconstruya mediante la estilización visual, lo que pone en riesgo la seguridad de los VLMs. El código está disponible en: https://github.com/ZHZisZZ/visual-stitching.",
      "upvotes": 2,
      "discussionId": "684134cb20ff8abcccb11334",
      "githubRepo": "https://github.com/ZHZisZZ/visual-stitching",
      "ai_summary": "VLMs exhibit visual stitching, an ability to integrate fragmented visual information, which enables harmful content to evade data moderation and be reconstructed during inference.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "visual stitching",
        "data moderation",
        "adversarial data poisoning",
        "image patches",
        "textual descriptions",
        "inference"
      ]
    },
    "publishedAt": "2025-06-04T02:46:06.000Z",
    "title": "VLMs Can Aggregate Scattered Training Patches",
    "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as visual\nstitching -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each (image, ID) pair into {(patch,\nID)} pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e5a7ba0b65dce1f87a7a2",
      "avatarUrl": "/avatars/3ae01c9330a47e98fac9f1eb0ba94073.svg",
      "fullname": "Zhanhui Zhou",
      "name": "ZHZisZZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01344",
      "authors": [
        {
          "_id": "6841009bdf863485e04879c8",
          "name": "Manan Suri",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879c9",
          "user": {
            "_id": "65c16444d4c3b8dff2f0d78d",
            "avatarUrl": "/avatars/4ed764c1657bd260d2a12ba61c111062.svg",
            "isPro": false,
            "fullname": "Puneet Mathur",
            "user": "puneetm",
            "type": "user"
          },
          "name": "Puneet Mathur",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:27:41.018Z",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879ca",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cb",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:52.205Z",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cc",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cd",
          "name": "Vivek Gupta",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879ce",
          "name": "Dinesh Manocha",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T06:02:41.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:44.204Z",
      "title": "Flujo superado: Caracterización de la propiedad de flujo micro por agentes neurosinboricos",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "Los diagramas de flujo son una herramienta importante para visualizar el proceso de liquidación. Sin embargo, su estructura no lineal y las complejas relaciones visuales y gramaticales generan desafíos para la interpretación de diagramas de flujo utilizando modelos de lenguaje de alto nivel (LLMs). Esto ha llevado a una reducción de la confianza en la automatización del procesamiento de diagramas de flujo en áreas importantes como la logística, la salud y la ingeniería. Proponemos el trabajo de atribución de elementos específicos de diagramas de flujo basado en las respuestas de un LLM, llamado Fine-grained Flowchart Attribution. La atribución de diagramas de flujo garantiza la comprobabilidad de las predicciones del LLM y mejora la interpretabilidad al conectar la respuesta generada con la estructura del diagrama de flujo. Proponemos FlowPathAgent, un agente neurosymbolico que utiliza razonamiento basado en grafos para realizar la atribución post hoc de gran detalle. Este agente divide los diagramas de flujo y los transforma en grafos estructurados de símbolos, interactuando dinámicamente con el grafo y generando caminos de atribución. Además, presentamos FlowExplainBench, un nuevo benchmark para evaluar la atribución de diagramas de flujo de diferentes estilos, dominios y tipos de preguntas. Los resultados de los experimentos muestran que FlowPathAgent reduce la explicación visual de las respuestas del LLM en la consulta de diagramas de flujo y supera los baselines del 10-14% en el conjunto de datos FlowExplainBench.",
      "upvotes": 2,
      "discussionId": "6841009ddf863485e0487a38",
      "ai_summary": "FlowPathAgent, a neurosymbolic agent, enhances the reliability of LLM predictions for flowchart interpretation by tracing specific components and generating accurate attribution paths.",
      "ai_keywords": [
        "Flowcharts",
        "Fine-grained Flowchart Attribution",
        "FlowPathAgent",
        "graph-based reasoning",
        "symbolic graph",
        "neurosymbolic agent",
        "flowExplainBench",
        "flowchart QA"
      ]
    },
    "publishedAt": "2025-06-02T02:02:41.000Z",
    "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents",
    "summary": "Flowcharts are a critical tool for visualizing decision-making processes.\nHowever, their non-linear structure and complex visual-textual relationships\nmake it challenging to interpret them using LLMs, as vision-language models\nfrequently hallucinate nonexistent connections and decision paths when\nanalyzing these diagrams. This leads to compromised reliability for automated\nflowchart processing in critical domains such as logistics, health, and\nengineering. We introduce the task of Fine-grained Flowchart Attribution, which\ntraces specific components grounding a flowchart referring LLM response.\nFlowchart Attribution ensures the verifiability of LLM predictions and improves\nexplainability by linking generated responses to the flowchart's structure. We\npropose FlowPathAgent, a neurosymbolic agent that performs fine-grained post\nhoc attribution through graph-based reasoning. It first segments the flowchart,\nthen converts it into a structured symbolic graph, and then employs an agentic\napproach to dynamically interact with the graph, to generate attribution paths.\nAdditionally, we present FlowExplainBench, a novel benchmark for evaluating\nflowchart attributions across diverse styles, domains, and question types.\nExperimental results show that FlowPathAgent mitigates visual hallucinations in\nLLM answers over flowchart QA, outperforming strong baselines by 10-14% on our\nproposed FlowExplainBench dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03817",
      "authors": [
        {
          "_id": "68415ee454d7c6b3f9786deb",
          "name": "Julius Gonsior",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786dec",
          "name": "Tim Rieß",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786ded",
          "name": "Anja Reusch",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786dee",
          "name": "Claudio Hartmann",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786def",
          "name": "Maik Thiele",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786df0",
          "name": "Wolfgang Lehner",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T10:41:37.000Z",
      "submittedOnDailyAt": "2025-06-05T07:40:58.805Z",
      "title": "Investigación de parámetros en aprendizaje activo: el desafío de la gran escala en el análisis de voz",
      "submittedOnDailyBy": {
        "_id": "637638fa1f0421002b42facb",
        "avatarUrl": "/avatars/ed1e3024cd2ed7284b437db4bbeb2668.svg",
        "isPro": false,
        "fullname": "Julius Gonsior",
        "user": "jgonsior",
        "type": "user"
      },
      "summary": "La explicación de los datos requiere tiempo y costo, pero tiene una necesidad inherente para el aprendizaje automático en casa (Homeline Machine Learning). El aprendizaje activo (AL) es un método ya establecido para seleccionar continuamente muestras sin etiquetas con la mayor cantidad de información y pedir explicaciones de un experto, lo que minimiza el trabajo de etiquetado humano y mejora el rendimiento de clasificación. Aunque el AL ha sido conocido durante décadas, su uso en aplicaciones reales es aún raro. Como se muestra en dos investigaciones web del comité de NLP, dos razones principales que impiden su uso son: 1. La complejidad de la configuración del AL y 2. La falta de confianza en su eficacia. Ambas razones se atribuyen a dos problemas: un gran espacio de parámetros de configuración, que puede llevar a resultados incorrectos del AL o experimentos no reproducibles. En este estudio, se realizaron las siguientes acciones: 1. Se generaron más de 4.6 millones de combinaciones de parámetros de configuración en una gran grilla de parámetros, 2. Se registraron los rendimientos de cada combinación y 3. Se analizaron los efectos de cada parámetro de configuración en los resultados. Finalmente, se proporcionaron recomendaciones sobre el impacto de cada parámetro de configuración, mostrando que el AL puede ser influyente en la implementación de estrategias concretas, y que con un esfuerzo computacional mínimo, se puede diseñar experimentos reproducibles de AL que contribuyan a futuros estudios de AL confiables.",
      "upvotes": 1,
      "discussionId": "68415ee554d7c6b3f9786e15",
      "githubRepo": "https://github.com/jgonsior/olympic-games-of-active-learning",
      "ai_summary": "The study investigates the impact of hyperparameters on Active Learning performance, providing insights to improve its practical application and reproducibility.",
      "ai_keywords": [
        "Active Learning",
        "hyperparameter space",
        "hyperparameter grid",
        "experimental study design",
        "reproducibility",
        "trustworthiness"
      ]
    },
    "publishedAt": "2025-06-04T06:41:37.000Z",
    "title": "Survey of Active Learning Hyperparameters: Insights from a Large-Scale\n  Experimental Grid",
    "summary": "Annotating data is a time-consuming and costly task, but it is inherently\nrequired for supervised machine learning. Active Learning (AL) is an\nestablished method that minimizes human labeling effort by iteratively\nselecting the most informative unlabeled samples for expert annotation, thereby\nimproving the overall classification performance. Even though AL has been known\nfor decades, AL is still rarely used in real-world applications. As indicated\nin the two community web surveys among the NLP community about AL, two main\nreasons continue to hold practitioners back from using AL: first, the\ncomplexity of setting AL up, and second, a lack of trust in its effectiveness.\nWe hypothesize that both reasons share the same culprit: the large\nhyperparameter space of AL. This mostly unexplored hyperparameter space often\nleads to misleading and irreproducible AL experiment results. In this study, we\nfirst compiled a large hyperparameter grid of over 4.6 million hyperparameter\ncombinations, second, recorded the performance of all combinations in the\nso-far biggest conducted AL study, and third, analyzed the impact of each\nhyperparameter in the experiment results. In the end, we give recommendations\nabout the influence of each hyperparameter, demonstrate the surprising\ninfluence of the concrete AL strategy implementation, and outline an\nexperimental study design for reproducible AL experiments with minimal\ncomputational effort, thus contributing to more reproducible and trustworthy AL\nresearch in the future.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03817.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637638fa1f0421002b42facb",
      "avatarUrl": "/avatars/ed1e3024cd2ed7284b437db4bbeb2668.svg",
      "fullname": "Julius Gonsior",
      "name": "jgonsior",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03538",
      "authors": [
        {
          "_id": "6841585dd777f13c59460b47",
          "name": "Chengqi Li",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b48",
          "name": "Zhihao Shi",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b49",
          "name": "Yangdi Lu",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b4a",
          "name": "Wenbo He",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b4b",
          "user": {
            "_id": "634e60454677a5891c0902f4",
            "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
            "isPro": false,
            "fullname": "Xiangyu Xu",
            "user": "xjcvcvxj",
            "type": "user"
          },
          "name": "Xiangyu Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T08:42:10.367Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T03:40:33.000Z",
      "submittedOnDailyAt": "2025-06-05T07:13:24.079Z",
      "title": "Utilizamos el Asmium Dual 3rd Gauss Spreading para el rendering de red neuronal en estado natural estable.",
      "submittedOnDailyBy": {
        "_id": "634e60454677a5891c0902f4",
        "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
        "isPro": false,
        "fullname": "Xiangyu Xu",
        "user": "xjcvcvxj",
        "type": "user"
      },
      "summary": "3D reconstrucción enfrenta desafíos debido a condiciones de iluminación inestables y factores de dispersión instantáneos. Los métodos actuales utilizan generalmente estrategias heurísticas para procesar datos de entrenamiento de baja calidad, pero estos dificultan la generación de reconstrucciones estables y consistentes, lo que resulta en artefactos visuales. En este artículo, proponemos un nuevo marco de trabajo llamado Asymmetric Dual 3DGS, que explota la randomidad de estos artefactos. Específicamente, nuestro enfoque entrena dos modelos 3D Gaussian Splatting (3DGS) en paralelo, imponen restricciones de coherencia para promover el entrenamiento confiable y restringe artefactos inciertos. Para evitar que ambos modelos converjan a un mismo modo de fracaso, aplicamos diferentes máscaras para prevenir la rotura por desbordamiento de checkpoints con bias. Concretamente, utilizamos máscaras múltiples aplicables y una máscara ligera automática para implementar procesos de entrenamiento desbalanceados entre ambos modelos y reducir los modos de error compartidos. Además, para optimizar el entrenamiento del modelo, introducimos una versión ligera llamada Dynamic EMA Proxy, que reemplaza el promedio móvil exponencial (EMA) para actualizar dinámicamente un modelo y mantiene la diversidad mediante procesos de máscara intercambiable. A través de amplios experimentos en difíciles conjuntos de datos reales, nuestro método supera las aproximaciones actuales y alcanza altas eficiencias. Los códigos y modelos entrenados están disponibles.",
      "upvotes": 1,
      "discussionId": "68415862d777f13c59460c85",
      "ai_summary": "A novel Asymmetric Dual 3DGS framework improves 3D reconstruction by training dual models with consistency constraints and divergent masking, outperforming existing methods with high efficiency.",
      "ai_keywords": [
        "3D reconstruction",
        "3D Gaussian Splatting (3DGS)",
        "stochastic artifacts",
        "consistency constraint",
        "confirmation bias",
        "divergent masking",
        "multi-cue adaptive mask",
        "self-supervised soft mask",
        "Dynamic EMA Proxy",
        "lightweight variant",
        "Exponential Moving Average (EMA)",
        "alternating masking strategy"
      ]
    },
    "publishedAt": "2025-06-03T23:40:33.000Z",
    "title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian\n  Splatting",
    "summary": "3D reconstruction from in-the-wild images remains a challenging task due to\ninconsistent lighting conditions and transient distractors. Existing methods\ntypically rely on heuristic strategies to handle the low-quality training data,\nwhich often struggle to produce stable and consistent reconstructions,\nfrequently resulting in visual artifacts. In this work, we propose Asymmetric\nDual 3DGS, a novel framework that leverages the stochastic nature of these\nartifacts: they tend to vary across different training runs due to minor\nrandomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)\nmodels in parallel, enforcing a consistency constraint that encourages\nconvergence on reliable scene geometry while suppressing inconsistent\nartifacts. To prevent the two models from collapsing into similar failure modes\ndue to confirmation bias, we introduce a divergent masking strategy that\napplies two complementary masks: a multi-cue adaptive mask and a\nself-supervised soft mask, which leads to an asymmetric training process of the\ntwo models, reducing shared error modes. In addition, to improve the efficiency\nof model training, we introduce a lightweight variant called Dynamic EMA Proxy,\nwhich replaces one of the two models with a dynamically updated Exponential\nMoving Average (EMA) proxy, and employs an alternating masking strategy to\npreserve divergence. Extensive experiments on challenging real-world datasets\ndemonstrate that our method consistently outperforms existing approaches while\nachieving high efficiency. Codes and trained models will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e60454677a5891c0902f4",
      "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
      "fullname": "Xiangyu Xu",
      "name": "xjcvcvxj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02294",
      "authors": [
        {
          "_id": "6840cd169241913d43af9d28",
          "name": "Niclas Popp",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d29",
          "name": "Kevin Alexander Laube",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d2a",
          "name": "Matthias Hein",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d2b",
          "name": "Lukas Schott",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T22:15:59.000Z",
      "submittedOnDailyAt": "2025-06-05T00:15:23.870Z",
      "title": "La mejora del conocimiento reducido por movimientos de vecindarios desconocidos ha llevado a la guía de confianza en el procesamiento de datos",
      "submittedOnDailyBy": {
        "_id": "655646baf8a2d3c020546ec8",
        "avatarUrl": "/avatars/4ca8de82745bb5a4fda511569bb6bd94.svg",
        "isPro": false,
        "fullname": "Niclas P",
        "user": "NPBP26",
        "type": "user"
      },
      "summary": "Los modelos básicos entrenados con grandes conjuntos de datos muestran una fuerte capacidad de 0-shot en diversas áreas. Cuando el tamaño de datos y el tamaño del modelo están limitados, su éxito se puede reproducir al incorporar conocimiento desde los modelos básicos a pequeñas red neuronales estudiantes. Sin embargo, el efecto de la absorción de conocimiento está estrictamente limitado por el tamaño de datos disponibles. Este artículo aborda los problemas comunes y útiles en la absorción de conocimiento, como la transformación variacional, y destaca las características espirales que aparecen durante el entrenamiento pero no durante la prueba. Aunque no se conocen bien, se investiga si estudiantes pueden aprender con fuerza cuando se tiene un profesor fuerte. Para resolver esto, se introduce una nueva estrategia de expansión de datos basada en Dif-fusion que maximiza la separación de opiniones entre el profesor y el estudiante para generar imágenes. Los experimentos muestran que esta aproximación no es afectada por el cambio de cóbayan, aumenta significativamente la precisión en los grupos peores y promedio de mAUC en CelebA, SpuCo Birds y ImageNet Espirales, y supera a los líneas más avanzados basados en Dif-fusion de datos.",
      "upvotes": 1,
      "discussionId": "6840cd199241913d43af9dac",
      "ai_summary": "A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.",
      "ai_keywords": [
        "knowledge distillation",
        "diffusion-based data augmentation",
        "covariate shift",
        "teacher-student model",
        "CelebA",
        "SpuCo Birds",
        "spurious ImageNet",
        "mean group accuracy",
        "worst group accuracy",
        "spurious mAUC"
      ]
    },
    "publishedAt": "2025-06-02T18:15:59.000Z",
    "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation",
    "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02294.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655646baf8a2d3c020546ec8",
      "avatarUrl": "/avatars/4ca8de82745bb5a4fda511569bb6bd94.svg",
      "fullname": "Niclas P",
      "name": "NPBP26",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]