[
  {
    "paper": {
      "id": "2503.04625",
      "authors": [
        {
          "_id": "67ca670d3e81e3344dc4c2d9",
          "user": {
            "_id": "65294b334d7cf551ac50d6a6",
            "avatarUrl": "/avatars/75d21e20b711b871616ef3850bb900b7.svg",
            "isPro": false,
            "fullname": "ChengpengLi",
            "user": "ChengpengLi",
            "type": "user"
          },
          "name": "Chengpeng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:12:37.350Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2da",
          "user": {
            "_id": "5f8946925d083370c711f296",
            "avatarUrl": "/avatars/14246aae3b1f8b7ad050f8ff2c8b260e.svg",
            "isPro": false,
            "fullname": "Mingfeng Xue",
            "user": "mingfengxue",
            "type": "user"
          },
          "name": "Mingfeng Xue",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:12:28.354Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2db",
          "user": {
            "_id": "64704e973601bb7b06643e98",
            "avatarUrl": "/avatars/52e51f4d1be6769e4397b8be2799cf32.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "Zhenru",
            "type": "user"
          },
          "name": "Zhenru Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:12:48.194Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2dc",
          "user": {
            "_id": "646df403ad20c6fa4f30b7ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646df403ad20c6fa4f30b7ec/Q64-XMghOcBoo3itZDGYA.jpeg",
            "isPro": false,
            "fullname": "Jiaxi Yang",
            "user": "jx-yang",
            "type": "user"
          },
          "name": "Jiaxi Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:12:57.082Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2dd",
          "user": {
            "_id": "64b93578ee257c3a4cfceed1",
            "avatarUrl": "/avatars/e6188562254f75a09b4048b800860016.svg",
            "isPro": false,
            "fullname": "Beichen Zhang",
            "user": "BeichenZhang",
            "type": "user"
          },
          "name": "Beichen Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:13:11.641Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2de",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2df",
          "user": {
            "_id": "6583ab7983a9e1460c67d876",
            "avatarUrl": "/avatars/74400bc448c3f07e23a4cd53d68a6af7.svg",
            "isPro": false,
            "fullname": "bowen",
            "user": "bowenYu",
            "type": "user"
          },
          "name": "Bowen Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:13:30.530Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2e0",
          "user": {
            "_id": "61e4c4ca1ab24785ac11ba69",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg",
            "isPro": false,
            "fullname": "Binyuan Hui",
            "user": "huybery",
            "type": "user"
          },
          "name": "Binyuan Hui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:13:37.341Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2e1",
          "user": {
            "_id": "620760a26e3b7210c2ff1943",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
            "isPro": false,
            "fullname": "Junyang Lin",
            "user": "JustinLin610",
            "type": "user"
          },
          "name": "Junyang Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:13:44.084Z",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2e2",
          "user": {
            "_id": "6434d4989bd5a84b5dd0b0f5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg",
            "isPro": false,
            "fullname": "Dayiheng Liu",
            "user": "Losin94",
            "type": "user"
          },
          "name": "Dayiheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:13:52.711Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T17:11:51.000Z",
      "title": "Autodidacto Razonador, utilizando herramientas.",
      "summary": "Los modelos lógicos de alto nivel (LRMs) como OpenAI-o1 y DeepSeek-R1 muestran una excelente capacidad para tareas lógicas complejas utilizando largas cadenas de razonamiento (CoT). Sin embargo, estos modelos pueden depender simplemente de sus procesos lógicos internos, lo que puede llevar a conversaciones excelentes y expresiones inadecuadas. En este artículo, se presenta START (Self-Taught Reasoner with Tools), un modelo de lógica larga de cadena (LLM) que integra herramientas externas y mejora significativamente su capacidad lógica. START puede ejecutar cálculos complejos, realizar auto-verificaciones, explorar diversas metodologías y realizar auto-debugging a través de la ejecución de código, lo que resuelve limitaciones de los LRMs. La innovación clave de START es un marco de aprendizaje automático, que se compone de dos tecnologías principales: 1) Hint-infer: inserta hints artificialmente diseñados (por ejemplo, «Es una buena idea usar Python ahí») en los errores de la lógica para estimular a los LRMs a utilizar herramientas externas. Hint-infer es un método efectivo y secuencial de escalado de tiempo; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): combina Hint-infer y RFT para evaluar, filtrar, cambiar y finalmente ajustar el proceso lógico de los LRMs generados por Hint-infer. Este marco de trabajo se utilizó para fine-tunar el modelo QwQ-32B en START. En la evaluación de pregrado de ciencia (GPQA), el rendimiento en el marco de pruebas de matemáticas de nivel de competencia (AMC23, AIME24, AIME25) y en el marco de pruebas de código de nivel de competencia (LiveCodeBench) fue de 63.6%, 95.0%, 66.7%, 47.1% y 47.3% respectivamente. Estos resultados superan significativamente a los modelos base y alcanzan un rendimiento igual al de los modelos más recientes de Open Weight (R1-Distill-Qwen-32B) y de los modelos de propiedad (o1-Preview).",
      "upvotes": 36,
      "discussionId": "67ca67103e81e3344dc4c366"
    },
    "publishedAt": "2025-03-06T22:35:47.725Z",
    "title": "START: Self-taught Reasoner with Tools",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04625.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03803",
      "authors": [
        {
          "_id": "67ca874c3ac187dbbed924d6",
          "user": {
            "_id": "62b5777f593a2c49da69dc02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658152070753-62b5777f593a2c49da69dc02.jpeg",
            "isPro": false,
            "fullname": "Jingkang Yang",
            "user": "Jingkang",
            "type": "user"
          },
          "name": "Jingkang Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:32.949Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924d7",
          "name": "Shuai Liu",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924d8",
          "name": "Hongming Guo",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924d9",
          "user": {
            "_id": "652965773a416e1f2173443b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
            "isPro": false,
            "fullname": "Yuhao Dong",
            "user": "THUdyh",
            "type": "user"
          },
          "name": "Yuhao Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:15:21.671Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924da",
          "name": "Xiamengwei Zhang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924db",
          "user": {
            "_id": "63f87c42b0ae1748524a9cfb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f87c42b0ae1748524a9cfb/I5ukv6iWoJVToWmcERmvx.jpeg",
            "isPro": false,
            "fullname": "Sicheng Zhang",
            "user": "fesvhtr",
            "type": "user"
          },
          "name": "Sicheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:15:37.377Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924dc",
          "user": {
            "_id": "62f113d3b58090c873d66481",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659966415211-noauth.jpeg",
            "isPro": false,
            "fullname": "Pengyun Wang",
            "user": "Alarak",
            "type": "user"
          },
          "name": "Pengyun Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:15:44.290Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924dd",
          "user": {
            "_id": "668eb3a1a2f3f9d5edf029eb",
            "avatarUrl": "/avatars/383636e449f5e48c790f428818dd6863.svg",
            "isPro": false,
            "fullname": "zhou zitang",
            "user": "Zzitang",
            "type": "user"
          },
          "name": "Zitang Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:15:56.178Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924de",
          "user": {
            "_id": "63f886a99f87cc3e645c99a8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f886a99f87cc3e645c99a8/qwj16BrFaDjN0DPFsJ-6v.jpeg",
            "isPro": false,
            "fullname": "Binzhu Xie",
            "user": "Nicous",
            "type": "user"
          },
          "name": "Binzhu Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:16:02.570Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924df",
          "name": "Ziyue Wang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e0",
          "name": "Bei Ouyang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e1",
          "name": "Zhengyu Lin",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e2",
          "name": "Marco Cominelli",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e3",
          "user": {
            "_id": "652d06833b5997ed71ce5c46",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg",
            "isPro": false,
            "fullname": "Zhongang Cai",
            "user": "caizhongang",
            "type": "user"
          },
          "name": "Zhongang Cai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:16:33.666Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e4",
          "user": {
            "_id": "62a993d80472c0b7f94027df",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a993d80472c0b7f94027df/j5vp-IwLA2YBexylUHiQU.png",
            "isPro": false,
            "fullname": "Zhang Yuanhan",
            "user": "ZhangYuanhan",
            "type": "user"
          },
          "name": "Yuanhan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:16:45.989Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e5",
          "user": {
            "_id": "63565cc56d7fcf1bedb7d347",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
            "isPro": false,
            "fullname": "Zhang Peiyuan",
            "user": "PY007",
            "type": "user"
          },
          "name": "Peiyuan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:16:57.532Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e6",
          "user": {
            "_id": "67443675924e80c3c8807b40",
            "avatarUrl": "/avatars/fb45422391e51d2ad641f09c8535653c.svg",
            "isPro": false,
            "fullname": "fangzhou HONG",
            "user": "h12345678",
            "type": "user"
          },
          "name": "Fangzhou Hong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:17:05.845Z",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e7",
          "name": "Joerg Widmer",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e8",
          "name": "Francesco Gringoli",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e9",
          "name": "Lei Yang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924ea",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924eb",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:15:05.677Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T18:54:16.000Z",
      "title": "Esto es lo que se traduce al español:\n\n\"Esta es la vida en el hogar para ayudar\"\n\nEgoLife: \"El asistente de vida para el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"El objetivo de EgoLife para ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el hogar para ayudar\n\nEgoLife: \"Ayudar a la vida en el egomodo\"\n\nEsta es la vida en el",
      "summary": "Este proyecto presenta un asistente de vida diaria egocentrico desarrollado utilizando el egolife y el AI proberto framework. Este asistente registra continuamente las actividades diarias con cámaras, captura diferentes videos egocentricos con lentes de AI y obtiene videos que reflejan la perspectiva de terceros. En este estudio se generaron conjuntos de datos diarios de 300 horas que incluyen datos egocentricos, interpretativos, multivista y multimodelo, así como conjuntos de datos egolife. Este conjunto de datos se utiliza para presentar tarea de respuesta a consultas con contexto largo y orientadas hacia la vida diaria, así como respuestas a preguntas de egolife. Al resolver preguntas específicas, el asistente puede recordar eventos pasados, monitorear hábitos de salud y proporcionar recomendaciones personalizadas. Se presenta un sistema integrado de egobater, egoGPT y egoRAG para desarrollar, identificar y responder a preguntas de contexto largo. EgoGPT se entrena con conjuntos de datos egocentricos y alcanza el mejor rendimiento en la comprensión de videos egocentricos. EgoRAG es un componente basado en búsqueda para responder a consultas de contexto largo. Se prueban las estructuras funcionales de este sistema y se identifican los problemas críticos y los puntos de fusión. Se publican los conjuntos de datos, modelos y marcos de referencia para fomentar el desarrollo de asistentes de inteligencia artificial egocentrico.",
      "upvotes": 9,
      "discussionId": "67ca874f3ac187dbbed925cc",
      "projectPage": "https://egolife-ai.github.io/",
      "githubRepo": "https://github.com/EvolvingLMMs-Lab/EgoLife"
    },
    "publishedAt": "2025-03-07T00:44:13.546Z",
    "title": "EgoLife: Towards Egocentric Life Assistant",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03803.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b5777f593a2c49da69dc02",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658152070753-62b5777f593a2c49da69dc02.jpeg",
      "fullname": "Jingkang Yang",
      "name": "Jingkang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04598",
      "authors": [
        {
          "_id": "67ca69063a6e3e8656bcc1d2",
          "user": {
            "_id": "66335b9c95c5b79ebf306f30",
            "avatarUrl": "/avatars/d57784ee65cbef014360c9bac1ad4119.svg",
            "isPro": false,
            "fullname": "Zhijian Zhuo",
            "user": "BryceZhuo",
            "type": "user"
          },
          "name": "Zhijian Zhuo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:27:08.401Z",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d3",
          "user": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "isPro": false,
            "fullname": "Yutao Zeng",
            "user": "Taoer",
            "type": "user"
          },
          "name": "Yutao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:43.334Z",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d4",
          "name": "Ya Wang",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d5",
          "name": "Sijun Zhang",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d6",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d7",
          "user": {
            "_id": "64648638351adef1a847a7ad",
            "avatarUrl": "/avatars/7518e058fcf81ee81a06c96e996531e9.svg",
            "isPro": false,
            "fullname": "Xiaoqing Li",
            "user": "LLIXQ",
            "type": "user"
          },
          "name": "Xiaoqing Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:26:45.058Z",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d8",
          "name": "Xun Zhou",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d9",
          "user": {
            "_id": "663a684d08778abaf0556df8",
            "avatarUrl": "/avatars/d95b517df5b80a8b42bac2b171604742.svg",
            "isPro": false,
            "fullname": "Majinwen",
            "user": "Breeze0417",
            "type": "user"
          },
          "name": "Jinwen Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:27:23.639Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T16:40:48.000Z",
      "title": "HybridNorm: Estabilización, entrenamiento de Transformers antiguos y ruidosos, mediante el híbrido de normalización.",
      "summary": "Transformers están ocupando una posición fundamental como arquitectura en una amplia gama de tareas de aprendizaje automático, especialmente en modelos de lenguaje de gran escala (LLMs). Aunque superan los rendimientos, existen problemas relacionados con la ubicación de la normalización en redes Transformer profundas. La estructura Pre-Norm ofrece una ruta identidad más clara y facilita el entrenamiento. Sin embargo, su rendimiento generalmente es inferior a la estructura Post-Norm. En este artículo, se propone una estrategia de normalización híbrida, llamada HybridNorm, que integra eficazmente los beneficios de Pre-Norm y Post-Norm. En particular, HybridNorm utiliza la normalización de QKV dentro de la estructura de Attention y aplica Post-Norm en el Feed-Forward Network (FFN) de cada bloque de Transformer. Esta disección logra tanto la estabilidad del entrenamiento como el mejoramiento del rendimiento, especialmente en áreas como la generación y comprensión de texto en LLMs. Se realizan experimentos detallados en ambas arquitecturas densa y esparse, demostrando que HybridNorm obtiene resultados consistentemente excelentes al compararse con Pre-Norm y Post-Norm, y alcanza los mejores resultados en muchos benchmarks. Estos hallazgos muestran la posibilidad de HybridNorm como una forma estable y efectiva para mejorar el entrenamiento y rendimiento de modelos profundos de Transformer. El código está disponible públicamente y se puede utilizar en el repositorio https://github.com/BryceZhuo/HybridNorm.",
      "upvotes": 7,
      "discussionId": "67ca69073a6e3e8656bcc244",
      "projectPage": "https://github.com/BryceZhuo/HybridNorm",
      "githubRepo": "https://github.com/BryceZhuo/HybridNorm"
    },
    "publishedAt": "2025-03-06T23:04:06.421Z",
    "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/DB_sfuRG7M-k8w6UVTgXy.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/F0lAhIiju8M-0fKBaPATA.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/g_741Ez-YVcMK69EqCsPa.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04598.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6371128eafbe42caa5a5222b",
      "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
      "fullname": "Yutao Zeng",
      "name": "Taoer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20258",
      "authors": [
        {
          "_id": "67ca7b557436e6327ca877ff",
          "user": {
            "_id": "655efd24afee0e00788bb589",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
            "isPro": false,
            "fullname": "Amr Mohamed",
            "user": "amr-mohamed",
            "type": "user"
          },
          "name": "Amr Mohamed",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:41.298Z",
          "hidden": false
        },
        {
          "_id": "67ca7b557436e6327ca87800",
          "user": {
            "_id": "67890323f8796231c857231e",
            "avatarUrl": "/avatars/f5ccd5186968d880fee9c36324a5f713.svg",
            "isPro": false,
            "fullname": "Mingmeng Geng",
            "user": "mgeng",
            "type": "user"
          },
          "name": "Mingmeng Geng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:34:23.443Z",
          "hidden": false
        },
        {
          "_id": "67ca7b557436e6327ca87801",
          "name": "Michalis Vazirgiannis",
          "hidden": false
        },
        {
          "_id": "67ca7b557436e6327ca87802",
          "user": {
            "_id": "6087e598e2b7cc3a117b0dc5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png",
            "isPro": false,
            "fullname": "Guokan Shang",
            "user": "guokan-shang",
            "type": "user"
          },
          "name": "Guokan Shang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:38.933Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T16:46:23.000Z",
      "title": "LLM es como un bracket telefono, donde la información se deforma debido a la generación repetitiva.",
      "summary": "Al igual que los grandes modelos de lenguaje, los responsabilidades de los modelos de contenido de internet cambian de vez en cuando. Se preocupa por el impacto de procesar repetidamente su salida. Basándose en la comunicación humana continua, este estudio investiga por qué los modelos de lenguaje grande (LLM) pueden deformar la información a través de la generación repetitiva. A través de experimentos basados en traducción, se observa que la deformación acumula con el tiempo y se ve afectada por la selección del lenguaje y la complejidad de las secuencias. Aunque la deterioración no puede ser evitado, puede ser mitigado mediante técnicas estratégicas de prompting. Estos hallazgos contribuyen a la discusión sobre los efectos a largo plazo de la propagación de la información a través de la IA, y plantean importantes cuestiones sobre la confianza de los contenidos generados en flujos de trabajo repetitivos con modelos de lenguaje grande.",
      "upvotes": 6,
      "discussionId": "67ca7b577436e6327ca878ec",
      "githubRepo": "https://github.com/amr-mohamedd/LLM-as-a-Broken-Telephone"
    },
    "publishedAt": "2025-03-06T23:56:18.841Z",
    "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655efd24afee0e00788bb589",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
      "fullname": "Amr Mohamed",
      "name": "amr-mohamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04094",
      "authors": [
        {
          "_id": "67ca7bcc06501013d727a5d7",
          "user": {
            "_id": "6658e1c8ce1b2838885b2d7f",
            "avatarUrl": "/avatars/8623555f14b62f40fd372da20cb59ccc.svg",
            "isPro": false,
            "fullname": "Seth Karten",
            "user": "milkkarten",
            "type": "user"
          },
          "name": "Seth Karten",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-07T06:07:26.564Z",
          "hidden": false
        },
        {
          "_id": "67ca7bcc06501013d727a5d8",
          "name": "Andy Luu Nguyen",
          "hidden": false
        },
        {
          "_id": "67ca7bcc06501013d727a5d9",
          "user": {
            "_id": "66749c510974bbc971139f6a",
            "avatarUrl": "/avatars/bfab9d8d8bc589bb9bd49925b76e04a4.svg",
            "isPro": false,
            "fullname": "Chi Jin",
            "user": "chijin",
            "type": "user"
          },
          "name": "Chi Jin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-07T04:53:33.942Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T05:06:27.000Z",
      "title": "PokéChamp: Agente de lenguaje de Minimax a nivel de Experto",
      "summary": "PokeChamp es el modelo más pequeño de agentes mínimo-máximo que incluye un LLM. Fue creado basándose en un marco general para aplicar en el entrenamiento de dos jugadores y se ha fortalecido utilizando las capacidades generales del LLM. Concretamente, reemplaza tres módulos clave para reducir el rango de exploración basándose en la historia del juego y la conocida humana, y abordar la observación parcial. En particular, el marco no requiere adicionales entrenamientos de LLM. PokeChamp se evalúa en el formato de juego de Pokemon Showdown. Al configurar GPT-4o como botón, alcanza un 76% de victorias frente a los mejores botes basados en LLM y un 84% frente a los más potentes botes basados en reglas, demostrando excelente rendimiento. Además, al utilizar el modelo abierto de Llama 3.1 con 8 bits de parámetros, alcanza un 64% de victorias más que Poke\\'ellmon configurado con GPT-4o. PokeChamp alcanza una predicción de 1300-1500 en el laderboy de Pokemon Showdown en línea, lo que lo coloca en el tope del 30%-10% de los jugadores humanos. Además, en esta investigación se construyó la base de datos más grande de Pokémon de los jugadores reales, que incluye más de 3 millones de partidas y 500,000 de encuentros de 1300-1500. Esta base de datos se utiliza para establecer un benchmark de combate y puzzles, y para crear evaluaciones y puzzles específicos para técnicas de combate. Además, se proporcionan actualizaciones importantes para el motor de juego local. Este estudio busca fomentar la investigación que aplique tecnologías de LLM y algoritmos de teoría de juego a problemas generales multi-agente. Los vídeos, código y base de datos están disponibles en la URL proporcionada: https://sites.google.com/view/pokechamp-llm",
      "upvotes": 5,
      "discussionId": "67ca7bcd06501013d727a668",
      "projectPage": "https://sites.google.com/view/pokechamp-llm",
      "githubRepo": "https://github.com/sethkarten/pokechamp"
    },
    "publishedAt": "2025-03-06T23:53:38.838Z",
    "title": "PokéChamp: an Expert-level Minimax Language Agent",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04094.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04222",
      "authors": [
        {
          "_id": "67ca64cdd153739fa9b9dbe6",
          "user": {
            "_id": "64c9b0f28d2d187c24d1e6c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1CPnAaB3gsupdpiNWaoDc.png",
            "isPro": false,
            "fullname": "ZiYi Yang",
            "user": "AALF",
            "type": "user"
          },
          "name": "Ziyi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:46.287Z",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbe7",
          "user": {
            "_id": "62ecbffd99112e99c5f7fded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
            "isPro": false,
            "fullname": "Fanqi Wan",
            "user": "Wanfq",
            "type": "user"
          },
          "name": "Fanqi Wan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:33:34.691Z",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbe8",
          "user": {
            "_id": "62b6d20416ff90e6198301b6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656148456743-noauth.png",
            "isPro": false,
            "fullname": "Longguang Zhong",
            "user": "GGLS",
            "type": "user"
          },
          "name": "Longguang Zhong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:33:41.092Z",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbe9",
          "user": {
            "_id": "63b93e6921add32ac6190b5c",
            "avatarUrl": "/avatars/7aa6a94d48e7f7c2bc56f8734d6c4e3d.svg",
            "isPro": false,
            "fullname": "Canbin Huang",
            "user": "OnewayLab",
            "type": "user"
          },
          "name": "Canbin Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:33:47.458Z",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbea",
          "name": "Guosheng Liang",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbeb",
          "user": {
            "_id": "63b57d75bda8d44adf2ff3ff",
            "avatarUrl": "/avatars/8a387036758b2f7fc7d7529dea206669.svg",
            "isPro": false,
            "fullname": "Xiaojun Quan",
            "user": "passerqxj",
            "type": "user"
          },
          "name": "Xiaojun Quan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:34:05.521Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T09:03:36.000Z",
      "title": "FuseChat-3.0: Optimización de Preferencias y Combinación de Modelos Diferentes",
      "summary": "FuseChat-3.0 es un sistema de grandes modelos de lenguaje (LLMs) diseñado para integrar las fortalezas de diferentes fuentes de modelos LLMs. Los modelos fuente incluyen Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct y Llama-3.1-70B-Instruct. En los modelos objetivo, se centra principalmente en versiones más pequeñas utilizadas ampliamente, como Llama-3.1-8B-Instruct, Gemma-2-9B-it y Qwen-2.5-7B-Instruct. Además, se incluyen opciones de reducción como Llama-3.2-3B-Instruct y Llama-3.2-1B-Instruct. Para aprovechar las diferentes capacidades de estos modelos fuente, se desarrolló un protocolo de construcción de datos especializado para cada tarea y área. El pipeline de entrenamiento de FuseChat-3.0 se compone de dos etapas principales: 1) ajuste de la distribución entre el modelo objetivo y los modelos fuente mediante retroalimentación suave (SFT), y 2) optimización directa de preferencias (DPO) para aplicar las preferencias de varios modelos LLMs fuente y mejorar el modelo objetivo. Este modelo de FuseChat-3.0 muestra un significativo aumento en rendimiento en tareas generales, matemáticas, programación y más, según la instrucción. Según la figura 1, cuando se utiliza Llama-3.1-8B-Instruct como modelo objetivo, el aumento promedio en 14 benchmarks es de 6.8 puntos. Además, se observan mejoras sorprendentes de 37.1 puntos en el benchmark AlpacaEval-2 y de 30.1 puntos en Arena-Hard, que evalúan el comportamiento según instrucciones. Los códigos, modelos y conjuntos de datos están disponibles en https://github.com/SLIT-AI/FuseChat-3.0.",
      "upvotes": 5,
      "discussionId": "67ca64ced153739fa9b9dc1b",
      "projectPage": "https://slit-ai.github.io/FuseChat-3.0/",
      "githubRepo": "https://github.com/SLIT-AI/FuseChat-3.0"
    },
    "publishedAt": "2025-03-06T22:20:34.462Z",
    "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62ecbffd99112e99c5f7fded/nmr7w6NOioBYwMmNfezcf.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04222.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ecbffd99112e99c5f7fded",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
      "fullname": "Fanqi Wan",
      "name": "Wanfq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04130",
      "authors": [
        {
          "_id": "67ca7baf6d5c2eafede56d35",
          "user": {
            "_id": "6449e5a3df4e6cb7eaefd2b8",
            "avatarUrl": "/avatars/a671cb507d5e02b238d8cd631e71649d.svg",
            "isPro": false,
            "fullname": "Jindong Jiang",
            "user": "jdps",
            "type": "user"
          },
          "name": "Jindong Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:34:59.750Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d36",
          "user": {
            "_id": "644570ba2d91b15b4c7f6311",
            "avatarUrl": "/avatars/d5e66012066d0c330b8f23718b1499d8.svg",
            "isPro": false,
            "fullname": "Xiuyu Li",
            "user": "xiuyul",
            "type": "user"
          },
          "name": "Xiuyu Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:35:06.739Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d37",
          "user": {
            "_id": "650dac79b959b0e1d41d7378",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dac79b959b0e1d41d7378/mzbN0MFk3k8b94FQ40I7L.jpeg",
            "isPro": false,
            "fullname": "Zhijian Liu",
            "user": "zhijianliu",
            "type": "user"
          },
          "name": "Zhijian Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:35:13.834Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d38",
          "user": {
            "_id": "66d8b322cf789857d384e5c4",
            "avatarUrl": "/avatars/1276726b27d312f48e69f5ae982daa24.svg",
            "isPro": false,
            "fullname": "Muyang Li",
            "user": "MuyangLI",
            "type": "user"
          },
          "name": "Muyang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:37:06.962Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d39",
          "name": "Guo Chen",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3a",
          "user": {
            "_id": "672aae13b2f2dc21e18570e0",
            "avatarUrl": "/avatars/0253107a2116d197dc0fe18660c2af90.svg",
            "isPro": false,
            "fullname": "Zhiqi Li",
            "user": "zhiqilinv",
            "type": "user"
          },
          "name": "Zhiqi Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:36:57.283Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3b",
          "user": {
            "_id": "641d1c5ec3983aa94915c162",
            "avatarUrl": "/avatars/127985b837ecf61e43c835deee578b5e.svg",
            "isPro": false,
            "fullname": "De-An Huang",
            "user": "deahuang",
            "type": "user"
          },
          "name": "De-An Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:36:27.913Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3c",
          "user": {
            "_id": "6656eb16e50d7c40881a14f0",
            "avatarUrl": "/avatars/c6822a51c8d5918debf6ee1d25fe1825.svg",
            "isPro": false,
            "fullname": "GuilinLiu",
            "user": "GuilinLiu",
            "type": "user"
          },
          "name": "Guilin Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:36:17.904Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3d",
          "user": {
            "_id": "66c8037c737ba92ae3fe0322",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c8037c737ba92ae3fe0322/WR_Yh5DWOVVh7IFlF24NM.jpeg",
            "isPro": false,
            "fullname": "Zhiding Yu",
            "user": "Zhiding",
            "type": "user"
          },
          "name": "Zhiding Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:36:09.646Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3e",
          "user": {
            "_id": "6251bf4b183aa4266924ad91",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678041834400-6251bf4b183aa4266924ad91.jpeg",
            "isPro": true,
            "fullname": "Kurt Keutzer",
            "user": "kurtkeutzer",
            "type": "user"
          },
          "name": "Kurt Keutzer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:36:02.351Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3f",
          "user": {
            "_id": "60e1d6d3de3cd4c1bfb0c208",
            "avatarUrl": "/avatars/0bc59ede9074557f15447d2457aaf07b.svg",
            "isPro": false,
            "fullname": "Sungjin Ahn",
            "user": "sdstony",
            "type": "user"
          },
          "name": "Sungjin Ahn",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:35:54.160Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d40",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d41",
          "user": {
            "_id": "65a8b7f69aec1645994e7a15",
            "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg",
            "isPro": false,
            "fullname": "Hongxu Yin",
            "user": "yinhongxu",
            "type": "user"
          },
          "name": "Hongxu Yin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:35:40.242Z",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d42",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d43",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d44",
          "user": {
            "_id": "66bf958296583c59b049085b",
            "avatarUrl": "/avatars/04df8dd45835b7ea0991e242784e7810.svg",
            "isPro": false,
            "fullname": "Wonmin Byeon",
            "user": "wbyeon",
            "type": "user"
          },
          "name": "Wonmin Byeon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:35:32.527Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T06:17:38.000Z",
      "title": "Modelos efectivos de tokenización para la comprensión de videos largos y diferentes modelos de LLM",
      "summary": "Recientemente, el desarrollo de modelos basados en imágenes para modelos de lenguaje (Video-LLMs) ha mejorado significativamente la comprensión de imágenes procesandolas como columnas de frame de imagen. Sin embargo, los métodos actuales procesan frames de manera independiente como candidatos visuales y tienen una falta de modelización temporal explícita, lo que limita su capacidad para detectar patrones dinámicos y procesar de manera eficiente largas secuencias de videos. Para resolver estas limitaciones, presentamos STORM (Reducción de Tokenes Espaciotemporales para Multimodal LLMs). Este es un nuevo arquitectura que inserta un especializado encoder temporal entre el encoder de imágenes y el LLM. Nuestro encoder temporal utiliza el Modelo de Estado de Espacio de Mamba para integrar información temporal en los tokens de imagen y genera una representación rica que almacena patrones dinámicos entre imágenes en toda la secuencia de video. Esta rica codificación mejora la capacidad de interpretación lógica de los videos, permite efectivamente estrategias de reducción de tokens como muestreo de tiempo en tiempo de prueba y entrenamiento, y reducir significativamente la dinámica computacional del LLM. Con la integración de esta tecnología, nuestro enfoque reduce tanto el tiempo de entrenamiento y de inferencia, mejora el rendimiento y permite una comprensión eficiente y potente de videos en contextos temporales ampliados. Según evaluaciones extendidas, STORM logra los resultados más avanzados en casi todos los marcos de referencia de comprensión de largos videos como MLVU y LongVideoBench, alcanzando una reducción del costo de cálculo de hasta 8 veces y una reducción del retraso de decodificación de 2.4 a 2.9 veces, bajo un número fijo de frames de entrada. La página del proyecto se puede acceder en https://research.nvidia.com/labs/lpr/storm.",
      "upvotes": 3,
      "discussionId": "67ca7bb16d5c2eafede56df1"
    },
    "publishedAt": "2025-03-06T23:53:09.588Z",
    "title": "Token-Efficient Long Video Understanding for Multimodal LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04130.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04725",
      "authors": [
        {
          "_id": "67ca9092ba1ee2b914e3fa4a",
          "user": {
            "_id": "65e0027c960938e63e4a0157",
            "avatarUrl": "/avatars/c8ca0b082ee8e8004f47a23d9393df67.svg",
            "isPro": false,
            "fullname": "Zhuo Chen",
            "user": "zhuoc3",
            "type": "user"
          },
          "name": "Zhuo Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:31.232Z",
          "hidden": false
        },
        {
          "_id": "67ca9092ba1ee2b914e3fa4b",
          "user": {
            "_id": "66e0619714d7a7711c6fc139",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e0619714d7a7711c6fc139/IZxEQ6Iv62DDHpCxFM-QG.jpeg",
            "isPro": false,
            "fullname": "Oriol Mayné i Comas",
            "user": "oriolmayne",
            "type": "user"
          },
          "name": "Oriol Mayné i Comas",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:37:41.309Z",
          "hidden": false
        },
        {
          "_id": "67ca9092ba1ee2b914e3fa4c",
          "name": "Zhuotao Jin",
          "hidden": false
        },
        {
          "_id": "67ca9092ba1ee2b914e3fa4d",
          "name": "Di Luo",
          "hidden": false
        },
        {
          "_id": "67ca9092ba1ee2b914e3fa4e",
          "name": "Marin Soljačić",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T18:59:48.000Z",
      "title": "L^2M: Regla de escalabilidad de la información mutua en modelos de lenguaje con contexto extenso",
      "summary": "En el procesamiento del lenguaje natural, se define con precisión una metodología de escalado de múltiples Fibonacci binarios que domina la dependencia a larga distancia. Esta metodología muestra escalado independientemente, lo cual es importante para el modelado de lenguajes de largo texto. Usando este método de escalado, se define la condición de modelación de lenguajes de largo texto (L^2M) que relaciona el tamaño de la potencia de estado que almacena la información pasada con la capacidad de modelar textos largos de manera válida. Nuestros resultados se han verificado experimentalmente en modelos de Transformer y en modelos de espacio de estados. Esta investigación proporciona una base teórica para el desarrollo de largas longitudes de texto en modelos de lenguaje de gran escala.",
      "upvotes": 2,
      "discussionId": "67ca90c1ba1ee2b914e405b9",
      "projectPage": "https://github.com/LSquaredM/mutual_info_scaling_law",
      "githubRepo": "https://github.com/LSquaredM/mutual_info_scaling_law"
    },
    "publishedAt": "2025-03-07T01:42:13.847Z",
    "title": "L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e0027c960938e63e4a0157/lMxohK6cMFgsw39hn0jga.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/65e0027c960938e63e4a0157/EqSl1OwTeggMI59pVQzeR.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04725.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e0027c960938e63e4a0157",
      "avatarUrl": "/avatars/c8ca0b082ee8e8004f47a23d9393df67.svg",
      "fullname": "Zhuo Chen",
      "name": "zhuoc3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03983",
      "authors": [
        {
          "_id": "67ca66c1cb7e422997cbd148",
          "user": {
            "_id": "627a354cc488a8ce15a2dec5",
            "avatarUrl": "/avatars/0d99a2fea8b193993fe5b9b7e5b74f40.svg",
            "isPro": true,
            "fullname": "Sreyan Ghosh",
            "user": "SreyanG-NVIDIA",
            "type": "user"
          },
          "name": "Sreyan Ghosh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:40:39.324Z",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd149",
          "user": {
            "_id": "652a4dfc36f031c5e6f8b8a6",
            "avatarUrl": "/avatars/9fb56b025dc25f91ca6c31136eaf74b2.svg",
            "isPro": false,
            "fullname": "Zhifeng Kong",
            "user": "ZhifengKong",
            "type": "user"
          },
          "name": "Zhifeng Kong",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-07T03:23:47.395Z",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14a",
          "name": "Sonal Kumar",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14b",
          "name": "S Sakshi",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14c",
          "user": {
            "_id": "63fc1124a3c067e62897a73f",
            "avatarUrl": "/avatars/aa63337a7cd73181b7c1e92decf635f4.svg",
            "isPro": false,
            "fullname": "Jaehyeon Kim",
            "user": "firecomputer",
            "type": "user"
          },
          "name": "Jaehyeon Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:41:17.053Z",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14d",
          "name": "Wei Ping",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14e",
          "user": {
            "_id": "6440ddd65d600fb09518daa8",
            "avatarUrl": "/avatars/ac5898afd2082d230e2ebf6fb867ad4f.svg",
            "isPro": false,
            "fullname": "Rafael Valle",
            "user": "rafaelvalle",
            "type": "user"
          },
          "name": "Rafael Valle",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:41:08.488Z",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14f",
          "user": {
            "_id": "6537a569568d8be8fa096b8c",
            "avatarUrl": "/avatars/bfda5cb252d8b5bc3ad737d99c0d7f49.svg",
            "isPro": false,
            "fullname": "Dinesh Manocha",
            "user": "manocha",
            "type": "user"
          },
          "name": "Dinesh Manocha",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:41:01.710Z",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd150",
          "user": {
            "_id": "6311021788942700629e6247",
            "avatarUrl": "/avatars/e7adc1632b76e80e7e4a590033d1c20a.svg",
            "isPro": false,
            "fullname": "Bryan Catanzaro",
            "user": "ctnzr",
            "type": "user"
          },
          "name": "Bryan Catanzaro",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:40:55.626Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T00:10:26.000Z",
      "title": "Audio Flamingo 2: Comprensión de voz larga y habilidades de experto en lenguaje de voz de un modelo de lenguaje de voz",
      "summary": "Entender el lenguaje no verbal y la música, y buscar la razón por la cual ocurre, es un elemento crucial para que los seres humanos y las IAs agentes interactúen efectivamente con su entorno. En este artículo, se presenta Audio Flamingo 2 (AF2), un modelo de lenguaje audio reciente que incorpora comprensión de voz y capacidad para entender razones. AF2 utiliza (i) modelos de CLAP personalizados, (ii) datos de preguntas y respuestas de sonido sintético diseñados para fines de razonamiento de voz, y (iii) una estrategia de aprendizaje multinivel. AF2 logró un desempeño avanzado, incluso con un pequeño modelo de lenguaje de 3B parámetros, superando a modelos abiertos y propietarios en más de 20 marcos de referencia. A continuación, se extiende la comprensión de voz a secciones de sonido largo (de 30 segundos a 5 minutos), y se propone LongAudio, un nuevo conjunto de datos grande para la captura de sonido largo y el aprendizaje de tareas de preguntas y respuestas en AF2. El ajuste de AF2 en LongAudio demostró excelentes resultados en un marco de referencia experto anotado para evaluar la comprensión de sonido largo, LongAudioBench. Además, se realizó un estudio de dispersión y se confirmó la efectividad de la aproximación. Sitio web del proyecto: https://research.nvidia.com/labs/adlr/AF2/.",
      "upvotes": 2,
      "discussionId": "67ca66c3cb7e422997cbd178"
    },
    "publishedAt": "2025-03-07T00:12:47.515Z",
    "title": "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03983.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c9664eb34e600d7eaa4beb",
      "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
      "fullname": "Ghosh",
      "name": "Sreyan88",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04606",
      "authors": [
        {
          "_id": "67ca7b8a2a2c299d98944909",
          "name": "Aoxiong Yin",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490a",
          "name": "Kai Shen",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490b",
          "user": {
            "_id": "64a0347b528a9bbe59d6e08c",
            "avatarUrl": "/avatars/6dd0bad84d711d1048a0a4169e621773.svg",
            "isPro": false,
            "fullname": "Yichong Leng",
            "user": "ustcscallion",
            "type": "user"
          },
          "name": "Yichong Leng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:41:57.733Z",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490c",
          "name": "Xu Tan",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490d",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490e",
          "user": {
            "_id": "67bc247b593452cc18965cb1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/EA3kTYaaff0Hr7-dGiOOj.png",
            "isPro": false,
            "fullname": "JUNCHENG LI",
            "user": "JunchengLi",
            "type": "user"
          },
          "name": "Juncheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:42:06.818Z",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490f",
          "name": "Siliang Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T16:53:14.000Z",
      "title": "Los dos mejores: la integración de modelos de lenguaje y modelos de difusión en la generación de vídeos",
      "summary": "El desarrollo reciente de la generación de vídeo (T2V) está impulsado por dos paradigmas competitivos: modelos de recuperación automática de lenguaje y modelos de branch. Sin embargo, cada paradigma tiene límites inherentes: los modelos de lenguaje enfrentan dificultades con la calidad visual y la acumulación de errores, mientras que los modelos de branch son deficientes en la comprensión de significado y la modelación causal. En este artículo, proponemos LanDiff, un marco híbrido, para fusionar los fortalezas de ambos paradigmas a través de una generación de gran a pequeño. La estructura introduce tres innovaciones: (1) un tokenizador de significado que comprime eficientemente a través de una representación discreta 1D a partir de características visuales 3D, alcanzando una compresión de 14,000 veces; (2) un modelo de lenguaje que genera tokens de significado con altos niveles de relación; (3) un modelo de branch que refina significados gruesos en vídeos de alta calidad. Los experimentos con el modelo LanDiff 5B logran un puntaje de 85.43 en el benchmark VBench T2V, superando tanto el modelo abierto de código líder Fanyuan Video (13B) como modelos comerciales como Shiro, Karin y Highway. Además, alcanzan los mejores resultados en la generación de vídeos largos, superando a otros modelos abiertos de código en esta área. Se puede ver el demo en https://landiff.github.io/.",
      "upvotes": 2,
      "discussionId": "67ca7b8d2a2c299d989449a8"
    },
    "publishedAt": "2025-03-06T23:52:33.338Z",
    "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04606.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02972",
      "authors": [
        {
          "_id": "67c96a61df4d64bfebd396d4",
          "name": "Jude Khouja",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396d5",
          "name": "Karolina Korgul",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396d6",
          "name": "Simi Hellsten",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396d7",
          "name": "Lingyi Yang",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396d8",
          "name": "Vlad Neacs",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396d9",
          "name": "Harry Mayne",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396da",
          "name": "Ryan Kearns",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396db",
          "name": "Andrew Bean",
          "hidden": false
        },
        {
          "_id": "67c96a61df4d64bfebd396dc",
          "name": "Adam Mahdi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T19:57:47.000Z",
      "title": "LINGOLY-TOO: Tematización de lenguaje y ordenación de texto para separar memoria y lógica",
      "summary": "La evaluación de la capacidad de inferencia de los modelos de lenguaje grande (LLMs) está en riesgo de ser exagerada debido a la exposición de los datos de los marcos de evaluación. Hemos desarrollado un marco de evaluación de lógica lingüística difícil llamado 'LINGOLY-TOO', utilizando un marco de creación de problemas de lógica lingüística, para reducir el impacto de la memoria en la evaluación del rendimiento del modelo. Hemos desarrollado un templado para ocultar dinámicamente las notaciones de escritura real de la lengua y generar variaciones de problemas. Estas variaciones reducen la probabilidad de que un ejemplo específico de problema sea incluido en el conjunto de datos de entrenamiento del modelo, mientras mantienen las etapas de inferencia necesarias para resolver cada problema. Las experimentaciones han afectado a modelos avanzados como OpenAI o1-preview y DeepSeem R1, impulsando a los modelos a no adaptarse a la inferencia avanzada. Además, la variación de los parámetros del mismo problema ha claramente mostrado diferencias en la precisión, mientras que los problemas inherentes al modelo muestran un rendimiento promedio más bueno. Estos hallazgos revelan la incertidumbre en la generación de respuestas en los LLMs y demuestran el impacto de la exposición a datos previos en la evaluación exagerada de la capacidad de inferencia de los modelos avanzados.",
      "upvotes": 1,
      "discussionId": "67c96a62df4d64bfebd3976e"
    },
    "publishedAt": "2025-03-07T04:50:00.681Z",
    "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/671d7763572a9cfd9a6ea053/apKiu-1ILDtcrTYqiP53g.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02972.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671d7763572a9cfd9a6ea053",
      "avatarUrl": "/avatars/0a0225b50d949bb7ab0971bec531fc92.svg",
      "fullname": "Jude Khouja",
      "name": "jkhouja",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01901",
      "authors": [
        {
          "_id": "67c90398ae4b9276f2d03643",
          "user": {
            "_id": "67bf67ade43da88cdfc1348e",
            "avatarUrl": "/avatars/7bd900ade802d99db7c562ad6c2f6661.svg",
            "isPro": false,
            "fullname": "Yuezhou Hu",
            "user": "yuezhouhu",
            "type": "user"
          },
          "name": "Yuezhou Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:45:37.578Z",
          "hidden": false
        },
        {
          "_id": "67c90398ae4b9276f2d03644",
          "name": "Weiyu Huang",
          "hidden": false
        },
        {
          "_id": "67c90398ae4b9276f2d03645",
          "user": {
            "_id": "67286718746a95c09d04cb1d",
            "avatarUrl": "/avatars/317efa8459cca08c2ff56c3ab116e15c.svg",
            "isPro": false,
            "fullname": "Zichen Liang",
            "user": "zcliang22",
            "type": "user"
          },
          "name": "Zichen Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:45:50.605Z",
          "hidden": false
        },
        {
          "_id": "67c90398ae4b9276f2d03646",
          "name": "Chang Chen",
          "hidden": false
        },
        {
          "_id": "67c90398ae4b9276f2d03647",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:26:51.606Z",
          "hidden": false
        },
        {
          "_id": "67c90398ae4b9276f2d03648",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "67c90398ae4b9276f2d03649",
          "user": {
            "_id": "65fcad0ba0d7adc40b54fac2",
            "avatarUrl": "/avatars/7564b5642378fddb46ec3b5ae57c0402.svg",
            "isPro": false,
            "fullname": "Jianfei Chen",
            "user": "surfingtomchen",
            "type": "user"
          },
          "name": "Jianfei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:46:07.369Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-28T07:04:19.000Z",
      "title": "Método para categorizar la masa medida por la técnica de integración cuantitativa posterior",
      "summary": "La cuantificación de pesos posterior al entrenamiento de un LLM reduce el tamaño dentro de la memoria y resuelve problemas de costo mediante la economía de bits para acelerar. No todos los dimensiones de los pesos tienen la misma importancia. Por lo tanto, se utiliza un métrico de sensibilidad que mide la influencia de cada elemento de la función de pérdida para procesar los pesos originales de manera que sean más adecuados para cuantificación. En este estudio, se realizó una investigación experimental sobre la precisión de este métrico de sensibilidad y se descubrió que los métricas basadas en gradientes y matrices de Hessian son muy poco precisas: esto se debe a que los rangos de convergencia de la aproximación local de segundo orden son pequeños, lo que disminuye la influencia de las medidas de cuantificación en la función de pérdida. Para resolver este problema, se propone el Post-quantization Integral (PQI), que es un métrico preciso para evaluar la sensibilidad después de la cuantificación. Para utilizar este métrico preciso, se propone ReQuant, un sencillo y potente marco de trabajo. ReQuant consiste en dos componentes densos y esparsos, principalmente constituidos por la selección de desfasajes autoajustados y la separación de pesos importantes en etapas. Finalmente, ReQuant mejora el rendimiento de la cuantificación posterior, y con el uso del QTIP, se logró mejorar significativamente la perplexidad en el modelo Llama 3.2 1B, aumentándola a 2.66.",
      "upvotes": 1,
      "discussionId": "67c90399ae4b9276f2d03671"
    },
    "publishedAt": "2025-03-07T04:23:41.486Z",
    "title": "Identifying Sensitive Weights via Post-quantization Integral",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01901.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01375",
      "authors": [
        {
          "_id": "67c6bdf644c2f41804ca95c6",
          "user": {
            "_id": "64e57772b15cf1b5d017b8ee",
            "avatarUrl": "/avatars/24653ae6259a706d9d4ed63692eac5b7.svg",
            "isPro": false,
            "fullname": "Daniil Sherki",
            "user": "dsherki",
            "type": "user"
          },
          "name": "Daniil Sherki",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T09:44:52.446Z",
          "hidden": false
        },
        {
          "_id": "67c6bdf644c2f41804ca95c7",
          "user": {
            "_id": "6169a581d05945bfd8718dfa",
            "avatarUrl": "/avatars/1892ab06a7ddb557232777de3cbec470.svg",
            "isPro": false,
            "fullname": "Ivan Oseledets",
            "user": "oseledets",
            "type": "user"
          },
          "name": "Ivan Oseledets",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:46:17.295Z",
          "hidden": false
        },
        {
          "_id": "67c6bdf644c2f41804ca95c8",
          "name": "Ekaterina Muravleva",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:17:56.000Z",
      "title": "Combinando Flow Matching y Transformers para una eficiente solución del problema inverso de Bayes",
      "summary": "Resolver eficientemente problemas bayesianos es un desafío significativo debido a la complejidad de la posterior distribución y al alto costo computacional de los métodos de muestreo tradicionales. Dada una secuencia de datos observados y un modelo de proceso, el objetivo es reconstruir la distribución de parámetros basada en los datos experimentales observados. Combinando el Método de Flujo de Condición (CFM) y una arquitectura basada en transformers, demostramos que es posible muestrear eficientemente estas distribuciones de acuerdo con la cantidad de variables de los datos observados.",
      "upvotes": 1,
      "discussionId": "67c6bdf744c2f41804ca960a"
    },
    "publishedAt": "2025-03-07T02:51:01.486Z",
    "title": "Combining Flow Matching and Transformers for Efficient Solution of Bayesian Inverse Problems",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01375.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e57772b15cf1b5d017b8ee",
      "avatarUrl": "/avatars/24653ae6259a706d9d4ed63692eac5b7.svg",
      "fullname": "Daniil Sherki",
      "name": "dsherki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02191",
      "authors": [
        {
          "_id": "67ca7fe72a83a60adcb6611a",
          "user": {
            "_id": "6331c3f618711776b468e9ec",
            "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
            "isPro": false,
            "fullname": "Mia Mohammad Imran",
            "user": "imranraad",
            "type": "user"
          },
          "name": "Mia Mohammad Imran",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:36.214Z",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611b",
          "name": "Robert Zita",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611c",
          "name": "Rebekah Copeland",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611d",
          "name": "Preetha Chatterjee",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611e",
          "user": {
            "_id": "64085e1992033c150739aa74",
            "avatarUrl": "/avatars/621a5ef8aaf27d9c322c4a22c7bbcf5b.svg",
            "isPro": false,
            "fullname": "Rahat Rizvi Rahman",
            "user": "rahat-rizvi",
            "type": "user"
          },
          "name": "Rahat Rizvi Rahman",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:48:32.885Z",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611f",
          "user": {
            "_id": "64ca97e1d469fc2cf822d9f6",
            "avatarUrl": "/avatars/efec75e454ada7026e8497137de5bceb.svg",
            "isPro": false,
            "fullname": "Kostadin Damevski",
            "user": "kdamevski",
            "type": "user"
          },
          "name": "Kostadin Damevski",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:48:25.771Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:01:37.000Z",
      "title": "El entendimiento y la predicción de la pérdida de información en la convolución de códigos de taxi\n\n(Nota: La traducción se ha realizado manteniendo la profundidad y la precisión del texto original, adaptándolo al español para que sea coherente y técnicamente preciso.)",
      "summary": "Los proyectos de software se prosperan gracias a la participación y contribución de personas de diferentes orígenes. Sin embargo, expresiones únicas y interacciones negativas pueden obstaculizar la participación y el permanecimiento de los contribuidores, así como excluir a nuevos participantes. Una estrategia activa de moderación es esfuerzada para prevenir expresiones únicas que se alejen del propósito del diálogo. El objetivo de este estudio es entender y predecir cómo las expresiones únicas pueden alejarse del propósito del diálogo en GitHub.\n\nPara fomentar este estudio, hemos recopilado 202 diálogos únicos en GitHub y creado un conjunto de datos marcados con puntos de alejamiento. Este conjunto de datos incluye 696 diálogos no únicos. Basándonos en este conjunto de datos, hemos identificado características de los diálogos únicos y puntos de alejamiento, incluyendo marcadores lingüísticos, pronombres de segundo persona, palabras negativas, el ambiente de frustración y impaciencia, y patrones dinámicos de diálogo entre contribuidores y participantes externos.\n\nBasándonos en estas observaciones experimentales, proponemos una aproximación activa para detectar y responder a diálogos potencialmente dañinos. Hemos desarrollado una tecnología de seguimiento de diálogos para identificar la evolución del tema y los primeros signos de alejamiento en diálogos de GitHub, utilizando técnicas de resumen de diálogos. Nuestros experimentos han demostrado que un prompt para proporcionar resumenes de diálogos de GitHub a través de un modelo de lenguaje de máquina (LLM) alcanza un F1-score de 69% para predecir el alejamiento, mostrando un mejoramiento significativo respecto a un enfoque estándar.",
      "upvotes": 1,
      "discussionId": "67ca7fe82a83a60adcb6615b",
      "githubRepo": "https://github.com/imranraad07/derailment-oss-replication"
    },
    "publishedAt": "2025-03-07T00:11:25.116Z",
    "title": "Understanding and Predicting Derailment in Toxic Conversations on GitHub",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02191.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6331c3f618711776b468e9ec",
      "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
      "fullname": "Mia Mohammad Imran",
      "name": "imranraad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04378",
      "authors": [
        {
          "_id": "67ca637e4cb4283da8ae2979",
          "name": "Zhilin Wang",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297a",
          "name": "Jiaqi Zeng",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297b",
          "user": {
            "_id": "6556379e10428134ff235afd",
            "avatarUrl": "/avatars/ec569729870d7392e806e59a02f37d0c.svg",
            "isPro": false,
            "fullname": "Olivier Delalleau",
            "user": "odelalleau",
            "type": "user"
          },
          "name": "Olivier Delalleau",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:51:28.127Z",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297c",
          "name": "Daniel Egert",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297d",
          "name": "Ellie Evans",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297e",
          "name": "Hoo-Chang Shin",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297f",
          "name": "Felipe Soares",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae2980",
          "name": "Yi Dong",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae2981",
          "name": "Oleksii Kuchaiev",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T12:30:24.000Z",
      "title": "Feedback profesional y modelos de edición permiten escalar la inferencia y se adaptan a tareas generales en el ámbito abierto.",
      "summary": "La escalado en inferencia ha desempeñado un papel crucial en el éxito reciente de modelos como OpenAI o1 y DeepSeek R1. Sin embargo, las tecnologías utilizadas en el escalado de inferencia están limitadas a tareas donde se puede confirmar la respuesta, y se concentran en áreas como la matemática, el código y el logístico. Los humanos aprenden a pedir retroalimentación detallada a través de los primeros intentos y a mejorar basándose en esos retroalimentos, y se están esfuerzando para asegurar que reciban el mismo tipo de retroalimentación en experimentos abiertos y amplios. En consecuencia, estamos colectando y entrenando datos para modelos de retroalimentación y edición específicos para el escalado en inferencia. Nuestro sistema está diseñado de manera que un modelo inicial genere una respuesta, seguido de que un segundo modelo proporcione retroalimentación, y que un tercer modelo edite la respuesta basándose en esos retroalimentos. Mostramos que, mediante el escalado, podemos mejorar el rendimiento en el benchmark Arena Hard, mostrando una mejora en el número de respuestas iniciales, la cantidad de retroalimentación válida y la cantidad de respuestas editadas. Con el escalado óptimo, nuestro sistema basado en el modelo Llama 3 familia de 70B podría alcanzar el mejor rendimiento en Arena Hard el 5 de marzo de 2025, superando a OpenAI o1-preview-2024-09-12 (90.4) y DeepSeek R1 (92.3).",
      "upvotes": 1,
      "discussionId": "67ca63804cb4283da8ae29da"
    },
    "publishedAt": "2025-03-06T22:10:18.014Z",
    "title": "Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04644",
      "authors": [
        {
          "_id": "67ca5d2783ac16a063a56241",
          "user": {
            "_id": "64dc29d9b5d625e0e9a6ecb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
            "isPro": false,
            "fullname": "Tingyu Song",
            "user": "songtingyu",
            "type": "user"
          },
          "name": "Tingyu Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:53:24.813Z",
          "hidden": false
        },
        {
          "_id": "67ca5d2783ac16a063a56242",
          "user": {
            "_id": "65dfeee3d16fb170031df293",
            "avatarUrl": "/avatars/05e6fe0e61d4bb87536554c782385dac.svg",
            "isPro": false,
            "fullname": "gan",
            "user": "guo9",
            "type": "user"
          },
          "name": "Guo Gan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:09:52.646Z",
          "hidden": false
        },
        {
          "_id": "67ca5d2783ac16a063a56243",
          "name": "Mingsheng Shang",
          "hidden": false
        },
        {
          "_id": "67ca5d2783ac16a063a56244",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-07T09:52:59.499Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T17:32:22.000Z",
      "title": "IFIR: Marco de referencia consistente para la evaluación de cumplimiento de guías en la búsqueda de información sobre áreas profesionales",
      "summary": "IFIR es el primer benchmark específico para evaluar la búsqueda de información (IR) que sigue instrucciones en especialidades. IFIR incluye 2,426 ejemplos de alta calidad y cubre 8 conjuntos de 4 especialidades: financiero, legal, médico y ciencia. Cada conjunto recreate escenarios realistas donde las instrucciones personalizadas son importantes y abordan tareas de búsqueda propias de cada especialidad. IFIR puede analizar con detalle la capacidad de procesar instrucciones en diferentes niveles de complejidad estructural. Además, propone un nuevo método de evaluación basado en modelos de lenguaje de máquina (LLM) para ofrecer una evaluación más precisa y confiable del rendimiento de los modelos. Se realizan experimentos ampliados con 15 modelos de búsqueda líder (incluyendo modelos basados en LLM) y revelan los graves problemas que los modelos actuales tienen al seguir instrucciones complejas y propias de las especialidades. Además, se realizan análisis detallados para clarificar estas limitaciones y proporcionan consejos valiosos para futuros avances.",
      "upvotes": 0,
      "discussionId": "67ca5d2983ac16a063a562a1"
    },
    "publishedAt": "2025-03-07T04:37:52.576Z",
    "title": "IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04644.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65dfeee3d16fb170031df293",
      "avatarUrl": "/avatars/05e6fe0e61d4bb87536554c782385dac.svg",
      "fullname": "gan",
      "name": "guo9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]