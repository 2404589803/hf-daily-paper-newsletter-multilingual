[
  {
    "paper": {
      "id": "2505.03318",
      "authors": [
        {
          "_id": "681aac4fd31f567552f0cc0e",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc0f",
          "name": "Zhimin Li",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc10",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:33.194Z",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc11",
          "name": "Chunyu Wang",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc12",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc13",
          "name": "Cheng Jin",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc14",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T08:46:41.000Z",
      "submittedOnDailyAt": "2025-05-07T00:19:35.245Z",
      "title": "Ajuste micro del modelo de recompensa multimodal de cosa en aprendizaje por refuerzo",
      "submittedOnDailyBy": {
        "_id": "654c6845bac6e6e49895a5b5",
        "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
        "isPro": false,
        "fullname": "Yibin Wang",
        "user": "CodeGoat24",
        "type": "user"
      },
      "summary": "El desarrollo reciente del modelo de recompensa de Damo (RM) muestra una notable expectativa en la provisión de señales de recompensa que alineen modelos visuales y preferencias humanas. Sin embargo, actualmente, los RM están limitados por respuestas directas o procesos de razonamiento superficiales, lo que a menudo genera incertidumbre en las señales de recompensa. Proponemos que incluir una larga cadena de pensamiento (CoT) explícitamente en el proceso de recompensa puede significativamente fortalecer su confianza y robustez. Además, creemos que la precisión de las respuestas directas puede mejorarse a través de la capacidad oculta de la RM para procesar CoT. En este sentido, este artículo propone UnifiedReward-Think, el primer modelo de recompensa basado en CoT en Damo. Este modelo puede realizar una consideración profunda de múltiples niveles en dos dominios: comprensión visual y generación de recompensa. Específicamente, adoptamos un enfoque de ajuste micro de aprendizaje reforzado para desarrollar la capacidad de pensamiento complejo del modelo y para estimular la generación de recompensa: (1) Primero, utilizamos datos de preferencia para la generación de pocas imágenes para entrenar el proceso de pensamiento de GPT-4o, utilizando esto como inicio de congelamiento para aprender la forma y estructura del CoT. (2) Luego, aprovechamos el conocimiento previo y la capacidad de generalización del modelo para preparar grandes conjuntos de datos de preferencia unificada en Damo, desarrollando el proceso de pensamiento del modelo en una amplia gama de tareas visuales. En este paso, mantenemos la precisión de la consideración y entrenamos el modelo utilizando técnicas de muestreo de rechazo. (3) Por otro lado, las predicciones inciertas son exploradas a través de un ajuste micro de aprendizaje reforzado basado en la política de grupo y optimización de GRPO, permitiendo explorar diversos pasos de pensamiento y optimizar decisiones precisas y robustas. Los experimentos ampliados en la tarea de recompensa visual demuestran la excelencia del modelo propuesto en este artículo.",
      "upvotes": 50,
      "discussionId": "681aac50d31f567552f0cc5d",
      "projectPage": "https://codegoat24.github.io/UnifiedReward/think",
      "githubRepo": "https://github.com/CodeGoat24/UnifiedReward",
      "ai_keywords": [
        "Multimodal Reward Models (RMs)",
        "long chains of thought (CoT)",
        "UnifiedReward-Think",
        "exploration-driven reinforcement fine-tuning",
        "small amount of image generation preference data",
        "GPT-4o",
        "large-scale unified multimodal preference data",
        "rejection sampling",
        "Group Relative Policy Optimization (GRPO)",
        "reinforcement fine-tuning"
      ]
    },
    "publishedAt": "2025-05-06T04:46:41.000Z",
    "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning",
    "summary": "Recent advances in multimodal Reward Models (RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallow reasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the reward reasoning process can significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt an\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill the reasoning process of GPT-4o,\nwhich is then used for the model's cold start to learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model's reasoning process across various vision\ntasks. During this phase, correct reasoning outputs are retained for rejection\nsampling to refine the model (3) while incorrect predicted samples are finally\nused for Group Relative Policy Optimization (GRPO) based reinforcement\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\nfor correct and robust solutions. Extensive experiments across various vision\nreward tasks demonstrate the superiority of our model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03318.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654c6845bac6e6e49895a5b5",
      "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
      "fullname": "Yibin Wang",
      "name": "CodeGoat24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03335",
      "authors": [
        {
          "_id": "681ab9b8f43603c60cab87a3",
          "name": "Andrew Zhao",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a4",
          "user": {
            "_id": "647eb24118274bce0308b2b8",
            "avatarUrl": "/avatars/463e49a89b61164ccfad85ced10658b2.svg",
            "isPro": false,
            "fullname": "Yiran Wu",
            "user": "kevinwyr",
            "type": "user"
          },
          "name": "Yiran Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:25.939Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a5",
          "user": {
            "_id": "649d475111592b1a765ac1a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
            "isPro": false,
            "fullname": "Yang Yue",
            "user": "Yang130",
            "type": "user"
          },
          "name": "Yang Yue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:23.436Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a6",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a7",
          "name": "Quentin Xu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a8",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a9",
          "name": "Matthieu Lin",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87aa",
          "user": {
            "_id": "6486dde1f74857df3f1a5828",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
            "isPro": false,
            "fullname": "Shenzhi Wang",
            "user": "shenzhi-wang",
            "type": "user"
          },
          "name": "Shenzhi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:28.623Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ab",
          "name": "Qingyun Wu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ac",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-07T01:39:06.052Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ad",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T09:08:00.000Z",
      "submittedOnDailyAt": "2025-05-07T00:10:48.130Z",
      "title": "「Inferencia de juegos de entrenamiento de aprendizaje reforzado basado en datos 0」",
      "submittedOnDailyBy": {
        "_id": "630482fbce6b12280b18971d",
        "avatarUrl": "/avatars/b07f31fd970d736bdf574d56da7a5634.svg",
        "isPro": false,
        "fullname": "Andrew Zhao",
        "user": "andrewzh",
        "type": "user"
      },
      "summary": "El aprendizaje por refuerzo basado en recompensas posibles (RLVR) demuestra que se puede mejorar la capacidad lógica de grandes modelos de lenguaje mediante recompensas basadas en resultados. Las recientes investigaciones en RLVR funcionan en un 0 de configuración, evitando así la etiquetado de procesos lógicos, pero aún dependen de la recopilación de preguntas y respuestas de calidad para el conjunto de datos de prueba. La escasez de ejemplos de alta calidad creada por humanos lleva a preocupaciones sobre la dependencia a largo plazo de humanos para la subetiquetación. Además, si el futuro del IA supera el inteligencia humana, la tarea que proporcionan los humanos podría tener un potencial de aprendizaje limitado para sistemas de alto nivel de inteligencia. Para enfrentar estas preocupaciones, proponemos que un modelo aprenda a maximizar su propio progreso en una tarea, lo que nos lleva a proponer un nuevo paradigma RLVR llamado 'Absolute Zero', con el objetivo de mejorar la capacidad lógica. En este paradigma, validamos la tarea lógica propuesta utilizando un código de verificación de códigos y confirmamos las respuestas, presentando así un sistema 'Absolute Zero Reasoner (AZR)' que mejora su capacidad lógica y su curso de entrenamiento a través de su propio desarrollo. AZR no depende completamente de datos externos, pero alcanza los mejores resultados en tareas lógicas de código y matemáticas, superando a los modelos de 0 configuración actuales. Además, muestra su aplicabilidad en diferentes escalas de modelos y clases de modelos, demostrando su efectividad en su aplicación.",
      "upvotes": 43,
      "discussionId": "681ab9baf43603c60cab881a",
      "projectPage": "https://andrewzh112.github.io/absolute-zero-reasoner/",
      "githubRepo": "https://github.com/LeapLabTHU/Absolute-Zero-Reasoner",
      "ai_keywords": [
        "Reinforcement learning with verifiable rewards (RLVR)",
        "zero setting",
        "outcome-based rewards",
        "manually curated collections",
        "superintelligent system",
        "Absolute Zero",
        "AzR (Absolute Zero Reasoner)",
        "training curriculum",
        "code executor",
        "verifiable reward",
        "open-ended yet grounded learning",
        "SOTA performance",
        "coding and mathematical reasoning tasks"
      ]
    },
    "publishedAt": "2025-05-06T05:08:00.000Z",
    "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03335.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630482fbce6b12280b18971d",
      "avatarUrl": "/avatars/b07f31fd970d736bdf574d56da7a5634.svg",
      "fullname": "Andrew Zhao",
      "name": "andrewzh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03730",
      "authors": [
        {
          "_id": "681abf3759155282c1cb2306",
          "name": "Shiyi Zhang",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2307",
          "user": {
            "_id": "64970d3d9c3b29dca8633f87",
            "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
            "isPro": false,
            "fullname": "JunhaoZhuang",
            "user": "JunhaoZhuang",
            "type": "user"
          },
          "name": "Junhao Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:58.214Z",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2308",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2309",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb230a",
          "name": "Yansong Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:58:02.000Z",
      "submittedOnDailyAt": "2025-05-07T00:37:06.442Z",
      "title": "FlexiAct: Control de acciones volátiles en escenarios heterogéneos",
      "submittedOnDailyBy": {
        "_id": "6315d306a9456afe2b9bf34a",
        "avatarUrl": "/avatars/7285b4e7d84b528d1a50f8ee4eb10727.svg",
        "isPro": false,
        "fullname": "ElevenZ",
        "user": "shiyi0408",
        "type": "user"
      },
      "summary": "La acción de personalización es el proceso de generar videos que ejecutan acciones indicadas por señales de control de entrada, utilizando guías de personalización orientadas o movimientos globales de personalización. Actualmente, los métodos disponibles sufren de limitaciones debido a la estricta restricción de la estructura espectral (por ejemplo, consistencia de la luz, escala, y perspectiva), lo que reduce su capacidad de adaptación a diversos temas y escenarios. Para superar estos limites, proponemos FlexiAct. FlexiAct tiene como objetivo transferir acciones desde un video de referencia a una imagen de destino, permitiendo variaciones en la estructura espectral entre el tema del video de referencia y la imagen de destino, mientras mantiene la consistencia de las acciones. Para lograr esto, es necesario un control preciso de las acciones, adaptación espectral, y mantener la consistencia de las acciones. Para ello, introducimos RefAdapter, que se centra en la adaptación espectral y la consistencia, superando los métodos actuales para lograr un equilibrio entre acuerdo y flexibilidad estructural. Además, de nuestros observatorios, hemos concluido que el proceso de denoising requiere atención diferente a movimientos lentos (bajas frecuencias) y detalles aperturas (altas frecuencias) a cada paso temporal. Por lo tanto, proponemos FAE (Extracción de Acciones con Conocimiento de Frecuencia). FAE realiza la extracción de acciones directamente durante el proceso de denoising, sin utilizar una arquitectura espectral-temporal separada. Los experimentos demuestran que nuestro método efectivamente transferirá acciones a diversos temas con diferentes consistencias de luz, escala, y perspectiva. Publicamos nuestro código y pesos del modelo para apoyar futuras investigaciones. https://shiyi-zh0408.github.io/projectpages/FlexiAct/",
      "upvotes": 17,
      "discussionId": "681abf3859155282c1cb23fa",
      "projectPage": "https://shiyi-zh0408.github.io/projectpages/FlexiAct/",
      "githubRepo": "https://github.com/shiyi-zh0408/FlexiAct",
      "ai_keywords": [
        "FlexiAct",
        "RefAdapter",
        "image-conditioned adapter",
        "spatial adaptation",
        "consistency preservation",
        "FAE",
        "Frequency-aware Action Extraction",
        "denoising process",
        "spatial-temporal architectures",
        "action extraction"
      ]
    },
    "publishedAt": "2025-05-06T13:58:02.000Z",
    "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios",
    "summary": "Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03730.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6315d306a9456afe2b9bf34a",
      "avatarUrl": "/avatars/7285b4e7d84b528d1a50f8ee4eb10727.svg",
      "fullname": "ElevenZ",
      "name": "shiyi0408",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02922",
      "authors": [
        {
          "_id": "681abdbef8feaef23b543877",
          "name": "Yaoqi Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543878",
          "name": "Jinkai Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543879",
          "user": {
            "_id": "667135bdcca06def1c2599a6",
            "avatarUrl": "/avatars/d94ab99265e1970852605d344b4d69e9.svg",
            "isPro": false,
            "fullname": "Baotong Lu",
            "user": "baotonglu",
            "type": "user"
          },
          "name": "Baotong Lu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-07T01:56:16.240Z",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387a",
          "user": {
            "_id": "66e96f58de9ebeee86f5e27f",
            "avatarUrl": "/avatars/e7d692aa47f02f1858186f47186166ad.svg",
            "isPro": false,
            "fullname": "Qianxi Zhang",
            "user": "qianxizhang",
            "type": "user"
          },
          "name": "Qianxi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:00.341Z",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387b",
          "name": "Chengruidong Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387c",
          "name": "Jingjia Luo",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387d",
          "name": "Di Liu",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387e",
          "name": "Huiqiang Jiang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387f",
          "name": "Qi Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543880",
          "name": "Jing Liu",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543881",
          "name": "Bailu Ding",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543882",
          "name": "Xiao Yan",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543883",
          "name": "Jiawei Jiang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543884",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543885",
          "name": "Mingxing Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543886",
          "name": "Yuqing Yang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543887",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543888",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T18:01:17.000Z",
      "submittedOnDailyAt": "2025-05-07T00:36:48.274Z",
      "title": "RetroInfer: Método de almacenamiento de vectores para la inferencia de LLM con largo contexto escalable",
      "submittedOnDailyBy": {
        "_id": "6278bd42541f3d2dfa77ea70",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
        "isPro": true,
        "fullname": "Huiqiang Jiang",
        "user": "iofu728",
        "type": "user"
      },
      "summary": "El crecimiento de la longitud de contexto de los modelos de lenguaje grande (LLMs) genera problemas graves para la inferencia eficiente debido a las limitaciones de memoria y ancho de banda de GPU. Presentamos un nuevo sistema llamado RetroInfer, que utiliza un caché de claves-valores (KV) basado en la rara atención para acelerar la inferencia de LLMs de largo contexto. El esencial de RetroInfer es el índice de onda. El índice de onda utiliza métodos como aproximaciones de atención, estimaciones de atención con limitaciones de precisión y clustering separado para facilitar la búsqueda eficiente y precisa de tokens importantes. Combinado con el buffer de onda, RetroInfer coordina la colocación del caché de KV, paraleliza la computación y la transmisión de datos entre GPU y CPU, manteniendo altos rendimientos. A diferencia de otros métodos basados en raridad, RetroInfer evita los problemas de selección de tokens y adaptación al hardware, ofreciendo un rendimiento fuerte sin sacrificar la precisión del modelo. Los experimentos en marcos de prueba de largo contexto muestran que RetroInfer logra un aumento de velocidad del 4.5 veces respecto a la atención completa, dentro de las limitaciones de memoria de GPU, y un aumento de velocidad del 10.5 veces al extender el caché de KV en memoria de CPU, manteniendo la precisión de la atención completa.",
      "upvotes": 16,
      "discussionId": "681abdc0f8feaef23b543926",
      "ai_keywords": [
        "RetrInfer",
        "key-value (KV) cache",
        "vector storage system",
        "wave index",
        "Attention-aWare VEctor index",
        "tripartite attention approximation",
        "accuracy-bounded attention estimation",
        "segmented clustering",
        "wave buffer",
        "GPU memory",
        "token selection",
        "hardware coordination"
      ]
    },
    "publishedAt": "2025-05-05T14:01:17.000Z",
    "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
    "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02922.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6278bd42541f3d2dfa77ea70",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
      "fullname": "Huiqiang Jiang",
      "name": "iofu728",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03005",
      "authors": [
        {
          "_id": "681ac8a09f4ed2ece10fac18",
          "user": {
            "_id": "647f4bac45baf21ad709fcd0",
            "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
            "isPro": false,
            "fullname": "Dan Goldstein",
            "user": "SmerkyG",
            "type": "user"
          },
          "name": "Daniel Goldstein",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:53.268Z",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac19",
          "name": "Eric Alcaide",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac1a",
          "name": "Janna Lu",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac1b",
          "name": "Eugene Cheah",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/sfcDfWZfka9sy8siQC6MV.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/yXcUOgY48NS_nfIZN0Kqq.png"
      ],
      "publishedAt": "2025-05-05T20:03:28.000Z",
      "submittedOnDailyAt": "2025-05-07T01:19:36.031Z",
      "title": "RADLADS: Estilo de atención rápida de atracción lineal en el escenario",
      "submittedOnDailyBy": {
        "_id": "647f4bac45baf21ad709fcd0",
        "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
        "isPro": false,
        "fullname": "Dan Goldstein",
        "user": "SmerkyG",
        "type": "user"
      },
      "summary": "Introducing RADLADS (Scale에서 빠른 어텐션 분절을 렐러어 어텐션 디코더로), an extension of the Laplacian Affine Transformation to the 렐러어 Attention Decoder. This protocol transforms a Softmax Attention Transformer into a 렐러어 Attention Decoder model, incorporating two new RWKV-variant architectures and specialized Qwen2.5 open-source models (7B, 32B, 72B sizes). The transformation process requires approximately 350-700M tokens, which is about 0.005% of the original teacher model's token count. Currently, the cost to convert a 72B 렐러어 Attention model is under $2,000, with inference quality nearly identical to the original Transformer. These models achieve state-of-the-art downstream performance on standard benchmarks for 렐러어 Attention model sizes. All models are released on HuggingFace under the Apache 2.0 license, with the 72B model subject to the same limitations under the Qwen License Agreement.\n\nThe models are publicly available at:\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\n\nThe training code is available at:\nhttps://github.com/recursal/RADLADS-paper",
      "upvotes": 14,
      "discussionId": "681ac8a19f4ed2ece10fac75",
      "ai_keywords": [
        "Rapid Attention Distillation",
        "RADLADS",
        "softmax attention transformers",
        "linear attention decoder models",
        "RWKV-variant",
        "Qwen2.5",
        "token count",
        "linear attention models",
        "HuggingFace",
        "Apache 2.0 license",
        "Qwen License Agreement"
      ]
    },
    "publishedAt": "2025-05-05T16:03:28.000Z",
    "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale",
    "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/sfcDfWZfka9sy8siQC6MV.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/yXcUOgY48NS_nfIZN0Kqq.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03005.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f4bac45baf21ad709fcd0",
      "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
      "fullname": "Dan Goldstein",
      "name": "SmerkyG",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02872",
      "authors": [
        {
          "_id": "681b03a33bf166767107c74b",
          "name": "Cfir Avraham Hadar",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74c",
          "name": "Omer Shubi",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74d",
          "name": "Yoav Meiri",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74e",
          "name": "Yevgeni Berzak",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T13:23:48.000Z",
      "submittedOnDailyAt": "2025-05-07T05:24:49.077Z",
      "title": "\"El descubrimiento de la finalidad de la exploración de información abierta a partir del movimiento ocular en el proceso de leer Roading\"",
      "submittedOnDailyBy": {
        "_id": "60ef001bed64a34082bfa0dd",
        "avatarUrl": "/avatars/78e4daeac169edbf4dc42fbed9b50d59.svg",
        "isPro": false,
        "fullname": "Omer Shubi",
        "user": "scaperex",
        "type": "user"
      },
      "summary": "Durante el proceso de lectura, a menudo se encuentra interés por información específica. Por ejemplo, cuando se está leyendo un artículo y se está interesado en modelos de LLMs (Large Language Models), en el diseño de experimentos, o en la pregunta \"¿Está efectivo?\", entre otros. En una amplia gama de contextos, incluyendo la vida diaria, los objetivos únicos de un texto guian las acciones de lectura. En este estudio, se investiga por primera vez si se puede interpretar automáticamente objetivos de lectura abiertos a partir de movimientos de ojos durante la lectura. Para resolver este problema, se propone una clasificación de objetivos y una tarea de configuración de objetivos, junto con un marco de evaluación. Se utiliza una gran cantidad de datos de seguimiento de ojos que incluyen tareas de exploración de información únicas de texto en inglés. Se desarrollan y comparan separadamente y generativamente diversos LLMs para la clasificación y configuración de objetivos. Estos experimentos muestran significativos éxitos en ambas tareas y demuestran que los LLMs pueden extraer información útil sobre los objetivos únicos de un texto de lectores a partir de movimientos oculares.",
      "upvotes": 11,
      "discussionId": "681b03a43bf166767107c787",
      "ai_keywords": [
        "goal classification",
        "goal reconstruction",
        "discriminative models",
        "generative models",
        "multimodal LLMs",
        "large-scale eye tracking",
        "text-specific information seeking"
      ]
    },
    "publishedAt": "2025-05-04T09:23:48.000Z",
    "title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in\n  Reading",
    "summary": "When reading, we often have specific information that interests us in a text.\nFor example, you might be reading this paper because you are curious about LLMs\nfor eye movements in reading, the experimental design, or perhaps you only care\nabout the question ``but does it work?''. More broadly, in daily life, people\napproach texts with any number of text-specific goals that guide their reading\nbehavior. In this work, we ask, for the first time, whether open-ended reading\ngoals can be automatically decoded from eye movements in reading. To address\nthis question, we introduce goal classification and goal reconstruction tasks\nand evaluation frameworks, and use large-scale eye tracking for reading data in\nEnglish with hundreds of text-specific information seeking tasks. We develop\nand compare several discriminative and generative multimodal LLMs that combine\neye movements and text for goal classification and goal reconstruction. Our\nexperiments show considerable success on both tasks, suggesting that LLMs can\nextract valuable information about the readers' text-specific goals from eye\nmovements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02872.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60ef001bed64a34082bfa0dd",
      "avatarUrl": "/avatars/78e4daeac169edbf4dc42fbed9b50d59.svg",
      "fullname": "Omer Shubi",
      "name": "scaperex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02214",
      "authors": [
        {
          "_id": "6819905519b420a2f91aa231",
          "user": {
            "_id": "64612660933afb0106a9dee3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
            "isPro": false,
            "fullname": "Xingyu Zheng",
            "user": "Xingyu-Zheng",
            "type": "user"
          },
          "name": "Xingyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:43.859Z",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa232",
          "name": "Yuye Li",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa233",
          "user": {
            "_id": "68164883244ee586cb159268",
            "avatarUrl": "/avatars/856ca8731ca156f616529e427d8fd76a.svg",
            "isPro": false,
            "fullname": "初浩然",
            "user": "HaoranChu",
            "type": "user"
          },
          "name": "Haoran Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:41.155Z",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa234",
          "name": "Yue Feng",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa235",
          "name": "Xudong Ma",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa236",
          "name": "Jie Luo",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa237",
          "name": "Jinyang Guo",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa238",
          "name": "Haotong Qin",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa239",
          "name": "Michele Magno",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa23a",
          "name": "Xianglong Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64612660933afb0106a9dee3/uQV5mzdTqoNatTz5fk6xC.png"
      ],
      "publishedAt": "2025-05-04T18:43:44.000Z",
      "submittedOnDailyAt": "2025-05-07T02:59:22.920Z",
      "title": "Investigación experimental de reducción de Qwen3",
      "submittedOnDailyBy": {
        "_id": "64612660933afb0106a9dee3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
        "isPro": false,
        "fullname": "Xingyu Zheng",
        "user": "Xingyu-Zheng",
        "type": "user"
      },
      "summary": "La serie Qwen ha emergido como una familia líder de grandes modelos de lenguaje natural (LLMs) abiertos, demostrando excelentes capacidades en tareas de comprensión de lenguaje natural. En la última actualización de Qwen3, se ha mostrado un desempeño superior en varios benchmarks, y ha aumentado la interés por procesar estos modelos de manera eficiente en entornos con limitaciones de recursos. La cuantificación a bajas prestaciones se ha propuesto como una solución prometedora, pero su impacto en el rendimiento de Qwen3 aún no ha sido estudiado en detalle. En este estudio, se evalua sistemáticamente la robustez de Qwen3 en diferentes configuraciones de cuantificación, y se busca revelar las oportunidades y desafíos asociados con la compresión de este modelo líder. Se evaluaron rigurosamente Qwen3 con cinco tecnologías clásicas de cuantificación post-entrenamiento, utilizando un rango de 1 a 8 bits de precisión, y se analizaron varios conjuntos de datos para evaluar su efecto. Nuestros resultados muestran que Qwen3 mantiene un rendimiento competitivo en prestaciones de precisión media, pero presenta un descenso notable en tareas de lenguaje natural a baja precisión, revelando problemas persistentes en la compresión de modelos de lenguaje natural. Estos resultados subrayan la necesidad de investigación para reducir la pérdida de rendimiento en escenarios de cuantificación extrema. Esperamos que este análisis experimental contribuya a desarrollar métodos de cuantificación adecuados para Qwen3 y futuros LLMs. Este proyecto ha sido publicado en https://github.com/Efficient-ML/Qwen3-Quantization y en https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.",
      "upvotes": 7,
      "discussionId": "6819905519b420a2f91aa27c",
      "githubRepo": "https://github.com/Efficient-ML/Qwen3-Quantization",
      "ai_keywords": [
        "Low-bit quantization",
        "Post-training quantization techniques",
        "Bit-widths",
        "Linguistic tasks",
        "LLM compression",
        "Quantization methods"
      ]
    },
    "publishedAt": "2025-05-04T14:43:44.000Z",
    "title": "An Empirical Study of Qwen3 Quantization",
    "summary": "The Qwen series has emerged as a leading family of open-source Large Language\nModels (LLMs), demonstrating remarkable capabilities in natural language\nunderstanding tasks. With the recent release of Qwen3, which exhibits superior\nperformance across diverse benchmarks, there is growing interest in deploying\nthese models efficiently in resource-constrained environments. Low-bit\nquantization presents a promising solution, yet its impact on Qwen3's\nperformance remains underexplored. This study conducts a systematic evaluation\nof Qwen3's robustness under various quantization settings, aiming to uncover\nboth opportunities and challenges in compressing this state-of-the-art model.\nWe rigorously assess 5 existing classic post-training quantization techniques\napplied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their\neffectiveness across multiple datasets. Our findings reveal that while Qwen3\nmaintains competitive performance at moderate bit-widths, it experiences\nnotable degradation in linguistic tasks under ultra-low precision, underscoring\nthe persistent hurdles in LLM compression. These results emphasize the need for\nfurther research to mitigate performance loss in extreme quantization\nscenarios. We anticipate that this empirical analysis will provide actionable\ninsights for advancing quantization methods tailored to Qwen3 and future LLMs,\nultimately enhancing their practicality without compromising accuracy. Our\nproject is released on https://github.com/Efficient-ML/Qwen3-Quantization and\nhttps://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64612660933afb0106a9dee3/uQV5mzdTqoNatTz5fk6xC.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02214.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64612660933afb0106a9dee3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
      "fullname": "Xingyu Zheng",
      "name": "Xingyu-Zheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03735",
      "authors": [
        {
          "_id": "681acb9c285636e830d37c8e",
          "user": {
            "_id": "6625ce8074ae2df4e3effa92",
            "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
            "isPro": false,
            "fullname": "Jiayuan Rao 饶珈源",
            "user": "Homie0609",
            "type": "user"
          },
          "name": "Jiayuan Rao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:50.372Z",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c8f",
          "name": "Zifeng Li",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c90",
          "user": {
            "_id": "632c7a0d1d303f5f9acf01b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
            "isPro": false,
            "fullname": "Haoning Wu",
            "user": "haoningwu",
            "type": "user"
          },
          "name": "Haoning Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-07T02:56:09.106Z",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c91",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c92",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c93",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:59:31.000Z",
      "submittedOnDailyAt": "2025-05-07T01:26:13.756Z",
      "title": "Un sistema de agentes para una comprensión general del fútbol",
      "submittedOnDailyBy": {
        "_id": "6625ce8074ae2df4e3effa92",
        "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
        "isPro": false,
        "fullname": "Jiayuan Rao 饶珈源",
        "user": "Homie0609",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo en el campo de la comprensión del fútbol dirigido por la IA ha estado avanzando rápidamente, pero los actuales estudios se centran principalmente en elementos separados y en tareas muy específicas. Para cerrar esta brecha, se propone un estricto marco para entender el fútbol en su totalidad. Concretamente, este artículo realiza las siguientes contribuciones: (i) se construye SoccerWiki, que es el primer grande base de conocimientos multimodelo en el fútbol, integrando conocimientos ricos de áreas como jugadores, equipos, árbitros y fanáticos, y permitiendo inferencias basadas en conocimiento. (ii) se presenta SoccerBench, el más estricto marco de referencia en el fútbol, que proporciona aproximadamente 10K pares de problemas de selección múltiple de diferentes tareas de comprensión del fútbol, evaluados por autocorrección y revisión manual. (iii) se introduce SoccerAgent, un nuevo sistema de agentes multiagentes, que divide problemas complejos del fútbol mediante inferencia colaborativa y utiliza conocimientos específicos de áreas en SoccerWiki para alcanzar altos rendimientos. (iv) se realizan evaluaciones muy amplias y pruebas de eliminación, benchmarkeando los MLLMs más avanzados en SoccerBench y claramente demostrando la superioridad del propuesto sistema de agentes. Todos los datos y códigos están disponibles en la URL siguiente: https://jyrao.github.io/SoccerAgent/",
      "upvotes": 5,
      "discussionId": "681acb9e285636e830d37d4b",
      "projectPage": "https://jyrao.github.io/SoccerAgent/",
      "githubRepo": "https://github.com/jyrao/SoccerAgent",
      "ai_keywords": [
        "multimodal",
        "knowledge base",
        "domain knowledge",
        "multimodal (text, image, video) multi-choice QA pairs",
        "multi-agent system",
        "collaborative reasoning",
        "state-of-the-art MLLMs"
      ]
    },
    "publishedAt": "2025-05-06T13:59:31.000Z",
    "title": "Multi-Agent System for Comprehensive Soccer Understanding",
    "summary": "Recent advancements in AI-driven soccer understanding have demonstrated rapid\nprogress, yet existing research predominantly focuses on isolated or narrow\ntasks. To bridge this gap, we propose a comprehensive framework for holistic\nsoccer understanding. Specifically, we make the following contributions in this\npaper: (i) we construct SoccerWiki, the first large-scale multimodal soccer\nknowledge base, integrating rich domain knowledge about players, teams,\nreferees, and venues to enable knowledge-driven reasoning; (ii) we present\nSoccerBench, the largest and most comprehensive soccer-specific benchmark,\nfeaturing around 10K standardized multimodal (text, image, video) multi-choice\nQA pairs across 13 distinct understanding tasks, curated through automated\npipelines and manual verification; (iii) we introduce SoccerAgent, a novel\nmulti-agent system that decomposes complex soccer questions via collaborative\nreasoning, leveraging domain expertise from SoccerWiki and achieving robust\nperformance; (iv) extensive evaluations and ablations that benchmark\nstate-of-the-art MLLMs on SoccerBench, highlighting the superiority of our\nproposed agentic system. All data and code are publicly available at:\nhttps://jyrao.github.io/SoccerAgent/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6625ce8074ae2df4e3effa92",
      "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
      "fullname": "Jiayuan Rao 饶珈源",
      "name": "Homie0609",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03368",
      "authors": [
        {
          "_id": "681b05ab8e325b4097318969",
          "user": {
            "_id": "661e841972c7030f3fd57eb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661e841972c7030f3fd57eb9/PysszxrOl__sZ-IToZfJU.jpeg",
            "isPro": false,
            "fullname": "Stef De Sabbata",
            "user": "sdesabbata",
            "type": "user"
          },
          "name": "Stef De Sabbata",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:24.829Z",
          "hidden": false
        },
        {
          "_id": "681b05ab8e325b409731896a",
          "name": "Stefano Mizzaro",
          "hidden": false
        },
        {
          "_id": "681b05ab8e325b409731896b",
          "name": "Kevin Roitero",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T09:40:06.000Z",
      "submittedOnDailyAt": "2025-05-07T05:33:25.973Z",
      "title": "El Giòspecrum Modelo de Explicación Mecánica de Lenguajes de Gran Escala",
      "submittedOnDailyBy": {
        "_id": "620cca6f06a4320dbf3b50d8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654011487582-620cca6f06a4320dbf3b50d8.png",
        "isPro": false,
        "fullname": "Kevin Roitero",
        "user": "kevinr",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) muestran capacidades sin precedentes en diversas tareas de procesamiento de lenguaje natural. Su habilidad para procesar y generar texto y código es utilizada ampliamente en varias áreas, pero su uso como base de conocimiento y herramienta de 'ronroneo' continúa siendo un campo de investigación. La literatura en el campo de la geología está aumentando en la evaluación de los conocimientos geográficos y capacidades espaciales de los LLMs, pero aun no existe una conciencia específica sobre cómo estos modelos procesan datos geográficos.\n\nEn esta sección, se construye un nuevo marco para investigar la posibilidad de explicar la estructura espacial de la Tierra. Este marco utiliza análisis espacial para realizar un análisis inverso de cómo los LLMs procesan datos geográficos. Nuestro objetivo es entender las representaciones internas generadas por los modelos cuando procesan datos geográficos. Esto permitirá considerar cómo estos modelos piensan sobre los datos geográficos, pero sin exagerar la antropización.\n\nPrimero, se explica la forma para descubrir la estructura interna de los LLMs. Luego, se discuten los papeles de la espectral diodidón y el autoencoder escaso en el campo de la explicación estructural. Estos modelos son analizados para descubrir sus representaciones internas múltiples y para decomonerlas en características únicas y explicables. En los experimentos, se utiliza la autocorrelación espacial para demostrar que las características asociadas a nombres geográficos están relacionadas con patrones espaciales, proporcionando una comprensión más profunda de cómo los modelos procesan datos geográficos. Finalmente, se discuten cómo nuestro marco puede ayudar a la investigación y el uso de modelos básicos en la geología.",
      "upvotes": 3,
      "discussionId": "681b05ad8e325b4097318a3d",
      "ai_keywords": [
        "probing",
        "mechanistic interpretability",
        "superposition hypothesis",
        "sparse autoencoders",
        "polysemantic",
        "monosemantic",
        "spatial autocorrelation",
        "placenames",
        "geospatial patterns"
      ]
    },
    "publishedAt": "2025-05-06T05:40:06.000Z",
    "title": "Geospatial Mechanistic Interpretability of Large Language Models",
    "summary": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03368.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620cca6f06a4320dbf3b50d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654011487582-620cca6f06a4320dbf3b50d8.png",
      "fullname": "Kevin Roitero",
      "name": "kevinr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03164",
      "authors": [
        {
          "_id": "681ac5d2626f5a368b2a7103",
          "name": "Ji Won Chung",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7104",
          "name": "Tongyu Zhou",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7105",
          "name": "Ivy Chen",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7106",
          "name": "Kevin Hsu",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7107",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7108",
          "name": "Alexa Siu",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7109",
          "name": "Shunan Guo",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710a",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:55.782Z",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710b",
          "name": "James Tompkin",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710c",
          "name": "Jeff Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T04:18:42.000Z",
      "submittedOnDailyAt": "2025-05-07T01:00:46.546Z",
      "title": "InfoVids: Realiza un reevaluación del experiencia de video de los usuarios y utiliza las relaciones entre diferentes métodos de visualización y presentación para realizar una reestructuración.",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "La representación tradicional de datos separa la presentación y la visualización en dos espacios diferentes: el mundo tridimensional y la pantalla bidimensional, lo que fortalece la narrativa de los diseñadores de visualizaciones. Para crear una experiencia humana para los espectadores, utilizamos InfoVids para conectar más equitativamente la presentación y la visualización. Estos videográficos de información se han diseñado para reestablecer la relación entre la presentación y la visualización. Durante el proceso de diseño de InfoVids, investigamos cómo la experiencia del espectador cambia con el uso de ruedas, fuerzas y interacciones. Con 30 participantes y 9 puntos de referencia, proporcionamos una presentación práctica y útil en comparación con los slides bidimensionales estándar. El análisis de métodos mixtos ha demostrado que este paradigma reduce la disminución de la atención del espectador, cambia la concentración de la visualización hacia la presentación, y logra una experiencia interactiva, natural y agradable de datos. Finalmente, InfoVids reevalúa la tradicional motivación entre la presentación y la visualización para los espectadores.",
      "upvotes": 3,
      "discussionId": "681ac5d4626f5a368b2a71f8"
    },
    "publishedAt": "2025-05-06T00:18:42.000Z",
    "title": "InfoVids: Reimagining the Viewer Experience with Alternative\n  Visualization-Presenter Relationships",
    "summary": "Traditional data presentations typically separate the presenter and\nvisualization into two separate spaces--the 3D world and a 2D screen--enforcing\nvisualization-centric stories. To create a more human-centric viewing\nexperience, we establish a more equitable relationship between the\nvisualization and the presenter through our InfoVids. These\ninfographics-inspired informational videos are crafted to redefine\nrelationships between the presenter and visualizations. As we design InfoVids,\nwe explore how the use of layout, form, and interactions affects the viewer\nexperience. We compare InfoVids against their baseline 2D `slides' equivalents\nacross 9 metrics with 30 participants and provide practical, long-term insights\nfrom an autobiographical perspective. Our mixed methods analyses reveal that\nthis paradigm reduced viewer attention splitting, shifted the focus from the\nvisualization to the presenter, and led to more interactive, natural, and\nengaging full-body data performances for viewers. Ultimately, InfoVids helped\nviewers re-imagine traditional dynamics between the presenter and\nvisualizations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03164.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03739",
      "authors": [
        {
          "_id": "681b232d7167935b1e979e64",
          "name": "Zuwei Long",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e65",
          "name": "Yunhang Shen",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e66",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e67",
          "name": "Heting Gao",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e68",
          "name": "Lijiang Li",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e69",
          "name": "Peixian Chen",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6a",
          "name": "Mengdan Zhang",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6b",
          "name": "Hang Shao",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6c",
          "name": "Jian Li",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6d",
          "name": "Jinlong Peng",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6e",
          "name": "Haoyu Cao",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6f",
          "name": "Ke Li",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e70",
          "name": "Rongrong Ji",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e71",
          "name": "Xing Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:59:53.000Z",
      "submittedOnDailyAt": "2025-05-07T08:07:03.177Z",
      "title": "VITA-Audio: Eficiente generación de tokens de modelos de lenguaje y voz en escalas grandes rápidas",
      "submittedOnDailyBy": {
        "_id": "6483143902f98c3f05aff915",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6483143902f98c3f05aff915/ZhWFFgrlRsQf4MXiInh5p.jpeg",
        "isPro": true,
        "fullname": "沈云航 Yunhang Shen",
        "user": "shenyunhang",
        "type": "user"
      },
      "summary": "El aumento de la necesidad de interacción natural entre humanos y computadoras ha llevado a que los sistemas basados en voz reciban más atención. Estos sistemas utilizan el habla, la forma más común de comunicación diaria, para interactuar. Sin embargo, los modelos de voz actuales presentan altos latencias al generar los primeros tokens de voz en el proceso de streaming, lo que puede causar un desgaste significativo. Para resolver estos problemas, proponemos VITA-Audio, un modelo de voz de gran escala desde el dispositivo hasta el dispositivo. En particular, introducimos un módulo ligero de Predicción de Tokens Multimodal (MCTP) para generar eficientemente múltiples tokens de voz en un solo paso de propagación del modelo, lo que acelera la velocidad de inferencia y reduce significativamente el latencia de la generación de la primera voz en escenarios de streaming. Además, revisamos una estrategia de entrenamiento progresiva en cuatro etapas para acelerar el modelo sin perder la calidad de la voz. Según nuestra experiencia, VITA-Audio es el primer modelo de voz de gran escala que puede generar salidas de voz en el proceso de propagación del modelo y permite comunicación en tiempo real con mínimo latencia. VITA-Audio es un modelo completamente reproducible y su dataset es de código abierto. Los resultados de los experimentos muestran que nuestro modelo logra un aumento de velocidad de inferencia de 3 a 5 veces en un tamaño de 7B parámetros, superando significativamente a otros modelos de código abierto de similar escala en pruebas como Reconocimiento de Lenguaje Automático (ASR), Generación de Contexto (TTS) y Respuesta a Preguntas Interactivas (SQA).",
      "upvotes": 2,
      "discussionId": "681b232e7167935b1e979ec6",
      "ai_keywords": [
        "end-to-end large speech model",
        "Multiple Cross-modal Token Prediction (MCTP)",
        "model forward pass",
        "inference",
        "streaming scenarios",
        "four-stage progressive training strategy",
        "multi-modal large language model",
        "real-time conversational capabilities",
        "inference speedup",
        "automatic speech recognition (ASR)",
        "text-to-speech (TTS)",
        "spoken question answering (SQA)"
      ]
    },
    "publishedAt": "2025-05-06T13:59:53.000Z",
    "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model",
    "summary": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03739.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6483143902f98c3f05aff915",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6483143902f98c3f05aff915/ZhWFFgrlRsQf4MXiInh5p.jpeg",
      "fullname": "沈云航 Yunhang Shen",
      "name": "shenyunhang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02311",
      "authors": [
        {
          "_id": "681a06459c99f4b5ce5f2838",
          "user": {
            "_id": "658e85bb5b7553ca5c29ba89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
            "isPro": false,
            "fullname": "Jihao Zhao",
            "user": "Robot2050",
            "type": "user"
          },
          "name": "Jihao Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T16:51:29.234Z",
          "hidden": false
        },
        {
          "_id": "681a06459c99f4b5ce5f2839",
          "name": "Chunlai Zhou",
          "hidden": false
        },
        {
          "_id": "681a06459c99f4b5ce5f283a",
          "name": "Biao Qin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T01:45:56.000Z",
      "submittedOnDailyAt": "2025-05-07T00:06:14.265Z",
      "title": "El Interfaz de Invocación es llamada según sea necesario: Resolución de problemas mediante la invocación adaptativa de modelos de lenguaje de gran escala",
      "submittedOnDailyBy": {
        "_id": "658e85bb5b7553ca5c29ba89",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
        "isPro": false,
        "fullname": "Jihao Zhao",
        "user": "Robot2050",
        "type": "user"
      },
      "summary": "El paradigma de colaboración entre grandes y pequeños modelos de lenguaje (LM) puede equilibrar efectivamente el balance entre rendimiento y costo, pero es crucial identificar precisamente el momento en que ocurren los ruidos en los pequeños LM. Los esfuerzos de optimización previos han centrado principalmente en métodos de post-procesamiento, que separan el proceso de explicación del LM y asocian altos costos computacionales y efectos limitados. En este artículo, proponemos una medida de evaluación práctica para calcular la acumulación y propagación de ruidos en el proceso de generación de pequeños LM, llamada \"AttenHScore\". Este indicador permite expandir continuamente los errores potenciales de explicación, lo que permitiría a grandes LM realizar mejores llamadas en tiempo real. Además, consideramos las limitaciones de la capacidad de explicación de pequeños LM y utilizamos la reconstrucción de conocimiento para tratar incertidumbres, lo que permite extraer información más efectivamente de diferentes textos de chatbots. Los experimentos expandidos muestran que \"AttenHScore\" mejora significativamente la capacidad de detección de ruidos en tiempo real en varios conjuntos de datos de preguntas y respuestas, especialmente en casos de preguntas complejas. Además, la estrategia de este artículo elimina la necesidad de entrenamiento adicional de modelos y demuestra su adaptabilidad a diferentes modelos de lenguaje basados en Transformer.",
      "upvotes": 2,
      "discussionId": "681a06469c99f4b5ce5f28a3",
      "ai_keywords": [
        "large language models (LMs)",
        "small language models (LMs)",
        "hallucinations",
        "post-processing techniques",
        "AttenHScore",
        "generation process",
        "reasoning errors",
        "dynamic adjustment",
        "real-time invocation",
        "reasoning capacity",
        "uncertainty-aware knowledge reorganization",
        "QA datasets",
        "complex queries",
        "transformer-based LMs"
      ]
    },
    "publishedAt": "2025-05-04T21:45:56.000Z",
    "title": "Invoke Interfaces Only When Needed: Adaptive Invocation for Large\n  Language Models in Question Answering",
    "summary": "The collaborative paradigm of large and small language models (LMs)\neffectively balances performance and cost, yet its pivotal challenge lies in\nprecisely pinpointing the moment of invocation when hallucinations arise in\nsmall LMs. Previous optimization efforts primarily focused on post-processing\ntechniques, which were separate from the reasoning process of LMs, resulting in\nhigh computational costs and limited effectiveness. In this paper, we propose a\npractical invocation evaluation metric called AttenHScore, which calculates the\naccumulation and propagation of hallucinations during the generation process of\nsmall LMs, continuously amplifying potential reasoning errors. By dynamically\nadjusting the detection threshold, we achieve more accurate real-time\ninvocation of large LMs. Additionally, considering the limited reasoning\ncapacity of small LMs, we leverage uncertainty-aware knowledge reorganization\nto assist them better capture critical information from different text chunks.\nExtensive experiments reveal that our AttenHScore outperforms most baseline in\nenhancing real-time hallucination detection capabilities across multiple QA\ndatasets, especially when addressing complex queries. Moreover, our strategies\neliminate the need for additional model training and display flexibility in\nadapting to various transformer-based LMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02311.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658e85bb5b7553ca5c29ba89",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
      "fullname": "Jihao Zhao",
      "name": "Robot2050",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.21650",
      "authors": [
        {
          "_id": "681b258f23dbf6b59c5fc735",
          "name": "Haiyang Zhou",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc736",
          "user": {
            "_id": "63f095be6309c84d5f48848a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f095be6309c84d5f48848a/pL2CKi-r-0mMfIGhYSAsm.jpeg",
            "isPro": false,
            "fullname": "Wangbo Yu",
            "user": "Drexubery",
            "type": "user"
          },
          "name": "Wangbo Yu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-07T09:19:13.250Z",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc737",
          "name": "Jiawen Guan",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc738",
          "name": "Xinhua Cheng",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc739",
          "name": "Yonghong Tian",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc73a",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T13:55:28.000Z",
      "submittedOnDailyAt": "2025-05-07T07:52:52.459Z",
      "title": "HoloTiempo: Usando un modelo de retroalimentación, se logra generar una escena panorámica 4D para el aprendizaje profundo en 2D.",
      "submittedOnDailyBy": {
        "_id": "66eeda3676a8038cb448f11d",
        "avatarUrl": "/avatars/8d6e61f4c9354c6720ccaa7be0fe1d9f.svg",
        "isPro": false,
        "fullname": "Haiyang Zhou",
        "user": "Marblueocean",
        "type": "user"
      },
      "summary": "El rápido desarrollo de los modelos de difusión muestra la posibilidad innovadora de aplicación de las tecnologías VR y AR. Generalmente, estas tecnologías requieren de 4D assets en nivel de escena para mejorar la experiencia del usuario. Sin embargo, los modelos de difusión actuales principalmente modelan escenas 3D estáticas o acciones a nivel de objeto, lo que limita la posibilidad de proporcionar una experiencia satisfactoria. Para resolver estos problemas, proponemos HoloTime. Este marco integra modelos de difusión de video para combinar la reconstrucción de escenas 360° 4D a partir de un prompt o imagen de referencia, transformando el video 360° en un 4D asset que permita a los usuarios experimentar completamente en 4D. Específicamente, para aplicar modelos de difusión de video en la generación de videos 360°, presentamos el dataset 360World. Este es el primer conjunto detallado de videos 360° diseñado para la reconstrucción de escenas 4D. Basándonos en este conjunto, proponemos Panoramic Animator, que utiliza modelos de difusión de video en dos etapas para convertir altas calidades de videos 360° en videos 360°. Además, presentamos Panoramic Space-Time Reconstruction, que utiliza métodos de estimación de profundidad en espacio-tiempo para convertir videos 360° en clusters de puntos 4D, facilitando la reconstrucción de escenas 4D en tiempo. Evaluamos el efecto de nuestro método comparativamente con los métodos actuales, demostrando su superioridad en la generación de videos 360° y la reconstrucción de escenas 4D. Esto demuestra que nuestro método, capacidad de mantener la interés del usuario y crear ambientes satisfactorios en tiempo real, mejora la experiencia de usuario en aplicaciones VR y AR.",
      "upvotes": 1,
      "discussionId": "681b259123dbf6b59c5fc7cf",
      "projectPage": "https://zhouhyocean.github.io/holotime/",
      "githubRepo": "https://github.com/PKU-YuanGroup/HoloTime",
      "ai_keywords": [
        "diffusion models",
        "HoloTime",
        "video diffusion models",
        "panoramic videos",
        "360-degree 4D scene reconstruction",
        "360World dataset",
        "Panoramic Animator",
        "image-to-video diffusion model",
        "Panoramic Space-Time Reconstruction",
        "space-time depth estimation",
        "4D point clouds",
        "Gaussian Splatting",
        "4D scenes",
        "spatially and temporally consistent"
      ]
    },
    "publishedAt": "2025-04-30T09:55:28.000Z",
    "title": "HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene\n  Generation",
    "summary": "The rapid advancement of diffusion models holds the promise of\nrevolutionizing the application of VR and AR technologies, which typically\nrequire scene-level 4D assets for user experience. Nonetheless, existing\ndiffusion models predominantly concentrate on modeling static 3D scenes or\nobject-level dynamics, constraining their capacity to provide truly immersive\nexperiences. To address this issue, we propose HoloTime, a framework that\nintegrates video diffusion models to generate panoramic videos from a single\nprompt or reference image, along with a 360-degree 4D scene reconstruction\nmethod that seamlessly transforms the generated panoramic video into 4D assets,\nenabling a fully immersive 4D experience for users. Specifically, to tame video\ndiffusion models for generating high-fidelity panoramic videos, we introduce\nthe 360World dataset, the first comprehensive collection of panoramic videos\nsuitable for downstream 4D scene reconstruction tasks. With this curated\ndataset, we propose Panoramic Animator, a two-stage image-to-video diffusion\nmodel that can convert panoramic images into high-quality panoramic videos.\nFollowing this, we present Panoramic Space-Time Reconstruction, which leverages\na space-time depth estimation method to transform the generated panoramic\nvideos into 4D point clouds, enabling the optimization of a holistic 4D\nGaussian Splatting representation to reconstruct spatially and temporally\nconsistent 4D scenes. To validate the efficacy of our method, we conducted a\ncomparative analysis with existing approaches, revealing its superiority in\nboth panoramic video generation and 4D scene reconstruction. This demonstrates\nour method's capability to create more engaging and realistic immersive\nenvironments, thereby enhancing user experiences in VR and AR applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21650.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66eeda3676a8038cb448f11d",
      "avatarUrl": "/avatars/8d6e61f4c9354c6720ccaa7be0fe1d9f.svg",
      "fullname": "Haiyang Zhou",
      "name": "Marblueocean",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.18373",
      "authors": [
        {
          "_id": "681af7a58f41e99d8ca05074",
          "user": {
            "_id": "6454d2ee1a543cf97b1ba671",
            "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
            "isPro": false,
            "fullname": "shen",
            "user": "lorashen",
            "type": "user"
          },
          "name": "Lei Shen",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-07T06:09:12.039Z",
          "hidden": false
        },
        {
          "_id": "681af7a58f41e99d8ca05075",
          "name": "Xiaoyu Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-25T14:17:47.000Z",
      "submittedOnDailyAt": "2025-05-07T04:49:41.738Z",
      "title": "Auto-SLURP: marco de referencia de conjunto de datos de evaluación para un framework de agente multiagente inteligente y privado",
      "submittedOnDailyBy": {
        "_id": "6454d2ee1a543cf97b1ba671",
        "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
        "isPro": false,
        "fullname": "shen",
        "user": "lorashen",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de marcos de trabajo multi-agente basados en modelos de lenguaje grandes (LLMs) ha avanzado de manera rápida. En este proceso, en particular, ha sido insuficiente la disponibilidad de conjuntos de datos de referencia para evaluar el rendimiento de estos marcos de trabajo multi-agente basados en LLMs. Para remediar esta deficiencia, presentamos \"Auto-SLURP\", un conjunto de datos de referencia para evaluar el rendimiento de marcos de trabajo multi-agente basados en LLMs, desarrollado en el contexto de asistentes personales inteligentes. Auto-SLURP extiende el conjunto de datos original SLURP, que se desarrolló inicialmente, a través de la reasignación de etiquetas de datos y la integración con servidores de computación y servicios externos. En esta extensión, se implementó un proceso de evaluación end-to-end coherente para evaluar la comprensión del lenguaje, la ejecución de tareas y la generación de respuestas. Nuestros experimentos representan los desafíos más importantes para los marcos de trabajo multi-agente líder en el momento, y claramente indican que un asistente personal inteligente multi-agente que se pueda confiar en es todavía en desarrollo. El conjunto de datos y el código relacionado están disponibles en https://github.com/lorashen/Auto-SLURP/.",
      "upvotes": 0,
      "discussionId": "681af7a68f41e99d8ca050ba",
      "githubRepo": "https://github.com/lorashen/Auto-SLURP/",
      "ai_keywords": [
        "multi-agent frameworks",
        "large language models (LLMs)",
        "benchmark datasets",
        "natural language understanding tasks",
        "simulated servers",
        "external services",
        "end-to-end evaluation pipeline",
        "language understanding",
        "task execution",
        "response generation"
      ]
    },
    "publishedAt": "2025-04-25T10:17:47.000Z",
    "title": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant",
    "summary": "In recent years, multi-agent frameworks powered by large language models\n(LLMs) have advanced rapidly. Despite this progress, there is still a notable\nabsence of benchmark datasets specifically tailored to evaluate their\nperformance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset\naimed at evaluating LLM-based multi-agent frameworks in the context of\nintelligent personal assistants. Auto-SLURP extends the original SLURP dataset\n-- initially developed for natural language understanding tasks -- by\nrelabeling the data and integrating simulated servers and external services.\nThis enhancement enables a comprehensive end-to-end evaluation pipeline,\ncovering language understanding, task execution, and response generation. Our\nexperiments demonstrate that Auto-SLURP presents a significant challenge for\ncurrent state-of-the-art frameworks, highlighting that truly reliable and\nintelligent multi-agent personal assistants remain a work in progress. The\ndataset and related code are available at\nhttps://github.com/lorashen/Auto-SLURP/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18373.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6454d2ee1a543cf97b1ba671",
      "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
      "fullname": "shen",
      "name": "lorashen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]