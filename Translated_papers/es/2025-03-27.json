[
  {
    "paper": {
      "id": "2503.19757",
      "authors": [
        {
          "_id": "67e3e1e20706b07bfb2713d6",
          "user": {
            "_id": "643fa1c318afbc4d1f3e5e59",
            "avatarUrl": "/avatars/f8f35355902b4cda72e9c6d768322fae.svg",
            "isPro": false,
            "fullname": "Zhi Hou",
            "user": "zhihou",
            "type": "user"
          },
          "name": "Zhi Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:43:49.995Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713d7",
          "user": {
            "_id": "64c9e86a6a26cddbecd9bae2",
            "avatarUrl": "/avatars/61a84989dbbc1898ebcba3236dbed039.svg",
            "isPro": false,
            "fullname": "Tianyi Zhang",
            "user": "TianyiZhang0213",
            "type": "user"
          },
          "name": "Tianyi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:44:21.118Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713d8",
          "name": "Yuwen Xiong",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713d9",
          "user": {
            "_id": "66ab30dfd456f0408b93f27b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ab30dfd456f0408b93f27b/nps4Kni_eOExO5Z92RiiF.jpeg",
            "isPro": false,
            "fullname": "Haonan Duan",
            "user": "robot-haonan",
            "type": "user"
          },
          "name": "Haonan Duan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:04:53.498Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713da",
          "user": {
            "_id": "648a1e44fe11ebd7489c289c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WwMcD9PK0gxIu2I0n0QyD.jpeg",
            "isPro": false,
            "fullname": "Hengjun Pu",
            "user": "MIASANMIA",
            "type": "user"
          },
          "name": "Hengjun Pu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:01.725Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713db",
          "user": {
            "_id": "66b9a5bb32be421cd8538cd6",
            "avatarUrl": "/avatars/a1f7c0fe3ed4741017db713b4e6d47c8.svg",
            "isPro": false,
            "fullname": "Ronglei Tong",
            "user": "TTTTTony",
            "type": "user"
          },
          "name": "Ronglei Tong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:07.673Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713dc",
          "user": {
            "_id": "679165b9c7f527ef3619504e",
            "avatarUrl": "/avatars/f3e6ce5fc3d05c8632d8b208f55c2987.svg",
            "isPro": false,
            "fullname": "Chengyang Zhao",
            "user": "chengyzhao",
            "type": "user"
          },
          "name": "Chengyang Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:13.906Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713dd",
          "user": {
            "_id": "64ae2359179421d320b1694b",
            "avatarUrl": "/avatars/c387a75191005bcaa473091de5383a10.svg",
            "isPro": false,
            "fullname": "Xizhou Zhu",
            "user": "Einsiedler",
            "type": "user"
          },
          "name": "Xizhou Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:20.614Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713de",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713df",
          "user": {
            "_id": "64686f7172d9180d4ac8b4e4",
            "avatarUrl": "/avatars/db67dd6c4b2b41054ddcce5a18ade6f8.svg",
            "isPro": false,
            "fullname": "Jifeng Dai",
            "user": "daijifeng",
            "type": "user"
          },
          "name": "Jifeng Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:27.039Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713e0",
          "user": {
            "_id": "632dab84fdb35759ea6646a0",
            "avatarUrl": "/avatars/857b0b4d115aa5ab2f143e60b0e4edc6.svg",
            "isPro": false,
            "fullname": "Yuntao Chen",
            "user": "YuntaoChen",
            "type": "user"
          },
          "name": "Yuntao Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:40.200Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643fa1c318afbc4d1f3e5e59/T0twf6ibM7Htfq525gf3L.mp4"
      ],
      "publishedAt": "2025-03-25T15:19:56.000Z",
      "submittedOnDailyAt": "2025-03-27T01:27:55.746Z",
      "title": "Dita: Ampliación de la política de acción de lenguaje de visión generalista de la Transición Paisa",
      "submittedOnDailyBy": {
        "_id": "643fa1c318afbc4d1f3e5e59",
        "avatarUrl": "/avatars/f8f35355902b4cda72e9c6d768322fae.svg",
        "isPro": false,
        "fullname": "Zhi Hou",
        "user": "zhihou",
        "type": "user"
      },
      "summary": "Recientemente, modelos de lenguaje visuolingüístico de acción entrenados con diferentes conjuntos de datos de robots han demostrado capacidades de generalización en áreas limitadas, pero no han podido adaptarse a otros espacios de acciones utilizando cabezas de acción simplificadas para predecir acciones discretas o continuas. En este contexto, se presenta Dita, un marco escalable que utiliza la arquitectura Transformer para disenar secuencias de acciones continuas directamente. A diferencia de métodos anteriores, Dita no basa su diseno en la combinación de red de bajo ancho con embajadas, sino que realiza el diseno condicionado por el contexto y permite una correspondencia subtil entre tokens visuos simples y acciones disenadas a partir de observaciones históricas. Este diseño permite modelar claramente las diferencias entre acciones y los detalles subtiles del entorno. Al combinar la escalabilidad de los Transformers, Dita integra de manera indirecta conjuntos de datos de diferentes máquinas que incluyen variedades en ángulos de captura, simulaciones de observación, tareas y espacios de acciones, lo que fortalece la capacidad de respuesta a la diversidad y promueve la ejecución exitosa de tareas a largo plazo. Las evaluaciones en diferentes marcos de referencia muestran performances avanzadas en simulaciones y resultados relativamente buenos en entornos reales. En particular, Dita realiza ajustes de 10 ejemplos, utilizando entradas de cámaras tercera persona, y aplica esto a cambios en el entorno y a tareas complejas y a largo plazo. Esta arquitectura ofrece una base funcional, ligera y abierta de código para el aprendizaje de políticas generales de robots. Página del proyecto: https://robodita.github.io.",
      "upvotes": 32,
      "discussionId": "67e3e1e40706b07bfb2714cd",
      "projectPage": "https://robodita.github.io",
      "githubRepo": "https://github.com/RoboDita/Dita",
      "ai_keywords": [
        "Transformer architectures",
        "multimodal diffusion process",
        "in-context conditioning",
        "action deltas",
        "environmental nuances",
        "cross-embodiment datasets",
        "long-horizon tasks",
        "10-shot finetuning",
        "third-person camera inputs",
        "generalist robot policy learning"
      ]
    },
    "publishedAt": "2025-03-25T11:19:56.000Z",
    "title": "Dita: Scaling Diffusion Transformer for Generalist\n  Vision-Language-Action Policy",
    "summary": "While recent vision-language-action models trained on diverse robot datasets\nexhibit promising generalization capabilities with limited in-domain data,\ntheir reliance on compact action heads to predict discretized or continuous\nactions constrains adaptability to heterogeneous action spaces. We present\nDita, a scalable framework that leverages Transformer architectures to directly\ndenoise continuous action sequences through a unified multimodal diffusion\nprocess. Departing from prior methods that condition denoising on fused\nembeddings via shallow networks, Dita employs in-context conditioning --\nenabling fine-grained alignment between denoised actions and raw visual tokens\nfrom historical observations. This design explicitly models action deltas and\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\nTransformer's scalability, Dita effectively integrates cross-embodiment\ndatasets across diverse camera perspectives, observation scenes, tasks, and\naction spaces. Such synergy enhances robustness against various variances and\nfacilitates the successful execution of long-horizon tasks. Evaluations across\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\nsimulation. Notably, Dita achieves robust real-world adaptation to\nenvironmental variances and complex long-horizon tasks through 10-shot\nfinetuning, using only third-person camera inputs. The architecture establishes\na versatile, lightweight and open-source baseline for generalist robot policy\nlearning. Project Page: https://robodita.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643fa1c318afbc4d1f3e5e59/T0twf6ibM7Htfq525gf3L.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19757.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643fa1c318afbc4d1f3e5e59",
      "avatarUrl": "/avatars/f8f35355902b4cda72e9c6d768322fae.svg",
      "fullname": "Zhi Hou",
      "name": "zhihou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.19990",
      "authors": [
        {
          "_id": "67e4d3df7e97884ba4150ec0",
          "user": {
            "_id": "662516d72419feed62fb3a0a",
            "avatarUrl": "/avatars/24c4157829e70a4e346aa984885aa5ad.svg",
            "isPro": false,
            "fullname": "Dian",
            "user": "KexianTang",
            "type": "user"
          },
          "name": "Kexian Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:46:12.696Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec1",
          "user": {
            "_id": "64a6ae0e0437599198cf3a98",
            "avatarUrl": "/avatars/6635432cc0589ba12dc170cad6986d6d.svg",
            "isPro": false,
            "fullname": "Junyao Gao",
            "user": "favourisnotyou",
            "type": "user"
          },
          "name": "Junyao Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:15.061Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec2",
          "user": {
            "_id": "63d4b843df01ef426a0f79fb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676365795587-63d4b843df01ef426a0f79fb.jpeg",
            "isPro": false,
            "fullname": "Yanhong Zeng",
            "user": "zengyh1900",
            "type": "user"
          },
          "name": "Yanhong Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:46:24.775Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec3",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:00:33.733Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec4",
          "name": "Yanan Sun",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec5",
          "user": {
            "_id": "62fb2a9dc95d426ff8f74c8d",
            "avatarUrl": "/avatars/25c1a68ee7b7d0cc7e9f56bde37f4914.svg",
            "isPro": false,
            "fullname": "Zhening Xing",
            "user": "Leoxing",
            "type": "user"
          },
          "name": "Zhening Xing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:00:36.341Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec6",
          "user": {
            "_id": "6385f8598b5acae8d24caf16",
            "avatarUrl": "/avatars/9d261f95d24e882157b987b8827098be.svg",
            "isPro": false,
            "fullname": "liuwenran",
            "user": "lwrshi1965",
            "type": "user"
          },
          "name": "Wenran Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:46:45.866Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec7",
          "user": {
            "_id": "6414230a0fcefcf72e5085dd",
            "avatarUrl": "/avatars/3a38dc8c84b0f27af846184d1c19f6ef.svg",
            "isPro": false,
            "fullname": "Kaifeng Lyu",
            "user": "vfleaking",
            "type": "user"
          },
          "name": "Kaifeng Lyu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:46:52.341Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec8",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/kjq_V0to2uTR2RSR3TyfV.png"
      ],
      "publishedAt": "2025-03-25T18:21:07.000Z",
      "submittedOnDailyAt": "2025-03-27T03:00:01.698Z",
      "title": "LEGO-Puzzles: Perfomance de MLLM en la lógica espacial multi-paso",
      "submittedOnDailyBy": {
        "_id": "63ee1379190ddd6214efd73a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
        "isPro": false,
        "fullname": "HAODONG DUAN",
        "user": "KennyUTC",
        "type": "user"
      },
      "summary": "La teoría del espacio de pasos en el mundo real es un concepto que abarca la comprensión de las relaciones espaciales en múltiples pasos secuenciales y la teoría de la lógica, lo cual es fundamental para aplicaciones complejas como la manipulación de robots, la navegación automática y la asambla automática. Para evaluar en qué medida los Multimodal Large Language Models (MLLMs) han adquirido estas habilidades básicas, se diseñó un escalable benchmark llamado LEGO-Puzzles, que evalúa las capacidades de comprensión espacial y lógica secuencial de los MLLMs mediante tareas basadas en LEGO. LEGO-Puzzles incluye 1,100 muestras de preguntas visuales y respuestas (VQA) bien seleccionadas y ofrece una amplia gama de 11 tipos de tareas, desde la comprensión básica del espacio hasta la lógica compleja de pasos secuenciales. Basándose en LEGO-Puzzles, se evaluó los MLLMs más potentes y se reveló una limitante crucial en su capacidad para la lógica espacial: los MLLMs más fuertes solo pueden responder a la mitad de los casos de prueba. Los participantes humanos alcanzan una precisión del 90% o más en estas tareas. Además, se evaluó la capacidad de los MLLMs para generar imágenes de LEGO según dibujos de asambla. Los resultados de los experimentos mostraron que otros MLLMs recrean las imágenes de entrada o producen salidas completamente irrelevantes. En general, LEGO-Puzzles ha revelado importantes deficiencias en la comprensión espacial y lógica secuencial de los MLLMs actuales, subrayando la necesidad de avances en la teoría del espacio de pasos.",
      "upvotes": 22,
      "discussionId": "67e4d3e07e97884ba4150f2b",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "LEGO-Puzzles",
        "visual question-answering (VQA)",
        "spatial understanding",
        "sequential reasoning",
        "Gemini-2.0-Flash",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-03-25T14:21:07.000Z",
    "title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
    "summary": "Multi-step spatial reasoning entails understanding and reasoning about\nspatial relationships across multiple sequential steps, which is crucial for\ntackling complex real-world applications, such as robotic manipulation,\nautonomous navigation, and automated assembly. To assess how well current\nMultimodal Large Language Models (MLLMs) have acquired this fundamental\ncapability, we introduce LEGO-Puzzles, a scalable benchmark designed\nto evaluate both spatial understanding and sequential\nreasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100\ncarefully curated visual question-answering (VQA) samples spanning 11 distinct\ntasks, ranging from basic spatial understanding to complex multi-step\nreasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of\nstate-of-the-art MLLMs and uncover significant limitations in their spatial\nreasoning capabilities: even the most powerful MLLMs can answer only about half\nof the test cases, whereas human participants achieve over 90\\% accuracy. In\naddition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images\nfollowing assembly illustrations. Our experiments show that only\nGemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these\ninstructions, while other MLLMs either replicate the input image or generate\ncompletely irrelevant outputs. Overall, LEGO-Puzzles exposes critical\ndeficiencies in existing MLLMs' spatial understanding and sequential reasoning\ncapabilities, and underscores the need for further advancements in multimodal\nspatial reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/kjq_V0to2uTR2RSR3TyfV.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ee1379190ddd6214efd73a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
      "fullname": "HAODONG DUAN",
      "name": "KennyUTC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20240",
      "authors": [
        {
          "_id": "67e4ae6787c92169aa3caa74",
          "user": {
            "_id": "66435efdc26b490acc85079b",
            "avatarUrl": "/avatars/16e0ee25734516d4295abe0fcc0e26a9.svg",
            "isPro": false,
            "fullname": "Prin Phunyaphibarn",
            "user": "prinphunya",
            "type": "user"
          },
          "name": "Prin Phunyaphibarn",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:34.055Z",
          "hidden": false
        },
        {
          "_id": "67e4ae6787c92169aa3caa75",
          "user": {
            "_id": "6342796a0875f2c99cfd313b",
            "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
            "isPro": false,
            "fullname": "Yuseung \"Phillip\" Lee",
            "user": "phillipinseoul",
            "type": "user"
          },
          "name": "Phillip Y. Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:32.144Z",
          "hidden": false
        },
        {
          "_id": "67e4ae6787c92169aa3caa76",
          "name": "Jaihoon Kim",
          "hidden": false
        },
        {
          "_id": "67e4ae6787c92169aa3caa77",
          "user": {
            "_id": "631f432b5ba8c026340a7890",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631f432b5ba8c026340a7890/9PK7A_TRMpugwYjCsNBf1.jpeg",
            "isPro": false,
            "fullname": "Minhyuk Sung",
            "user": "Minhyuk",
            "type": "user"
          },
          "name": "Minhyuk Sung",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:47:11.255Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T05:11:38.000Z",
      "submittedOnDailyAt": "2025-03-27T00:22:30.335Z",
      "title": "¡Claramente es crucial que el modelo líder sea el primero! Es importante mejorar la generación condicional ajustada del modelo de Difusion.",
      "submittedOnDailyBy": {
        "_id": "6342796a0875f2c99cfd313b",
        "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
        "isPro": false,
        "fullname": "Yuseung \"Phillip\" Lee",
        "user": "phillipinseoul",
        "type": "user"
      },
      "summary": "La Guía de Clase Frente (CFG) es una tecnología básica para el entrenamiento de modelos de diferenciación condicional. Una práctica general del entrenamiento basado en CFG utiliza una sola red para aprender a predecir ruido condicional y no condicional, y utiliza un bajo porcentaje de dropout para el entrenamiento condicional. Sin embargo, hemos observado que el aprendizaje simultáneo de ruido no condicional y ruido condicional dentro de un límite de entrenamiento puede deteriorar la precisión en casos no condicionales. Es más, estos malas predicciones de ruido no condicional son una de las principales causas que afectan la calidad de la generación condicional. Basándonos en que muchos modelos condicionales basados en CFG se entrenan a través de ajustes en el modelo base, demostramos que reemplazar el ruido no condicional en la CFG por lo que el modelo base predice puede significativamente mejorar la generación condicional. Además, demostramos que modelos entrenados con diferenciación condicional pueden ser reemplazados en el ruido no condicional. En ambos casos de generación de imágenes y videos, experimentamos con esta hipótesis en modelos condicionales basados en CFG como Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter y InstructPix2Pix.",
      "upvotes": 17,
      "discussionId": "67e4ae6a87c92169aa3cabc2",
      "ai_keywords": [
        "Classifier-Free Guidance (CFG)",
        "conditional diffusion models",
        "noise prediction",
        "dropout rate",
        "priors",
        "fine-tuning",
        "base model",
        "unconditional generation",
        "variance scaling",
        "Zero-1-to-3",
        "Versatile Diffusion",
        "DiT",
        "DynamiCrafter",
        "InstructPix2Pix"
      ]
    },
    "publishedAt": "2025-03-26T01:11:38.000Z",
    "title": "Unconditional Priors Matter! Improving Conditional Generation of\n  Fine-Tuned Diffusion Models",
    "summary": "Classifier-Free Guidance (CFG) is a fundamental technique in training\nconditional diffusion models. The common practice for CFG-based training is to\nuse a single network to learn both conditional and unconditional noise\nprediction, with a small dropout rate for conditioning. However, we observe\nthat the joint learning of unconditional noise with limited bandwidth in\ntraining results in poor priors for the unconditional case. More importantly,\nthese poor unconditional noise predictions become a serious reason for\ndegrading the quality of conditional generation. Inspired by the fact that most\nCFG-based conditional models are trained by fine-tuning a base model with\nbetter unconditional generation, we first show that simply replacing the\nunconditional noise in CFG with that predicted by the base model can\nsignificantly improve conditional generation. Furthermore, we show that a\ndiffusion model other than the one the fine-tuned model was trained on can be\nused for unconditional noise replacement. We experimentally verify our claim\nwith a range of CFG-based conditional models for both image and video\ngeneration, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and\nInstructPix2Pix.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20240.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6342796a0875f2c99cfd313b",
      "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
      "fullname": "Yuseung \"Phillip\" Lee",
      "name": "phillipinseoul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20215",
      "authors": [
        {
          "_id": "67e4f2507e97884ba4205660",
          "name": "Jin Xu",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205661",
          "user": {
            "_id": "661e577cbac5d981f883b743",
            "avatarUrl": "/avatars/95e55e9707a6b55594c264081202d7f4.svg",
            "isPro": false,
            "fullname": "GuoZhifang",
            "user": "ZhifangGuo",
            "type": "user"
          },
          "name": "Zhifang Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:52:32.600Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205662",
          "user": {
            "_id": "6594f06ac04427eb38444bce",
            "avatarUrl": "/avatars/b13fbf589b25eff038deb3fa12d95871.svg",
            "isPro": false,
            "fullname": "Jinzheng He",
            "user": "jinzheng-he",
            "type": "user"
          },
          "name": "Jinzheng He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:52:48.707Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205663",
          "name": "Hangrui Hu",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205664",
          "name": "Ting He",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205665",
          "user": {
            "_id": "63451cf0a05b51f7ded25505",
            "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg",
            "isPro": false,
            "fullname": "shuai bai",
            "user": "bluelike",
            "type": "user"
          },
          "name": "Shuai Bai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:02.818Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205666",
          "user": {
            "_id": "6461d675681b2e19b6acb5a5",
            "avatarUrl": "/avatars/0d95d65d30f6672ec09dc92155324d7f.svg",
            "isPro": false,
            "fullname": "Keqin Chen",
            "user": "chenkq",
            "type": "user"
          },
          "name": "Keqin Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:19.770Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205667",
          "user": {
            "_id": "649a3ba9342f14148357c367",
            "avatarUrl": "/avatars/81a769fa38b7384f382ff3cc10d6d624.svg",
            "isPro": false,
            "fullname": "Jialin Wang",
            "user": "JialinWang",
            "type": "user"
          },
          "name": "Jialin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:26.467Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205668",
          "name": "Yang Fan",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205669",
          "user": {
            "_id": "6712930f0fac3235c56edf5b",
            "avatarUrl": "/avatars/cafe7cb56ce7c3b2572f5f2d0b89357a.svg",
            "isPro": false,
            "fullname": "kai dang",
            "user": "1vk5i",
            "type": "user"
          },
          "name": "Kai Dang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:33.105Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566a",
          "name": "Bin Zhang",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566b",
          "name": "Xiong Wang",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566c",
          "user": {
            "_id": "62c6a751a71b40cf26f359a8",
            "avatarUrl": "/avatars/49abd2e71946035452c316d703baaac6.svg",
            "isPro": false,
            "fullname": "Yunfei Chu",
            "user": "faychu",
            "type": "user"
          },
          "name": "Yunfei Chu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:51.503Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566d",
          "user": {
            "_id": "620760a26e3b7210c2ff1943",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
            "isPro": false,
            "fullname": "Junyang Lin",
            "user": "JustinLin610",
            "type": "user"
          },
          "name": "Junyang Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:44.277Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/659e513ea9bc1f60189ac148/3pDelIehF3CWGPS0jrZvN.png"
      ],
      "publishedAt": "2025-03-26T04:17:55.000Z",
      "submittedOnDailyAt": "2025-03-27T05:14:09.555Z",
      "title": "Informe Técnico Qwen2.5-Omni\n\nEl Informe Técnico Qwen2.5-Omni detalla exhaustivamente las características técnicas y el rendimiento del modelo Qwen2.5-Omni, constituyendo un informe importante que demuestra el desarrollo y la innovación en los modelos de IA. El informe incluye el contexto de desarrollo del modelo, los principios teóricos, la evaluación del rendimiento y casos de uso prácticos, proporcionando información valiosa para especialistas y profesionales en la área de IA.",
      "submittedOnDailyBy": {
        "_id": "659e513ea9bc1f60189ac148",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659e513ea9bc1f60189ac148/DBDDqjGTQ0SvjWyuYu7py.jpeg",
        "isPro": false,
        "fullname": "YuanjunLv",
        "user": "Bakerbunker",
        "type": "user"
      },
      "summary": "Este informe presenta Qwen2.5-Omni, un modelo que reconoce las diversas características desde el punto final hasta el punto final. Este modelo puede reconocer simultáneamente modelos de texto, imágenes, voz y vídeo, y genera respuestas vocales naturales en modo streaming. Para procesar diferentes tipos de entrada en modo streaming, el codificador de voz y el codificador visual utilizan un enfoque de bloque. Para sincronizar el tiempo de los videos con la voz, se propone un nuevo enfoque de mapeo de posición llamado TMRoPE (Time-aligned Multimodal RoPE), que ordena la voz y el video en secuencia y propone un enfoque de mapeo de posición. Para generar texto y voz simultáneamente y evitar interferencias entre los modelos, se propone la arquitectura Thinker-Talker. En este marco, Thinker es un modelo de lenguaje de gran escala que genera texto, mientras que Talker es un modelo de auto-incremento de dos tramas que puede generar tokens de voz utilizando las representaciones ocultas provenientes de Thinker. Los modelos Thinker y Talker permiten el entrenamiento y la inferencia desde el punto final hasta el punto final. Para decodificar tokens de voz en modo streaming, se introdujo el DiT (Diffusion Transformer) con ventanas deslizantes, con el objetivo de limitar el área de entrada y reducir el retraso inicial de paquetes. Qwen2.5-Omni muestra un rendimiento relativamente similar al de Qwen2.5-VL de la misma tamaño y mejora el rendimiento del Qwen2-Audio. Además, en diferentes marcos de referencia como Omni-Bench, Qwen2.5-Omni alcanza los mejores resultados. Específicamente, basado en pruebas de benchmark como MMLU y GSM8K, el rendimiento de Qwen2.5-Omni en la ejecución de instrucciones de voz desde el punto final hasta el punto final es igual al de la entrada de texto. En la generación de voz, el Talker de Qwen2.5-Omni en modo streaming muestra una fuerte robustez y naturalidad, superando muchos otros modelos tanto en opciones streaming como no streaming.",
      "upvotes": 17,
      "discussionId": "67e4f2527e97884ba42056df",
      "projectPage": "https://qwenlm.github.io/blog/qwen2.5-omni/",
      "githubRepo": "https://github.com/QwenLM/Qwen2.5-Omni",
      "ai_keywords": [
        "multimodal model",
        "block-wise processing",
        "interleaved manner",
        "TMRoPE (Time-aligned Multimodal RoPE)",
        "position embedding",
        "Thinker-Talker architecture",
        "large language model",
        "dual-track autoregressive model",
        "end-to-end manner",
        "sliding-window DiT",
        "receptive field",
        "initial package delay",
        "Omni-Bench",
        "MMLU",
        "GSM8K",
        "end-to-end speech instruction following"
      ]
    },
    "publishedAt": "2025-03-26T00:17:55.000Z",
    "title": "Qwen2.5-Omni Technical Report",
    "summary": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model\ndesigned to perceive diverse modalities, including text, images, audio, and\nvideo, while simultaneously generating text and natural speech responses in a\nstreaming manner. To enable the streaming of multimodal information inputs,\nboth audio and visual encoders utilize a block-wise processing approach. To\nsynchronize the timestamps of video inputs with audio, we organize the audio\nand video sequentially in an interleaved manner and propose a novel position\nembedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently\ngenerate text and speech while avoiding interference between the two\nmodalities, we propose Thinker-Talker architecture. In this framework,\nThinker functions as a large language model tasked with text generation, while\nTalker is a dual-track autoregressive model that directly utilizes the hidden\nrepresentations from the Thinker to produce audio tokens as output. Both the\nThinker and Talker models are designed to be trained and inferred in an\nend-to-end manner. For decoding audio tokens in a streaming manner, we\nintroduce a sliding-window DiT that restricts the receptive field, aiming to\nreduce the initial package delay. Qwen2.5-Omni is comparable with the similarly\nsized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni\nachieves state-of-the-art performance on multimodal benchmarks like Omni-Bench.\nNotably, Qwen2.5-Omni's performance in end-to-end speech instruction following\nis comparable to its capabilities with text inputs, as evidenced by benchmarks\nsuch as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming\nTalker outperforms most existing streaming and non-streaming alternatives in\nrobustness and naturalness.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/659e513ea9bc1f60189ac148/3pDelIehF3CWGPS0jrZvN.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20215.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "659e513ea9bc1f60189ac148",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659e513ea9bc1f60189ac148/DBDDqjGTQ0SvjWyuYu7py.jpeg",
      "fullname": "YuanjunLv",
      "name": "Bakerbunker",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20314",
      "authors": [
        {
          "_id": "67e4b65a080a33e3955b340c",
          "name": "WanTeam",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b340e",
          "user": {
            "_id": "63f1f1727ddf724fbcbc9c7e",
            "avatarUrl": "/avatars/9e0516d9b1036c23c78f313c79872f55.svg",
            "isPro": false,
            "fullname": "Ang Wang",
            "user": "ang-annng",
            "type": "user"
          },
          "name": "Ang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:47:28.144Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b340f",
          "user": {
            "_id": "64755ff5a51711a3b59118af",
            "avatarUrl": "/avatars/2e899088902db94e785107c3ec2abe85.svg",
            "isPro": false,
            "fullname": "Baole Ai",
            "user": "baoleai",
            "type": "user"
          },
          "name": "Baole Ai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:47:49.260Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3410",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3411",
          "user": {
            "_id": "6458970cab9a44f42f620a80",
            "avatarUrl": "/avatars/f9779b0621c931f922440fec95342444.svg",
            "isPro": false,
            "fullname": "chaojie mao",
            "user": "chaojiemao",
            "type": "user"
          },
          "name": "Chaojie Mao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:02.730Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3412",
          "user": {
            "_id": "66592c72f4124d863fd55574",
            "avatarUrl": "/avatars/98f0d5e6ba3728e8a1164aa5188a3298.svg",
            "isPro": false,
            "fullname": "Chenwei Xie",
            "user": "chenweix7",
            "type": "user"
          },
          "name": "Chen-Wei Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:10.933Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3413",
          "name": "Di Chen",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3414",
          "name": "Feiwu Yu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3415",
          "user": {
            "_id": "67a73767282aa06f7bcaeeb1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/J28OVrPhD0xYulWMgICmW.png",
            "isPro": false,
            "fullname": "Haiming Zhao",
            "user": "HermanZ",
            "type": "user"
          },
          "name": "Haiming Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:26.135Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3416",
          "user": {
            "_id": "651441e92c5da979038df5ee",
            "avatarUrl": "/avatars/85cdafcccb522eced50dc9e4770b630a.svg",
            "isPro": false,
            "fullname": "Jianxiao Yang",
            "user": "Jianxiao0203",
            "type": "user"
          },
          "name": "Jianxiao Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:33.714Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3417",
          "user": {
            "_id": "6274b866f978441a764b30f6",
            "avatarUrl": "/avatars/953b1ff82f63e371a7358a85d68304cd.svg",
            "isPro": false,
            "fullname": "jianyuan.zengjy",
            "user": "filwsyl",
            "type": "user"
          },
          "name": "Jianyuan Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:40.108Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3418",
          "name": "Jiayu Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3419",
          "user": {
            "_id": "66f0e0262aee3cb7e981bbac",
            "avatarUrl": "/avatars/f8f1e70469b5e047dc6e0e9dec6c5bc1.svg",
            "isPro": false,
            "fullname": "Jingfeng Zhang",
            "user": "jingfengzhang",
            "type": "user"
          },
          "name": "Jingfeng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:58.316Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341a",
          "user": {
            "_id": "602f88f5e8149a962412a667",
            "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
            "isPro": false,
            "fullname": "Zhou",
            "user": "Jingren",
            "type": "user"
          },
          "name": "Jingren Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:49:09.146Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341b",
          "user": {
            "_id": "627c93b2bec91eb1720b8bad",
            "avatarUrl": "/avatars/89c31c71aa5027543ed5be0471fe1109.svg",
            "isPro": false,
            "fullname": "Jinkai Wang",
            "user": "zwsjink",
            "type": "user"
          },
          "name": "Jinkai Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:49:15.680Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341c",
          "user": {
            "_id": "6465941d0e6c7618f615675b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6465941d0e6c7618f615675b/W4EHqlCucz_bojFLFEeV_.jpeg",
            "isPro": false,
            "fullname": "Jixuan Chen",
            "user": "Mayome",
            "type": "user"
          },
          "name": "Jixuan Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:49:25.437Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341d",
          "name": "Kai Zhu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341e",
          "name": "Kang Zhao",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341f",
          "name": "Keyu Yan",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3420",
          "name": "Lianghua Huang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3421",
          "user": {
            "_id": "63b4ec15103617b0a5b3101e",
            "avatarUrl": "/avatars/e6faad833b31ad5d892faccf621e7a34.svg",
            "isPro": false,
            "fullname": "Mengyang Feng",
            "user": "archerfmy",
            "type": "user"
          },
          "name": "Mengyang Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:50:01.919Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3422",
          "user": {
            "_id": "66eae63f533fd44f8a8ca60b",
            "avatarUrl": "/avatars/38cecb4c80cc7a6e63028fcb572e3a22.svg",
            "isPro": false,
            "fullname": "Zhang Ningyi",
            "user": "ZhangNy",
            "type": "user"
          },
          "name": "Ningyi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:50:13.628Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3423",
          "name": "Pandeng Li",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3424",
          "user": {
            "_id": "64c5182771947b03ffee931c",
            "avatarUrl": "/avatars/478f4e06ac1bced092dde0f11963a975.svg",
            "isPro": false,
            "fullname": "Wupingyu",
            "user": "wpy1999",
            "type": "user"
          },
          "name": "Pingyu Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:50:38.625Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3425",
          "user": {
            "_id": "642e3bcb958faf258a40e89c",
            "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
            "isPro": false,
            "fullname": "Ruihang Chu",
            "user": "Ruihang",
            "type": "user"
          },
          "name": "Ruihang Chu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:50:46.771Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3426",
          "user": {
            "_id": "6790e2b74932687e24024b4a",
            "avatarUrl": "/avatars/951f55648490e1f520483a3e425621dd.svg",
            "isPro": false,
            "fullname": "Ruili",
            "user": "RuiliFeng",
            "type": "user"
          },
          "name": "Ruili Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:03.191Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3427",
          "name": "Shiwei Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3428",
          "user": {
            "_id": "62bbf42ac9633b01802a6d45",
            "avatarUrl": "/avatars/0fee1462d228f5e7f22d5c240900a3ad.svg",
            "isPro": false,
            "fullname": "Siyang Sun",
            "user": "sunsiyang",
            "type": "user"
          },
          "name": "Siyang Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:10.461Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3429",
          "name": "Tao Fang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342a",
          "name": "Tianxing Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342b",
          "name": "Tianyi Gui",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342c",
          "name": "Tingyu Weng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342d",
          "name": "Tong Shen",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342e",
          "name": "Wei Lin",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342f",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3430",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3431",
          "user": {
            "_id": "623c6253389748c9f72ca287",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654828369523-623c6253389748c9f72ca287.jpeg",
            "isPro": false,
            "fullname": "wenmeng zhou",
            "user": "wenmengzhou",
            "type": "user"
          },
          "name": "Wenmeng Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:38.310Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3432",
          "user": {
            "_id": "644240b1251730a7ee243ef3",
            "avatarUrl": "/avatars/c4ca99739e2b6f3d3d0ca83ecc54766a.svg",
            "isPro": false,
            "fullname": "wente.wang",
            "user": "shiftc",
            "type": "user"
          },
          "name": "Wente Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:46.041Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3433",
          "user": {
            "_id": "64af91eb5c17fe25cfcbebc3",
            "avatarUrl": "/avatars/ffc6e7b6a40300e05e66f544264dddbc.svg",
            "isPro": false,
            "fullname": "Wenting Shen",
            "user": "SeventeenSSS",
            "type": "user"
          },
          "name": "Wenting Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:53.298Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3434",
          "name": "Wenyuan Yu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3435",
          "user": {
            "_id": "642e19b26748dd4f8eea1321",
            "avatarUrl": "/avatars/a534e61c21d2fb3c7a4c4d4dba98fafb.svg",
            "isPro": false,
            "fullname": "Xianzhong Shi",
            "user": "itutor",
            "type": "user"
          },
          "name": "Xianzhong Shi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:19.514Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3436",
          "user": {
            "_id": "65105ab08c4b535a97052fe8",
            "avatarUrl": "/avatars/a97862045a26a74ca33d1a47b6a1f2b4.svg",
            "isPro": false,
            "fullname": "xiaominghuang",
            "user": "xiaominghuang",
            "type": "user"
          },
          "name": "Xiaoming Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:52:03.599Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3437",
          "name": "Xin Xu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3438",
          "name": "Yan Kou",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3439",
          "name": "Yangyu Lv",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343a",
          "name": "Yifei Li",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343b",
          "user": {
            "_id": "67d39e61943a965360fbbc0c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-JwILFmblPdd6Sv28c1J7.png",
            "isPro": false,
            "fullname": "yijing liu",
            "user": "86diphda",
            "type": "user"
          },
          "name": "Yijing Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:52:18.647Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343c",
          "name": "Yiming Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343d",
          "name": "Yingya Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343e",
          "name": "Yitong Huang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343f",
          "name": "Yong Li",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3440",
          "name": "You Wu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3441",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3442",
          "name": "Yulin Pan",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3443",
          "name": "Yun Zheng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3444",
          "name": "Yuntao Hong",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3445",
          "name": "Yupeng Shi",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3446",
          "name": "Yutong Feng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3447",
          "name": "Zeyinzi Jiang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3448",
          "name": "Zhen Han",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3449",
          "name": "Zhi-Fan Wu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b344a",
          "name": "Ziyu Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T08:25:43.000Z",
      "submittedOnDailyAt": "2025-03-27T00:52:37.426Z",
      "title": "\"Modelo de Generación de Video de Gran Escala Abierto y Avanzado\"",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "En este informe se presenta Wan, un útil y abierto modelo de hojas basado en video. Este modelo ha sido diseñado para superar los límites de la generación de videos. Se ha construido en el paradigma principal de transformadores distribuidos y, a través de una serie de innovaciones como el VAE nuevo, estrategias de aprendizaje previo escalable, gran colección de datos y el cálculo automático de indicadores evaluativos, Wan ha logrado un importante avance en su capacidad de generación. Estas contribuciones mejoran tanto el rendimiento como la diversidad del modelo. En particular, Wan posee las siguientes cuatro características:\n\n- Rendimiento líder: El modelo Wan de 14B ha sido entrenado con un gran conjunto de imágenes y videos, demostrando la ley de escala de la generación de videos. En varios benchmarks internos y externos, Wan supera a los modelos abiertos más avanzados y a las soluciones comerciales más innovadoras, mostrando un rendimiento superior y significativo.\n- Detallado: Ofrece dos modelos capaces, uno de 1.3B y otro de 14B parámetros, demostrando tanto eficiencia como eficacia. Además, procesa una variedad de aplicaciones de video, desde el editado de videos a partir de imágenes hasta la generación de videos de uso comercial.\n- Eficiencia al nivel del consumidor: El modelo de 1.3B muestra una eficiencia especial, solo requiriendo 8.19 GB de VRAM, lo que se ajusta a una amplia gama de GPUs de consumo.\n- Abierto: Se ha abierto toda la serie de Wan, incluyendo código fuente y todos los modelos, con el objetivo de fomentar el crecimiento de la comunidad de video generación. Esta abiertura amplía significativamente la posibilidad creativa de la industria en la producción de videos y proporciona modelos de alta calidad a la comunidad académica. Todo el código y los modelos están disponibles en https://github.com/Wan-Video/Wan2.1.",
      "upvotes": 16,
      "discussionId": "67e4b663080a33e3955b371a",
      "ai_keywords": [
        "diffusion transformer",
        "VAE",
        "large-scale data curation",
        "automated evaluation metrics",
        "scaling laws",
        "image-to-video",
        "instruction-guided video editing",
        "personal video generation"
      ]
    },
    "publishedAt": "2025-03-26T04:25:43.000Z",
    "title": "Wan: Open and Advanced Large-Scale Video Generative Models",
    "summary": "This report presents Wan, a comprehensive and open suite of video foundation\nmodels designed to push the boundaries of video generation. Built upon the\nmainstream diffusion transformer paradigm, Wan achieves significant\nadvancements in generative capabilities through a series of innovations,\nincluding our novel VAE, scalable pre-training strategies, large-scale data\ncuration, and automated evaluation metrics. These contributions collectively\nenhance the model's performance and versatility. Specifically, Wan is\ncharacterized by four key features: Leading Performance: The 14B model of Wan,\ntrained on a vast dataset comprising billions of images and videos,\ndemonstrates the scaling laws of video generation with respect to both data and\nmodel size. It consistently outperforms the existing open-source models as well\nas state-of-the-art commercial solutions across multiple internal and external\nbenchmarks, demonstrating a clear and significant performance superiority.\nComprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B\nparameters, for efficiency and effectiveness respectively. It also covers\nmultiple downstream applications, including image-to-video, instruction-guided\nvideo editing, and personal video generation, encompassing up to eight tasks.\nConsumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource\nefficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range\nof consumer-grade GPUs. Openness: We open-source the entire series of Wan,\nincluding source code and all models, with the goal of fostering the growth of\nthe video generation community. This openness seeks to significantly expand the\ncreative possibilities of video production in the industry and provide academia\nwith high-quality video foundation models. All the code and models are\navailable at https://github.com/Wan-Video/Wan2.1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20314.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20201",
      "authors": [
        {
          "_id": "67e4b04c8c0347025bd0fe84",
          "user": {
            "_id": "6109bc89e84ad84682a69754",
            "avatarUrl": "/avatars/067aac8784320d4e8e875379dc4cc209.svg",
            "isPro": false,
            "fullname": "Salaheddin Alzubi",
            "user": "salzubi401",
            "type": "user"
          },
          "name": "Salaheddin Alzubi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:03.915Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe85",
          "user": {
            "_id": "673f945e6cd62dbd4b02790d",
            "avatarUrl": "/avatars/3742e4e6b88d4f8b78d5c5308f55773e.svg",
            "isPro": false,
            "fullname": "Creston Brooks",
            "user": "cabxyz",
            "type": "user"
          },
          "name": "Creston Brooks",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-27T01:56:28.853Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe86",
          "user": {
            "_id": "666619508a270cedd594e55e",
            "avatarUrl": "/avatars/79bb2b09a663cae555140ec9379f05d9.svg",
            "isPro": false,
            "fullname": "Purva Chiniya",
            "user": "pchiniya",
            "type": "user"
          },
          "name": "Purva Chiniya",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:10.184Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe87",
          "name": "Edoardo Contente",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe88",
          "name": "Chiara von Gerlach",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe89",
          "user": {
            "_id": "62296d3f2df798b7e951e475",
            "avatarUrl": "/avatars/661c23416c3d418e2996f9b9a024db82.svg",
            "isPro": false,
            "fullname": "Lucas Irwin",
            "user": "ljirwin",
            "type": "user"
          },
          "name": "Lucas Irwin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:26.927Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8a",
          "name": "Yihan Jiang",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8b",
          "user": {
            "_id": "67759bf644ceb61f96739324",
            "avatarUrl": "/avatars/44cb431a9cbc73e060aff7d90435c42d.svg",
            "isPro": false,
            "fullname": "Arda Kaz",
            "user": "speedyarda",
            "type": "user"
          },
          "name": "Arda Kaz",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:57.570Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8c",
          "user": {
            "_id": "64b98bcf842aa47891bc0f63",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/L-smrOCQ3MXtvnISJqmxJ.png",
            "isPro": false,
            "fullname": "Windsor Nguyen",
            "user": "windsornguyen",
            "type": "user"
          },
          "name": "Windsor Nguyen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:50.840Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8d",
          "user": {
            "_id": "6756dffd88428044e2ddbdd9",
            "avatarUrl": "/avatars/41dbb83c68b56546cdf8e34379faf6b3.svg",
            "isPro": false,
            "fullname": "Sewoong Oh",
            "user": "sewoong79",
            "type": "user"
          },
          "name": "Sewoong Oh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:55:20.604Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8e",
          "user": {
            "_id": "65f86cc77b704590d4a5439f",
            "avatarUrl": "/avatars/1a828cf755839f058241fb19ca83341f.svg",
            "isPro": false,
            "fullname": "Himanshu Tyagi",
            "user": "HimanshuTyagi",
            "type": "user"
          },
          "name": "Himanshu Tyagi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:55:27.139Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8f",
          "name": "Pramod Viswanath",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T03:51:32.000Z",
      "submittedOnDailyAt": "2025-03-27T00:26:59.804Z",
      "title": "Open Deep Search: El motivo de la Open Source: El motivo de la Open Source es lograr un desarrollo más rápido y mejor, así como promover la colaboración y la innovación en el campo de la tecnología. Los desarrolladores de código abierto pueden contribuir y mejorar los códigos de manera más rápida y eficiente, ya que el código está disponible para todas las personas. Además, la Open Source fomenta la transparencia y la confianza en el sistema, ya que todos pueden ver cómo funcionan los componentes y los mecanismos internos. También permite a los usuarios y desarrolladores crear y adaptar aplicaciones personalizadas, lo que aumenta la flexibilidad y la adaptabilidad de los sistemas. Finalmente, la Open Source reduce los costos asociados con el desarrollo y la implementación de software, ya que los códigos abiertos son gratuitos y pueden ser utilizados sin costo adicional.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "ODS se introduce. ODS tiene como objetivo reducir las diferencias entre las soluciones de inteligencia de búsqueda de la empresa como Sonar Reasoning Pro de Perplexity y la Preview de la búsqueda de GPT-4o de OpenAI y su versión abierta. En ODS, la innovación principal consiste en fortalecer las capacidades de inferencia de los últimos LLMs abiertos utilizando agentes de inferencia para utilizar con cuidado un herramienta de búsqueda web para responder a preguntas. Concretamente, ODS se compone de dos componentes que funcionan en colaboración con el LLM base elegido por el usuario: la herramienta de búsqueda abierta y el agente de razonamiento abierto. El agente de razonamiento abierto entiende la tarea dada y escribe una matriz para completar la tarea, que incluye turnos con la herramienta de búsqueda abierta. La herramienta de búsqueda abierta es una nueva herramienta de búsqueda web más avanzada que la de la versión propietaria. ODS combina un potente LLM de inferencia abierto como DeepSeek-R1 para obtener rendimientos cercanos a los límites actuales de los líneamientos base en los benchmarks SimpleQA y FRAMES, y a veces supera los mismos. Por ejemplo, en el benchmark de evaluación FRAMES, ODS mejoró el mejor línea base recientemente publicado de la Preview de búsqueda de GPT-4o en un 9.7% y aumentó la precisión. ODS proporciona un marco de trabajo semi-estructurado para cualquier LLM y mejora las capacidades de búsqueda e inferencia. Por ejemplo, DeepSeek-R1 alcanza un rendimiento de 82.4% en SimpleQA y 30.1% en FRAMES, pero con ODS, alcanza un rendimiento líder en el límite en SimpleQA de 88.3% y en FRAMES de 75.3%.",
      "upvotes": 12,
      "discussionId": "67e4b04c8c0347025bd0fed2",
      "ai_keywords": [
        "LLMs",
        "reasoning agents",
        "web search tools",
        "Open Search Tool",
        "Open Reasoning Agent",
        "DeepSeek-R1",
        "SimpleQA",
        "FRAMES",
        "GPT-4o Search Preview"
      ]
    },
    "publishedAt": "2025-03-25T23:51:32.000Z",
    "title": "Open Deep Search: Democratizing Search with Open-source Reasoning Agents",
    "summary": "We introduce Open Deep Search (ODS) to close the increasing gap between the\nproprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and\nOpenAI's GPT-4o Search Preview, and their open-source counterparts. The main\ninnovation introduced in ODS is to augment the reasoning capabilities of the\nlatest open-source LLMs with reasoning agents that can judiciously use web\nsearch tools to answer queries. Concretely, ODS consists of two components that\nwork with a base LLM chosen by the user: Open Search Tool and Open Reasoning\nAgent. Open Reasoning Agent interprets the given task and completes it by\norchestrating a sequence of actions that includes calling tools, one of which\nis the Open Search Tool. Open Search Tool is a novel web search tool that\noutperforms proprietary counterparts. Together with powerful open-source\nreasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses\nthe existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES.\nFor example, on the FRAMES evaluation benchmark, ODS improves the best existing\nbaseline of the recently released GPT-4o Search Preview by 9.7% in accuracy.\nODS is a general framework for seamlessly augmenting any LLMs -- for example,\nDeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search\nand reasoning capabilities to achieve state-of-the-art performance: 88.3% on\nSimpleQA and 75.3% on FRAMES.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20201.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19480",
      "authors": [
        {
          "_id": "67e3693eebafaa1efbed08d2",
          "user": {
            "_id": "67d30d9ae45dc43004b31425",
            "avatarUrl": "/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg",
            "isPro": false,
            "fullname": "Shijie Ma",
            "user": "msj9817",
            "type": "user"
          },
          "name": "Shijie Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-26T20:44:37.274Z",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d3",
          "user": {
            "_id": "6455cc8f654d8bccae50e4d4",
            "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
            "isPro": false,
            "fullname": "Yuying Ge",
            "user": "tttoaster",
            "type": "user"
          },
          "name": "Yuying Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:55:42.975Z",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d4",
          "name": "Teng Wang",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d5",
          "user": {
            "_id": "67bc21106a3e748d80d11dc7",
            "avatarUrl": "/avatars/fbc7e76a3266a3c06d03e85db96a51cf.svg",
            "isPro": false,
            "fullname": "yuxin guo",
            "user": "aether25",
            "type": "user"
          },
          "name": "Yuxin Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:56:02.897Z",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d6",
          "user": {
            "_id": "640e9762b03f4cd29f58d982",
            "avatarUrl": "/avatars/81da37d628163fe3e094b247c7c3a3b5.svg",
            "isPro": false,
            "fullname": "Yixiao Ge",
            "user": "yxgeee",
            "type": "user"
          },
          "name": "Yixiao Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:56:08.653Z",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d7",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:56:14.275Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T09:15:34.000Z",
      "submittedOnDailyAt": "2025-03-27T01:26:37.430Z",
      "title": "GenHancer: Modelos generativos incompletos son poderosos ocultamente\n  Álbum de Cannes アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバム アルバ",
      "submittedOnDailyBy": {
        "_id": "67d30d9ae45dc43004b31425",
        "avatarUrl": "/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg",
        "isPro": false,
        "fullname": "Shijie Ma",
        "user": "msj9817",
        "type": "user"
      },
      "summary": "El fusion de modelos generativos y discriminativos recibe mucha atención. El Contrastive Language-Image Pre-Training (CLIP) discriminativo supera el significado lingüístico de alto nivel, pero presenta dificultades para comprender detalles visuales minuciosos. Generalmente, modelos generativos mejoran la representación usando como condición los característicos visuales de CLIP. Sin embargo, los fundamentos de este proceso no han sido suficientemente investigados. En este artículo, se ha experimentalmente descubierto que la generación visual completa no siempre es óptima para mejorar la representación. El esencia es extraer eficazmente conocimientos minuciosos en modelos generativos mientras restrinja información irrelevante. Para investigar estos factores, se revisaron tres aspectos: 1. Estructura de condición: el uso de tokens locales en escalas pequeñas redució significativamente los problemas de reconstrucción y se observó que el aprendizaje se desgastaba. Por lo tanto, se concluyó que la condición debe ser únicamente de tokens visuais globales. 2. Configuración del decoder: se encontró que el aprendizaje incluyendo información de inicio y fin mejora el proceso. En respuesta, se propone una estrategia de aprendizaje en dos etapas, priorizando el aprendizaje de conocimientos visuales útiles. Además, se confirmó que un decoder ligero puede demostrar notables mejoras. 3. Paradigma de generación: se intentó un decoder continuo y distribuido y se evaluó su aplicabilidad general. Tras una revisión profunda, se propone el método óptimo como GenHancer, superando las técnicas existentes en el benchmark MMVP-VLM (por ejemplo, mejorando a CLIP de OpenAI en un 6.0%). El CLIP reforzado puede ser integrado en diferentes modelos de lenguaje para mejorar el rendimiento en escenarios visuales. Todos los modelos y códigos están disponibles.",
      "upvotes": 11,
      "discussionId": "67e36940ebafaa1efbed0951",
      "projectPage": "https://mashijie1028.github.io/GenHancer/",
      "githubRepo": "https://github.com/mashijie1028/GenHancer",
      "ai_keywords": [
        "generative",
        "discriminative",
        "Contrastive Language-Image Pre-Training (CLIP)",
        "visual features",
        "reconstruction",
        "local tokens",
        "global visual tokens",
        "Conditioning mechanisms",
        "end-to-end training",
        "denoising configurations",
        "two-stage training",
        "lightweight denoisers",
        "continuous denoisers",
        "discrete denoisers",
        "Generation paradigms",
        "GenHancer",
        "MMVP-VLM benchmark",
        "OpenAICLIP",
        "multimodal large language models",
        "vision-centric performance"
      ]
    },
    "publishedAt": "2025-03-25T05:15:34.000Z",
    "title": "GenHancer: Imperfect Generative Models are Secretly Strong\n  Vision-Centric Enhancers",
    "summary": "The synergy between generative and discriminative models receives growing\nattention. While discriminative Contrastive Language-Image Pre-Training (CLIP)\nexcels in high-level semantics, it struggles with perceiving fine-grained\nvisual details. Generally, to enhance representations, generative models take\nCLIP's visual features as conditions for reconstruction. However, the\nunderlying principle remains underexplored. In this work, we empirically found\nthat visually perfect generations are not always optimal for representation\nenhancement. The essence lies in effectively extracting fine-grained knowledge\nfrom generative models while mitigating irrelevant information. To explore\ncritical factors, we delve into three aspects: (1) Conditioning mechanisms: We\nfound that even a small number of local tokens can drastically reduce the\ndifficulty of reconstruction, leading to collapsed training. We thus conclude\nthat utilizing only global visual tokens as conditions is the most effective\nstrategy. (2) Denoising configurations: We observed that end-to-end training\nintroduces extraneous information. To address this, we propose a two-stage\ntraining strategy to prioritize learning useful visual knowledge. Additionally,\nwe demonstrate that lightweight denoisers can yield remarkable improvements.\n(3) Generation paradigms: We explore both continuous and discrete denoisers\nwith desirable outcomes, validating the versatility of our method. Through our\nin-depth explorations, we have finally arrived at an effective method, namely\nGenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark,\ne.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into\nmultimodal large language models for better vision-centric performance. All the\nmodels and codes are made publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19480.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67d30d9ae45dc43004b31425",
      "avatarUrl": "/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg",
      "fullname": "Shijie Ma",
      "name": "msj9817",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20672",
      "authors": [
        {
          "_id": "67e4b82c672b3d9c9cb42c70",
          "name": "Yuyang Peng",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c71",
          "name": "Shishi Xiao",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c72",
          "user": {
            "_id": "66bf00ca5b4e241fe266059d",
            "avatarUrl": "/avatars/f3eedfecf5baa8e2ac80d37abe42c63f.svg",
            "isPro": false,
            "fullname": "Keming Wu",
            "user": "wukeming11",
            "type": "user"
          },
          "name": "Keming Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:20.576Z",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c73",
          "user": {
            "_id": "672894ff1905afcdc9132fc6",
            "avatarUrl": "/avatars/78297eadad816c45d680aa70cea3b973.svg",
            "isPro": false,
            "fullname": "QISHENG LIAO",
            "user": "Marseclipse",
            "type": "user"
          },
          "name": "Qisheng Liao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:14.624Z",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c74",
          "user": {
            "_id": "64ba249e5c4deebf69aa17fd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/h7YdQe9wEr1TTJfP1ALhb.jpeg",
            "isPro": false,
            "fullname": "chen",
            "user": "bohanChen",
            "type": "user"
          },
          "name": "Bohan Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:07.459Z",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c75",
          "user": {
            "_id": "6298fd95b58e71e2ac9f3ad8",
            "avatarUrl": "/avatars/7d34644d537bc5c17cf1e4ce4095355c.svg",
            "isPro": false,
            "fullname": "Kevin Lin",
            "user": "kevinlin311tw",
            "type": "user"
          },
          "name": "Kevin Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:01.691Z",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c76",
          "name": "Danqing Huang",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c77",
          "name": "Ji Li",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c78",
          "user": {
            "_id": "631f108bb45367a05fe74260",
            "avatarUrl": "/avatars/c20da6800b643728062712d9a2771648.svg",
            "isPro": false,
            "fullname": "Researcher",
            "user": "YuanYuhui",
            "type": "user"
          },
          "name": "Yuhui Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:56:39.210Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T16:04:57.000Z",
      "submittedOnDailyAt": "2025-03-27T01:00:23.038Z",
      "title": "BizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen: Visualización y texto de nivel artículo en gráficos de información: avances\n\nBizGen",
      "submittedOnDailyBy": {
        "_id": "62333a88fd7bb4a39b92d387",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
        "isPro": false,
        "fullname": "Alex Jinpeng Wang",
        "user": "Awiny",
        "type": "user"
      },
      "summary": "Recientemente, modelos de generación de imágenes como Flux y Ideogram 2.0 han logrado importantes avances en la renderización visual de texto a nivel de documento. En este artículo, se centra en un escenario más complejo: el rendimiento visual de texto a nivel de negocio. Se aborda la creación de alta calidad de contenidos de negocio (grafos de información, presentaciones, etc.) basados en explicaciones de negocio proporcionadas por el usuario y un alto densidad de diseño. Los problemas básicos son la longitud de contexto larga y la escasez de datos de contenido de negocio de alta calidad.\n\nEn comparación con otros programas, en el contexto de contenidos de negocio, centrarse en múltiples sub-líneas y documentos de alto nivel de detalle es más difícil que mantener precisamente la alta densidad de diseño. Hemos realizado dos contribuciones técnicas principales: la construcción de un dataset de alta calidad y escalable de contenidos de negocio llamado Infographics-650K, y la implementación de un método de generación de infográficos basado en búsqueda por capas para preparar la alta densidad de diseño y el contenido de negocio. Además, hemos desarrollado un método de guiadado por sub-líneas que inserta y refina precisamente los sub-líneas durante la inferencia, con un enfoque de atención cruzada.\n\nMostramos los resultados potentes de nuestro sistema comparados con el sistema SORT DARK (Flux, SD3) en conjunto de conjuntos de explicaciones de alto nivel de detalle. Además, para comprobar el efecto de cada componente, realizamos experimentos de eliminación detallada. Esperamos que el desarrollo de Infographics-650K y BizEval impulsará el crecimiento de la generación de contenidos de negocio.",
      "upvotes": 7,
      "discussionId": "67e4b831672b3d9c9cb42ebb",
      "ai_keywords": [
        "scalable, high-quality business content dataset",
        "Infographics-650K",
        "layer-wise retrieval-augmented infographic generation scheme",
        "layout-guided cross attention scheme",
        "cropped region latent space",
        "layout conditional CFG",
        "BizEval prompt set",
        "ablation experiments"
      ]
    },
    "publishedAt": "2025-03-26T12:04:57.000Z",
    "title": "BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation",
    "summary": "Recently, state-of-the-art text-to-image generation models, such as Flux and\nIdeogram 2.0, have made significant progress in sentence-level visual text\nrendering. In this paper, we focus on the more challenging scenarios of\narticle-level visual text rendering and address a novel task of generating\nhigh-quality business content, including infographics and slides, based on user\nprovided article-level descriptive prompts and ultra-dense layouts. The\nfundamental challenges are twofold: significantly longer context lengths and\nthe scarcity of high-quality business content data.\n  In contrast to most previous works that focus on a limited number of\nsub-regions and sentence-level prompts, ensuring precise adherence to\nultra-dense layouts with tens or even hundreds of sub-regions in business\ncontent is far more challenging. We make two key technical contributions: (i)\nthe construction of scalable, high-quality business content dataset, i.e.,\nInfographics-650K, equipped with ultra-dense layouts and prompts by\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\nand (ii) a layout-guided cross attention scheme, which injects tens of\nregion-wise prompts into a set of cropped region latent space according to the\nultra-dense layouts, and refine each sub-regions flexibly during inference\nusing a layout conditional CFG.\n  We demonstrate the strong results of our system compared to previous SOTA\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\nconduct thorough ablation experiments to verify the effectiveness of each\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\nthe broader community to advance the progress of business content generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20672.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20757",
      "authors": [
        {
          "_id": "67e4c08fd9b7021d4a600fa4",
          "user": {
            "_id": "662b4e3bc709a61df840fda1",
            "avatarUrl": "/avatars/fc73c63a4e1f8fbb084ec43ec9af0af0.svg",
            "isPro": false,
            "fullname": "Hu Yunhai",
            "user": "AlexCCtop",
            "type": "user"
          },
          "name": "Yunhai Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:51.052Z",
          "hidden": false
        },
        {
          "_id": "67e4c08fd9b7021d4a600fa5",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-27T03:05:54.275Z",
          "hidden": false
        },
        {
          "_id": "67e4c08fd9b7021d4a600fa6",
          "user": {
            "_id": "660103ec4ae78d4ded4633fc",
            "avatarUrl": "/avatars/efce106d70f5d092bf44d0638aa49984.svg",
            "isPro": false,
            "fullname": "CHEN Zhao",
            "user": "chenzhao",
            "type": "user"
          },
          "name": "Chen Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:04.608Z",
          "hidden": false
        },
        {
          "_id": "67e4c08fd9b7021d4a600fa7",
          "user": {
            "_id": "5f5ba21188f57f65f951f255",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1599840760465-noauth.png",
            "isPro": false,
            "fullname": "Arman Cohan",
            "user": "armanc",
            "type": "user"
          },
          "name": "Arman Cohan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:57.092Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:46:08.000Z",
      "submittedOnDailyAt": "2025-03-27T01:36:43.674Z",
      "title": "MCTS-RAG: Monte Carlo Tree Search para Reinforcement Generation",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "MCTS-RAG es un nuevo enfoque para fortalecer las capacidades de inferencia de pequeños modelos de lenguaje. Este enfoque utiliza RAG (Retrieval-Augmented Generation) para proporcionar contextos relevantes y MCTS (Monte Carlo Tree Search) para refinar las rutas de inferencia, así como para abordar tareas de densidad de conocimiento. MCTS-RAG integra revisión y inferencia de manera dinámica a través de un proceso iterativo de toma de decisiones. A diferencia de los métodos RAG estándar, MCTS-RAG extrae revisión y inferencia de manera independiente, lo que no permite la óptima integración del conocimiento. Además, la inferencia tradicional de MCTS depende únicamente del conocimiento interno del modelo y no incluye hechos externos, por lo que MCTS-RAG combina una inferencia estructurada y una revisión adaptativa. Esta aproximación integrada fortalece la toma de decisiones, reduce la ambiguedad y asegura la precisión factual y la coherencia de las respuestas. A través de experimentos en conjuntos de datos de inferencia y densidad de conocimiento como ComplexWebQA, GPQA y FoolMeTwice, nuestro método demuestra que pequeños LMs pueden alcanzar niveles de rendimiento similares a GPT-4o, estableciendo nuevos estándares en la inferencia de pequeños modelos.",
      "upvotes": 6,
      "discussionId": "67e4c092d9b7021d4a60108b",
      "ai_keywords": [
        "MCTS-RAG",
        "retrieval-augmented generation (RAG)",
        "Monte Carlo Tree Search (MCTS)",
        "reasoning paths",
        "iterative decision-making",
        "knowledge suboptimally",
        "structured reasoning",
        "adaptive retrieval",
        "decision-making",
        "hallucinations",
        "factual accuracy",
        "response consistency",
        "ComplexWebQA",
        "GPQA",
        "FoolMeTwice",
        "small-scale LMs",
        "frontier LLMs (GPT-4o)",
        "scaling inference-time compute"
      ]
    },
    "publishedAt": "2025-03-26T13:46:08.000Z",
    "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search",
    "summary": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20757.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20020",
      "authors": [
        {
          "_id": "67e4b288fa81c69f446da710",
          "name": "Gemini Robotics Team",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da711",
          "user": {
            "_id": "61fd7ac3fbafe89f48101d83",
            "avatarUrl": "/avatars/cb2c86a04574498e71d6c447c2b289c1.svg",
            "isPro": false,
            "fullname": "Saminda Abeyruwan",
            "user": "saminda",
            "type": "user"
          },
          "name": "Saminda Abeyruwan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:16.630Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da712",
          "name": "Joshua Ainslie",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da713",
          "user": {
            "_id": "6253ee39e4e98393660b5c35",
            "avatarUrl": "/avatars/9c032f6a0729bfe5c16b3affe190834d.svg",
            "isPro": false,
            "fullname": "Jean-Baptiste Alayrac",
            "user": "jalayrac",
            "type": "user"
          },
          "name": "Jean-Baptiste Alayrac",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:27.846Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da714",
          "user": {
            "_id": "63f47a88121f9894707465ed",
            "avatarUrl": "/avatars/d85d409d19068aea02a2532b587dd1ef.svg",
            "isPro": false,
            "fullname": "Montserrat Gonzalez Arenas",
            "user": "montse90",
            "type": "user"
          },
          "name": "Montserrat Gonzalez Arenas",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:33.087Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da715",
          "user": {
            "_id": "66140283cf3fef4fa812e92f",
            "avatarUrl": "/avatars/033586adc3931d6c85bf9e84220992b4.svg",
            "isPro": false,
            "fullname": "Travis Armstrong",
            "user": "TravisAStrong",
            "type": "user"
          },
          "name": "Travis Armstrong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:39.411Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da716",
          "user": {
            "_id": "653ad36e5f1703225b266b7b",
            "avatarUrl": "/avatars/170e6b54a9859c7fca0289a09654c47f.svg",
            "isPro": false,
            "fullname": "Ashwin Balakrishna",
            "user": "abalakrishna123",
            "type": "user"
          },
          "name": "Ashwin Balakrishna",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:46.130Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da717",
          "name": "Robert Baruch",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da718",
          "name": "Maria Bauza",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da719",
          "name": "Michiel Blokzijl",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71a",
          "name": "Steven Bohez",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71b",
          "name": "Konstantinos Bousmalis",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71c",
          "name": "Anthony Brohan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71d",
          "name": "Thomas Buschmann",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71e",
          "name": "Arunkumar Byravan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71f",
          "name": "Serkan Cabi",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da720",
          "name": "Ken Caluwaerts",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da721",
          "name": "Federico Casarini",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da722",
          "name": "Oscar Chang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da723",
          "name": "Jose Enrique Chen",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da724",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da725",
          "name": "Hao-Tien Lewis Chiang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da726",
          "name": "Krzysztof Choromanski",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da727",
          "name": "David D'Ambrosio",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da728",
          "name": "Sudeep Dasari",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da729",
          "name": "Todor Davchev",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72a",
          "name": "Coline Devin",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72b",
          "user": {
            "_id": "62dac377f014388f908974f4",
            "avatarUrl": "/avatars/39bf0ca206441575a45f577060cdd8bc.svg",
            "isPro": false,
            "fullname": "Norman Di Palo",
            "user": "normandipalo",
            "type": "user"
          },
          "name": "Norman Di Palo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:09.232Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72c",
          "user": {
            "_id": "64d89da3bab152b24713108e",
            "avatarUrl": "/avatars/22e10f9a13b0d18b3a3b1f5281c7124d.svg",
            "isPro": false,
            "fullname": "Tianli Ding",
            "user": "Tding",
            "type": "user"
          },
          "name": "Tianli Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:18.146Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72d",
          "user": {
            "_id": "63b7d36469b7bd7324f9f438",
            "avatarUrl": "/avatars/67b1864c378102b1cf2de571cce7bf9a.svg",
            "isPro": false,
            "fullname": "Adil Dostmohamed",
            "user": "adild",
            "type": "user"
          },
          "name": "Adil Dostmohamed",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:29.517Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72e",
          "user": {
            "_id": "67225875b46c703941fa7967",
            "avatarUrl": "/avatars/7c89fbdd9a135210209bcd0cbfe7988a.svg",
            "isPro": false,
            "fullname": "Danny Driess",
            "user": "dannydriess",
            "type": "user"
          },
          "name": "Danny Driess",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:37.044Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72f",
          "user": {
            "_id": "63c9bd445fdc575773c732fe",
            "avatarUrl": "/avatars/def472d1ab3fbf751225357c0932ae7e.svg",
            "isPro": false,
            "fullname": "Yilun Du",
            "user": "yilundu",
            "type": "user"
          },
          "name": "Yilun Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:43.648Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da730",
          "name": "Debidatta Dwibedi",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da731",
          "user": {
            "_id": "66f6e9a737473469e871cae8",
            "avatarUrl": "/avatars/8c247380cee5d879aac204299963d3a7.svg",
            "isPro": false,
            "fullname": "Michael Elabd",
            "user": "michaelelabd",
            "type": "user"
          },
          "name": "Michael Elabd",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:55.368Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da732",
          "name": "Claudio Fantacci",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da733",
          "name": "Cody Fong",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da734",
          "name": "Erik Frey",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da735",
          "name": "Chuyuan Fu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da736",
          "name": "Marissa Giustina",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da737",
          "name": "Keerthana Gopalakrishnan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da738",
          "name": "Laura Graesser",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da739",
          "name": "Leonard Hasenclever",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73a",
          "name": "Nicolas Heess",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73b",
          "name": "Brandon Hernaez",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73c",
          "name": "Alexander Herzog",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73d",
          "name": "R. Alex Hofer",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73e",
          "name": "Jan Humplik",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73f",
          "name": "Atil Iscen",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da740",
          "name": "Mithun George Jacob",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da741",
          "name": "Deepali Jain",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da742",
          "name": "Ryan Julian",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da743",
          "user": {
            "_id": "64b7bf04a5018e3c7ca2ecda",
            "avatarUrl": "/avatars/4a434c344f68f2915c6e823262e62946.svg",
            "isPro": false,
            "fullname": "Dmitry Kalashnikov",
            "user": "dmitry-kalashnikov",
            "type": "user"
          },
          "name": "Dmitry Kalashnikov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:01:27.098Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da744",
          "name": "M. Emre Karagozler",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da745",
          "name": "Stefani Karp",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da746",
          "name": "Chase Kew",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da747",
          "name": "Jerad Kirkland",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da748",
          "name": "Sean Kirmani",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da749",
          "name": "Yuheng Kuang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74a",
          "user": {
            "_id": "631fa7e9124782a19efd20f2",
            "avatarUrl": "/avatars/456ef70cdb2a6a1f79a078746e96034a.svg",
            "isPro": false,
            "fullname": "Thomas Lampe",
            "user": "tlampe",
            "type": "user"
          },
          "name": "Thomas Lampe",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:01:36.252Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74b",
          "name": "Antoine Laurens",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74c",
          "name": "Isabel Leal",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74d",
          "name": "Alex X. Lee",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74e",
          "name": "Tsang-Wei Edward Lee",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74f",
          "name": "Jacky Liang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da750",
          "name": "Yixin Lin",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da751",
          "name": "Sharath Maddineni",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da752",
          "name": "Anirudha Majumdar",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da753",
          "name": "Assaf Hurwitz Michaely",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da754",
          "name": "Robert Moreno",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da755",
          "name": "Michael Neunert",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da756",
          "name": "Francesco Nori",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da757",
          "name": "Carolina Parada",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da758",
          "name": "Emilio Parisotto",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da759",
          "name": "Peter Pastor",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75a",
          "name": "Acorn Pooley",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75b",
          "name": "Kanishka Rao",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75c",
          "name": "Krista Reymann",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75d",
          "name": "Dorsa Sadigh",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75e",
          "name": "Stefano Saliceti",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75f",
          "name": "Pannag Sanketi",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da760",
          "name": "Pierre Sermanet",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da761",
          "name": "Dhruv Shah",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da762",
          "name": "Mohit Sharma",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da763",
          "name": "Kathryn Shea",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da764",
          "name": "Charles Shu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da765",
          "name": "Vikas Sindhwani",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da766",
          "name": "Sumeet Singh",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da767",
          "name": "Radu Soricut",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da768",
          "name": "Jost Tobias Springenberg",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da769",
          "name": "Rachel Sterneck",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76a",
          "name": "Razvan Surdulescu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76b",
          "name": "Jie Tan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76c",
          "name": "Jonathan Tompson",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76d",
          "name": "Vincent Vanhoucke",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76e",
          "name": "Jake Varley",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76f",
          "name": "Grace Vesom",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da770",
          "name": "Giulia Vezzani",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da771",
          "name": "Oriol Vinyals",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da772",
          "name": "Ayzaan Wahid",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da773",
          "name": "Stefan Welker",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da774",
          "name": "Paul Wohlhart",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da775",
          "name": "Fei Xia",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da776",
          "name": "Ted Xiao",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da777",
          "name": "Annie Xie",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da778",
          "name": "Jinyu Xie",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da779",
          "name": "Peng Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77a",
          "name": "Sichun Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77b",
          "name": "Ying Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77c",
          "name": "Zhuo Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77d",
          "name": "Yuxiang Yang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77e",
          "name": "Rui Yao",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77f",
          "name": "Sergey Yaroshenko",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da780",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da781",
          "name": "Wentao Yuan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da782",
          "name": "Jingwei Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da783",
          "name": "Tingnan Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da784",
          "name": "Allan Zhou",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da785",
          "name": "Yuxiang Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T19:02:56.000Z",
      "submittedOnDailyAt": "2025-03-27T00:36:27.703Z",
      "title": "Mini robotics: introducción de la inteligencia artificial en el mundo físico\n\n**Nota**: Este traducción es simplemente el resultado de la traducción del texto. No se proporcionan explicaciones o descripciones adicionales sobre el contenido o contexto. Para asegurar la precisión y profundidad del traducción, se recomienda revisar el texto traducido según las necesidades específicas.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "El desarrollo reciente de modelos grandes de multimodal ha demostrado una excelente capacidad general en el ámbito digital, pero su aplicación a robots y otros agentes físicos sigue siendo un problema crucial. Este informe presenta un nuevo modelo de IA diseñado específicamente para robots, construido en base al Gemini 2.0. Presentamos Gemini Robotics, un modelo VLA (Visión, Lenguaje, Acción) altamente efectivo para controlar directamente a un robot. Gemini Robotics realiza reacciones fluidas y dinámicas para abordar tareas complejas, es robusto frente a cambios en la naturaleza y ubicación de los objetos, puede adaptarse a entornos desconocidos y trabaja con una amplia gama de lenguajes abiertos. A través de ajustes adicionales, Gemini Robotics puede especializarse en nuevas capacidades, aprender tareas de plazo corto a partir de 100 tareas de plazo largo, dominar la estructura completamente nueva de un robot y resolver tareas de alta complejidad y duración. Esto es posible debido a que Gemini Robotics se basa en el modelo Gemini Robotics-ER, que ha expandido la capacidad de razonamiento físico del modelo Gemini, fortaleciendo la comprensión espacial y temporal. Así, Gemini Robotics-ER (Reasoning Embodied) puede desarrollar habilidades como la detección de objetos, orientación, predicción de rutas y objetos, respuesta a múltiples perspectivas y predicción de cajas de bounding 3D. Esta combinación apoya una variedad de aplicaciones en el robotecé. Además, se discuten y resuelven consideraciones de seguridad importantes relacionadas con este nuevo modelo de robotecé. La familia Gemini Robotics ha dado un paso importante en el desarrollo de robots generales que realicen la fuerza de la IA en el mundo físico.",
      "upvotes": 6,
      "discussionId": "67e4b28cfa81c69f446da8c7",
      "ai_keywords": [
        "Vision-Language-Action (VLA) generalist model",
        "multimodal model",
        "multimodal reasoning capabilities",
        "fine-tuning",
        "long-horizon, highly dexterous tasks",
        "short-horizon tasks",
        "robot embodiments",
        "embodied reasoning",
        "object detection",
        "pointing",
        "trajectory prediction",
        "grasp prediction",
        "multi-view correspondence",
        "3D bounding box predictions",
        "robotics applications",
        "safety considerations",
        "robotics foundation models",
        "general-purpose robots"
      ]
    },
    "publishedAt": "2025-03-25T15:02:56.000Z",
    "title": "Gemini Robotics: Bringing AI into the Physical World",
    "summary": "Recent advancements in large multimodal models have led to the emergence of\nremarkable generalist capabilities in digital domains, yet their translation to\nphysical agents such as robots remains a significant challenge. This report\nintroduces a new family of AI models purposefully designed for robotics and\nbuilt upon the foundation of Gemini 2.0. We present Gemini Robotics, an\nadvanced Vision-Language-Action (VLA) generalist model capable of directly\ncontrolling robots. Gemini Robotics executes smooth and reactive movements to\ntackle a wide range of complex manipulation tasks while also being robust to\nvariations in object types and positions, handling unseen environments as well\nas following diverse, open vocabulary instructions. We show that with\nadditional fine-tuning, Gemini Robotics can be specialized to new capabilities\nincluding solving long-horizon, highly dexterous tasks, learning new\nshort-horizon tasks from as few as 100 demonstrations and adapting to\ncompletely novel robot embodiments. This is made possible because Gemini\nRobotics builds on top of the Gemini Robotics-ER model, the second model we\nintroduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends\nGemini's multimodal reasoning capabilities into the physical world, with\nenhanced spatial and temporal understanding. This enables capabilities relevant\nto robotics including object detection, pointing, trajectory and grasp\nprediction, as well as multi-view correspondence and 3D bounding box\npredictions. We show how this novel combination can support a variety of\nrobotics applications. We also discuss and address important safety\nconsiderations related to this new class of robotics foundation models. The\nGemini Robotics family marks a substantial step towards developing\ngeneral-purpose robots that realizes AI's potential in the physical world.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20020.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19950",
      "authors": [
        {
          "_id": "67e4b27cfe1f5acc68028de9",
          "user": {
            "_id": "6399c67bf78f75ae73146760",
            "avatarUrl": "/avatars/d1c3b0eff67b598a1ad58e297382957f.svg",
            "isPro": false,
            "fullname": "CHEN Han",
            "user": "Concyclics",
            "type": "user"
          },
          "name": "Han Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:29.747Z",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028dea",
          "user": {
            "_id": "650ccf6a36ac7eba06ea1cfa",
            "avatarUrl": "/avatars/50374fca4f6cc2cf7a6601cd8d3f725b.svg",
            "isPro": false,
            "fullname": "Zicong Jiang",
            "user": "Zicong99",
            "type": "user"
          },
          "name": "Zicong Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:02:00.596Z",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028deb",
          "user": {
            "_id": "64b781c5da8017900e7b8b25",
            "avatarUrl": "/avatars/0db9a83b0908cc6b9417360ed77fcc1a.svg",
            "isPro": false,
            "fullname": "zining zhang",
            "user": "deciding",
            "type": "user"
          },
          "name": "Zining Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:02:07.675Z",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028dec",
          "name": "Bingsheng He",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028ded",
          "name": "Pingyi Luo",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028dee",
          "name": "Mian Lu",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028def",
          "name": "Yuqiang Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T16:24:45.000Z",
      "submittedOnDailyAt": "2025-03-27T02:17:05.033Z",
      "title": "LogQuant: Nivel superior de mantenimiento de precisión mediante cuantización de distribución de log de caché KV a 2 bits",
      "submittedOnDailyBy": {
        "_id": "6399c67bf78f75ae73146760",
        "avatarUrl": "/avatars/d1c3b0eff67b598a1ad58e297382957f.svg",
        "isPro": false,
        "fullname": "CHEN Han",
        "user": "Concyclics",
        "type": "user"
      },
      "summary": "LogQuant es una tecnología de reducción para la caché de KV en la inferencia de modelos de lenguaje grandes (LLM). Es una innovación que permite reducir significativamente la capacidad de memoria, manteniendo un rendimiento excelente. Los métodos existentes asumen que los tokens secundarios son importantes o utilizan patrones de atención más rápidos para predecir los tokens importantes. Sin embargo, ambos enfoques pueden presentar problemas como pérdida de rendimiento o errores de predicción.\n\nLogQuant utiliza una estructura de filtrado basada en logaritmos para reducir selectivamente la caché de KV en todo el contexto, lo que permite alcanzar un rendimiento mejorado, comparado con los métodos existentes, con la misma o menos cantidad de memoria. En pruebas de benchmark, logra aumentar el rendimiento de la transformación de 25% sin aumentar la consumo de memoria, y aumentar la tamaño de la lote en un 60%. En tareas complejas como matemáticas o completación de código, logra aumentar la precisión en un 40% a 200% en la misma proporción de computación, superando a las tecnologías comparativas.\n\nLogQuant se integra fácilmente con frameworks populares de inferencia como el librería transformers de Python. La implementación está disponible en https://github.com/Concyclics/LogQuantKV.",
      "upvotes": 4,
      "discussionId": "67e4b27efe1f5acc68028e72",
      "githubRepo": "https://github.com/Concyclics/LogQuantKV",
      "ai_keywords": [
        "KV Cache",
        "large language model (LLM)",
        "token",
        "attention pattern",
        "log-based filtering mechanism",
        "throughput",
        "batch size",
        "accuracy",
        "Math Completion",
        "Code Completion",
        "compression ratio",
        "transformers library"
      ]
    },
    "publishedAt": "2025-03-25T12:24:45.000Z",
    "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
    "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19950.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6399c67bf78f75ae73146760",
      "avatarUrl": "/avatars/d1c3b0eff67b598a1ad58e297382957f.svg",
      "fullname": "CHEN Han",
      "name": "Concyclics",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.19462",
      "authors": [
        {
          "_id": "67e3641cd8da46951f860d84",
          "user": {
            "_id": "645b8bf6438d6cfbe1ae47ae",
            "avatarUrl": "/avatars/3e4dc0523ba8e314b298567cf8f0c0ae.svg",
            "isPro": false,
            "fullname": "Haiyu Zhang",
            "user": "aejion",
            "type": "user"
          },
          "name": "Haiyu Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:02:45.776Z",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d85",
          "user": {
            "_id": "643e943a70c6a27621eb1c89",
            "avatarUrl": "/avatars/73ec521ab5ba84cc7908c52c0acef6ef.svg",
            "isPro": false,
            "fullname": "Xinyuan Chen",
            "user": "AriaChen",
            "type": "user"
          },
          "name": "Xinyuan Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:02:58.635Z",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d86",
          "user": {
            "_id": "63201256c6b20f03c829c4b8",
            "avatarUrl": "/avatars/a42092119777d65e60b12eb5ba0e45f1.svg",
            "isPro": false,
            "fullname": "Yaohui Wang",
            "user": "YaohuiW",
            "type": "user"
          },
          "name": "Yaohui Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:03:18.769Z",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d87",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:03:26.468Z",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d88",
          "name": "Yunhong Wang",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d89",
          "name": "Yu Qiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T08:52:07.000Z",
      "submittedOnDailyAt": "2025-03-27T00:39:48.103Z",
      "title": "AccVideo: Aceleración del Modelo de Difusión de Video Usando Datasets Sintéticos",
      "submittedOnDailyBy": {
        "_id": "645b8bf6438d6cfbe1ae47ae",
        "avatarUrl": "/avatars/3e4dc0523ba8e314b298567cf8f0c0ae.svg",
        "isPro": false,
        "fullname": "Haiyu Zhang",
        "user": "aejion",
        "type": "user"
      },
      "summary": "El modelo de difusión ha logrado un desarrollo significativo en la generación de películas. Sin embargo, debido a sus múltiples etapas de eliminación de ruido, se requieren muchas etapas de inferencia para crear una película, lo que es lento y requiere muchos cálculos. En este artículo, se realiza un análisis detallado de los problemas del método de difusión distillado existente y se propone un nuevo método eficiente llamado AccVideo para acelerar el modelo de eliminación de ruido en películas. Utilizando un conjunto de datos sintéticos, se generan múltiples puntos de datos válidos de eliminación de ruido con un modelo de eliminación de ruido entrenado, que se utilizan como conjunto de datos sintéticos para evitar el uso de puntos de datos inútiles en el proceso de eliminación de ruido. Basándose en este conjunto de datos sintéticos, se diseña una guía de pocas etapas basada en trayectorias para aprender la mapeo de la película desde el ruido. Además, el conjunto de datos sintéticos detectan la distribución de datos en cada etapa de eliminación de ruido, por lo que se introducen estrategias de aprendizaje adversario para ajustar la distribución de la salida del modelo estudiante a la distribución de nuestro conjunto de datos sintéticos y mejorar la calidad de la película. Según los experimentos extendidos, nuestro modelo logra un aumento en la velocidad de generación de un 8.5 veces en comparación con modelos supervisados, manteniendo una mejora relativa en el rendimiento. En comparación con los métodos de aceleración anteriores, nuestro enfoque permite la generación de películas de alta calidad y alta resolución, con un tamaño de 5 segundos, 720x1280 y 24 fps.",
      "upvotes": 4,
      "discussionId": "67e3641ed8da46951f860e12",
      "ai_keywords": [
        "diffusion models",
        "video generation",
        "iterative denoising",
        "inference steps",
        "diffusion distillation",
        "AccVideo",
        "synthetic dataset",
        "pretrained video diffusion model",
        "denoising trajectories",
        "trajectory-based few-step guidance",
        "noise-to-video mapping",
        "adversarial training strategy"
      ]
    },
    "publishedAt": "2025-03-25T04:52:07.000Z",
    "title": "AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset",
    "summary": "Diffusion models have achieved remarkable progress in the field of video\ngeneration. However, their iterative denoising nature requires a large number\nof inference steps to generate a video, which is slow and computationally\nexpensive. In this paper, we begin with a detailed analysis of the challenges\npresent in existing diffusion distillation methods and propose a novel\nefficient method, namely AccVideo, to reduce the inference steps for\naccelerating video diffusion models with synthetic dataset. We leverage the\npretrained video diffusion model to generate multiple valid denoising\ntrajectories as our synthetic dataset, which eliminates the use of useless data\npoints during distillation. Based on the synthetic dataset, we design a\ntrajectory-based few-step guidance that utilizes key data points from the\ndenoising trajectories to learn the noise-to-video mapping, enabling video\ngeneration in fewer steps. Furthermore, since the synthetic dataset captures\nthe data distribution at each diffusion timestep, we introduce an adversarial\ntraining strategy to align the output distribution of the student model with\nthat of our synthetic dataset, thereby enhancing the video quality. Extensive\nexperiments demonstrate that our model achieves 8.5x improvements in generation\nspeed compared to the teacher model while maintaining comparable performance.\nCompared to previous accelerating methods, our approach is capable of\ngenerating videos with higher quality and resolution, i.e., 5-seconds,\n720x1280, 24fps.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19462.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b8bf6438d6cfbe1ae47ae",
      "avatarUrl": "/avatars/3e4dc0523ba8e314b298567cf8f0c0ae.svg",
      "fullname": "Haiyu Zhang",
      "name": "aejion",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20756",
      "authors": [
        {
          "_id": "67e4b0b850ca38886d7e78d0",
          "name": "Chenxi Wang",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d1",
          "name": "Jizhan Fang",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d2",
          "name": "Xiang Chen",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d3",
          "name": "Bozhong Tian",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d4",
          "name": "Ziwen Xu",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d5",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d6",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:45:29.000Z",
      "submittedOnDailyAt": "2025-03-27T00:28:51.612Z",
      "title": "ADS-Edit: ADS-Edit es un conjunto de datos educativos de diversos campos de entrenamiento para sistemas de conducción autónoma.",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los grandes modelos multimodal (LMMs) ha demostrado efectos deseados en los sistemas de conducción autónoma (ADS). Sin embargo, en su aplicación directa a los ADS, se encuentran limitados por errores en la comprensión del conocimiento de tránsito, situaciones complejas de la carretera y diferentes estados del vehículo, entre otros problemas. Para enfrentar estos desafíos, se propone una solución que permita modificar el comportamiento del modelo de manera específica, evitando la necesidad de entrenamiento completo. Además, se presenta un conjunto de datos de edición de conocimiento para ADS, llamado \"ADS-Edit\", que incluye escenarios reales, múltiples tipos de datos y criterios de evaluación detallados. Mediante experimentos concretos, se obtuvieron numerosos conclusiones interesantes. Nuestra investigación busca contribuir al desarrollo de aplicaciones de edición de conocimiento en los sistemas de conducción autónoma. Los códigos y datos están disponibles en https://github.com/zjunlp/EasyEdit.",
      "upvotes": 3,
      "discussionId": "67e4b0bb50ca38886d7e79d0",
      "ai_keywords": [
        "Knowledge Editing",
        "ADS-Edit",
        "multimodal knowledge editing dataset",
        "autonomous driving systems"
      ]
    },
    "publishedAt": "2025-03-26T13:45:29.000Z",
    "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\n  Systems",
    "summary": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20756.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20271",
      "authors": [
        {
          "_id": "67e4dc6a38e4d1444c71ce70",
          "name": "Haoqin Tu",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce71",
          "name": "Weitao Feng",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce72",
          "name": "Hardy Chen",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce73",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce74",
          "name": "Xianfeng Tang",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce75",
          "name": "Cihang Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T06:38:31.000Z",
      "submittedOnDailyAt": "2025-03-27T06:03:39.751Z",
      "title": "ViLBench: Sistema de Modelación de Compensación de Modelos de Procesamiento de Lenguaje Visual",
      "submittedOnDailyBy": {
        "_id": "604ae011caabafacfa48e3de",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1615519738679-noauth.jpeg",
        "isPro": false,
        "fullname": "Haoqin Tu",
        "user": "PahaII",
        "type": "user"
      },
      "summary": "El modelo de recompensa de observación del proceso tiene la capacidad de elegir adecuadamente la trayectoria causal de tareas complejas, y proporciona retroalimentación detallada en cada etapa. Su excelencia se manifiesta además en que la evaluación de los PRMs es particularmente insuficiente en muchos modelos, y para llenar este vacío, este artículo compara por primera vez dos modelos de lenguaje visual (VLLMs) utilizando dos modelos de recompensa, demostrando que ninguno de ellos muestra excelencia consistente en cada tarea. Además, muestra que los VLLMs excelentes no necesariamente mejoran su rendimiento con la recompensa. Para fomentar el desarrollo de los VLLMs, se diseña el VilBench, un benchmark de lenguaje visual que requiere de señales de recompensa de proceso rigurosas, con el objetivo de mejorar la evaluación de los VLLMs actuales. En particular, OpenAI's GPT-4o alcanza una precisión de 27.3% utilizando Chain-of-Thought (CoT), lo que refleja las dificultades en la evaluación actual de los VLLMs. Finalmente, para demostrar la posibilidad de llenar el vacío entre VLLMs generales y modelos de recompensa, se recopila 73.6K datos de procesos de lenguaje visual de recompensa utilizando un algoritmo de búsqueda de árbol extendido, y se demuestra que un modelo de 3B mejora en promedio en 3.3% con respecto a CoT, y que cuando se elige la generación de OpenAI o1, se obtiene una mejora de 2.5% en comparación con sus componentes no entrenados. La implementación está disponible en https://ucsc-vlaa.github.io/VilBench.",
      "upvotes": 2,
      "discussionId": "67e4dc6b38e4d1444c71cee2",
      "ai_keywords": [
        "vision large language models (VLLMs)",
        "output reward models (ORMs)",
        "process reward models (PRMs)",
        "vision-language benchmarks",
        "Chain-of-Thought (CoT)",
        "vision-language process reward data",
        "tree-search algorithm",
        "ViLBench"
      ]
    },
    "publishedAt": "2025-03-26T02:38:31.000Z",
    "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling",
    "summary": "Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\nand data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20271.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "604ae011caabafacfa48e3de",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1615519738679-noauth.jpeg",
      "fullname": "Haoqin Tu",
      "name": "PahaII",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20198",
      "authors": [
        {
          "_id": "67e4b98039509b0149142daa",
          "name": "Alex Jinpeng Wang",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dab",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dac",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dad",
          "name": "Lijuan Wang",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dae",
          "name": "Min Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T03:44:25.000Z",
      "submittedOnDailyAt": "2025-03-27T01:06:03.239Z",
      "title": "WORD más lejos: Avances en la generación de imágenes largas mediante modelos automáticos de Diálogo de Imagenes",
      "submittedOnDailyBy": {
        "_id": "62333a88fd7bb4a39b92d387",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
        "isPro": false,
        "fullname": "Alex Jinpeng Wang",
        "user": "Awiny",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de modelos automáticos de restauración y de ramas ha logrado un excelente rendimiento en la generación de imágenes de breves frases. Sin embargo, una de las principales limitaciones de los modelos generativos actuales es la capacidad de generar complejas y largas frases sobre imágenes (por ejemplo, párrafos de slides o documentos). Presentamos un estudio pionero en la generación de imágenes de largas frases, abordando una de las deficiencias principales de los sistemas de imagen de frases: su tendencia a procesar solo cortas frases o palabras. A través de un análisis detallado de los modelos de generación automática de vanguardia, se reconoció que el tokenizador de imágenes es un límite crucial en la calidad de la generación de frases. Para resolver este problema, se introdujo un tokenizador de frases y se optimizaron las características específicas de frases en escenas concretas. Este tokenizador fue expandido para desarrollar un modelo automático de restauración multifuncional para la generación de alta calidad de imágenes de largas frases. Este modelo ofrece una fuerte control sobre características de la frase como estilo de letra, tamaño, color y disposición, permitiendo una mayor flexibilidad y precisión en la generación. Los experimentos extendidos comparados con DALL-E 3, SD3.5 Large y GPT4o demostraron altas precisión, consistencia y flexibilidad en la generación de largas frases. Este modelo abre nuevas oportunidades en aplicaciones creativas como la generación cruzada de documentos y slides, y se dirige hacia un nuevo horizonte en la generación de imágenes de largas frases.",
      "upvotes": 2,
      "discussionId": "67e4b98439509b0149142f3c",
      "ai_keywords": [
        "autoregressive models",
        "diffusion models",
        "image tokenizer",
        "binary tokenizer",
        "multimodal autoregressive model",
        "font style",
        "text properties",
        "size",
        "color",
        "alignment",
        "SD3.5 Large",
        "GPT4o",
        "DALL-E 3",
        "long-text image generation"
      ]
    },
    "publishedAt": "2025-03-25T23:44:25.000Z",
    "title": "Beyond Words: Advancing Long-Text Image Generation via Multimodal\n  Autoregressive Models",
    "summary": "Recent advancements in autoregressive and diffusion models have led to strong\nperformance in image generation with short scene text words. However,\ngenerating coherent, long-form text in images, such as paragraphs in slides or\ndocuments, remains a major challenge for current generative models. We present\nthe first work specifically focused on long text image generation, addressing a\ncritical gap in existing text-to-image systems that typically handle only brief\nphrases or single sentences. Through comprehensive analysis of state-of-the-art\nautoregressive generation models, we identify the image tokenizer as a critical\nbottleneck in text generating quality. To address this, we introduce a novel\ntext-focused, binary tokenizer optimized for capturing detailed scene text\nfeatures. Leveraging our tokenizer, we develop \\ModelName, a multimodal\nautoregressive model that excels in generating high-quality long-text images\nwith unprecedented fidelity. Our model offers robust controllability, enabling\ncustomization of text properties such as font style, size, color, and\nalignment. Extensive experiments demonstrate that \\ModelName~significantly\noutperforms SD3.5 Large~sd3 and GPT4o~gpt4o with DALL-E\n3~dalle3 in generating long text accurately, consistently, and flexibly.\nBeyond its technical achievements, \\ModelName~opens up exciting opportunities\nfor innovative applications like interleaved document and PowerPoint\ngeneration, establishing a new frontier in long-text image generating.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20198.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19846",
      "authors": [
        {
          "_id": "67e4c449672b3d9c9cb8aa4b",
          "user": {
            "_id": "67e4c444692ba54248a6b337",
            "avatarUrl": "/avatars/7695a30d185ccb2354129c72538e9180.svg",
            "isPro": false,
            "fullname": "Aaron Serianni",
            "user": "serianni",
            "type": "user"
          },
          "name": "Aaron Serianni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:27.573Z",
          "hidden": false
        },
        {
          "_id": "67e4c449672b3d9c9cb8aa4c",
          "name": "Tyler Zhu",
          "hidden": false
        },
        {
          "_id": "67e4c449672b3d9c9cb8aa4d",
          "name": "Olga Russakovsky",
          "hidden": false
        },
        {
          "_id": "67e4c449672b3d9c9cb8aa4e",
          "name": "Vikram V. Ramaswamy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:11:39.000Z",
      "submittedOnDailyAt": "2025-03-27T01:57:35.164Z",
      "title": "Atención IoU: Ranking IoU: Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking Ranking",
      "submittedOnDailyBy": {
        "_id": "67e4c444692ba54248a6b337",
        "avatarUrl": "/avatars/7695a30d185ccb2354129c72538e9180.svg",
        "isPro": false,
        "fullname": "Aaron Serianni",
        "user": "serianni",
        "type": "user"
      },
      "summary": "Los modelos de visión computacional muestran la capacidad de representar y expandir sesgos en amplios conjuntos de datos y tareas. Los métodos existentes para cuantificar los sesgos en modelos de clasificación de clases se centran en la distribución del conjunto de datos y el rendimiento de subgrupos del modelo, pero se olvidan de analizar las funciones internas del modelo. Presentamos puntuaciones relacionadas con el métrica Attention-IoU (Attention Intersection over Union), que utilizan mapas de atención para claramente identificar sesgos en las representaciones internas del modelo, y para determinar las características de las imágenes que pueden provocar sesgos. Primero, validamos la Attention-IoU en el conjunto de datos de datos sintéticos \"Waterbirds\", mostrando cómo esta métrica mide precisamente los sesgos del modelo. Luego, analizamos el conjunto de datos CelebA y demostramos que la Attention-IoU evidencia una correlación más clara que la diferencia de precisión. Investigamos métodos específicos de representación de sesgos en CelebA mediante la investigación de características individuales utilizando el atributo protegido \"Male\". Finalmente, cambiamos la relación entre atributos al submuestrear parcialmente el conjunto de entrenamiento, y la Attention-IoU permite identificar variables latentes potenciales que no están incluidas en los etiquetas del conjunto de datos.",
      "upvotes": 2,
      "discussionId": "67e4c44a672b3d9c9cb8aaae",
      "ai_keywords": [
        "Attention-IoU",
        "attention maps",
        "internal representations",
        "image features",
        "Waterbirds dataset",
        "CelebA dataset",
        "accuracy disparities",
        "protected attribute",
        "attribute correlations",
        "confounding variables"
      ]
    },
    "publishedAt": "2025-03-25T13:11:39.000Z",
    "title": "Attention IoU: Examining Biases in CelebA using Attention Maps",
    "summary": "Computer vision models have been shown to exhibit and amplify biases across a\nwide array of datasets and tasks. Existing methods for quantifying bias in\nclassification models primarily focus on dataset distribution and model\nperformance on subgroups, overlooking the internal workings of a model. We\nintroduce the Attention-IoU (Attention Intersection over Union) metric and\nrelated scores, which use attention maps to reveal biases within a model's\ninternal representations and identify image features potentially causing the\nbiases. First, we validate Attention-IoU on the synthetic Waterbirds dataset,\nshowing that the metric accurately measures model bias. We then analyze the\nCelebA dataset, finding that Attention-IoU uncovers correlations beyond\naccuracy disparities. Through an investigation of individual attributes through\nthe protected attribute of Male, we examine the distinct ways biases are\nrepresented in CelebA. Lastly, by subsampling the training set to change\nattribute correlations, we demonstrate that Attention-IoU reveals potential\nconfounding variables not present in dataset labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19846.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "67e4c444692ba54248a6b337",
      "avatarUrl": "/avatars/7695a30d185ccb2354129c72538e9180.svg",
      "fullname": "Aaron Serianni",
      "name": "serianni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20220",
      "authors": [
        {
          "_id": "67e4b035af7d0551dc377e13",
          "name": "Weijie Guo",
          "hidden": false
        },
        {
          "_id": "67e4b035af7d0551dc377e14",
          "name": "Guofeng Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b035af7d0551dc377e15",
          "name": "Wufei Ma",
          "hidden": false
        },
        {
          "_id": "67e4b035af7d0551dc377e16",
          "name": "Alan Yuille",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T04:23:53.000Z",
      "submittedOnDailyAt": "2025-03-27T00:26:48.304Z",
      "title": "DINeMo: Método para entrenar modelos de malla 3D sin anotaciones 3D",
      "submittedOnDailyBy": {
        "_id": "625f81afe1994410eef1c36a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650426282769-noauth.jpeg",
        "isPro": false,
        "fullname": "Wufei Ma",
        "user": "wufeim",
        "type": "user"
      },
      "summary": "La estimación de posiciones 3D/6D en categorías es un paso crucial para la comprensión de escenas 3D y puede ser ampliamente aplicada en el campo de la robótica y el AI concreto. Recientes estudios han analizado modelos neurales de malla basados en la síntesis desde una perspectiva de la comprensión basada en la síntesis, mejorando significativamente la robustez frente a la parcialidad de la visión y el cambio de dominio. Sin embargo, estos métodos dependen fuertemente del aprendizaje de etiquetas 3D por comparación de partes, lo que limita su expansión eficiente y su aplicación a categorías limitadas.\n\nEn este estudio, se presenta un nuevo modelo neural de malla, denominado DINeMo, que no requiere etiquetado 3D, utilizando un Factory Robot Composition Stage (Escenario de Composición de Robot de Fabrica) obtenido de modelos visuales de gran escala. Se adopta un método de generación de escenarios de robot de fabrica en ambos sentidos, utilizando tanto características locales como información de contexto global para crear escenarios de robot de fabrica. Los resultados de experimentos en conjuntos de datos de automóviles muestran que nuestro modelo DINeMo supera significativamente a los estudios previos en la estimación de posiciones 3D 0-shot y few-shot, reduciendo el error comparado con métodos completos en 67.3%. DINeMo puede expandirse eficientemente incluyendo imágenes sin etiquetas adicionales durante el entrenamiento y muestra un rendimiento superior a los modelos de aprendizaje por sub-supervision que dependen de etiquetado 3D. Para más información, el sitio web del proyecto está disponible en https://analysis-by-synthesis.github.io/DINeMo/.",
      "upvotes": 1,
      "discussionId": "67e4b038af7d0551dc377f07",
      "projectPage": "https://analysis-by-synthesis.github.io/DINeMo/",
      "ai_keywords": [
        "neural mesh models",
        "analysis-by-synthesis",
        "robustness",
        "partial occlusion",
        "domain shifts",
        "part-contrastive learning",
        "pseudo-correspondence",
        "visual foundation models",
        "bidirectional pseudo-correspondence generation",
        "local appearance features",
        "global context information",
        "zero-shot",
        "few-shot",
        "fully-supervised methods"
      ]
    },
    "publishedAt": "2025-03-26T00:23:53.000Z",
    "title": "DINeMo: Learning Neural Mesh Models with no 3D Annotations",
    "summary": "Category-level 3D/6D pose estimation is a crucial step towards comprehensive\n3D scene understanding, which would enable a broad range of applications in\nrobotics and embodied AI. Recent works explored neural mesh models that\napproach a range of 2D and 3D tasks from an analysis-by-synthesis perspective.\nDespite the largely enhanced robustness to partial occlusion and domain shifts,\nthese methods depended heavily on 3D annotations for part-contrastive learning,\nwhich confines them to a narrow set of categories and hinders efficient\nscaling. In this work, we present DINeMo, a novel neural mesh model that is\ntrained with no 3D annotations by leveraging pseudo-correspondence obtained\nfrom large visual foundation models. We adopt a bidirectional\npseudo-correspondence generation method, which produce pseudo correspondence\nutilize both local appearance features and global context information.\nExperimental results on car datasets demonstrate that our DINeMo outperforms\nprevious zero- and few-shot 3D pose estimation by a wide margin, narrowing the\ngap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively\nand efficiently when incorporating more unlabeled images during training, which\ndemonstrate the advantages over supervised learning methods that rely on 3D\nannotations. Our project page is available at\nhttps://analysis-by-synthesis.github.io/DINeMo/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20220.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625f81afe1994410eef1c36a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650426282769-noauth.jpeg",
      "fullname": "Wufei Ma",
      "name": "wufeim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19953",
      "authors": [
        {
          "_id": "67e4b46cc90e5edf25f581f8",
          "name": "Stefan Stojanov",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581f9",
          "name": "David Wendt",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fa",
          "name": "Seungwoo Kim",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fb",
          "name": "Rahul Venkatesh",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fc",
          "name": "Kevin Feigelis",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fd",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fe",
          "name": "Daniel LK Yamins",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:58:52.000Z",
      "submittedOnDailyAt": "2025-03-27T00:44:41.359Z",
      "title": "Optimización del reconocimiento automático de conceptos de acción mediante aprendizaje parámetrico de contadores",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Estimar movimientos en video es uno de los importantes problemas de visión computacional que incluye varias aplicaciones subyacentes. Este problema abarca áreas como la generación de vídeo controlable o la robótica. Actualmente, las soluciones principales se basan en modelos entrenados con datos sintéticos o en la ajuste de heurísticas apropiadas para cada situación, lo cual limita la capacidad del modelo en contextos reales. Recientemente, ha habido un avance significativo en la aprendizaje automático de grandes escalas en vídeo, pero la aplicación de estas representaciones para el estimar movimientos en vídeo ha sido menos estudiada. En este trabajo, se desarrolló una técnica de reconocimiento automático para estimar flujos y ocultas usando un modelo de predicción de la siguiente frame ya entrenado. Opt-CWM utiliza un modelo de vídeo básico con entradas de vídeo real para extraer información de movimiento, evitando así la necesidad de heurísticas fijas. Este enfoque logra alcanzar los mejores resultados en la estimación de movimientos en vídeo real, sin necesidad de datos estándar.",
      "upvotes": 1,
      "discussionId": "67e4b46ec90e5edf25f582db",
      "ai_keywords": [
        "self-supervised learning",
        "flow estimation",
        "occlusion estimation",
        "next-frame prediction model",
        "counterfactual probes",
        "motion estimation"
      ]
    },
    "publishedAt": "2025-03-25T13:58:52.000Z",
    "title": "Self-Supervised Learning of Motion Concepts by Optimizing\n  Counterfactuals",
    "summary": "Estimating motion in videos is an essential computer vision problem with many\ndownstream applications, including controllable video generation and robotics.\nCurrent solutions are primarily trained using synthetic data or require tuning\nof situation-specific heuristics, which inherently limits these models'\ncapabilities in real-world contexts. Despite recent developments in large-scale\nself-supervised learning from videos, leveraging such representations for\nmotion estimation remains relatively underexplored. In this work, we develop\nOpt-CWM, a self-supervised technique for flow and occlusion estimation from a\npre-trained next-frame prediction model. Opt-CWM works by learning to optimize\ncounterfactual probes that extract motion information from a base video model,\navoiding the need for fixed heuristics while training on unrestricted video\ninputs. We achieve state-of-the-art performance for motion estimation on\nreal-world videos while requiring no labeled data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19953.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17358",
      "authors": [
        {
          "_id": "67e3b41d0fa8f886db6b323a",
          "name": "Jerred Chen",
          "hidden": false
        },
        {
          "_id": "67e3b41d0fa8f886db6b323b",
          "name": "Ronald Clark",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/jIZXKaHvYvl_B_Tg_SAeY.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/cn0aTY48FezRlyiSMberj.mp4"
      ],
      "publishedAt": "2025-03-21T17:58:56.000Z",
      "submittedOnDailyAt": "2025-03-27T07:45:14.887Z",
      "title": "El video estima el movimiento de la cámara a partir del brucior de movimiento de la IMU.",
      "submittedOnDailyBy": {
        "_id": "6305ee63d70693fdf1c7dbb8",
        "avatarUrl": "/avatars/0e81ed3757b4e65be82063b538c3fe49.svg",
        "isPro": false,
        "fullname": "Ronald Clark",
        "user": "r0nn13",
        "type": "user"
      },
      "summary": "En aplicaciones de robótica y VR/AR, las altas velocidades de movimiento de cámaras generan altos niveles de blur y métodos existentes para medir la posición de la cámara fallan. En este artículo, proponemos un nuevo marco para funcionalizar el blur como un gran canal de medición de movimiento y mejorar los métodos que fallan. Nuestro enfoque funciona predeciendo directamente de una imagen de blur, un campo de flujo de movimiento rico y una mapa de profundidad de objetos. Luego, asumimos pequeñas movidas para resolver un problema de mínimos cuadrados lineal y reconstruir la velocidad instantánea de la cámara. Fundamentalmente, nuestro método proporciona una medición robusta, similar a la de IMU, para detección de altas velocidades y movimientos agresivos. Para entrenar el modelo, construimos un gran conjunto de datos con movimientos de blur realistas sintéticos obtenidos de ScanNet++v2, y entrenamos el modelo completamente diferenciable hasta el final de los datos reales para mejorarlo. En evaluaciones extensas en benchmarks de la realidad, nuestro método realiza la medición más avanzada de velocidad angular y velocidad de movimiento en comparación con los métodos actuales, demostrando superar técnicas como MASt3R y COLMAP.",
      "upvotes": 1,
      "discussionId": "67e3b4200fa8f886db6b3328",
      "projectPage": "https://jerredchen.github.io/image-as-imu/",
      "ai_keywords": [
        "motion blur",
        "camera pose estimation",
        "motion flow field",
        "monocular depth map",
        "linear least squares problem",
        "small motion assumption",
        "IMU-like measurement",
        "ScanNet++v2",
        "fully differentiable pipeline",
        "real-world benchmarks",
        "angular velocity estimates",
        "translational velocity estimates"
      ]
    },
    "publishedAt": "2025-03-21T13:58:56.000Z",
    "title": "Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred\n  Image",
    "summary": "In many robotics and VR/AR applications, fast camera motions cause a high\nlevel of motion blur, causing existing camera pose estimation methods to fail.\nIn this work, we propose a novel framework that leverages motion blur as a rich\ncue for motion estimation rather than treating it as an unwanted artifact. Our\napproach works by predicting a dense motion flow field and a monocular depth\nmap directly from a single motion-blurred image. We then recover the\ninstantaneous camera velocity by solving a linear least squares problem under\nthe small motion assumption. In essence, our method produces an IMU-like\nmeasurement that robustly captures fast and aggressive camera movements. To\ntrain our model, we construct a large-scale dataset with realistic synthetic\nmotion blur derived from ScanNet++v2 and further refine our model by training\nend-to-end on real data using our fully differentiable pipeline. Extensive\nevaluations on real-world benchmarks demonstrate that our method achieves\nstate-of-the-art angular and translational velocity estimates, outperforming\ncurrent methods like MASt3R and COLMAP.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/jIZXKaHvYvl_B_Tg_SAeY.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/cn0aTY48FezRlyiSMberj.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17358.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6305ee63d70693fdf1c7dbb8",
      "avatarUrl": "/avatars/0e81ed3757b4e65be82063b538c3fe49.svg",
      "fullname": "Ronald Clark",
      "name": "r0nn13",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16870",
      "authors": [
        {
          "_id": "67e4c860136c8a867191e52e",
          "user": {
            "_id": "6627ff2a3b4fbc8420a416c3",
            "avatarUrl": "/avatars/de3aafdaf5563fe25edcdb92b394474f.svg",
            "isPro": false,
            "fullname": "AR",
            "user": "Anshumann",
            "type": "user"
          },
          "name": "Anshumann",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:03:45.930Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e52f",
          "user": {
            "_id": "61765fe0b0715831eab6d465",
            "avatarUrl": "/avatars/830406cebdd7ddf66799bf71f6bbff4b.svg",
            "isPro": false,
            "fullname": "Mohd Abbas Zaidi",
            "user": "ya-mehdi",
            "type": "user"
          },
          "name": "Mohd Abbas Zaidi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:03:57.163Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e530",
          "name": "Akhil Kedia",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e531",
          "user": {
            "_id": "65d61eebee922bc7a777d5a6",
            "avatarUrl": "/avatars/180efd503ef412b4dc728e6aa477c16e.svg",
            "isPro": false,
            "fullname": "Jinwoo Ahn",
            "user": "AndrewAhn",
            "type": "user"
          },
          "name": "Jinwoo Ahn",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:07.687Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e532",
          "user": {
            "_id": "629059cdb90dde28ef5cbb30",
            "avatarUrl": "/avatars/451b0e8999447b3a2f03378fe98c0661.svg",
            "isPro": false,
            "fullname": "Taehwak Kwon",
            "user": "Rock222",
            "type": "user"
          },
          "name": "Taehwak Kwon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:23.731Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e533",
          "user": {
            "_id": "6354137306d707b332451421",
            "avatarUrl": "/avatars/46770f32702e3ad08f91faeef9e4ea6e.svg",
            "isPro": false,
            "fullname": "Kangwook Lee",
            "user": "kw1jjang",
            "type": "user"
          },
          "name": "Kangwook Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:33.400Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e534",
          "user": {
            "_id": "653b6e7232bd4db35d981615",
            "avatarUrl": "/avatars/0fbf0c5d502b840bf26baf8c420c7593.svg",
            "isPro": false,
            "fullname": "Haejun Lee",
            "user": "haejunlee",
            "type": "user"
          },
          "name": "Haejun Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:39.155Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e535",
          "user": {
            "_id": "64b4be0665a7e15eac085878",
            "avatarUrl": "/avatars/dd19ec2987fb0735457c6492b53aacfe.svg",
            "isPro": false,
            "fullname": "Joo-hyung Lee",
            "user": "snrbs17",
            "type": "user"
          },
          "name": "Joohyung Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:46.702Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T05:58:18.000Z",
      "submittedOnDailyAt": "2025-03-27T02:09:27.795Z",
      "title": "Sparse Logit Sampling: Método para Acelerar la Conversión de Conocimientos en LLM",
      "submittedOnDailyBy": {
        "_id": "61765fe0b0715831eab6d465",
        "avatarUrl": "/avatars/830406cebdd7ddf66799bf71f6bbff4b.svg",
        "isPro": false,
        "fullname": "Mohd Abbas Zaidi",
        "user": "ya-mehdi",
        "type": "user"
      },
      "summary": "La conversión de conocimientos es un método eficiente para transformar el conocimiento de modelos de lenguaje grandes en un momento en que la función logística de salida del profesor se ha calculado previamente y se ha caché. Sin embargo, aún no se ha explorado extensamente para aplicarlo exitosamente en entrenamiento previo. En este estudio, se propone una aproximación intuitiva para la conversión de conocimientos raros (por ejemplo, caché de los Top-K de probabilidades) que hace que la distribución de probabilidades del profesor se afecte hacia el estudiante, pero no se conecte con el mejor rendimiento y la compensación. Proponemos la \"Conversión de Conocimientos Aleatoria\" basada en muestras de importancia, que proporciona evaluaciones sin sesgo, mantiene la inclinación y almacena funciones logísticas muy raras de manera energética, sin perder precisión. Este método puede acelerar el entrenamiento del modelo estudiante en menos de 10% de ajustes comparado con entrenamiento basado en entropía cruzada, y mantiene una conversión completa y competentes en modelos de diferentes tamaños, desde 300M hasta 3B.",
      "upvotes": 1,
      "discussionId": "67e4c861136c8a867191e58c",
      "ai_keywords": [
        "Knowledge distillation",
        "Large Language Models",
        "teacher output logits",
        "pre-computed",
        "cached",
        "sparse knowledge distillation",
        "Top-K probabilities",
        "biased estimates",
        "teacher probability distribution",
        "student",
        "suboptimal performance",
        "calibration",
        "importance-sampling-based method",
        "Random Sampling Knowledge Distillation",
        "unbiased estimates",
        "gradient in expectation",
        "storing significantly sparser logits",
        "cross-entropy based training",
        "competitive performance",
        "model sizes"
      ]
    },
    "publishedAt": "2025-03-21T01:58:18.000Z",
    "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
    "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61765fe0b0715831eab6d465",
      "avatarUrl": "/avatars/830406cebdd7ddf66799bf71f6bbff4b.svg",
      "fullname": "Mohd Abbas Zaidi",
      "name": "ya-mehdi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]