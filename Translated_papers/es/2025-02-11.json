[
  {
    "paper": {
      "id": "2502.06394",
      "authors": [
        {
          "_id": "67aafead3711ca5b760f324c",
          "user": {
            "_id": "61ade264f602880813dbe10b",
            "avatarUrl": "/avatars/a92dea7d853bbabbf60b351c207b6875.svg",
            "isPro": false,
            "fullname": "Daniil Moskovskiy",
            "user": "etomoscow",
            "type": "user"
          },
          "name": "Daniil Moskovskiy",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:17.448Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f324d",
          "user": {
            "_id": "634c72e6fe1bfa967d6c2b5c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634c72e6fe1bfa967d6c2b5c/WFWIAlWl-FsiJRyGxQTTx.jpeg",
            "isPro": false,
            "fullname": "Nikita Sushko",
            "user": "chameleon-lizard",
            "type": "user"
          },
          "name": "Nikita Sushko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:21.453Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f324e",
          "user": {
            "_id": "5dfa8e07da6d0311fd3d5430",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651090418656-5dfa8e07da6d0311fd3d5430.png",
            "isPro": false,
            "fullname": "Sergey Pletenev",
            "user": "memyprokotow",
            "type": "user"
          },
          "name": "Sergey Pletenev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:59:47.063Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f324f",
          "user": {
            "_id": "662f8d645c4db70c77a203b0",
            "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
            "isPro": false,
            "fullname": "Elena Tutubalina",
            "user": "tlenusik",
            "type": "user"
          },
          "name": "Elena Tutubalina",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:59:50.003Z",
          "hidden": false
        },
        {
          "_id": "67aafead3711ca5b760f3250",
          "name": "Alexander Panchenko",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T12:30:25.000Z",
      "title": "SynthDetoxM: Modern LLMs son annotators de un único paso de diseño paralelo de datos de feedback detoxification.",
      "summary": "Actualmente, el tratamiento de detección de tóxicos en textos multilingües está limitado por la escasez de conjuntos de datos paralelos multilingües. En este estudio, proponemos una solución mediante la implementación de un pipeline para la generación de datos de detección de tóxicos multilingües. Además, utilizamos el conjunto de datos de textos multilingües de detección de tóxicos, SynthDetoxM, que se ha recopilado de mano y generado sintéticamente, para crear 16,000 pares de consultas de detección de tóxicos de alta calidad en inglés, francés, español y ruso. Este conjunto de datos ha sido construido con diferentes conjuntos de datos de evaluación de tóxicos y ha sido reescrito con parámetros de entrenamiento en varios modelos de lenguaje libre de código modernos (LLM). Los resultados de los experimentos muestran que los modelos entrenados con este conjunto de datos sintético superan a los modelos entrenados con el conjunto de datos MultiParaDetox annotados por humanos, incluso cuando se trata de situaciones con limitaciones de datos. Los modelos entrenados con SynthDetoxM superan a todos los LLM evaluados. Publicamos nuestro conjunto de datos y código para contribuir más al campo de la investigación en detección de tóxicos en textos multilingües.",
      "upvotes": 55,
      "discussionId": "67aafeae3711ca5b760f3280"
    },
    "publishedAt": "2025-02-11T03:03:12.135Z",
    "title": "SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06394.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61ade264f602880813dbe10b",
      "avatarUrl": "/avatars/a92dea7d853bbabbf60b351c207b6875.svg",
      "fullname": "Daniil Moskovskiy",
      "name": "etomoscow",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06781",
      "authors": [
        {
          "_id": "67aacd7e078cdf445284f9f6",
          "name": "Chengqi Lyu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f7",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f8",
          "name": "Yuzhe Gu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9f9",
          "user": {
            "_id": "64e8505321540e1da3226b54",
            "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
            "isPro": false,
            "fullname": "Wenwei Zhang",
            "user": "ZwwWayne",
            "type": "user"
          },
          "name": "Wenwei Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:40.279Z",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fa",
          "name": "Jianfei Gao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fb",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fc",
          "name": "Ziyi Wang",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fd",
          "name": "Shuaibin Li",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9fe",
          "name": "Qian Zhao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284f9ff",
          "name": "Haian Huang",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa00",
          "name": "Weihan Cao",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa01",
          "name": "Jiangning Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa02",
          "name": "Hongwei Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa03",
          "name": "Junnan Liu",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa04",
          "user": {
            "_id": "630716d11801ecc7d2595021",
            "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
            "isPro": false,
            "fullname": "Songyang Zhang",
            "user": "zsytony",
            "type": "user"
          },
          "name": "Songyang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:37.733Z",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa05",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67aacd7e078cdf445284fa06",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:57:29.000Z",
      "title": "El límite de la matemática del aprendizaje de los logros y las recompensas",
      "summary": "Uno de los elementos clave del inteligencia artificial, la capacidad de inferencia, especialmente la capacidad para resolver problemas matemáticos complejos, ha mostrado un desarrollo reciente que ha recibido atención, como en los modelos de series de OpenAI. Sin embargo, la tecnología completa no está publicada, y se supone que la tecnología utilizada es Reinforcement Learning (RL) y pensamiento en cadenas largas. En este artículo, proponemos un nuevo marco de RL llamado OREAL para alcanzar el límite de la performance posible en tareas de inferencia matemática basado en recompensas basadas en resultados. Este marco de RL proporciona una demostración teórica que el aprendizaje de estrategias óptimas KL-normalizadas puede ser suficiente cuando se pueden obtener recompensas binarias de resultados. Esta fórmula también significa que deben garantizar la consistencia del gradiente entre positivos y negativos. Para mitigar el problema de recompensas raras que ha existido en RL durante mucho tiempo, aplicamos un modelo de recompensas basado en tokens adicionalmente, que muestra tokens importantes para el aprendizaje en las rutas de inferencia, considerando la precisión parcial de pensamiento en cadenas largas en tareas de inferencia matemática. Mediante OREAL, un modelo de 7B ha logrado por primera vez una precisión de 94.0 pass@1 en MATH-500, lo que puede compararse con un modelo de 32B. El OREAL-32B alcanzó una precisión de 95.0 pass@1 en MATH-500, superando a un modelo de 32B previamente entrenado por distillation. Nuestro estudio destaca la importancia de los modelos de estrategias iniciales y las consultas de entrenamiento de RL. Código, modelos y datos se publicarán para fomentar futuras investigaciones. https://github.com/InternLM/OREAL.",
      "upvotes": 30,
      "discussionId": "67aacd7f078cdf445284fa4b"
    },
    "publishedAt": "2025-02-10T23:18:11.727Z",
    "title": "Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06781.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06703",
      "authors": [
        {
          "_id": "67aabf93c0f8648f68c68ce4",
          "user": {
            "_id": "667187ba9ab144eb3ac43a1b",
            "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
            "isPro": false,
            "fullname": "Runze Liu",
            "user": "RyanLiu112",
            "type": "user"
          },
          "name": "Runze Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:22.940Z",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce5",
          "name": "Junqi Gao",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce6",
          "name": "Jian Zhao",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce7",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:18.725Z",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce8",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ce9",
          "name": "Biqing Qi",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68cea",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "67aabf93c0f8648f68c68ceb",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T17:30:23.000Z",
      "title": "¿El 1B LLM puede superar al 405B LLM? Considerar la optimización del tamaño de tiempo de prueba para aumentar la capacidad.",
      "summary": "TTS (Test Time Scaling) es un método importante para mejorar el rendimiento de grandes modelos de lenguaje (LLMs) durante la fase de inferencia, utilizando cálculos adicionales. Sin embargo, actualmente los estudios no realizan un análisis sistemático sobre cómo afecta TTS a los modelos de política, Process Reward Models (PRMs) y los problemas. Este deficiente análisis limita la comprensión y la aplicación práctica de TTS. En este artículo, nos centramos en dos preguntas clave: (1) ¿Qué es el mejor enfoque para escalar el cálculo de TTS, considerando el nivel de dificultad de los modelos de política, PRMs y los problemas? (2) ¿Cómo afecta el cálculo adicional a la mejora del rendimiento de LLMs en tareas complejas, y ¿es posible que un pequeño modelo de lenguaje supere un grande modelo con esta aproximación? Realizamos experimentos detallados en MATH-500 y tareas desafiantes como AIME24, y obtuvimos los siguientes resultados: (1) La estrategia de TTS optimizada por cálculo depende muy del modelo de política, PRM y el nivel de dificultad del problema. (2) Al utilizar la estrategia de TTS optimizada por cálculo, un pequeño modelo de política puede superar un modelo grande. Por ejemplo, un 1B LLM supera a un 405B LLM en MATH-500. Además, en MATH-500 y AIME24, un 0.5B LLM supera a GPT-4o, un 3B LLM supera a un 405B LLM, y un 7B LLM supera a o1 y DeepSeek-R1, mostrando una alta eficiencia de inferencia. Estos resultados demuestran que es crucial que la estrategia de TTS se adapte a las características específicas de cada tarea y modelo, y que TTS es un enfoque potencialmente prometedor para mejorar el rendimiento de LLMs.",
      "upvotes": 25,
      "discussionId": "67aabf94c0f8648f68c68d19"
    },
    "publishedAt": "2025-02-11T00:36:11.270Z",
    "title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05609",
      "authors": [
        {
          "_id": "67aacaaaa03eecbc2d72835f",
          "user": {
            "_id": "64ec4c04c782d648d28d70fc",
            "avatarUrl": "/avatars/6975526fcf4b513cc934b5bc45370a48.svg",
            "isPro": false,
            "fullname": "Sukmin Cho",
            "user": "zomss",
            "type": "user"
          },
          "name": "Sukmin Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:43.377Z",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728360",
          "name": "Sangjin Choi",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728361",
          "user": {
            "_id": "64d1e70a84f205869017703b",
            "avatarUrl": "/avatars/215d0d4db5f79cb74df4d888b18c6a0d.svg",
            "isPro": false,
            "fullname": "Taeho Hwang",
            "user": "doubleyyh",
            "type": "user"
          },
          "name": "Taeho Hwang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:45.737Z",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728362",
          "name": "Jeongyeon Seo",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728363",
          "name": "Soyeong Jeong",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728364",
          "name": "Huije Lee",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728365",
          "name": "Hoyun Song",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728366",
          "name": "Jong C. Park",
          "hidden": false
        },
        {
          "_id": "67aacaaaa03eecbc2d728367",
          "name": "Youngjin Kwon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T15:32:53.000Z",
      "title": "Una escritura jerárquica basada en la proximidad temporal para acelerar el modelo de lenguaje de gran escala sin distorsión",
      "summary": "La mejora de la velocidad de inferencia en los modelos de lenguaje grande (LLMs) es crucial en interacciones en tiempo real. Una de las soluciones algoritmicas que ha recibido mucha atención para mejorar la velocidad de inferencia es el método de \"preview\" de tokens, que permite generar múltiples tokens en una sola caché hacia adelante después de ver y validar su precisión previamente. Sin embargo, las estrategias actuales de \"preview\" requieren a menudo una ajuste micro importante o muestran desempeños desbalanceados para cada tarea. Para resolver estos problemas, proponemos un enfoque de \"preview\" sin pérdida basado en la localidad temporal, llamado Hierarchy Drafting (HD), que utiliza una estructura multi-capa para configurar diferentes fuentes de tokens. En el paso de \"preview\", HD accede secuencialmente a varios bases de datos, desde la mayor localidad hasta la menor, garantizando un acelero consistente en diferentes tareas y minimizando la demora del \"preview\". Los experimentos en Spec-Bench muestran que, utilizando modelos de LLMs con 7B y 13B parámetros, HD logra una mejora significativa en la velocidad de inferencia, superando a los métodos de \"preview\" de base de datos existentes y demostrando un fuerte aumento en la velocidad de inferencia independientemente del tamaño del modelo, la tarea y la temperatura.",
      "upvotes": 12,
      "discussionId": "67aacaaca03eecbc2d728394"
    },
    "publishedAt": "2025-02-10T22:58:41.471Z",
    "title": "Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05609.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec4c04c782d648d28d70fc",
      "avatarUrl": "/avatars/6975526fcf4b513cc934b5bc45370a48.svg",
      "fullname": "Sukmin Cho",
      "name": "zomss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05415",
      "authors": [
        {
          "_id": "67aaea0a0acaa007694aed73",
          "user": {
            "_id": "65708920806dee337da0eef5",
            "avatarUrl": "/avatars/945e328dedc8e1e3111f48c344ad5b03.svg",
            "isPro": false,
            "fullname": "xuchenkai",
            "user": "UnhurriedDawn",
            "type": "user"
          },
          "name": "Chenkai Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:28.861Z",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed74",
          "user": {
            "_id": "6644548a3a16452261cdb173",
            "avatarUrl": "/avatars/4643db904204e3a60202a29e8c884139.svg",
            "isPro": false,
            "fullname": "wangxu",
            "user": "asunalove",
            "type": "user"
          },
          "name": "Xu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:26.432Z",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed75",
          "name": "Zhenyi Liao",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed76",
          "name": "Yishun Li",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed77",
          "name": "Tianqi Hou",
          "hidden": false
        },
        {
          "_id": "67aaea0a0acaa007694aed78",
          "user": {
            "_id": "64bba541da140e461924dfed",
            "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
            "isPro": false,
            "fullname": "zhijie deng",
            "user": "zhijie3",
            "type": "user"
          },
          "name": "Zhijie Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:54:24.089Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T02:52:25.000Z",
      "title": "Show Ottoverbal: Accelerando la comprensión y generación de monomodales integrados",
      "summary": "Se está aumentando el interés en la investigación en ciertos campos, y en particular, Show-o está recibiendo mucha atención, destacando por su excelente rendimiento en la generación de imágenes a partir de texto y viceversa. El aprendizaje de Show-o se enfrenta a problemas en la evolución de los tokens de imagen y la recuperación automática de tokens de texto, ambos enfrentando desafíos. En este artículo, se presenta Show-o Turbo para mejorar la conexión entre estos grupos. Primero, se identifica una perspectiva unificada de desdenización basada en la explicación paralela de tokens de texto en la generación de imágenes y texto en Show-o. Luego, se extiende el diseño de desdenización multimodal de Show-o con un diseño de estilo consistente (CD) para proporcionar una aproximación adecuada para reducir el proceso de desdenización del modelo de difusión. Se introduce una estrategia de división del proyecto y un proceso de cribado para mejorar la convergencia del entrenamiento. Experimentalmente, en la generación de imágenes a partir de texto, el uso de CFG sin utilizar se logra un score de GenEval de 0.625 en 4 pasos, superando a Show-o original. En la generación de texto a partir de imágenes, se muestra una mejora de 1.5 veces en la velocidad sin perder significativamente el rendimiento. El código está disponible en https://github.com/zhijie-group/Show-o-Turbo.",
      "upvotes": 9,
      "discussionId": "67aaea100acaa007694aeea5"
    },
    "publishedAt": "2025-02-11T02:09:27.778Z",
    "title": "Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bba541da140e461924dfed",
      "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
      "fullname": "zhijie deng",
      "name": "zhijie3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06772",
      "authors": [
        {
          "_id": "67aac8adfe33f6d8d695bc40",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc41",
          "name": "Zhaochen Yu",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc42",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "67aac8adfe33f6d8d695bc43",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:51:47.000Z",
      "title": "ReasonFlux: Escalado de templates para lógica de LLM jerárquica",
      "summary": "Proponemos la teoría lógica de un LLM heurístico utilizando un simple template de pensamiento escalable, demostrando que podemos superar la capacidad lógica matemática de un LLM fuerte (como OpenAI o1-preview, DeepSeek V3). Entrenamos el modelo ReasonFlux-32B en 8 GAFs y introducimos tres innovaciones: (i) una biblioteca de templates de pensamiento estructurados de géneros comunes, que incluye aproximadamente 500 templates de pensamiento de alto nivel y permite generalizar problemas lógicos similares o relacionados; (ii) realizamos un aprendizaje reforzado heurístico con una secuencia de templates de pensamiento, optimizando el planificación del trayecto óptimo para resolver problemas complejos de manera iterativa; (iii) introducimos un nuevo sistema de escalado de inferencia, que permite adaptar los templates de pensamiento para facilitar la teoría lógica de un LLM heurístico. Al tener un trayecto de templates de pensamiento que incluye un orden lógico, ReasonFlux-32B alcanza un nivel de lógica matemática de primera clase. En particular, alcanza una precisión del 91.2% en el benchmark MATH, superando a o1-preview en un 6.7%. En el benchmark USA Math Olympiad (AIME), ReasonFlux-32B resuelve un promedio del 56.7% de los problemas, superando a o1-preview en un 27% y a DeepSeek-V3 en un 45%. Código: https://github.com/Gen-Verse/ReasonFlux",
      "upvotes": 9,
      "discussionId": "67aac8affe33f6d8d695bcbd"
    },
    "publishedAt": "2025-02-10T22:49:56.390Z",
    "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06772.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06049",
      "authors": [
        {
          "_id": "67aac01bd7b18841e7c266df",
          "name": "Jikun Kang",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e0",
          "name": "Wenqi Wu",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e1",
          "name": "Filippos Christianos",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e2",
          "name": "Alex J. Chan",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e3",
          "name": "Fraser Greenlee",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e4",
          "name": "George Thomas",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e5",
          "name": "Marvin Purtorab",
          "hidden": false
        },
        {
          "_id": "67aac01bd7b18841e7c266e6",
          "name": "Andy Toulis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T22:11:42.000Z",
      "title": "Dae Jiyeo Mudeol",
      "summary": "En este artículo se presenta el modelo de memoria grande (LM2). LM2 es una arquitectura transformer basada en un analizador que resuelve las limitaciones de la inferencia paso a paso, lógica relacional y la síntesis de información dispersa en contextos largos, agregando un módulo de memoria auxiliar. El propuesto LM2 ejecuta la función de un repositorio de representaciones de contexto, interactúa con los tokens de entrada y la atención cruzada, y actualiza su estado mediante una estructura de gates. Para mantener las funciones de un transformer general, LM2 integra el paso de memoria auxiliar mientras mantiene el flujo de información original. Los resultados de los experimentos en el benchmark BABILong muestran que el modelo LM2 obtiene resultados promedio 37.1% más altos que el modelo RMT y 86.3% más altos que el modelo Llama-3.2 básico. LM2 demuestra habilidades especializadas en la inferencia paso a paso, lógica numérica y respuestas a contextos largos. En el conjunto de datos MMLU, se logró un mejoramiento del 5.0% en comparación con la versión entrenada, demostrando que su módulo de memoria no degrada el rendimiento en tareas generales. Además, se revisaron la explicabilidad de la memoria, el efecto del módulo de memoria y el comportamiento durante la prueba. Nuestros hallazgos subrayan la importancia de la memoria explícita para fortalecer la arquitectura transformer.",
      "upvotes": 8,
      "discussionId": "67aac01dd7b18841e7c26739"
    },
    "publishedAt": "2025-02-10T22:13:17.117Z",
    "title": "LM2: Large Memory Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06049.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6489e10ca13f65198dc6e122",
      "avatarUrl": "/avatars/4aa9eab488157711b2f0298ddadee2f4.svg",
      "fullname": "Kang",
      "name": "JaxonK",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03628",
      "authors": [
        {
          "_id": "67aab82e6024056209d727a8",
          "name": "Zhuowei Li",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727a9",
          "name": "Haizhou Shi",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727aa",
          "name": "Yunhe Gao",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ab",
          "name": "Di Liu",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ac",
          "name": "Zhenting Wang",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ad",
          "name": "Yuxiao Chen",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727ae",
          "name": "Ting Liu",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727af",
          "name": "Long Zhao",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727b0",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "67aab82e6024056209d727b1",
          "name": "Dimitris N. Metaxas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T21:34:02.000Z",
      "title": "El secreto de los tokens: la reducción de la hashmisión en grandes visión lenguaje modelos mediante el control de la información visual",
      "summary": "El modelo de lenguaje de visión de la pantalla (LVLMs) es capaz de inferir contexto y entradas visuales efectivamente, pero tienden a imaginar contenido que es gramaticalmente coherente pero no basado en información visual. En este artículo, se investiga el orden de los tokenes de la raíz para revelar tres patrones principales que se presentan en el procesamiento de información en LVLMs: 1) pérdida de información visual en curso - los tokenes basados en información visual tienen su prioridad gradualmente disminuida durante el proceso de generación. 2) anticipación inicial - tokenes con significado tienen un pico de actividad más rápido que los capas finales. 3) información oculta - tokenes basados en información visual se deciden finalmente, pero mantienen una posición relativamente alta durante la inferencia. Basándose en estos patrones, se propone VISTA (framework de intervención de la información visual mediante el aumento de la raíz de tokenes). VISTA es un framework de inferencia que no requiere un supervisor externo, reduce la imaginación y promueve la información real. VISTA utiliza la fortalecimiento de la información visual en el espacio activo y la actividad en las capas iniciales para promover la decodificación con significado. Comparado con los métodos existentes, VISTA puede aplicarse a diversas estrategias de decodificación sin necesidad de un supervisor externo. A través de experimentos extendidos, VISTA redució la imaginación en aproximadamente el 40% y obtuvo resultados excelentes en cuatro marcos de referencia, combinando tres estrategias de decodificación, superando a los métodos existentes.",
      "upvotes": 8,
      "discussionId": "67aab82f6024056209d727f6"
    },
    "publishedAt": "2025-02-10T21:38:53.032Z",
    "title": "The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03628.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64dfcc62e8b6f3f3baa950e0",
      "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
      "fullname": "Zhenting Wang",
      "name": "ztwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06786",
      "authors": [
        {
          "_id": "67aae91b83b1182df7c0cf54",
          "name": "Pranav Nair",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf55",
          "name": "Puranjay Datta",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf56",
          "name": "Jeff Dean",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf57",
          "name": "Prateek Jain",
          "hidden": false
        },
        {
          "_id": "67aae91b83b1182df7c0cf58",
          "name": "Aditya Kusupati",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:59:10.000Z",
      "title": "MATRIZOHA KUANCHITUSHONG",
      "summary": "La cuantificación de los pesos de un modelo tiene un significado fundamental para reducir los costos de comunicación y de inferencia en grandes modelos. Sin embargo, la cuantificación del modelo (especialmente en niveles de precisión bajos como int4 o int2) puede tener un efecto negativo en la calidad del modelo. En particular, int2 ha sido conocido por afectar significativamente la calidad del modelo. Por lo tanto, los profesionales generalmente tienen que manejar varios modelos con diferentes niveles de cuantificación o ofrecer un modelo que satisface tanto la calidad como la pérdida de latencia. En cambio, los tipos de datos enteros (como int8) tienen la estructura de 'martyrical' que envuelve los enteros de pequeños bit-size (como int4 o int2) en los más significativos. Este artículo propone una nueva tecnología de cuantificación a diferentes escalas llamada 'MatQuant', con el objetivo de abordar la necesidad de modelos cuantificados en diferentes niveles de precisión. Esta tecnología permite entrenar un modelo y proporcionarlo en diferentes niveles de precisión. Además, debido a la colaborativa entrenamiento y normalización en MatQuant, los modelos obtenidos con int2 en MatQuant pueden tener una precisión superior en un 10% más que los modelos cuantificados estándarmente con int2 (como QAT o OmniQuant). Esto demuestra que, utilizando la misma receta, un modelo Gemma-2 9B con FFN cuantificado a int2 puede tener una precisión superior a un modelo Gemma-2 2B con FFN cuantificado a int8, mostrando el avance en la cuantificación de modelos.",
      "upvotes": 7,
      "discussionId": "67aae91d83b1182df7c0cff6"
    },
    "publishedAt": "2025-02-11T01:07:50.116Z",
    "title": "Matryoshka Quantization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06786.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06788",
      "authors": [
        {
          "_id": "67aac64de37429ebdbdafc40",
          "name": "Haiwen Diao",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc41",
          "name": "Xiaotong Li",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc42",
          "name": "Yufeng Cui",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc43",
          "name": "Yueze Wang",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc44",
          "name": "Haoge Deng",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc45",
          "user": {
            "_id": "6565bc5ee5aac326bfc98e39",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vIfHy9Y1yAK6A96UCHNBH.jpeg",
            "isPro": false,
            "fullname": "Ting Pan",
            "user": "PhyscalX",
            "type": "user"
          },
          "name": "Ting Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:09.401Z",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc46",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc47",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "67aac64de37429ebdbdafc48",
          "name": "Xinlong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:59:58.000Z",
      "title": "EVEv2: Mejora de la línea base básica sin encoder de modelos de visión-lenguaje",
      "summary": "Los modelos de lenguaje visual (VLMs) sin encoder actuales están reduciendo rápidamente la diferencia de rendimiento con los modelos basados en encoder. Esto proporciona una evidencia clara sobre la posibilidad de modelos estructuralmente simples y eficientes. Hemos analizado profundamente el rendimiento de VLMs sin encoder, construidos utilizando un encoder previamente entrenado, un tokenizador discreto y capas minimalistas de capa visual. Hemos desarrollado una estrategia eficiente que permite a los VLMs sin encoder competir con los modelos basados en encoder en términos de rendimiento. A través de una investigación detallada, hemos presentado a la comunidad el nuevo y mejor VLM sin encoder, EVEv2.0. Mostramos: (i) la capacidad de decomposir y conectar adecuadamente la visión y el lenguaje en un solo modelo, reduciendo así el entrelazamiento entre modelos. (ii) Un buen estrategia de entrenamiento facilita la optimización efectiva de los VLMs sin encoder. Mediante evaluaciones ampliadas, demostramos que nuestro EVEv2.0 muestra una arquitectura desarrollada solo en el decodificador, con alta eficiencia en datos y un fuerte rendimiento en reconocimiento visual. El código está disponible para uso público: https://github.com/baaivision/EVE.",
      "upvotes": 6,
      "discussionId": "67aac64ee37429ebdbdafc96"
    },
    "publishedAt": "2025-02-10T22:40:39.442Z",
    "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06788.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4a717aa03b6520839e9b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
      "fullname": "Haiwen Diao",
      "name": "Paranioar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06782",
      "authors": [
        {
          "_id": "67aae76c71a9983f50e134ef",
          "name": "Dongyang Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f0",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f1",
          "name": "Yutong Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f2",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f3",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f4",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f5",
          "name": "Qi Qin",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f6",
          "name": "Yufei Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f7",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f8",
          "name": "Zhongyu Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134f9",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fa",
          "name": "Chenyang Si",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fb",
          "name": "Yuewen Cao",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fc",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fd",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134fe",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e134ff",
          "name": "Qibin Hou",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e13500",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "67aae76c71a9983f50e13501",
          "name": "Peng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:58:11.000Z",
      "title": "Lumina-Video: Next-DiT para la generación de vídeos eficientes y flexibles a través de escalas múltiples",
      "summary": "El último desarrollo ha colocado a los Transformers de Difusión (DiTs) como el principal marco de modelado generativo. Basándose en este éxito, Lumina-Next ha logrado un rendimiento muy alto en la generación de imágenes realistas utilizando Next-DiT. Sin embargo, la posibilidad de generar vídeos no ha sido significativamente desarrollada, ya que existen grandes desafíos para modelar la complejidad espacio-temporal inherente a los datos de vídeo. En respuesta a esto, presentamos Lumina-Video. Este marco de trabajo utiliza las fortalezas de Next-DiT y introduce soluciones adecuadas para la síntesis de vídeos. Lumina-Video utiliza una arquitectura multiescala de Next-DiT, lo que permite garantizar tanto eficiencia como flexibilidad durante el entrenamiento. Además, utilizando el puntaje de movimiento como condición explícita, Lumina-Video puede controlar directamente la dinámica de los vídeos generados. Este enfoque combina un esquema de entrenamiento avanzado, que aumenta la resolución y la FPS, con un esquema de entrenamiento más flexible para entrenar junto a datos de síntesis natural, asegurando una alta eficiencia tanto en el entrenamiento como en la inferencia, así como una excelente calidad artística y fluidez. Además, proponemos Lumina-V2A, un modelo basado en Next-DiT para generar audio a partir de vídeos. Este modelo tiene como objetivo generar vídeos y sonidos sincronizados. El código está disponible en https://www.github.com/Alpha-VLLM/Lumina-Video.",
      "upvotes": 5,
      "discussionId": "67aae76e71a9983f50e1357d"
    },
    "publishedAt": "2025-02-11T01:00:25.383Z",
    "title": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05431",
      "authors": [
        {
          "_id": "67aac392385da1f07cc7fcbd",
          "user": {
            "_id": "64f58b970b24e548a85522bc",
            "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
            "isPro": false,
            "fullname": "Xinyu Yang",
            "user": "Hanyuezhuohua",
            "type": "user"
          },
          "name": "Xinyu Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:13.131Z",
          "hidden": false
        },
        {
          "_id": "67aac392385da1f07cc7fcbe",
          "name": "Tianqi Chen",
          "hidden": false
        },
        {
          "_id": "67aac392385da1f07cc7fcbf",
          "name": "Beidi Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T03:41:16.000Z",
      "title": "APE: Generación de Acción y Percepción en Entornos Públicos y Privados",
      "summary": "El método de generación con augmentación de contexto (CAG), incluyendo RAG y ICL, requiere la eficiente combinación de múltiples contextos para generar respuestas a las solicitudes del usuario. La entrada secuencial de estos contextos puede imponer una gran carga computacional debido a la combinación de contextos, por lo que se deben recodificar cada solicitud. Para resolver este problema, se revisa la posibilidad de codificación paralela, donde se calculan y cachean independientemente los estados KV de cada contexto. Este enfoque permite que, en el momento de la inferencia, se lean directamente los estados cacheados, lo que permite aumentar la cantidad de contextos. Sin embargo, la asimetría de la distribución de la atención puede significativamente afectar el rendimiento si se aplica la codificación paralela directamente. Para abordar este problema, se propone la Adaptive Parallel Encoding (APE), que ajusta la distribución de la codificación paralela a la de la codificación secuencial utilizando un prefijo común, una temperatura de atención y un factor de escalado. En los resultados de las tareas de RAG y ICL, se observa que con el mismo input, la codificación secuencial mantiene un rendimiento de 98% y 93%, mientras que la codificación paralela supera en 3.6% y 7.9%. Además, la APE puede aplicarse a múltiples ejemplos de CAG y codificar cientos de contextos al mismo tiempo. Según la evaluación de eficiencia, la APE reduce el tiempo de predicción para contextos de 128K de longitud en un 28 veces y puede realizar un aumento de velocidad de hasta 4.5 veces.",
      "upvotes": 5,
      "discussionId": "67aac393385da1f07cc7fd17"
    },
    "publishedAt": "2025-02-10T22:29:36.102Z",
    "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05431.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64f58b970b24e548a85522bc",
      "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
      "fullname": "Xinyu Yang",
      "name": "Hanyuezhuohua",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06155",
      "authors": [
        {
          "_id": "67aab9b4a2bf5e5ea03d4c19",
          "user": {
            "_id": "643a451ee2b979ae6141329d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a451ee2b979ae6141329d/HN3M5vyroanQoUEiXJFyB.jpeg",
            "isPro": false,
            "fullname": "Hangliang Ding",
            "user": "foreverpiano",
            "type": "user"
          },
          "name": "Hangliang Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:29.115Z",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1a",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1b",
          "name": "Runlong Su",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1c",
          "name": "Peiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1d",
          "user": {
            "_id": "64bba541da140e461924dfed",
            "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
            "isPro": false,
            "fullname": "zhijie deng",
            "user": "zhijie3",
            "type": "user"
          },
          "name": "Zhijie Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:25.471Z",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1e",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "67aab9b4a2bf5e5ea03d4c1f",
          "name": "Hao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T05:00:56.000Z",
      "title": "Efficient-vDiT: Transformador de Attención de Video Dipénsión Eficiente",
      "summary": "DiTs tiene una estructura completamente de atención 3D, lo que permite la síntesis de vídeos de alta calidad, pero su costo de computación de atención y muchos pasos de muestreo lo hace caro. Por ejemplo, el modelo Open-Sora-Plan, muy popular, requiere más de 9 minutos para generar una sola vídeo de 29 frame. En este artículo, abordamos estas desventajas desde dos perspectivas: 1) reducimos la atención completa 3D basándonos en el desuso en los datos de vídeo. Identificamos patrones de recreación tipo tiling que son visibles en los mapas de atención 3D de los datos de vídeo, y proponemos una nueva familia de atenciones esparsas 3D con complejidad lineal con respecto al número de frames de vídeo. 2) Introducimos un diseño de múltiples pasos de consistencia para cortar el proceso de muestreo. Dividimos el trazado de muestreo en etapas que pueden ser resueltas, y en cada etapa realizamos un diseño de consistencia para activar la capacidad de múltiples pasos de generación. Además, diseñamos un proceso de entrenamiento en tres etapas para integrar la atención de baja complejidad y la capacidad de generación de múltiples pasos. En particular, utilizando solo 0.1% de datos previamente entrenados, podemos crear el modelo Open-Sora-Plan-1.2 7.4x-7.8x más rápido y ajustar su rendimiento en VBench. Además, nuestro enfoque permite aplicar la distribuida computación y, utilizando programación paralela de secuencias en 4 gráficos, podemos lograr un aumento adicional de velocidad del 3.91x.",
      "upvotes": 5,
      "discussionId": "67aab9bca2bf5e5ea03d4e3c"
    },
    "publishedAt": "2025-02-10T22:09:58.181Z",
    "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06155.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63565cc56d7fcf1bedb7d347",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
      "fullname": "Zhang Peiyuan",
      "name": "PY007",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 82
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06527",
      "authors": [
        {
          "_id": "67aae4128d478dcb4b39a097",
          "name": "D. She",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a098",
          "name": "Mushui Liu",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a099",
          "name": "Jingxuan Pang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09a",
          "name": "Jin Wang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09b",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09c",
          "name": "Wanggui He",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09d",
          "name": "Guanghao Zhang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09e",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a09f",
          "name": "Qihan Huang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a0",
          "name": "Haobin Tang",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a1",
          "name": "Yunlong Yu",
          "hidden": false
        },
        {
          "_id": "67aae4128d478dcb4b39a0a2",
          "name": "Siming Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T14:50:32.000Z",
      "title": "CustomVideoX: 3D Referencia con Adaptación Dinámica para VideoDifusion Transformer Personalizado en 0 Fotos",
      "summary": "La generación personalizada ha logrado avances significativos en el campo de la síntesis de imágenes, pero la generación de videos personalizados se vuelve difícil debido a la incertidumbre temporal y la deterioro de la calidad. En este artículo se presenta un nuevo marco llamado \"CustomVideoX\" para generar videos personalizados a partir de imágenes de referencia. CustomVideoX utiliza una red de videos entrenada previamente, extrayendo características de referencia a través de parámetros de entrenamiento independientes de LoRA, asegurando así eficiencia y adaptabilidad. Para promover la interacción sinfín entre la imagen de referencia y el contenido del video, se propone la Attención de Referencia 3D. Esto permite que las características de la imagen de referencia se correspondan directamente y simultáneamente con las dimensiones espaciales y temporales de todos los frames del video. Para reducir el exceso de influencia de las características de la imagen de referencia y el contexto en el contenido del video generado durante la inferencia, se implementa la estrategia de la Atención de Referencia con Bias Tiempo-Aware (TAB). Esta estrategia ajusta el sesgo de referencia de manera dinámica en función de los pasos temporales. Además, se presenta el módulo Entity Region-Aware Enhancement (ERAE). Este módulo ajusta el sesgo de atención en función de la injección de características de referencia, mejorando las regiones de alta activación de marcas de entidades clave, lo que mejora el efecto. Para evaluar de manera detallada la generación de videos personalizados, se construyó un nuevo estándar de referencia que incluye más de 50 objetos y 100 pistas, llamado \"VideoBench\". Los resultados experimentales muestran que CustomVideoX supera significativamente a los métodos existentes en la consistencia y calidad de los videos.",
      "upvotes": 4,
      "discussionId": "67aae4178d478dcb4b39a1e7"
    },
    "publishedAt": "2025-02-11T00:46:11.168Z",
    "title": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06635",
      "authors": [
        {
          "_id": "67aac0ba91e6f5eb5476ea76",
          "name": "Qingshui Gu",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea77",
          "name": "Shu Li",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea78",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tianyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:15.968Z",
          "hidden": false
        },
        {
          "_id": "67aac0ba91e6f5eb5476ea79",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T16:31:37.000Z",
      "title": "Steel-LLM: Scratch desde abierto -- Una pasión personal para la construcción de un LLM centrado en el chino",
      "summary": "Steel-LLM es un modelo de lenguaje desarrollado en China con el objetivo de crear un modelo de alta calidad de código abierto, a pesar de los limites de recursos computacionales. Este proyecto, publicado en marzo de 2024, se centra en la formación de un modelo con 100 millones de parámetros, priorizando la transparencia y la compartión de información práctica para ayudar a otros miembros de la comunidad. El proceso de entrenamiento se realiza principalmente con datos chinos, incluyendo algunos datos en inglés, y complementa las deficiencias de los modelos abiertos de lenguaje actuales, proporcionando una descripción más detallada del proceso de diseño del modelo. Steel-LLM ha demostrado un desempeño competitivo en marcos de referencia como CEVAL y CMMLU, y ha superado los modelos iniciales de instituciones de gran escala. Este artículo resume las principales contribuciones del proyecto. Los recursos para el desarrollo del modelo se proporcionan a investigadores y practicantes como una herramienta útil. Los puntos de chequeo del modelo y los scripts de entrenamiento están disponibles en https://github.com/zhanshijinwat/Steel-LLM.",
      "upvotes": 4,
      "discussionId": "67aac0bb91e6f5eb5476eab8"
    },
    "publishedAt": "2025-02-10T22:20:38.168Z",
    "title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06635.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ab99dcb76bfd863eba64c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
      "fullname": "TY.Zheng",
      "name": "aaabiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.04370",
      "authors": [
        {
          "_id": "67aafd90141fac22732a79b3",
          "name": "Zhenglin Zhou",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b4",
          "name": "Xiaobo Xia",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b5",
          "name": "Fan Ma",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b6",
          "name": "Hehe Fan",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b7",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "67aafd90141fac22732a79b8",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T11:03:08.000Z",
      "title": "DreamDPO: Optimización Directa de la Preferencia para la Generación de 3D Texto que Coincide con las Preferencias Humanas",
      "summary": "La generación 3D a partir de texto realiza la generación automática de contenido 3D a partir de descripciones textuales, presentando una gran potencial de innovación en diversas áreas. Sin embargo, los métodos actuales presentan dificultades para alinear el contenido generado con las preferencias personales de las personas y limitan la aplicabilidad y la flexibilidad. Para resolver estos problemas, este artículo propone un marco de trabajo basado en optimización que integra directamente las preferencias personales en el proceso de generación 3D, llamado \"DreamDPO\". De manera práctica, DreamDPO construye un ejemplo de par de personas y compara su preferencia con funciones de alineamiento y grandes escenarios de difusión, finalmente optimizando la representación 3D utilizando una función de pérdida que se orienta hacia las preferencias. Al comparar con los par de personas, DreamDPO incrementa la precisión en evaluaciones puntuales sin depender de ellas, y mejora la capacidad de control micro-controlado de las preferencias. Los experimentos muestran que DreamDPO produce resultados excelentes en comparación con los métodos actuales y ofrece contenido 3D de alta calidad con control posible. Los códigos y modelos están disponibles bajo licencia abierta.",
      "upvotes": 3,
      "discussionId": "67aafd94141fac22732a7adc"
    },
    "publishedAt": "2025-02-11T02:46:33.870Z",
    "title": "DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6425318d175bd2952281065e/R7cMLIsmYovAMtL1vhsDn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6425318d175bd2952281065e",
      "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
      "fullname": "ZhenglinZhou",
      "name": "zhenglin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05957",
      "authors": [
        {
          "_id": "67aaecec114e64d6e15e7f41",
          "name": "Jiabin Tang",
          "hidden": false
        },
        {
          "_id": "67aaecec114e64d6e15e7f42",
          "name": "Tianyu Fan",
          "hidden": false
        },
        {
          "_id": "67aaecec114e64d6e15e7f43",
          "name": "Chao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T16:53:56.000Z",
      "title": "MetaChain: Construye agentes de LLM utilizando un marco de trabajo completamente automático y sin código.",
      "summary": "Los modelos de lenguaje grande (LLM) de agentes han demostrado capacidades sorprendentes en la automatización de tareas y toma de decisiones inteligentes, impulsando la amplia introducción de marcos de desarrollo de agentes como LangChain y AutoGen. Sin embargo, estos marcos principalmente se destinan a desarrolladores con un conocimiento técnico avanzado, lo que limita su accesibilidad a una minoría de la población. Este enfoque estricto indica que solo el 0.03% de la población mundial tiene acceso a las técnicas de programación necesarias, lo que actúa como un gran límite. Este enfoque plantea fundamentalmente la pregunta de si personas sin conocimientos técnicos pueden construir directamente agentes de LLM. Para enfrentar estas desafíos, presentamos MetaChain, un marco de desarrollo completamente automátizado y de alto nivel de automatización. Este marco solo requiere texto natural para generar y distribuir agentes de LLM. MetaChain funciona como un sistema operativo automático de agentes, compuesto de cuatro principales componentes: Utilidades de Sistema de Agentes, Motor de Acceso a la Base de Conocimientos de LLM, Sistema de Archivos Autogestionados y Modularización de Agentes de Juego. Este sistema puede generar y cambiar eficientemente y dinámicamente los herramientas, agentes y flujos de trabajo sin necesidad de requerir código o manualidad. MetaChain ofrece funciones de desarrollo de agentes que no requieren programación, manteniendo así la diversidad en sistemas de agentes comunes. En el GAIA benchmark, MetaChain demostró su eficiencia en tareas de agentes generales, superando los métodos más superiores actuales. Además, las funciones de generador de agentes de revisión de MetaChain (RAG) presentan un rendimiento consistente y excelente comparado con otras soluciones basadas en múltiples modelos de LLM.",
      "upvotes": 3,
      "discussionId": "67aaecef114e64d6e15e802c"
    },
    "publishedAt": "2025-02-11T01:33:35.134Z",
    "title": "MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05957.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643b751cc5f633a7fa84b325",
      "avatarUrl": "/avatars/a094b856cf3d51eb78d16a14361def62.svg",
      "fullname": "Tang",
      "name": "Jiabin99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06764",
      "authors": [
        {
          "_id": "67aac6052c02e43558b6b4b0",
          "name": "Kiwhan Song",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b1",
          "name": "Boyuan Chen",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b2",
          "name": "Max Simchowitz",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b3",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b4",
          "name": "Russ Tedrake",
          "hidden": false
        },
        {
          "_id": "67aac6052c02e43558b6b4b5",
          "name": "Vincent Sitzmann",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T18:44:25.000Z",
      "title": "Historia Guía de Ruta Difusión",
      "summary": "Filterpreadventure",
      "upvotes": 3,
      "discussionId": "67aac6072c02e43558b6b543"
    },
    "publishedAt": "2025-02-11T00:55:33.866Z",
    "title": "History-Guided Video Diffusion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06764.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6017
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06023",
      "authors": [
        {
          "_id": "67aac3a9ef5570c0c9047095",
          "user": {
            "_id": "640f6299ef5c6dcac8b1df52",
            "avatarUrl": "/avatars/022f21183abc8a8b5ce1b198d3ba96dc.svg",
            "isPro": false,
            "fullname": "Amir",
            "user": "sahsaeedi",
            "type": "user"
          },
          "name": "Amir Saeidi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-11T03:31:48.492Z",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047096",
          "name": "Yiran Luo",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047097",
          "name": "Agneet Chatterjee",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047098",
          "name": "Shamanthak Hegde",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c9047099",
          "name": "Bimsara Pathiraja",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c904709a",
          "name": "Yezhou Yang",
          "hidden": false
        },
        {
          "_id": "67aac3a9ef5570c0c904709b",
          "name": "Chitta Baral",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T20:34:43.000Z",
      "title": "Modelo de Optimización de Preferencia Dualcapacitivo",
      "summary": "Recientemente, el desarrollo de la optimización de las ociosidades ha sido originalmente asociado con el desarrollo de Modelos de Lenguaje Grandes (LLMs), mostrando también una considerable posibilidad en la mejora de modelos que evolucionan desde frases hasta imágenes. Estos métodos tienen como objetivo aprender la distribución de las ociosidades mientras distinguen entre distribuciones diferentes. Sin embargo, los conjuntos de datos actuales de ociosidades tienen distribuciones que se sobreponen, lo que genera situaciones donde las distribuciones en conflicto son frecuentes. Además, se han encontrado imágenes que contienen información irrelevante a los prompts de entrada, limitando así la predicción precisa de ruido en los métodos de optimización de ociosidades, conocidos como problemas de prompts irrelevantes. Para resolver estos problemas, proponemos un nuevo enfoque llamado Dual Caption Preference Optimization (DCPO), que utiliza dos captiones diferentes para inhibir los prompts irrelevantes. Para abordar las distribuciones en conflicto, proponemos el conjunto de datos Pick-Double Caption, que es una mejora de Pick-a-Pic v2 y permite usar captiones para imágenes relacionadas con las ociosidades y para imágenes irrelevantes. Además, proponemos tres estrategias diferentes para la generación de captiones: captchising, parmibivisive y híbrido. Nuestros experimentos muestran que DCPO mejora significativamente la calidad de las imágenes y su relación con los prompts, superando a Stable Diffusion (SD) 2.1, SFT_Chosen, Diffusion-DPO y MaPO, y se ha realizado un ajuste de retropropagación de SD 2.1 utilizando múltiples métricas como Pickscore, HPSv2.1, GenEval, CLIPscore y ImageReward.",
      "upvotes": 3,
      "discussionId": "67aac3b1ef5570c0c9047264"
    },
    "publishedAt": "2025-02-10T22:33:17.468Z",
    "title": "Dual Caption Preference Optimization for Diffusion Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06023.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640f6299ef5c6dcac8b1df52",
      "avatarUrl": "/avatars/022f21183abc8a8b5ce1b198d3ba96dc.svg",
      "fullname": "Amir",
      "name": "sahsaeedi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06060",
      "authors": [
        {
          "_id": "67ab1314385da1f07cda1271",
          "user": {
            "_id": "63abbf74ad514ca8d14a0548",
            "avatarUrl": "/avatars/b1357b73b8f9a8ff9908710ad64154ef.svg",
            "isPro": false,
            "fullname": "Bidipta Sarkar",
            "user": "bidiptas",
            "type": "user"
          },
          "name": "Bidipta Sarkar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:51:17.933Z",
          "hidden": false
        },
        {
          "_id": "67ab1314385da1f07cda1272",
          "name": "Warren Xia",
          "hidden": false
        },
        {
          "_id": "67ab1314385da1f07cda1273",
          "name": "C. Karen Liu",
          "hidden": false
        },
        {
          "_id": "67ab1314385da1f07cda1274",
          "name": "Dorsa Sadigh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T22:44:45.000Z",
      "title": "El entrenamiento de modelos de lenguaje para diálogos sociales utilizando aprendizaje por refuerzo multi-agente",
      "summary": "La comunicación en lenguaje natural es un instrumento efectivo que permite a los agentes independientes en un entorno multi-agente compartir información parcial observada y facilitar la colaboración con los humanos en un escenario de aprendizaje de 0 shot. Sin embargo, las investigaciones previas han sido en gran parte basadas en demostraciones a gran escala humana o han presentado limitaciones en la capacidad de generar estrategias de comunicación naturales y efectivas. En este trabajo, se entrena la comunicación mediante la discusión en lenguaje natural sobre el entorno, incluyendo demostraciones humanas. Se decomone el problema de comunicación en escucha y diálogo, y se considera la idea fundamental de generar señales básicas de danza para guiar el diálogo con la predicción de información útil basada en el objetivo del agente. Específicamente, se mejora la capacidad de escucha al entrenar el modelo con información sobre el entorno predicha en base a un diálogo, mientras se utiliza el aprendizaje por refuerzo entre múltiples agentes para mejorar la capacidad de diálogo mediante recompensas basadas en mensajes que influyen entre los agentes. Se investiga el papel y la necesidad de la comunicación en entornos sociales complejos, estudiando juegos de explicación social basados en muestradores y abordando problemas importantes como la investigación de personajes antagónicos irregulares. Se analizan los fenómenos generados por nuestro método, se presentan cuestiones, se proporcionan evidencias para descubrir fenómenos, y se logra una discusión fuerte, alcanzando un rendimiento dos veces superior al aprendizaje por refuerzo estándar. Nuestro código y modelo están disponibles en la siguiente URL: https://socialdeductionllm.github.io/",
      "upvotes": 2,
      "discussionId": "67ab1315385da1f07cda12a5"
    },
    "publishedAt": "2025-02-11T04:08:55.672Z",
    "title": "Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63abbf74ad514ca8d14a0548",
      "avatarUrl": "/avatars/b1357b73b8f9a8ff9908710ad64154ef.svg",
      "fullname": "Bidipta Sarkar",
      "name": "bidiptas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05795",
      "authors": [
        {
          "_id": "67ab189a8087b66340398b01",
          "name": "Wenfang Sun",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b02",
          "name": "Xinyuan Song",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b03",
          "user": {
            "_id": "64245f2c089d5fae56b4549a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
            "isPro": false,
            "fullname": "Pengxiang Li",
            "user": "pengxiang",
            "type": "user"
          },
          "name": "Pengxiang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T09:51:15.671Z",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b04",
          "name": "Lu Yin",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b05",
          "name": "Yefeng Zheng",
          "hidden": false
        },
        {
          "_id": "67ab189a8087b66340398b06",
          "name": "Shiwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T07:03:36.000Z",
      "title": "「El Maleficio de la Mente del Modelo de Lenguaje」",
      "summary": "En este artículo, mediante el concepto de \"Depth of Depth\", se desvela y analiza el fenómeno en que en los grandes modelos de lenguaje modernos (LLMs) más de la mitad de las capas producen efectos inesperadamente bajos, proponiendo soluciones. Primero, se confirma la amplia presencia de este fenómeno en famosos LLMs como Llama, Mistral, DeepSeek y Qwen. Mediante análisis teórico y experimental, se identifica la utilización extensa de Pre-Layer Normalization (Pre-LN) como la causa fundamental de la ineficiencia de las profundas capas de los LLMs. La Pre-LN estabiliza el entrenamiento de los modelos Transformer, pero la varianza de la salida aumenta exponentiamente con la profundidad, y las derivadas de los bloques Transformer profundos se convierten en matrices de escalación, contribuyendo muy poco al entrenamiento. Para resolver estos problemas, se propone LayerNorm Scaling. LayerNorm Scaling escala la varianza de la normalización de capas por la raíz cuadrada inversa de la profundidad, lo que inhibe la explosión de la varianza de la salida de las capas profundas y mejora su contribución al entrenamiento. Los resultados experimentales muestran que, en un rango de tamaños de modelo de 130M a 1B, LayerNorm Scaling mejora significativamente el rendimiento del entrenamiento de los LLMs en comparación con Pre-LN, y esta mejora se debe a que LayerNorm Scaling permite que las capas profundas contribuyan más efectivamente durante el entrenamiento.",
      "upvotes": 1,
      "discussionId": "67ab189b8087b66340398b3b"
    },
    "publishedAt": "2025-02-11T04:30:30.043Z",
    "title": "The Curse of Depth in Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05795.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64245f2c089d5fae56b4549a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
      "fullname": "Pengxiang Li",
      "name": "pengxiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]