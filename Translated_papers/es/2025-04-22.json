[
  {
    "paper": {
      "id": "2504.14945",
      "authors": [
        {
          "_id": "6806fdeda296cac1cf860505",
          "name": "Jianhao Yan",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf860506",
          "name": "Yafu Li",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf860507",
          "name": "Zican Hu",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf860508",
          "name": "Zhi Wang",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf860509",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf86050a",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf86050b",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf86050c",
          "name": "Yue Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T08:09:13.000Z",
      "submittedOnDailyAt": "2025-04-22T00:57:31.276Z",
      "title": "Comprender el motivo de seguir el Guía de Oportunidades para el Aprendizaje\n\n**Translation:**\n\n**Entendiendo el Motivo de Seguir el Guía de Oportunidades para el Aprendizaje**\n\n**Nota:** La traducción se ha realizado manteniendo la profundidad y la precisión del texto original, asegurando que el contenido sea coherente y relevante para un público específico.",
      "submittedOnDailyBy": {
        "_id": "6086838b19137b3a6ba760e7",
        "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
        "isPro": false,
        "fullname": "Jianhao Yan",
        "user": "Elliott",
        "type": "user"
      },
      "summary": "El desarrollo reciente de grandes modelos de inferencia (LRMs) muestra que acciones complejas como multi-step inference y auto-reflection aparecen en el aprendizaje por refuerzo (RL) basado en reglas simples de recompensa. Sin embargo, el enfoque de RL inicial con cero inicio limita el aprendizaje a una única 'política' y restringe la obtención de habilidades inferenciales superiores a las iniciales. Presentamos LUFFY (Framework para el aprendizaje de inferencia desde una guía de políticas), un marco que agrega el seguimiento de la inferencia de políticas a RL inicial, combinando el aprendizaje de políticas y el aprendizaje por exploración en un equilibrio dinámico durante el entrenamiento. En particular, proponemos el aprendizaje de políticas mixtas utilizando muestreo de importancia normalizada para evitar la fijación de la simulación y mejorar la generalización. LUFFY ha logrado un beneficio promedio de 7.0 en 6 benchmarks matemáticos y un desafío de 6.2 puntos en tareas fuera de distribución. Además, supera significativamente la regulación normativa de la simulación, aportando gran beneficio para la generalización. La análisis muestra que LUFFY no efectúa la simulación de manera efectiva, sino que proporciona pasos viables para entrenar modelos de inferencia que superan la guía de políticas, haciendo que la exploración exceda la orientación.",
      "upvotes": 37,
      "discussionId": "6806fdeea296cac1cf860553",
      "githubRepo": "https://github.com/ElliottYan/LUFFY",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "zero-RL",
        "on-policy",
        "off-policy",
        "LUFFY",
        "imitation learning",
        "on-policy rollouts",
        "policy shaping",
        "regularized importance sampling",
        "mixed-policy training",
        "math benchmarks",
        "out-of-distribution tasks",
        "imitation-based supervised fine-tuning (SFT)",
        "generalizable reasoning models"
      ]
    },
    "publishedAt": "2025-04-21T04:09:13.000Z",
    "title": "Learning to Reason under Off-Policy Guidance",
    "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14945.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6086838b19137b3a6ba760e7",
      "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
      "fullname": "Jianhao Yan",
      "name": "Elliott",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15257",
      "authors": [
        {
          "_id": "68070ee593d1301c2f2ade99",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9a",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9b",
          "name": "Yufei He",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9c",
          "user": {
            "_id": "6214e4ee1e35c843d42d1f88",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
            "isPro": false,
            "fullname": "Longxu Dou",
            "user": "dreamerdeo",
            "type": "user"
          },
          "name": "Longxu Dou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:46.818Z",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9d",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9e",
          "name": "Zhijie Deng",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9f",
          "name": "Bryan Hooi",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2adea0",
          "name": "Min Lin",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2adea1",
          "name": "Tianyu Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:35:42.000Z",
      "submittedOnDailyAt": "2025-04-22T02:12:27.161Z",
      "title": "FlowReasoner: Fortalecimiento del Meta Agente de Nivel de Consulta",
      "submittedOnDailyBy": {
        "_id": "6650c77a74664a42ddfb9187",
        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
        "isPro": false,
        "fullname": "yueliu1999",
        "user": "yueliu1999",
        "type": "user"
      },
      "summary": "En este artículo, se propone la meta-agente de nivel de consulta llamada ForceReasoning Maester, con el objetivo de automatizar el diseño de sistemas multi-agente de nivel de consulta. La idea clave es fomentar meta-agentes lógicos basados en retroalimentación de ejecución externa. Concretamente, se diseña FlowReasoner, que se obtiene a partir de la distillación de DeepSeek R1, dando a los sistemas multi-agente una capacidad lógica básica. A continuación, se realiza un aprendizaje por refuerzo (RL) utilizando retroalimentación de ejecución externa. Se diseña una recompensa multifuncional en términos de rendimiento, complejidad y eficiencia, permitiendo que FlowReasoner cree un sistema multi-agente especializado para cada consulta del usuario. Los experimentos en marcos de prueba de ingeniería y compe coder demuestran la excelente performance de FlowReasoner. En particular, en tres marcos de prueba, FlowReasoner superó a o1-mini en precisión en un 10.52%. El código está disponible en https://github.com/sail-sg/FlowReasoner.",
      "upvotes": 27,
      "discussionId": "68070ee693d1301c2f2aded1",
      "ai_keywords": [
        "DeepSeek R1",
        "reinforcement learning",
        "reward",
        "performance",
        "complexity",
        "efficiency",
        "deliberative reasoning"
      ]
    },
    "publishedAt": "2025-04-21T13:35:42.000Z",
    "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
    "summary": "This paper proposes a query-level meta-agent named FlowReasoner to automate\nthe design of query-level multi-agent systems, i.e., one system per user query.\nOur core idea is to incentivize a reasoning-based meta-agent via external\nexecution feedback. Concretely, by distilling DeepSeek R1, we first endow the\nbasic reasoning ability regarding the generation of multi-agent systems to\nFlowReasoner. Then, we further enhance it via reinforcement learning (RL) with\nexternal execution feedback. A multi-purpose reward is designed to guide the RL\ntraining from aspects of performance, complexity, and efficiency. In this\nmanner, FlowReasoner is enabled to generate a personalized multi-agent system\nfor each user query via deliberative reasoning. Experiments on both engineering\nand competition code benchmarks demonstrate the superiority of FlowReasoner.\nRemarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks.\nThe code is available at https://github.com/sail-sg/FlowReasoner.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15257.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6650c77a74664a42ddfb9187",
      "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
      "fullname": "yueliu1999",
      "name": "yueliu1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15271",
      "authors": [
        {
          "_id": "680748d088578d9444349293",
          "name": "Guo Chen",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349294",
          "user": {
            "_id": "6582d86e58df0a2e21db80b8",
            "avatarUrl": "/avatars/a8245b1644183bd3ee7dc06b218c6e47.svg",
            "isPro": true,
            "fullname": "ZhiqiLi",
            "user": "RealZhiqiLi",
            "type": "user"
          },
          "name": "Zhiqi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:20.873Z",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349295",
          "name": "Shihao Wang",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349296",
          "name": "Jindong Jiang",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349297",
          "name": "Yicheng Liu",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349298",
          "name": "Lidong Lu",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349299",
          "name": "De-An Huang",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929a",
          "name": "Wonmin Byeon",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929b",
          "name": "Matthieu Le",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929c",
          "name": "Tuomas Rintamaki",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929d",
          "name": "Tyler Poon",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929e",
          "name": "Max Ehrlich",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929f",
          "name": "Tuomas Rintamaki",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a0",
          "name": "Tyler Poon",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a1",
          "name": "Tong Lu",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a2",
          "name": "Limin Wang",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a3",
          "name": "Bryan Catanzaro",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a4",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a5",
          "name": "Andrew Tao",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a6",
          "name": "Zhiding Yu",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a7",
          "name": "Guilin Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:57:28.000Z",
      "submittedOnDailyAt": "2025-04-22T06:14:17.808Z",
      "title": "Eagle 2.5: Mejora la entrenamiento de post-procesado en el contexto de la visión frontera de la ruta de retroalimentación.",
      "submittedOnDailyBy": {
        "_id": "6392c73390b8e99a6779a7b0",
        "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
        "isPro": false,
        "fullname": "Guo Chen",
        "user": "cg1177",
        "type": "user"
      },
      "summary": "Eagle 2.5, introduce a cutting-edge visual language model (VLM) family. Our research addresses the challenges of long video understanding and high-resolution image understanding, proposing a general framework applicable to two tasks. The proposed training framework includes two techniques: automatic low-resolution sampling and image area preservation, which ensure context completion and visual details are maintained. The framework includes significant efficiency improvements for training on long video data. Finally, we propose a new dataset called Eagle-Video-110K, which integrates video-level and clip-level descriptions to enhance long video understanding. Eagle 2.5 demonstrates significant improvements on long video multimodal benchmarks and provides a powerful solution to the limitations of current VLMs. Notably, our best model, Eagle 2.5-8B, achieves 72.4% on Video-MME using 512 frames, surpassing top-tier corporate models like GPT-4o, Qwen2.5-VL-72B, and InternVL2.5-78B, as well as large-scale open-source models.",
      "upvotes": 24,
      "discussionId": "680748d188578d9444349311",
      "projectPage": "https://nvlabs.github.io/EAGLE/",
      "githubRepo": "https://github.com/NVlabs/EAGLE",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "long-context multimodal learning",
        "long video comprehension",
        "high-resolution image understanding",
        "Automatic Degrade Sampling",
        "Image Area Preservation",
        "efficiency optimizations",
        "long-context data training",
        "Eagle-Video-110K",
        "story-level annotations",
        "clip-level annotations",
        "long-video understanding",
        "multimodal benchmarks",
        "Video-MME"
      ]
    },
    "publishedAt": "2025-04-21T13:57:28.000Z",
    "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier\n  Vision-Language Models",
    "summary": "We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)\nfor long-context multimodal learning. Our work addresses the challenges in long\nvideo comprehension and high-resolution image understanding, introducing a\ngeneralist framework for both tasks. The proposed training framework\nincorporates Automatic Degrade Sampling and Image Area Preservation, two\ntechniques that preserve contextual integrity and visual details. The framework\nalso includes numerous efficiency optimizations in the pipeline for\nlong-context data training. Finally, we propose Eagle-Video-110K, a novel\ndataset that integrates both story-level and clip-level annotations,\nfacilitating long-video understanding. Eagle 2.5 demonstrates substantial\nimprovements on long-context multimodal benchmarks, providing a robust solution\nto the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B\nachieves 72.4% on Video-MME with 512 input frames, matching the results of\ntop-tier commercial model such as GPT-4o and large-scale open-source models\nlike Qwen2.5-VL-72B and InternVL2.5-78B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15271.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6392c73390b8e99a6779a7b0",
      "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
      "fullname": "Guo Chen",
      "name": "cg1177",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13958",
      "authors": [
        {
          "_id": "6806fe1bec9fb3764b875ea0",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea1",
          "user": {
            "_id": "63888d3fd68e37abd599f428",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
            "isPro": true,
            "fullname": "emre can",
            "user": "emrecanacikgoz",
            "type": "user"
          },
          "name": "Emre Can Acikgoz",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:57.521Z",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea2",
          "name": "Qi He",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea3",
          "name": "Hongru Wang",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea4",
          "name": "Xiusi Chen",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea5",
          "name": "Dilek Hakkani-Tür",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea6",
          "name": "Gokhan Tur",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea7",
          "name": "Heng Ji",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/v-Pmd0mDnq-v5479tRcgC.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/pCPBgXm4IAZdhYjc-ZSrw.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/0c5XM8ZGXnieGz3IzuYTh.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/omlm8xwODvorHTUkfFOgR.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/1n73QmTy-PA5FfFQJMTej.png"
      ],
      "publishedAt": "2025-04-16T21:45:32.000Z",
      "submittedOnDailyAt": "2025-04-22T01:37:59.400Z",
      "title": "ToolRL: La recompensa es esencial para todo el aprendizaje de herramientas.",
      "submittedOnDailyBy": {
        "_id": "63888d3fd68e37abd599f428",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
        "isPro": true,
        "fullname": "emre can",
        "user": "emrecanacikgoz",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grandes actuales (LLMs) obtienen generalmente la capacidad de uso de herramientas a través de ajustes microregulados (SFT). Sin embargo, el SFT es difícil de generalizar en escenarios de uso de herramientas inusuales y complejos. El desarrollo reciente del aprendizaje por refuerzo (RL), especialmente en modelos como R1, ha demostrado la expectativa de habilidades de explicación y generalización. Sin embargo, la diseño de recompensas para el uso de herramientas tiene problemas especiales: varias herramientas se llaman con parámetros diferentes, y la recompensa se proporciona de manera gruesa, no proporcionando retroalimentación fina necesaria para un aprendizaje efectivo. En este artículo, se realiza una investigación detallada inicial sobre el diseño de recompensas para la selección de herramientas y la aplicación de tareas dentro del paradigma de aprendizaje por refuerzo. Se explora ampliamente la gama de estrategias de recompensa, analizando su tipo, escala, granularidad y dinámica temporal. Basándose en estas observaciones, se propone un diseño de recompensa fundamental para tareas de uso de herramientas y se entrena los LLMs utilizando la Política de Optimización Relativa de Grupos (GRPO). Las evaluaciones en diferentes marcos de referencia muestran que nuestro enfoque es impactante, escalable y logra un entrenamiento estable, mejorando en un 17% respecto a los modelos básicos y en un 15% respecto a los modelos SFT. Estos resultados demuestran claramente la importancia de diseñar una recompensa considerando el uso de herramientas y la generalización en los LLMs. Todo el código se publica para fomentar futuras investigaciones.",
      "upvotes": 19,
      "discussionId": "6806fe1dec9fb3764b875f0c",
      "githubRepo": "https://github.com/qiancheng0/ToolRL",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "R1-like models",
        "reasoning and generalization",
        "tool selection",
        "tool application",
        "reward design",
        "finegrained feedback",
        "reward strategies",
        "Group Relative Policy Optimization (GRPO)"
      ]
    },
    "publishedAt": "2025-04-16T17:45:32.000Z",
    "title": "ToolRL: Reward is All Tool Learning Needs",
    "summary": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/v-Pmd0mDnq-v5479tRcgC.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/pCPBgXm4IAZdhYjc-ZSrw.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/0c5XM8ZGXnieGz3IzuYTh.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/omlm8xwODvorHTUkfFOgR.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/1n73QmTy-PA5FfFQJMTej.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13958.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63888d3fd68e37abd599f428",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
      "fullname": "emre can",
      "name": "emrecanacikgoz",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15281",
      "authors": [
        {
          "_id": "68072d05362af0cf18fb4b0c",
          "name": "Cailin Zhuang",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b0d",
          "name": "Yaoqi Hu",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b0e",
          "name": "Xuanyang Zhang",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b0f",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:44.772Z",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b10",
          "name": "Jiacheng Bao",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b11",
          "name": "Shengqi Liu",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b12",
          "user": {
            "_id": "67da6acc05101e8e1d2c20a2",
            "avatarUrl": "/avatars/1cfa3a1f59687db58af4e1b4a8767bfd.svg",
            "isPro": false,
            "fullname": "Wang",
            "user": "Yiying12",
            "type": "user"
          },
          "name": "Yiying Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:42.183Z",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b13",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b14",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b15",
          "name": "Ming Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/EeggKsuKNbiiygcWDKxQZ.webm"
      ],
      "publishedAt": "2025-04-21T17:59:55.000Z",
      "submittedOnDailyAt": "2025-04-22T04:18:58.848Z",
      "title": "\"STYLEME3D: Style Transfer con Multiple Encoders sobre 3D Gaussianes con Inicialización Separada\"",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "3D Gaussian Splatting (3DGS) tiene ventajas en la reconstrucción de escenas realistas, pero puede presentar limitaciones en contenidos estilizados como personajes o juegos, donde pueden surgir la destrucción de texturas, errores significativos y limitaciones de adaptación a características artísticas abstractas. Proponemos un marco integrado para la estilización 3D, llamado StyleMe3D. Este marco integra la optimización de estilo multimodal, ajuste significativo multinivel y mejora de calidad visual. Nuestra idea principal consiste en lo siguiente: (1) Optimizar solo los atributos RGB mantiene la armonía geométrica en la estilización. (2) Separar significados de diferentes niveles es crucial para la coherencia de la estilización. (3) La escalabilidad de objetos separados o escenografías complejas es necesaria para aplicaciones prácticas. StyleMe3D introduce cuatro nuevos componentes: Dynamic Style Score Distillation (DSSD), el espacio potencial de Stable Diffusion para ajuste significativo, Contrastive Style Descriptor (CSD) para transferencia de textura localizada y contexto-aware, 3D Gaussian Quality Assessment (3DG-QA) para separar detalles estéticos y estructurales, Simultaneously Optimized Scale (SOS), y una dirección artística diferenciable basada en datos de evaluación humana para mejorar la estilización artística y la armonía visual. StyleMe3D evalua un conjunto de datos sintéticos de NERF (objetos) y tantdt db (escenografías), demostrando un rendimiento superior a los métodos más recientes, conservando detalles geométricos y garantizando la coherencia estilística de la escenografía completa. Este estudio desarrolla aplicaciones en juegos, mundos virtuales y arte digital al combinar la 3D Gaussian Splatting realista con la estilización artística.",
      "upvotes": 18,
      "discussionId": "68072d08362af0cf18fb4c7b",
      "projectPage": "https://styleme3d.github.io/",
      "githubRepo": "https://github.com/AIGCResearch/styleme3d",
      "ai_keywords": [
        "Gaussian Splatting",
        "StyleMe3D",
        "multi-modal style conditioning",
        "multi-level semantic alignment",
        "perceptual quality enhancement",
        "Dynamic Style Score Distillation",
        "Stable Diffusion",
        "latent space",
        "Contrastive Style Descriptor",
        "localized, content-aware texture transfer",
        "Simultaneously Optimized Scale",
        "3D Gaussian Quality Assessment",
        "differentiable aesthetic prior",
        "NeRF synthetic dataset",
        "tandt db",
        "preserving geometric details",
        "stylistic consistency",
        "real-time rendering"
      ]
    },
    "publishedAt": "2025-04-21T13:59:55.000Z",
    "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on\n  3D Gaussians",
    "summary": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction\nbut struggles with stylized scenarios (e.g., cartoons, games) due to fragmented\ntextures, semantic misalignment, and limited adaptability to abstract\naesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer\nthat integrates multi-modal style conditioning, multi-level semantic alignment,\nand perceptual quality enhancement. Our key insights include: (1) optimizing\nonly RGB attributes preserves geometric integrity during stylization; (2)\ndisentangling low-, medium-, and high-level semantics is critical for coherent\nstyle transfer; (3) scalability across isolated objects and complex scenes is\nessential for practical deployment. StyleMe3D introduces four novel components:\nDynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent\nspace for semantic alignment; Contrastive Style Descriptor (CSD) for localized,\ncontent-aware texture transfer; Simultaneously Optimized Scale (SOS) to\ndecouple style details and structural coherence; and 3D Gaussian Quality\nAssessment (3DG-QA), a differentiable aesthetic prior trained on human-rated\ndata to suppress artifacts and enhance visual harmony. Evaluated on NeRF\nsynthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D\noutperforms state-of-the-art methods in preserving geometric details (e.g.,\ncarvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,\ncoherent lighting in landscapes), while maintaining real-time rendering. This\nwork bridges photorealistic 3D GS and artistic stylization, unlocking\napplications in gaming, virtual worlds, and digital art.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/EeggKsuKNbiiygcWDKxQZ.webm"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15281.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14396",
      "authors": [
        {
          "_id": "6806fcc4f349e60f6c1b928c",
          "user": {
            "_id": "630461624ec2dfa82a5ad7e7",
            "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
            "isPro": false,
            "fullname": "Minho Park",
            "user": "mpark",
            "type": "user"
          },
          "name": "Minho Park",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:59.880Z",
          "hidden": false
        },
        {
          "_id": "6806fcc4f349e60f6c1b928d",
          "name": "Taewoong Kang",
          "hidden": false
        },
        {
          "_id": "6806fcc4f349e60f6c1b928e",
          "user": {
            "_id": "6369f693bf21b20c5692937b",
            "avatarUrl": "/avatars/e937dc8234b3e456149882bfce34841f.svg",
            "isPro": false,
            "fullname": "Jooyeol Yun",
            "user": "YeolJoo",
            "type": "user"
          },
          "name": "Jooyeol Yun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:51:02.918Z",
          "hidden": false
        },
        {
          "_id": "6806fcc4f349e60f6c1b928f",
          "name": "Sungwon Hwang",
          "hidden": false
        },
        {
          "_id": "6806fcc4f349e60f6c1b9290",
          "name": "Jaegul Choo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-19T19:59:11.000Z",
      "submittedOnDailyAt": "2025-04-22T00:50:52.270Z",
      "title": "Sphyra Dif: La generación de imágenes y videos en todas las direcciones sin tunear, utilizando la representación Sphyra Transformer",
      "submittedOnDailyBy": {
        "_id": "630461624ec2dfa82a5ad7e7",
        "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
        "isPro": false,
        "fullname": "Minho Park",
        "user": "mpark",
        "type": "user"
      },
      "summary": "La demanda de aplicaciones AR/VR está aumentando, lo que ha enfatizado la necesidad de contenido de panorama 360 grados de alta calidad. Sin embargo, la severa distorsión causada por la proyección en plano esférico (ERP) es la causa de que la generación de imágenes y videos de panorama 360 grados de alta calidad sea un desafío. Los métodos actuales utilizan modelos difusion preentrenados con conjuntos de datos ERP limitados o intentan entrenar sin límites, pero ambos tienen discontinuidades cerca de la POOL. En este artículo, se presenta una nueva aproximación para generar imágenes y videos de panorama 360 grados suaves sin realizar adicionales entrenamientos, utilizando modelos difusiones más recientes. Se define una representación potencial esférica y se garantiza una distribución coherente desde un punto de vista global, mitigando la distorsión por ERP. Se expande MultiDiffusion en el espacio potencial esférico y se propone un método de muestreo potencial esférico, permitiendo el uso directo de modelos difusion preentrenados. Además, se introduce una ponderación promedio para mejorar la calidad de generación durante el proceso de proyección. Nuestro método supera los métodos actuales en la generación de contenido de panorama 360 grados, manteniendo alta calidad y proporcionando una solución sólida para aplicaciones satisfactorias AR/VR. El código se puede consultar en: https://github.com/pmh9960/SphereDiff",
      "upvotes": 18,
      "discussionId": "6806fcc7f349e60f6c1b93ab",
      "projectPage": "https://pmh9960.github.io/research/SphereDiff/",
      "githubRepo": "https://github.com/pmh9960/SphereDiff",
      "ai_keywords": [
        "SphereDiff",
        "diffusion models",
        "equirectangular projection (ERP)",
        "spherical latent representation",
        "MultiDiffusion",
        "spherical latent space",
        "spherical latent sampling method",
        "distortion-aware weighted averaging",
        "high-fidelity",
        "immersive AR/VR applications"
      ]
    },
    "publishedAt": "2025-04-19T15:59:11.000Z",
    "title": "SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video\n  Generation via Spherical Latent Representation",
    "summary": "The increasing demand for AR/VR applications has highlighted the need for\nhigh-quality 360-degree panoramic content. However, generating high-quality\n360-degree panoramic images and videos remains a challenging task due to the\nsevere distortions introduced by equirectangular projection (ERP). Existing\napproaches either fine-tune pretrained diffusion models on limited ERP datasets\nor attempt tuning-free methods that still rely on ERP latent representations,\nleading to discontinuities near the poles. In this paper, we introduce\nSphereDiff, a novel approach for seamless 360-degree panoramic image and video\ngeneration using state-of-the-art diffusion models without additional tuning.\nWe define a spherical latent representation that ensures uniform distribution\nacross all perspectives, mitigating the distortions inherent in ERP. We extend\nMultiDiffusion to spherical latent space and propose a spherical latent\nsampling method to enable direct use of pretrained diffusion models. Moreover,\nwe introduce distortion-aware weighted averaging to further improve the\ngeneration quality in the projection process. Our method outperforms existing\napproaches in generating 360-degree panoramic content while maintaining high\nfidelity, making it a robust solution for immersive AR/VR applications. The\ncode is available here. https://github.com/pmh9960/SphereDiff",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14396.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630461624ec2dfa82a5ad7e7",
      "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
      "fullname": "Minho Park",
      "name": "mpark",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13203",
      "authors": [
        {
          "_id": "68070197c1bd0fc00c7d90a7",
          "name": "Salman Rahman",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90a8",
          "name": "Liwei Jiang",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90a9",
          "name": "James Shiffer",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90aa",
          "name": "Genglin Liu",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90ab",
          "name": "Sheriff Issaka",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90ac",
          "name": "Md Rizwan Parvez",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90ad",
          "name": "Hamid Palangi",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90ae",
          "name": "Kai-Wei Chang",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90af",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90b0",
          "name": "Saadia Gabriel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:11:28.000Z",
      "submittedOnDailyAt": "2025-04-22T01:13:32.158Z",
      "title": "X-Teaming: Destruir el palillo de las manos y defensa de agentes adaptativos multiagentes",
      "submittedOnDailyBy": {
        "_id": "625913bd5f80a3c1aad074b6",
        "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
        "isPro": false,
        "fullname": "Salman Rahman",
        "user": "salmannyu",
        "type": "user"
      },
      "summary": "En el intercambio de lenguas, el intercambio de modelos de lenguaje (LM) con múltiples lenguas puede llevar a riesgos estratégicos, ya que puede promover la propagación de intenciones malintencionadas. Sin embargo, la mayoría de los estudios previos se centran en la seguridad de los intercambios de palabras, y la aplicabilidad y diversidad del test de intercambio de múltiples lenguas son uno de los principales desafíos. Para abordar estos problemas, proponemos un marco escalable llamado X-Teaming. Este marco tiene como objetivo explorar y generar escenarios de ataques que pueden desarrollar resultados perjudiciales de manera sistemática, mediante intercambios invisibles y sin daño. X-Teaming utiliza agentes cooperativos para la planificación, optimización de los ataques y verificación, y logra alcanzar los mejores resultados de eficacia y diversidad en el intercambio de múltiples lenguas, con un éxito del 98.1% en modelos abiertos y cercanos a la fuente. En particular, X-Teaming alcanza un éxito del 96.2% en el modelo más reciente, Claude 3.7 Sonnet, lo que significa que es casi inmune contra ataques de palabras. Basado en X-Teaming, presentamos XGuard-Train, un conjunto de datos de entrenamiento abierto para la seguridad de intercambios de múltiples lenguas. Este conjunto de datos es 20 veces más grande que los mejores recursos previos, incluye 30K interacciones de downbol y permite ajustar la seguridad de intercambios de múltiples lenguas en los modelos de lenguaje. Nuestro estudio proporciona herramientas y insights esenciales para responder a ataques complejos de diálogo y tiene como objetivo mejorar la seguridad de intercambios de múltiples lenguas.",
      "upvotes": 17,
      "discussionId": "68070198c1bd0fc00c7d9101",
      "projectPage": "https://x-teaming.github.io/",
      "githubRepo": "https://github.com/salman-lui/x-teaming",
      "ai_keywords": [
        "multi-turn interactions",
        "language models (LMs)",
        "safety risks",
        "harmful intent",
        "single-turn safety",
        "adaptability",
        "diversity",
        "X-Teaming",
        "collaborative agents",
        "attack scenarios",
        "jailbreak effectiveness",
        "jailbreak success rates",
        "Claude 3.7 Sonnet model",
        "multi-turn jailbreak",
        "XGuard-Train",
        "safety training dataset",
        "interactive jailbreaks",
        "multi-turn safety alignment"
      ]
    },
    "publishedAt": "2025-04-15T12:11:28.000Z",
    "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
    "summary": "Multi-turn interactions with language models (LMs) pose critical safety\nrisks, as harmful intent can be strategically spread across exchanges. Yet, the\nvast majority of prior work has focused on single-turn safety, while\nadaptability and diversity remain among the key challenges of multi-turn\nred-teaming. To address these challenges, we present X-Teaming, a scalable\nframework that systematically explores how seemingly harmless interactions\nescalate into harmful outcomes and generates corresponding attack scenarios.\nX-Teaming employs collaborative agents for planning, attack optimization, and\nverification, achieving state-of-the-art multi-turn jailbreak effectiveness and\ndiversity with success rates up to 98.1% across representative leading\nopen-weight and closed-source models. In particular, X-Teaming achieves a 96.2%\nattack success rate against the latest Claude 3.7 Sonnet model, which has been\nconsidered nearly immune to single-turn attacks. Building on X-Teaming, we\nintroduce XGuard-Train, an open-source multi-turn safety training dataset that\nis 20x larger than the previous best resource, comprising 30K interactive\njailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our\nwork offers essential tools and insights for mitigating sophisticated\nconversational attacks, advancing the multi-turn safety of LMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13203.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625913bd5f80a3c1aad074b6",
      "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
      "fullname": "Salman Rahman",
      "name": "salmannyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14603",
      "authors": [
        {
          "_id": "6806fab1b89f3c89c81afbe0",
          "user": {
            "_id": "654dbac9938fbf1e696be8aa",
            "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
            "isPro": false,
            "fullname": "Chaoyun Zhang",
            "user": "vyokky",
            "type": "user"
          },
          "name": "Chaoyun Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:51:25.217Z",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe1",
          "name": "He Huang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe2",
          "name": "Chiming Ni",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe3",
          "name": "Jian Mu",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe4",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe5",
          "name": "Shilin He",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe6",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe7",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe8",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe9",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbea",
          "name": "Liqun Li",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbeb",
          "name": "Yu Kang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbec",
          "name": "Zhao Jiang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbed",
          "name": "Suzhen Zheng",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbee",
          "name": "Rujia Wang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbef",
          "name": "Jiaxu Qian",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf0",
          "name": "Minghua Ma",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf1",
          "name": "Jian-Guang Lou",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf2",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf3",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf4",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-20T13:04:43.000Z",
      "submittedOnDailyAt": "2025-04-22T00:41:48.404Z",
      "title": "UFO2: motor de diseño de accesorios para el ordenador",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "Los últimos Agentes de Uso de Computadora (CUAs) presentan la posibilidad de automatizar flujos de trabajo complejos de escritorio. Sin embargo, los actuales CUAs permanecen como conceptos prototipales debido a la baja integración con el sistema operativo, la interacción basada en capturas de pantalla débile y la ejecución destructiva.\n\nUFO2 es un AgentOS multi-agente para el escritorio de Windows. UFO2 caracteriza a un centro de host agente para la descomposición y coordinación de tareas, junto con una colección de agentes especializados en aplicaciones que cuentan con API nativas, conocimientos dominio específicos y una capa de acciones de GUI-API integrada. Esta arquitectura mantiene una ejecución robusta mientras mantiene modularidad y extensibilidad. La pipeline híbrida de detección de control combina la automatización de la interfaz de usuario (UIA) con análisis visual para soportar diferentes estilos de interfaz. La eficiencia de ejecución se mejora mediante planes predictivos de múltiples acciones, reduciendo los costos asociados a la utilización de modelos de lenguaje de inteligencia artificial (LLM). Finalmente, la interfaz Picture-in-Picture (PiP) permite la automatización en vistas independientes dentro del escritorio virtual, y permite a los usuarios y agentes manipular simultáneamente.\n\nUFO2 ha evaluado más de 20 aplicaciones de Windows prácticas y ha demostrado una fuerte confianza y alta precisión de ejecución en comparación con otros CUAs anteriores. Como resultado, la profunda integración con el sistema operativo abre una ruta expandible para una automatización de escritorio centrada en el usuario y confiable.",
      "upvotes": 15,
      "discussionId": "6806fabdb89f3c89c81affb5",
      "projectPage": "https://microsoft.github.io/UFO/",
      "githubRepo": "https://github.com/microsoft/UFO",
      "ai_keywords": [
        "multimodal large language models",
        "task decomposition",
        "coordination",
        "application-specialized AppAgent",
        "native APIs",
        "domain-specific knowledge",
        "unified GUI--API action layer",
        "hybrid control detection pipeline",
        "Windows UI Automation (UIA)",
        "vision-based parsing",
        "speculative multi-action planning",
        "Picture-in-Picture (PiP) interface",
        "runtime efficiency"
      ]
    },
    "publishedAt": "2025-04-20T09:04:43.000Z",
    "title": "UFO2: The Desktop AgentOS",
    "summary": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14603.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15280",
      "authors": [
        {
          "_id": "680703e4c0ce7eea9ba1a548",
          "user": {
            "_id": "6499eca0685215f7247bd5ce",
            "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
            "isPro": false,
            "fullname": "Chun-Hsiao Yeh",
            "user": "danielchyeh",
            "type": "user"
          },
          "name": "Chun-Hsiao Yeh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:54.878Z",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a549",
          "user": {
            "_id": "641bd539aebaa27e07540613",
            "avatarUrl": "/avatars/000e8057e7280b9eba9bec5116d14718.svg",
            "isPro": false,
            "fullname": "chenyu wang",
            "user": "ch-chenyu",
            "type": "user"
          },
          "name": "Chenyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:52.696Z",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54a",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54b",
          "name": "Ta-Ying Cheng",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54c",
          "name": "Rouyu Wang",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54d",
          "name": "Tianzhe Chu",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54e",
          "name": "Yuexiang Zhai",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54f",
          "name": "Yubei Chen",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a550",
          "name": "Shenghua Gao",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a551",
          "name": "Yi Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:59:53.000Z",
      "submittedOnDailyAt": "2025-04-22T01:49:06.289Z",
      "title": "「Ver desde otras perspectivas: Evaluación de comprensión multidisciplinaria de MLLM」",
      "submittedOnDailyBy": {
        "_id": "637c7420f219c71f93ec8f81",
        "avatarUrl": "/avatars/969b72bd4320423af89e6a5d0ffa03cc.svg",
        "isPro": false,
        "fullname": "frog",
        "user": "frog123123123123",
        "type": "user"
      },
      "summary": "La comprensión de múltiples visiones y la integración de información visual en múltiples visiones es un problema básico en los modelos de lenguaje multimodal (MLLMs) que se utilizan por agentes automáticos. Los MLLMs recientes han demostrado una sorprendente evolución en la razón y planificación de alto nivel, pero presentan deficiencias en la coherencia geométrica de múltiples visiones y en las correspondencias entre visiones. Para evaluar completamente los problemas de la razón en el espacio de múltiples visiones de MLLMs, hemos colectado 2,100 pares de preguntas y respuestas de atención de más de 2,100 personas para 90 espacios reales diferentes. Este conjunto de datos, denominado Multi30K, proporciona una evaluación exhaustiva de la capacidad de MLLMs para entender y integrar múltiples visiones en diferentes contextos espaciales. Los resultados revelan que aunque los MLLMs han avanzado significativamente en la comprensión de la estructura y la relación entre visiones, aún tienen mucho por mejorar en la precisión y la coherencia en la interpretación de múltiples visiones en el mundo real.",
      "upvotes": 9,
      "discussionId": "680703e6c0ce7eea9ba1a63e",
      "projectPage": "https://danielchyeh.github.io/All-Angles-Bench/",
      "githubRepo": "https://github.com/Chenyu-Wang567/All-Angles-Bench/tree/main",
      "ai_keywords": [
        "Multi-Modal Large Language Models (MLLMs)",
        "multi-view geometric consistency",
        "cross-view correspondence",
        "All-Angles Bench",
        "Gemini-2.0-Flash",
        "Claude-3.7-Sonnet",
        "GPT-4o",
        "geometric correspondence",
        "camera pose estimation",
        "cross-view correspondence",
        "partially occluded views",
        "coarse camera poses",
        "multi-view awareness"
      ]
    },
    "publishedAt": "2025-04-21T13:59:53.000Z",
    "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in\n  MLLMs",
    "summary": "Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15280.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c7420f219c71f93ec8f81",
      "avatarUrl": "/avatars/969b72bd4320423af89e6a5d0ffa03cc.svg",
      "fullname": "frog",
      "name": "frog123123123123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14239",
      "authors": [
        {
          "_id": "6806e9e819a9fa609609fe65",
          "name": "Yuhang Liu",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe66",
          "user": {
            "_id": "64245f2c089d5fae56b4549a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
            "isPro": false,
            "fullname": "Pengxiang Li",
            "user": "pengxiang",
            "type": "user"
          },
          "name": "Pengxiang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:51:50.804Z",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe67",
          "user": {
            "_id": "629084f3a391a907d5e0484e",
            "avatarUrl": "/avatars/3a1d6d5aa90b3b8372505d13ccf8f2dd.svg",
            "isPro": false,
            "fullname": "unk",
            "user": "xieck13",
            "type": "user"
          },
          "name": "Congkai Xie",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-22T00:59:21.913Z",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe68",
          "name": "Xavier Hu",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe69",
          "user": {
            "_id": "650dde4ce14eeb01d42b37a1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dde4ce14eeb01d42b37a1/n5Yv24uofZ2XJjXdYCrKd.png",
            "isPro": false,
            "fullname": "Xiaotian Han",
            "user": "xiaotianhan",
            "type": "user"
          },
          "name": "Xiaotian Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:51:27.543Z",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe6a",
          "name": "Shengyu Zhang",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe6b",
          "name": "Hongxia Yang",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe6c",
          "name": "Fei Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-19T09:25:55.000Z",
      "submittedOnDailyAt": "2025-04-22T02:26:02.459Z",
      "title": "InfiGUI-R1: Desarrollo de agentes GUI multimodelo a partir de investigación cuantitativa de actores reactivos",
      "submittedOnDailyBy": {
        "_id": "64245f2c089d5fae56b4549a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
        "isPro": false,
        "fullname": "Pengxiang Li",
        "user": "pengxiang",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje multimodal de DamoDal (MLLMs) están impulsando agentes de interfaz gráfica de usuario (GUI) y demostrando buenos resultados en la automatización de tareas en dispositivos de cálculo. Recientes estudios evalúan razones en tareas de GUI y obtienen resultados evaluativos. Sin embargo, muchos enfoques actuales buscan razones robustas y adaptables en entornos complejos de GUI, basándose en templates de razones diseñados manualmente. Por otro lado, los agentes existentes principalmente funcionan como actores reactivos, perdiendo la profundidad necesaria para planificar y corregir errores en GUI, basándose en razones ocultas. Necesitamos que estos agentes se transformen en acciones basadas en razones decididas, en lugar de actuar continuamente como actores reactivos. Para fomentar esta transformación, presentamos InfiGUI-R1, un agente de GUI basado en MLLM, y desarrollamos un agente que evoluciona mediante un enfoque de entrenamiento de dos etapas centrado en razones, desarrollado a través de nuestro marco de trabajo Actor2Reasoner. El primer paso se centra en la introducción de razones, construyendo razones básicas. Utilizamos la distancia espacial de razones para propagar la capacidad de MLLM de entender razones en diferentes modalidades, integrando información visual de GUI y razones lógicas. El segundo paso se enfoca en la fortalecimiento de la decisión, entrenando las razones básicas en razones decisivas utilizando aprendizaje por refuerzo. En este paso, introducimos dos enfoques: la Guía de Sub-objetivos crea modelos que generan sub-objetivos precisos y otorga recompensas, mientras que la Construcción de Scenarios de Recuperación de Errores genera escenarios de entrenamiento para fallos y recuperación. Los resultados de los experimentos muestran que InfiGUI-R1 presenta excelentes resultados en el graping de GUI y en tareas de trazas. Más información está disponible en https://github.com/Reallm-Labs/InfiGUI-R1.",
      "upvotes": 7,
      "discussionId": "6806e9e919a9fa609609fea1",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Graphical User Interface (GUI) Agents",
        "Spatial Reasoning Distillation",
        "Actor2Reasoner framework",
        "Reasoning Injection",
        "Deliberation Enhancement",
        "Sub-goal Guidance",
        "Error Recovery Scenario Construction",
        "GUI grounding",
        "trajectory tasks"
      ]
    },
    "publishedAt": "2025-04-19T05:25:55.000Z",
    "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to\n  Deliberative Reasoners",
    "summary": "Multimodal Large Language Models (MLLMs) have powered Graphical User\nInterface (GUI) Agents, showing promise in automating tasks on computing\ndevices. Recent works have begun exploring reasoning in GUI tasks with\nencouraging results. However, many current approaches rely on manually designed\nreasoning templates, which may result in reasoning that is not sufficiently\nrobust and adaptive for complex GUI environments. Meanwhile, some existing\nagents continue to operate as Reactive Actors, relying primarily on implicit\nreasoning that may lack sufficient depth for GUI tasks demanding planning and\nerror recovery. We argue that advancing these agents requires a shift from\nreactive acting towards acting based on deliberate reasoning. To facilitate\nthis transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed\nthrough our Actor2Reasoner framework, a reasoning-centric, two-stage training\napproach designed to progressively evolve agents from Reactive Actors to\nDeliberative Reasoners. The first stage, Reasoning Injection, focuses on\nestablishing a basic reasoner. We employ Spatial Reasoning Distillation to\ntransfer cross-modal spatial reasoning capabilities from teacher models to\nMLLMs through trajectories with explicit reasoning steps, enabling models to\nintegrate GUI visual-spatial information with logical reasoning before action\ngeneration. The second stage, Deliberation Enhancement, refines the basic\nreasoner into a deliberative one using Reinforcement Learning. This stage\nintroduces two approaches: Sub-goal Guidance, which rewards models for\ngenerating accurate intermediate sub-goals, and Error Recovery Scenario\nConstruction, which creates failure-and-recovery training scenarios from\nidentified prone-to-error steps. Experimental results show InfiGUI-R1 achieves\nstrong performance in GUI grounding and trajectory tasks. Resources at\nhttps://github.com/Reallm-Labs/InfiGUI-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14239.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64245f2c089d5fae56b4549a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
      "fullname": "Pengxiang Li",
      "name": "pengxiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14655",
      "authors": [
        {
          "_id": "68070991b0cb768cae20c585",
          "name": "Yunhui Xia",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c586",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c587",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c588",
          "user": {
            "_id": "6731c04d5f55903a1d8c307c",
            "avatarUrl": "/avatars/704b1d628c55e3141194b08736f21267.svg",
            "isPro": false,
            "fullname": "Jason Klein Liu",
            "user": "jasonkleinlove",
            "type": "user"
          },
          "name": "Jason Klein Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-22T03:14:26.859Z",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c589",
          "name": "Huifeng Sun",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c58a",
          "name": "Siyue Wu",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c58b",
          "user": {
            "_id": "63f6c04ac96958470d1e9043",
            "avatarUrl": "/avatars/da46cdd9e21498e120ca91b67bfbfb5e.svg",
            "isPro": false,
            "fullname": "Jian Hu",
            "user": "chuyi777",
            "type": "user"
          },
          "name": "Jian Hu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-22T03:14:26.859Z",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c58c",
          "name": "Xiaolong Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-20T15:28:16.000Z",
      "submittedOnDailyAt": "2025-04-22T01:46:36.359Z",
      "title": "LeetCode Dataset: Dataset de datos de tiempo para entrenamiento eficiente y evaluación sólida",
      "submittedOnDailyBy": {
        "_id": "6468823272d9180d4ac90bdf",
        "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
        "isPro": false,
        "fullname": "Wei Shen",
        "user": "Swtheking",
        "type": "user"
      },
      "summary": "Introduzco el dataset LeetCodeDataset. Este dataset es una marca de referencia de alta calidad para resolver dos problemas importantes en la investigación de modelos de lenguaje grande (LLM). Resuelve la escasez de marcos de referencia lógicos de código y la escasez de conjuntos de entrenamiento y prueba independientes. Utilizando problemas de Python de LeetCode como fuente de datos, este dataset cuenta con una amplia gama de metadatos, una amplia cobertura y un número de casos de prueba de 100 puntos o más para cada problema. Mediante la división temporal en tiempo (aproximadamente julio de 2024), permite evaluaciones no conectadas y ajustes eficientes con reglas (SFT). Según los experimentos, modelos lógicos son significativamente mejores que modelos ilógicos, y un SFT que generó 2.6K modelos logró alcanzar el rendimiento de un modelo con 110K muestras. Este dataset y el marco de evaluación están disponibles en Hugging Face y GitHub.",
      "upvotes": 6,
      "discussionId": "68070992b0cb768cae20c5ca",
      "ai_keywords": [
        "LeetCodeDataset",
        "code-generation models",
        "LLM research",
        "evaluation",
        "supervised fine-tuning (SFT)",
        "reasoning models"
      ]
    },
    "publishedAt": "2025-04-20T11:28:16.000Z",
    "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient\n  Training of Code LLMs",
    "summary": "We introduce LeetCodeDataset, a high-quality benchmark for evaluating and\ntraining code-generation models, addressing two key challenges in LLM research:\nthe lack of reasoning-focused coding benchmarks and self-contained training\ntestbeds. By curating LeetCode Python problems with rich metadata, broad\ncoverage, 100+ test cases per problem, and temporal splits (pre/post July\n2024), our dataset enables contamination-free evaluation and efficient\nsupervised fine-tuning (SFT). Experiments show reasoning models significantly\noutperform non-reasoning counterparts, while SFT with only 2.6K model-generated\nsolutions achieves performance comparable to 110K-sample counterparts. The\ndataset and evaluation framework are available on Hugging Face and Github.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14655.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6468823272d9180d4ac90bdf",
      "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
      "fullname": "Wei Shen",
      "name": "Swtheking",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13805",
      "authors": [
        {
          "_id": "680662b97415e191e3579b9e",
          "user": {
            "_id": "64d761b98ebc40443831f82a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
            "isPro": false,
            "fullname": "lgy0404",
            "user": "lgy0404",
            "type": "user"
          },
          "name": "Guangyi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:52:04.194Z",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579b9f",
          "user": {
            "_id": "65a088f4300957620ba45c70",
            "avatarUrl": "/avatars/56ed45e10d3455531979f30881b2d3f9.svg",
            "isPro": false,
            "fullname": "pengxiang zhao",
            "user": "Pengxiangzhao",
            "type": "user"
          },
          "name": "Pengxiang Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:52:01.390Z",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba0",
          "name": "Liang Liu",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba1",
          "name": "Zhiming Chen",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba2",
          "name": "Yuxiang Chai",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba3",
          "name": "Shuai Ren",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba4",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba5",
          "name": "Shibo He",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba6",
          "name": "Wenchao Meng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T17:13:34.000Z",
      "submittedOnDailyAt": "2025-04-22T01:14:18.820Z",
      "title": "LearnAct: Benchmark de un Agente de Interfaz Gráfica Movil con Muestras Unificadas",
      "submittedOnDailyBy": {
        "_id": "6458ce236fa580137af5aa95",
        "avatarUrl": "/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg",
        "isPro": false,
        "fullname": "Yuxiang Chai",
        "user": "Yuxiang007",
        "type": "user"
      },
      "summary": "El modelo de GUI móvil de agente demostró buenos resultados en la automatización de tareas, pero encontró problemas para generalizarse en diferentes escenarios reales. El enfoque tradicional de aprendizaje previo y ajuste fino enfrenta dificultades con la diversidad de aplicaciones móviles y las tareas específicas de los usuarios. Enfatizamos en mejorar la capacidad del agente de GUI móvil a través de ejemplos humanos, priorizando el mejoramiento en escenarios no vistos y renunciando al aprendizaje generalizado con grandes conjuntos de datos. Para lograr esto, presentamos el primer conjunto de datos detallado para el aprendizaje, LearnGUI. Este conjunto incluye 2,252 tareas offline y 101 tareas online, así como ejemplos de alta calidad humanos. Además, desarrollamos un marco de trabajo complejo de agentes, LearnAct, para mejorar la completación de las tareas. Este marco combina tres agentes especializados: DemoParser para extraer conocimiento, KnowSeeker para buscar conocimiento asociado y ActExecutor para ejecutar tareas basadas en ejemplos. Los resultados de nuestros experimentos muestran una mejora significativa en las evaluaciones offline y online. En la evaluación offline, un ejemplo mejoró el rendimiento del modelo y aumentó la precisión de Gemini-1.5-Pro del 19.3% al 51.7%. En la evaluación online, el rendimiento de la tasa de éxito de la tarea de UI-TARS-7B-SFT se incrementó del 18.1% al 32.8%. El marco de trabajo LearnAct y el benchmark LearnGUI reconocen una dirección deseable para un aprendizaje basado en ejemplos, haciendo que el aprendizaje sea más adaptativo y facilitando la implementación de plataformas o aplicaciones móviles personalizadas.",
      "upvotes": 6,
      "discussionId": "680662bd7415e191e3579c7d",
      "ai_keywords": [
        "LearnGUI",
        "LearnAct",
        "DemoParser",
        "KnowSeeker",
        "ActExecutor",
        "demonstration-based learning",
        "mobile GUI agents",
        "human demonstrations",
        "multi-agent framework"
      ]
    },
    "publishedAt": "2025-04-18T13:13:34.000Z",
    "title": "LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration\n  Benchmark",
    "summary": "Mobile GUI agents show promise in automating tasks but face generalization\nchallenges in diverse real-world scenarios. Traditional approaches using\npre-training or fine-tuning with massive datasets struggle with the diversity\nof mobile applications and user-specific tasks. We propose enhancing mobile GUI\nagent capabilities through human demonstrations, focusing on improving\nperformance in unseen scenarios rather than pursuing universal generalization\nthrough larger datasets. To realize this paradigm, we introduce LearnGUI, the\nfirst comprehensive dataset specifically designed for studying\ndemonstration-based learning in mobile GUI agents, comprising 2,252 offline\ntasks and 101 online tasks with high-quality human demonstrations. We further\ndevelop LearnAct, a sophisticated multi-agent framework that automatically\nextracts knowledge from demonstrations to enhance task completion. This\nframework integrates three specialized agents: DemoParser for knowledge\nextraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for\ndemonstration-enhanced task execution. Our experimental results show\nsignificant performance gains in both offline and online evaluations. In\noffline assessments, a single demonstration improves model performance,\nincreasing Gemini-1.5-Pro's accuracy from 19.3% to 51.7%. In online\nevaluations, our framework enhances UI-TARS-7B-SFT's task success rate from\n18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish\ndemonstration-based learning as a promising direction for more adaptable,\npersonalized, and deployable mobile GUI agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13805.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6458ce236fa580137af5aa95",
      "avatarUrl": "/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg",
      "fullname": "Yuxiang Chai",
      "name": "Yuxiang007",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08902",
      "authors": [
        {
          "_id": "680730598ce2be7f5450bd77",
          "user": {
            "_id": "64e3950d9ec4cf50009ce960",
            "avatarUrl": "/avatars/8aade78bdde8423bd811ca8394cb08c5.svg",
            "isPro": false,
            "fullname": "Pascal Chang",
            "user": "pascalchang87",
            "type": "user"
          },
          "name": "Pascal Chang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:31.652Z",
          "hidden": false
        },
        {
          "_id": "680730598ce2be7f5450bd78",
          "user": {
            "_id": "66a38c9e053b7e9c907fba72",
            "avatarUrl": "/avatars/59588845e21ec6ad63631c892066f4ad.svg",
            "isPro": false,
            "fullname": "Sergio Sancho",
            "user": "ssancho",
            "type": "user"
          },
          "name": "Sergio Sancho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:28.796Z",
          "hidden": false
        },
        {
          "_id": "680730598ce2be7f5450bd79",
          "name": "Jingwei Tang",
          "hidden": false
        },
        {
          "_id": "680730598ce2be7f5450bd7a",
          "name": "Markus Gross",
          "hidden": false
        },
        {
          "_id": "680730598ce2be7f5450bd7b",
          "name": "Vinicius C. Azevedo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T18:12:01.000Z",
      "submittedOnDailyAt": "2025-04-22T04:59:10.950Z",
      "title": "Locarling Glass: Generative Anamorphosis by Laplacian Pyramid Warping",
      "submittedOnDailyBy": {
        "_id": "66863bd107019f3fe48a21ab",
        "avatarUrl": "/avatars/3297e18e43d40e902b9554a077a34a8a.svg",
        "isPro": false,
        "fullname": "Manuel Kansy",
        "user": "manuelkansy",
        "type": "user"
      },
      "summary": "Anamorphosis es un tipo de imagen que parece no ser reconocible al verlo directamente, pero su clase puede ser identificada. Su forma es visible solo desde ciertos puntos de vista. La configuración de estas disposiciones matemáticas se puede trazar hasta el siglo XVII, pero solo es visible desde ciertos puntos de vista y pierde significado cuando se ve de manera general. En este artículo, se revisa esta famosa óptica anamorfósica y se propone una nueva metodología agregando deformaciones generativas. Se propone un método para generar imágenes anamorfósicas que mantienen una interpretación válida al ser vistas directamente, utilizando modelos de normalización potencial. Para ello, se introduce el warping de la pirámide laplaciana y se resume un método de warping de imágenes en función de la frecuencia para la generación de imágenes de alta calidad. Nuestro trabajo extiende el modelo Visual Anagrams (arXiv:2311.17919) a espacios potenciales, permitiendo la creación de nuevas ópticas generativas para transformaciones espaciales amplias.",
      "upvotes": 6,
      "discussionId": "6807305f8ce2be7f5450bf69",
      "ai_keywords": [
        "latent rectified flow models",
        "Laplacian Pyramid Warping",
        "frequency-aware image warping",
        "generative perceptual illusions"
      ]
    },
    "publishedAt": "2025-04-11T14:12:01.000Z",
    "title": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping",
    "summary": "Anamorphosis refers to a category of images that are intentionally distorted,\nmaking them unrecognizable when viewed directly. Their true form only reveals\nitself when seen from a specific viewpoint, which can be through some\ncatadioptric device like a mirror or a lens. While the construction of these\nmathematical devices can be traced back to as early as the 17th century, they\nare only interpretable when viewed from a specific vantage point and tend to\nlose meaning when seen normally. In this paper, we revisit these famous optical\nillusions with a generative twist. With the help of latent rectified flow\nmodels, we propose a method to create anamorphic images that still retain a\nvalid interpretation when viewed directly. To this end, we introduce Laplacian\nPyramid Warping, a frequency-aware image warping technique key to generating\nhigh-quality visuals. Our work extends Visual Anagrams (arXiv:2311.17919) to\nlatent space models and to a wider range of spatial transforms, enabling the\ncreation of novel generative perceptual illusions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08902.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "66863bd107019f3fe48a21ab",
      "avatarUrl": "/avatars/3297e18e43d40e902b9554a077a34a8a.svg",
      "fullname": "Manuel Kansy",
      "name": "manuelkansy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15217",
      "authors": [
        {
          "_id": "68070e1895e06498588c4497",
          "user": {
            "_id": "63239bd492e07e3ca2068a16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63239bd492e07e3ca2068a16/OFESFFWQVHC1EtJpnGOWv.jpeg",
            "isPro": true,
            "fullname": "Yatong Bai",
            "user": "Bai-YT",
            "type": "user"
          },
          "name": "Yatong Bai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:50.514Z",
          "hidden": false
        },
        {
          "_id": "68070e1895e06498588c4498",
          "name": "Jonah Casebeer",
          "hidden": false
        },
        {
          "_id": "68070e1895e06498588c4499",
          "name": "Somayeh Sojoudi",
          "hidden": false
        },
        {
          "_id": "68070e1895e06498588c449a",
          "user": {
            "_id": "648119ee7a741c7f33f49f25",
            "avatarUrl": "/avatars/183d2143ba20e1cc8712c63c055aadd7.svg",
            "isPro": false,
            "fullname": "Nicholas J. Bryan",
            "user": "Njb",
            "type": "user"
          },
          "name": "Nicholas J. Bryan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-22T03:35:40.134Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/648119ee7a741c7f33f49f25/V_gAc821aOtnXsoaUFvCQ.mp4"
      ],
      "publishedAt": "2025-04-21T16:41:40.000Z",
      "submittedOnDailyAt": "2025-04-22T02:07:09.310Z",
      "title": "DRAGON: Optimización de Distribución de Repartos Repartos Repartos Modelo de Herencia de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramificación de Ramifica",
      "submittedOnDailyBy": {
        "_id": "648119ee7a741c7f33f49f25",
        "avatarUrl": "/avatars/183d2143ba20e1cc8712c63c055aadd7.svg",
        "isPro": false,
        "fullname": "Nicholas J. Bryan",
        "user": "Njb",
        "type": "user"
      },
      "summary": "DRAGON es un marco de trabajo que utiliza diversas metodologías para alinear un modelo generativo con el output deseado. No se concentra solo en evaluar funciones de valor existentes, sino que también puede evaluar sus distribuciones, lo que permite evaluar una amplia gama de valores relacionados con ejemplos individuales, la relación entre ejemplos y distribuciones, y la relación entre distribuciones. Esta diversidad se utiliza para seleccionar un encoder y ejemplos para crear una distribución de muestras y construir nuevas funciones de valor. En el caso de un encoder multimodal, los ejemplos pueden cambiar a datos de otras modalidades, como texto o audio. A continuación, DRAGON reúne la generación en línea y sobrecarga, los cuales se puntuan para construir conjuntos de ejemplos positivos y negativos, y utiliza estas contrastes para maximizar la recompensa. La evaluación utiliza 20 funciones de valor para finetunar un modelo de expansión musical a partir de texto de audio, incluyendo modelos de evaluación de música personalizados, puntuaciones CLAP, diversidad Vendi y distancia de audio Frechet (FAD). Además, se comparan las configuraciones de FAD para canciones individuales y conjuntos de datos completos, y se omiten varios encoderes de FAD y conjuntos de referencia. En total, DRAGON alcanza un promedio de tasa de éxito del 81.45% con las 20 funciones de valor objetivo. Además, las funciones de valor basadas en conjuntos de muestras mejoran significativamente la generación y pueden compararse con funciones de valor basadas en modelos. Usando conjuntos de muestras adecuados, DRAGON logra un 60.95% de éxito en la calidad de la música, independientemente de la preferencia de la raza en los datos de entrenamiento. De este modo, DRAGON presenta una nueva aproximación para el diseño y optimización de funciones de valor que mejoren la calidad de la música en observaciones humanas. Los ejemplos de sonido están disponibles para descargar en https://ml-dragon.github.io/web.",
      "upvotes": 5,
      "discussionId": "68070e1f95e06498588c467f",
      "ai_keywords": [
        "Distributional RewArds for Generative OptimizatioN (DRAGON)",
        "fine-tuning",
        "media generation models",
        "reinforcement learning with human feedback (RLHF)",
        "direct preference optimization (DPO)",
        "reward functions",
        "exemplar distribution",
        "cross-modality encoders",
        "CLAP",
        "online and on-policy generations",
        "positive demonstration set",
        "negative set",
        "contrast",
        "audio-domain text-to-music diffusion model",
        "music aesthetics model",
        "Vendi diversity",
        "Frechet audio distance (FAD)",
        "full-dataset FAD",
        "human-voted music quality"
      ]
    },
    "publishedAt": "2025-04-21T12:41:40.000Z",
    "title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models",
    "summary": "We present Distributional RewArds for Generative OptimizatioN (DRAGON), a\nversatile framework for fine-tuning media generation models towards a desired\noutcome. Compared with traditional reinforcement learning with human feedback\n(RLHF) or pairwise preference approaches such as direct preference optimization\n(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate\neither individual examples or distributions of them, making it compatible with\na broad spectrum of instance-wise, instance-to-distribution, and\ndistribution-to-distribution rewards. Leveraging this versatility, we construct\nnovel reward functions by selecting an encoder and a set of reference examples\nto create an exemplar distribution. When cross-modality encoders such as CLAP\nare used, the reference examples may be of a different modality (e.g., text\nversus audio). Then, DRAGON gathers online and on-policy generations, scores\nthem to construct a positive demonstration set and a negative set, and\nleverages the contrast between the two sets to maximize the reward. For\nevaluation, we fine-tune an audio-domain text-to-music diffusion model with 20\ndifferent reward functions, including a custom music aesthetics model, CLAP\nscore, Vendi diversity, and Frechet audio distance (FAD). We further compare\ninstance-wise (per-song) and full-dataset FAD settings while ablating multiple\nFAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an\n81.45% average win rate. Moreover, reward functions based on exemplar sets\nindeed enhance generations and are comparable to model-based rewards. With an\nappropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality\nwin rate without training on human preference annotations. As such, DRAGON\nexhibits a new approach to designing and optimizing reward functions for\nimproving human-perceived quality. Sound examples at\nhttps://ml-dragon.github.io/web.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/648119ee7a741c7f33f49f25/V_gAc821aOtnXsoaUFvCQ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15217.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648119ee7a741c7f33f49f25",
      "avatarUrl": "/avatars/183d2143ba20e1cc8712c63c055aadd7.svg",
      "fullname": "Nicholas J. Bryan",
      "name": "Njb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15133",
      "authors": [
        {
          "_id": "68073a2d19a9fa6096218691",
          "name": "Ziwen Xu",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218692",
          "name": "Shuxun Wang",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218693",
          "name": "Kewei Xu",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218694",
          "name": "Haoming Xu",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218695",
          "name": "Mengru Wang",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218696",
          "name": "Xinle Deng",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218697",
          "name": "Yunzhi Yao",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218698",
          "name": "Guozhou Zheng",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218699",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa609621869a",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T14:33:55.000Z",
      "submittedOnDailyAt": "2025-04-22T05:12:13.220Z",
      "title": "EasyEdit2: F ramework sencillo para la edición de modelos de lenguaje grande",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "En este artículo, se presenta el marco de trabajo EasyEdit2, que proporciona la capacidad de controlar las acciones de un LLM (Modelo de Largo Marco) mediante plugins y ajustes. EasyEdit2 apoya una amplia gama de interacciones en pruebas de seguridad, emociones, personalidades, patrones de razonamiento, verdad y características lingüísticas. A diferencia de los trabajos anteriores, EasyEdit2 destaca por su arquitectura innovadora que permite el control de modelos de manera indiscriminada. Esta arquitectura incluye módulos clave como generadores de vectores de streaming y aplicadores de vectores de streaming, lo que permite la generación y aplicación automática de vectores de streaming que pueden afectar el comportamiento del modelo mientras se cambian sus parámetros. Una de las principales ventajas de EasyEdit2 es su facilidad de uso, ya que no requiere conocimientos técnicos avanzados. Una persona puede guiar y ajustar eficazmente la respuesta del modelo, y alcanzar un control determinante de manera eficiente. Experimentalmente, se reporta el rendimiento de control del modelo para diferentes LLMs, demostrando la eficacia de esta tecnología. Se ha publicado el código fuente en GitHub en https://github.com/zjunlp/EasyEdit y se ofrece un notebook de demostración. Además, se proporciona una demostración vía video en https://zjunlp.github.io/project/EasyEdit2/video para una introducción rápida.",
      "upvotes": 4,
      "discussionId": "68073a2e19a9fa60962186db",
      "projectPage": "https://zjunlp.github.io/project/EasyEdit2/",
      "githubRepo": "https://github.com/zjunlp/EasyEdit",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "test-time interventions",
        "steering vector generator",
        "steering vector applier",
        "seamless model steering",
        "model steering performance"
      ]
    },
    "publishedAt": "2025-04-21T10:33:55.000Z",
    "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models",
    "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15133.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14717",
      "authors": [
        {
          "_id": "680720e066b60d551b653f1b",
          "name": "Bowei Zhang",
          "hidden": false
        },
        {
          "_id": "680720e066b60d551b653f1c",
          "name": "Lei Ke",
          "hidden": false
        },
        {
          "_id": "680720e066b60d551b653f1d",
          "name": "Adam W. Harley",
          "hidden": false
        },
        {
          "_id": "680720e066b60d551b653f1e",
          "name": "Katerina Fragkiadaki",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/D3KLUoyXGD0mILRMznzGG.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/qsRjn3Gr351OY_A8w0tGc.gif"
      ],
      "publishedAt": "2025-04-20T19:09:43.000Z",
      "submittedOnDailyAt": "2025-04-22T03:25:44.916Z",
      "title": "TAPIP3D: Trazamiento de cualquier punto en la geometría tridimensional eterna",
      "submittedOnDailyBy": {
        "_id": "6258a6455ea3a0a9b6de3f22",
        "avatarUrl": "/avatars/6eeed72a97fb24465e5e65583fbe50cf.svg",
        "isPro": false,
        "fullname": "Lei Ke",
        "user": "lkeab",
        "type": "user"
      },
      "summary": "TAPIP3D introduce una nueva aproximación para el seguimiento tridimensional de puntos a largo plazo en vídeos de RGB y RGB-D. TAPIP3D representa los vídeos mediante una nube de características espacial-temporales estables de la cámara, y utiliza información de profundidad y movimiento de la cámara para mapear las características 2D del vídeo a un espacio tridimensional del mundo, efectivamente cancelando el movimiento de la cámara. En esta representación estable, TAPIP3D puede refinar el movimiento tridimensional a largo plazo para realizar un seguimiento fuerte. Para gestionar la distribución irregular de los puntos 3D, propone una estructura de atención pareja local. Esta estrategia de contextualización 3D utiliza efectivamente las relaciones espaciales en 3D para formar características nativas que aportan información para el cálculo preciso de las trayectorias 3D. Nuestro enfoque centrado en 3D supera significativamente los métodos actuales de seguimiento de puntos 3D, y, en caso de tener profundidad precisa, mejora la precisión del seguimiento 2D en comparación con los seguidores 2D tradicionales. Este enfoque soporta inferencia tanto en coordenadas de la cámara (es decir, coordenadas no estables) como en coordenadas del mundo, y corregir el movimiento de la cámara para mejorar el rendimiento del seguimiento. Nuestro enfoque reemplaza los tradicionales nucleos cuadrados 2D utilizados en seguidores 2D y 3D anteriores, permitiendo obtener resultados más robustos y precisos en diferentes benchmarks de seguimiento de puntos 3D. Página del proyecto: https://tapip3d.github.io",
      "upvotes": 4,
      "discussionId": "680720e166b60d551b653f7d",
      "projectPage": "https://tapip3d.github.io/",
      "githubRepo": "https://github.com/zbw001/TAPIP3D",
      "ai_keywords": [
        "camera-stabilized",
        "spatio-temporal feature clouds",
        "depth and camera motion information",
        "2D video features",
        "3D world space",
        "multi-frame 3D motion estimates",
        "Local Pair Attention mechanism",
        "3D contextualization strategy",
        "spatial relationships in 3D",
        "feature neighborhoods",
        "3D trajectory estimation",
        "3D point tracking",
        "2D tracker",
        "inference in both camera coordinates",
        "world coordinates",
        "3D point tracking benchmarks",
        "2D square correlation neighborhoods"
      ]
    },
    "publishedAt": "2025-04-20T15:09:43.000Z",
    "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry",
    "summary": "We introduce TAPIP3D, a novel approach for long-term 3D point tracking in\nmonocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized\nspatio-temporal feature clouds, leveraging depth and camera motion information\nto lift 2D video features into a 3D world space where camera motion is\neffectively canceled. TAPIP3D iteratively refines multi-frame 3D motion\nestimates within this stabilized representation, enabling robust tracking over\nextended periods. To manage the inherent irregularities of 3D point\ndistributions, we propose a Local Pair Attention mechanism. This 3D\ncontextualization strategy effectively exploits spatial relationships in 3D,\nforming informative feature neighborhoods for precise 3D trajectory estimation.\nOur 3D-centric approach significantly outperforms existing 3D point tracking\nmethods and even enhances 2D tracking accuracy compared to conventional 2D\npixel trackers when accurate depth is available. It supports inference in both\ncamera coordinates (i.e., unstabilized) and world coordinates, and our results\ndemonstrate that compensating for camera motion improves tracking performance.\nOur approach replaces the conventional 2D square correlation neighborhoods used\nin prior 2D and 3D trackers, leading to more robust and accurate results across\nvarious 3D point tracking benchmarks. Project Page: https://tapip3d.github.io",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/D3KLUoyXGD0mILRMznzGG.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/qsRjn3Gr351OY_A8w0tGc.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14717.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6258a6455ea3a0a9b6de3f22",
      "avatarUrl": "/avatars/6eeed72a97fb24465e5e65583fbe50cf.svg",
      "fullname": "Lei Ke",
      "name": "lkeab",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15047",
      "authors": [
        {
          "_id": "6806fb8e27dabde0a109776d",
          "user": {
            "_id": "645b663eca5d8a297712f2e1",
            "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
            "isPro": false,
            "fullname": "Quy-Anh Dang",
            "user": "quyanh",
            "type": "user"
          },
          "name": "Quy-Anh Dang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:51:09.578Z",
          "hidden": false
        },
        {
          "_id": "6806fb8e27dabde0a109776e",
          "user": {
            "_id": "63105f7463b70252b4775783",
            "avatarUrl": "/avatars/89cfe71302cde6c2834f58d44bbcdbd5.svg",
            "isPro": false,
            "fullname": "Chris Ngo",
            "user": "tnngo2",
            "type": "user"
          },
          "name": "Chris Ngo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:51:07.511Z",
          "hidden": false
        },
        {
          "_id": "6806fb8e27dabde0a109776f",
          "name": "Truong-Son Hy",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/XbUmMPAitDuX-g7n50bTa.png",
        "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/a16jiYkgx_UtIyM9BiBc2.png"
      ],
      "publishedAt": "2025-04-21T12:04:57.000Z",
      "submittedOnDailyAt": "2025-04-22T00:47:26.080Z",
      "title": "Raybo Plus: Generación y fortalecimiento de pronósticos de combate mediante el uso de la tecnología de Eboruchitueru Karitedevizhousini",
      "submittedOnDailyBy": {
        "_id": "645b663eca5d8a297712f2e1",
        "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
        "isPro": false,
        "fullname": "Quy-Anh Dang",
        "user": "quyanh",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) muestran capacidades excepcionales, pero son vulnerables a los prompts contrarios que los utilizan para generar salidas inestables o sesgadas. El método actual de mezcla de equipos rojos tiene problemas de escalabilidad, requiere recursos y limita la diversidad de las estrategias de ataque. Proponemos un nuevo marco de mezcla de equipos rojos basado en cálculo evolutivo llamado \"RainbowPlus\". Este método incorpora innovaciones en algoritmos evolutivos clásicos (por ejemplo, MAP-Elites) y fortalece la generación de prompts contrarios a través de búsquedas adaptativas de calidad y diversidad. Utiliza una arquitectura de archivo multi-elemento para almacenar prompts de alta calidad diversos y una función de evaluación que evalúa múltiples prompts en paralelo, superando así los limites de un solo archivo de prompts y la restricción de dos comparaciones de métodos anteriores como Rainbow Teaming. En experimentos comparativos con 6 conjuntos de datos de benchmark y 4 LLMs de código abierto, RainbowPlus muestra un excelente rendimiento de éxito de ataque (ASR) y una diversidad (Diverse-Score aproximadamente 0.84), generando más de 100 veces más prompts únicos (por ejemplo, 10,418 vs. 100 para Ministral-8B-Instruct-2410). Con respecto a los 9 métodos óptimos del dataset HarmBench, utilizando 12 LLMs (10 de código abierto y 2 de código cerrado), RainbowPlus alcanza un ASR promedio de 81.1%, superando a AutoDAN-Turbo en un margen de 3.9% y es 9 veces más rápido, demorando solo 1.45 horas en lugar de 13.50 horas. Este método fomenta el desarrollo de la seguridad de los LLMs y proporciona una herramienta escalable para la evaluación de vulnerabilidades. Los códigos y recursos están disponibles en https://github.com/knoveleng/rainbowplus.",
      "upvotes": 3,
      "discussionId": "6806fb9127dabde0a1097849",
      "githubRepo": "https://github.com/knoveleng/rainbowplus",
      "ai_keywords": [
        "adversarial prompts",
        "vulnerabilities",
        "unsafe outputs",
        "biased outputs",
        "red-teaming methods",
        "scalability challenges",
        "resource-intensive requirements",
        "quality-diversity (QD) search",
        "evolutionary computation",
        "MAP-Elites",
        "adaptive quality-diversity (QD) search",
        "multi-element archive",
        "fitness function",
        "single-prompt archives",
        "pairwise comparisons",
        "attack success rate (ASR)",
        "Diverse-Score",
        "superior attack success rate (ASR)",
        "diversity",
        "unique prompts",
        "benchmark datasets",
        "open-source LLMS",
        "HarmBench",
        "closed-source",
        "AutoDAN-Turbo",
        "vulnerability assessment"
      ]
    },
    "publishedAt": "2025-04-21T08:04:57.000Z",
    "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary\n  Quality-Diversity Search",
    "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score approx 0.84), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/XbUmMPAitDuX-g7n50bTa.png",
      "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/a16jiYkgx_UtIyM9BiBc2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15047.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "645b663eca5d8a297712f2e1",
      "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
      "fullname": "Quy-Anh Dang",
      "name": "quyanh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15270",
      "authors": [
        {
          "_id": "68074ea84a1c690663814e56",
          "name": "Ji Qi",
          "hidden": false
        },
        {
          "_id": "68074ea84a1c690663814e57",
          "name": "Yuan Yao",
          "hidden": false
        },
        {
          "_id": "68074ea84a1c690663814e58",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "68074ea84a1c690663814e59",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "68074ea84a1c690663814e5a",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "68074ea84a1c690663814e5b",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "68074ea84a1c690663814e5c",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:57:21.000Z",
      "submittedOnDailyAt": "2025-04-22T06:39:45.004Z",
      "title": "Tecnología de Compresión Estructurada de Imágenes Fortalecida para la Comprensión Eficiente de Imágenes Utilizando LMM",
      "submittedOnDailyBy": {
        "_id": "64ed568ccf6118a9379a61b8",
        "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
        "isPro": false,
        "fullname": "Yushi Bai",
        "user": "bys0318",
        "type": "user"
      },
      "summary": "Los modelos de grandes multi-object tracking (LMMs) generan un efecto computacional de eficacia en TV, reconociendo secuencias de frames continuos y considerando la variación inherente en la densidad temporal de la información en la TV. En este artículo, se presenta Quicksviewer. Quicksviewer utiliza la técnica de la belleza de Sohmox para dividir la densidad inhomogénea de la TV en granes de luz (voxels), y realiza un resampling uniforme en cada gran de luz para implementar un nuevo paradigma de reconocimiento eficiente, el LMM. Este enfoque sencillo y intuitivo permite comprimir en tiempo dinámico basado en la densidad temporal, reduciendo significativamente la redundancia espacio-temporal (compresión del 45 por ciento), y facilita un entrenamiento eficiente con un ancho de visión amplio. Se entrena el modelo en una pantalla de texto y procesa, en tres etapas, un largo video promedio de 420 segundos a 1 fps, aumentando la eficiencia del reconocimiento. Con un total de 0.8M de muestras de texto y video, Quicksviewer logra un aumento de precisión máximo de 8.72 puntos sobre un límite directo de comparación, demostrando su eficacia. En Video-MME, Quicksviewer utiliza menos del 5% de tokens por frame necesarios para el baseline, mostrando un rendimiento avanzado con secuencias ligeras. En este paradigma, al aumentar la cantidad de frames de entrada, se observa una ley clara de poder de la capa de poder del modelo. Además, los segmentos generados por la red de voxels ayudan a analizar los eventos continuos de la TV.",
      "upvotes": 2,
      "discussionId": "68074eab4a1c690663814ef7",
      "projectPage": "https://quicksviewer.github.io/",
      "githubRepo": "https://github.com/quicksviewer/quicksviewer",
      "ai_keywords": [
        "LMMs (Large Multimodal Models)",
        "Quicksviewer",
        "Gumbel Softmax",
        "spatiotemporal redundancy",
        "large receptive field",
        "language backbone",
        "Video-MME",
        "power law",
        "cubing network"
      ]
    },
    "publishedAt": "2025-04-21T13:57:21.000Z",
    "title": "An LMM for Efficient Video Understanding via Reinforced Compression of\n  Video Cubes",
    "summary": "Large Multimodal Models (LMMs) uniformly perceive video frames, creating\ncomputational inefficiency for videos with inherently varying temporal\ninformation density. This paper present Quicksviewer, an LMM with new\nperceiving paradigm that partitions a video of nonuniform density into varying\ncubes using Gumbel Softmax, followed by a unified resampling for each cube to\nachieve efficient video understanding. This simple and intuitive approach\ndynamically compress video online based on its temporal density, significantly\nreducing spatiotemporal redundancy (overall 45times compression rate), while\nenabling efficient training with large receptive field. We train the model from\na language backbone through three progressive stages, each incorporating\nlengthy videos on average of 420s/1fps thanks to the perceiving efficiency.\nWith only 0.8M total video-text samples for training, our model outperforms the\ndirect baseline employing a fixed partitioning strategy by a maximum of 8.72 in\naccuracy, demonstrating the effectiveness in performance. On Video-MME,\nQuicksviewer achieves SOTA under modest sequence lengths using just up to 5\\%\nof tokens per frame required by baselines. With this paradigm, scaling up the\nnumber of input frames reveals a clear power law of the model capabilities. It\nis also empirically verified that the segments generated by the cubing network\ncan help for analyzing continuous events in videos.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ed568ccf6118a9379a61b8",
      "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
      "fullname": "Yushi Bai",
      "name": "bys0318",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14899",
      "authors": [
        {
          "_id": "68075a2bdadb28dddc13dfc6",
          "name": "Chenjie Cao",
          "hidden": false
        },
        {
          "_id": "68075a2bdadb28dddc13dfc7",
          "name": "Jingkai Zhou",
          "hidden": false
        },
        {
          "_id": "68075a2bdadb28dddc13dfc8",
          "name": "Shikai Li",
          "hidden": false
        },
        {
          "_id": "68075a2bdadb28dddc13dfc9",
          "name": "Jingyun Liang",
          "hidden": false
        },
        {
          "_id": "68075a2bdadb28dddc13dfca",
          "name": "Chaohui Yu",
          "hidden": false
        },
        {
          "_id": "68075a2bdadb28dddc13dfcb",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "68075a2bdadb28dddc13dfcc",
          "name": "Xiangyang Xue",
          "hidden": false
        },
        {
          "_id": "68075a2bdadb28dddc13dfcd",
          "name": "Yanwei Fu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64892bc82eafb7b91182bec5/ldOrEdwnbNQcTKZBcT4Oq.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/64892bc82eafb7b91182bec5/g5LyXAgsA-XhlQDd-q-_S.mp4"
      ],
      "publishedAt": "2025-04-21T07:10:41.000Z",
      "submittedOnDailyAt": "2025-04-22T07:30:21.972Z",
      "title": "Uni3C: Sistema de generación de vídeo de precisión integrando cámaras 3D de fortalecimiento y control de movimientos humanos",
      "submittedOnDailyBy": {
        "_id": "64892bc82eafb7b91182bec5",
        "avatarUrl": "/avatars/c9eed91e60f3ea227b35e111d3fc4200.svg",
        "isPro": false,
        "fullname": "chenjie cao",
        "user": "ewrfcas",
        "type": "user"
      },
      "summary": "La control de cámaras y movimientos humanos se ha estudiado ampliamente en el campo de la generación de vídeos, pero los métodos actuales generalmente tratan estos aspectos separadamente y están limitados por la disponibilidad de datos de etiquetado de alta calidad. Para superar estos desafíos, se propone Well3C (Uni3C), un marco de trabajo integrado de expansión 3D para el control preciso de cámaras y movimientos humanos en la generación de vídeos. Uni3C presenta dos contribuciones principales. En primer lugar, se propone el Flag & Play Controller PCDController, un controlador de movimientos de cámaras que se entrena cuando el fondo de vídeo está inmovil, utilizando clusters de puntos de no-entrada de vídeos de vídeos de vídeos. Estos clusters, con su fuerte liderazgo 3D y la fuerza de los modelos de vídeo fundamentales, permiten que PCDController demostre una impresionante capacidad de generalización, mostrando flexibilidad que permite que otros módulos de Uni3C puedan entrenarse en áreas específicas y se centrar en el control de cámaras o movimientos humanos, reduciendo así la dependencia de datos etiquetados comúnmente. En segundo lugar, se propone un guía 3D común en la etapa de inferencia, que integra de manera sintética puntos de clusters de escena y personajes SMPL-X, y los señales de control de cámaras y movimientos humanos se unifican para realizar un control efectivo de la generación de vídeos, dependiendo de si el fondo está ajustado o no. Los experimentos extendidos demuestran la fuerte robustez de PCDController al controlar la movimiento de cámaras con fondos ajustados. Uni3C mejora significativamente la posibilidad de control de cámaras y la calidad de movimientos humanos. Además, se recopila un conjunto de prueba adecuado con movimientos de cámaras y acciones humanas difíciles para evaluar el efecto de nuestro método.",
      "upvotes": 2,
      "discussionId": "68075a2edadb28dddc13e0ac",
      "projectPage": "https://ewrfcas.github.io/Uni3C/",
      "ai_keywords": [
        "plug-and-play control module",
        "PCDController",
        "unprojected point clouds",
        "monocular depth",
        "3D priors",
        "video foundational models",
        "jointly aligned 3D world guidance",
        "scenery point clouds",
        "SMPL-X characters",
        "camera controllability",
        "human motion quality"
      ]
    },
    "publishedAt": "2025-04-21T03:10:41.000Z",
    "title": "Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls\n  for Video Generation",
    "summary": "Camera and human motion controls have been extensively studied for video\ngeneration, but existing approaches typically address them separately,\nsuffering from limited data with high-quality annotations for both aspects. To\novercome this, we present Uni3C, a unified 3D-enhanced framework for precise\ncontrol of both camera and human motion in video generation. Uni3C includes two\nkey contributions. First, we propose a plug-and-play control module trained\nwith a frozen video generative backbone, PCDController, which utilizes\nunprojected point clouds from monocular depth to achieve accurate camera\ncontrol. By leveraging the strong 3D priors of point clouds and the powerful\ncapacities of video foundational models, PCDController shows impressive\ngeneralization, performing well regardless of whether the inference backbone is\nfrozen or fine-tuned. This flexibility enables different modules of Uni3C to be\ntrained in specific domains, i.e., either camera control or human motion\ncontrol, reducing the dependency on jointly annotated data. Second, we propose\na jointly aligned 3D world guidance for the inference phase that seamlessly\nintegrates both scenic point clouds and SMPL-X characters to unify the control\nsignals for camera and human motion, respectively. Extensive experiments\nconfirm that PCDController enjoys strong robustness in driving camera motion\nfor fine-tuned backbones of video generation. Uni3C substantially outperforms\ncompetitors in both camera controllability and human motion quality.\nAdditionally, we collect tailored validation sets featuring challenging camera\nmovements and human actions to validate the effectiveness of our method.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64892bc82eafb7b91182bec5/ldOrEdwnbNQcTKZBcT4Oq.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/64892bc82eafb7b91182bec5/g5LyXAgsA-XhlQDd-q-_S.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14899.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64892bc82eafb7b91182bec5",
      "avatarUrl": "/avatars/c9eed91e60f3ea227b35e111d3fc4200.svg",
      "fullname": "chenjie cao",
      "name": "ewrfcas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13941",
      "authors": [
        {
          "_id": "6806f6ff67a715240a5ab9f8",
          "name": "Syeda Nahida Akter",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9f9",
          "user": {
            "_id": "66980b9c9baa4382e1678809",
            "avatarUrl": "/avatars/1a516bb7aa7871834c19de708cdd853a.svg",
            "isPro": false,
            "fullname": "Shrimai Prabhumoye",
            "user": "shrimai19",
            "type": "user"
          },
          "name": "Shrimai Prabhumoye",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-22T01:55:12.561Z",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fa",
          "name": "Matvei Novikov",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fb",
          "name": "Seungju Han",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fc",
          "name": "Ying Lin",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fd",
          "name": "Evelina Bakhturi",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fe",
          "name": "Eric Nyberg",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9ff",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5aba00",
          "name": "Mostofa Patwary",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5aba01",
          "name": "Mohammad Shoeybi",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5aba02",
          "name": "Bryan Catanzaro",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T21:37:13.000Z",
      "submittedOnDailyAt": "2025-04-22T00:28:47.499Z",
      "title": "NEMOTRON-CROSSTHINK: Escalado de aprendizaje automático en un rango más amplio que la matemática",
      "submittedOnDailyBy": {
        "_id": "6338dd1776421c0543150467",
        "avatarUrl": "/avatars/4539dcec644e40be33f4a0d419fa66cb.svg",
        "isPro": false,
        "fullname": "Syeda Nahida Akter",
        "user": "SieraL",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) muestran una fuerte capacidad lógica especialmente cuando se fortalecen mediante aprendizaje por refuerzo (RL). En estudios anteriores, los modelos lograron aplicar con éxito reglas y precisión en matemáticas, pero la generalización a una amplia gama de áreas lógicas fue difícil debido a limitaciones de datos, estructuras de recompensa imprevisibles y las exigencias de varias tareas. En este artículo, proponemos un marco llamado 'NEMOTRON-CROSSTHINK' para mejorar la extensibilidad de diferentes tareas lógicas mediante la integración sistemática de corpus de diversas áreas en la entrenamiento con RL. NEMOTRON-CROSSTHINK incluye: (1) el uso de datos de diversas áreas como ciencias exactas, humanidades y ciencias sociales; (2) la aplicación de templates estructurados (por ejemplo, multiple choice y abierta) para controlar la complejidad del espacio de respuestas; (3) filtrado de respuestas que incluyan demostraciones; (4) la optimización de estrategias de bridging de datos para efectivamente utilizar datos de múltiples áreas. Nuestro enfoque permite modelar la compensación con demostraciones, lo que mejora la precisión tanto en matemáticas (MATH-500: +30.1%, AMC23: +27.5%) como en marcos de prueba lógicos no matemáticos (MMLU-PRO: +12.8%, GPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Además, NEMOTRON-CROSSTHINK utiliza más de 28% de tokens para mostrar respuestas correctas, lo que significa un gran aumento en la eficiencia de las respuestas y una mayor concentración y efectividad en la lógica. A través de NEMOTRON-CROSSTHINK, demostramos que la integración de datos de diversas áreas y formatos en RL puede llevar a la realización de modelos de lenguaje grande más precisos, eficientes y generalizables.",
      "upvotes": 2,
      "discussionId": "6806f70067a715240a5aba4c",
      "ai_keywords": [
        "Reinforcement Learning",
        "NEMOTRON-CROSSTHINK",
        "multi-domain corpora",
        "structured templates",
        "answer-space complexity",
        "verifiable answers",
        "data blending strategies",
        "reward modeling",
        "MATH-500",
        "AMC23",
        "MMLU-PRO",
        "GPQA-DIAMOND",
        "AGIEVAL",
        "SUPERGPQA",
        "response efficiency",
        "tokens"
      ]
    },
    "publishedAt": "2025-04-15T17:37:13.000Z",
    "title": "NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning",
    "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities,\nparticularly when enhanced through Reinforcement Learning (RL). While prior\nwork has successfully applied RL to mathematical reasoning -- where rules and\ncorrectness are well-defined -- generalizing these methods to broader reasoning\ndomains remains challenging due to limited data, the lack of verifiable reward\nstructures, and diverse task requirements. In this work, we propose\nNEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain\ncorpora, including both synthetic and real-world question-answer pairs, into RL\ntraining to improve generalization across diverse reasoning tasks.\nNEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from\nvaried sources spanning STEM, humanities, social sciences, etc.; (2) applying\nstructured templates (e.g., multiple-choice and open-ended) to control\nanswer-space complexity; (3) filtering for verifiable answers; and (4)\noptimizing data blending strategies that utilizes data from multiple sources\neffectively. Our approach enables scalable and verifiable reward modeling\nbeyond mathematics and demonstrates improved accuracies on both math (MATH-500:\n+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,\nGPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,\nNEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --\nusing 28% fewer tokens for correct answers -- highlighting more focused and\neffective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that\nintegrating multi-domain, multi-format data in RL leads to more accurate,\nefficient, and generalizable LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13941.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6338dd1776421c0543150467",
      "avatarUrl": "/avatars/4539dcec644e40be33f4a0d419fa66cb.svg",
      "fullname": "Syeda Nahida Akter",
      "name": "SieraL",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.12186",
      "authors": [
        {
          "_id": "680764930b2f7fda706f6c03",
          "name": "Alejandro Newell",
          "hidden": false
        },
        {
          "_id": "680764930b2f7fda706f6c04",
          "name": "Peiyun Hu",
          "hidden": false
        },
        {
          "_id": "680764930b2f7fda706f6c05",
          "name": "Lahav Lipson",
          "hidden": false
        },
        {
          "_id": "680764930b2f7fda706f6c06",
          "name": "Stephan R. Richter",
          "hidden": false
        },
        {
          "_id": "680764930b2f7fda706f6c07",
          "name": "Vladlen Koltun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T15:40:15.000Z",
      "submittedOnDailyAt": "2025-04-22T08:13:12.160Z",
      "title": "CoMotion: Movimientos 3D Continuos Multifuncionales",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "Presentamos un método para detectar y seguir las posiciones 3D detalladas de múltiples personas en un streaming con una cámara única. Nuestro sistema mantiene la coincidencia temporal en escenarios complejos y ocultos. Nuestro modelo realiza una detección fuerte y actualizaciones de posición aprendidas en cada frame, y sigue a las personas de frame a frame. En lugar de alinear temporalmente la detección, actualizamos directamente la posición de la pose en nuevas imágenes de entrada y permitimos la seguimiento en línea a pesar de la ocultación. Usando conjuntos de datos de imagenes y videos, expandimos las etiquetas de etiquetado topológico para alcanzar la precisión de los sistemas de estimación de pose 3D de primera clase, y generamos modelos más rápidos y precisos al seguir a múltiples personas a lo largo del tiempo. El código y los pesos están disponibles en https://github.com/apple/ml-comotion.",
      "upvotes": 0,
      "discussionId": "680764950b2f7fda706f6ca7",
      "ai_keywords": [
        "3D poses",
        "monocular camera",
        "temporally coherent predictions",
        "occlusions",
        "pose update",
        "online tracking",
        "pseudo-labeled annotations",
        "state-of-the-art systems",
        "3D pose estimation accuracy"
      ]
    },
    "publishedAt": "2025-04-16T11:40:15.000Z",
    "title": "CoMotion: Concurrent Multi-person 3D Motion",
    "summary": "We introduce an approach for detecting and tracking detailed 3D poses of\nmultiple people from a single monocular camera stream. Our system maintains\ntemporally coherent predictions in crowded scenes filled with difficult poses\nand occlusions. Our model performs both strong per-frame detection and a\nlearned pose update to track people from frame to frame. Rather than match\ndetections across time, poses are updated directly from a new input image,\nwhich enables online tracking through occlusion. We train on numerous image and\nvideo datasets leveraging pseudo-labeled annotations to produce a model that\nmatches state-of-the-art systems in 3D pose estimation accuracy while being\nfaster and more accurate in tracking multiple people through time. Code and\nweights are provided at https://github.com/apple/ml-comotion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12186.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 829
    },
    "isAuthorParticipating": false
  }
]