[
  {
    "paper": {
      "id": "2501.18492",
      "authors": [
        {
          "_id": "679c4ac5e2c0dbf282597d35",
          "user": {
            "_id": "64b708351a4d97b5d7edd369",
            "avatarUrl": "/avatars/960c1033f9cf218220f86de22c06915b.svg",
            "isPro": false,
            "fullname": "Yue Liu",
            "user": "yueliu1998",
            "type": "user"
          },
          "name": "Yue Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:41:25.697Z",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d36",
          "user": {
            "_id": "62728f4f6253fe2068da1021",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
            "isPro": false,
            "fullname": "Hongcheng Gao",
            "user": "HongchengGao",
            "type": "user"
          },
          "name": "Hongcheng Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-31T08:35:51.645Z",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d37",
          "user": {
            "_id": "6366429195204b4649c658b8",
            "avatarUrl": "/avatars/5d80e9ebe0b57fd815f36796b9187248.svg",
            "isPro": false,
            "fullname": "Shengfang Zhai",
            "user": "zsf",
            "type": "user"
          },
          "name": "Shengfang Zhai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:41:32.474Z",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d38",
          "user": {
            "_id": "679c68bbfc30f43de85206f5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/IJWda9ZYtjzlhr2ehsLHu.jpeg",
            "isPro": false,
            "fullname": "Jun Xia",
            "user": "JunXia97",
            "type": "user"
          },
          "name": "Jun Xia",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:41:53.366Z",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d39",
          "name": "Tianyi Wu",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d3a",
          "user": {
            "_id": "63f42ca3520c1461892ee929",
            "avatarUrl": "/avatars/095241acfe7c783d2406abf63ff81f65.svg",
            "isPro": false,
            "fullname": "xuezhiwei",
            "user": "lakxtxue",
            "type": "user"
          },
          "name": "Zhiwei Xue",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:42:30.842Z",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d3b",
          "user": {
            "_id": "65efc25828426de60f977dfc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8ZcIoo58JPLdnjm-jZeo.png",
            "isPro": false,
            "fullname": "Yulin Chen",
            "user": "CallMeChen",
            "type": "user"
          },
          "name": "Yulin Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:42:41.013Z",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d3c",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d3d",
          "user": {
            "_id": "669e19e5dac1eb34c0f5f505",
            "avatarUrl": "/avatars/bec7d1d1dac2ad6570844d1f00e7df0a.svg",
            "isPro": false,
            "fullname": "Jiaheng Zhang",
            "user": "jiaheng233",
            "type": "user"
          },
          "name": "Jiaheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:37:04.493Z",
          "hidden": false
        },
        {
          "_id": "679c4ac5e2c0dbf282597d3e",
          "user": {
            "_id": "651d8032c50012d33e914f2f",
            "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
            "isPro": false,
            "fullname": "Bryan Hooi",
            "user": "bhooi",
            "type": "user"
          },
          "name": "Bryan Hooi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:42:50.273Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-30T17:06:06.000Z",
      "title": "Gard Ridgerunner: Objetivo de la guía de seguridad de LLM basada en razones",
      "summary": "La influencia de los LLMs en la seguridad de aplicaciones se vuelve cada vez más importante, y la garantía de seguridad con el uso de GuardLine es un problema crucial. En este artículo, se propone un nuevo dispositivo de seguridad llamado GuardReasoner, que permite que GuardModel se entrene lógicamente. Concretamente, se genera un dataset llamado \"GuardReasonerTrain\" que consiste en 127K muestras y 460K razones lógicas detalladas, y se introduce una SFT (Supervised Fine-Tuning) para habilidades de explicación. Además, se presenta una Hard Sample DPO para mejorar las muestras difíciles. De esta manera, GuardReasoner mejora su rendimiento, explicatividad y capacidad de generalización. A través de experimentos y análisis extendidos en 13 marcos de referencia para 3 tareas de GuardLine, se demuestra una excelente performance. En particular, GuardReasoner 8B supera, en términos de puntuación F1, a GPT-4o+CoT en un promedio de 5.74% y a LLaMA Guard 3 8B en un promedio de 20.84%. Se publican los datos de entrenamiento, código y modelos (de tamaños 1B, 3B y 8B) de GuardReasoner en: https://github.com/yueliu1999/GuardReasoner/.",
      "upvotes": 29,
      "discussionId": "679c4ac6e2c0dbf282597d80"
    },
    "publishedAt": "2025-01-30T23:01:47.466Z",
    "title": "GuardReasoner: Towards Reasoning-based LLM Safeguards",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6650c77a74664a42ddfb9187/Kza1q-PVKsgu_6SaQ9Oze.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6650c77a74664a42ddfb9187/rqViZgnFQQJcAfgC1a17n.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6650c77a74664a42ddfb9187/5Dk0HJkhOCoSXoWdVUzBo.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6650c77a74664a42ddfb9187/DWg1wTHDx939H4bZPVj1W.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18492.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6650c77a74664a42ddfb9187",
      "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
      "fullname": "yueliu1999",
      "name": "yueliu1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18362",
      "authors": [
        {
          "_id": "679c5b0034f5df4416915177",
          "name": "Yuxin Zuo",
          "hidden": false
        },
        {
          "_id": "679c5b0034f5df4416915178",
          "user": {
            "_id": "65597738deee83130a1301d5",
            "avatarUrl": "/avatars/9bcc40aebe4db079927675d95c00463c.svg",
            "isPro": false,
            "fullname": "Shang (Lindsay) Qu",
            "user": "lindsay-qu",
            "type": "user"
          },
          "name": "Shang Qu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-31T08:35:48.269Z",
          "hidden": false
        },
        {
          "_id": "679c5b0034f5df4416915179",
          "name": "Yifei Li",
          "hidden": false
        },
        {
          "_id": "679c5b0034f5df441691517a",
          "name": "Zhangren Chen",
          "hidden": false
        },
        {
          "_id": "679c5b0034f5df441691517b",
          "name": "Xuekai Zhu",
          "hidden": false
        },
        {
          "_id": "679c5b0034f5df441691517c",
          "name": "Ermo Hua",
          "hidden": false
        },
        {
          "_id": "679c5b0034f5df441691517d",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "679c5b0034f5df441691517e",
          "user": {
            "_id": "60cf4bcb1ce3775ebb86e5d5",
            "avatarUrl": "/avatars/12bcd18d215abf91f297f93007733148.svg",
            "isPro": false,
            "fullname": "Ning Ding",
            "user": "stingning",
            "type": "user"
          },
          "name": "Ning Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-31T09:50:45.999Z",
          "hidden": false
        },
        {
          "_id": "679c5b0034f5df441691517f",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-30T14:07:56.000Z",
      "title": "MedXpertQA: Nivel de razonamiento y criterios de comprensión profesional médico",
      "summary": "MedXpertQA es un modelo que posee un alto rendimiento en evaluaciones de referencia, lo que lo hace ideal para evaluar conocimientos médicos profesionales y una lógica avanzada. MedXpertQA incluye 4,460 preguntas distribuidas entre 17 áreas profesionales y 11 sistemas de examen físico, divididas en subconjuntos de evaluación de texto y el MM subconjunto para evaluaciones multimodales. En particular, MM introduce preguntas de alto nivel profesional que incluyen imágenes y una gran cantidad de información clínica, lo que permite una evaluación más rigurosa que las tradicionales marcas de referencia médicas, que solo evalúan pares de respuestas simples generados a partir de capturas de imágenes. Para abordar la falta de dificultad en los actuales marcos de referencia, MedXpertQA aplica una filtración estricta y una extensión, lo que mejora la calidad y la pertinencia de las preguntas en las áreas profesionales, asegurando así una evaluación más precisa y confiable. Además, se realizan evaluaciones profesionales múltiples para garantizar la precisión y la confianza en el rendimiento del modelo. En MedXpertQA se evalúan 16 modelos avanzados. Además, la medicina tiene una profunda relación con decisiones reales, y ofrece entornos representativos que evalúan habilidades de lógica más allá de matemáticas y programación. Por lo tanto, se desarrollan subconjuntos lógicos para evaluar mejor el potencial lógico de modelos como o1.",
      "upvotes": 7,
      "discussionId": "679c5b0234f5df44169151e9"
    },
    "publishedAt": "2025-01-31T04:14:53.856Z",
    "title": "MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18362.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65597738deee83130a1301d5",
      "avatarUrl": "/avatars/9bcc40aebe4db079927675d95c00463c.svg",
      "fullname": "Shang (Lindsay) Qu",
      "name": "lindsay-qu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.18585",
      "authors": [
        {
          "_id": "679c5ca666c379e215bc9e74",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e75",
          "user": {
            "_id": "63e60ff62d704152abac8af8",
            "avatarUrl": "/avatars/a54c34fb87a7ed5aeba792852747de92.svg",
            "isPro": false,
            "fullname": "Qiuzhi Liu",
            "user": "Dennis364",
            "type": "user"
          },
          "name": "Qiuzhi Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:45:37.562Z",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e76",
          "user": {
            "_id": "660399710f1fc2f16de18072",
            "avatarUrl": "/avatars/c22a749cc45db693c2d9ea877c7cace4.svg",
            "isPro": false,
            "fullname": "Jiahao Xu",
            "user": "Jiahao004",
            "type": "user"
          },
          "name": "Jiahao Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:45:31.807Z",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e77",
          "name": "Tian Liang",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e78",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e79",
          "user": {
            "_id": "638439ca834d3558a398d035",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669609868550-noauth.png",
            "isPro": false,
            "fullname": "Zhiwei He",
            "user": "zwhe99",
            "type": "user"
          },
          "name": "Zhiwei He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:44:45.300Z",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e7a",
          "user": {
            "_id": "64c94eddcb2f1bf0e7db5a4d",
            "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
            "isPro": false,
            "fullname": "Linfeng Song",
            "user": "freesunshine0316",
            "type": "user"
          },
          "name": "Linfeng Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:44:29.221Z",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e7b",
          "user": {
            "_id": "62d58fd53bf5e059f7cc3245",
            "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
            "isPro": false,
            "fullname": "Dian Yu",
            "user": "yudian",
            "type": "user"
          },
          "name": "Dian Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:44:23.114Z",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e7c",
          "user": {
            "_id": "6670e285b0c03c4e9d6e0985",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uCZHm4gKSHZ2b0hpHWgZv.jpeg",
            "isPro": false,
            "fullname": "Juntao Li",
            "user": "douvleplus",
            "type": "user"
          },
          "name": "Juntao Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:44:12.069Z",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e7d",
          "user": {
            "_id": "5f82f9f7f0801648bf8844b2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669627733134-5f82f9f7f0801648bf8844b2.jpeg",
            "isPro": false,
            "fullname": "Zhuosheng Zhang",
            "user": "cooelf",
            "type": "user"
          },
          "name": "Zhuosheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:44:05.749Z",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e7e",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e7f",
          "user": {
            "_id": "67485743561b1e6f9579389f",
            "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
            "isPro": false,
            "fullname": "Zhaopeng Tu",
            "user": "zptu",
            "type": "user"
          },
          "name": "Zhaopeng Tu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:43:27.683Z",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e80",
          "user": {
            "_id": "65147a1426fbd558dbd08f1b",
            "avatarUrl": "/avatars/86574ee2d5c22e940be1c4e50be88675.svg",
            "isPro": false,
            "fullname": "Haitao Mi",
            "user": "haitaominlp",
            "type": "user"
          },
          "name": "Haitao Mi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:43:21.871Z",
          "hidden": false
        },
        {
          "_id": "679c5ca666c379e215bc9e81",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-30T18:58:18.000Z",
      "title": "Los pensamientos en cada lugar se dispersan, y una reflexión sobre la falta de pensamiento en los modelos de tipo o1-Like LLMs.",
      "summary": "Por ejemplo, el modelo de lenguaje grande (LLMs) como el o1 de OpenAI escala la cantidad de cálculos en las pruebas para mostrar pensamientos profundos como los de un ser humano y demostrar excelentes habilidades en tareas de teoría de la razón. Sin embargo, hemos encontrado que modelos como el o1 no realizan suficiente exploración para alcanzar soluciones deseadas, lo que llamamos \"underthinking\" y reconocemos este fenómeno. Esta conducta lleva a una falta de profundidad en la teoría de la razón y a una pérdida de rendimiento. Específicamente, este problema es más pronunciado cuando se enfrenta a problemas matemáticos difíciles.\n\nPara analizar este problema de manera sistemática, hemos realizado experimentos con tres conjuntos de pruebas difíciles y con dos de los principales modelos de código abierto similares al o1, revelando así la asociación entre la frecuente cambio de pensamiento y las respuestas negativas. Hemos medido la eficiencia de los tokens de las respuestas negativas y introducido un nuevo métrico para cuantificar \"underthinking\". Propusimos una estrategia de decodificación con una penalización por cambio de pensamiento, denominada \"TIP\", con el objetivo de inhibir el cambio de pensamiento excesivo y promover una profunda exploración en cada paso de la teoría de la razón.\n\nLos resultados de los experimentos muestran que nuestro enfoque mejora la precisión del modelo en conjuntos de datos difíciles, evitando necesariamente ajustes del modelo. Lo que hemos encontrado contribuye a entender la adecuación de la teoría de la razón en modelos como el o1 y proporciona soluciones prácticas para mejorar su capacidad para resolver problemas.",
      "upvotes": 6,
      "discussionId": "679c5ca766c379e215bc9eb1"
    },
    "publishedAt": "2025-01-31T00:16:36.453Z",
    "title": "Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18585.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5875
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16411",
      "authors": [
        {
          "_id": "679c4f344061a1ab60ebe6fa",
          "user": {
            "_id": "644b71ddb2e7823a76abcf91",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
            "isPro": false,
            "fullname": "zhou wei",
            "user": "WeiChow",
            "type": "user"
          },
          "name": "Wei Chow",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-31T08:35:49.674Z",
          "hidden": false
        },
        {
          "_id": "679c4f344061a1ab60ebe6fb",
          "name": "Jiageng Mao",
          "hidden": false
        },
        {
          "_id": "679c4f344061a1ab60ebe6fc",
          "user": {
            "_id": "620dd3888528f797e88cb9b5",
            "avatarUrl": "/avatars/af04728788d78fe7d6375e19e32a535e.svg",
            "isPro": false,
            "fullname": "Boyi Li",
            "user": "Boyiliee",
            "type": "user"
          },
          "name": "Boyi Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:46:09.305Z",
          "hidden": false
        },
        {
          "_id": "679c4f344061a1ab60ebe6fd",
          "name": "Daniel Seita",
          "hidden": false
        },
        {
          "_id": "679c4f344061a1ab60ebe6fe",
          "name": "Vitor Guizilini",
          "hidden": false
        },
        {
          "_id": "679c4f344061a1ab60ebe6ff",
          "name": "Yue Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T18:59:58.000Z",
      "title": "PhysBench: Marcador y Mejora para Modelos de Lenguaje Visuo-Sintáctico para Comprender el Mundo Físico",
      "summary": "En el contexto de la AI corporal, la comprensión del mundo físico es un problema fundamental, siendo crucial para la ejecución de tareas complejas en entornos reales y la manipulación segura. Los modelos de visión-lenguaje (VLMs) muestran una posibilidad excepcional en la lógica y planificación de tareas de los agentes corporales, pero su comprensión de fenómenos físicos es muy limitada. Para cerrar esta brecha, presentamos PhysBench. Este es un marco de evaluación detallado diseñado para evaluar la comprensión del mundo físico, y evalúa la comprensión física en diversas tareas. PhysBench se clasifica en cuatro áreas principales: propiedades de objetos físicos, relaciones entre objetos físicos, comprensión del espacio físico y física de la mecánica, y se desarrolla en 19 subclases y 8 diferentes niveles de habilidad. Nuestros experimentos de extensión se realizaron con 75 VLMs representativos, demostrando claramente que, aunque estos modelos superan a la lógica general, tienen dificultades en la comprensión del mundo físico, lo que se atribuye a deficiencias en los datos de entrenamiento que faltan conocimientos físicos. Para resolver estas deficiencias, presentamos PhysAgent. Este es un nuevo marco de trabajo que integra la generalización de los VLMs y el conocimiento específico de modelos visuales, mejorando significativamente la comprensión física en diversas tareas. Se observó un aumento del 18.4% en el caso de GPT-4o. Además, nuestros resultados demuestran que la comprensión del mundo físico de MOKA, como un agente corporal, puede ser mejorada. Consideramos que PhysBench y PhysAgent proporcionan una guía útil para cerrar la brecha entre los VLMs y la comprensión del mundo físico, contribuyendo significativamente a este objetivo.",
      "upvotes": 6,
      "discussionId": "679c4f394061a1ab60ebe7f0"
    },
    "publishedAt": "2025-01-30T23:19:24.751Z",
    "title": "PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644b71ddb2e7823a76abcf91",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
      "fullname": "zhou wei",
      "name": "WeiChow",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.18438",
      "authors": [
        {
          "_id": "679c7d0ebd893fb2b7159aa3",
          "user": {
            "_id": "657b3a44de028a439ea2ed9d",
            "avatarUrl": "/avatars/9f05e8eb6809a0ce1b50cd1fc9b5a044.svg",
            "isPro": false,
            "fullname": "Aitor Arrieta",
            "user": "aitorarrieta",
            "type": "user"
          },
          "name": "Aitor Arrieta",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-31T07:34:38.875Z",
          "hidden": false
        },
        {
          "_id": "679c7d0ebd893fb2b7159aa4",
          "name": "Miriam Ugarte",
          "hidden": false
        },
        {
          "_id": "679c7d0ebd893fb2b7159aa5",
          "user": {
            "_id": "65001514f322f9156663f096",
            "avatarUrl": "/avatars/e8712f60d4e8b7c70ac02c532ad547ef.svg",
            "isPro": false,
            "fullname": "Pablo Valle",
            "user": "pablovalle",
            "type": "user"
          },
          "name": "Pablo Valle",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-31T08:35:44.931Z",
          "hidden": false
        },
        {
          "_id": "679c7d0ebd893fb2b7159aa6",
          "user": {
            "_id": "63527de67e4cc3135fd16651",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63527de67e4cc3135fd16651/bkeQlJEwsPs3E4EsvmmLB.jpeg",
            "isPro": false,
            "fullname": "José Antonio Parejo Maestre",
            "user": "japarejo",
            "type": "user"
          },
          "name": "José Antonio Parejo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:49:45.440Z",
          "hidden": false
        },
        {
          "_id": "679c7d0ebd893fb2b7159aa7",
          "user": {
            "_id": "6790d642a1863df579840ae3",
            "avatarUrl": "/avatars/a10a6f4af327c1bb67513c56d7f84820.svg",
            "isPro": false,
            "fullname": "Sergio Segura",
            "user": "ssegura",
            "type": "user"
          },
          "name": "Sergio Segura",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-31T07:34:38.876Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-30T15:45:56.000Z",
      "title": "o3-mini vs DeepSeek-R1: ¿Cuál es más Seguro?",
      "summary": "El incursión de DeepSeek-R1 ha actuado como un punto de transición en todo el sector de la industria de la IA, especialmente en el campo de los LLM. Su capacidad ha demostrado excelentes resultados en tareas como pensamiento creativo, generación de código, matemáticas y modificación automática de programas, pero su costo de ejecución ha sido considerablemente bajo. Sin embargo, los LLM deben mantener importantes características cualitativas. Es decir, deben mantenerse los valores de seguridad y la coincidencia con los valores humanos. El DeepSeek-R1 tiene un oponente claro en el modelo o3-mini de OpenAI, un estadounidense. Este modelo se espera que establiga altos estándares en rendimiento, seguridad y costo. En este artículo, se evalua sistemáticamente el nivel de seguridad de DeepSeek-R1 (versión 70b) y o3-mini (versión beta de OpenAI). Para ello, se utilizó el herramienta de pruebas de seguridad automatizada recién publicada, ASTRAL. Utilizando esta herramienta, se generaron y ejecutaron automáticamente un total de 1260 entradas de prueba inseguras para ambos modelos. Tras evaluar de manera semi-automáticamente los resultados proporcionados por ambos modelos, se observó que DeepSeek-R1 muestra un nivel de inseguridad superior a o3-mini. Según nuestra evaluación, DeepSeek-R1 respondió inseguramente al 11.98% de los formularios ejecutados, mientras que o3-mini respondió inseguramente solo al 1.19%.",
      "upvotes": 4,
      "discussionId": "679c7d0ebd893fb2b7159af5"
    },
    "publishedAt": "2025-01-31T02:35:40.107Z",
    "title": "o3-mini vs DeepSeek-R1: Which One is Safer?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18438.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65001514f322f9156663f096",
      "avatarUrl": "/avatars/e8712f60d4e8b7c70ac02c532ad547ef.svg",
      "fullname": "Pablo Valle",
      "name": "pablovalle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.18511",
      "authors": [
        {
          "_id": "679c9419a01fd6df443d5729",
          "user": {
            "_id": "62f7f4efe7c1c9bf10c81465",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f7f4efe7c1c9bf10c81465/AYlOg0fkP1o4GAP-8Y3xt.jpeg",
            "isPro": true,
            "fullname": "Benjamin Feuer",
            "user": "penfever",
            "type": "user"
          },
          "name": "Benjamin Feuer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T09:35:53.653Z",
          "hidden": false
        },
        {
          "_id": "679c9419a01fd6df443d572a",
          "name": "Chinmay Hegde",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-30T17:21:44.000Z",
      "title": "WILDCHAT-50M: Revisión Profunda del Papel de los Datos Sintéticos Tras la Entrenamiento",
      "summary": "El procesamiento posterior de modelos de lenguaje (MLL) puede ser utilizado para revisar acciones desde el DPO hasta la distillación y desarrollar nuevos habilidades. Sin embargo, este tipo de procesamiento aún está en los inicios. Uno de los factores limitantes es la dificultad de comparar analíticamente a grandes escalas modelos de datos sintéticos y evaluadores de MLL. Para resolver esto, presentamos WILDCHAT-50M, el conjunto de datos de conversación más grande público hasta el momento. Extendemos el conjunto de datos actual de WildChat y incluimos respuestas de más de 50 modelos de pesos abiertos además de GPT. Realizamos un análisis comparativo a gran escala y creamos RE-WILD, nuestra mezcla de entrenamiento abierta, para mostrar la posibilidad de este conjunto de datos. Excede la mezcla de entrenamiento Tulu-3 de la Allen AI recientemente publicada, con una proporción de 40% de los muestras. El conjunto de datos, muestras y código están disponibles en https://github.com/penfever/wildchat-50m.",
      "upvotes": 2,
      "discussionId": "679c941da01fd6df443d5907"
    },
    "publishedAt": "2025-01-31T04:13:28.061Z",
    "title": "WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in Post-Training",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18511.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60107b385ac3e86b3ea4fc34",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
      "fullname": "Daniel van Strien",
      "name": "davanstrien",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isMod": false,
      "followerCount": 519
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18009",
      "authors": [
        {
          "_id": "679c5b0259e9218a222ab742",
          "user": {
            "_id": "6689f7fb8c440fe1955a51b5",
            "avatarUrl": "/avatars/9b23ee2f05f55615c6174a678436b30d.svg",
            "isPro": false,
            "fullname": "Lan Pan",
            "user": "louanna",
            "type": "user"
          },
          "name": "Lan Pan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-01-31T06:33:49.785Z",
          "hidden": false
        },
        {
          "_id": "679c5b0259e9218a222ab743",
          "user": {
            "_id": "63fd543a3c880680af459cad",
            "avatarUrl": "/avatars/2a90a4b002fe0d09e28ce0e111357748.svg",
            "isPro": false,
            "fullname": "Hanbo Xie",
            "user": "xhb120633",
            "type": "user"
          },
          "name": "Hanbo Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-31T08:51:40.573Z",
          "hidden": false
        },
        {
          "_id": "679c5b0259e9218a222ab744",
          "name": "Robert C. Wilson",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-29T21:51:17.000Z",
      "title": "El lenguaje de la mayoría de los lenguajes de programación se almacena de manera eficiente para que puedan ser explorados rápidamente.",
      "summary": "Los modelos de lenguaje general han desarrollado una gran variedad de habilidades inteligentes. Para evaluarlos se han utilizado muchos marcos de referencia, pero su capacidad de exploración ha recibido menos atención, y se han omitido investigaciones sobre su capacidad para descubrir nuevas informaciones en entornos naturales y artificiales. No está claro cuán limitados son los límites de exploración de los modelos de lenguaje general (LLM) en tareas abiertas que superan al ser humano. Este estudio investiga si los LLM pueden explorar más allá del ser humano en tareas abiertas, utilizando Little Alchemy 2 como paradigma. Los resultados muestran que la mayoría de los LLM son menos eficientes que los humanos, con el modelo o1 como excepción. Los modelos tradicionales de LLM principalmente utilizan estrategias basadas en incertidumbre, mientras que los humanos equilibran incertidumbre y motivación. La análisis de las representaciones de modelos que utilizan autoencoders esparsos muestra que la incertidumbre y la elección se manifiestan en los bloques transformers iniciales, y el valor de la motivación se procesa en la mitad posterior. Los LLM hacen excesivas decisiones y impiden una exploración válida. Estos hallazgos indican que los límites de exploración de los LLM son claros y proponen direcciones para mejorar su adaptabilidad.",
      "upvotes": 2,
      "discussionId": "679c5b0359e9218a222ab76f"
    },
    "publishedAt": "2025-01-31T00:09:40.077Z",
    "title": "Large Language Models Think Too Fast To Explore Effectively",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5875
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.18512",
      "authors": [
        {
          "_id": "679ca01ecad2402cec0a939a",
          "name": "Arthur Douillard",
          "hidden": false
        },
        {
          "_id": "679ca01ecad2402cec0a939b",
          "name": "Yanislav Donchev",
          "hidden": false
        },
        {
          "_id": "679ca01ecad2402cec0a939c",
          "name": "Keith Rush",
          "hidden": false
        },
        {
          "_id": "679ca01ecad2402cec0a939d",
          "name": "Satyen Kale",
          "hidden": false
        },
        {
          "_id": "679ca01ecad2402cec0a939e",
          "name": "Zachary Charles",
          "hidden": false
        },
        {
          "_id": "679ca01ecad2402cec0a939f",
          "name": "Zachary Garrett",
          "hidden": false
        },
        {
          "_id": "679ca01ecad2402cec0a93a0",
          "name": "Gabriel Teston",
          "hidden": false
        },
        {
          "_id": "679ca01ecad2402cec0a93a1",
          "name": "Dave Lacey",
          "hidden": false
        },
        {
          "_id": "679ca01ecad2402cec0a93a2",
          "name": "Ross McIlroy",
          "hidden": false
        },
        {
          "_id": "679ca01ecad2402cec0a93a3",
          "name": "Jiajun Shen",
          "hidden": false
        },
        {
          "_id": "679ca01ecad2402cec0a93a4",
          "name": "Alexandre Ramé",
          "hidden": false
        },
        {
          "_id": "679ca01ecad2402cec0a93a5",
          "name": "Arthur Szlam",
          "hidden": false
        },
        {
          "_id": "679ca01ecad2402cec0a93a6",
          "name": "Marc'Aurelio Ranzato",
          "hidden": false
        },
        {
          "_id": "679ca01ecad2402cec0a93a7",
          "name": "Paul Barham",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-30T17:23:50.000Z",
      "title": "Streaming Droco y la comunicación: buscando el relajo en la libertad de sistemas distribuidos",
      "summary": "El entrenamiento de modelos de lenguaje grande (LLMs) generalmente se realiza con la ayuda de varios aceleradores para reducir el tiempo de entrenamiento. Debido a que los estados internos y los valores de gradiente de los parámetros se intercambian en cada paso de gradiente, todos los dispositivos deben usar enlaces de comunicación de alta velocidad, como banda ancha, para trabajar de forma coordinada. Recientemente, algoritmos de distribución como DiLoCo han mitigado estas restricciones, permitiendo agrupar los aceleradores como 'workers' y reduciendo la frecuencia de la sincronización entre los workers. Esto ha permitido que los workers utilicen enlaces de comunicación de baja velocidad, pero es necesario asegurarse de que esto no afecte la calidad del entrenamiento. Sin embargo, estos métodos siguen necesitando sincronizar todos los parámetros entre todos los workers, lo que mantiene la demanda de banda ancha alta. En este artículo, se mejoran las tres formas de DiLoCo. Primero, se permite sincronizar parcialmente los parámetros sin sincronizar todos, sincronizando en orden subconjuntos de parámetros para reducir significativamente la demanda de banda ancha. Segundo, se permite que los workers continúen sincronizando mientras se entrenan para reducir el tiempo de sincronización. Tercero, se digitalizan los datos intercambiados entre los workers para reducir aún más la demanda de banda ancha. Combinando estas mejoras adecuadamente, se demuestra experimentalmente que se puede entrenar un modelo con 100 millones de parámetros de manera distribuida y alcanzar la calidad del entrenamiento anterior, reduciendo la demanda de banda ancha en dos etapas.",
      "upvotes": 1,
      "discussionId": "679ca01fcad2402cec0a9404"
    },
    "publishedAt": "2025-01-31T05:07:14.120Z",
    "title": "Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.18512.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622792366303bf1dc304f49f",
      "avatarUrl": "/avatars/975c1cc3eb2f97cf8e848162056d5bea.svg",
      "fullname": "Arthur Douillard",
      "name": "ArthurDouillard",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]