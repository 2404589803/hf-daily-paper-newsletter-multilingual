[
  {
    "paper": {
      "id": "2502.18411",
      "authors": [
        {
          "_id": "67be834ae7b05f9e43b172b2",
          "user": {
            "_id": "6530e62f536dbca918e71c3e",
            "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
            "isPro": false,
            "fullname": "Xiangyu Z",
            "user": "PhoenixZ",
            "type": "user"
          },
          "name": "Xiangyu Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-26T08:26:02.247Z",
          "hidden": false
        },
        {
          "_id": "67be834ae7b05f9e43b172b3",
          "user": {
            "_id": "646cd947da8e99940b6e55cf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
            "isPro": false,
            "fullname": "Shengyuan Ding",
            "user": "ChrisDing1105",
            "type": "user"
          },
          "name": "Shengyuan Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-26T08:25:59.887Z",
          "hidden": false
        },
        {
          "_id": "67be834ae7b05f9e43b172b4",
          "user": {
            "_id": "675aa937ab6aa7ecd09341ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/d_CNUsNOw92pg7MVhf9Vm.png",
            "isPro": false,
            "fullname": "Zicheng Zhang",
            "user": "UniverseCA",
            "type": "user"
          },
          "name": "Zicheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:49:10.028Z",
          "hidden": false
        },
        {
          "_id": "67be834ae7b05f9e43b172b5",
          "name": "Haian Huang",
          "hidden": false
        },
        {
          "_id": "67be834ae7b05f9e43b172b6",
          "name": "Maosong Cao",
          "hidden": false
        },
        {
          "_id": "67be834ae7b05f9e43b172b7",
          "user": {
            "_id": "619507e7b74b6c591f794340",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
            "isPro": false,
            "fullname": "Weiyun Wang",
            "user": "Weiyun1025",
            "type": "user"
          },
          "name": "Weiyun Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:48:45.520Z",
          "hidden": false
        },
        {
          "_id": "67be834ae7b05f9e43b172b8",
          "user": {
            "_id": "64638c4d51fa6e63060521b5",
            "avatarUrl": "/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg",
            "isPro": false,
            "fullname": "JIaqi",
            "user": "Jiaqiwang",
            "type": "user"
          },
          "name": "Jiaqi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:48:38.876Z",
          "hidden": false
        },
        {
          "_id": "67be834ae7b05f9e43b172b9",
          "user": {
            "_id": "64f5f8dd9b17cd59c453c57f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
            "isPro": false,
            "fullname": "Xinyu Fang",
            "user": "nebulae09",
            "type": "user"
          },
          "name": "Xinyu Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-26T08:26:04.433Z",
          "hidden": false
        },
        {
          "_id": "67be834ae7b05f9e43b172ba",
          "user": {
            "_id": "64d1c560c0c627dfa71bdbe0",
            "avatarUrl": "/avatars/f42794fe25bffcd870a1bcee69b95298.svg",
            "isPro": false,
            "fullname": "wenhai.wang",
            "user": "wangwhcore",
            "type": "user"
          },
          "name": "Wenhai Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:48:28.151Z",
          "hidden": false
        },
        {
          "_id": "67be834ae7b05f9e43b172bb",
          "name": "Guangtao Zhai",
          "hidden": false
        },
        {
          "_id": "67be834ae7b05f9e43b172bc",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:48:20.155Z",
          "hidden": false
        },
        {
          "_id": "67be834ae7b05f9e43b172bd",
          "name": "Hua Yang",
          "hidden": false
        },
        {
          "_id": "67be834ae7b05f9e43b172be",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T18:05:14.000Z",
      "title": "OmniAlign-V: Mejora de la correspondencia entre las preferencias humanas y los MLLM.",
      "summary": "El desarrollo reciente de los modelos de lenguaje multimodal (MLLM) abierto fuente ha enfocado principalmente en mejorar sus capacidades básicas, pero presenta notables deficiencias en lo que respecta a las preferencias humanas. En este artículo, se presenta OmniAlign-V, un conjunto de datos unificado caracterizado por 200K muestras de alta calidad, que incluye diferentes imágenes, preguntas complejas y formatos de respuesta variados, con el objetivo de mejorar la alineación de MLLM con las preferencias humanas. Además, se introduce MM-AlignBench, un benchmark diseñado específicamente para evaluar MLLM en términos de valoración humana. Los resultados de los experimentos muestran que el fine-tuning supervisado (SFT) o la optimización directa de preferencias (DPO) utilizando OmniAlign-V pueden mantener o mejorar el rendimiento en los estándares de benchmark VQA, mientras mejoran significativamente la alineación con las preferencias humanas y mantienen las capacidades básicas. Nuestro conjunto de datos, benchmark, código y checkpoints están disponibles en https://github.com/PhoenixZ810/OmniAlign-V.",
      "upvotes": 46,
      "discussionId": "67be834ce7b05f9e43b1730a"
    },
    "publishedAt": "2025-02-25T22:01:56.532Z",
    "title": "OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6530e62f536dbca918e71c3e",
      "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
      "fullname": "Xiangyu Z",
      "name": "PhoenixZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.18137",
      "authors": [
        {
          "_id": "67be8443ed8e258c0f70063a",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-26T08:25:57.704Z",
          "hidden": false
        },
        {
          "_id": "67be8443ed8e258c0f70063b",
          "user": {
            "_id": "6329bdbbde087eac2921e6a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1663679904323-noauth.jpeg",
            "isPro": false,
            "fullname": "Xiangchendong",
            "user": "Xiang-cd",
            "type": "user"
          },
          "name": "Chendong Xiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:49:29.341Z",
          "hidden": false
        },
        {
          "_id": "67be8443ed8e258c0f70063c",
          "name": "Haofeng Huang",
          "hidden": false
        },
        {
          "_id": "67be8443ed8e258c0f70063d",
          "name": "Jia Wei",
          "hidden": false
        },
        {
          "_id": "67be8443ed8e258c0f70063e",
          "user": {
            "_id": "65d5a000ec7e31555e4db57e",
            "avatarUrl": "/avatars/aab8319fbaffdd53faff59a40ca5a5ea.svg",
            "isPro": false,
            "fullname": "Haocheng Xi",
            "user": "hxi0408",
            "type": "user"
          },
          "name": "Haocheng Xi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:49:45.446Z",
          "hidden": false
        },
        {
          "_id": "67be8443ed8e258c0f70063f",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "67be8443ed8e258c0f700640",
          "user": {
            "_id": "65fcad0ba0d7adc40b54fac2",
            "avatarUrl": "/avatars/7564b5642378fddb46ec3b5ae57c0402.svg",
            "isPro": false,
            "fullname": "Jianfei Chen",
            "user": "surfingtomchen",
            "type": "user"
          },
          "name": "Jianfei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:49:52.550Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T12:02:17.000Z",
      "title": "SpargeAttn: Attención Esparsa Exacta. Este es un método de acceso que se utiliza para acelerar la inferencia del modelo.",
      "summary": "La importancia de la implementación eficiente de acciones en modelos de gran escala se debe a su complejidad temporal. A su alegría, las acciones generalmente muestran una gran espesitud, es decir, la mayoría de los valores en la matriz de acciones son aproximadamente iguales, lo que permite reducir significativamente los cálculos relativos. Muchos estudios están intentando acelerar las acciones utilizando patrones espesitos, pero la mayoría de estos estudios se centran en optimizar las acciones dentro de un modelo específico, utilizando patrones espesitos específicos en la matriz de acciones. Hasta el momento, no se han encontrado acciones espesitas generales que mejoren la velocidad de diversos modelos y garanticen un rendimiento constante. En este artículo, proponemos SpargeAttn, una acción espesita y cuantificada general que puede ser utilizada en todos los modelos. Nuestro método utiliza dos filtros en línea: en el primer paso, se predecie rápidamente y precisamente la matriz de acciones, permitiendo saltar parte de la multiplicación de matrices. En el segundo paso, se diseña un filtro para la softmax en línea que permita saltar más multiplicaciones de matrices sin añadir sobrecarga adicional. Las experimentaciones demuestran que nuestro método puede acelerar modelos de generación de lenguaje, imágenes y vídeos, manteniendo los criterios de rendimiento sin perdida. El código está disponible en https://github.com/thu-ml/SpargeAttn.",
      "upvotes": 33,
      "discussionId": "67be8447ed8e258c0f70075f"
    },
    "publishedAt": "2025-02-25T22:04:57.351Z",
    "title": "SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18137.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17363",
      "authors": [
        {
          "_id": "67bd6d2bbf6d46017e619f31",
          "user": {
            "_id": "66078994c50f8393c56ed837",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/aYYde45zaFACRllyEhJyU.jpeg",
            "isPro": true,
            "fullname": "Tianrui Zhu",
            "user": "xilluill",
            "type": "user"
          },
          "name": "Tianrui Zhu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-25T07:24:35.845Z",
          "hidden": false
        },
        {
          "_id": "67bd6d2bbf6d46017e619f32",
          "user": {
            "_id": "6315d306a9456afe2b9bf34a",
            "avatarUrl": "/avatars/7285b4e7d84b528d1a50f8ee4eb10727.svg",
            "isPro": false,
            "fullname": "ElevenZ",
            "user": "shiyi0408",
            "type": "user"
          },
          "name": "Shiyi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-26T08:30:48.262Z",
          "hidden": false
        },
        {
          "_id": "67bd6d2bbf6d46017e619f33",
          "user": {
            "_id": "646c6985d072747f7ebf352a",
            "avatarUrl": "/avatars/8aaf92045687b21b56c257db62bf4fa5.svg",
            "isPro": false,
            "fullname": "Jiawei Shao",
            "user": "jewelshaw",
            "type": "user"
          },
          "name": "Jiawei Shao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:50:09.030Z",
          "hidden": false
        },
        {
          "_id": "67bd6d2bbf6d46017e619f34",
          "name": "Yansong Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T17:40:09.000Z",
      "title": "KV-Edit: Tecnología de edición de imágenes sin entrenamiento que permite la conservación precisa del fondo.",
      "summary": "La consistencia de fondo es un problema importante en el trabajo de edición de imágenes, y la tecnología actual ha desarrollado este problema durante mucho tiempo. En la investigación actual, la armonía entre mantener la similitud con la imagen original y crear contenido que cumpla con el objetivo sigue siendo un desafío. En este contexto, se propone un enfoque sin necesidad de entrenamiento llamado KV-Edit. KV-Edit utiliza el cache de KV de DiTs para almacenar y evitar la recuperación de los tokens de fondo, manteniendo la consistencia del fondo mientras evita la necesidad de una compleja estructura o una entrenamiento costoso, y se centra en crear nuevo contenido continuo con el fondo en el área proporcionada por el usuario. Además, se investiga la consumo de memoria del cache de KV durante el proceso de edición, optimizando la complejidad espacial a O(1) sin necesidad de reversiones utilizando métodos sin reversiones. Nuestro enfoque permite expandir modelos de generación basados en DiT sin necesidad de entrenamiento adicional. Según los experimentos, KV-Edit supera significativamente las métricas de calidad tanto del fondo como de la imagen, demostrando que supera los métodos basados en entrenamiento. El sitio web del proyecto está disponible en https://xilluill.github.io/projectpages/KV-Edit.",
      "upvotes": 22,
      "discussionId": "67bd6d2dbf6d46017e619f99"
    },
    "publishedAt": "2025-02-25T21:36:19.851Z",
    "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17363.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66078994c50f8393c56ed837",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/aYYde45zaFACRllyEhJyU.jpeg",
      "fullname": "Tianrui Zhu",
      "name": "xilluill",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.18449",
      "authors": [
        {
          "_id": "67be845a8a5a80542314579f",
          "user": {
            "_id": "632a176259950c1d279d5ea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632a176259950c1d279d5ea7/xsSGhBXalt9RaKzSKY8uk.jpeg",
            "isPro": false,
            "fullname": "Yuxiang Wei",
            "user": "yuxiang630",
            "type": "user"
          },
          "name": "Yuxiang Wei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:50:44.837Z",
          "hidden": false
        },
        {
          "_id": "67be845a8a5a8054231457a0",
          "name": "Olivier Duchenne",
          "hidden": false
        },
        {
          "_id": "67be845a8a5a8054231457a1",
          "user": {
            "_id": "6481e0ac50b759c75d5fdad0",
            "avatarUrl": "/avatars/49f08d989ca505ae01bce5578a94f6fe.svg",
            "isPro": false,
            "fullname": "Jade Copet",
            "user": "JadeCopet",
            "type": "user"
          },
          "name": "Jade Copet",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:50:58.290Z",
          "hidden": false
        },
        {
          "_id": "67be845a8a5a8054231457a2",
          "name": "Quentin Carbonneaux",
          "hidden": false
        },
        {
          "_id": "67be845a8a5a8054231457a3",
          "user": {
            "_id": "656f473c14fa8cfccd14559e",
            "avatarUrl": "/avatars/8f4fef3d835a7a11c2ab66dbf04f3424.svg",
            "isPro": false,
            "fullname": "Lingming Zhang",
            "user": "lingming",
            "type": "user"
          },
          "name": "Lingming Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:51:10.640Z",
          "hidden": false
        },
        {
          "_id": "67be845a8a5a8054231457a4",
          "name": "Daniel Fried",
          "hidden": false
        },
        {
          "_id": "67be845a8a5a8054231457a5",
          "user": {
            "_id": "630eac7931970d1cd4fbacf2",
            "avatarUrl": "/avatars/b7ccbddfa745db854dc342be1327cd53.svg",
            "isPro": false,
            "fullname": "Gabriel Synnaeve",
            "user": "gsynnaeve",
            "type": "user"
          },
          "name": "Gabriel Synnaeve",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:51:21.641Z",
          "hidden": false
        },
        {
          "_id": "67be845a8a5a8054231457a6",
          "user": {
            "_id": "6597e5a6420dcc68501a69e9",
            "avatarUrl": "/avatars/da48b13e07c367ecd5c891abfd6c3ded.svg",
            "isPro": false,
            "fullname": "Rishabh Singh",
            "user": "RishabhSingh021",
            "type": "user"
          },
          "name": "Rishabh Singh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:51:28.321Z",
          "hidden": false
        },
        {
          "_id": "67be845a8a5a8054231457a7",
          "name": "Sida I. Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T18:45:04.000Z",
      "title": "SWE-RL: El desarrollo de la inferencia de LLM mediante aprendizaje reforzado - Un estudio sobre el desarrollo de software abierto",
      "summary": "La reciente publicación de DeepSeek-R1 ha demostrado el gran potencial de la aprendizaje por refuerzo (RL) para fortalecer las capacidades generales de inteligencia artificial (IA) en modelos grandes de lenguaje (LLMs). Mientras que otros trabajos continuos han aplicado RL principalmente a maratón de programación y problemas de matemáticas, este artículo presenta el primer enfoque para expandir la inteligencia artificial basada en RL en el ámbito de la ingeniería de software de nivel mundial. Utilizando una pequeña regla de recompensa basada (por ejemplo, puntuación de la similitud entre la solución real y la generada por el modelo de lenguaje), SWE-RL permite que el modelo de lenguaje (LLM) recupere automáticamente los procesos de inteligencia artificial de los desarrolladores a partir de los datos de evolución de software abierto, incluyendo eventos como ciclos de síntesis de software, capturas de código, cambios de código, errores y solicitudes de reposición. Tras ser entrenado en Llama 3, nuestro modelo de inteligencia artificial, Llama3-SWE-RL-70B, logró un rendimiento de 41.0% en SWE-bench Verified - un conjunto de problemas reales y humanamente certificados en GitHub. Según nuestra experiencia, esto es el mejor rendimiento reportado hasta el momento para modelos de LLMs de tamaño intermedio (<100B), y es considerablemente superior a los avances de modelos de IA personales como GPT-4o. Sorprendentemente, Llama3-SWE-RL, que se ha entrenado con datos de evolución de software limitados, ha descubierto una inteligencia artificial ampliada. Por ejemplo, ha mostrado mejoras en codificación de funciones, uso de bibliotecas, inteligencia artificial de código, matemáticas y comprensión general del lenguaje en cinco tareas externas. Además, el entrenamiento observacional basado en regresión lineal promedia un descenso en el rendimiento. En general, SWE-RL abre una nueva dirección para mejorar la inteligencia artificial de los modelos de lenguaje a través del aprendizaje por refuerzo con datos de ingeniería de software grande.",
      "upvotes": 14,
      "discussionId": "67be845b8a5a8054231457d6"
    },
    "publishedAt": "2025-02-25T22:03:08.515Z",
    "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18449.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6218
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17262",
      "authors": [
        {
          "_id": "67bd3870a917fc506d9f3d15",
          "user": {
            "_id": "66ab06956b8847339d449128",
            "avatarUrl": "/avatars/d71490acb91981459121005b84e556d8.svg",
            "isPro": false,
            "fullname": "Xu Chengyin",
            "user": "JerryXu98",
            "type": "user"
          },
          "name": "Chengyin Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:44.252Z",
          "hidden": false
        },
        {
          "_id": "67bd3870a917fc506d9f3d16",
          "user": {
            "_id": "636b4d796e6981ebad73f398",
            "avatarUrl": "/avatars/bcd405b98c12afaf1e32d85ad8ce7f23.svg",
            "isPro": false,
            "fullname": "Kaiyuan Chen",
            "user": "Lucky2022",
            "type": "user"
          },
          "name": "Kaiyuan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:01.532Z",
          "hidden": false
        },
        {
          "_id": "67bd3870a917fc506d9f3d17",
          "name": "Xiao Li",
          "hidden": false
        },
        {
          "_id": "67bd3870a917fc506d9f3d18",
          "user": {
            "_id": "645604eebabbbbd3486dc615",
            "avatarUrl": "/avatars/17a5ca8274e2bfc8f183a4af9878a930.svg",
            "isPro": false,
            "fullname": "shenke",
            "user": "shenke18",
            "type": "user"
          },
          "name": "Ke Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:49.578Z",
          "hidden": false
        },
        {
          "_id": "67bd3870a917fc506d9f3d19",
          "name": "Chenggang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T15:44:57.000Z",
      "title": "Metodos para esclarecer la escala de resultados de LLM: Metodos de visualizacion basados en clustering",
      "summary": "El desarrollo rápido de los calculadoras avanzadas tiene un gran impacto en el tamaño y costo de entrenamiento de modelos de lenguaje grandes (LLM). Predecir con precisión el rendimiento de tareas posteriores es importante para la distribución eficiente de recursos, pero se enfrentan dos principales limitaciones: (1) \"El fenómeno\" hace que los métricas de rendimiento de las tareas posteriores tengan significado detallado después del entrenamiento, pero su capacidad para predecirlas con pequeños modelos es limitada; (2) la distribución no uniforme de la dificultad de las tareas y la ausencia de leyes de escalado consistentes generan grandes variaciones en los métricas. Los métodos actuales de predicción de rendimiento tienen limitaciones en precisión y confianza, lo que impide evaluar los potenciales de los LLM. Para abordar estos problemas, se propone un marco de predicción de rendimiento de tareas posteriores basado en la dificultad (COD). COD agrupa las tareas según sus características de dificultad para construir subconjuntos soportables y estrategicamente excluye clusters no observables y no escalables. Los puntajes de los subconjuntos seleccionados actúan como buenos predecidores intermedios de la rendimiento de la evaluación completa. Con apoyo teórico, se puede obtener una función que transforma métricas de subconjuntos posibles en el conjunto de evaluación completo. El método se aplica a la predicción de escalado de rendimiento de un LLM de 70B, proporciona acciones que pueden ayudar a la distribución de recursos de entrenamiento y observar el proceso de entrenamiento. En particular, COD alcanza una precisión notable en la predicción de un LLM de 70B usando un ensamble de pequeños modelos, con un desvío absoluto medio de 1.36% en 8 importantes pruebas de benchmark de LLM.",
      "upvotes": 13,
      "discussionId": "67bd3872a917fc506d9f3d8f"
    },
    "publishedAt": "2025-02-25T22:18:24.064Z",
    "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17262.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636b4d796e6981ebad73f398",
      "avatarUrl": "/avatars/bcd405b98c12afaf1e32d85ad8ce7f23.svg",
      "fullname": "Kaiyuan Chen",
      "name": "Lucky2022",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15499",
      "authors": [
        {
          "_id": "67be86743ea16c7e9491ff16",
          "name": "Ya Wang",
          "hidden": false
        },
        {
          "_id": "67be86743ea16c7e9491ff17",
          "user": {
            "_id": "66335b9c95c5b79ebf306f30",
            "avatarUrl": "/avatars/d57784ee65cbef014360c9bac1ad4119.svg",
            "isPro": false,
            "fullname": "Zhijian Zhuo",
            "user": "BryceZhuo",
            "type": "user"
          },
          "name": "Zhijian Zhuo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:58:10.556Z",
          "hidden": false
        },
        {
          "_id": "67be86743ea16c7e9491ff18",
          "user": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "isPro": false,
            "fullname": "Yutao Zeng",
            "user": "Taoer",
            "type": "user"
          },
          "name": "Yutao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-26T08:25:55.016Z",
          "hidden": false
        },
        {
          "_id": "67be86743ea16c7e9491ff19",
          "user": {
            "_id": "62533db4a06ec75172eeabe7",
            "avatarUrl": "/avatars/b1a4dad90afae5c00df97233a97777db.svg",
            "isPro": false,
            "fullname": "xunzhou",
            "user": "xunzhou",
            "type": "user"
          },
          "name": "Xun Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:52:26.974Z",
          "hidden": false
        },
        {
          "_id": "67be86743ea16c7e9491ff1a",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67be86743ea16c7e9491ff1b",
          "user": {
            "_id": "64648638351adef1a847a7ad",
            "avatarUrl": "/avatars/7518e058fcf81ee81a06c96e996531e9.svg",
            "isPro": false,
            "fullname": "Xiaoqing Li",
            "user": "LLIXQ",
            "type": "user"
          },
          "name": "Xiaoqing Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:51:57.314Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T14:49:34.000Z",
      "title": "Escape de la distribución de escala: hace posible la entrenamiento estable y efectivo de modelos de lenguaje de gran escala.",
      "summary": "La estabilidad del entrenamiento es un problema largo duradero conocido en el aprendizaje previo de modelos de lenguaje grandes (LLM), especialmente en estructuras como el Transformer Post-Norm, que son vulnerables a explosiones de gradientes y a la pérdida. En este estudio, se propone una nueva aproximación llamada \"Separación de la Distribución de Escalas (SDD)\" para prevenir la explosión y la pérdida de gradientes y estabilizar el entrenamiento. La SDD separa explícitamente las escalas y distribuciones de los matrices de pesos en todas las capas de la unión, utilizando estructuras de normalización y vectores de escalabilidad aprendibles para regular la activación. De esta manera, previene la explosión y la pérdida de gradientes y optimiza la eficiencia de la optimización. Esta separación garantiza una propagación estable de gradientes, y funciona efectivamente incluso en redes muy profundas. Los resultados de los experimentos demuestran que la SDD garantiza la estabilidad del entrenamiento de diferentes estructuras de LLM y obtiene resultados superiores a los métodos existentes en diferentes configuraciones de normalización. Además, la propuesta es ligera, compatible con los frameworks existentes y proporciona una solución práctica efectiva para la estabilidad del entrenamiento de LLM. El código está disponible en https://github.com/kaihemo/SDD.",
      "upvotes": 10,
      "discussionId": "67be86753ea16c7e9491ff49"
    },
    "publishedAt": "2025-02-25T22:26:11.421Z",
    "title": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/eu6jpeTjTn34I1SJ4_K1a.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/P6mXXagZPsH6fwQ6myMlr.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15499.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6371128eafbe42caa5a5222b",
      "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
      "fullname": "Yutao Zeng",
      "name": "Taoer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.18364",
      "authors": [
        {
          "_id": "67be81414084d82ee69ad4a2",
          "user": {
            "_id": "647e83257f9ad5e44babe82a",
            "avatarUrl": "/avatars/2d9593775c49856fe5dfa5bd23dfcda7.svg",
            "isPro": false,
            "fullname": "yifan pu",
            "user": "yifanpu001",
            "type": "user"
          },
          "name": "Yifan Pu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:58:24.942Z",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4a3",
          "user": {
            "_id": "65e78ebf24a38e0fc5e149e6",
            "avatarUrl": "/avatars/d05e267f29de7de226c4fc0ae37c95ff.svg",
            "isPro": false,
            "fullname": "Yiming Zhao",
            "user": "2JZ",
            "type": "user"
          },
          "name": "Yiming Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:58:30.860Z",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4a4",
          "name": "Zhicong Tang",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4a5",
          "name": "Ruihong Yin",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4a6",
          "user": {
            "_id": "65229f2f6b01183a67e86370",
            "avatarUrl": "/avatars/b218207fce28497b30e22c807d44b2d2.svg",
            "isPro": false,
            "fullname": "Haoxing Ye",
            "user": "131131yhx",
            "type": "user"
          },
          "name": "Haoxing Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:58:52.821Z",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4a7",
          "name": "Yuhui Yuan",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4a8",
          "user": {
            "_id": "666470a28f5513b0cf11e850",
            "avatarUrl": "/avatars/7beea758882677ad32a12ce56d4d084a.svg",
            "isPro": false,
            "fullname": "Dong Chen",
            "user": "DongChen06",
            "type": "user"
          },
          "name": "Dong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:59:16.526Z",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4a9",
          "user": {
            "_id": "646b2f4bb1202bc77c0fb396",
            "avatarUrl": "/avatars/6b09dec5d5affe817ad6acda60f61740.svg",
            "isPro": false,
            "fullname": "Jianmin_bao",
            "user": "JianminBao",
            "type": "user"
          },
          "name": "Jianmin Bao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:59:22.654Z",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4aa",
          "user": {
            "_id": "64f7f119a92703ef65d9a717",
            "avatarUrl": "/avatars/118524faab66cecba6d4da622034b44b.svg",
            "isPro": false,
            "fullname": "Sirui Zhang",
            "user": "zsr200901",
            "type": "user"
          },
          "name": "Sirui Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:59:30.766Z",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4ab",
          "user": {
            "_id": "67965a5a9f57883759a6efc3",
            "avatarUrl": "/avatars/9138a879fbe1f60c2f4720810bfdfda6.svg",
            "isPro": false,
            "fullname": "Yanbin Wang",
            "user": "yanbinwang",
            "type": "user"
          },
          "name": "Yanbin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:59:38.138Z",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4ac",
          "name": "Lin Liang",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4ad",
          "user": {
            "_id": "6672e20d1dbdf7da8310dd92",
            "avatarUrl": "/avatars/5d2fb23f92a7f9ff025a5be17a26de4d.svg",
            "isPro": false,
            "fullname": "lijuanwang",
            "user": "lijuanwang228",
            "type": "user"
          },
          "name": "Lijuan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T09:00:05.520Z",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4ae",
          "name": "Ji Li",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4af",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4b0",
          "user": {
            "_id": "64c882f7527d7636555bbb2c",
            "avatarUrl": "/avatars/578a118a945dd6fa62fd3be9d6e4e986.svg",
            "isPro": false,
            "fullname": "Zhouhui Lian",
            "user": "lianzhouhui",
            "type": "user"
          },
          "name": "Zhouhui Lian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T08:59:57.943Z",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4b1",
          "name": "Gao Huang",
          "hidden": false
        },
        {
          "_id": "67be81414084d82ee69ad4b2",
          "name": "Baining Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T16:57:04.000Z",
      "title": "ART: Variable Multi-Layer Transparent Image Generation with Non-Named Region Transformer",
      "summary": "La generación de imágenes multi-capa es una tarea fundamental que permite a los usuarios separar, seleccionar y editar capas específicas de una imagen, lo que es esencial para innovar la interacción entre modelos de generación. En este artículo, se introduce el Anonymous Region Transformer (ART), que genera imágenes multi-capa directamente basándose en un texto global y la posición de regiones anónimas. ART es un modelo estructurante de conocimiento basado en teoría sencilla, diferenciándose de los principales modelos de posición de regiones significativas existentes, ya que permite que el modelo de generación determine automáticamente la correspondencia entre tokens de texto y tokens visuales. Además, la estructura de corte de regiones, que selecciona solo los tokens visuales pertenecientes a cada region anónima, reduce significativamente el costo computacional y permite la generación eficiente de imágenes multi-capa (por ejemplo, más de 50 capas). Comparado con un enfoque global, ART es más rápido en al menos 12 veces y reduce las colisiones entre capas. Además, ART permite la generación eficiente de capas y el control preciso de la visualidad, estableciendo un nuevo patrón interactivo de generación de contenido.",
      "upvotes": 7,
      "discussionId": "67be81464084d82ee69ad576"
    },
    "publishedAt": "2025-02-25T21:50:19.941Z",
    "title": "ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18364.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646f69a6041e48e1c4728de3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f69a6041e48e1c4728de3/U5OaW6PgsXTXnfG03xs9Q.png",
      "fullname": "GlyphByT5",
      "name": "GlyphByT5",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 34
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18461",
      "authors": [
        {
          "_id": "67bea0cc2d6011a72335f704",
          "user": {
            "_id": "67be9daa65ae638b17e461e9",
            "avatarUrl": "/avatars/30ab04b8a6a4d3e1d211943c0344b95e.svg",
            "isPro": false,
            "fullname": "Ziheng Ouyang",
            "user": "oyzh2005",
            "type": "user"
          },
          "name": "Ziheng Ouyang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-26T08:25:46.941Z",
          "hidden": false
        },
        {
          "_id": "67bea0cc2d6011a72335f705",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67bea0cc2d6011a72335f706",
          "name": "Qibin Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T18:59:12.000Z",
      "title": "K-LoRA: Método de integrar LoRA de cualquier estilo y sección de gratis",
      "summary": "Recientes estudios han revisado el método de combinar diferentes LoRA para generar estilos y contenidos en el mismo tiempo. Sin embargo, los métodos actuales no pueden conservar efectivamente los dos temas y estilos de forma simultánea, o requieren entrenamiento adicional. En este artículo, se afirma que las características internas de LoRA pueden fusionar eficazmente temas y estilos aprendidos en modelos de dispersión. Basándose en esta perspectiva, se propone K-LoRA, una aproximación sencilla y efectiva sin entrenamiento adicional para combinar LoRA. En cada capa de atención, K-LoRA compara los K elementos más importantes de los LoRA a fusionar y selecciona el LoRA que mejor se adapta a la fusión óptima. Esta estructura de selección garantiza que los principales características de tema y estilo se mantengan durante el proceso de fusión y que su contribución se equilibre efectivamente. A través de los resultados experimentales, se muestra que el método propuesto integra efectivamente la información de tema y estilo aprendida por los LoRA originales y supera los estándares de entrenamiento en términos cualitativos y cuantitativos.",
      "upvotes": 6,
      "discussionId": "67bea0cf2d6011a72335f7aa"
    },
    "publishedAt": "2025-02-26T00:56:27.275Z",
    "title": "K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18461.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6285a9133ab6642179158944",
      "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
      "fullname": "Zhen Li",
      "name": "Paper99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18356",
      "authors": [
        {
          "_id": "67be8866823e790d21a2bb90",
          "user": {
            "_id": "6529aa1460e706730575baa9",
            "avatarUrl": "/avatars/550fac58a6ebf937a65d19a48e71eb45.svg",
            "isPro": false,
            "fullname": "George Thomas",
            "user": "georgethomas",
            "type": "user"
          },
          "name": "George Thomas",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T09:06:53.500Z",
          "hidden": false
        },
        {
          "_id": "67be8866823e790d21a2bb91",
          "user": {
            "_id": "636c1e4415cd58e915bc45df",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c1e4415cd58e915bc45df/KnPgdPe0G5ngvXaCBua6R.jpeg",
            "isPro": false,
            "fullname": "Alex J. Chan",
            "user": "XanderJC",
            "type": "user"
          },
          "name": "Alex J. Chan",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-26T03:20:08.029Z",
          "hidden": false
        },
        {
          "_id": "67be8866823e790d21a2bb92",
          "name": "Jikun Kang",
          "hidden": false
        },
        {
          "_id": "67be8866823e790d21a2bb93",
          "user": {
            "_id": "63a2f1dfc8a2aa5d9e85f8f6",
            "avatarUrl": "/avatars/f2191e3a0ce92563f9bfe83283d8d966.svg",
            "isPro": false,
            "fullname": "Wenqi Wu",
            "user": "BiggieW",
            "type": "user"
          },
          "name": "Wenqi Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T09:08:43.843Z",
          "hidden": false
        },
        {
          "_id": "67be8866823e790d21a2bb94",
          "user": {
            "_id": "64f46b681d337935d0495d4d",
            "avatarUrl": "/avatars/cce5a4910617931fb13062b832e14ef8.svg",
            "isPro": false,
            "fullname": "Filippos Christianos",
            "user": "semitable",
            "type": "user"
          },
          "name": "Filippos Christianos",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T09:08:23.534Z",
          "hidden": false
        },
        {
          "_id": "67be8866823e790d21a2bb95",
          "user": {
            "_id": "5f195784925b9863e28ad610",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1595496291585-noauth.png",
            "isPro": false,
            "fullname": "Fraser Greenlee",
            "user": "Fraser",
            "type": "user"
          },
          "name": "Fraser Greenlee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T09:08:16.380Z",
          "hidden": false
        },
        {
          "_id": "67be8866823e790d21a2bb96",
          "name": "Andy Toulis",
          "hidden": false
        },
        {
          "_id": "67be8866823e790d21a2bb97",
          "user": {
            "_id": "6787c4a970c0f5272f456968",
            "avatarUrl": "/avatars/bdfa53add57b0f0a9e4e94e24115b354.svg",
            "isPro": false,
            "fullname": "Marvin Purtorab",
            "user": "comvergent-marvin",
            "type": "user"
          },
          "name": "Marvin Purtorab",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T09:07:42.610Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T16:45:08.000Z",
      "title": "Webjuego: Agente de búsqueda AI para sitios web comunes difíciles",
      "summary": "WebGames es una hoja de cálculo estricta de referencia diseñada para evaluar los agentes de navegación web, reúniendo problemas interactivos con un valor de 50 o más puntos. Este conjunto de problemas ha sido especialmente diseñado para ser faciles para un ser humano, y tiene como objetivo la verificación sistemática de los límites de los sistemas AI actuales en términos de interacción básica con el navegador, procesamiento de entradas altamente complejas, tareas cognitivas, automatización de flujos de trabajo y entretenimiento interactivo. El marco de trabajo asegura evaluaciones reproducibles en un entorno de prueba armonioso, excluyendo dependencias externas. Se comparan los rendimientos de modelos visuales avanzados como GPT-4o, Claude Computer-Use, Gemini-1.5-Pro y Qwen2-VL con el rendimiento humano. Los resultados muestran significativas diferencias de capacidad, con el mejor sistema AI alcanzando un rendimiento del 95.7% en un 43.1% de los casos. Este benchmark revela fundamentales límites en la capacidad de procesar patrones de interacción web que son intuitivos para el ser humano. WebGames está disponible en webgames.convergence.ai y proporciona una implementación ligera en el lado cliente para acelerar los ciclos de evaluación. Mediante la especificación de problemas estandarizados y una arquitectura modular, WebGames ofrece una sólida base para medir el progreso en el desarrollo de AI avanzados para navegación web.",
      "upvotes": 4,
      "discussionId": "67be8868823e790d21a2bbea"
    },
    "publishedAt": "2025-02-25T22:20:16.916Z",
    "title": "WebGames: Challenging General-Purpose Web-Browsing AI Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18356.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6218
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16794",
      "authors": [
        {
          "_id": "67be86a78a5a80542314f0e6",
          "user": {
            "_id": "6531a65daed617662c7f1007",
            "avatarUrl": "/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg",
            "isPro": false,
            "fullname": "Xilin Jiang",
            "user": "xi-j",
            "type": "user"
          },
          "name": "Xilin Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-26T08:25:52.841Z",
          "hidden": false
        },
        {
          "_id": "67be86a78a5a80542314f0e7",
          "user": {
            "_id": "661361993bb67cb4f356c3de",
            "avatarUrl": "/avatars/b707c07f9c70d2ed1e8cd8cff2551c69.svg",
            "isPro": false,
            "fullname": "Sukru Samet Dindar",
            "user": "susameddin",
            "type": "user"
          },
          "name": "Sukru Samet Dindar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T09:11:37.706Z",
          "hidden": false
        },
        {
          "_id": "67be86a78a5a80542314f0e8",
          "user": {
            "_id": "670e8671ba29b3fca221b8c9",
            "avatarUrl": "/avatars/20f6479bd5218d6d3e304539df5003f9.svg",
            "isPro": false,
            "fullname": "Vishal Choudhari",
            "user": "vchoudhari",
            "type": "user"
          },
          "name": "Vishal Choudhari",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T09:11:45.258Z",
          "hidden": false
        },
        {
          "_id": "67be86a78a5a80542314f0e9",
          "name": "Stephan Bickel",
          "hidden": false
        },
        {
          "_id": "67be86a78a5a80542314f0ea",
          "name": "Ashesh Mehta",
          "hidden": false
        },
        {
          "_id": "67be86a78a5a80542314f0eb",
          "name": "Guy M McKhann",
          "hidden": false
        },
        {
          "_id": "67be86a78a5a80542314f0ec",
          "name": "Adeen Flinker",
          "hidden": false
        },
        {
          "_id": "67be86a78a5a80542314f0ed",
          "name": "Daniel Friedman",
          "hidden": false
        },
        {
          "_id": "67be86a78a5a80542314f0ee",
          "name": "Nima Mesgarani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T03:06:45.000Z",
      "title": "AAD-LLM: Attención de Red para Comprensión de Escalas Acústicas",
      "summary": "Modelos basados en el sonido, incluyendo específicamente modelos de lenguaje de lenguaje de sonido (LLMs), procesan de manera uniforme todos los entradas sonoras y no dependen de la percepción de los oyentes. Sin embargo, la percepción acústica humana tiene una selección inherente: los oyentes pueden centrarse en un solo hablante en un escenario complejo de sonidos, ignorando el resto. Actualmente, los modelos no tienen esta capacidad de selección, lo que limita su capacidad para generar respuestas adecuadas. Para abordar este problema, introducimos la Inteligencia Informada de Comprensión del Escenario Acústico (II-ASU) y presentamos un sistema prototipo, el Modelo de Lenguaje de Lenguaje de Sonido (AAD-LLM), que estima la atención del oyente integrando señales cerebrales. El AAD-LLM interpreta al hablante en el que el oyente está prestando atención y genera respuestas mejoradas mediante la integración de registros de programación de valores eléctricos cerebrales (iEEG). El modelo predice al hablante en el que el oyente está prestando atención y condiciona la generación de respuestas en función del estado de atención predecido. El AAD-LLM ha demostrado mejorar las evaluaciones subjetivas y objetivas de los hablantes, así como la precisión de la identificación de hablantes, traducción de discursos, extracción de información y respuestas a preguntas, en escenarios de múltiples hablantes. Esta investigación es la primera etapa en la inteligencia artificial de sonido orientada a la intención y explora un nuevo paradigma en el que la percepción humana proporciona información a la audición de la máquina. Los demostraciones y códigos están disponibles en la siguiente URL: https://aad-llm.github.io.",
      "upvotes": 4,
      "discussionId": "67be86a98a5a80542314f16e"
    },
    "publishedAt": "2025-02-25T22:20:08.416Z",
    "title": "AAD-LLM: Neural Attention-Driven Auditory Scene Understanding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16794.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6531a65daed617662c7f1007",
      "avatarUrl": "/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg",
      "fullname": "Xilin Jiang",
      "name": "xi-j",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17425",
      "authors": [
        {
          "_id": "67bddd63c7d8b835b82ced9a",
          "user": {
            "_id": "635364b3c41f548fe39db945",
            "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
            "isPro": false,
            "fullname": "Runpeng Yu",
            "user": "rp-yu",
            "type": "user"
          },
          "name": "Runpeng Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T09:14:07.580Z",
          "hidden": false
        },
        {
          "_id": "67bddd63c7d8b835b82ced9b",
          "user": {
            "_id": "64396ebc21221ac7411852b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
            "isPro": false,
            "fullname": "Xinyin Ma",
            "user": "horseee",
            "type": "user"
          },
          "name": "Xinyin Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T09:14:13.670Z",
          "hidden": false
        },
        {
          "_id": "67bddd63c7d8b835b82ced9c",
          "user": {
            "_id": "63fc03a50aab060792ffef39",
            "avatarUrl": "/avatars/9d5b1bb2a41928e08176b703935133ab.svg",
            "isPro": false,
            "fullname": "Wangxinchao",
            "user": "wxcTest",
            "type": "user"
          },
          "name": "Xinchao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T09:14:53.838Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:56:12.000Z",
      "title": "Introduzco la forma de integrar el Token de Percepción Visual en un Modelo de Lenguaje de Grandes Tamaños Multimodal.",
      "summary": "La utilización de la información visual depende del modelo de lenguaje multimodal de diferentes modalidades (MLLM), que depende del procesamiento de reconocimiento visual del encoder visual. La completitud y la precisión del reconocimiento visual tienen un gran impacto en la lógica espacial, el entendimiento detallado y la precisión de otras tareas. Sin embargo, el MLLM no tiene la capacidad de revisar selectivamente áreas específicas de una imagen o centrarse en información relacionada con categorías de objetos específicas. En este estudio, se propone el concepto de token de reconocimiento visual y se busca asignar una estructura que permita que el MLLM controle el procesamiento de reconocimiento visual. Se diseñan dos tipos de tokens de reconocimiento visual: Token de Selección de Regiones y Token de Reencodificación Visual. El MLLM genera estos tokens para iniciar acciones adicionales de reconocimiento visual. El Token de Selección de Regiones define claramente una región de la imagen y continúa con esa región. El Token de Reencodificación Visual utiliza su estado oculto como señal de control para guiar un procesamiento visual adicional. Los experimentos extendidos muestran las ventajas de estos tokens en la lógica espacial, el entendimiento detallado y otras tareas. En promedio, la introducción de los tokens de reconocimiento visual mejora el rendimiento del modelo de 2B en un 23.6%, aumentando los puntajes de 0.572 a 0.708, y sobrepasa en más de 13.4% el rendimiento del modelo de 7B (de 0.624 a). Se solicita una verificación local. https://github.com/yu-rp/VisualPerceptionToken",
      "upvotes": 3,
      "discussionId": "67bddd64c7d8b835b82cee5a"
    },
    "publishedAt": "2025-02-26T02:37:36.287Z",
    "title": "Introducing Visual Perception Token into Multimodal Large Language Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17425.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635364b3c41f548fe39db945",
      "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
      "fullname": "Runpeng Yu",
      "name": "rp-yu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17535",
      "authors": [
        {
          "_id": "67beaec94a1d9d7e368a7840",
          "user": {
            "_id": "66a4a319a1711696948b045c",
            "avatarUrl": "/avatars/1d92d57a949332cb8227697b9a0c2f39.svg",
            "isPro": false,
            "fullname": "Zhenheng Tang",
            "user": "coolzhtang",
            "type": "user"
          },
          "name": "Zhenheng Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T09:15:34.971Z",
          "hidden": false
        },
        {
          "_id": "67beaec94a1d9d7e368a7841",
          "name": "Xiang Liu",
          "hidden": false
        },
        {
          "_id": "67beaec94a1d9d7e368a7842",
          "name": "Qian Wang",
          "hidden": false
        },
        {
          "_id": "67beaec94a1d9d7e368a7843",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67beaec94a1d9d7e368a7844",
          "name": "Bingsheng He",
          "hidden": false
        },
        {
          "_id": "67beaec94a1d9d7e368a7845",
          "user": {
            "_id": "6676935fcd0b89a0115174b0",
            "avatarUrl": "/avatars/4caca1b672d29e787814f9a30bf20bcc.svg",
            "isPro": false,
            "fullname": "Xiaowen Chu",
            "user": "wenxinsiju",
            "type": "user"
          },
          "name": "Xiaowen Chu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-26T09:15:41.884Z",
          "hidden": false
        },
        {
          "_id": "67beaec94a1d9d7e368a7846",
          "name": "Bo Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T15:39:35.000Z",
      "title": "La hipótesis de LLM Rotellieri, ¿Qué habilidades deben mantenerse durante la compresión de un LLM?",
      "summary": "El descenso de costos computacionales y de almacenamiento ha llevado a que los modelos de LLMs (Modelos de Lenguaje de Gran Tamaño) y la compresión de caché KV (Cache de Palabras y Vectores) sean objetos de atención para los investigadores. Sin embargo, los métodos actuales enfrentan principalmente el desafío de medir el rendimiento de los modelos comprimidos de LLMs, priorizando a menudo la preservación de precisión sencilla. Este blog ofrece una breve revisión sobre los últimos avances en la generación de asambleas de búsqueda relacionadas con los LLMs, lógica multinivel, herramientas externas y representación computacional, que pueden significativamente mejorar su rendimiento. A continuación, se propone una hipótesis sobre el existencia de pequeños modelos de LLMs para el idioma latino, apoyada por la lógica multinivel y las herramientas externas. Basándose en una revisión del desarrollo actual de los LLMs, se discuten las capacidades básicas necesarias para los modelos de LLMs y la compresión de caché KV, y se explica lo que se ha dejado de lado por el actual derecho.",
      "upvotes": 3,
      "discussionId": "67beaeca4a1d9d7e368a7875"
    },
    "publishedAt": "2025-02-26T01:04:23.776Z",
    "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17535.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17092",
      "authors": [
        {
          "_id": "67bea8cc7e54112af6c372aa",
          "user": {
            "_id": "63d9e09f1cae35c27bf80cb2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675223055197-noauth.jpeg",
            "isPro": true,
            "fullname": "Syed Abdul Gaffar Shakhadri",
            "user": "SyedAbdul",
            "type": "user"
          },
          "name": "Syed Abdul Gaffar Shakhadri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-26T05:52:19.355Z",
          "hidden": false
        },
        {
          "_id": "67bea8cc7e54112af6c372ab",
          "user": {
            "_id": "5fb7ae48e6ae537272bdeb3c",
            "avatarUrl": "/avatars/e5d01cb428f4b22161e0d17895a5c678.svg",
            "isPro": false,
            "fullname": "Kruthika",
            "user": "kruthika",
            "type": "user"
          },
          "name": "Kruthika KR",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-26T05:38:21.529Z",
          "hidden": false
        },
        {
          "_id": "67bea8cc7e54112af6c372ac",
          "user": {
            "_id": "677cc34fe4cf361eedccd085",
            "avatarUrl": "/avatars/e97a3f9a84ed258ab4b75c12865562d6.svg",
            "isPro": false,
            "fullname": "Kartik Basavaraj Angadi",
            "user": "KartikAngadi",
            "type": "user"
          },
          "name": "Kartik Basavaraj Angadi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-26T05:38:21.529Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T12:15:07.000Z",
      "title": "Shakti-VLMs: Modelo de Visión-Lenguaje Escalable para la IA de la Empresa",
      "summary": "Shakti VLM es una familia de modelos de lógica de lenguaje con 1B y 4B parámetros. Estos modelos se diseñaron para resolver los problemas de eficiencia de datos en entrenamiento de múltiples modos. Los VLMs recientes logran altos rendimientos con grandes conjuntos de datos de entrenamiento, pero los modelos Shakti utilizan innovaciones arquitectónicas para obtener resultados competitivos con pocos tokens. Los puntos principales de mejora incluyen la QK-Normalization (estabilización de la atención), el método de normalización híbrido y la mejora en el codificado de posición. Una estrategia de treinamiento en tres etapas mejora la eficiencia del aprendizaje. Según los resultados de evaluación, Shakti-VLM-1B y Shakti-VLM-4B excelen en comprensión de documentos, lógica visual, extracción OCR y lógica general multi-modal. Nuestros resultados demuestran que altos rendimientos pueden ser logrados por el diseño y estrategia de entrenamiento del modelo, lo que actúa como una solución eficiente para tareas multi-modal de escala empresarial, no solo por el tamaño de los datos.",
      "upvotes": 2,
      "discussionId": "67bea8cd7e54112af6c37305"
    },
    "publishedAt": "2025-02-26T00:38:42.527Z",
    "title": "Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17092.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d9e09f1cae35c27bf80cb2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675223055197-noauth.jpeg",
      "fullname": "Syed Abdul Gaffar Shakhadri",
      "name": "SyedAbdul",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  }
]