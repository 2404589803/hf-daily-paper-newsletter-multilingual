[
  {
    "paper": {
      "id": "2507.03724",
      "authors": [
        {
          "_id": "686c7266364e2ad167eb5319",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531a",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531b",
          "name": "Chenyang Xi",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531c",
          "name": "Hanyu Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531d",
          "name": "Chen Tang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531e",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531f",
          "name": "Ding Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5320",
          "name": "Jiawei Yang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5321",
          "name": "Chunyu Li",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5322",
          "name": "Qingchen Yu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5323",
          "name": "Jihao Zhao",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5324",
          "name": "Yezhaohui Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5325",
          "name": "Peng Liu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5326",
          "name": "Zehao Lin",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5327",
          "name": "Pengyuan Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5328",
          "name": "Jiahao Huo",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5329",
          "name": "Tianyi Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532a",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532b",
          "name": "Kehang Li",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532c",
          "name": "Zhen Tao",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532d",
          "name": "Junpeng Ren",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532e",
          "name": "Huayi Lai",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532f",
          "name": "Hao Wu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5330",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5331",
          "name": "Zhenren Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5332",
          "name": "Zhaoxin Fan",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5333",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5334",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5335",
          "name": "Junchi Yan",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5336",
          "name": "Mingchuan Yang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5337",
          "name": "Tong Xu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5338",
          "name": "Wei Xu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5339",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533a",
          "name": "Haofeng Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533b",
          "name": "Hongkang Yang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533c",
          "user": {
            "_id": "686c965f418acea658859af4",
            "avatarUrl": "/avatars/4f453abeb7e82a3042cbec751b5cdb63.svg",
            "isPro": false,
            "fullname": "Wentao Zhang",
            "user": "Wentao-PKU",
            "type": "user"
          },
          "name": "Wentao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:32.391Z",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533d",
          "name": "Zhi-Qin John Xu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533e",
          "name": "Siheng Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533f",
          "name": "Feiyu Xiong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/twRhDA4ThpQMfInSFdYDA.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/9xHrSG0a8X2CYHxvfMFSH.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/aaUeE4u2RhCnFYNKCqvyn.gif"
      ],
      "publishedAt": "2025-07-04T17:21:46.000Z",
      "submittedOnDailyAt": "2025-07-08T01:46:43.501Z",
      "title": "MemOS: Sistema de Memoria del Óptico para Sistemas de Inteligencia Artificial",
      "submittedOnDailyBy": {
        "_id": "669e0b93c7cb0568dac6e92e",
        "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
        "isPro": false,
        "fullname": "hanyu Wang",
        "user": "UglyToilet",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje de gran escala (LLMs) se han establecido como una infraestructura importante para la inteligencia artificial (AGI), pero su deficiencia en el manejo de la memoria tiene un impacto negativo en la lógica de largo contexto, la personalización continua y el desarrollo de la coherencia de la conocida. Actualmente, los modelos dependen de parámetros fijos y estados de contexto temporales cortos, limitando su capacidad para seguir las preferencias del usuario o actualizar conocimientos a largo plazo. La RAG (Recuperación de Conocimiento con Texto) agrega conocimientos externos en forma de texto, pero es una solución ligera sin estado y carece de control sobre la vida ciclo de la información o la integración de representaciones sostenibles. Recientes estudios han demostrado que modelar el costo de entrenamiento y inferencia de los LLMs desde la perspectiva de la capa de memoria, y agregando una capa clara de memoria entre los parámetros y los datos externos, puede reducirse significativamente este costo. Además de la eficiencia computacional, los LLMs pueden abordar una amplia gama de problemas que surgen en función del tiempo y el contexto. Para resolver estos problemas, proponemos MemOS (Sistema Operativo de Memoria). MemOS trata la memoria como un recurso de sistema, integrando la representación de memoria en texto, basada en activación y a nivel de parámetros, junto con la programación y la evolución para facilitar almacenamiento y búsqueda de manera costo-eficiente. En términos básicos, MemCube es un conjunto de datos que agrupa el contenido de la memoria, los datos originales y información de versión. MemCube puede asemblarse, moverse y fusionarse a lo largo del tiempo, permitiendo la transición flexible de tipos de memoria y combinar el aprendizaje basado en parámetros con la búsqueda. MemOS construye un marco de sistema centrado en la memoria, proporcionando a los LLMs la posibilidad de control, flexibilidad y evolución, y se centra en la construcción de una base para el aprendizaje continuo y la modelación de la personalización.",
      "upvotes": 65,
      "discussionId": "686c7266364e2ad167eb5340",
      "projectPage": "https://memos.openmem.net/",
      "githubRepo": "https://github.com/MemTensor/MemOS",
      "ai_summary": "MemOS is proposed as a memory operating system for Large Language Models to enhance memory management, enabling efficient storage and retrieval, and facilitating continual learning and personalized modeling.",
      "ai_keywords": [
        "LLMs",
        "Artificial General Intelligence",
        "AGI",
        "Retrieval-Augmented Generation",
        "RAG",
        "MemOS",
        "memory operating system",
        "MemCube",
        "activation-based memory"
      ],
      "githubStars": 527
    },
    "publishedAt": "2025-07-04T13:21:46.000Z",
    "title": "MemOS: A Memory OS for AI System",
    "summary": "Large Language Models (LLMs) have become an essential infrastructure for\nArtificial General Intelligence (AGI), yet their lack of well-defined memory\nmanagement systems hinders the development of long-context reasoning, continual\npersonalization, and knowledge consistency.Existing models mainly rely on\nstatic parameters and short-lived contextual states, limiting their ability to\ntrack user preferences or update knowledge over extended periods.While\nRetrieval-Augmented Generation (RAG) introduces external knowledge in plain\ntext, it remains a stateless workaround without lifecycle control or\nintegration with persistent representations.Recent work has modeled the\ntraining and inference cost of LLMs from a memory hierarchy perspective,\nshowing that introducing an explicit memory layer between parameter memory and\nexternal retrieval can substantially reduce these costs by externalizing\nspecific knowledge. Beyond computational efficiency, LLMs face broader\nchallenges arising from how information is distributed over time and context,\nrequiring systems capable of managing heterogeneous knowledge spanning\ndifferent temporal scales and sources. To address this challenge, we propose\nMemOS, a memory operating system that treats memory as a manageable system\nresource. It unifies the representation, scheduling, and evolution of\nplaintext, activation-based, and parameter-level memories, enabling\ncost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates\nboth memory content and metadata such as provenance and versioning. MemCubes\ncan be composed, migrated, and fused over time, enabling flexible transitions\nbetween memory types and bridging retrieval with parameter-based learning.\nMemOS establishes a memory-centric system framework that brings\ncontrollability, plasticity, and evolvability to LLMs, laying the foundation\nfor continual learning and personalized modeling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/twRhDA4ThpQMfInSFdYDA.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/9xHrSG0a8X2CYHxvfMFSH.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/aaUeE4u2RhCnFYNKCqvyn.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03724.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669e0b93c7cb0568dac6e92e",
      "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
      "fullname": "hanyu Wang",
      "name": "UglyToilet",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05163",
      "authors": [
        {
          "_id": "686c928b364e2ad167eb53f1",
          "name": "Yutian Chen",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f2",
          "name": "Shi Guo",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f3",
          "name": "Tianshuo Yang",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f4",
          "name": "Lihe Ding",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f5",
          "name": "Xiuyuan Yu",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f6",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f7",
          "name": "Tianfan Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T16:18:35.000Z",
      "submittedOnDailyAt": "2025-07-08T02:14:44.087Z",
      "title": "4DSloMo: Captura asincrónica de escenas rápidas en 4D configuración",
      "submittedOnDailyBy": {
        "_id": "64970d3d9c3b29dca8633f87",
        "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
        "isPro": false,
        "fullname": "JunhaoZhuang",
        "user": "JunhaoZhuang",
        "type": "user"
      },
      "summary": "La reconstrucción 4D de alta velocidad es crucial para el análisis de movimiento rápido y la reconstrucción 4D en tiempo real. Sin embargo, muchos sistemas de captura 4D están limitados a 30 FPS (frames por segundo) o menos, lo que puede llevar a resultados no deseables al intentar reconstruir movimientos rápidos directamente a partir de entradas de baja FPS. En este artículo, se propone un sistema de captura 4D de alta velocidad utilizando solo cámaras de baja FPS mediante el uso de nuevos módulos de captura y procesamiento. A nivel de captura, se propone una estrategia de captura asincrónica para aumentar la tasa de frames efectivamente, alineando el inicio de la cámara con el tiempo de muestra, lo que permite extender la base de 25 FPS a una tasa equivalente de 100-200 FPS. A nivel de procesamiento, se propone un nuevo modelo generativo para corregir los artefactos causados por la reconstrucción visual espacial 4D, y se entrena un modelo de artefacto corrección basado en la división de vídeo para el fin de reparar los defectos, mantener la coherencia temporal y mejorar la calidad de la reconstrucción en su conjunto. Los resultados experimentales demuestran que este método mejora significativamente la reconstrucción 4D de alta velocidad en comparación con la captura sincrónica.",
      "upvotes": 29,
      "discussionId": "686c928b364e2ad167eb53f8",
      "ai_summary": "A high-speed 4D capturing system using low FPS cameras with asynchronous capture and video-diffusion-based artifact correction enhances reconstruction quality.",
      "ai_keywords": [
        "asynchronous capture",
        "video-diffusion-based artifact-fix model",
        "sparse 4D reconstruction",
        "temporal consistency"
      ]
    },
    "publishedAt": "2025-07-07T12:18:35.000Z",
    "title": "4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous\n  Capture",
    "summary": "Reconstructing fast-dynamic scenes from multi-view videos is crucial for\nhigh-speed motion analysis and realistic 4D reconstruction. However, the\nmajority of 4D capture systems are limited to frame rates below 30 FPS (frames\nper second), and a direct 4D reconstruction of high-speed motion from low FPS\ninput may lead to undesirable results. In this work, we propose a high-speed 4D\ncapturing system only using low FPS cameras, through novel capturing and\nprocessing modules. On the capturing side, we propose an asynchronous capture\nscheme that increases the effective frame rate by staggering the start times of\ncameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our\nmethod achieves an equivalent frame rate of 100-200 FPS without requiring\nspecialized high-speed cameras. On processing side, we also propose a novel\ngenerative model to fix artifacts caused by 4D sparse-view reconstruction, as\nasynchrony reduces the number of viewpoints at each timestamp. Specifically, we\npropose to train a video-diffusion-based artifact-fix model for sparse 4D\nreconstruction, which refines missing details, maintains temporal consistency,\nand improves overall reconstruction quality. Experimental results demonstrate\nthat our method significantly enhances high-speed 4D reconstruction compared to\nsynchronous capture.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05163.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64970d3d9c3b29dca8633f87",
      "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
      "fullname": "JunhaoZhuang",
      "name": "JunhaoZhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04447",
      "authors": [
        {
          "_id": "686cab67364e2ad167eb5464",
          "name": "Wenyao Zhang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5465",
          "name": "Hongsi Liu",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5466",
          "user": {
            "_id": "63c3e8abc7d7f4c63a515a02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
            "isPro": false,
            "fullname": "Zekun Qi",
            "user": "qizekun",
            "type": "user"
          },
          "name": "Zekun Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:05:57.749Z",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5467",
          "name": "Yunnan Wang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5468",
          "name": "XinQiang Yu",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5469",
          "name": "Jiazhao Zhang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546a",
          "user": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "isPro": false,
            "fullname": "Runpei Dong",
            "user": "RunpeiDong",
            "type": "user"
          },
          "name": "Runpei Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:00.401Z",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546b",
          "name": "Jiawei He",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546c",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546d",
          "name": "Zhizheng Zhang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546e",
          "name": "Li Yi",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546f",
          "name": "Wenjun Zeng",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5470",
          "name": "Xin Jin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/Oe8hIH4_I_Pql9N72iHbn.mp4"
      ],
      "publishedAt": "2025-07-06T16:14:29.000Z",
      "submittedOnDailyAt": "2025-07-08T03:55:36.991Z",
      "title": "DREAM VLA: Modelo de Visión, Lenguaje y Acción que comprende ampliamente los conocimientos del mundo",
      "submittedOnDailyBy": {
        "_id": "6201fc5d91d53938a6432fbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
        "isPro": false,
        "fullname": "Runpei Dong",
        "user": "RunpeiDong",
        "type": "user"
      },
      "summary": "El desarrollo reciente de modelos de lenguaje visual y acción (VLA) ha demostrado notables logros en la generación de imágenes y la predicción de acciones, generalizando y inferiendo la manipulación de máquinas. Sin embargo, los métodos actuales están limitados por la necesidad de predecir información compleja y extensa en imágenes, y faltan conocimientos estructurales que incluyan información dinámica, espacial y significativa. Para resolver estos limitaciones, proponemos un nuevo marco de VLA que integra la predicción de conocimientos concretos del mundo. En particular, DreamVLA proporciona una representación comprimida y detallada en la planificación de acciones, combinando predicciones de conocimientos del mundo dinámico, espacial y significativo. Este diseño se alinea con la forma en que la humanidad interactúa con el mundo, formando una secuencia continua de razones abstractas de múltiples tipos para realizar acciones. Durante el entrenamiento, utilizamos una estructura de bloques de aténción para facilitar la integración indirecta de información dinámica, espacial y significativa, evitando la pérdida de información y separando claramente cada representación. Además, modelamos la distribución de condiciones para acciones futuras utilizando un transformador basado en difusión, separando la representación de acciones de características potenciales comunes. Experimentos de difusión en entornos reales y de simulación muestran que DreamVLA logra un éxito del 76.7% en tareas de robotización y un promedio de longitud de 4.44 en el benchmark CALVIN ABC-D.",
      "upvotes": 26,
      "discussionId": "686cab67364e2ad167eb5471",
      "projectPage": "https://zhangwenyao1.github.io/DreamVLA/",
      "githubRepo": "https://github.com/Zhangwenyao1/DreamVLA",
      "ai_summary": "DreamVLA improves robot manipulation through a VLA framework that incorporates world knowledge, dynamic-region guidance, and a diffusion-based transformer to ensure clear, disentangled representations for action planning.",
      "ai_keywords": [
        "vision-language-action",
        "dynamic-region-guided",
        "world knowledge prediction",
        "spatial and semantic cues",
        "block-wise structured attention",
        "diffusion-based transformer"
      ],
      "githubStars": 24
    },
    "publishedAt": "2025-07-06T12:14:29.000Z",
    "title": "DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive\n  World Knowledge",
    "summary": "Recent advances in vision-language-action (VLA) models have shown promise in\nintegrating image generation with action prediction to improve generalization\nand reasoning in robot manipulation. However, existing methods are limited to\nchallenging image-based forecasting, which suffers from redundant information\nand lacks comprehensive and critical world knowledge, including dynamic,\nspatial and semantic information. To address these limitations, we propose\nDreamVLA, a novel VLA framework that integrates comprehensive world knowledge\nforecasting to enable inverse dynamics modeling, thereby establishing a\nperception-prediction-action loop for manipulation tasks. Specifically,\nDreamVLA introduces a dynamic-region-guided world knowledge prediction,\nintegrated with the spatial and semantic cues, which provide compact yet\ncomprehensive representations for action planning. This design aligns with how\nhumans interact with the world by first forming abstract multimodal reasoning\nchains before acting. To mitigate interference among the dynamic, spatial and\nsemantic information during training, we adopt a block-wise structured\nattention mechanism that masks their mutual attention, preventing information\nleakage and keeping each representation clean and disentangled. Moreover, to\nmodel the conditional distribution over future actions, we employ a\ndiffusion-based transformer that disentangles action representations from\nshared latent features. Extensive experiments on both real-world and simulation\nenvironments demonstrate that DreamVLA achieves 76.7% success rate on real\nrobot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/Oe8hIH4_I_Pql9N72iHbn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04447.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6201fc5d91d53938a6432fbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
      "fullname": "Runpei Dong",
      "name": "RunpeiDong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05197",
      "authors": [
        {
          "_id": "686c7f78364e2ad167eb5354",
          "name": "Shihan Dou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5355",
          "name": "Shichun Liu",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5356",
          "user": {
            "_id": "655c6b1abfb531437a54c0e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/V8Md2mMX83hrSowKk6qMS.jpeg",
            "isPro": false,
            "fullname": "Yuming Yang",
            "user": "Umean",
            "type": "user"
          },
          "name": "Yuming Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:28.979Z",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5357",
          "name": "Yicheng Zou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5358",
          "name": "Yunhua Zhou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5359",
          "name": "Shuhao Xing",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535a",
          "name": "Chenhao Huang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535b",
          "name": "Qiming Ge",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535c",
          "name": "Demin Song",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535d",
          "name": "Haijun Lv",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535e",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535f",
          "name": "Chengqi Lv",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5360",
          "name": "Enyu Zhou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5361",
          "name": "Honglin Guo",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5362",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5363",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5364",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5365",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5366",
          "name": "Xipeng Qiu",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5367",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5368",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5369",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T16:56:31.000Z",
      "submittedOnDailyAt": "2025-07-08T02:18:45.573Z",
      "title": "La identificación de políticas es un modelo de recompensas generales.",
      "submittedOnDailyBy": {
        "_id": "6234238485575ce6ff1f169a",
        "avatarUrl": "/avatars/e5fff05f21cdea4e5aebc8ba426fac29.svg",
        "isPro": false,
        "fullname": "Yicheng Zou",
        "user": "RowitZou",
        "type": "user"
      },
      "summary": "Nosotros presentamos la modelación de recompensas como un formalizador de políticas para ofrecer una nueva perspectiva. Este método mide las diferencias entre dos políticas y genera señales de recompensa para guiar la política de entrenamiento hacia un objetivo de acciones deseadas. Basándonos en estas observaciones conceptuales, proponemos el Método de Pre-entrenamiento Escalable, el Policy Discriminative Learning (POLAR). Este método entrena un modelo de recompensa (RM) para distinguir entre una política y otra diferente. A diferencia de los métodos tradicionales de modelación de recompensas, POLAR identifica las diferencias relativas entre una política y cualquier política objetivo, permitiendo un modelado de objetivos de optimización escalable y de alto nivel. Utilizando el patrón de pre-entrenamiento de POLAR, proporcionamos modelos de RM con escalas de parámetros de 1.8B a 7B. Los resultados empíricos muestran una significativa mejora en el rendimiento comparado con métodos tradicionales no pre-entrenados. Por ejemplo, en el desafío STEM, la precisión de preferencia se incrementó del 54.8% al 81.0%, y en el desafío de escritura creativa, del 57.9% al 85.5%. POLAR muestra una capacidad de generalización robusta en la fine-tuning de aprendizaje por retroalimentación humana (RLHF). Esto proporciona señales de recompensa confiables, lo que significa un gran aumento en la eficacia de la política. Por ejemplo, LLaMa3.1-8B se mejoró del 47.36% al 56.33%, y Qwen2.5-32B del 64.49% al 70.47%. Además, los experimentos de extensión muestran una relación exponente clara entre el cálculo y el rendimiento. Los resultados sorprendentes, la fuerte generalización y las características de extensión indican que POLAR es una dirección prometedora para el desarrollo de potentes modelos de recompensa generales.",
      "upvotes": 24,
      "discussionId": "686c7f78364e2ad167eb536a",
      "githubRepo": "https://github.com/InternLM/POLAR",
      "ai_summary": "A scalable reward modeling method, Policy Discriminative Learning (POLAR), enhances reward model performance and generalizes robustly in reinforcement learning through policy comparison.",
      "ai_keywords": [
        "policy discriminator",
        "reward model",
        "Policy Discriminative Learning",
        "POLAR",
        "Reinforcement Fine-tuning"
      ],
      "githubStars": 35
    },
    "publishedAt": "2025-07-07T12:56:31.000Z",
    "title": "Pre-Trained Policy Discriminators are General Reward Models",
    "summary": "We offer a novel perspective on reward modeling by formulating it as a policy\ndiscriminator, which quantifies the difference between two policies to generate\na reward signal, guiding the training policy towards a target policy with\ndesired behaviors. Based on this conceptual insight, we propose a scalable\npre-training method named Policy Discriminative Learning (POLAR), which trains\na reward model (RM) to discern identical policies and discriminate different\nones. Unlike traditional reward modeling methods relying on absolute\npreferences, POLAR captures the relative difference between one policy and an\narbitrary target policy, which is a scalable, high-level optimization objective\nsuitable for modeling generic ranking relationships. Leveraging the POLAR\npre-training paradigm, we present a series of RMs with parameter scales from\n1.8B to 7B. Empirical results show that POLAR substantially outperforms\ntraditional non-pre-trained methods, significantly enhancing RM performance.\nFor instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on\nSTEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA\nbaselines. POLAR also shows robust generalization capabilities in RLHF using\nReinforcement Fine-tuning (RFT), providing reliable reward signals and markedly\nenhancing policy performance--improving LLaMa3.1-8B from an average of 47.36%\nto 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover,\nscaling experiments reveal a clear power-law relationship between computation\nand performance, supported by linear correlation coefficients approaching 0.99.\nThe impressive performance, strong generalization, and scaling properties\nsuggest that POLAR is a promising direction for developing general and strong\nreward models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05197.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6234238485575ce6ff1f169a",
      "avatarUrl": "/avatars/e5fff05f21cdea4e5aebc8ba426fac29.svg",
      "fullname": "Yicheng Zou",
      "name": "RowitZou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.00994",
      "authors": [
        {
          "_id": "6864e267d59a9eda59024bab",
          "user": {
            "_id": "65fa95405355a52c784633fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fa95405355a52c784633fc/rSfBUHPa7eSAsLd8DuOq4.png",
            "isPro": false,
            "fullname": "Hippolyte Gisserot-Boukhlef",
            "user": "hgissbkh",
            "type": "user"
          },
          "name": "Hippolyte Gisserot-Boukhlef",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:53:51.109Z",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bac",
          "user": {
            "_id": "62be186a5f59ff2320e6e32b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
            "isPro": false,
            "fullname": "Nicolas-BZRD",
            "user": "Nicolas-BZRD",
            "type": "user"
          },
          "name": "Nicolas Boizard",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T12:22:12.240Z",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bad",
          "name": "Manuel Faysse",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bae",
          "name": "Duarte M. Alves",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024baf",
          "name": "Emmanuel Malherbe",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bb0",
          "name": "André F. T. Martins",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bb1",
          "name": "Céline Hudelot",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bb2",
          "name": "Pierre Colombo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-01T17:45:48.000Z",
      "submittedOnDailyAt": "2025-07-08T07:51:12.578Z",
      "title": "¿Se entrena el encoder del modelo de lenguaje de máscara directamente?",
      "submittedOnDailyBy": {
        "_id": "62be186a5f59ff2320e6e32b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
        "isPro": false,
        "fullname": "Nicolas-BZRD",
        "user": "Nicolas-BZRD",
        "type": "user"
      },
      "summary": "La representación de texto de alta calidad tiene una importancia fundamental en una amplia variedad de tareas de NLP. El aprendizaje previo de un encoder basado en modelo de lenguaje oculto (MLM) se ha utilizado tradicionalmente, pero recientes evidencias han demostrado que un modelo de decodificador previamente entrenado por modelo de lenguaje causal (CLM) puede ser reutilizado como un encoder y que puede superar el rendimiento de los encoders tradicionales en las pruebas de representación de texto. Sin embargo, aún no se ha clarificado completamente por qué estos resultados se obtienen, ya sea debido a las ventajas inherentes del CLM, o a otros factores como el tamaño del modelo o la cantidad de datos. En este artículo, se abordaron estos problemas mediante experimentos de aprendizaje previo ajustado a gran escala, entrenando 30 modelos en un rango de 2.1 millón a 10 billones de parámetros y realizando 15,000 o más fine-tuning y evaluaciones. Se observó que entrenar un modelo con MLM generalmente muestra un mejor rendimiento en tareas de representación de texto en general, mientras que los modelos entrenados con CLM son más eficientes con el datos y mejoran la estabilidad del fine-tuning. Basándonos en estas observaciones, se presentó una estrategia de entrenamiento en dos etapas para alcanzar el mejor rendimiento, utilizando un cálculo fijo de entrenamiento. Además, esta estrategia demuestra que se puede inicializar un modelo de decodificador previamente entrenado en un ecosistema de LLMs y reducir la carga de entrenamiento de los modelos más recientes de encoder, operando de manera más eficiente. Se publicaron todos los artefactos del proyecto para fomentar la investigación.",
      "upvotes": 23,
      "discussionId": "6864e267d59a9eda59024bb3",
      "githubRepo": "https://github.com/Nicolas-BZRD/EuroBERT",
      "githubStars": 59
    },
    "publishedAt": "2025-07-01T13:45:48.000Z",
    "title": "Should We Still Pretrain Encoders with Masked Language Modeling?",
    "summary": "Learning high-quality text representations is fundamental to a wide range of\nNLP tasks. While encoder pretraining has traditionally relied on Masked\nLanguage Modeling (MLM), recent evidence suggests that decoder models\npretrained with Causal Language Modeling (CLM) can be effectively repurposed as\nencoders, often surpassing traditional encoders on text representation\nbenchmarks. However, it remains unclear whether these gains reflect an inherent\nadvantage of the CLM objective or arise from confounding factors such as model\nand data scale. In this paper, we address this question through a series of\nlarge-scale, carefully controlled pretraining ablations, training a total of 30\nmodels ranging from 210 million to 1 billion parameters, and conducting over\n15,000 fine-tuning and evaluation runs. We find that while training with MLM\ngenerally yields better performance across text representation tasks,\nCLM-trained models are more data-efficient and demonstrate improved fine-tuning\nstability. Building on these findings, we experimentally show that a biphasic\ntraining strategy that sequentially applies CLM and then MLM, achieves optimal\nperformance under a fixed computational training budget. Moreover, we\ndemonstrate that this strategy becomes more appealing when initializing from\nreadily available pretrained CLM models (from the existing LLM ecosystem),\nreducing the computational burden needed to train best-in-class encoder models.\nWe release all project artifacts at https://hf.co/MLMvsCLM to foster further\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00994.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "62be186a5f59ff2320e6e32b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
      "fullname": "Nicolas-BZRD",
      "name": "Nicolas-BZRD",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03483",
      "authors": [
        {
          "_id": "686c90d4364e2ad167eb53d8",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53d9",
          "name": "Guanyu Li",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53da",
          "name": "Yutao Fan",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53db",
          "name": "Honglin Guo",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53dc",
          "name": "Yufang Liu",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53dd",
          "name": "Xiaoran Fan",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53de",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53df",
          "name": "Jingchao Ding",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e0",
          "name": "Wangmeng Zuo",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e1",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e2",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e3",
          "name": "Tao Ji",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e4",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e5",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e6",
          "name": "Xuanjing Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T11:20:09.000Z",
      "submittedOnDailyAt": "2025-07-08T02:00:41.875Z",
      "title": "BMMR: Razones de la gran escena de Broadway de Billy Joel y daski\nDataset",
      "submittedOnDailyBy": {
        "_id": "638ef0b0c67af472d31674a6",
        "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg",
        "isPro": false,
        "fullname": "Honglin Guo",
        "user": "KYLN24",
        "type": "user"
      },
      "summary": "Este artículo presenta BMMR, un conjunto de datos de gran escala que incluye bibliométricas, modelos múltiples y teoría de la multiculturalidad. Este conjunto de datos puede ser utilizado por la comunidad para el desarrollo y evaluación de modelos múltiples (LMMs) a gran escala. BMMR consta de 110,000 preguntas de nivel universitario, que extienden 300 temas definidos por UNESCO, y se presenta en diferentes formatos (múltiples de elección, respuestas en blanco, preguntas abiertas), tomadas de materiales impresos y digitales como libros, exámenes y quizzes. Todo el conjunto de datos ha sido curado mediante un marco escalable que involucra la participación humana, y cada instancia tiene asociada una buena razón. El conjunto de datos se divide en dos secciones: BMMR-Eval y BMMR-Train. BMMR-Eval incluye 20,458 instancias de alta calidad y permite evaluar la teoría de la multiculturalidad y las razones en chino y inglés. BMMR-Train se centra en la razón actual del matemáticas y apoya la investigación y desarrollo en diferentes disciplinas y disciplinarias. Además, se propone un instrumento de evaluación basado en procesos para la experiencia multicultural, el BMMR-Verifier, que realiza una evaluación precisa de los pasos de razón. Los experimentos de escala en 24 modelos revelan que: (i) los modelos de estado de la arte (por ejemplo, o3 y Gemini-2.5-Pro) tienen mucho potencial de mejora en BMMR-Eval; (ii) los modelos de razón muestran sesgos disciplinarios y superan a los LMMs en ciertos temas; (iii) los modelos de código abierto superan a los modelos propietarios; (iv) la fine-tuning en BMMR-Train reduce estos errores. Además, mediante estudios detallados con BMMR-Verifier, se han identificado los problemas actuales de los LMMs en la teoría de la multiculturalidad. Se publica el conjunto de datos para beneficiar a la comunidad.",
      "upvotes": 19,
      "discussionId": "686c90d4364e2ad167eb53e7",
      "projectPage": "https://bmmr.pages.dev/",
      "githubRepo": "https://github.com/woooodyy/BMMR",
      "ai_summary": "A large-scale dataset and verification tool are introduced for assessing and improving cross-disciplinary reasoning capabilities in multimodal models.",
      "ai_keywords": [
        "bilingual",
        "multimodal",
        "multi-disciplinary",
        "large multimodal models",
        "LMMs",
        "UNESCO-defined subjects",
        "multiple-choice",
        "fill-in-the-blank",
        "open-ended QA",
        "human-in-the-loop",
        "scalable framework",
        "high-quality reasoning path",
        "multidisciplinary reasoning",
        "BMMR-Verifier",
        "reasoning models",
        "discipline bias",
        "reasoning-chain analysis"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-07-04T07:20:09.000Z",
    "title": "BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning\n  Dataset",
    "summary": "In this paper, we introduce BMMR, a large-scale bilingual, multimodal,\nmulti-disciplinary reasoning dataset for the community to develop and evaluate\nlarge multimodal models (LMMs). BMMR comprises 110k college-level questions\nspanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice,\nfill-in-the-blank, and open-ended QA-and sourced from both print and digital\nmedia such as books, exams, and quizzes. All data are curated and filtered via\na human-in-the-loop and scalable framework, and each instance is paired with a\nhigh-quality reasoning path. The dataset is organized into two parts: BMMR-Eval\nthat comprises 20,458 high-quality instances to comprehensively assess LMMs'\nknowledge and reasoning across multiple disciplines in both Chinese and\nEnglish; and BMMR-Train that contains 88,991 instances to support further\nresearch and development, extending the current focus on mathematical reasoning\nto diverse disciplines and domains. In addition, we propose the process-based\nmulti-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained\nevaluation of reasoning paths. Extensive experiments on 24 models reveal that\n(i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom\non BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs\nonly on specific subjects; (iii) open-source models still trail their\nproprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap.\nAdditionally, we conduct reasoning-chain analyses using BMMR-Verifier and other\nin-depth studies, uncovering the challenges LMMs currently face in\nmultidisciplinary reasoning. We will release the data, and we hope our work can\noffer insights and contributions to the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03483.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638ef0b0c67af472d31674a6",
      "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg",
      "fullname": "Honglin Guo",
      "name": "KYLN24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02029",
      "authors": [
        {
          "_id": "68679569213f123a1f88b87c",
          "name": "BAAI RoboBrain Team",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b87d",
          "user": {
            "_id": "668fa476cbcaf7ab0e4c58b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668fa476cbcaf7ab0e4c58b3/F5Jj-nPCjU6uxZyfkY3qw.jpeg",
            "isPro": false,
            "fullname": "Mingyu Cao",
            "user": "cmyopu",
            "type": "user"
          },
          "name": "Mingyu Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-07T11:24:50.715Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b87e",
          "name": "Huajie Tan",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b87f",
          "user": {
            "_id": "668f5478b3991ac0c3fc9c2f",
            "avatarUrl": "/avatars/a775853d3b88e7b1c8494ca837b5495c.svg",
            "isPro": false,
            "fullname": "yuhengji",
            "user": "yuheng2000",
            "type": "user"
          },
          "name": "Yuheng Ji",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:53:03.017Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b880",
          "user": {
            "_id": "67e406e64b80e9b39e2a85d6",
            "avatarUrl": "/avatars/a021be64341e5d7a079858916fa34c28.svg",
            "isPro": false,
            "fullname": "MinglanLin",
            "user": "MinglanLin",
            "type": "user"
          },
          "name": "Minglan Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:52:59.066Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b881",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b882",
          "user": {
            "_id": "66aadfc1344279e0243d4569",
            "avatarUrl": "/avatars/d2afbceb2e6279e42ed9ad98dffa7f0a.svg",
            "isPro": false,
            "fullname": "Caozhou",
            "user": "Caozhou1995",
            "type": "user"
          },
          "name": "Zhou Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:48.313Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b883",
          "name": "Pengwei Wang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b884",
          "user": {
            "_id": "63f08dc79cf89c9ed1bb89cd",
            "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
            "isPro": false,
            "fullname": "Zhoues",
            "user": "Zhoues",
            "type": "user"
          },
          "name": "Enshen Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:52:57.069Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b885",
          "name": "Yi Han",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b886",
          "name": "Yingbo Tang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b887",
          "name": "Xiangqi Xu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b888",
          "name": "Wei Guo",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b889",
          "name": "Yaoxu Lyu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88a",
          "name": "Yijie Xu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88b",
          "name": "Jiayu Shi",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88c",
          "user": {
            "_id": "650bf938677f9e45963d672e",
            "avatarUrl": "/avatars/7d4159067b5005a3a635e36b26b7b239.svg",
            "isPro": false,
            "fullname": "Cheng Chi",
            "user": "ChuckChi",
            "type": "user"
          },
          "name": "Cheng Chi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:53:01.035Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88d",
          "name": "Mengdi Zhao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88e",
          "name": "Xiaoshuai Hao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88f",
          "name": "Shanyu Rong",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b890",
          "name": "Zhengliang Cai",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b891",
          "name": "Bolun Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b892",
          "name": "Shuyi Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b893",
          "name": "Huaihai Lyu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b894",
          "name": "Mengfei Du",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b895",
          "name": "Lingfeng Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b896",
          "name": "Xi Feng",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b897",
          "name": "Xiaodan Liu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b898",
          "name": "Yance Jiao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b899",
          "name": "Chenrui He",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89a",
          "user": {
            "_id": "63157214362e3e95ea553db5",
            "avatarUrl": "/avatars/a421c25128c71be6d0c92490cebbbccc.svg",
            "isPro": false,
            "fullname": "lyu",
            "user": "ceci3",
            "type": "user"
          },
          "name": "Mengsi Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:50.528Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89b",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89c",
          "name": "Yulong Ao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89d",
          "name": "Xue Sun",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89e",
          "name": "Zheqi He",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89f",
          "name": "Jingshu Zheng",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a0",
          "name": "Xi Yang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a1",
          "name": "Donghai Shi",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a2",
          "name": "Kunchang Xie",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a3",
          "name": "Bochao Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a4",
          "name": "Shaokai Nie",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a5",
          "name": "Chunlei Men",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a6",
          "name": "Yonghua Lin",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a7",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a8",
          "name": "Tiejun Huang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a9",
          "name": "Shanghang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:05:33.000Z",
      "submittedOnDailyAt": "2025-07-08T06:32:16.956Z",
      "title": "RoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRoboBrain 2.0 Informe Técnico\n\nRobo",
      "submittedOnDailyBy": {
        "_id": "63a369d98c0c89dcae3b8329",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
        "isPro": true,
        "fullname": "Adina Yakefu",
        "user": "AdinaY",
        "type": "user"
      },
      "summary": "RoboBrain 2.0, presentamos nuestro último generación de lenguaje de visión basado en modelos. Este modelo integra la reconocimiento, lógica y planificación de tareas complejas en entornos físicos. Preparamos dos versiones: el modelo 7B y el 32B. Se caracteriza por una arquitectura estructurada limitada, compuesta por un encoder de visión y un modelo de lenguaje. A pesar de su pequeño tamaño, RoboBrain 2.0 muestra un excelente rendimiento en una amplia gama de tareas de visión lógica. La versión 32B también ha alcanzado resultados líderes en los marcos de referencia espaciales y temporales, superando tanto modelos abiertos como propietarios. En particular, apoya la comprensión espacial (por ejemplo, predicción de funciones, gasto espacial, predicción de rutas) y la determinación temporal (por ejemplo, interacción en ciclos cerrados, planificación de agentes múltiples y actualización de grafos de escenas). En este informe, detallamos la arquitectura del modelo, la construcción de datos, la estrategia de entrenamiento multi-etapa y aplicaciones prácticas. Esperamos que RoboBrain 2.0 fomente la investigación en AI de visión y sea un paso práctico para la construcción de agentes visuales generales. Los códigos, puntos de chequeo y marcos de referencia están disponibles en https://superrobobrain.github.io.",
      "upvotes": 14,
      "discussionId": "68679569213f123a1f88b8aa",
      "ai_summary": "RoboBrain 2.0, a vision-language foundation model, achieves top performance in embodied tasks through its heterogeneous architecture and multi-stage training strategies.",
      "ai_keywords": [
        "embodied vision-language",
        "embodied reason",
        "vision encoder",
        "spatial understanding",
        "affordance prediction",
        "spatial referring",
        "trajectory forecasting",
        "temporal decision-making",
        "closed-loop interaction",
        "multi-agent long-horizon planning",
        "scene graph updating"
      ]
    },
    "publishedAt": "2025-07-02T13:05:33.000Z",
    "title": "RoboBrain 2.0 Technical Report",
    "summary": "We introduce RoboBrain 2.0, our latest generation of embodied vision-language\nfoundation models, designed to unify perception, reasoning, and planning for\ncomplex embodied tasks in physical environments. It comes in two variants: a\nlightweight 7B model and a full-scale 32B model, featuring a heterogeneous\narchitecture with a vision encoder and a language model. Despite its compact\nsize, RoboBrain 2.0 achieves strong performance across a wide spectrum of\nembodied reasoning tasks. On both spatial and temporal benchmarks, the 32B\nvariant achieves leading results, surpassing prior open-source and proprietary\nmodels. In particular, it supports key real-world embodied AI capabilities,\nincluding spatial understanding (e.g., affordance prediction, spatial\nreferring, trajectory forecasting) and temporal decision-making (e.g.,\nclosed-loop interaction, multi-agent long-horizon planning, and scene graph\nupdating). This report details the model architecture, data construction,\nmulti-stage training strategies, infrastructure and practical applications. We\nhope RoboBrain 2.0 advances embodied AI research and serves as a practical step\ntoward building generalist embodied agents. The code, checkpoint and benchmark\nare available at https://superrobobrain.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02029.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 857
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04009",
      "authors": [
        {
          "_id": "686cd234cc230c60b4100aec",
          "name": "Ziyang Miao",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100aed",
          "name": "Qiyu Sun",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100aee",
          "name": "Jingyuan Wang",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100aef",
          "user": {
            "_id": "66a48a77f9565635ebc33a87",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a48a77f9565635ebc33a87/WW8BFd1D9xZbGIPZPfdHk.png",
            "isPro": false,
            "fullname": "GYC",
            "user": "oGYCo",
            "type": "user"
          },
          "name": "Yuchen Gong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:42.785Z",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100af0",
          "user": {
            "_id": "642fef28a043f0ac7defa8a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
            "isPro": false,
            "fullname": "Yaowei Zheng",
            "user": "hiyouga",
            "type": "user"
          },
          "name": "Yaowei Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:39:35.178Z",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100af1",
          "name": "Shiqi Li",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100af2",
          "name": "Richong Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/Ztx5877vKyqqZINgge_1B.mp4"
      ],
      "publishedAt": "2025-07-05T11:38:59.000Z",
      "submittedOnDailyAt": "2025-07-08T06:40:18.914Z",
      "title": "Easy Dataset: Usando el marco de trabajo de unidades y un marco de trabajo expandible, sintético de datos de fine-tuning de LLM a partir de documentos no estructurados",
      "submittedOnDailyBy": {
        "_id": "642fef28a043f0ac7defa8a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
        "isPro": false,
        "fullname": "Yaowei Zheng",
        "user": "hiyouga",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) muestran un desempeño impresionante en tareas generales, pero su aplicación en áreas específicas se ve limitada por la escasez de datos de alta calidad. Los herramientas de síntesis de datos actuales enfrentan dificultades para extraer datos de fine-tuning confiables, lo que ha llevado a la propuesta del marco de trabajo Easy Dataset. Este marco permite la síntesis de datos de fine-tuning a partir de documentos no estructurados a través de una interfaz gráfica de usuario intuitiva (GUI). En particular, Easy Dataset permite a los usuarios configurar fácilmente modelos de extracción de texto y etapas de craqueo, transformando documentos simples en chats de craqueo columnados. Posteriormente, se utilizan los LLMs publicados y se aplica un enfoque de prompting poterado para generar pares de preguntas y respuestas diversos. En todo este flujo, la persona puede revisar y mejorar los resultados intermedios mediante una interfaz de visualización dentro del bucle, asegurando la calidad de los datos. Las experimentaciones en tareas de preguntas y respuestas en tareas financieras muestran que el ajuste de los LLMs con datos sintéticos mejora significativamente el rendimiento específico mientras mantiene el conocimiento general. El código fuente y las bibliotecas instalables están disponibles en https://github.com/ConardLi/easy-dataset, y este proyecto ha alcanzado más de 9,000 puntos de calificación en GitHub.",
      "upvotes": 12,
      "discussionId": "686cd234cc230c60b4100af3",
      "githubRepo": "https://github.com/ConardLi/easy-dataset",
      "ai_summary": "A unified framework called Easy Dataset synthesizes fine-tuning data from unstructured documents using a GUI and LLMs, improving domain-specific performance of LLMs while maintaining general knowledge.",
      "ai_keywords": [
        "Large language models",
        "fine-tuning",
        "unstructured documents",
        "graphical user interface",
        "text extraction models",
        "chunking strategies",
        "persona-driven prompting",
        "human-in-the-loop",
        "financial question-answering task"
      ],
      "githubStars": 9180
    },
    "publishedAt": "2025-07-05T07:38:59.000Z",
    "title": "Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM\n  Fine-Tuning Data from Unstructured Documents",
    "summary": "Large language models (LLMs) have shown impressive performance on\ngeneral-purpose tasks, yet adapting them to specific domains remains\nchallenging due to the scarcity of high-quality domain data. Existing data\nsynthesis tools often struggle to extract reliable fine-tuning data from\nheterogeneous documents effectively. To address this limitation, we propose\nEasy Dataset, a unified framework for synthesizing fine-tuning data from\nunstructured documents via an intuitive graphical user interface (GUI).\nSpecifically, Easy Dataset allows users to easily configure text extraction\nmodels and chunking strategies to transform raw documents into coherent text\nchunks. It then leverages a persona-driven prompting approach to generate\ndiverse question-answer pairs using public-available LLMs. Throughout the\npipeline, a human-in-the-loop visual interface facilitates the review and\nrefinement of intermediate outputs to ensure data quality. Experiments on a\nfinancial question-answering task show that fine-tuning LLMs on the synthesized\ndataset significantly improves domain-specific performance while preserving\ngeneral knowledge. The source code and installable package are available at\nhttps://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub\nstars.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/Ztx5877vKyqqZINgge_1B.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642fef28a043f0ac7defa8a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
      "fullname": "Yaowei Zheng",
      "name": "hiyouga",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2185
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03253",
      "authors": [
        {
          "_id": "686c86ff364e2ad167eb53a8",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53a9",
          "name": "Shenghua Liu",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53aa",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ab",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ac",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ad",
          "name": "Yiwei Wang",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ae",
          "user": {
            "_id": "63120517ae8896941da4c5da",
            "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
            "isPro": false,
            "fullname": "Lingrui Mei",
            "user": "Chevalier",
            "type": "user"
          },
          "name": "Lingrui Mei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:44.909Z",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53af",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53b0",
          "name": "Jiafeng Guo",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53b1",
          "name": "Xueqi Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T02:19:58.000Z",
      "submittedOnDailyAt": "2025-07-08T01:53:47.804Z",
      "title": "RefineX: Mejora del entrenamiento de datos preparados en programas de aprendizaje expertoide recibidos según la escala",
      "submittedOnDailyBy": {
        "_id": "642577e06d0f0f5f1dc68904",
        "avatarUrl": "/avatars/3df7cfcf9ca6cd9c9e9b10a73f4efc35.svg",
        "isPro": false,
        "fullname": "Bibaolong",
        "user": "Bibaolong",
        "type": "user"
      },
      "summary": "Los grandes modelos de lenguaje (LLMs) tienen una capacidad fundamental que está profundamente influenciada por la calidad de los datos previos. Sin embargo, mejorar la calidad de los datos a través de la escala es un gran desafío, principalmente debido al equilibrio entre el efecto de la precisión y la eficiencia de la procesamiento. El filtrado basado en reglas es el paradigma principal, pero generalmente funciona a nivel de documento y carece de la precisión necesaria para mejorar específicamente contenidos. Basándonos en investigaciones emergentes como ProX, proponemos un nuevo marco llamado RefineX. RefineX realiza la precisión de grandes cantidades de datos previos externos a través de tareas de edición de lenguaje programado. RefineX permite una precisión eficiente de datos, mientras que mantiene la diversidad y naturaleza del texto original con confianza. Uno de los puntos fuertes de RefineX es que puede aceptar resultados de precisión de alta calidad con un mínimo de ediciones. Este flujo de trabajo de integración de resultados de alta calidad permite mejorar sistemáticamente todas las instancias del corpus a través de la escala, entrenando modelos de precisión eficientes y confiables. RefineX evalua datos previos desde el principio, muestra un desempeño superior consistente en muchas modelos de tamaño y tareas posteriores, y registra un aumento promedio del 2.6% a 7.2% en la tarea lighteval en un modelo de 750M, al reducir significativamente los tokens de entrenamiento. Un análisis más profundo muestra que RefineX mejora confiablemente la calidad del texto, es más efectivo que los métodos existentes, y supera a Prox-C en la generación final de texto, demostrando que RefineX es una solución escalable, efectiva y confiable para la optimización de datos previos en la actualización de LLMs.",
      "upvotes": 12,
      "discussionId": "686c86ff364e2ad167eb53b2",
      "ai_summary": "RefineX is a scalable framework for improving the quality of large language model pre-training data through programmatic editing, yielding better performance than alternative methods across various downstream tasks.",
      "ai_keywords": [
        "large language models",
        "pre-training corpora",
        "rule-based filtering",
        "document level",
        "granular refinement",
        "programmatic editing",
        "refine model",
        "data refinement",
        "text quality",
        "end-to-end generation",
        "lighteval tasks"
      ]
    },
    "publishedAt": "2025-07-03T22:19:58.000Z",
    "title": "RefineX: Learning to Refine Pre-training Data at Scale from\n  Expert-Guided Programs",
    "summary": "The foundational capabilities of large language models (LLMs) are deeply\ninfluenced by the quality of their pre-training corpora. However, enhancing\ndata quality at scale remains a significant challenge, primarily due to the\ntrade-off between refinement effectiveness and processing efficiency. While\nrule-based filtering remains the dominant paradigm, it typically operates at\nthe document level and lacks the granularity needed to refine specific content\nwithin documents. Inspired by emerging work such as ProX, we propose\nRefineX, a novel framework for large-scale, surgical refinement of\npre-training data through programmatic editing tasks. RefineX enables efficient\nand fine-grained data refinement while reliably preserving the diversity and\nnaturalness of raw text. The core strength of RefineX lies in distilling\nhigh-quality, expert-guided end-to-end refinement results into minimal\nedit-based deletion programs. This high-precision distillation pipeline is used\nto train an efficient and reliable refine model that can systematically improve\nevery instance in the corpus at scale. We evaluate RefineX across from-scratch\npre-training at multiple model scales and find that it consistently outperforms\nmodels trained on raw, filtered, or alternatively refined data across diverse\ndownstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on\nlighteval tasks, and achieves comparable performance using significantly fewer\ntraining tokens. Further analysis shows that RefineX reliably enhances text\nquality with both high efficiency and precision, outperforming prior approaches\nsuch as end-to-end generation and Prox-C. These results position RefineX as a\nscalable, effective, and reliable solution for optimizing pre-training data in\nmodern LLM pipelines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642577e06d0f0f5f1dc68904",
      "avatarUrl": "/avatars/3df7cfcf9ca6cd9c9e9b10a73f4efc35.svg",
      "fullname": "Bibaolong",
      "name": "Bibaolong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05108",
      "authors": [
        {
          "_id": "686cc8bc364e2ad167eb54e3",
          "name": "Yuyi Zhang",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e4",
          "name": "Peirong Zhang",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e5",
          "name": "Zhenhua Yang",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e6",
          "name": "Pengyu Yan",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e7",
          "name": "Yongxin Shi",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e8",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e9",
          "name": "Fengjun Guo",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54ea",
          "name": "Lianwen Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T15:26:17.000Z",
      "submittedOnDailyAt": "2025-07-08T06:01:26.616Z",
      "title": "Cultura Patrimonio: Nuevas Metodologías de Representación Crítica de Documentos Históricos",
      "submittedOnDailyBy": {
        "_id": "65fba5700b78c48c9e393a3e",
        "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
        "isPro": false,
        "fullname": "Yuyi Zhang",
        "user": "ZZXF",
        "type": "user"
      },
      "summary": "Los documentos históricos son valiosos patrimonios culturales que se enfrentan a deterioros graves como daños, corrosión, oxidación y erosión debido a el paso del tiempo. La técnica actual de recreación (HDR) se centra principalmente en modelos únicos o de tamaño limitado, lo que no satisface las necesidades prácticas. Para resolver este problema, se proponen FPHDR (Conjunto de Datos HDR de Página Completa) y AutoHDR (Solución Automática HDR). FPHDR incluye 1,633 imágenes de imagen real y 6,543 imágenes sintéticas, además de información sobre la posición de nivel de letra y fila, y descripciones de la gravedad del daño. AutoHDR modela el flujo de trabajo de los historiadores y utiliza un enfoque de tres etapas: detección de posiciones dañadas mediante OCR, predicción de texto en contexto de lenguaje visual y recreación automática de la apariencia de los bordes con patches. La arquitectura modular de AutoHDR permite colaboración entre humanos y máquinas, con intervención flexible y optimización en cada etapa de recreación. Los experimentos muestran un desempeño notable de AutoHDR, con un aumento de la precisión de OCR del 46.83% al 84.05% en documentos con graves daños, y un rendimiento de 94.25% al combinar colaboración humano-máquina. Esta investigación representa un importante avance en la automatización de la recreación de documentos históricos y contribuye significativamente a la conservación de los patrimonios culturales. El modelo y el conjunto de datos están disponibles en https://github.com/SCUT-DLVCLab/AutoHDR.",
      "upvotes": 7,
      "discussionId": "686cc8bd364e2ad167eb54eb",
      "githubRepo": "https://github.com/SCUT-DLVCLab/AutoHDR",
      "githubStars": 22
    },
    "publishedAt": "2025-07-07T11:26:17.000Z",
    "title": "Reviving Cultural Heritage: A Novel Approach for Comprehensive\n  Historical Document Restoration",
    "summary": "Historical documents represent an invaluable cultural heritage, yet have\nundergone significant degradation over time through tears, water erosion, and\noxidation. Existing Historical Document Restoration (HDR) methods primarily\nfocus on single modality or limited-size restoration, failing to meet practical\nneeds. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel\nautomated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and\n6,543 synthetic images with character-level and line-level locations, as well\nas character annotations in different damage grades. AutoHDR mimics historians'\nrestoration workflows through a three-stage approach: OCR-assisted damage\nlocalization, vision-language context text prediction, and patch autoregressive\nappearance restoration. The modular architecture of AutoHDR enables seamless\nhuman-machine collaboration, allowing for flexible intervention and\noptimization at each restoration stage. Experiments demonstrate AutoHDR's\nremarkable performance in HDR. When processing severely damaged documents, our\nmethod improves OCR accuracy from 46.83\\% to 84.05\\%, with further enhancement\nto 94.25\\% through human-machine collaboration. We believe this work represents\na significant advancement in automated historical document restoration and\ncontributes substantially to cultural heritage preservation. The model and\ndataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05108.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fba5700b78c48c9e393a3e",
      "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
      "fullname": "Yuyi Zhang",
      "name": "ZZXF",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.03745",
      "authors": [
        {
          "_id": "686ca04e364e2ad167eb543c",
          "user": {
            "_id": "63fedca388b9695964c33ad8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fedca388b9695964c33ad8/XFE8Tm9yRRbo6XdDw7c2O.png",
            "isPro": false,
            "fullname": "Aki",
            "user": "AkiCumulo",
            "type": "user"
          },
          "name": "Akio Kodaira",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:07.401Z",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb543d",
          "name": "Tingbo Hou",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb543e",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb543f",
          "name": "Masayoshi Tomizuka",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb5440",
          "name": "Yue Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/eFed7d-PiKdUBGIPDkIoo.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/50KoaI5mgYLe_IvIOaVJh.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/zWzNfOJX8C-CNICabQyHV.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/m072g2U7H133AyOGShNZ0.mp4"
      ],
      "publishedAt": "2025-07-04T18:00:01.000Z",
      "submittedOnDailyAt": "2025-07-08T04:21:42.132Z",
      "title": "StreamDiT: Generación de video en tiempo real con texto de mapeo",
      "submittedOnDailyBy": {
        "_id": "63fedca388b9695964c33ad8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fedca388b9695964c33ad8/XFE8Tm9yRRbo6XdDw7c2O.png",
        "isPro": false,
        "fullname": "Aki",
        "user": "AkiCumulo",
        "type": "user"
      },
      "summary": "Recientemente, se ha logrado un gran avance en la generación de videos de alta calidad a través de la expansión de modelos basados en transformers a cientos de millones de parámetros. Sin embargo, actualmente estos modelos solo generan pequeñas secuencias en línea, limitando su uso en aplicaciones interactivas o en tiempo real. En este artículo, se propone StreamDiT, un modelo de generación de vídeos en tiempo real, para resolver estos problemas. El entrenamiento de StreamDiT se basa en el flow matching con la adición de un buffer de movimiento. Se diseña una entrenamiento mixto en diferentes secciones de la frase del buffer, mejorando tanto la consistencia del contenido como la calidad visual. El modelado de StreamDiT se basa en adaLN DiT utilizando codificación temporal y atención de ventanas. Para practicar la propuesta, se entrena un modelo StreamDiT de 4B parámetros. Además, se propone un método de distillation multistep adaptado para StreamDiT. Se realiza la distancia de muestreo en cada sección de las secuencias de frames seleccionadas, y después, la cantidad total de evaluaciones de función (NFEs) disminuye en función del número de bloques del buffer. Finalmente, nuestro modelo implementa un funcionamiento en tiempo real entre 1 y 16 FPS, permitiendo la generación de vídeos en tiempo real en resolución 512p. Nuestro método se evalúa mediante métricas estadísticas y evaluaciones humanas. Nuestro modelo permite la generación de flujos, la generación interactiva y la aplicación en tiempo real de vídeos como vídeos. Se ofrecen resultados y ejemplos en el sitio web del proyecto: <a href=\"https://cumulo-autumn.github.io/StreamDiT/\">this https URL.</a>",
      "upvotes": 7,
      "discussionId": "686ca04f364e2ad167eb5441",
      "projectPage": "https://cumulo-autumn.github.io/StreamDiT/",
      "ai_summary": "A streaming video generation model named StreamDiT, based on transformer-based diffusion models, enables real-time video generation with high content consistency and visual quality.",
      "ai_keywords": [
        "transformer-based diffusion models",
        "StreamDiT",
        "flow matching",
        "moving buffer",
        "mixed training",
        "adaLN DiT",
        "varying time embedding",
        "window attention",
        "multistep distillation",
        "sampling distillation",
        "function evaluations",
        "real-time performance",
        "streaming generation",
        "interactive generation",
        "video-to-video"
      ]
    },
    "publishedAt": "2025-07-04T14:00:01.000Z",
    "title": "StreamDiT: Real-Time Streaming Text-to-Video Generation",
    "summary": "Recently, great progress has been achieved in text-to-video (T2V) generation\nby scaling transformer-based diffusion models to billions of parameters, which\ncan generate high-quality videos. However, existing models typically produce\nonly short clips offline, restricting their use cases in interactive and\nreal-time applications. This paper addresses these challenges by proposing\nStreamDiT, a streaming video generation model. StreamDiT training is based on\nflow matching by adding a moving buffer. We design mixed training with\ndifferent partitioning schemes of buffered frames to boost both content\nconsistency and visual quality. StreamDiT modeling is based on adaLN DiT with\nvarying time embedding and window attention. To practice the proposed method,\nwe train a StreamDiT model with 4B parameters. In addition, we propose a\nmultistep distillation method tailored for StreamDiT. Sampling distillation is\nperformed in each segment of a chosen partitioning scheme. After distillation,\nthe total number of function evaluations (NFEs) is reduced to the number of\nchunks in a buffer. Finally, our distilled model reaches real-time performance\nat 16 FPS on one GPU, which can generate video streams at 512p resolution. We\nevaluate our method through both quantitative metrics and human evaluation. Our\nmodel enables real-time applications, e.g. streaming generation, interactive\ngeneration, and video-to-video. We provide video results and more examples in\nour project website: <a href=\"https://cumulo-autumn.github.io/StreamDiT/\">this\nhttps URL.</a>",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/eFed7d-PiKdUBGIPDkIoo.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/50KoaI5mgYLe_IvIOaVJh.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/zWzNfOJX8C-CNICabQyHV.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/m072g2U7H133AyOGShNZ0.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fedca388b9695964c33ad8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fedca388b9695964c33ad8/XFE8Tm9yRRbo6XdDw7c2O.png",
      "fullname": "Aki",
      "name": "AkiCumulo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.04952",
      "authors": [
        {
          "_id": "686c8487364e2ad167eb5386",
          "name": "Chenchen Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5387",
          "name": "Yuhang Li",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5388",
          "name": "Can Xu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5389",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538a",
          "name": "Ao Liu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538b",
          "name": "Shihui Hu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538c",
          "name": "Dengpeng Wu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538d",
          "name": "Guanhua Huang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538e",
          "name": "Kejiao Li",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538f",
          "name": "Qi Yi",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5390",
          "name": "Ruibin Xiong",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5391",
          "name": "Haotian Zhu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5392",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5393",
          "name": "Yuhao Jiang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5394",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5395",
          "name": "Zenan Xu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5396",
          "name": "Bohui Zhai",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5397",
          "name": "Guoxiang He",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5398",
          "name": "Hebin Li",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5399",
          "name": "Jie Zhao",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539a",
          "name": "Le Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539b",
          "name": "Lingyun Tan",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539c",
          "name": "Pengyu Guo",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539d",
          "name": "Xianshu Pang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539e",
          "name": "Yang Ruan",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539f",
          "name": "Zhifeng Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a0",
          "name": "Zhonghu Wang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a1",
          "name": "Ziyan Xu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a2",
          "name": "Zuopu Yin",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a3",
          "name": "Wiggin Zhou",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a4",
          "name": "Chayse Zhou",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a5",
          "name": "Fengzong Lian",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/Ib4IFSlNDtkWKjjyVWG-S.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/4OtfeyvZ2jMLaebiU0NKA.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/k-024E68E9-6iaF0QtZad.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/VBS9HcDQ2oHj3bmxgyZVB.png"
      ],
      "publishedAt": "2025-07-07T12:53:00.000Z",
      "submittedOnDailyAt": "2025-07-08T02:53:13.802Z",
      "title": "ArtifactsBench: Evaluación de la generación de código de IAs de lenguaje basada en la interacción visual entre ellas",
      "submittedOnDailyBy": {
        "_id": "64b74b906ab5d14ca7f289cd",
        "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
        "isPro": false,
        "fullname": "xxzcc",
        "user": "xxzcc",
        "type": "user"
      },
      "summary": "La capacidad de generación de los LLM está expandiéndose rápidamente hacia el dinámico y visual interactivo en códigos estáticos. Este desarrollo está limitado por un importante lago de evaluación, ya que los marcos de referencia existentes se centran en la precisión de los algoritmos, sin considerar la confianza visual y la integración interactiva que definen la experiencia de usuario moderna. Para resolver este lago, presentamos ArtifactsBench, un nuevo marco de referencia y paradigma para la evaluación automática multiforme de la generación de código visual. Nuestro marco de referencia representa cada contenido generado de los artefactos de adicción programaticamente y lo visualiza a través de secuencias de capturas de pantalla para comprender su dinámica. Esta evidencia visual y el código generado son evaluados por un MLLM (Multimodal LLM) que actúa como juez, con una puntuación rigurosa basada en listas de chequeo que garantiza la reproducibilidad. Hemos construido un benchmark de 1,825 tareas diferentes y evaluado a más de 30 modelos avanzados de LLM. Nuestra evaluación automática alcanza un 94.4% de concordancia con WebDev Arena y supera el 90% de la concordancia entre expertos. Así, ArtifactsBench es el primer marco de referencia que automatiza la evaluación de la calidad humana a una escala confiable. Nuestros análisis proporcionan mapas de alta resolución actuales, muestran que modelos generales pueden superar modelos de dominio y ofrecen los resultados del benchmark, la evaluación y los límites de referencia en https://artifactsbenchmark.github.io/. Esto proporciona a la comunidad una herramienta escalable y precisa, acelerando el desarrollo de modelos de generación centrados en el usuario.",
      "upvotes": 6,
      "discussionId": "686c8487364e2ad167eb53a6",
      "ai_summary": "ArtifactsBench, a novel benchmark and evaluation framework, automates the assessment of visual code generation quality using temporal screenshots and a multimodal language model judge.",
      "ai_keywords": [
        "Large Language Models",
        "LLMS",
        "dynamic",
        "interactive visual artifacts",
        "visual fidelity",
        "interactive integrity",
        "ArtifactsBench",
        "benchmark",
        "Multimodal LLM",
        "MLLM-as-Judge",
        "fine-grained",
        "per-task checklist",
        "ranking consistency",
        "WebDev Arena",
        "pairwise agreement",
        "human-perceived quality",
        "generalist models",
        "domain-specific ones"
      ]
    },
    "publishedAt": "2025-07-07T08:53:00.000Z",
    "title": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation",
    "summary": "The generative capabilities of Large Language Models (LLMs) are rapidly\nexpanding from static code to dynamic, interactive visual artifacts. This\nprogress is bottlenecked by a critical evaluation gap: established benchmarks\nfocus on algorithmic correctness and are blind to the visual fidelity and\ninteractive integrity that define modern user experiences. To bridge this gap,\nwe introduce ArtifactsBench, a new benchmark and paradigm for the automated,\nmultimodal evaluation of visual code generation. Our framework programmatically\nrenders each generated artifact and captures its dynamic behavior through\ntemporal screenshots. This visual evidence, alongside the source code, is then\nassessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a\nfine-grained, per-task checklist to ensure holistic and reproducible scoring.\nWe construct a new benchmark of 1,825 diverse tasks and evaluate over 30\nleading LLMs. Our automated evaluation achieves a striking 94.4% ranking\nconsistency with WebDev Arena, the gold-standard for human preference in web\ndevelopment, and over 90% pairwise agreement with human experts. This\nestablishes ArtifactsBench as the first framework to reliably automate the\nassessment of human-perceived quality at scale. Our analysis provides a\nhigh-resolution map of the current SOTA, revealing that generalist models often\noutperform domain-specific ones. We open-source ArtifactsBench, including the\nbenchmark, evaluation harness, and baseline results at\nhttps://artifactsbenchmark.github.io/, to provide the community with a scalable\nand accurate tool to accelerate the development of user-centric generative\nmodels.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/Ib4IFSlNDtkWKjjyVWG-S.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/4OtfeyvZ2jMLaebiU0NKA.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/k-024E68E9-6iaF0QtZad.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/VBS9HcDQ2oHj3bmxgyZVB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b74b906ab5d14ca7f289cd",
      "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
      "fullname": "xxzcc",
      "name": "xxzcc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04590",
      "authors": [
        {
          "_id": "686c96a0364e2ad167eb540c",
          "name": "Rui Meng",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb540d",
          "user": {
            "_id": "64778fb8168cb428e00f69b0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64778fb8168cb428e00f69b0/D_XYg74zHG9K3HUJj_gD4.jpeg",
            "isPro": true,
            "fullname": "Ziyan Jiang",
            "user": "ziyjiang",
            "type": "user"
          },
          "name": "Ziyan Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:15.117Z",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb540e",
          "name": "Ye Liu",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb540f",
          "name": "Mingyi Su",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5410",
          "name": "Xinyi Yang",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5411",
          "name": "Yuepeng Fu",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5412",
          "name": "Can Qin",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5413",
          "name": "Zeyuan Chen",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5414",
          "name": "Ran Xu",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5415",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5416",
          "name": "Yingbo Zhou",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5417",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5418",
          "name": "Semih Yavuz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T00:51:57.000Z",
      "submittedOnDailyAt": "2025-07-08T02:26:58.880Z",
      "title": "VLM2Vec-V2: Desarrollo de la codificación multimodal de películas, imágenes y documentos visuales",
      "submittedOnDailyBy": {
        "_id": "64778fb8168cb428e00f69b0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64778fb8168cb428e00f69b0/D_XYg74zHG9K3HUJj_gD4.jpeg",
        "isPro": true,
        "fullname": "Ziyan Jiang",
        "user": "ziyjiang",
        "type": "user"
      },
      "summary": "El modelo multimodal desempeña un papel crucial en facilitar diversas tareas de bajo nivel (semántica de similitud, búsqueda de información, agrupamiento). Sin embargo, los modelos multimodal actuales (VLM2Vec, E5-V, GME) se centran principalmente en imágenes naturales. La soporte para otros formatos visuales (videos, documentos visuales) está limitado, lo que reduce su aplicabilidad en situaciones reales. En entornos como agentes de IA, búsqueda y recomendación multimodal, y generación de agregados de búsqueda (RAG), estas limitaciones son evidentes. Para resolver estos problemas, se propone VLM2Vec-V2 y un marco de trabajo unificado para el aprendizaje multimodal de diversos formatos visuales. Primero, se presenta un nuevo benchmark detallado, MMEB-V2, que incluye cinco nuevos tipos de tareas adicionales en comparación con MMEB. Luego, se entrena VLM2Vec-V2, un modelo multimodal general que admite entradas de gramática, imágenes, videos y documentos visuales. Los resultados de experimentos extendidos muestran que VLM2Vec-V2 presenta un excelente desempeño en nuevas tareas de búsqueda de videos y documentos, y supera los estándares de referencia en los benchmarks de imágenes existentes. A través de estas evaluaciones extendidas, este estudio explica la capacidad de generalización de los modelos multimodal y revela estrategias efectivas para el aprendizaje multimodal unificado, estableciendo una base para la representación de aprendizaje más escalable y aplicable en ambos el ámbito académico y en situaciones reales.",
      "upvotes": 4,
      "discussionId": "686c96a0364e2ad167eb5419",
      "projectPage": "https://tiger-ai-lab.github.io/VLM2Vec/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VLM2Vec",
      "ai_summary": "A unified framework VLM2Vec-V2 is proposed for learning embeddings across diverse visual forms such as videos and documents, demonstrating strong performance on new tasks and improving upon existing benchmarks for images.",
      "ai_keywords": [
        "multimodal embedding models",
        "VLM2Vec",
        "E5-V",
        "GME",
        "MMEB-V2",
        "visual document retrieval",
        "video retrieval",
        "temporal grounding",
        "video classification",
        "video question answering",
        "general-purpose embedding model",
        "unified embedding learning",
        "representation learning"
      ],
      "githubStars": 290
    },
    "publishedAt": "2025-07-06T20:51:57.000Z",
    "title": "VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and\n  Visual Documents",
    "summary": "Multimodal embedding models have been crucial in enabling various downstream\ntasks such as semantic similarity, information retrieval, and clustering over\ndifferent modalities. However, existing multimodal embeddings like VLM2Vec,\nE5-V, GME are predominantly focused on natural images, with limited support for\nother visual forms such as videos and visual documents. This restricts their\napplicability in real-world scenarios, including AI agents, multi-modal search\nand recommendation, and retrieval-augmented generation (RAG). To close this\ngap, we propose VLM2Vec-V2, a unified framework for learning embeddings across\ndiverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark\nthat extends MMEB with five new task types: visual document retrieval, video\nretrieval, temporal grounding, video classification and video question\nanswering - spanning text, image, video, and visual document inputs. Next, we\ntrain VLM2Vec-V2, a general-purpose embedding model that supports text, image,\nvideo, and visual document inputs. Extensive experiments show that VLM2Vec-V2\nachieves strong performance not only on the newly introduced video and document\nretrieval tasks, but also improves over prior baselines on the original image\nbenchmarks. Through extensive evaluation, our study offers insights into the\ngeneralizability of various multimodal embedding models and highlights\neffective strategies for unified embedding learning, laying the groundwork for\nmore scalable and adaptable representation learning in both research and\nreal-world settings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64778fb8168cb428e00f69b0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64778fb8168cb428e00f69b0/D_XYg74zHG9K3HUJj_gD4.jpeg",
      "fullname": "Ziyan Jiang",
      "name": "ziyjiang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03607",
      "authors": [
        {
          "_id": "686cb574364e2ad167eb54c3",
          "user": {
            "_id": "65e5bc754230174d547fa1dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xQ1NGVasxexuR_Hs0dvNl.jpeg",
            "isPro": false,
            "fullname": "Cédric",
            "user": "cedricbonhomme",
            "type": "user"
          },
          "name": "Cédric Bonhomme",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:05:41.165Z",
          "hidden": false
        },
        {
          "_id": "686cb574364e2ad167eb54c4",
          "user": {
            "_id": "677d08a57038d6b09078649a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jEUTW_G_VCgbvqfCxlNTj.png",
            "isPro": false,
            "fullname": "Dulaunoy",
            "user": "adulau",
            "type": "user"
          },
          "name": "Alexandre Dulaunoy",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:39:37.700Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65e5bc754230174d547fa1dc/S4_uGwFUDxVHZqJMljzP8.jpeg"
      ],
      "publishedAt": "2025-07-04T14:28:14.000Z",
      "submittedOnDailyAt": "2025-07-08T06:42:20.895Z",
      "title": "VLAI: Clasificación de la gravedad de errores automatizados basada en un modelo ROBERTa",
      "submittedOnDailyBy": {
        "_id": "65e5bc754230174d547fa1dc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xQ1NGVasxexuR_Hs0dvNl.jpeg",
        "isPro": false,
        "fullname": "Cédric",
        "user": "cedricbonhomme",
        "type": "user"
      },
      "summary": "Este artículo presenta un modelo VLAI basado en Transformer. VLAI se ha desarrollado sobre RoBERTa y se ha entrenado con más de 600,000 datos de vulnerabilidades reales, logrando un nivel de precisión del 82% o más en la predicción de los efectos de las vulnerabilidades. Este modelo ofrece una evaluación más rápida y coherente que la puntuación automática CVSS. El modelo y el conjunto de datos están disponibles bajo licencia abierta y se integran en el servicio Vulnerability-Lookup.",
      "upvotes": 4,
      "discussionId": "686cb574364e2ad167eb54c5",
      "projectPage": "https://www.vulnerability-lookup.org",
      "githubRepo": "https://github.com/vulnerability-lookup/VulnTrain",
      "ai_summary": "A transformer-based model predicts software vulnerability severity levels directly from text, enhancing triage efficiency and consistency.",
      "ai_keywords": [
        "transformer",
        "RoBERTa",
        "parameter-efficient fine-tuning",
        "predicting severity categories",
        "CVSS scoring",
        "open-source",
        "Vulnerability-Lookup service"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-07-04T10:28:14.000Z",
    "title": "VLAI: A RoBERTa-Based Model for Automated Vulnerability Severity\n  Classification",
    "summary": "This paper presents VLAI, a transformer-based model that predicts software\nvulnerability severity levels directly from text descriptions. Built on\nRoBERTa, VLAI is fine-tuned on over 600,000 real-world vulnerabilities and\nachieves over 82% accuracy in predicting severity categories, enabling faster\nand more consistent triage ahead of manual CVSS scoring. The model and dataset\nare open-source and integrated into the Vulnerability-Lookup service.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e5bc754230174d547fa1dc/S4_uGwFUDxVHZqJMljzP8.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03607.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e5bc754230174d547fa1dc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xQ1NGVasxexuR_Hs0dvNl.jpeg",
      "fullname": "Cédric",
      "name": "cedricbonhomme",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.04036",
      "authors": [
        {
          "_id": "686c92c5364e2ad167eb53fa",
          "name": "Jingwei Shi",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fb",
          "user": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "name": "Zeyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:20.306Z",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fc",
          "name": "Biao Wu",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fd",
          "name": "Yanjie Liang",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fe",
          "name": "Meng Fang",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53ff",
          "name": "Ling Chen",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb5400",
          "name": "Yang Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/Kwa3r-mdnTK5Itx86d1f6.mp4"
      ],
      "publishedAt": "2025-07-05T13:24:15.000Z",
      "submittedOnDailyAt": "2025-07-08T02:14:06.764Z",
      "title": "Actual agent: Agente multimodal para la generación de videos de presentación",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "El presentador es un agente multimodal que transforma largos documentos en videos de presentación narrativa. Los métodos actuales están limitados a slides estáticos o resúmenes de texto, pero nuestro método supera estos límites y genera contenido visual y oral completamente sincronizado que se parece al estilo de una presentación humana. Para lograr esta integración, el presentador divide el documento sistemáticamente, planea y renderiza slides visuales, y utiliza un modelo de producción de lado con un modelo de lenguaje grande y texto contextual para generar narraciones orales contextualizadas. El video final se completa con una reconocimiento de voz y visión preciso. Considerando la complejidad de evaluar este tipo de multimodal output, hemos introducido un marco de evaluación unificado en tres dimensiones: fidelidad al contenido, visualización y comprensión de audiencia basada en prompts, incluyendo modelos de lenguaje de visión. Nuestra prueba experimental se realizó con un conjunto de datos de presentaciones de 30 páginas, demostrando que el presentador se acerca al nivel humano en todos los indicadores de calidad. Estos resultados destacan el gran potencial de un agente multimodal controlable para convertir materiales de texto estáticos en formatos dinámicos, eficientes y accesibles de presentación. El código está disponible en https://github.com/AIGeeksGroup/PresentAgent.",
      "upvotes": 3,
      "discussionId": "686c92c6364e2ad167eb5401",
      "githubRepo": "https://github.com/AIGeeksGroup/PresentAgent",
      "ai_summary": "A multimodal agent transforms documents into detailed presentation videos with audio, evaluated using a comprehensive framework involving vision-language models.",
      "ai_keywords": [
        "multimodal agent",
        "narrated presentation videos",
        "static slides",
        "text summaries",
        "large language models",
        "Text-to-Speech models",
        "audio-visual alignment",
        "Vision-Language Models",
        "content fidelity",
        "visual clarity",
        "audience comprehension",
        "prompt-based evaluation"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-07-05T09:24:15.000Z",
    "title": "PresentAgent: Multimodal Agent for Presentation Video Generation",
    "summary": "We present PresentAgent, a multimodal agent that transforms long-form\ndocuments into narrated presentation videos. While existing approaches are\nlimited to generating static slides or text summaries, our method advances\nbeyond these limitations by producing fully synchronized visual and spoken\ncontent that closely mimics human-style presentations. To achieve this\nintegration, PresentAgent employs a modular pipeline that systematically\nsegments the input document, plans and renders slide-style visual frames,\ngenerates contextual spoken narration with large language models and\nText-to-Speech models, and seamlessly composes the final video with precise\naudio-visual alignment. Given the complexity of evaluating such multimodal\noutputs, we introduce PresentEval, a unified assessment framework powered by\nVision-Language Models that comprehensively scores videos across three critical\ndimensions: content fidelity, visual clarity, and audience comprehension\nthrough prompt-based evaluation. Our experimental validation on a curated\ndataset of 30 document-presentation pairs demonstrates that PresentAgent\napproaches human-level quality across all evaluation metrics. These results\nhighlight the significant potential of controllable multimodal agents in\ntransforming static textual materials into dynamic, effective, and accessible\npresentation formats. Code will be available at\nhttps://github.com/AIGeeksGroup/PresentAgent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/Kwa3r-mdnTK5Itx86d1f6.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04036.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05259",
      "authors": [
        {
          "_id": "686ca7ef364e2ad167eb544d",
          "user": {
            "_id": "6499eca0685215f7247bd5ce",
            "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
            "isPro": false,
            "fullname": "Chun-Hsiao Yeh",
            "user": "danielchyeh",
            "type": "user"
          },
          "name": "Chun-Hsiao Yeh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:03.462Z",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb544e",
          "name": "Yilin Wang",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb544f",
          "name": "Nanxuan Zhao",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5450",
          "name": "Richard Zhang",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5451",
          "name": "Yuheng Li",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5452",
          "name": "Yi Ma",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5453",
          "name": "Krishna Kumar Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T17:59:56.000Z",
      "submittedOnDailyAt": "2025-07-08T03:46:34.916Z",
      "title": "X-Planner permite editar imágenes basándose en comandos complejos y es una herramienta que permite realizar ediciones de imágenes no solo sencillas, sino que también soporta ediciones de imágenes que se ajustan a contextos o condiciones complejos.",
      "submittedOnDailyBy": {
        "_id": "6499eca0685215f7247bd5ce",
        "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
        "isPro": false,
        "fullname": "Chun-Hsiao Yeh",
        "user": "danielchyeh",
        "type": "user"
      },
      "summary": "Recientemente, los métodos de edición de imágenes basados en difusión han demostrado significativos avances en tareas basadas en texto, pero su capacidad para entender indicaciones complejas e indirectas es limitada, lo que hace que los modelos actuales experimenten dificultades debido a la mantención de la salida, ediciones no intencionales o la importancia de la participación manual y el uso de máscaras dinámicas. Para resolver estos problemas, introducimos un sistema de planificación basado en el Multimodal Large Language Model (MLLM) llamado X-Planner, que busca efectivamente conectar las intenciones del usuario con las capacidades del modelo de edición. X-Planner utiliza la razonamiento de cadena de pensamiento para descomponer indicaciones complejas en pasos sencillos y claros. Para cada paso de instrucción, X-Planner genera automáticamente tipos de edición con alta precisión y máscaras de partición, eliminando la participación manual y asegurando que las ediciones sean localizadas y mantienen la salida. Además, proponemos un nuevo proceso automático para generar grandes cantidades de datos para la entrenamiento de X-Planner, obteniendo resultados pioneros en los benchmarks actuales y en nuevos benchmarks de edición compleja.",
      "upvotes": 2,
      "discussionId": "686ca7f0364e2ad167eb5454",
      "projectPage": "https://danielchyeh.github.io/x-planner/",
      "ai_summary": "X-Planner, a planning system utilizing a multimodal large language model, decomposes complex text-guided image editing instructions into precise sub-instructions, ensuring localized, identity-preserving edits and achieving top performance on established benchmarks.",
      "ai_keywords": [
        "Multimodal Large Language Model",
        "MLLM",
        "chain-of-thought reasoning",
        "segmentation masks",
        "image editing",
        "identity preservation",
        "automated pipeline"
      ]
    },
    "publishedAt": "2025-07-07T13:59:56.000Z",
    "title": "Beyond Simple Edits: X-Planner for Complex Instruction-Based Image\n  Editing",
    "summary": "Recent diffusion-based image editing methods have significantly advanced\ntext-guided tasks but often struggle to interpret complex, indirect\ninstructions. Moreover, current models frequently suffer from poor identity\npreservation, unintended edits, or rely heavily on manual masks. To address\nthese challenges, we introduce X-Planner, a Multimodal Large Language Model\n(MLLM)-based planning system that effectively bridges user intent with editing\nmodel capabilities. X-Planner employs chain-of-thought reasoning to\nsystematically decompose complex instructions into simpler, clear\nsub-instructions. For each sub-instruction, X-Planner automatically generates\nprecise edit types and segmentation masks, eliminating manual intervention and\nensuring localized, identity-preserving edits. Additionally, we propose a novel\nautomated pipeline for generating large-scale data to train X-Planner which\nachieves state-of-the-art results on both existing benchmarks and our newly\nintroduced complex editing benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05259.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6499eca0685215f7247bd5ce",
      "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
      "fullname": "Chun-Hsiao Yeh",
      "name": "danielchyeh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.04562",
      "authors": [
        {
          "_id": "686ca9e2364e2ad167eb5461",
          "name": "Janna Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-06T22:26:59.000Z",
      "submittedOnDailyAt": "2025-07-08T03:51:31.373Z",
      "title": "「Comparación y Evaluación de LLMs y Predictores Humanos para la Predicción del Mundo Real」",
      "submittedOnDailyBy": {
        "_id": "66b3d98e040c500914ef558f",
        "avatarUrl": "/avatars/a90f8306dbd7747520ce5b941ee3bbcb.svg",
        "isPro": false,
        "fullname": "Janna",
        "user": "jannalu",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) muestran capacidades extraordinarias en varias tareas, pero su capacidad para predecir eventos futuros es poco estudiada. Un año atrás, los modelos de lenguaje grande encontraban dificultades para alcanzar la precisión de un grupo humano. En Metaculus, se evaluaron 464 problemas de predicción y se compararon los mejores modelos de LLMs con los mejores predictores humanos. El modelo Frontier parece superar el Brier score, pero se desvanece claramente cuando se compara con el grupo de predictores humanos.",
      "upvotes": 1,
      "discussionId": "686ca9e2364e2ad167eb5462",
      "ai_summary": "State-of-the-art large language models are evaluated on forecasting questions and show lower accuracy compared to human superforecasters.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "Brier scores",
        "human superforecasters"
      ]
    },
    "publishedAt": "2025-07-06T18:26:59.000Z",
    "title": "Evaluating LLMs on Real-World Forecasting Against Human Superforecasters",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their ability to forecast future events remains\nunderstudied. A year ago, large language models struggle to come close to the\naccuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting\nquestions from Metaculus, comparing their performance against human\nsuperforecasters. Frontier models achieve Brier scores that ostensibly surpass\nthe human crowd but still significantly underperform a group of\nsuperforecasters.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04562.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b3d98e040c500914ef558f",
      "avatarUrl": "/avatars/a90f8306dbd7747520ce5b941ee3bbcb.svg",
      "fullname": "Janna",
      "name": "jannalu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04376",
      "authors": [
        {
          "_id": "686cb0ab364e2ad167eb54a1",
          "name": "Georgios Ioannides",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a2",
          "name": "Christos Constantinou",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a3",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a4",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:05:49.355Z",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a5",
          "name": "Aaron Elkins",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-06T12:46:57.000Z",
      "submittedOnDailyAt": "2025-07-08T04:20:01.196Z",
      "title": "MOD-X: Propuesta de un marco de intercambio modular y distribuido abierto\nIntercambiabilidad de agentes artificiales con múltiples sobreescrituras posibles",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "El sistema de IA se está moviendo de un modelo único hacia una ecosistema de agentes especializados, lo que hace cada vez más importante la necesidad de protocolos de comunicación estándarizados. En este artículo, se presenta una nueva arquitectura de marco MOD-X (Modular Open Decentralized eXchange), diseñada para resolver los principales limitaciones de los protocolos actuales. A diferencia de los métodos de acceso actuales, MOD-X propone una estructura de capas que incluye un Bus de Mensajes Universal, un manejo detallado de estado, funciones de traducción y una estructura de seguridad basada en la blockchain. Se presenta la arquitectura de MOD-X, se compara con los protocolos actuales, y se muestra cómo la integración efectiva de agentes inteligentes (sistemas basados en reglas, redes neuronales, motores de lógica de símbolos, software genético) se logra a través de un trabajo de taller que permite la representación de otras arquitecturas, negocios, capacidades y conocimientos. Las principales innovaciones de MOD-X incluyen el modelo de comunicación de subcriptivos de productos, la detección de capacidades significativas y la arquitectura de flujos de trabajo dinámicas, proporcionando un marco que conecta la teoría formal con la implementación práctica. Esta arquitectura es esencial para la expansión de una verdadera ecosistema de agentes intercambiables y distribuidos, capaz de escalarse eficientemente sin necesidad de un control centralizado.",
      "upvotes": 1,
      "discussionId": "686cb0ab364e2ad167eb54a6"
    },
    "publishedAt": "2025-07-06T08:46:57.000Z",
    "title": "MOD-X: A Modular Open Decentralized eXchange Framework proposal for\n  Heterogeneous Interoperable Artificial Agents",
    "summary": "As Artificial Intelligence systems evolve from monolithic models to\necosystems of specialized agents, the need for standardized communication\nprotocols becomes increasingly critical. This paper introduces MOD-X (Modular\nOpen Decentralized eXchange), a novel architectural framework proposal for\nagent interoperability that addresses key limitations of existing protocols.\nUnlike current approaches, MOD-X proposes a layered architecture with a\nUniversal Message Bus, thorough state management, translation capabilities, and\nblockchain-based security mechanisms. We present MOD-X's architecture, compare\nit with existing protocols, and demonstrate its application through a worked\nexample how it enables integration between heterogeneous specialist agents\n(agents with different architectures, vendors, capabilities, and knowledge\nrepresentations--including rule-based systems, neural networks, symbolic\nreasoning engines, and legacy software with agent wrappers). MOD-X's key\ninnovations include a publish-subscribe communication model, semantic\ncapability discovery, and dynamic workflow orchestration--providing a framework\nthat bridges theoretical formalism with practical implementation. This\narchitecture addresses the growing need for truly decentralized, interoperable\nagent ecosystems that can scale effectively without the need for central\ncoordination.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03336",
      "authors": [
        {
          "_id": "686c67e2364e2ad167eb5314",
          "user": {
            "_id": "637859f98f288aba3d01f588",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637859f98f288aba3d01f588/eP8YNMOtxTvxH-rYEYn4f.png",
            "isPro": false,
            "fullname": "Ashutosh Hathidara",
            "user": "ashutosh1919",
            "type": "user"
          },
          "name": "Ashutosh Hathidara",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:34.896Z",
          "hidden": false
        },
        {
          "_id": "686c67e2364e2ad167eb5315",
          "name": "Julien Yu",
          "hidden": false
        },
        {
          "_id": "686c67e2364e2ad167eb5316",
          "name": "Sebastian Schreiber",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T06:49:02.000Z",
      "submittedOnDailyAt": "2025-07-08T02:47:33.416Z",
      "title": "Una empresa que cuenta con un centro de diseño de pantalones micro ajustables puede llamar a herramientas de negocios para reducir el riesgo, ya que el LLM se acerca a lo real.",
      "submittedOnDailyBy": {
        "_id": "637859f98f288aba3d01f588",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637859f98f288aba3d01f588/eP8YNMOtxTvxH-rYEYn4f.png",
        "isPro": false,
        "fullname": "Ashutosh Hathidara",
        "user": "ashutosh1919",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje grande (LLMs) aumentan la tarea de llamar a las API de las empresas, sin embargo, en casos donde herramientas similares compiten para el mismo propósito de un usuario o faltan argumentos necesarios, por ejemplo, siempre fallan. En este contexto, se presenta DiaFORGE (DíaPorge, marco de diálogo conversacional para generación y evaluación de respuestas abierto-código). DiaFORGE es una pipeline de tres etapas, con una dinámica de diversidad sinérgica. Además, cumple con (i) la síntesis de diálogos de dialogo de turno profesionales para que los asistentes puedan diferenciar herramientas similares, (ii) la configuración de subobjetos que incluyen registros de razonamiento en modelos abierto-código de 3B a 70B parámetros, y (iii) una serie dinámica para evaluar la preparación en el mundo real. Los modelos entrenados con DiaFORGE en el Benchmark Dinámico DiaBENCH mejoran la eficiencia de los prompts, superando a GPT-4o en 27pp y a Claude-3.5-Sonnet en 49pp. Además, DiaFORGE mejora la tasa de éxito en la llamada a las API. Además, en DiaFORGE se publican en pares de corpus abiertos 5000 especificaciones de motores de productos de API empresariales y diálogos de diversidad centrados, con una estrategia práctica para la construcción de agentes de llamada a herramientas empresariales confiables.",
      "upvotes": 1,
      "discussionId": "686c67e2364e2ad167eb5317",
      "ai_summary": "DiaFORGE is a disambiguation framework that enhances large language models' ability to invoke enterprise APIs accurately through dialogue synthesis, supervised fine-tuning, and real-world evaluation.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "enterprise APIs",
        "persona-driven",
        "multi-turn dialogues",
        "supervised fine-tuning",
        "reasoning traces",
        "end-to-end goal completion",
        "dynamic benchmark",
        "tool-invocation success",
        "disambiguation-focused dialogues"
      ]
    },
    "publishedAt": "2025-07-04T02:49:02.000Z",
    "title": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky",
    "summary": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03336.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637859f98f288aba3d01f588",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637859f98f288aba3d01f588/eP8YNMOtxTvxH-rYEYn4f.png",
      "fullname": "Ashutosh Hathidara",
      "name": "ashutosh1919",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.02659",
      "authors": [
        {
          "_id": "686736ed9db35afc9c304cea",
          "name": "Ramchalam Kinattinkara Ramakrishnan",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304ceb",
          "user": {
            "_id": "65d989790733541e06823258",
            "avatarUrl": "/avatars/cf658c5f586c02e9b1d610b5e49b2826.svg",
            "isPro": false,
            "fullname": "Zhaocong Yuan",
            "user": "justinyyy",
            "type": "user"
          },
          "name": "Zhaocong Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-07T15:46:47.568Z",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cec",
          "name": "Shaojie Zhuo",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304ced",
          "name": "Chen Feng",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cee",
          "name": "Yicheng Lin",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cef",
          "name": "Chenzheng Su",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cf0",
          "name": "Xiaopeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T14:20:41.000Z",
      "submittedOnDailyAt": "2025-07-08T02:34:16.093Z",
      "title": "Omnidraft: herramienta de decodificación especial omnidireñual para decodificación crossbow de uso en línea",
      "submittedOnDailyBy": {
        "_id": "65d989790733541e06823258",
        "avatarUrl": "/avatars/cf658c5f586c02e9b1d610b5e49b2826.svg",
        "isPro": false,
        "fullname": "Zhaocong Yuan",
        "user": "justinyyy",
        "type": "user"
      },
      "summary": "El específico decodificación generalmente recomienda el uso de pequeños modelos eficientes de draught. Este modelo se ha entrenado previamente para series de modelos de meta (por ejemplo: Llama, Qwen modelos) o está desactualizado. Sin embargo, en las configuraciones de entrenamiento en línea, existen dos grandes problemas significativos: 1) el uso de modelos de meta que no coinciden con el modelo de draught; 2) la esperanza de mejoras radiacionales a medida que se utiliza. En este artículo, se propone el framework nodoide OmniDraft, que permitirá que el modelo de draught se adapte dinámicamente a todos los modelos de meta. Además, para resolver el problema de la mala correspondencia cruzada entre el modelo de draught y el modelo de meta, se introduce el ajuste final combinado de modelos antiguos y se utiliza la técnica de adaptación para mejorar la velocidad de decodificación. OmniDraft es particularmente adecuado para aplicaciones de LLM en dispositivos, donde los principales puntos de discusión son el costo del modelo, su efectividad y la personalización para el usuario. Esto resalta la necesidad de resolver los problemas mencionados y se alinea con el paradigma de \"un solo modelo de draught\". Se muestra el efecto del framework OmniDraft en tareas de cálculo matemático, programación y generación de texto a través de entrenamiento en línea. En particular, OmniDraft puede realizar decodificación especializada con modelos de meta diversos, como Llama-68M, Vicuna-7B, Qwen2-7B y Llama3-8B, con un aumento de velocidad del 1.5 a 2 veces.",
      "upvotes": 0,
      "discussionId": "686736ed9db35afc9c304cf1",
      "ai_summary": "OmniDraft, a unified framework, addresses cross-vocabulary mismatch and improves decoding speed by allowing a single draft model to interact dynamically with diverse target models in online settings.",
      "ai_keywords": [
        "n-gram cache",
        "hybrid distillation fine-tuning",
        "adaptive drafting",
        "on-device LLM applications"
      ]
    },
    "publishedAt": "2025-07-03T10:20:41.000Z",
    "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
    "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the ``one drafter for all'' paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02659.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d989790733541e06823258",
      "avatarUrl": "/avatars/cf658c5f586c02e9b1d610b5e49b2826.svg",
      "fullname": "Zhaocong Yuan",
      "name": "justinyyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]