[
  {
    "paper": {
      "id": "2503.03601",
      "authors": [
        {
          "_id": "67cbfff12cc05acaab147f07",
          "name": "Kristian Kuznetsov",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f08",
          "user": {
            "_id": "636254dc2691058b19d9276a",
            "avatarUrl": "/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg",
            "isPro": false,
            "fullname": "Kushnareva",
            "user": "Kushnareva",
            "type": "user"
          },
          "name": "Laida Kushnareva",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:23:18.630Z",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f09",
          "name": "Polina Druzhinina",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0a",
          "user": {
            "_id": "6172aaeec8e66e2aa84c06b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6172aaeec8e66e2aa84c06b9/ZdRZSp3P1SU6CIDbvQwkv.jpeg",
            "isPro": false,
            "fullname": "Anton Razzhigaev",
            "user": "razzant",
            "type": "user"
          },
          "name": "Anton Razzhigaev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:23:21.197Z",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0b",
          "name": "Anastasia Voznyuk",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0c",
          "name": "Irina Piontkovskaya",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0d",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0e",
          "name": "Serguei Barannikov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T15:33:52.000Z",
      "title": "Conocimiento sobre el sparse autómatro codificador que detecta literatura artificialmente escrita a nivel de características",
      "summary": "La detección de texto artificial (ATD) se vuelve cada vez más importante con el aumento de los modelos de lenguaje de alto nivel (LLMs). Sin embargo, a pesar de los esfuerzos conjuntos, no es posible mostrar un buen rendimiento consistente para diferentes tipos de texto iniciales y tampoco se puede garantizar una extensión válida para nuevos LLMs. La interpretabilidad juega un papel crucial para alcanzar este objetivo. En este estudio, se utilizan autoencoders esparsos (SAE) para extraer características de la streaming residual de Gemma-2-2b y mejorar la interpretabilidad de la ATD. Se identifican dos características interpretables y eficientes, y se analizan su significado y asociación mediante estadísticas propias del dominio y del modelo, un enfoque de estacionamiento, o analisis manual o basado en LLMs. Nuestro método proporciona información valiosa sobre cómo el texto escrito por humanos difiere en diferentes modelos. Además, los actuales LLMs tienen una representación especial en dominios con alta densidad de información y pueden generar salidas similares a las de los humanos utilizando técnicas de programación profesional.",
      "upvotes": 85,
      "discussionId": "67cbfff22cc05acaab147f4d",
      "ai_keywords": [
        "Sparse Autoencoders",
        "Gemma-2-2b",
        "residual stream",
        "interpretability",
        "domain-specific statistics",
        "model-specific statistics",
        "steering approach",
        "LLM-based interpretation",
        "writing style",
        "information-dense domains",
        "human-like outputs"
      ]
    },
    "publishedAt": "2025-03-05T10:33:52.000Z",
    "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
    "summary": "Artificial Text Detection (ATD) is becoming increasingly important with the\nrise of advanced Large Language Models (LLMs). Despite numerous efforts, no\nsingle algorithm performs consistently well across different types of unseen\ntext or guarantees effective generalization to new LLMs. Interpretability plays\na crucial role in achieving this goal. In this study, we enhance ATD\ninterpretability by using Sparse Autoencoders (SAE) to extract features from\nGemma-2-2b residual stream. We identify both interpretable and efficient\nfeatures, analyzing their semantics and relevance through domain- and\nmodel-specific statistics, a steering approach, and manual or LLM-based\ninterpretation. Our methods offer valuable insights into how texts from various\nmodels differ from human-written content. We show that modern LLMs have a\ndistinct writing style, especially in information-dense domains, even though\nthey can produce human-like outputs with personalized prompts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03601.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07365",
      "authors": [
        {
          "_id": "67cf9cd037bc7273882147a3",
          "name": "Fanqing Meng",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a4",
          "name": "Lingxiao Du",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a5",
          "name": "Zongkai Liu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a6",
          "name": "Zhixiang Zhou",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a7",
          "name": "Quanfeng Lu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a8",
          "name": "Daocheng Fu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a9",
          "name": "Botian Shi",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147aa",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ab",
          "name": "Junjun He",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ac",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ad",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ae",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147af",
          "name": "Qiaosheng Zhang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147b0",
          "name": "Wenqi Shao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T14:23:12.000Z",
      "title": "MM-Eureka: Aprendizaje por refuerzo basado en reglas para explorar visualmente los \"momentos de dolor\" a gran escala",
      "summary": "MM-Eureka es un modelo de lógica multi-modelo y ha tenido éxito al extender el aprendizaje por refuerzo basado en reglas (RL) a un modelo de lógica multi-modelo. El aprendizaje por refuerzo basado en reglas ha demostrado un éxito sorprendente en mejorar las capacidades de lógica de los modelos de lenguaje grandes (LLM) en contextos, pero su aplicación en un entorno multi-modelo ha sido difícil. Nuestra investigación ha recreato las principales características de sistemas de aprendizaje por refuerzo basado en contexto, como DeepSeek-R1, en un espacio multi-modelo, y ha incluido mejoras en precisión, un aumento estable de la longitud de respuesta y la aparición de acciones de retroalimentación. Mostramos que, utilizando el aprendizaje por refuerzo basado en reglas, modelos de instancia de entrenamiento y modelos previamente entrenados pueden desarrollar una fuerte capacidad de lógica multi-modelo sin la limitación de un profesor, y que presentan una mayor eficiencia en datos en comparación con otros enfoques. Abrimos el código completo de nuestra pipeline en GitHub en https://github.com/ModalMinds/MM-EUREKA y promovemos el desarrollo de esta área. Todo nuestro código, modelos y datos están disponibles en un nuevo lanzamiento.",
      "upvotes": 38,
      "discussionId": "67cf9cd137bc7273882147e2",
      "ai_keywords": [
        "multimodal reasoning",
        "rule-based reinforcement learning (RL)",
        "large-scale rule-based reinforcement learning (RL)",
        "DeepSeek-R1",
        "multimodal space",
        "accuracy reward",
        "response length",
        "reflection behaviors",
        "instruction-tuned",
        "pre-trained models",
        "multimodal reasoning capabilities",
        "rule-based RL",
        "supervised fine-tuning",
        "data efficiency"
      ]
    },
    "publishedAt": "2025-03-10T10:23:12.000Z",
    "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning",
    "summary": "We present MM-Eureka, a multimodal reasoning model that successfully extends\nlarge-scale rule-based reinforcement learning (RL) to multimodal reasoning.\nWhile rule-based RL has shown remarkable success in improving LLMs' reasoning\nabilities in text domains, its application to multimodal settings has remained\nchallenging. Our work reproduces key characteristics of text-based RL systems\nlike DeepSeek-R1 in the multimodal space, including steady increases in\naccuracy reward and response length, and the emergence of reflection behaviors.\nWe demonstrate that both instruction-tuned and pre-trained models can develop\nstrong multimodal reasoning capabilities through rule-based RL without\nsupervised fine-tuning, showing superior data efficiency compared to\nalternative approaches. We open-source our complete pipeline to foster further\nresearch in this area. We release all our codes, models, data, etc. at\nhttps://github.com/ModalMinds/MM-EUREKA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07365.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07605",
      "authors": [
        {
          "_id": "67cfa0c1edb742caa3572982",
          "name": "Xun Liang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572983",
          "user": {
            "_id": "669e0b93c7cb0568dac6e92e",
            "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
            "isPro": false,
            "fullname": "hanyu Wang",
            "user": "UglyToilet",
            "type": "user"
          },
          "name": "Hanyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:46.104Z",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572984",
          "name": "Huayi Lai",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572985",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572986",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572987",
          "name": "Jiawei Yang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572988",
          "name": "Jihao Zhao",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572989",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa357298a",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa357298b",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:59:03.000Z",
      "title": "SEAP: Entrenamiento sin necesidad de pasos de paso para preparar el aprendizaje activo de un modelo de lenguaje de gran escala.",
      "summary": "El modelo de lenguaje natural es capaz de lograr éxitos impresionantes en diversas tareas de procesamiento del lenguaje natural, pero su costo de cálculo alto durante la inferencia sigue siendo un obstáculo principal. En este artículo, se presenta una técnica de reducción de parámetros sin entrenamiento llamada Sparse Expert Activation Pruning (SEAP), que permite reducir el overhead de inferencia al mantener parámetros seleccionados relacionados con la tarea. Inspirado en el estado oculto y patrones de activación de modelos grandes de lenguaje, SEAP identifica patrones de activación específicos para cada tarea, reduciendo el modelo mientras mantiene la eficiencia computacional y la precisión. Los resultados de los experimentos muestran que SEAP puede reducir significativamente el overhead de cálculo y mantener una precisión relativa. En particular, demostró un efecto superior al de WandA y FLAP al reducir el modelo en un 50%, y solo una pérdida de 2.2% en el rendimiento comparado con modelos densos al reducir el 20%. Estos hallazgos demuestran la escalabilidad y eficiencia de SEAP, y sugieren la posibilidad de una aproximación adecuada para la optimización de grandes modelos de lenguaje natural.",
      "upvotes": 36,
      "discussionId": "67cfa0c2edb742caa35729dc",
      "githubRepo": "https://github.com/IAAR-Shanghai/SEAP",
      "ai_keywords": [
        "Sparse Expert Activation Pruning (SEAP)",
        "hidden states",
        "activations",
        "task-specific expert activation patterns",
        "computational efficiency"
      ]
    },
    "publishedAt": "2025-03-10T13:59:03.000Z",
    "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models",
    "summary": "Large Language Models have achieved remarkable success across various natural\nlanguage processing tasks, yet their high computational cost during inference\nremains a major bottleneck. This paper introduces Sparse Expert Activation\nPruning (SEAP), a training-free pruning method that selectively retains\ntask-relevant parameters to reduce inference overhead. Inspired by the\nclustering patterns of hidden states and activations in LLMs, SEAP identifies\ntask-specific expert activation patterns and prunes the model while preserving\ntask performance and enhancing computational efficiency. Experimental results\ndemonstrate that SEAP significantly reduces computational overhead while\nmaintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both\nWandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2%\nperformance drop compared to the dense model. These findings highlight SEAP's\nscalability and effectiveness, making it a promising approach for optimizing\nlarge-scale LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07605.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07002",
      "authors": [
        {
          "_id": "67cfa814d212c9c5048845a0",
          "name": "Jiazheng Liu",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a1",
          "name": "Sipeng Zheng",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a2",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:32.095Z",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a3",
          "name": "Zongqing Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T07:32:53.000Z",
      "title": "Nota y concentración - Un enfoque de diálogo multimodal\n\nAprendizaje",
      "summary": "Introduzco el conjunto de datos MMDiag de modelos múltiples. Este conjunto de datos fue diseñado especialmente con reglas específicas y con ayuda de GPT, generado conjuntamente, manteniendo una fuerte correlación entre las preguntas, entre las preguntas y las imágenes, y entre diferentes áreas de imágenes, lo que ha permitido crear una experiencia más cercana a la realidad. MMDiag desempeña un papel de marco de referencia fuerte en el aprendizaje de conjuntos de datos múltiples, presentando mayores desafíos para la capacidad de razonamiento lógico de los MLLM. Además, presento un MLLM llamado DiagNote, que se conecta a la procesamiento visual humano. DiagNote funciona con dos módulos que interactúan mutuamente: la planificación y la visualización. DiagNote experimenta mostrando una combinación de mejores bases de conocimiento, información visual y lenguaje, lo que permite una mejor capacidad de razonamiento lógico.",
      "upvotes": 29,
      "discussionId": "67cfa818d212c9c504884689",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "vision towers",
        "multi-turn vision question-answering tasks",
        "multi-turn multimodal dialogue dataset (MMDiag)",
        "GPT assistant",
        "multimodal dialogue learning",
        "grounding",
        "reasoning capabilities",
        "Deliberate module",
        "Gaze module",
        "Chain-of-Thought"
      ]
    },
    "publishedAt": "2025-03-10T03:32:53.000Z",
    "title": "Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue\n  Learning",
    "summary": "Multimodal large language models (MLLMs), built on large-scale pre-trained\nvision towers and language models, have shown great capabilities in multimodal\nunderstanding. However, most existing MLLMs are trained on single-turn vision\nquestion-answering tasks, which do not accurately reflect real-world human\nconversations. In this paper, we introduce MMDiag, a multi-turn multimodal\ndialogue dataset. This dataset is collaboratively generated through\ndeliberately designed rules and GPT assistance, featuring strong correlations\nbetween questions, between questions and images, and among different image\nregions; thus aligning more closely with real-world scenarios. MMDiag serves as\na strong benchmark for multi-turn multimodal dialogue learning and brings more\nchallenges to the grounding and reasoning capabilities of MLLMs. Further,\ninspired by human vision processing, we present DiagNote, an MLLM equipped with\nmultimodal grounding and reasoning capabilities. DiagNote consists of two\nmodules (Deliberate and Gaze) interacting with each other to perform\nChain-of-Thought and annotations respectively, throughout multi-turn dialogues.\nWe empirically demonstrate the advantages of DiagNote in both grounding and\njointly processing and reasoning with vision and language information over\nexisting MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07002.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07314",
      "authors": [
        {
          "_id": "67cfa750c8f2a661dc9798fe",
          "name": "Weijia Wu",
          "hidden": false
        },
        {
          "_id": "67cfa750c8f2a661dc9798ff",
          "name": "Zeyu Zhu",
          "hidden": false
        },
        {
          "_id": "67cfa750c8f2a661dc979900",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T13:33:27.000Z",
      "title": "Automatización de la generación de películas para el planificamiento de contexto multi-agente",
      "summary": "Actualmente, los marcos de trabajo de creación de largas vídeos están limitados por una falta de planificación automática, lo que requiere entradas manuales de mano para la interacción entre historias, escenas, diseño de vídeos y personajes, lo que genera altos costos y eficiencias insatisfactorias. Para resolver estos problemas, presentamos MovieAgent. MovieAgent es un marco de trabajo automático de generación de películas utilizando una planificación efectiva de Cadena de Pensamiento (CoT). MovieAgent ofrece dos principales ventajas: 1) exploramos y definimos un patrón de generación automática de películas/vídeos largos en el inicio. Al proporcionar un script y un vector de personajes, nuestro MovieAgent genera vídeos largos de varias escenas y ángulos, manteniendo la coherencia de los personajes, la subtítulos coordinados y la coherencia de los sub-narrados, asegurando una coherencia y una estructura coherente. 2) MovieAgent introduce un proceso de razonamiento heurístico basado en CoT para construir automáticamente la composición de escenas, configuración de cámaras y diseño de vídeos, reduciendo significativamente el esfuerzo humano. Mimetizando el papel de directores de películas, diseñadores de pantalla, artístas de storyboard y gestores de rostros, MovieAgent utiliza múltiples agentes de IA para streamline el proceso productivo. Las experimentaciones han logrado resultados óptimos en dependencia de script, coherencia de personajes y conectividad de narrativa utilizando la nueva estrategia de distancia mínima. Nuestro marco heurístico ofrece nuevas perspectivas en la generación automática completa de películas. Los códigos y el sitio web del proyecto están disponibles en las siguientes URLs: https://github.com/showlab/MovieAgent y https://weijiawu.github.io/MovieAgent.",
      "upvotes": 24,
      "discussionId": "67cfa752c8f2a661dc9799b8",
      "ai_keywords": [
        "MovieAgent",
        "Chain of Thought (CoT)",
        "automated movie/long-video generation",
        "multi-scene, multi-shot long-form videos",
        "coherent narrative",
        "character consistency",
        "synchronized subtitles",
        "stable audio",
        "hierarchical CoT-based reasoning",
        "multiple LLM agents",
        "director",
        "screenwriter",
        "storyboard artist",
        "location manager",
        "script faithfulness",
        "narrative coherence",
        "fully automated movie generation"
      ]
    },
    "publishedAt": "2025-03-10T09:33:27.000Z",
    "title": "Automated Movie Generation via Multi-Agent CoT Planning",
    "summary": "Existing long-form video generation frameworks lack automated planning,\nrequiring manual input for storylines, scenes, cinematography, and character\ninteractions, resulting in high costs and inefficiencies. To address these\nchallenges, we present MovieAgent, an automated movie generation via\nmulti-agent Chain of Thought (CoT) planning. MovieAgent offers two key\nadvantages: 1) We firstly explore and define the paradigm of automated\nmovie/long-video generation. Given a script and character bank, our MovieAgent\ncan generates multi-scene, multi-shot long-form videos with a coherent\nnarrative, while ensuring character consistency, synchronized subtitles, and\nstable audio throughout the film. 2) MovieAgent introduces a hierarchical\nCoT-based reasoning process to automatically structure scenes, camera settings,\nand cinematography, significantly reducing human effort. By employing multiple\nLLM agents to simulate the roles of a director, screenwriter, storyboard\nartist, and location manager, MovieAgent streamlines the production pipeline.\nExperiments demonstrate that MovieAgent achieves new state-of-the-art results\nin script faithfulness, character consistency, and narrative coherence. Our\nhierarchical framework takes a step forward and provides new insights into\nfully automated movie generation. The code and project website are available\nat: https://github.com/showlab/MovieAgent and\nhttps://weijiawu.github.io/MovieAgent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07314.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07216",
      "authors": [
        {
          "_id": "67cfa6fcd77496ce0c154bdc",
          "name": "Sangwoo Park",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bdd",
          "name": "Seanie Lee",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bde",
          "name": "Byungjoo Kim",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bdf",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T11:55:50.000Z",
      "title": "FedRand: Mejora de la protección de la información personal en el aprendizaje colaborativo mediante la randomización de LoRA en unidades de actualización",
      "summary": "Federado Learning (FL) es un marco de trabajo ampliamente utilizado para entrenar modelos de manera distribuida. Este método está diseñado de manera que un servidor central no tenga acceso directo a los datos locales. Sin embargo, este enfoque presenta problemas debido a que cuando el modelo se centra en el servidor central, la privacidad de los datos no puede ser protegida completamente. Este problema es particularmente relevante cuando se entrena un Modelo de Visión y Lenguaje (VLM) mediante FL. Los VLM pueden recordar fácilmente las instancias de los datos de entrenamiento y pueden ser vulnerables a ataques de inferencia de miembros (MIAs). Para enfrentar estas desafíos, se propone el marco de trabajo FedRand. Este marco está diseñado de manera que ningún parámetro de todas las computadoras sea publicado. En FedRand, cada computadora selecciona subparámetros de adaptación de bajo rango (LoRA) aleatoriamente del servidor y mantiene los otros pesos de LoRA como parámetros no publicos. Después de entrenar ambos parámetros en el conjunto de datos no publico de la computadora, solo se envía al servidor los parámetros no publicos, lo que concentra el modelo VLM. Este enfoque reduce el riesgo de exposición de los parámetros de VLM de la computadora y mejora la privacidad de los datos. Experimentalmente, FedRand mejora la robustez frente a MIAs en comparación con referencias pertinentes y demuestra que su precisión es equivalente a la de métodos que envían completamente los parámetros LoRA en diferentes conjuntos de datos de prueba.",
      "upvotes": 22,
      "discussionId": "67cfa6fdd77496ce0c154c18",
      "ai_keywords": [
        "Federated Learning (FL)",
        "vision-language models (VLMs)",
        "membership inference attacks (MIAs)",
        "FedRand framework",
        "Low-Rank Adaptation (LoRA)",
        "subparameters",
        "non-private client parameters",
        "client parameters",
        "aggregation",
        "robustness"
      ]
    },
    "publishedAt": "2025-03-10T07:55:50.000Z",
    "title": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
    "summary": "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07216.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07067",
      "authors": [
        {
          "_id": "67cfa99b7c95194db8d75468",
          "user": {
            "_id": "64b7628af902508f0d7ae112",
            "avatarUrl": "/avatars/83c155254486e80c1dfd14676fdf9215.svg",
            "isPro": false,
            "fullname": "Jongwoo Ko",
            "user": "jongwooko",
            "type": "user"
          },
          "name": "Jongwoo Ko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:29.622Z",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d75469",
          "user": {
            "_id": "64ad94f05a4a60156925ec96",
            "avatarUrl": "/avatars/643bdb076e703bfcc89cec6fccb756c6.svg",
            "isPro": false,
            "fullname": "Tianyi Chen",
            "user": "tianyic",
            "type": "user"
          },
          "name": "Tianyi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:27.139Z",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546a",
          "name": "Sungnyun Kim",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546b",
          "name": "Tianyu Ding",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546c",
          "name": "Luming Liang",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546d",
          "name": "Ilya Zharkov",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546e",
          "name": "Se-Young Yun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T08:51:32.000Z",
      "title": "DistiLLM-2: Un enfoque contrastivo para mejorar el nivel de estilización de los LLM",
      "summary": "Aunque se ha logrado el éxito en la desdistilación de grandes modelos de lenguaje (LLMs), casi todos los estudios previos aplican la misma función de pérdida a los datos de generación de modelos de maestro y estudiante. Estas estrategias ignoran la asociación entre la representación de la pérdida y el tipo de datos, y también ignoran que no se pueden obtener resultados óptimos para mejorar el rendimiento del modelo estudiante. En este sentido, proponemos un enfoque relativamente que utiliza la asociación de manera que el modelo estudiante disminuya la probabilidad de su respuesta cuando la probabilidad de la respuesta del modelo maestro se incrementa. Nuestras amplias experimentaciones muestran que DistiLLM-2 puede construir un altamente efectivo modelo estudiante que resuelve diversos problemas, como la generación de instrucciones y código, y apoya aplicaciones como la configuración de preferencias y la extensión del lenguaje visual. Estos hallazgos revelan que este enfoque relativo puede realizar efectivamente la asociación entre modelos maestro y estudiante, y mejorar el efecto de la desdistilación de LLMs para diferentes tipos de datos.",
      "upvotes": 19,
      "discussionId": "67cfa99c7c95194db8d754bf",
      "githubRepo": "https://github.com/jongwooko/distillm-2",
      "ai_keywords": [
        "contrastive approach",
        "likelihood",
        "DistiLLM-2",
        "instruction-following",
        "code generation",
        "preference alignment",
        "vision-language extensions"
      ]
    },
    "publishedAt": "2025-03-10T04:51:32.000Z",
    "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs",
    "summary": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07067.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06680",
      "authors": [
        {
          "_id": "67cf94d9f2b1fe815db6db40",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db41",
          "user": {
            "_id": "641a9a4b05290a135041a3ed",
            "avatarUrl": "/avatars/95d66ac607973abe95bd3558c6c93739.svg",
            "isPro": false,
            "fullname": "Pluto",
            "user": "CharonBony",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T01:41:47.194Z",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db42",
          "name": "Zhongxin Guo",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db43",
          "name": "Shaoguang Mao",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db44",
          "name": "Wen Luo",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db45",
          "name": "Guangyue Peng",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db46",
          "name": "Yangyu Huang",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db47",
          "name": "Houfeng Wang",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db48",
          "name": "Scarlett Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T16:11:57.000Z",
      "title": "FEA-Bench: FEA-Bench es un marco de referencia utilizado para evaluar la generación de códigos a nivel de repositorio de implementación de características de FEA (Análisis de Elementos Finitos).",
      "summary": "Implementación de nuevas funciones en un nivel de código es un campo importante para modelos de código generación. Sin embargo, actualmente los marcos de evaluación no cubren su capacidad de manera profesional. Para remediar esto, se presenta FEA-Bench. FEA-Bench está diseñado para evaluar la capacidad de grandes modelos de lenguaje (LLMs) para realizar desarrollos dentro de un repositorio. Se recopilaron solicitudes de pull request en un repositorio GitHub de 83 páginas y se construyeron instancias de tarea con un enfoque en el desarrollo de nuevas funciones, utilizando filtros basados en reglas y basados en intención. Las modificaciones de código en cada instancia de tarea pueden ser validadas con los archivos de pruebas unitarias relacionados. Para la implementación de nuevas funciones, los LLMs necesitan completar el código de nuevos componentes y editar otras partes relacionadas dentro del repositorio. FEA-Bench evalúa más detalladamente la capacidad de los LLMs para la auto-generación de software. Los resultados de las pruebas muestran que los LLMs presentaron un rendimiento significativamente bajo en FEA-Bench y se descubrieron importantes problemas en el desarrollo de nivel de repositorio.",
      "upvotes": 16,
      "discussionId": "67cf94dbf2b1fe815db6db9e"
    },
    "publishedAt": "2025-03-09T12:11:57.000Z",
    "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation",
    "summary": "Implementing new features in repository-level codebases is a crucial\napplication of code generation models. However, current benchmarks lack a\ndedicated evaluation framework for this capability. To fill this gap, we\nintroduce FEA-Bench, a benchmark designed to assess the ability of large\nlanguage models (LLMs) to perform incremental development within code\nrepositories. We collect pull requests from 83 GitHub repositories and use\nrule-based and intent-based filtering to construct task instances focused on\nnew feature development. Each task instance containing code changes is paired\nwith relevant unit test files to ensure that the solution can be verified. The\nfeature implementation requires LLMs to simultaneously possess code completion\ncapabilities for new components and code editing abilities for other relevant\nparts in the code repository, providing a more comprehensive evaluation method\nof LLMs' automated software engineering capabilities. Experimental results show\nthat LLMs perform significantly worse in the FEA-Bench, highlighting\nconsiderable challenges in such repository-level incremental code development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06680.png",
    "numComments": 5,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07027",
      "authors": [
        {
          "_id": "67cf98fd59dbba733d8c531e",
          "user": {
            "_id": "636b3f9ce3ad78bc68b67541",
            "avatarUrl": "/avatars/2b7e745953ae39e01222e99fb63b279e.svg",
            "isPro": false,
            "fullname": "yuxuan",
            "user": "zzyx",
            "type": "user"
          },
          "name": "Yuxuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:57.021Z",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c531f",
          "name": "Yirui Yuan",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5320",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5321",
          "user": {
            "_id": "637745113a63a2983ffbde13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
            "isPro": false,
            "fullname": "Haofan Wang",
            "user": "wanghaofan",
            "type": "user"
          },
          "name": "Haofan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:54.821Z",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5322",
          "name": "Jiaming Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T08:07:17.000Z",
      "title": "EasyControl: Sistema que agrega un control eficiente y flexible a modelos de difusión\nTransformer",
      "summary": "Recientemente, el desarrollo de modelos de difusión basados en Unet ha introducido estructuras efectivas de control espacial y temática, como ControlNet y IP-Adapter. Sin embargo, la arquitectura DiT (Diffusion Transformer) enfrenta desafíos en términos de eficiencia y flexibilidad de control. Para resolver estos problemas, se propone el nuevo marco de trabajo EasyControl. Este marco integra un transformer de difusión inducido por condiciones, ofreciendo alta eficiencia y flexibilidad. Se basa en tres innovaciones clave: primero, el módulo de Inyección de Condiciones con Round Weights LoRA, que procesa señales de condición separadamente y funciona como solución plug-in y play-in, garantizando la compatibilidad con modelos de usuario y permitiendo la inyección flexible de diversas condiciones. Además, soporta la expansión de 0-shot multi-condition con harmonía y fuerza, incluso con solo un dato de condición. Segundo, se propone el Paradigma de Entrenamiento Conjunto de Posición, que normaliza las condiciones de entrada en una región estándar, permitiendo la generación de imágenes con cualquier proporción y región flexible. Este enfoque optimiza la eficiencia de cálculo y ofrece una aplicabilidad práctica en aplicaciones reales. Finalmente, se aplica la mecánica de atención causal y la tecnología de caché KV en tareas de generación condicionada, reduciendo significativamente la latencia de síntesis de imágenes y mejorando la eficiencia del marco completo. En experimentos extendidos, EasyControl muestra excelentes resultados en diversos escenarios de aplicación. Estas innovaciones comparten la alta eficiencia, flexibilidad y características adecuadas para diversas tareas del marco.",
      "upvotes": 15,
      "discussionId": "67cf990359dbba733d8c545d",
      "ai_keywords": [
        "Unet-based diffusion models",
        "ControlNet",
        "IP-Adapter",
        "DiT (Diffusion Transformer)",
        "Condition Injection LoRA Module",
        "Condition Injection",
        "zero-shot multi-condition generalization",
        "Position-Aware Training Paradigm",
        "Position-Aware",
        "Causal Attention Mechanism",
        "KV Cache",
        "conditional generation tasks",
        "image synthesis"
      ]
    },
    "publishedAt": "2025-03-10T04:07:17.000Z",
    "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
    "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07027.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07602",
      "authors": [
        {
          "_id": "67cfb2efb77bc8e7d415f904",
          "name": "Yujie Wei",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f905",
          "name": "Shiwei Zhang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f906",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:32.780Z",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f907",
          "name": "Biao Gong",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f908",
          "user": {
            "_id": "6492a0d8d4ae24c933ace44d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DXIky2sdPwmiCOR9p-JBQ.png",
            "isPro": false,
            "fullname": "Longxiang Tang",
            "user": "lloong",
            "type": "user"
          },
          "name": "Longxiang Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:30.700Z",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f909",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90a",
          "name": "Haonan Qiu",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90b",
          "name": "Hengjia Li",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90c",
          "name": "Shuai Tan",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90d",
          "name": "Yingya Zhang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90e",
          "name": "Hongming Shan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:03.000Z",
      "title": "DreamRelation: Centro de Relaciones: Servicio de Video Personalizado",
      "summary": "La creación de vídeos de relaciones Cascade es un trabajo importante para entender las relaciones entre dos temas específicos en contenidos visuales. Los métodos existentes pueden especializar la apariencia y el comportamiento de los temas, pero no se adaptan a los complejos vídeos de relaciones de Cascade. Es crucial tener un modelo de relaciones preciso y una alta generalización de categorías de temas. Una de las principales dificultades es la configuración compleja de espacios, cambios de orden y movimientos temporales en las relaciones. Actualmente, los modelos no detectan interacciones significativas y enfatizan demasiado en detalles visuales irrelevantes de las relaciones. Para resolver estos problemas, proponemos DreamRelation. DreamRelation es una nueva aproximación para especializar relaciones utilizando pequeños videos de muestra. Utiliza dos componentes principales: el aprendizaje de decodificación de relaciones y la arquitectura dinámica de relaciones. En el aprendizaje de decodificación de relaciones, utilizamos un estrategia de entrenamiento con tuplas de rotadores de relaciones y máscaras combinadas para separar las relaciones de la apariencia de los temas y garantizar una mejor generalización en diferentes relaciones. Además, analizamos los roles diferentes de la estructura de acciones de MM-DiT para optimizar el diseño de las tuplas de rotadores de relaciones, lo que hace de DreamRelation un primer marco de trabajo explicable para la generación de vídeos de primera relación. En la arquitectura dinámica de relaciones, introducimos un pérdida de comparación espacial de relaciones para priorizar la dinámica de las relaciones y evitar depender de la apariencia detallada de los temas. Los resultados de validación de extensión muestran que DreamRelation supera los métodos estado de la arte en la representación de vídeos de relaciones de Cascade. El código y el modelo están disponibles públicamente.",
      "upvotes": 10,
      "discussionId": "67cfb2f1b77bc8e7d415f96b",
      "ai_keywords": [
        "Relational Decoupling Learning",
        "Relational Dynamics Enhancement",
        "relation LoRA triplet",
        "hybrid mask training strategy",
        "attention mechanism",
        "space-time relational contrastive loss",
        "MM-DiT"
      ]
    },
    "publishedAt": "2025-03-10T13:58:03.000Z",
    "title": "DreamRelation: Relation-Centric Video Customization",
    "summary": "Relational video customization refers to the creation of personalized videos\nthat depict user-specified relations between two subjects, a crucial task for\ncomprehending real-world visual content. While existing methods can personalize\nsubject appearances and motions, they still struggle with complex relational\nvideo customization, where precise relational modeling and high generalization\nacross subject categories are essential. The primary challenge arises from the\nintricate spatial arrangements, layout variations, and nuanced temporal\ndynamics inherent in relations; consequently, current models tend to\noveremphasize irrelevant visual details rather than capturing meaningful\ninteractions. To address these challenges, we propose DreamRelation, a novel\napproach that personalizes relations through a small set of exemplar videos,\nleveraging two key components: Relational Decoupling Learning and Relational\nDynamics Enhancement. First, in Relational Decoupling Learning, we disentangle\nrelations from subject appearances using relation LoRA triplet and hybrid mask\ntraining strategy, ensuring better generalization across diverse relationships.\nFurthermore, we determine the optimal design of relation LoRA triplet by\nanalyzing the distinct roles of the query, key, and value features within\nMM-DiT's attention mechanism, making DreamRelation the first relational video\ngeneration framework with explainable components. Second, in Relational\nDynamics Enhancement, we introduce space-time relational contrastive loss,\nwhich prioritizes relational dynamics while minimizing the reliance on detailed\nsubject appearances. Extensive experiments demonstrate that DreamRelation\noutperforms state-of-the-art methods in relational video customization. Code\nand models will be made publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07602.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06580",
      "authors": [
        {
          "_id": "67cfa71827c7f0b2db19f7c2",
          "user": {
            "_id": "645b4a2978730bcc103dfe4d",
            "avatarUrl": "/avatars/de544de899897fd0a83506ff287123bc.svg",
            "isPro": false,
            "fullname": "Yuxiang Zhang",
            "user": "TokerZ",
            "type": "user"
          },
          "name": "Yuxiang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:40.336Z",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c3",
          "name": "Yuqi Yang",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c4",
          "name": "Jiangming Shu",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c5",
          "name": "Xinyan Wen",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c6",
          "name": "Jitao Sang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T12:19:47.000Z",
      "title": "Agent Model: Modelo que encapsula la generación de acciones en razones lógicas continuas",
      "summary": "El flujo de trabajo tradicional de agentes utiliza prompts externos para manejar la interacción entre herramientas y entornos, lo que limita la autonomía de los modelos de inferencia. Proponemos la generación interna de la Cadena de Acciones (CoA) para decidir autónomamente el uso de herramientas externas en el tiempo y métodos elegidos, utilizando LAMs (Modelos de Railz Generative). El marco propuesto, AutoCoA, combina entrenanmiento normal (SFT) y entrenamiento de refuerzo (RL) para permitir que los modelos continúen inferiendo y actuando de manera eficiente, gestionando la interacción con el entorno. Los principales componentes incluyen el trigger de acciones por etapa, la optimización de la CoA a nivel de trayectoria, y el modelo de mundo interno. En evaluaciones para tareas de respuesta a preguntas abiertas, los modelos de agente entrenados con AutoCoA superan significativamente a los flujos de trabajo basados en ReAct, especialmente en tareas que requieren razonamiento a largo plazo y acciones multi-nivel. El código y conjuntos de datos están disponibles en https://github.com/ADaM-BJTU/AutoCoA.",
      "upvotes": 10,
      "discussionId": "67cfa71927c7f0b2db19f817",
      "githubRepo": "https://github.com/ADaM-BJTU/AutoCoA",
      "ai_keywords": [
        "Large Agent Models (LAMs)",
        "Chain-of-Action (CoA)",
        "AutoCoA framework",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "step-level action triggering",
        "trajectory-level CoA optimization",
        "internal world model"
      ]
    },
    "publishedAt": "2025-03-09T08:19:47.000Z",
    "title": "Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models",
    "summary": "Traditional agentic workflows rely on external prompts to manage interactions\nwith tools and the environment, which limits the autonomy of reasoning models.\nWe position Large Agent Models (LAMs) that internalize the generation of\nChain-of-Action (CoA), enabling the model to autonomously decide when\nand how to use external tools. Our proposed AutoCoA framework combines\nsupervised fine-tuning (SFT) and reinforcement learning (RL), allowing the\nmodel to seamlessly switch between reasoning and action while efficiently\nmanaging environment interactions. Main components include step-level action\ntriggering, trajectory-level CoA optimization, and an internal world model to\nreduce real-environment interaction costs. Evaluations on open-domain QA tasks\ndemonstrate that AutoCoA-trained agent models significantly outperform\nReAct-based workflows in task completion, especially in tasks that require\nlong-term reasoning and multi-step actions. Code and dataset are available at\nhttps://github.com/ADaM-BJTU/AutoCoA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06580.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07608",
      "authors": [
        {
          "_id": "67cfa5bcb17ca92d24da9033",
          "user": {
            "_id": "65a4a180c8a09bd5e8e900b8",
            "avatarUrl": "/avatars/c135db68f6ff2c40119acd2e9ddce968.svg",
            "isPro": false,
            "fullname": "Bo Jiang",
            "user": "rb93dett",
            "type": "user"
          },
          "name": "Bo Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:43.665Z",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9034",
          "name": "Shaoyu Chen",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9035",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9036",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9037",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:59:42.000Z",
      "title": "AlphaDrive: Aprendizaje por Refuerzo y Lógica para Liberar los Capacidades de los VLMs en Conducción Autónoma",
      "summary": "OpenAI o1 y DeepSeek R1 desempeñan un papel crucial en las áreas complejas de la matemática y la ciencia, al alcanzar o superar el nivel de rendimiento de un profesional humano a través del aprendizaje por refuerzo (RL) y la lógica. En el campo de la conducción automática, los modelos han mejorado significativamente su capacidad de planificación desde hace poco, pero siguen presentando problemas debido a limitaciones en la conocida y la capacidad lógica, especialmente en la resolución de problemas de colas largas. Algunas investigaciones están integrando modelos de lenguaje visuo-lingüístico (VLMs) para el estudio de conducción automática, pero generalmente dependen de un aprendizaje supervisado simple (SFT) y no experimentan estrategias de optimización adecuadas para el planificación. En este trabajo, proponemos un marco de RL y lógica para VLMs en conducción automática. AlphaDrive introduce cuatro recompensas de RL basadas en GRPO adecuadas para el planificación y utiliza una estrategia de aprendizaje lógico-matemático de dos etapas combinando SFT y RL. Esto ha permitido que AlphaDrive mejore significativamente su rendimiento de planificación y la eficiencia de aprendizaje en comparación con casos donde solo se utiliza SFT o se excluye la lógica. Además, después de la aprendizaje por RL, AlphaDrive ha demostrado la capacidad de descubrir diversas habilidades de planificación que son cruciales para la seguridad y eficiencia del manejo. Reconocemos que nuestras limitaciones de conocimiento impiden que AlphaDrive sea un modelo completo de RL y lógica de planificación para conducción automática. El código se publicará para fomentar futuras investigaciones.",
      "upvotes": 9,
      "discussionId": "67cfa5bdb17ca92d24da9064",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "reasoning",
        "end-to-end models",
        "vision-language models (VLMs)",
        "supervised fine-tuning (SFT)",
        "GRPO-based RL rewards",
        "two-stage planning reasoning training strategy",
        "emergent multimodal planning capabilities"
      ]
    },
    "publishedAt": "2025-03-10T13:59:42.000Z",
    "title": "AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via\n  Reinforcement Learning and Reasoning",
    "summary": "OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level\nperformance in complex domains like mathematics and science, with reinforcement\nlearning (RL) and reasoning playing a crucial role. In autonomous driving,\nrecent end-to-end models have greatly improved planning performance but still\nstruggle with long-tailed problems due to limited common sense and reasoning\nabilities. Some studies integrate vision-language models (VLMs) into autonomous\ndriving, but they typically rely on pre-trained models with simple supervised\nfine-tuning (SFT) on driving data, without further exploration of training\nstrategies or optimizations specifically tailored for planning. In this paper,\nwe propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous\ndriving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning\nand employs a two-stage planning reasoning training strategy that combines SFT\nwith RL. As a result, AlphaDrive significantly improves both planning\nperformance and training efficiency compared to using only SFT or without\nreasoning. Moreover, we are also excited to discover that, following RL\ntraining, AlphaDrive exhibits some emergent multimodal planning capabilities,\nwhich is critical for improving driving safety and efficiency. To the best of\nour knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning\nreasoning into autonomous driving. Code will be released to facilitate future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07608.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05244",
      "authors": [
        {
          "_id": "67cfebe18a4265f3656a50aa",
          "user": {
            "_id": "642d430a7f9efee76b8713c0",
            "avatarUrl": "/avatars/4981f166a6df8e2ea60cd4c41c2f44d4.svg",
            "isPro": false,
            "fullname": "YuningWu",
            "user": "AQuarterMile",
            "type": "user"
          },
          "name": "Yuning Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:24:29.900Z",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ab",
          "name": "Jiahao Mei",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ac",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ad",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ae",
          "name": "SHaopeng Lai",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50af",
          "name": "Yuran Ren",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b0",
          "name": "Zijia Wang",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b1",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b2",
          "name": "Mengyue Wu",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b3",
          "name": "Qin Jin",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b4",
          "name": "Fei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T08:56:20.000Z",
      "title": "WritingBench: Marca de Prueba para Imágenes Complejas de Generadores",
      "summary": "El reciente desarrollo de los grandes modelos de lenguaje (LLMs) ha aumentado significativamente la capacidad de generación de texto, pero la evaluación del rendimiento de las frases generadas es un problema complejo. Los actuales marcos de referencia se centran principalmente en la generación de texto general o en tareas específicas de frases, pero no comprenden las exigencias de contenido de alta calidad en diversas formas de frases. Para resolver esto, presentamos WritingBench. Este marco de referencia consta de 6 direcciones clave y 100 subdirecciones detalladas, incluyendo frases creativas y explicativas, informativas y técnicas. Además, proponemos un marco de evaluación que depende en preguntas que LLMs pueden generar dinamicamente para evaluar criterios de evaluación propios. Este marco de evaluación se complementa con modelos de evaluación que permiten la evaluación de estilo, formato y longitud. La efectividad de este marco de evaluación se demuestra al mostrar que un modelo de 7B parámetros se acerca a los rendimientos más recientes (SOTA). Publicamos este marco de referencia, herramientas de evaluación y componentes modulares del marco de evaluación para contribuir al desarrollo de frases en LLMs.",
      "upvotes": 9,
      "discussionId": "67cfebe38a4265f3656a5136",
      "githubRepo": "https://github.com/X-PLUG/WritingBench",
      "ai_keywords": [
        "large language models (LLMs)",
        "text generation",
        "generative writing",
        "benchmarks",
        "writing domains",
        "subdomains",
        "creative writing",
        "persuasive writing",
        "informative writing",
        "technical writing",
        "query-dependent evaluation framework",
        "instance-specific assessment criteria",
        "critic model",
        "criteria-aware scoring",
        "data curation"
      ]
    },
    "publishedAt": "2025-03-07T03:56:20.000Z",
    "title": "WritingBench: A Comprehensive Benchmark for Generative Writing",
    "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05244.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04629",
      "authors": [
        {
          "_id": "67cfbab6607797f40c6d4164",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4165",
          "name": "Shiyang Feng",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4166",
          "name": "Jiakang Yuan",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4167",
          "name": "Renqiu Xia",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4168",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4169",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d416a",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T17:15:48.000Z",
      "title": "Slide Poering: Offline Heuristic, Memory-Driven Generation, and Multi-Dimensional Evaluation of Automated Report Writing",
      "summary": "Los artículos de revisión desempeñan un papel crucial en la investigación científica, especialmente en el contexto del rápido aumento de los artículos de investigación. Recientemente, los investigadores están dedicando esfuerzos para automatizar la generación de artículos de revisión y mejorar la eficiencia de la investigación utilizando Modelos de Lenguaje de Alto Nivel (LLM). Sin embargo, hay diferencias claras en la calidad de los artículos de revisión generados por LLM y los escritos por humanos, especialmente en la calidad de los enfoques y la precisión de las citas. Para mitigar estas diferencias, presentamos SurveyForge. SurveyForge analiza la estructura lógica de los enfoques humanos y genera artículos de revisión basándose en artículos de la misma área de investigación. Luego, utilizamos nuestro navegador académico para generar y mejorar el contenido de los artículos generados a través de artículos de alta calidad encontrados. Además, para realizar una evaluación global, hemos construido SurveyBench, que incluye 100 artículos de revisión escritos por humanos y compara la eficacia de los artículos generados por AI en términos de calidad de los enfoques y contenido, evaluándolos en tres dimensiones. Los experimentos muestran que SurveyForge produce resultados superiores a los anteriores.",
      "upvotes": 9,
      "discussionId": "67cfbab9607797f40c6d4206",
      "ai_keywords": [
        "LLMs (Large Language Models)",
        "SurveyForge",
        "SurveyBench",
        "AutoSurvey"
      ]
    },
    "publishedAt": "2025-03-06T12:15:48.000Z",
    "title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing",
    "summary": "Survey paper plays a crucial role in scientific research, especially given\nthe rapid growth of research publications. Recently, researchers have begun\nusing LLMs to automate survey generation for better efficiency. However, the\nquality gap between LLM-generated surveys and those written by human remains\nsignificant, particularly in terms of outline quality and citation accuracy. To\nclose these gaps, we introduce SurveyForge, which first generates the outline\nby analyzing the logical structure of human-written outlines and referring to\nthe retrieved domain-related articles. Subsequently, leveraging high-quality\npapers retrieved from memory by our scholar navigation agent, SurveyForge can\nautomatically generate and refine the content of the generated article.\nMoreover, to achieve a comprehensive evaluation, we construct SurveyBench,\nwhich includes 100 human-written survey papers for win-rate comparison and\nassesses AI-generated survey papers across three dimensions: reference,\noutline, and content quality. Experiments demonstrate that SurveyForge can\noutperform previous works such as AutoSurvey.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04629.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04812",
      "authors": [
        {
          "_id": "67ce5542818e1825dea7440b",
          "user": {
            "_id": "6626449503e1f561573d30e9",
            "avatarUrl": "/avatars/e7f9720ccd01bae32d0a03a1b0dacab5.svg",
            "isPro": false,
            "fullname": "Zhibin Lan",
            "user": "zhibinlan",
            "type": "user"
          },
          "name": "Zhibin Lan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:54.535Z",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440c",
          "user": {
            "_id": "635239137d071f23d083b056",
            "avatarUrl": "/avatars/1f1a0ed38d8de499d4b78922801c6d95.svg",
            "isPro": false,
            "fullname": "liqiang niu",
            "user": "lqniu",
            "type": "user"
          },
          "name": "Liqiang Niu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:51.713Z",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440d",
          "name": "Fandong Meng",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440e",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440f",
          "name": "Jinsong Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T10:21:57.000Z",
      "title": "LLAVE: Comparación de Entrenamiento Ponderado con Modelos Intrínsecos de Lenguaje y Visión",
      "summary": "Los modelos de embedding multimodal universales desempeñan un papel crítico en tareas como la recuperación intercalada de imágenes y texto, la RAG multimodal y la agrupación multimodal. Sin embargo, nuestros resultados empíricos indican que los modelos de embedding basados en LMM entrenados con la pérdida estándar InfoNCE presentan una alta superposición en la distribución de similitud entre parejas positivas y negativas, lo que dificulta la distinción efectiva de parejas negativas difíciles. Para abordar este problema, proponemos un marco sencillo pero efectivo que mejora dinámicamente el aprendizaje de representación del modelo de embedding para las parejas negativas basándose en su dificultad discriminativa. Dentro de este marco, entrenamos una serie de modelos llamados LLaVE y evaluamoslos en el benchmark MMEB, que cubre 4 tareas meta y 36 conjuntos de datos. Los resultados experimentales muestran que LLaVE establece bases más fuertes que alcanzan el rendimiento estado de la arte (SOTA) mientras demostrarán gran escalabilidad y eficiencia. Específicamente, LLaVE-2B supera los modelos SOTA de 7B anteriores, mientras que LLaVE-7B logra una mejora en el rendimiento adicional de 6.2 puntos. A pesar de que LLaVE se entrena en datos de imágenes y texto, puede generalizarse a tareas de recuperación de texto-video de manera zero-shot y lograr un rendimiento fuerte, demostrando su potencial notable para transferencia a otros tareas de embedding.",
      "upvotes": 9,
      "discussionId": "67ce5543818e1825dea74480",
      "githubRepo": "https://github.com/DeepLearnXMU/LLaVE",
      "ai_keywords": [
        "multimodal embedding models",
        "interleaved image-text retrieval",
        "multimodal RAG",
        "multimodal clustering",
        "LMM-based embedding models",
        "InfoNCE loss",
        "similarity distribution",
        "hard negative pairs",
        "representation learning",
        "LLaVE",
        "MMEB benchmark",
        "meta-tasks",
        "datasets",
        "state-of-the-art (SOTA)",
        "scalability",
        "efficiency",
        "text-video retrieval tasks",
        "zero-shot manner",
        "transfer"
      ]
    },
    "publishedAt": "2025-03-04T05:21:57.000Z",
    "title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted\n  Contrastive Learning",
    "summary": "Universal multimodal embedding models play a critical role in tasks such as\ninterleaved image-text retrieval, multimodal RAG, and multimodal clustering.\nHowever, our empirical results indicate that existing LMM-based embedding\nmodels trained with the standard InfoNCE loss exhibit a high degree of overlap\nin similarity distribution between positive and negative pairs, making it\nchallenging to distinguish hard negative pairs effectively. To deal with this\nissue, we propose a simple yet effective framework that dynamically improves\nthe embedding model's representation learning for negative pairs based on their\ndiscriminative difficulty. Within this framework, we train a series of models,\nnamed LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks\nand 36 datasets. Experimental results show that LLaVE establishes stronger\nbaselines that achieve state-of-the-art (SOTA) performance while demonstrating\nstrong scalability and efficiency. Specifically, LLaVE-2B surpasses the\nprevious SOTA 7B models, while LLaVE-7B achieves a further performance\nimprovement of 6.2 points. Although LLaVE is trained on image-text data, it can\ngeneralize to text-video retrieval tasks in a zero-shot manner and achieve\nstrong performance, demonstrating its remarkable potential for transfer to\nother embedding tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04812.png",
    "numComments": 2,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07459",
      "authors": [
        {
          "_id": "67cfd1934fed2b7e3e4cbb34",
          "user": {
            "_id": "63357c608adfa81faf2ac180",
            "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
            "isPro": false,
            "fullname": "Xiangru Tang",
            "user": "RTT1",
            "type": "user"
          },
          "name": "Xiangru Tang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T06:00:52.457Z",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb35",
          "name": "Daniel Shao",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb36",
          "name": "Jiwoong Sohn",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb37",
          "name": "Jiapeng Chen",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb38",
          "name": "Jiayi Zhang",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb39",
          "name": "Jinyu Xiang",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3a",
          "name": "Fang Wu",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3b",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3c",
          "name": "Chenglin Wu",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3d",
          "user": {
            "_id": "65cae89119683f9817c049ea",
            "avatarUrl": "/avatars/b08b10d7c72e2cf1108147e659411b32.svg",
            "isPro": false,
            "fullname": "Wenqi Shi",
            "user": "wshi83",
            "type": "user"
          },
          "name": "Wenqi Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:16.321Z",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3e",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3f",
          "name": "Mark Gerstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T15:38:44.000Z",
      "title": "MedAgentsBench: Modelo de pensamiento y marco de agentes para evaluaciones de ética médica complejas",
      "summary": "Los modelos de lenguaje grande (LLMs) han demostrado un desempeño sorprendente en la evaluación de preguntas médicas actuales. Esta alta eficiencia ha dificultado la diferenciación clara entre evaluaciones significativas y desarrollos avanzados. Aunque nuestros modelos muestran un excelente desempeño en pruebas estándar, se enfrentan desafíos para centrarse en la constitución de diagnósticos, planes de tratamiento y escenarios clínicos multifacéticos. Actualmente, los modelos muestran un desempeño fuerte en pruebas estándar, pero falta una centración en la constitución de diagnósticos, planes de tratamiento y escenarios clínicos multifacéticos.",
      "upvotes": 7,
      "discussionId": "67cfd1944fed2b7e3e4cbb81",
      "ai_keywords": [
        "MedAgentsBench",
        "multi-step clinical reasoning",
        "diagnosis formulation",
        "treatment planning",
        "MedAgentsBench",
        "DeepSeek R1",
        "OpenAI o3",
        "advanced search-based agent methods"
      ]
    },
    "publishedAt": "2025-03-10T11:38:44.000Z",
    "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning",
    "summary": "Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https://github.com/gersteinlab/medagents-benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07459.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06749",
      "authors": [
        {
          "_id": "67cfb6495944a8e54f24cd9a",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9b",
          "name": "Bohan Jia",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9c",
          "name": "Zijie Zhai",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9d",
          "name": "Shaosheng Cao",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9e",
          "name": "Zheyu Ye",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9f",
          "name": "Fei Zhao",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cda0",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cda1",
          "name": "Shaohui Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T20:06:45.000Z",
      "title": "Visión R1: Modelo que fomenta la capacidad de los modelos de lenguaje multilingües para entender y procesar varios idiomas.",
      "summary": "DeepSeek-R1-Zero ha logrado descubrir las capacidades cognitivas lógicas de un modelo de lenguaje de máquina (LLM) a través del aprendizaje por refuerzo (RL). Este innovador enfoque investigó si el aprendizaje por refuerzo puede mejorar las capacidades cognitivas lógicas de un MLLM. Sin embargo, se reconoció que el aprendizaje directo por refuerzo es difícil para activar complejas capacidades cognitivas lógicas en un MLLM (por ejemplo, preguntas y reflexión). Para resolver este problema, se propone un MLLM lógico, Vision-R1. Específicamente, se construyó un alto rendimiento dataset lógico diverso utilizando modelo intercambio y filtrado de datos, junto con 200K de datos lógicos diversos y el dataset Vision-R1-cold. Este dataset se utiliza como datos iniciales para Vision-R1. Se propone una estrategia de control de pensamiento progresivo (PTST) para mitigar los problemas de optimización excesivo en los primeros pasos, y se entrena gradualmente la capacidad de aprendizaje de procesos lógicos complejos utilizando un dataset de 10K de datos matemáticos diversos, con el grupo de políticas de optimización (GRPO) y una función de recompensa formalizada estricta. Los experimentos detallados demostraron un aumento promedio de 6% en los benchmarks de matemáticas lógicas. Vision-R1-7B alcanzó una precisión del 73.5% en el benchmark MathVista, mostrando el nivel más alto entre los modelos lógicos, con un rendimiento 0.4% inferior a OpenAI O1. Los datasets y códigos están disponibles en la URL siguiente: https://github.com/Osilly/Vision-R1",
      "upvotes": 7,
      "discussionId": "67cfb64f5944a8e54f24cf33",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "MLLMs",
        "multimodal reasoning",
        "CoT dataset",
        "modality bridging",
        "data filtering",
        "Vision-R1-cold dataset",
        "Progressive Thinking Suppression Training (PTST)",
        "Group Relative Policy Optimization (GRPO)",
        "hard formatting result reward function",
        "multimodal math dataset",
        "MathVista benchmark",
        "OpenAI O1"
      ]
    },
    "publishedAt": "2025-03-09T16:06:45.000Z",
    "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models",
    "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning\ncapabilities in LLMs purely through Reinforcement Learning (RL). Inspired by\nthis breakthrough, we explore how RL can be utilized to enhance the reasoning\ncapability of MLLMs. However, direct training with RL struggles to activate\ncomplex reasoning capabilities such as questioning and reflection in MLLMs, due\nto the absence of substantial high-quality multimodal reasoning data. To\naddress this issue, we propose the reasoning MLLM, Vision-R1, to improve\nmultimodal reasoning capability. Specifically, we first construct a\nhigh-quality multimodal CoT dataset without human annotations by leveraging an\nexisting MLLM and DeepSeek-R1 through modality bridging and data filtering to\nobtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as\ncold-start initialization data for Vision-R1. To mitigate the optimization\nchallenges caused by overthinking after cold start, we propose Progressive\nThinking Suppression Training (PTST) strategy and employ Group Relative Policy\nOptimization (GRPO) with the hard formatting result reward function to\ngradually refine the model's ability to learn correct and complex reasoning\nprocesses on a 10K multimodal math dataset. Comprehensive experiments show our\nmodel achieves an average improvement of sim6% across various multimodal\nmath reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely\nused MathVista benchmark, which is only 0.4% lower than the leading reasoning\nmodel, OpenAI O1. The datasets and code will be released in:\nhttps://github.com/Osilly/Vision-R1 .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06749.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07507",
      "authors": [
        {
          "_id": "67cfa44c3a9d50150f59ffe1",
          "name": "Jie Hu",
          "hidden": false
        },
        {
          "_id": "67cfa44c3a9d50150f59ffe2",
          "name": "Shizun Wang",
          "hidden": false
        },
        {
          "_id": "67cfa44c3a9d50150f59ffe3",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T16:29:10.000Z",
      "title": "PE3R: Eficiencia de la Observación en Reconstrucción 3D",
      "summary": "El desarrollo reciente del reconocimiento 3D a partir de imágenes 2D ha mejorado significativamente la comprensión de escenas 3D a partir de imágenes 2D. Sin embargo, los métodos actuales presentan importantes problemas, como la generalización limitada entre escenas, la disminución de la precisión del reconocimiento y la lentidumbre en la reconstrucción, entre otros. Para resolver estos limitaciones, proponemos un nuevo marco de trabajo \"Perception-Efficient 3D Reconstruction (PE3R)\" que logra al mismo tiempo precisión y eficiencia. PE3R utiliza una arquitectura proactiva que permite la reconstrucción rápida de campos de significado 3D. Este marco de trabajo muestra una fuerte generalización 0-shot para diferentes escenas y objetos, y mejora significativamente la velocidad de reconstrucción. Experimentos de división de vectores abiertos en 2D y reconstrucción 3D han demostrado los efectos y la amplia aplicabilidad de PE3R. Este marco de trabajo ha alcanzado una velocidad de reconstrucción de campos de significado 3D más de 9 veces más rápido, mostrando también un gran impacto en la precisión del reconocimiento y la reconstrucción, estableciendo nuevos estándares en el campo. El código está disponible en https://github.com/hujiecpp/PE3R.",
      "upvotes": 5,
      "discussionId": "67cfa4503a9d50150f5a0137",
      "ai_keywords": [
        "feed-forward architecture",
        "3D semantic field reconstruction",
        "zero-shot generalization",
        "2D-to-3D open-vocabulary segmentation",
        "perception accuracy",
        "reconstruction precision",
        "speedup"
      ]
    },
    "publishedAt": "2025-03-10T12:29:10.000Z",
    "title": "PE3R: Perception-Efficient 3D Reconstruction",
    "summary": "Recent advancements in 2D-to-3D perception have significantly improved the\nunderstanding of 3D scenes from 2D images. However, existing methods face\ncritical challenges, including limited generalization across scenes, suboptimal\nperception accuracy, and slow reconstruction speeds. To address these\nlimitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel\nframework designed to enhance both accuracy and efficiency. PE3R employs a\nfeed-forward architecture to enable rapid 3D semantic field reconstruction. The\nframework demonstrates robust zero-shot generalization across diverse scenes\nand objects while significantly improving reconstruction speed. Extensive\nexperiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction\nvalidate the effectiveness and versatility of PE3R. The framework achieves a\nminimum 9-fold speedup in 3D semantic field reconstruction, along with\nsubstantial gains in perception accuracy and reconstruction precision, setting\nnew benchmarks in the field. The code is publicly available at:\nhttps://github.com/hujiecpp/PE3R.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07507.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07197",
      "authors": [
        {
          "_id": "67cfa76cf36e4221c5009654",
          "user": {
            "_id": "624f909eac5dd186b01ac3f5",
            "avatarUrl": "/avatars/71a5c93c491064ef9e1eda80fda90665.svg",
            "isPro": false,
            "fullname": "Zebin You",
            "user": "yyyou",
            "type": "user"
          },
          "name": "Zebin You",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:38.059Z",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009655",
          "name": "Jingyang Ou",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009656",
          "name": "Xiaolu Zhang",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009657",
          "name": "Jun Hu",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009658",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009659",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T11:27:12.000Z",
      "title": "Modelo efectivo y eficiente para la generación de imágenes de máscaras",
      "summary": "Los modelos de codificación oculta y los modelos de difusión oculta, aunque diseñados con motivos y objetivos diferentes, podemos integrarlos en un solo marco. Desde esta perspectiva, investigamos el espacio de diseño de entrenamiento y muestreo, identificando los factores que contribuyen a la eficiencia y el rendimiento. Basándonos en estas mejoras, desarrollamos nuestro modelo como eMIGM (Enhanced Mixture of Implicit Generative Models). Experimentalmente, eMIGM demostró un fuerte impacto en la generación de ImageNet. En particular, en ImageNet 256x256, utilizando un número similar de evaluaciones de función (NFEs) y un número de parámetros del modelo, eMIGM superó a los modelos más recientes de VAR. Mientras el número de NFEs y parámetros del modelo aumenta, eMIGM logra un rendimiento similar a los modelos de difusión continua, pero requiere menos de 40% de NFEs. Además, en ImageNet 512x512, con una reducción de aproximadamente 60% de NFEs, eMIGM superó a los modelos de difusión continua.",
      "upvotes": 5,
      "discussionId": "67cfa76df36e4221c5009686",
      "ai_keywords": [
        "masked image generation models",
        "masked diffusion models",
        "training and sampling",
        "Fr\\'echet Inception Distance (FID)",
        "function evaluations (NFEs)",
        "VAR",
        "continuous diffusion models"
      ]
    },
    "publishedAt": "2025-03-10T07:27:12.000Z",
    "title": "Effective and Efficient Masked Image Generation Models",
    "summary": "Although masked image generation models and masked diffusion models are\ndesigned with different motivations and objectives, we observe that they can be\nunified within a single framework. Building upon this insight, we carefully\nexplore the design space of training and sampling, identifying key factors that\ncontribute to both performance and efficiency. Based on the improvements\nobserved during this exploration, we develop our model, referred to as eMIGM.\nEmpirically, eMIGM demonstrates strong performance on ImageNet generation, as\nmeasured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet\n256x256, with similar number of function evaluations (NFEs) and model\nparameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model\nparameters increase, eMIGM achieves performance comparable to the\nstate-of-the-art continuous diffusion models while requiring less than 40% of\nthe NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE,\neMIGM outperforms the state-of-the-art continuous diffusion models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07197.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06520",
      "authors": [
        {
          "_id": "67cf990ca80a73999cc816c3",
          "name": "Yuqi Liu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c4",
          "name": "Bohao Peng",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c5",
          "name": "Zhisheng Zhong",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c6",
          "name": "Zihao Yue",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c7",
          "name": "Fanbin Lu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c8",
          "name": "Bei Yu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c9",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T08:48:51.000Z",
      "title": "Seg-Zero: Conexión de Razones mediante Fortalecimiento Cerebral: Guia de División",
      "summary": "Los métodos tradicionales de división de inferencia utilizan etiquetas de clasificación y breves explicaciones para depender de ajustes micro-regulados y estándarizados, limitando la capacidad de generalización fuera de los dominios y presentando una falta de procesos de inferencia explícitos. Para resolver estos limites, proponemos un nuevo marco de trabajo llamado Seg-Zero. Este marco de trabajo muestra una capacidad de generalización notable y permite obtener cadenas de inferencia explícitas a través de fortalecimiento cognitivo. Seg-Zero introduce una arquitectura decoder-encoder compuesta por modelos de razonamiento y modelos de división. El modelo de razonamiento entiende el propósito del usuario y genera cadenas de inferencia explícitas, mientras que el modelo de división genera máscaras a nivel de píxel utilizando fronteras de posición. Hemos diseñado una estructura de recompensa compleja que incluye precisión y formalidad, y hemos entrenado de manera especial utilizando GRPO, lo que permite a Seg-Zero demostrar una potente capacidad de generalización de 0-shot sin necesidad de datos de inferencia explícitos. Las pruebas muestran que Seg-Zero-7B tiene un rendimiento de 57.5 en Zero-shot, superando a LISA-7B en más de 18%, demostrando un impresionante aumento que demuestra la capacidad de Seg-Zero para generalizar fuera de los dominios y proporcionar procesos de inferencia explícitos. El código está disponible en https://github.com/dvlab-research/Seg-Zero.",
      "upvotes": 5,
      "discussionId": "67cf990da80a73999cc81723",
      "ai_keywords": [
        "Seg-Zero",
        "decoupled architecture",
        "reasoning model",
        "segmentation model",
        "positional prompts",
        "pixel-level masks",
        "cognitive reinforcement",
        "reward mechanism",
        "reinforcement learning",
        "GRPO",
        "zero-shot generalization",
        "ReasonSeg benchmark",
        "emergent test-time reasoning"
      ]
    },
    "publishedAt": "2025-03-09T04:48:51.000Z",
    "title": "Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement",
    "summary": "Traditional methods for reasoning segmentation rely on supervised fine-tuning\nwith categorical labels and simple descriptions, limiting its out-of-domain\ngeneralization and lacking explicit reasoning processes. To address these\nlimitations, we propose Seg-Zero, a novel framework that demonstrates\nremarkable generalizability and derives explicit chain-of-thought reasoning\nthrough cognitive reinforcement. Seg-Zero introduces a decoupled architecture\nconsisting of a reasoning model and a segmentation model. The reasoning model\ninterprets user intentions, generates explicit reasoning chains, and produces\npositional prompts, which are subsequently used by the segmentation model to\ngenerate precious pixel-level masks. We design a sophisticated reward mechanism\nthat integrates both format and accuracy rewards to effectively guide\noptimization directions. Trained exclusively via reinforcement learning with\nGRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot\ngeneralization and exhibits emergent test-time reasoning capabilities.\nExperiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on\nthe ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant\nimprovement highlights Seg-Zero's ability to generalize across domains while\npresenting an explicit reasoning process. Code is available at\nhttps://github.com/dvlab-research/Seg-Zero.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06520.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06121",
      "authors": [
        {
          "_id": "67cfa436d37b8309603da1ee",
          "user": {
            "_id": "66de61d7174e9c6971dbb253",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sM0xfS7HAkf_6GmkEGjDk.png",
            "isPro": false,
            "fullname": "Alic Li",
            "user": "Alic-Li",
            "type": "user"
          },
          "name": "Li weile",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T04:29:57.207Z",
          "hidden": false
        },
        {
          "_id": "67cfa436d37b8309603da1ef",
          "user": {
            "_id": "6176b32847ee6431f632981e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg",
            "isPro": false,
            "fullname": "IvanD",
            "user": "xiaol",
            "type": "user"
          },
          "name": "Liu Xiao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T03:33:06.087Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-08T08:31:18.000Z",
      "title": "Black Ghost Rimmer: Utiliza RWKV-7 como un sustituto simple y excelente de Transformers, y es un método adecuado para modelos de secuencias temporales de gran escala.",
      "summary": "Los modelos de series temporales plantean problemas cruciales para procesar grandes conjuntos de datos complejos, lo que es necesario para alcanzar la escala de modelos de lenguaje grandes como los LLMs. Debido a las características de los datos de series temporales y las exigencias computacionales del tamaño del modelo, se necesitan nuevas aproximaciones. Los investigadores han explorado arquitecturas como Transformers, LSTMs y GRUs para abordar estos desafíos, pero proponen ahora una nueva solución introduciendo meta-aprendizaje con RWKV-7. Integrando los componentes de mixeo temporal y mixeo de canales de RWKV-7 en un modelo de series temporal basado en Transformers llamado Timer, se logró mejorar el rendimiento en un factor de aproximadamente 1.13 a 43.3 con un número de parámetros de 1/23, mientras que reducía el tiempo de entrenamiento en un factor de 4.5. Nuestro código y los pesos del modelo están disponibles en https://github.com/Alic-Li/BlackGoose_Rimer y pueden contribuir a la evolución de la investigación y el desarrollo.",
      "upvotes": 5,
      "discussionId": "67cfa437d37b8309603da253",
      "ai_keywords": [
        "Transformers",
        "LSTMs",
        "GRUs",
        "RWKV-7",
        "meta-learning",
        "state update mechanism",
        "time mix",
        "channel mix",
        "Timer",
        "performance improvement",
        "training time",
        "parameters"
      ]
    },
    "publishedAt": "2025-03-08T03:31:18.000Z",
    "title": "BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement\n  for Transformers in Large-Scale Time Series Modeling",
    "summary": "Time series models face significant challenges in scaling to handle large and\ncomplex datasets, akin to the scaling achieved by large language models (LLMs).\nThe unique characteristics of time series data and the computational demands of\nmodel scaling necessitate innovative approaches. While researchers have\nexplored various architectures such as Transformers, LSTMs, and GRUs to address\nthese challenges, we propose a novel solution using RWKV-7, which incorporates\nmeta-learning into its state update mechanism. By integrating RWKV-7's time mix\nand channel mix components into the transformer-based time series model Timer,\nwe achieve a substantial performance improvement of approximately 1.13 to 43.3x\nand a 4.5x reduction in training time with 1/23 parameters, all while utilizing\nfewer parameters. Our code and model weights are publicly available for further\nresearch and development at https://github.com/Alic-Li/BlackGoose_Rimer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06121.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03499",
      "authors": [
        {
          "_id": "67cb02680a2a716f25805cb4",
          "name": "Wonjun Kang",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb5",
          "name": "Kevin Galim",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb6",
          "name": "Yuchen Zeng",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb7",
          "name": "Minjae Lee",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb8",
          "name": "Hyung Il Koo",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb9",
          "name": "Nam Ik Cho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T13:44:42.000Z",
      "title": "Estado Offset Tuning: Modelo de Espacio de Estado para la Eficiente Ajuste de Parámetros Basado en Estado",
      "summary": "Los modelos de espacio de estados (SSMs) se convirtieron en métodos más eficientes para reducir los costos de cálculo del Transformer. Sin embargo, la aplicación de métodos de fine-tuning eficiente en parámetros (PEFT) para SSMs ha sido poco investigada. En particular, la Tuning de Prompt y la Tuning de Prefix, que son ampliamente utilizados en Transformers, no muestran excelentes resultados en SSMs. En respuesta a esto, proponemos un método basado en estados que puede ser una buena alternativa al Tuning de Prompt. Esta nueva familia de métodos nace naturalmente de las características estructurales de los SSMs. El método basado en estados no depende de prompts externos y ajusta directamente las características relacionadas con el estado. Además, presentamos un nuevo método de PEFT basado en estados llamado State-offset Tuning. Este método afecta directamente el estado actual en cada etapa temporal. Demostramos la eficiencia de nuestro método a través de experimentos con diferentes conjuntos de datos. El código está disponible en https://github.com/furiosa-ai/ssm-state-tuning.",
      "upvotes": 3,
      "discussionId": "67cb02690a2a716f25805cfd",
      "githubRepo": "https://github.com/furiosa-ai/ssm-state-tuning",
      "ai_keywords": [
        "State Space Models (SSMs)",
        "Parameter-Efficient Fine-Tuning (PEFT)",
        "Prompt Tuning",
        "Prefix-Tuning",
        "State-based methods",
        "State-offset Tuning",
        "timesteps",
        "state-related features",
        "state-at-the-current-step"
      ]
    },
    "publishedAt": "2025-03-05T08:44:42.000Z",
    "title": "State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models",
    "summary": "State Space Models (SSMs) have emerged as efficient alternatives to\nTransformers, mitigating their quadratic computational cost. However, the\napplication of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains\nlargely unexplored. In particular, prompt-based methods like Prompt Tuning and\nPrefix-Tuning, which are widely used in Transformers, do not perform well on\nSSMs. To address this, we propose state-based methods as a superior alternative\nto prompt-based methods. This new family of methods naturally stems from the\narchitectural characteristics of SSMs. State-based methods adjust state-related\nfeatures directly instead of depending on external prompts. Furthermore, we\nintroduce a novel state-based PEFT method: State-offset Tuning. At every\ntimestep, our method directly affects the state at the current step, leading to\nmore effective adaptation. Through extensive experiments across diverse\ndatasets, we demonstrate the effectiveness of our method. Code is available at\nhttps://github.com/furiosa-ai/ssm-state-tuning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03499.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07603",
      "authors": [
        {
          "_id": "67cfc310f2b1fe815dc24ebf",
          "name": "Sedrick Keh",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec0",
          "name": "Jean Mercat",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec1",
          "name": "Samir Yitzhak Gadre",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec2",
          "name": "Kushal Arora",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec3",
          "name": "Igor Vasiljevic",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec4",
          "name": "Benjamin Burchfiel",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec5",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec6",
          "name": "Russ Tedrake",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec7",
          "name": "Thomas Kollar",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec8",
          "name": "Ludwig Schmidt",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec9",
          "name": "Achal Dave",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:19.000Z",
      "title": "VLMs necesitan ser entrenados previamente con datos de imagen.",
      "summary": "Los LLMs preprocesados muestran excelente rendimiento en tareas de lenguaje visual después de una entrenamiento adicional con datos de imagen. Al añadir imágenes en el segundo paso de entrenamiento, se pueden desarrollar estas capacidades de manera efectiva, pero comparado con VLMs que integran rápidamente las imágenes, no es claro qué beneficios se obtienen ni qué pérdidas se producen. Para investigar esto, se limitaron los conjuntos de datos, la escala, la proporción de imágenes a texto y el nivel de preprocesamiento, y se entrenaron modelos con estos parámetros. Posteriormente, se evaluaron estos modelos en tareas de lenguaje visual y en tareas que solo incluían texto, y se realizó un ajuste fino para comparar su rendimiento. Los modelos que se preprocesaron con una mezcla de imágenes y texto mantuvieron un buen rendimiento en evaluaciones con texto solo, pero mostraron un mejor rendimiento en tareas de lenguaje visual. En un promedio de seis tareas diferentes, al añadir los tokens de imagen en el 80% del procesamiento en un modelo de 1B, el rendimiento promedio mejoró en un 2%.",
      "upvotes": 2,
      "discussionId": "67cfc314f2b1fe815dc24fe3",
      "ai_keywords": [
        "pre-trained LLMs",
        "vision-language tasks",
        "fine-tune",
        "vision tokens"
      ]
    },
    "publishedAt": "2025-03-10T13:58:19.000Z",
    "title": "Should VLMs be Pre-trained with Image Data?",
    "summary": "Pre-trained LLMs that are further trained with image data perform well on\nvision-language tasks. While adding images during a second training phase\neffectively unlocks this capability, it is unclear how much of a gain or loss\nthis two-step pipeline gives over VLMs which integrate images earlier into the\ntraining process. To investigate this, we train models spanning various\ndatasets, scales, image-text ratios, and amount of pre-training done before\nintroducing vision tokens. We then fine-tune these models and evaluate their\ndownstream performance on a suite of vision-language and text-only tasks. We\nfind that pre-training with a mixture of image and text data allows models to\nperform better on vision-language tasks while maintaining strong performance on\ntext-only evaluations. On an average of 6 diverse tasks, we find that for a 1B\nmodel, introducing visual tokens 80% of the way through pre-training results in\na 2% average improvement over introducing visual tokens to a fully pre-trained\nmodel.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07603.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06885",
      "authors": [
        {
          "_id": "67cfc1c5182d970d40896a5e",
          "user": {
            "_id": "655b813476e4fad5529f3256",
            "avatarUrl": "/avatars/73d83e45d921531f9830a0ea80f76491.svg",
            "isPro": false,
            "fullname": "Yan Yang",
            "user": "HelloKKMe",
            "type": "user"
          },
          "name": "Yan Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:28.574Z",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a5f",
          "user": {
            "_id": "6357362f811ee2fa05070f64",
            "avatarUrl": "/avatars/2cf37efb80f5cfb3e4e9d08674de6dd1.svg",
            "isPro": false,
            "fullname": "Dongxu Li",
            "user": "dxli1",
            "type": "user"
          },
          "name": "Dongxu Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T04:53:27.837Z",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a60",
          "name": "Haoning Wu",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a61",
          "name": "Bei Chen",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a62",
          "name": "Liu Liu",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a63",
          "name": "Liyuan Pan",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a64",
          "name": "Junnan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T03:29:18.000Z",
      "title": "ProBench: Evaluación de un modelo basado en multimodalidad para evaluar expertos de diversas áreas en la evaluación de acciones abiertas.",
      "summary": "Los tareas de alto nivel de multimodalidad son la esencia de la inteligencia general Máxima. Mientras la capacidad de los modelos de lenguaje multimodal (MLLMs) continúa mejorando, es necesario y difícil evaluar y comprender el desarrollo de su inteligencia multimodal. En este artículo, se presenta ProBench, un marco de referencia de preguntas abiertas que requieren conocimientos especializados y su desarrollo. ProBench cubre una amplia gama de 10 áreas y 56 subáreas, desde ciencias hasta artes, letras, programación, matemáticas y literatura creativa. Experimentalmente, se evaluaron y compararon 24 modelos más recientes utilizando MLLM-as-a-Judge. Finalmente, el mejor modelo abierto fue en contraposición con el modelo proprietario, pero ProBench plantea cuestiones cruciales sobre el conocimiento visual, la comprensión contextual, el conocimiento de las áreas y el motivo del desarrollo, proporcionando direcciones para futuros estudios en IA multimodal.",
      "upvotes": 2,
      "discussionId": "67cfc1c7182d970d40896b1d",
      "projectPage": "https://yan98.github.io/ProBench/",
      "githubRepo": "https://github.com/Yan98/ProBench_eval",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "benchmark",
        "user queries",
        "professional expertise",
        "advanced reasoning",
        "high-quality samples",
        "daily productivity demands",
        "fields",
        "sub-fields",
        "visual perception",
        "textual understanding",
        "domain knowledge"
      ]
    },
    "publishedAt": "2025-03-09T23:29:18.000Z",
    "title": "ProBench: Judging Multimodal Foundation Models on Open-ended\n  Multi-domain Expert Tasks",
    "summary": "Solving expert-level multimodal tasks is a key milestone towards general\nintelligence. As the capabilities of multimodal large language models (MLLMs)\ncontinue to improve, evaluation of such advanced multimodal intelligence\nbecomes necessary yet challenging. In this work, we introduce ProBench, a\nbenchmark of open-ended user queries that require professional expertise and\nadvanced reasoning. ProBench consists of 4,000 high-quality samples\nindependently submitted by professionals based on their daily productivity\ndemands. It spans across 10 fields and 56 sub-fields, including science, arts,\nhumanities, coding, mathematics, and creative writing. Experimentally, we\nevaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal\nthat although the best open-source models rival the proprietary ones, ProBench\npresents significant challenges in visual perception, textual understanding,\ndomain knowledge and advanced reasoning, thus providing valuable directions for\nfuture multimodal AI research efforts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06885.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02199",
      "authors": [
        {
          "_id": "67c90dad6f3ef3c2c77689b0",
          "name": "Ailin Deng",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b1",
          "name": "Tri Cao",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b2",
          "user": {
            "_id": "67cbb6ea2cc05acaab023f75",
            "avatarUrl": "/avatars/79272c8889a8c472cf75172ead72daea.svg",
            "isPro": false,
            "fullname": "Zhirui Chen",
            "user": "ryanchen42",
            "type": "user"
          },
          "name": "Zhirui Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:03:34.042Z",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b3",
          "name": "Bryan Hooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:21:07.000Z",
      "title": "Words or Vision: Lenguaje y Visión: ¿Creer en el Lenguaje o en la Visión?",
      "summary": "Modelos de Visión-Lenguaje (VLMs) diseñados para datos de imágenes similares y diversas entradas de contexto muestran un excelente rendimiento al integrar la información de las imágenes y el contexto. Sin embargo, se conoce que la investigación sobre el equilibrio de preferencias entre modelos no es suficiente. Para investigar la preferencia entre modelos de VLMs para datos de imágenes y diversos contextos, se evaluaron 10 VLMs aplicando cambios en el contexto en 4 tareas similares de imágenes. Se descubrió el fenómeno de \"ignorar el contexto\": cuando existe asimetría, los VLMs confían excesivamente en la información del contexto en comparación con la de las imágenes, lo que puede llevar a una pérdida de rendimiento y dudas sobre la seguridad. Se analizaron los factores que provocan la inclinación hacia el contexto: planificación de comandos, tamaño del modelo de lenguaje, relevancia del contexto, orden de tokens, y la interacción entre la confianza en la imagen y el contexto. La inclinación hacia el contexto puede ser exacerbada por la posición del modelo de lenguaje, ya que factores como el orden de tokens pueden empeorar esta inclinación. Para solucionar este problema, se intentaron expandir el contexto y entrenar con supervisación final, demostrando sus efectos. Además, se proporcionan análisis teóricos que indican que el fenómeno de ignorar el contexto es causado por la simple desigualdad entre contextos durante el entrenamiento y la asimetría entre diferentes datos de modelos. Estos hallazgos subrayan la importancia del ajuste de interacción entre modelos y el equilibrio del entrenamiento en VLMs, así como la necesidad de manejar la asimetría entre los datos de modelos para mejorar la robustez y confianza.",
      "upvotes": 2,
      "discussionId": "67c90dae6f3ef3c2c77689ec",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "blind faith in text",
        "modality preferences",
        "textual variations",
        "vision-centric tasks",
        "text bias",
        "instruction prompts",
        "language model size",
        "token order",
        "positional biases",
        "multi-modal data",
        "supervised fine-tuning",
        "text augmentation",
        "balanced training",
        "modality interactions"
      ]
    },
    "publishedAt": "2025-03-03T21:21:07.000Z",
    "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
    "summary": "Vision-Language Models (VLMs) excel in integrating visual and textual\ninformation for vision-centric tasks, but their handling of inconsistencies\nbetween modalities is underexplored. We investigate VLMs' modality preferences\nwhen faced with visual data and varied textual inputs in vision-centered\nsettings. By introducing textual variations to four vision-centric tasks and\nevaluating ten Vision-Language Models (VLMs), we discover a ``blind faith\nin text'' phenomenon: VLMs disproportionately trust textual data over visual\ndata when inconsistencies arise, leading to significant performance drops under\ncorrupted text and raising safety concerns. We analyze factors influencing this\ntext bias, including instruction prompts, language model size, text relevance,\ntoken order, and the interplay between visual and textual certainty. While\ncertain factors, such as scaling up the language model size, slightly mitigate\ntext bias, others like token order can exacerbate it due to positional biases\ninherited from language models. To address this issue, we explore supervised\nfine-tuning with text augmentation and demonstrate its effectiveness in\nreducing text bias. Additionally, we provide a theoretical analysis suggesting\nthat the blind faith in text phenomenon may stem from an imbalance of pure text\nand multi-modal data during training. Our findings highlight the need for\nbalanced training and careful consideration of modality interactions in VLMs to\nenhance their robustness and reliability in handling multi-modal data\ninconsistencies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02199.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07595",
      "authors": [
        {
          "_id": "67cfa440aff9c98bb3f45a56",
          "user": {
            "_id": "64b3fc1fa24816979609dcb3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b3fc1fa24816979609dcb3/cHRMs4YegRcgbZO8_bBaZ.jpeg",
            "isPro": false,
            "fullname": "Sinclair Schneider",
            "user": "SinclairSchneider",
            "type": "user"
          },
          "name": "Sinclair Schneider",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T02:48:03.261Z",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a57",
          "name": "Florian Steuber",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a58",
          "name": "Joao A. G. Schneider",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a59",
          "name": "Gabi Dreo Rodosek",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:56:25.000Z",
      "title": "Tecnología de evasión de detección en modelos de lenguaje grandes",
      "summary": "La expansión de los modelos de lenguaje grandes ha llevado a una amplia utilización, pero también ha incrementado riesgos y ha provocado problemas como la propagación de la verdad. Por ello, ha sido importante el desarrollo de sistemas de clasificación como DetectGPT. Estos detectores han mostrado ser vulnerables a técnicas como el evasión de bajo nivel, lo cual se ha demostrado en las serie de experimentos. Al cambiar sistemáticamente la temperatura de generación, los detectores de aprendizaje superficial han mostrado la menor confianza. Al ajustar los modelos de generación mediante enseñanza adicional, se ha podido evitar los detectores basados en BERT. Finalmente, la reconfiguración ha logrado alcanzar más de 90% de evasión en detectores como DetectGPT, aunque las frases resultantes eran similares a las originales en términos de similitud. Comparados con los estudios previos, los métodos presentados han demostrado un mejor rendimiento. También se están discutiendo los impactos en la sociedad y las posibilidades de investigación.",
      "upvotes": 1,
      "discussionId": "67cfa441aff9c98bb3f45a95",
      "ai_keywords": [
        "large language models",
        "fake news",
        "classification systems",
        "DetectGPT",
        "evasion techniques",
        "generative models",
        "temperature",
        "shallow learning-detectors",
        "fine-tuning",
        "reinforcement learning",
        "BERT-based-detectors",
        "zero-shot-detectors",
        "rephrasing"
      ]
    },
    "publishedAt": "2025-03-10T13:56:25.000Z",
    "title": "Detection Avoidance Techniques for Large Language Models",
    "summary": "The increasing popularity of large language models has not only led to\nwidespread use but has also brought various risks, including the potential for\nsystematically spreading fake news. Consequently, the development of\nclassification systems such as DetectGPT has become vital. These detectors are\nvulnerable to evasion techniques, as demonstrated in an experimental series:\nSystematic changes of the generative models' temperature proofed shallow\nlearning-detectors to be the least reliable. Fine-tuning the generative model\nvia reinforcement learning circumvented BERT-based-detectors. Finally,\nrephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT,\nalthough texts stayed highly similar to the original. A comparison with\nexisting work highlights the better performance of the presented methods.\nPossible implications for society and further research are discussed.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07595.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07465",
      "authors": [
        {
          "_id": "67cfaaed7f229132171f596b",
          "name": "Ao Wang",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596c",
          "name": "Lihao Liu",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596d",
          "name": "Hui Chen",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596e",
          "name": "Zijia Lin",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596f",
          "name": "Jungong Han",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f5970",
          "name": "Guiguang Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T15:42:59.000Z",
      "title": "YOLOE: eficiente en el tiempo, eso se puede ver fácilmente",
      "summary": "Detección y segmentación de objetos son ampliamente utilizados en aplicaciones de visión por computador, pero modelos tradicionales como la serie YOLO están limitados por categorías decisivas y su adaptación en escenarios abiertos es reducida. Métodos recientes de conjuntos abiertos utilizan prompts de texto, visuales o sin prompts para superar estos problemas, pero su alto consumo de cálculo y complejidad de conjunto puede llevar a pérdidas en rendimiento y eficiencia. En este artículo, proponemos un enfoque que integra detección y segmentación de objetos en tiempo real dentro de un modelo altamente eficiente llamado YOLOE, utilizando diferentes estructuras de prompts abiertos. Proponemos Re-parameterizable Region-Text Alignment (RepRTA) para prompts de texto, que utiliza una red auxiliar ligera y re-parametrizable para refinar codificaciones de texto previamente entrenadas y fortalecer el alineamiento visual-textual. Para prompts visuales, presentamos Semantic-Activated Visual Prompt Encoder (SAVPE), que utiliza semánticas separadas y activaciones de vectores para implementar una mejora en la codificación visual con precisión y baja complejidad. Para escenarios sin prompts, proponemos Lazy Region-Prompt Contrast (LRPC), que utiliza un grande vocabulario y codificaciones especializadas para reconocer todos los objetos. Experimentos extendidos muestran la excelente capacidad de 0-shot de YOLOE, su alta eficiencia de inferencia y bajo costo de entrenamiento. Específicamente, en LVIS, se logra un costo de entrenamiento 3 veces menor y un aumento de velocidad de inferencia del 1.4 veces, y YOLOE-v8-S supera a YOLO-Worldv2-S en 3.5 AP. En COCO, YOLOE-v8-L obtiene un tiempo de entrenamiento aproximadamente 4 veces menor que YOLOv8-L en conjuntos cerrados, con mejoras de 0.6 AP^b y 0.4 AP^m. El código y modelos están disponibles en https://github.com/THU-MIG/yoloe.",
      "upvotes": 0,
      "discussionId": "67cfaaf27f229132171f5ab4",
      "ai_keywords": [
        "Object detection",
        "Segmentation",
        "YOLO series",
        "Open-set methods",
        "Text prompts",
        "Visual cues",
        "Prompt-free paradigm",
        "YOLOE",
        "Rep-parameterizable Region-Text Alignment (RepRTA)",
        "Pretrained textual embeddings",
        "Re-parameterizable lightweight auxiliary network",
        "Semantic-Activated Visual Prompt Encoder (SAVPE)",
        "Decoupled semantic and activation branches",
        "Visual embedding",
        "Lazy Region-Prompt Contrast (LRPC)",
        "Large vocabulary",
        "Specialized embedding",
        "LVIS",
        "Zero-shot performance",
        "Transferability",
        "Inference efficiency",
        "Training cost",
        "AP",
        "COCO",
        "Closed-set YOLOv8-L",
        "Inference speedup",
        "Training time"
      ]
    },
    "publishedAt": "2025-03-10T11:42:59.000Z",
    "title": "YOLOE: Real-Time Seeing Anything",
    "summary": "Object detection and segmentation are widely employed in computer vision\napplications, yet conventional models like YOLO series, while efficient and\naccurate, are limited by predefined categories, hindering adaptability in open\nscenarios. Recent open-set methods leverage text prompts, visual cues, or\nprompt-free paradigm to overcome this, but often compromise between performance\nand efficiency due to high computational demands or deployment complexity. In\nthis work, we introduce YOLOE, which integrates detection and segmentation\nacross diverse open prompt mechanisms within a single highly efficient model,\nachieving real-time seeing anything. For text prompts, we propose\nRe-parameterizable Region-Text Alignment (RepRTA) strategy. It refines\npretrained textual embeddings via a re-parameterizable lightweight auxiliary\nnetwork and enhances visual-textual alignment with zero inference and\ntransferring overhead. For visual prompts, we present Semantic-Activated Visual\nPrompt Encoder (SAVPE). It employs decoupled semantic and activation branches\nto bring improved visual embedding and accuracy with minimal complexity. For\nprompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy.\nIt utilizes a built-in large vocabulary and specialized embedding to identify\nall objects, avoiding costly language model dependency. Extensive experiments\nshow YOLOE's exceptional zero-shot performance and transferability with high\ninference efficiency and low training cost. Notably, on LVIS, with 3times\nless training cost and 1.4times inference speedup, YOLOE-v8-S surpasses\nYOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6\nAP^b and 0.4 AP^m gains over closed-set YOLOv8-L with nearly 4times less\ntraining time. Code and models are available at\nhttps://github.com/THU-MIG/yoloe.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07465.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07426",
      "authors": [
        {
          "_id": "67cff321f2b1fe815dce3722",
          "name": "Junkang Wu",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3723",
          "name": "Kexin Huang",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3724",
          "name": "Xue Wang",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3725",
          "name": "Jinyang Gao",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3726",
          "name": "Bolin Ding",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3727",
          "name": "Jiancan Wu",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3728",
          "name": "Xiangnan He",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3729",
          "user": {
            "_id": "65fca775fa59bdf4737b1a84",
            "avatarUrl": "/avatars/a161b510bde8f57e7686cbb0b4aa6a52.svg",
            "isPro": false,
            "fullname": "Xiang Wang",
            "user": "xiangwang1223",
            "type": "user"
          },
          "name": "Xiang Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T08:24:02.839Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T15:11:07.000Z",
      "title": "RePO: Optimización de Preferencias Basada en ReLU",
      "summary": "Adecuar la naturaleza humana de los LLM es importante en funciones realistas, pero los métodos actuales tienen problemas de cálculo y estabilidad. DPO construye un patrón offline utilizando un único parámetro beta, pero métodos posteriores como SimPO reintroducen la complejidad y utilizan dos parámetros (beta, gamma). Proponemos un algoritmo de optimización de características basada en ReLU (RePO). RePO mantiene un marjinal sin referencia de SimPO, elimina beta mediante análisis gráfico, y introduce una pérdida max-margin basada en ReLU para filtrar pares ligeros de manera natural, así eliminando beta. Teóricamente, RePO se convierte en el límite de SimPO, donde los pesos de la función logística convergen a un valor binario y forma el convex hull de la pérdida 0-1. Los resultados de experimentos en AlpacaEval 2 y Arena-Hard indican que RePO supera a DPO y SimPO en varios modelos de base, y solo requiere ajuste de un parámetro.",
      "upvotes": 0,
      "discussionId": "67cff322f2b1fe815dce3787",
      "ai_keywords": [
        "LLMS",
        "RLHF",
        "DPO",
        "SimPO",
        "ReLU-based Preference Optimization (RePO)",
        "reference-free margins",
        "gradient analysis",
        "ReLU-based max-margin loss",
        "convex envelope",
        "0-1 loss"
      ]
    },
    "publishedAt": "2025-03-10T11:11:07.000Z",
    "title": "RePO: ReLU-based Preference Optimization",
    "summary": "Aligning large language models (LLMs) with human preferences is critical for\nreal-world deployment, yet existing methods like RLHF face computational and\nstability challenges. While DPO establishes an offline paradigm with single\nhyperparameter beta, subsequent methods like SimPO reintroduce complexity\nthrough dual parameters (beta, gamma). We propose {ReLU-based Preference\nOptimization (RePO)}, a streamlined algorithm that eliminates beta via two\nadvances: (1) retaining SimPO's reference-free margins but removing beta\nthrough gradient analysis, and (2) adopting a ReLU-based max-margin loss that\nnaturally filters trivial pairs. Theoretically, RePO is characterized as\nSimPO's limiting case (beta to infty), where the logistic weighting\ncollapses to binary thresholding, forming a convex envelope of the 0-1 loss.\nEmpirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO\nand SimPO across multiple base models, requiring only one hyperparameter to\ntune.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07426.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06362",
      "authors": [
        {
          "_id": "67cffd119f703990a8e25925",
          "name": "Umberto Cappellazzo",
          "hidden": false
        },
        {
          "_id": "67cffd119f703990a8e25926",
          "name": "Minsu Kim",
          "hidden": false
        },
        {
          "_id": "67cffd119f703990a8e25927",
          "name": "Stavros Petridis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T00:02:10.000Z",
      "title": "Adaptive Audio-Visual Speech Recognition with Matrizon-Based Monomodal LLMs",
      "summary": "La Reconocimiento de Voces Visuales (AVSR) utiliza dos modelos, unos de voz y otros de visión, para mejorar la robustez del reconocimiento de voz en entornos con mucho ruido. El reciente avance de los modelos de lenguaje de gran escala (LLMs) también ha mostrado eficacia tanto en el reconocimiento de voz como en el AVSR. Sin embargo, la longitud de las representaciones de voz es larga, lo que hace que la integración directa con los LLMs resulte muy costoso en términos de cálculos. Los métodos existentes abordan este problema al compresionar las representaciones de voz antes de que se usen como entrada para los LLMs. Sin embargo, un alto porcentaje de compresión puede causar un deterioro en el rendimiento y un equilibrio entre costos de cálculo y precisión del reconocimiento. Para resolver estos desafíos, proponemos el primer LLM basado en MATRIO-SK para el AVSR, Llama-MTSK. Este modelo puede ajustar la asignación de tokens de voz y visión de manera flexible según las restricciones de cálculo, manteniendo así altos rendimientos. Nuestro enfoque utiliza el aprendizaje de la representación MATRIO-SK como modelo, codifica las representaciones de voz y visión multigranularmente dentro del modelo y elimina la necesidad de entrenar modelos con diferentes niveles de compresión. Además, introducimos tres estrategias basadas en LoRA para la fine-tuning eficiente del LLM, utilizando módulos LoRA globales y propios de escala. Las pruebas en los dos conjuntos de datos máximos de AVSR muestran que Llama-MTSK realiza los resultados más avanzados y supera los resultados de modelos entrenados independientemente en un nivel de compresión fijo.",
      "upvotes": 0,
      "discussionId": "67cffd129f703990a8e25990",
      "ai_keywords": [
        "Audio-Visual Speech Recognition (AVSR)",
        "Large Language Models (LLMs)",
        "speech representations",
        "computational costs",
        "audio-visual token allocation",
        "Matryoshka-based Multimodal LLM",
        "Matryoshka Representation Learning",
        "global LoRA modules",
        "scale-specific LoRA modules",
        "LoRA-based Matryoshka strategies"
      ]
    },
    "publishedAt": "2025-03-08T19:02:10.000Z",
    "title": "Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs",
    "summary": "Audio-Visual Speech Recognition (AVSR) leverages both audio and visual\nmodalities to enhance speech recognition robustness, particularly in noisy\nenvironments. Recent advancements in Large Language Models (LLMs) have\ndemonstrated their effectiveness in speech recognition, including AVSR.\nHowever, due to the significant length of speech representations, direct\nintegration with LLMs imposes substantial computational costs. Prior approaches\naddress this by compressing speech representations before feeding them into\nLLMs. However, higher compression ratios often lead to performance degradation,\nnecessitating a trade-off between computational efficiency and recognition\naccuracy. To address this challenge, we propose Llama-MTSK, the first\nMatryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation of\nthe audio-visual token allocation based on specific computational constraints\nwhile preserving high performance. Our approach, inspired by Matryoshka\nRepresentation Learning, encodes audio-visual representations at multiple\ngranularities within a single model, eliminating the need to train separate\nmodels for different compression levels. Moreover, to efficiently fine-tune the\nLLM, we introduce three LoRA-based Matryoshka strategies using global and\nscale-specific LoRA modules. Extensive evaluations on the two largest AVSR\ndatasets demonstrate that Llama-MTSK achieves state-of-the-art results,\nmatching or surpassing models trained independently at fixed compression\nlevels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06362.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05283",
      "authors": [
        {
          "_id": "67cef721e5ab8ec0550b7a66",
          "name": "Souhail Hadgi",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a67",
          "name": "Luca Moschella",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a68",
          "user": {
            "_id": "5e8ef1f14957053f606489e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
            "isPro": false,
            "fullname": "Andrea Santilli",
            "user": "teelinsan",
            "type": "user"
          },
          "name": "Andrea Santilli",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T14:35:44.397Z",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a69",
          "name": "Diego Gomez",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6a",
          "name": "Qixing Huang",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6b",
          "name": "Emanuele Rodolà",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6c",
          "name": "Simone Melzi",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6d",
          "name": "Maks Ovsjanikov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T09:51:56.000Z",
      "title": "El escape de 3D y espacios de potencia de texto por alineamiento de arreglos del viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo viejo",
      "summary": "Recientes estudios han mostrado que los características aprendidas por un único encoder visual 2D y un encoder de texto durante la entrenamiento a escala no se afectan de manera significativa por las representaciones de otras modalidades, demostrando una estructura sorprendente y intrínseca. Sin embargo, el papel que puede desempeñar un encoder 3D en relación con otras modalidades no ha sido investigado. Además, los modelos 3D Fundamental actuales se han entrenado con grandes conjuntos de datos, pero generalmente se han entrenado basándose en encoders libres de otras representaciones y objetivos de alineamiento explícitos. En este estudio, se investiga la posibilidad de alinear espacios de características obtenidos de un único encoder 3D con espacios de características basados en texto y de procesamiento posterior. El rendimiento de un alineamiento de características basado en procesamiento posterior en un único modalidad de texto y un encoder 3D es limitado. Posteriormente, se extraen espacios parciales de los espacios de características correspondientes y se proyectan las representaciones entrenadas en un espacio parcial de baja dimensión seleccionado, lo que significativamente mejora la calidad del alineamiento y aumenta la precisión en tareas de matching y búsqueda. Nuestro análisis revela que estos espacios parciales compartidos tienen características principalmente semánticas y geométricas, diferenciándose claramente de otras representaciones. En general, nuestro estudio intenta establecer un estándar para el alineamiento de espacios de características entre un único encoder 3D y un espacio de características basado en texto, y revela características generales compartidas y características distintivas comparadas con datos 3D y otras representaciones.",
      "upvotes": 0,
      "discussionId": "67cef723e5ab8ec0550b7ac8",
      "ai_keywords": [
        "uni-modal 2D vision",
        "text encoders",
        "learned features",
        "3D encoders",
        "3D foundation models",
        "alignment objectives",
        "feature alignment",
        "subspaces",
        "lower-dimensional subspaces",
        "semantic data representations",
        "geometric data representations",
        "matching tasks",
        "retrieval tasks"
      ]
    },
    "publishedAt": "2025-03-07T04:51:56.000Z",
    "title": "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent\n  Spaces",
    "summary": "Recent works have shown that, when trained at scale, uni-modal 2D vision and\ntext encoders converge to learned features that share remarkable structural\nproperties, despite arising from different representations. However, the role\nof 3D encoders with respect to other modalities remains unexplored.\nFurthermore, existing 3D foundation models that leverage large datasets are\ntypically trained with explicit alignment objectives with respect to frozen\nencoders from other representations. In this work, we investigate the\npossibility of a posteriori alignment of representations obtained from\nuni-modal 3D encoders compared to text-based feature spaces. We show that naive\npost-training feature alignment of uni-modal text and 3D encoders results in\nlimited performance. We then focus on extracting subspaces of the corresponding\nfeature spaces and discover that by projecting learned representations onto\nwell-chosen lower-dimensional subspaces the quality of alignment becomes\nsignificantly higher, leading to improved accuracy on matching and retrieval\ntasks. Our analysis further sheds light on the nature of these shared\nsubspaces, which roughly separate between semantic and geometric data\nrepresentations. Overall, ours is the first work that helps to establish a\nbaseline for post-training alignment of 3D uni-modal and text feature spaces,\nand helps to highlight both the shared and unique properties of 3D data\ncompared to other representations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05283.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03511",
      "authors": [
        {
          "_id": "67cd7ace999766d8cd73fb18",
          "user": {
            "_id": "6732f2c24c2f18a60e76b915",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6732f2c24c2f18a60e76b915/W6oozAjM-zu7E3SL9uQ97.jpeg",
            "isPro": false,
            "fullname": "Fan",
            "user": "KianYale",
            "type": "user"
          },
          "name": "Qingyu Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:41.233Z",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb19",
          "name": "Yinghao Cai",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1a",
          "name": "Chao Li",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1b",
          "name": "Wenzhe He",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1c",
          "name": "Xudong Zheng",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1d",
          "name": "Tao Lu",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1e",
          "name": "Bin Liang",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1f",
          "name": "Shuo Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T13:57:37.000Z",
      "title": "NeuGrasp: Detección de Producción de Objetos Materiales Independientes mediante Construcción de Superficies Neurales Generalizables utilizando Perfiles de Características",
      "summary": "Cuando un robot captura objetos transparentes y con alta reflexividad, los métodos que se basan en información de profundidad exacta tienen grandes problemas. En este artículo, se presenta NeuGrasp, una técnica que utiliza proyectos de fondo para realizar operaciones de ruido irrelevante independientemente del material. NeuGrasp combina transformers y proyectos globales para codificar espacialmente y acumular características de puntos de vista, permitiendo una reconstrucción de superficies potente incluso en condiciones de puntos de vista estrechos o raros. Se fortalece las características residuales en objetos como los porocones y se mejora la percepción espacial utilizando el proyecto de leyenda. NeuGrasp muestra excelente rendimiento incluso cuando captura objetos transparentes y con alta reflexividad. Para obtener más detalles, puede consultar el sitio web https://neugrasp.github.io/.",
      "upvotes": 0,
      "discussionId": "67cd7ad0999766d8cd73fb77",
      "ai_keywords": [
        "neural surface reconstruction",
        "background priors",
        "material-agnostic grasp detection",
        "transformers",
        "global prior volumes",
        "multi-view features",
        "spatial encoding",
        "narrow and sparse viewing conditions",
        "residual feature enhancement",
        "occupancy-prior volume",
        "transparent objects",
        "specular surfaces"
      ]
    },
    "publishedAt": "2025-03-05T08:57:37.000Z",
    "title": "NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection",
    "summary": "Robotic grasping in scenes with transparent and specular objects presents\ngreat challenges for methods relying on accurate depth information. In this\npaper, we introduce NeuGrasp, a neural surface reconstruction method that\nleverages background priors for material-agnostic grasp detection. NeuGrasp\nintegrates transformers and global prior volumes to aggregate multi-view\nfeatures with spatial encoding, enabling robust surface reconstruction in\nnarrow and sparse viewing conditions. By focusing on foreground objects through\nresidual feature enhancement and refining spatial perception with an\noccupancy-prior volume, NeuGrasp excels in handling objects with transparent\nand specular surfaces. Extensive experiments in both simulated and real-world\nscenarios show that NeuGrasp outperforms state-of-the-art methods in grasping\nwhile maintaining comparable reconstruction quality. More details are available\nat https://neugrasp.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03511.png",
    "numComments": 1,
    "isAuthorParticipating": true
  }
]