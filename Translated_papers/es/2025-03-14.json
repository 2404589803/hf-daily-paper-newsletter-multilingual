[
  {
    "paper": {
      "id": "2503.10613",
      "authors": [
        {
          "_id": "67d393ca336d57afb21bbf63",
          "user": {
            "_id": "67a99ec47b754f038d110926",
            "avatarUrl": "/avatars/e1ff318a42ccb75b094bbe7dae0cabec.svg",
            "isPro": false,
            "fullname": "Advait Gupta",
            "user": "advaitgupta",
            "type": "user"
          },
          "name": "Advait Gupta",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:36.855Z",
          "hidden": false
        },
        {
          "_id": "67d393ca336d57afb21bbf64",
          "user": {
            "_id": "672f89e6d7f4171f374dacea",
            "avatarUrl": "/avatars/4a8b378e13e862586bb428fdf000b3cc.svg",
            "isPro": false,
            "fullname": "NandaKiran Velaga",
            "user": "nandakiran09",
            "type": "user"
          },
          "name": "NandaKiran Velaga",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:34.327Z",
          "hidden": false
        },
        {
          "_id": "67d393ca336d57afb21bbf65",
          "name": "Dang Nguyen",
          "hidden": false
        },
        {
          "_id": "67d393ca336d57afb21bbf66",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:39.157Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:55:45.000Z",
      "submittedOnDailyAt": "2025-03-14T01:33:20.201Z",
      "title": "CoSTAast: Agente de segmentación de costa y agente de herramienta para editar imágenes de turno.",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Los modelos de texto a imagen como Stable Diffusion y DALLE-3 revelan dificultades en la edición de imágenes multinivel. Se considera la posibilidad de desagregar esta tarea en un flujo de trabajo de agentes efectivos, resolviendo varias sub-tareas secuencialmente. Los algoritmos de búsqueda tradicionales requieren una búsqueda costosa para encontrar los pasos de herramientas. Por otro lado, los modelos de lenguaje grandes (LLMs) tienen conocimientos previos sobre la planificación de sub-tareas, pero no saben si pueden evaluar de manera precisa la capacidad y costo de las herramientas en cada sub-tarea. Se investiga si se puede combinar los beneficios de los LLMs y el algoritmo de búsqueda de grafos para encontrar pasos de herramientas costo-eficientes. Se propone un enfoque tridimensional llamado \"CoSTA*\" que divide el trabajo en tres etapas: construir un árbol de sub-tareas utilizando LLMs, reducir el grafo de herramientas relacionadas con el trabajo, y luego realizar una búsqueda de A* en pequeños sub-grafos. Para equilibrar mejor el costo y la calidad del trabajo completo, CoSTA* integra dos criterios de evaluación de las herramientas en cada sub-tarea y guia la búsqueda de A*. El output de cada sub-tarea es evaluado por un modelo de lenguaje visual (VLM), y si falla, se actualizan el costo y la calidad de las herramientas. Así, la búsqueda de A* puede recuperarse rápidamente de un fallo y explorar otros pasos. Además, CoSTA* cambia automáticamente la modelación entre sub-tareas para lograr un mejor equilibrio de costo y calidad. Se ha construido un nuevo benchmark de edición de imágenes multinivel, y CoSTA* supera a los modelos de edición de imágenes más avanzados y a los agentes en términos de costo y calidad, realizando distintas balances que se ajustan a las preferencias del usuario.",
      "upvotes": 32,
      "discussionId": "67d393cf336d57afb21bc0db",
      "githubRepo": "https://github.com/tianyi-lab/CoSTAR",
      "ai_keywords": [
        "text-to-image models",
        "stable diffusion",
        "DALLE-3",
        "multi-turn image editing",
        "agentic workflow",
        "tool use",
        "subtasks",
        "AI tools",
        "cost-efficient",
        "large language models (LLMs)",
        "subtask planning",
        "graph search",
        "three-stage approach",
        "CoSTA*",
        "subtask tree",
        "pruning",
        "A* search",
        "subgraph",
        "cost-quality trade-off",
        "vision-language model (VLM)",
        "failure",
        "total cost",
        "quality",
        "modality switching",
        "benchmark",
        "state-of-the-art image-editing models",
        "user preference"
      ]
    },
    "publishedAt": "2025-03-13T13:55:45.000Z",
    "title": "CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing",
    "summary": "Text-to-image models like stable diffusion and DALLE-3 still struggle with\nmulti-turn image editing. We decompose such a task as an agentic workflow\n(path) of tool use that addresses a sequence of subtasks by AI tools of varying\ncosts. Conventional search algorithms require expensive exploration to find\ntool paths. While large language models (LLMs) possess prior knowledge of\nsubtask planning, they may lack accurate estimations of capabilities and costs\nof tools to determine which to apply in each subtask. Can we combine the\nstrengths of both LLMs and graph search to find cost-efficient tool paths? We\npropose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask\ntree, which helps prune a graph of AI tools for the given task, and then\nconducts A* search on the small subgraph to find a tool path. To better balance\nthe total cost and quality, CoSTA* combines both metrics of each tool on every\nsubtask to guide the A* search. Each subtask's output is then evaluated by a\nvision-language model (VLM), where a failure will trigger an update of the\ntool's cost and quality on the subtask. Hence, the A* search can recover from\nfailures quickly to explore other paths. Moreover, CoSTA* can automatically\nswitch between modalities across subtasks for a better cost-quality trade-off.\nWe build a novel benchmark of challenging multi-turn image editing, on which\nCoSTA* outperforms state-of-the-art image-editing models or agents in terms of\nboth cost and quality, and performs versatile trade-offs upon user preference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10613.png",
    "numComments": 7,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10480",
      "authors": [
        {
          "_id": "67d38a42d3d16e1166d81bed",
          "user": {
            "_id": "64c3c631e77ea9f28111172a",
            "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
            "isPro": false,
            "fullname": "Siyin Wang",
            "user": "sinwang",
            "type": "user"
          },
          "name": "Siyin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:44.686Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bee",
          "user": {
            "_id": "629ef8544313a7c1dd671130",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png",
            "isPro": false,
            "fullname": "Zhaoye Fei",
            "user": "ngc7293",
            "type": "user"
          },
          "name": "Zhaoye Fei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:04.607Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bef",
          "name": "Qinyuan Cheng",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf0",
          "user": {
            "_id": "64196e45060a651c415d5cf7",
            "avatarUrl": "/avatars/71a43232a7bae851eca252782387a63d.svg",
            "isPro": false,
            "fullname": "Shiduo Zhang",
            "user": "CyberDJ",
            "type": "user"
          },
          "name": "Shiduo Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:20.440Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf1",
          "name": "Panpan Cai",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf2",
          "user": {
            "_id": "618497ea8aaadc9253c2dfa9",
            "avatarUrl": "/avatars/2eb3954a99f5aede6f31b8ae49b8c910.svg",
            "isPro": false,
            "fullname": "Fu Jinlan",
            "user": "Jinlan",
            "type": "user"
          },
          "name": "Jinlan Fu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:38.981Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf3",
          "user": {
            "_id": "61457b8deff2c9fdb4de4988",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg",
            "isPro": false,
            "fullname": "Xipeng Qiu",
            "user": "xpqiu",
            "type": "user"
          },
          "name": "Xipeng Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:46.041Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T15:49:56.000Z",
      "submittedOnDailyAt": "2025-03-14T01:42:40.120Z",
      "title": "El mundo del modelado puede crear mejores planificadores: optimización de dos preferencias en la planificación de tareas estructuradas",
      "submittedOnDailyBy": {
        "_id": "64c3c631e77ea9f28111172a",
        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
        "isPro": false,
        "fullname": "Siyin Wang",
        "user": "sinwang",
        "type": "user"
      },
      "summary": "El desarrollo reciente de los grandes modelos de lenguaje visual y lingüístico (LVLMs) ha mostrado la posibilidad de potenciales para tareas de planificación concretas, pero se enfrenta a limitaciones de dependencia y problemas de eficiencia. Los métodos existentes solo optimizan las elecciones de acción o utilizan modelos del mundo durante la inferencia, pero no aprovechan los beneficios de modelar el mundo para mejorar la capacidad de planificación. Proponemos un nuevo marco de aprendizaje \"Dual Preference Optimization (D^2PO)\", que optimiza la predicción del estado y la elección de acción de manera conjunta mediante aprendizaje. De esta manera, los LVLMs pueden comprender la dinámica del ambiente y realizar mejores planes. Además del feedback humano, los datos de rutas y preferencias de etapas se recopilan automáticamente utilizando una estructura de búsqueda de árbol para ampliar la exploración. Los experimentos extendidos realizados en VoTa-Bench muestran que el método basado en D^2PO supera significativamente los métodos actuales y GPT-4o, al lograr una alta tasa de éxito en la tarea, junto con rutas de ejecución eficientes.",
      "upvotes": 25,
      "discussionId": "67d38a44d3d16e1166d81c54",
      "ai_keywords": [
        "Dual Preference Optimization (D$^2$PO)",
        "preference learning",
        "state prediction",
        "action selection",
        "environment dynamics",
        "tree search mechanism",
        "VoTa-Bench",
        "Qwen2-VL",
        "LLaVA-1.6",
        "LLaMA-3.2",
        "task success rates",
        "efficient execution paths"
      ]
    },
    "publishedAt": "2025-03-13T11:49:56.000Z",
    "title": "World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning",
    "summary": "Recent advances in large vision-language models (LVLMs) have shown promise\nfor embodied task planning, yet they struggle with fundamental challenges like\ndependency constraints and efficiency. Existing approaches either solely\noptimize action selection or leverage world models during inference,\noverlooking the benefits of learning to model the world as a way to enhance\nplanning capabilities. We propose Dual Preference Optimization (D^2PO), a new\nlearning framework that jointly optimizes state prediction and action selection\nthrough preference learning, enabling LVLMs to understand environment dynamics\nfor better planning. To automatically collect trajectories and stepwise\npreference data without human annotation, we introduce a tree search mechanism\nfor extensive exploration via trial-and-error. Extensive experiments on\nVoTa-Bench demonstrate that our D^2PO-based method significantly outperforms\nexisting methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and\nLLaMA-3.2 (11B), achieving superior task success rates with more efficient\nexecution paths.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10480.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "64c3c631e77ea9f28111172a",
      "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
      "fullname": "Siyin Wang",
      "name": "sinwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09669",
      "authors": [
        {
          "_id": "67d37754e07f664c7325f236",
          "user": {
            "_id": "63bbf972d8d676a2299cdb44",
            "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
            "isPro": false,
            "fullname": "Sangwon",
            "user": "agwmon",
            "type": "user"
          },
          "name": "Sangwon Jang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:49.038Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f237",
          "user": {
            "_id": "66c6edcc91dced946471bc13",
            "avatarUrl": "/avatars/55cc8593da6540e1566e1de9d7133f9f.svg",
            "isPro": false,
            "fullname": "June Suk Choi",
            "user": "wchoi403",
            "type": "user"
          },
          "name": "June Suk Choi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:59.636Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f238",
          "user": {
            "_id": "65e5bd4568234ef5d6decadc",
            "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
            "isPro": false,
            "fullname": "Jaehyeong Jo",
            "user": "harryjo97",
            "type": "user"
          },
          "name": "Jaehyeong Jo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:06:19.773Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f239",
          "user": {
            "_id": "635097ec59bfa9a85d4207b2",
            "avatarUrl": "/avatars/787085894e9e6538b6b3e3051efe9eea.svg",
            "isPro": false,
            "fullname": "Kimin Lee",
            "user": "kiminle2",
            "type": "user"
          },
          "name": "Kimin Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:06:26.649Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f23a",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T17:21:57.000Z",
      "submittedOnDailyAt": "2025-03-14T02:05:42.787Z",
      "title": "サイレントブランドリングアタック：トリガー無しデータポイズニングアタックとしてのテキストから画像への拡散モデルに対する攻撃",
      "submittedOnDailyBy": {
        "_id": "63bbf972d8d676a2299cdb44",
        "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
        "isPro": false,
        "fullname": "Sangwon",
        "user": "agwmon",
        "type": "user"
      },
      "summary": "Los modelos que se difunden como imágenes desde el texto han tenido un éxito notable en la generación de contenido de alta calidad desde el texto. Sin embargo, la dependencia de datos disponibles y el aumento de la compartir datos hacen que estos modelos sean particularmente vulnerables a ataques de población de datos. En este artículo, presentamos un nuevo método de población de datos llamado 'attack of the Silent Branding'. Este método manipula modelos que se difunden como imágenes desde el texto para generar imágenes que incluyan logos o símbolos de una marca, incluso sin necesidad de un trigger de texto. Hemos descubierto que los modelos aprenden a recrear patrones visuales repetidos en los datos de entrenamiento de manera natural. Utilizamos esto para desarrollar un algoritmo automático de población de datos y agregamos patrones sin detener la detección, sin añadir marcas a las imágenes originales. Con este conjunto de datos de población, los modelos entrenados pueden generar imágenes que incluyen marcas, evitando la degradación de la calidad de la imagen y el desorden del texto. Hemos probado experimentalmente que nuestro método logra altos rendimientos en dos configuraciones prácticas: un gran conjunto de imágenes de alta calidad y un conjunto de datos de estilo para narraciones. Demostramos, incluyendo evaluaciones humanas y métricas cuantitativas de detección de logos, que nuestro método puede insertar marcas sin detenerla.",
      "upvotes": 25,
      "discussionId": "67d37759e07f664c7325f3c5",
      "projectPage": "https://silent-branding.github.io/",
      "ai_keywords": [
        "text-to-image diffusion models",
        "high-quality contents",
        "text prompts",
        "data poisoning attacks",
        "Silent Branding Attack",
        "visual patterns",
        "data poisoning algorithm",
        "logos",
        "style personalization datasets",
        "logo detection"
      ]
    },
    "publishedAt": "2025-03-12T13:21:57.000Z",
    "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on\n  Text-to-Image Diffusion Models",
    "summary": "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality contents from text prompts. However, their reliance on publicly\navailable data and the growing trend of data sharing for fine-tuning make these\nmodels particularly vulnerable to data poisoning attacks. In this work, we\nintroduce the Silent Branding Attack, a novel data poisoning method that\nmanipulates text-to-image diffusion models to generate images containing\nspecific brand logos or symbols without any text triggers. We find that when\ncertain visual patterns are repeatedly in the training data, the model learns\nto reproduce them naturally in its outputs, even without prompt mentions.\nLeveraging this, we develop an automated data poisoning algorithm that\nunobtrusively injects logos into original images, ensuring they blend naturally\nand remain undetected. Models trained on this poisoned dataset generate images\ncontaining logos without degrading image quality or text alignment. We\nexperimentally validate our silent branding attack across two realistic\nsettings on large-scale high-quality image datasets and style personalization\ndatasets, achieving high success rates even without a specific text trigger.\nHuman evaluation and quantitative metrics including logo detection show that\nour method can stealthily embed logos.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbf972d8d676a2299cdb44",
      "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
      "fullname": "Sangwon",
      "name": "agwmon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10639",
      "authors": [
        {
          "_id": "67d3a632db36a4d5d95dbcff",
          "user": {
            "_id": "65b8724123d948d884b379b1",
            "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
            "isPro": false,
            "fullname": "Rongyao Fang",
            "user": "LucasFang",
            "type": "user"
          },
          "name": "Rongyao Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:14.110Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd00",
          "user": {
            "_id": "64a2b496e2e19de17db7de65",
            "avatarUrl": "/avatars/241448ca487833d6cc5d57bb1fdb6ee5.svg",
            "isPro": false,
            "fullname": "Duan Chengqi",
            "user": "gogoduan",
            "type": "user"
          },
          "name": "Chengqi Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:01.612Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd01",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd02",
          "user": {
            "_id": "65fc7c824d36be78e66ba92d",
            "avatarUrl": "/avatars/d4a55c820cae533f91724e062427516a.svg",
            "isPro": false,
            "fullname": "Linjiang Huang",
            "user": "LjHuang",
            "type": "user"
          },
          "name": "Linjiang Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:08.707Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd03",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd04",
          "user": {
            "_id": "65273fea0ef49cfb783fa5c1",
            "avatarUrl": "/avatars/0c9e204bc2151c8cc533311900d05a36.svg",
            "isPro": false,
            "fullname": "shilinyan",
            "user": "shilinyan",
            "type": "user"
          },
          "name": "Shilin Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:18.212Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd05",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd06",
          "user": {
            "_id": "666d4a0fe70e5838d95aebee",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6dkjoFA_sOjCkjvcvozZ5.jpeg",
            "isPro": false,
            "fullname": "zengxingyu",
            "user": "zengxingyu",
            "type": "user"
          },
          "name": "Xingyu Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:42.264Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd07",
          "name": "Rui Zhao",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd08",
          "user": {
            "_id": "64686f7172d9180d4ac8b4e4",
            "avatarUrl": "/avatars/db67dd6c4b2b41054ddcce5a18ade6f8.svg",
            "isPro": false,
            "fullname": "Jifeng Dai",
            "user": "daijifeng",
            "type": "user"
          },
          "name": "Jifeng Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:14:59.460Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd09",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:14:50.924Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd0a",
          "user": {
            "_id": "65c04e9c27a5fdca81abcbd9",
            "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
            "isPro": false,
            "fullname": "Hongsheng LI",
            "user": "hsli-cuhk",
            "type": "user"
          },
          "name": "Hongsheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:14:43.402Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:59.000Z",
      "submittedOnDailyAt": "2025-03-14T02:16:04.349Z",
      "title": "GoT: Libertad de la capacidad lógica para generación y edición visual con un grande modelo de lenguaje multimodal\n\n(注意：虽然您要求不添加解释或额外的文本，但为了确保翻译的准确性和专业性，我保留了原文中的缩写“GoT”，并保持了其原有的格式。如果需要完全按照您的要求进行翻译，不保留缩写和格式，请告知。)",
      "submittedOnDailyBy": {
        "_id": "65b8724123d948d884b379b1",
        "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
        "isPro": false,
        "fullname": "Rongyao Fang",
        "user": "LucasFang",
        "type": "user"
      },
      "summary": "Actualmente, los métodos de generación y edición de imágenes procesan directamente los textos de prompts sin considerar las razones visuales y específicas de la composición y manipulación. Proponemos un nuevo paradigma llamado \"Generation Chain-of-Thought (GoT)\", que permite generar y editar imágenes mediante un proceso de razonamiento lingüístico explícito. Este enfoque mejora el valor a través de un marco de guía de razonamiento que analiza las relaciones significativas y la posición espacial. Definimos la normalización de GoT y construimos un conjunto de datos de GoT de gran escala, que incluye más de 9M muestras con cadenas de razonamiento específicas para comprender las relaciones significativas-espaciales. Para aprovechar las excelencias de GoT, implementamos una serie de marcos que integran la generación de cadenas de razonamiento con Qwen2.5-VL y el modelo de guía significativa-espacial nuevo, lo que permite expandir el modelo desde el final hasta el comienzo. Los experimentos demostraron un gran mejoramiento en las tareas de generación y edición, permitiendo un proceso de razonamiento lingüístico explícito y generar visualizaciones interactivas que ajustan precisamente las imágenes. GoT abre una nueva dirección en la generación y edición de visualizaciones, proporcionando una forma de crear imágenes que coinciden mejor con las intenciones humanas. Para futuros estudios, publicamos el conjunto de datos, el código y modelos pretrenados.",
      "upvotes": 21,
      "discussionId": "67d3a636db36a4d5d95dbdeb",
      "githubRepo": "https://github.com/rongyaofang/GoT",
      "ai_keywords": [
        "Generation Chain-of-Thought (GoT)",
        "text-to-image generation",
        "editing tasks",
        "reasoning chain generation",
        "end-to-end diffusion model",
        "Semantic-Spatial Guidance Module",
        "semantic relationships",
        "spatial arrangements",
        "large-scale GoT datasets",
        "detailed reasoning chains",
        "semantic-spatial relationships",
        "interactive visual generation",
        "reasoning steps",
        "human intent"
      ]
    },
    "publishedAt": "2025-03-13T13:59:59.000Z",
    "title": "GoT: Unleashing Reasoning Capability of Multimodal Large Language Model\n  for Visual Generation and Editing",
    "summary": "Current image generation and editing methods primarily process textual\nprompts as direct inputs without reasoning about visual composition and\nexplicit operations. We present Generation Chain-of-Thought (GoT), a novel\nparadigm that enables generation and editing through an explicit language\nreasoning process before outputting images. This approach transforms\nconventional text-to-image generation and editing into a reasoning-guided\nframework that analyzes semantic relationships and spatial arrangements. We\ndefine the formulation of GoT and construct large-scale GoT datasets containing\nover 9M samples with detailed reasoning chains capturing semantic-spatial\nrelationships. To leverage the advantages of GoT, we implement a unified\nframework that integrates Qwen2.5-VL for reasoning chain generation with an\nend-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance\nModule. Experiments show our GoT framework achieves excellent performance on\nboth generation and editing tasks, with significant improvements over\nbaselines. Additionally, our approach enables interactive visual generation,\nallowing users to explicitly modify reasoning steps for precise image\nadjustments. GoT pioneers a new direction for reasoning-driven visual\ngeneration and editing, producing images that better align with human intent.\nTo facilitate future research, we make our datasets, code, and pretrained\nmodels publicly available at https://github.com/rongyaofang/GoT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b8724123d948d884b379b1",
      "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
      "fullname": "Rongyao Fang",
      "name": "LucasFang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10633",
      "authors": [
        {
          "_id": "67d3ba5e4d3a41ed9f8651eb",
          "user": {
            "_id": "630dd4218df86f1e5beb2ed7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
            "isPro": false,
            "fullname": "Eliahu Horwitz",
            "user": "Eliahu",
            "type": "user"
          },
          "name": "Eliahu Horwitz",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:04.270Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ec",
          "user": {
            "_id": "674ec6d1ce68874ee4f2d53b",
            "avatarUrl": "/avatars/4c15c9bdcf51d4bf5e6fceb86195e480.svg",
            "isPro": false,
            "fullname": "Nitzan Kurer",
            "user": "nitzankur",
            "type": "user"
          },
          "name": "Nitzan Kurer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T10:09:18.093Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ed",
          "user": {
            "_id": "6465fd33dac127ac80f0b334",
            "avatarUrl": "/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg",
            "isPro": false,
            "fullname": "Jonathan Kahana",
            "user": "jonkahana",
            "type": "user"
          },
          "name": "Jonathan Kahana",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T10:09:25.338Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ee",
          "user": {
            "_id": "669ffff5944b597ce2a1aa5b",
            "avatarUrl": "/avatars/559ca0ad82b1a52208510f09492fafa6.svg",
            "isPro": false,
            "fullname": "Liel Amar",
            "user": "LielAmar",
            "type": "user"
          },
          "name": "Liel Amar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:01.634Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ef",
          "user": {
            "_id": "646cfc3b4220471ca0c56b20",
            "avatarUrl": "/avatars/19d6ab141ec2cd25c1c3b45fd8f69910.svg",
            "isPro": false,
            "fullname": "Yedid Hoshen",
            "user": "yedid",
            "type": "user"
          },
          "name": "Yedid Hoshen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T10:09:39.859Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/630dd4218df86f1e5beb2ed7/HClm12KfVuYMozbJaIp9_.png"
      ],
      "publishedAt": "2025-03-13T17:59:53.000Z",
      "submittedOnDailyAt": "2025-03-14T03:51:41.703Z",
      "title": "Haz un mapa y navegación de la modelo AtoRA de la Huezing Face.",
      "submittedOnDailyBy": {
        "_id": "630dd4218df86f1e5beb2ed7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
        "isPro": false,
        "fullname": "Eliahu Horwitz",
        "user": "Eliahu",
        "type": "user"
      },
      "summary": "Actualmente existen millones de nueral networks disponibles públicamente, lo que ha convertido la búsqueda y análisis de sus directorios en una tarea crucial. Para seleccionar estas modelos de diversas formas, es necesario un atlas, pero la mayoría de ellos tienen poca explicación, dificultando así la construcción del atlas. Para explorar la potencial de los directorios de modelos, hemos creado un atlas inicial incluyendo descripciones en Hugging Face. Este atlas permite visualizar la estructura y evolución de los modelos de manera impresionante. Mostramos en este atlas cómo se pueden predecir atributos de los modelos (por ejemplo, precisión) o analizar tendencias en modelos de visión por computador. Sin embargo, el atlas actual no está completo, y proponemos un método para describir áreas no explicadas. En particular, identificamos estructuras estructurales de alta confianza basadas en procesos de entrenamiento realistas y utilizamos estas estructuras para mapear precisamente partes del atlas que no habían sido explicadas previamente. Publicamos conjuntos de datos, código y un atlas interactivo.",
      "upvotes": 20,
      "discussionId": "67d3ba634d3a41ed9f86533a",
      "projectPage": "https://horwitz.ai/model-atlas",
      "githubRepo": "https://github.com/eliahuhorwitz/Model-Atlas",
      "ai_keywords": [
        "neural networks",
        "model repositories",
        "atlas",
        "model landscape",
        "model evolution",
        "predicting model attributes",
        "trends in computer vision models",
        "high-confidence structural priors",
        "dominant real-world model training practices",
        "interactive atlas"
      ]
    },
    "publishedAt": "2025-03-13T13:59:53.000Z",
    "title": "Charting and Navigating Hugging Face's Model Atlas",
    "summary": "As there are now millions of publicly available neural networks, searching\nand analyzing large model repositories becomes increasingly important.\nNavigating so many models requires an atlas, but as most models are poorly\ndocumented charting such an atlas is challenging. To explore the hidden\npotential of model repositories, we chart a preliminary atlas representing the\ndocumented fraction of Hugging Face. It provides stunning visualizations of the\nmodel landscape and evolution. We demonstrate several applications of this\natlas including predicting model attributes (e.g., accuracy), and analyzing\ntrends in computer vision models. However, as the current atlas remains\nincomplete, we propose a method for charting undocumented regions.\nSpecifically, we identify high-confidence structural priors based on dominant\nreal-world model training practices. Leveraging these priors, our approach\nenables accurate mapping of previously undocumented areas of the atlas. We\npublicly release our datasets, code, and interactive atlas.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/630dd4218df86f1e5beb2ed7/HClm12KfVuYMozbJaIp9_.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10633.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630dd4218df86f1e5beb2ed7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
      "fullname": "Eliahu Horwitz",
      "name": "Eliahu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09662",
      "authors": [
        {
          "_id": "67d3daf40034469b0d6cc872",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc873",
          "name": "Zikai Zhou",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc874",
          "name": "Dian Xie",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc875",
          "name": "Yuetong Fang",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc876",
          "name": "Tian Ye",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc877",
          "name": "Lichen Bai",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc878",
          "name": "Zeke Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T15:15:25.000Z",
      "submittedOnDailyAt": "2025-03-14T06:07:49.038Z",
      "title": "Kore^2: Recolección, reflexión, reinicio para generar más óptimos números de azar de manera más rápida.",
      "submittedOnDailyBy": {
        "_id": "66015e8aa4d296af07de538e",
        "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
        "isPro": false,
        "fullname": "Ye",
        "user": "Owen777",
        "type": "user"
      },
      "summary": "Mejorar la generación de modelos de imagen a partir de texto (T2I) en direcciónes rápidas y de alta calidad es una dirección de investigación prometedora, conocida como una de las líneas de trabajo más representativas. Los estudios anteriores han sacrificado la eficiencia de la muestra para mejorar la calidad visual de las imágenes sintéticas o han acelerado la muestración significativamente sin mejorar la capacidad generativa del modelo básico. Además, la mayoría de los métodos de inferencia no han logrado garantizar un rendimiento estable tanto en modelos de difusión (DMs) como en modelos auto-regressivos de visión (ARMs).\n\nEn este artículo, se presenta un nuevo paradigma de inferencia basado en plug-in e parámetros llamado CoRe^2. CoRe^2 está compuesto por tres procesos: Colección, Reflexión y Refinamiento. Inicialmente, CoRe^2 recopila la trayectoria de retroalimentación de un guía (CFG) sin restricciones de filtrado de clase, y utiliza estos datos para reflejar contenidos que son fáciles de aprender, reduciendo así la cantidad de evaluaciones funcionales en la inferencia. A continuación, CoRe^2 mejora las salidas condicionales a partir de un guía débil hacia un guía fuerte, y mejora la capacidad de generación de contenidos de alta frecuencia y realistas que son fáciles de entender por el modelo básico.\n\nA pesar de los límites de nuestras conocidas, CoRe^2 es el primer método que demostra que puede mostrar tanto eficiencia como eficacia en una amplia gama de modelos, incluyendo DMs como SDXL, SD3.5 y FLUX, y ARMs como LlamaGen. Se ha observado un mejoramiento significativo en pruebas como HPD v2, Pick-of-Pic, Drawbench, GenEval y T2I-Compbench. Además, CoRe^2 permite la integración de la mejor técnica de sampling Z y la combinación de la pureza, mejorando los scores PickScore y AES en 0.3 y 0.16, respectivamente, y reduciendo el tiempo de generación en SD3.5 en 5.64 segundos. El código está disponible en https://github.com/xie-lab-ml/CoRe/tree/main.",
      "upvotes": 20,
      "discussionId": "67d3dafb0034469b0d6ccac0",
      "ai_keywords": [
        "diffusion models (DMs)",
        "visual autoregressive models (ARMs)",
        "classifier-free guidance (CFG)",
        "HPD v2",
        "Pick-of-Pic",
        "Drawbench",
        "GenEval",
        "T2I-Compbench",
        "PickScore",
        "AES",
        "Z-Sampling",
        "SDXL",
        "SD3.5",
        "FLUX",
        "LlamaGen"
      ]
    },
    "publishedAt": "2025-03-12T11:15:25.000Z",
    "title": "CoRe^2: Collect, Reflect and Refine to Generate Better and Faster",
    "summary": "Making text-to-image (T2I) generative model sample both fast and well\nrepresents a promising research direction. Previous studies have typically\nfocused on either enhancing the visual quality of synthesized images at the\nexpense of sampling efficiency or dramatically accelerating sampling without\nimproving the base model's generative capacity. Moreover, nearly all inference\nmethods have not been able to ensure stable performance simultaneously on both\ndiffusion models (DMs) and visual autoregressive models (ARMs). In this paper,\nwe introduce a novel plug-and-play inference paradigm, CoRe^2, which comprises\nthree subprocesses: Collect, Reflect, and Refine. CoRe^2 first collects\nclassifier-free guidance (CFG) trajectories, and then use collected data to\ntrain a weak model that reflects the easy-to-learn contents while reducing\nnumber of function evaluations during inference by half. Subsequently, CoRe^2\nemploys weak-to-strong guidance to refine the conditional output, thereby\nimproving the model's capacity to generate high-frequency and realistic\ncontent, which is difficult for the base model to capture. To the best of our\nknowledge, CoRe^2 is the first to demonstrate both efficiency and effectiveness\nacross a wide range of DMs, including SDXL, SD3.5, and FLUX, as well as ARMs\nlike LlamaGen. It has exhibited significant performance improvements on HPD v2,\nPick-of-Pic, Drawbench, GenEval, and T2I-Compbench. Furthermore, CoRe^2 can be\nseamlessly integrated with the state-of-the-art Z-Sampling, outperforming it by\n0.3 and 0.16 on PickScore and AES, while achieving 5.64s time saving using\nSD3.5.Code is released at https://github.com/xie-lab-ml/CoRe/tree/main.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09662.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "66015e8aa4d296af07de538e",
      "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
      "fullname": "Ye",
      "name": "Owen777",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10622",
      "authors": [
        {
          "_id": "67d3b0a87443e648e8aa1ea6",
          "user": {
            "_id": "6552126dd8a8835b66653767",
            "avatarUrl": "/avatars/0b1dad9ebaeada8f5e7ebe453123960b.svg",
            "isPro": false,
            "fullname": "Jiachen Zhu",
            "user": "JiachenZhu",
            "type": "user"
          },
          "name": "Jiachen Zhu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T05:13:31.648Z",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1ea7",
          "name": "Xinlei Chen",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1ea8",
          "name": "Kaiming He",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1ea9",
          "name": "Yann LeCun",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1eaa",
          "name": "Zhuang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:06.000Z",
      "submittedOnDailyAt": "2025-03-14T02:59:49.783Z",
      "title": "Transformers sin Normalización",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Las capas de normalización han sido consideradas esenciales en redes neuronales modernas, aunque su utilización ha disminuido con el tiempo. Este artículo muestra que un Transformer sin capas de normalización puede alcanzar o superar el rendimiento de un Transformer con capas de normalización utilizando una técnica muy sencilla. Se presenta el Dynamic Tanh (DyT), un elemento que se ejecuta de manera dinámica en DyT(x) = tanh(alpha x), reemplazando el dropout de las capas de normalización en un Transformer. El DyT se diseñó basándose en la observación de que las capas de normalización en un Transformer generan una mapeo de entrada-salida S-formado, y se adaptó para que tenga las características de la función tanh. Al aplicar el DyT, un Transformer sin capas de normalización puede superar casi sin ajustes de parámetros el rendimiento de un Transformer con capas de normalización. Se verificaron los efectos del DyT en un Transformer en diferentes configuraciones, como entrenamiento generativo, supervisado, automático, visión por computadora y modelos de lenguaje. Estos hallazgos plantean dudas sobre la idea tradicional de que las capas de normalización son esenciales en redes neuronales modernas, y proporcionan una nueva perspectiva sobre el papel de las capas de normalización en redes neuronales profundas.",
      "upvotes": 16,
      "discussionId": "67d3b0a97443e648e8aa1f22",
      "ai_keywords": [
        "Dynamic Tanh (DyT)",
        "Transformers",
        "normalization layers",
        "layer normalization",
        "hyperparameter tuning",
        "supervised learning",
        "self-supervised learning",
        "computer vision",
        "language models"
      ]
    },
    "publishedAt": "2025-03-13T13:59:06.000Z",
    "title": "Transformers without Normalization",
    "summary": "Normalization layers are ubiquitous in modern neural networks and have long\nbeen considered essential. This work demonstrates that Transformers without\nnormalization can achieve the same or better performance using a remarkably\nsimple technique. We introduce Dynamic Tanh (DyT), an element-wise operation\nDyT(x) = tanh(alpha x), as a drop-in replacement for normalization\nlayers in Transformers. DyT is inspired by the observation that layer\nnormalization in Transformers often produces tanh-like, S-shaped input-output\nmappings. By incorporating DyT, Transformers without normalization can match or\nexceed the performance of their normalized counterparts, mostly without\nhyperparameter tuning. We validate the effectiveness of Transformers with DyT\nacross diverse settings, ranging from recognition to generation, supervised to\nself-supervised learning, and computer vision to language models. These\nfindings challenge the conventional understanding that normalization layers are\nindispensable in modern neural networks, and offer new insights into their role\nin deep networks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10622.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10596",
      "authors": [
        {
          "_id": "67d3a8950ada3dfbf617fc23",
          "name": "Rui Hu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc24",
          "name": "Lianghui Zhu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc25",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc26",
          "name": "Tianheng Cheng",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc27",
          "name": "Lei Liu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc28",
          "name": "Heng Liu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc29",
          "name": "Longjin Ran",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc2a",
          "name": "Xiaoxin Chen",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc2b",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc2c",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:43:10.000Z",
      "submittedOnDailyAt": "2025-03-14T02:31:30.611Z",
      "title": "GroundingSuite: Evaluación de Detección de Pixeles en Multigranularidad",
      "submittedOnDailyBy": {
        "_id": "646b3db131968a60a01e4cf5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
        "isPro": false,
        "fullname": "Tianheng Cheng",
        "user": "wondervictor",
        "type": "user"
      },
      "summary": "Pixel grounding es un campo que ha ganado mucho interés debido a su potencial para llenar la brecha entre modelos de visión y lenguaje, incluyendo tareas como Segmentación de Expresiones Referentes (RES). Sin embargo, su desarrollo actual está limitado por categorías de objetos restringidas, la poca diversidad de contextos y la falta de descripciones de alta calidad. Para mitigar estos limitaciones, se presenta GroundingSuite. GroundingSuite incluye tres componentes: 1) un marco de trabajo para describir datos utilizando agentes de modelos de lenguaje y visión, 2) un conjunto de datos de entrenamiento de 9.56 millones de expresiones referentes y sus correspondientes segmentaciones, y 3) un marco de evaluación benchmark de base de datos con 3.800 imágenes. El conjunto de datos de entrenamiento de GroundingSuite es diseñado para permitir que modelos entrenados alcanzan resultados más recientes. En particular, se ha alcanzado un cIoU de 68.9 en gRefCOCO y un gIoU de 55.3 en RefCOCOm. Además, el marco de trabajo de descripción de GroundingSuite es 4.5 veces más eficiente que los métodos de descripción actuales, lo que afecta directamente a la velocidad y calidad de la tarea de descripción.",
      "upvotes": 15,
      "discussionId": "67d3a8960ada3dfbf617fc8d",
      "ai_keywords": [
        "Referring Expression Segmentation (RES)",
        "Vision-Language Model (VLM)",
        "GroundingSuite",
        "cIoU",
        "gIoU",
        "gRefCOCO",
        "RefCOCOm",
        "GLaMM"
      ]
    },
    "publishedAt": "2025-03-13T13:43:10.000Z",
    "title": "GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding",
    "summary": "Pixel grounding, encompassing tasks such as Referring Expression Segmentation\n(RES), has garnered considerable attention due to its immense potential for\nbridging the gap between vision and language modalities. However, advancements\nin this domain are currently constrained by limitations inherent in existing\ndatasets, including limited object categories, insufficient textual diversity,\nand a scarcity of high-quality annotations. To mitigate these limitations, we\nintroduce GroundingSuite, which comprises: (1) an automated data annotation\nframework leveraging multiple Vision-Language Model (VLM) agents; (2) a\nlarge-scale training dataset encompassing 9.56 million diverse referring\nexpressions and their corresponding segmentations; and (3) a meticulously\ncurated evaluation benchmark consisting of 3,800 images. The GroundingSuite\ntraining dataset facilitates substantial performance improvements, enabling\nmodels trained on it to achieve state-of-the-art results. Specifically, a cIoU\nof 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the\nGroundingSuite annotation framework demonstrates superior efficiency compared\nto the current leading data annotation method, i.e., 4.5 times faster than\nthe GLaMM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10596.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646b3db131968a60a01e4cf5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
      "fullname": "Tianheng Cheng",
      "name": "wondervictor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 27
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10351",
      "authors": [
        {
          "_id": "67d39b35acb72b994659d4fd",
          "name": "Sinuo Liu",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d4fe",
          "name": "Chenyang Lyu",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d4ff",
          "name": "Minghao Wu",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d500",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d501",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d502",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6527d8b077bceabaab382a75/3_8muRazw1wwHmG9IxRGk.png"
      ],
      "publishedAt": "2025-03-13T13:27:53.000Z",
      "submittedOnDailyAt": "2025-03-14T01:29:07.562Z",
      "title": "Nuevas tendencias en traducción automática moderna: introducción a los métodos que utilizan modelos lógicos de gran escala.",
      "submittedOnDailyBy": {
        "_id": "6527d8b077bceabaab382a75",
        "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
        "isPro": false,
        "fullname": "Chenyang Lyu",
        "user": "ChenyangLyu",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de grandes modelos de inferencia (LRMs) ha abrido nuevas posibilidades en el campo de la traducción automática (MT), especialmente con el uso de la inferencia de cadena de pensamiento (CoT). Este artículo argumenta que los LRMs han reestructurado la tarea de traducción como entendimiento contextual, cultural y lingüístico, así como una tarea dinámica de inferencia, cambiando significativamente el paradigma tradicional de MT basado en red neuronal o en modelos de lenguaje grandes (LLMs). En este sentido, destacamos tres cambios fundamentales: 1) La coherencia contextual, ya que los LRMs pueden inferir explícitamente sobre oraciones complejas o contextos, o la falta de contexto, resolviendo incertidumbres y manteniendo la estructura lógica; 2) La intencionalidad cultural, ya que los modelos pueden adaptar su salida a la intención del interlocutor, las expectativas del receptor y las reglas de lenguaje social; 3) La auto-reconocimiento, ya que los LRMs pueden corregir errores potenciales en la traducción y muestran una mayor robustez, especialmente en situaciones altamente ruidosas. Investigamos diversos escenarios de traducción, como traducción estilizada, traducción a nivel documental y traducción con diversidad de modelos, demostrando la superioridad de los LRMs a través de experimentos. Además, destacamos fenómenos interesantes en la traducción, como la traducción automática de fibras, la sobrexpansión regional de la traducción y la eficiencia de la inferencia. En conclusión, pensamos que los LRMs redefinirán la traducción no como un simple traductor de texto, sino como un equipo cognitivo multilingüe, considerando el problema en un contexto más amplio y abriendo nuevas posibilidades. Este cambio de paradigma impulsa a considerar los problemas de traducción en un contexto más amplio, abriendo caminos para explorar nuevas posibilidades.",
      "upvotes": 14,
      "discussionId": "67d39b40acb72b994659d916",
      "ai_keywords": [
        "Chain-of-Thought reasoning (CoT)",
        "Large Reasoning Models (LRMs)",
        "Neural MT",
        "Contextual coherence",
        "Cultural intentionality",
        "Self-reflection",
        "Stylized translation",
        "Document-level translation",
        "Multimodal translation",
        "Auto-pivot translation",
        "Over-localisation",
        "Inference efficiency",
        "Multilingual cognitive agents"
      ]
    },
    "publishedAt": "2025-03-13T09:27:53.000Z",
    "title": "New Trends for Modern Machine Translation with Large Reasoning Models",
    "summary": "Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6527d8b077bceabaab382a75/3_8muRazw1wwHmG9IxRGk.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10351.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6527d8b077bceabaab382a75",
      "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
      "fullname": "Chenyang Lyu",
      "name": "ChenyangLyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04723",
      "authors": [
        {
          "_id": "67d39576de5ce3cc428b1909",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190a",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190b",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqing Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:31.682Z",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190c",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190d",
          "name": "Ming Shan Hee",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190e",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190f",
          "name": "Roy Ka-Wei Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T18:59:37.000Z",
      "submittedOnDailyAt": "2025-03-14T01:04:48.148Z",
      "title": "1. La transición del estudio de LLMs de largo contexto desde el input hasta el output\n2. La transición del estudio de LLMs de largo contexto hacia el output\n3. La transición del estudio de LLMs de largo contexto desde el input hasta el output\n4. La transición del estudio de LLMs de largo contexto desde el input al output\n5. La transición del estudio de LLMs de largo contexto hacia el output\n6. La transición del estudio de LLMs de largo contexto hacia el output\n7. La transición del estudio de LLMs de largo contexto hacia el output\n8. La transición del estudio de LLMs de largo contexto hacia el output\n9. La transición del estudio de LLMs de largo contexto hacia el output\n10. La transición del estudio de LLMs de largo contexto hacia el output",
      "submittedOnDailyBy": {
        "_id": "63369da91ba5d5ece24118a4",
        "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
        "isPro": false,
        "fullname": "wuyuhao",
        "user": "mozhu",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de los modelos de lenguaje de contexto largo (LLM) ha centrado su atención en la comprensión de largos contextos, logrando gran avances en este aspecto, con la excepción de que el generación de salidas en largos contextos ha recibido menos atención. Este artículo se enfoca en el cambio del paradigma de la investigación en NLP y en la resolución de los desafíos asociados con la generación de salidas en largos contextos. Nuevas tareas que requieren escribir textos largos, planificación a largo plazo y explicaciones complejas exigen que los modelos entiendan amplios contextos y generen textos largos que sean coherentes, contextualmente ricos y armoniosos. Estas demandas revelan limitaciones importantes en las capacidades actuales de los LLM. Este trabajo subraya la importancia de investigar esta área poco explorada y exige un esfuerzo concentrado para el desarrollo de LLM que generen salidas de alta calidad en largos contextos. Estos modelos tienen gran potencial para aplicaciones reales.",
      "upvotes": 12,
      "discussionId": "67d39577de5ce3cc428b194f",
      "ai_keywords": [
        "long-context Large Language Models (LLMs)",
        "long-context comprehension",
        "long-output generation",
        "novel writing",
        "long-term planning",
        "complex reasoning",
        "coherent",
        "contextually rich",
        "logically consistent",
        "extended text",
        "high-quality",
        "long-form outputs"
      ]
    },
    "publishedAt": "2025-03-06T13:59:37.000Z",
    "title": "Shifting Long-Context LLMs Research from Input to Output",
    "summary": "Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04723.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63369da91ba5d5ece24118a4",
      "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
      "fullname": "wuyuhao",
      "name": "mozhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10582",
      "authors": [
        {
          "_id": "67d387ff45b17e31c16d05d1",
          "name": "Yiming Jia",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d2",
          "name": "Jiachen Li",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d3",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d4",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d5",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d6",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d7",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-14T01:36:13.720Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/VBzj4fQkEBEzfx26BsANS.png"
      ],
      "publishedAt": "2025-03-13T17:32:48.000Z",
      "submittedOnDailyAt": "2025-03-14T00:47:38.699Z",
      "title": "VisualWebInstruct: Expansión de datos de instrucciones multimodal por búsqueda web",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "Debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la escasez de conjuntos de datos de temas similares, debido a la esc",
      "upvotes": 9,
      "discussionId": "67d3880d45b17e31c16d09d1",
      "projectPage": "https://tiger-ai-lab.github.io/VisualWebInstruct/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VisualWebInstruct",
      "ai_keywords": [
        "Vision-Language Models",
        "VisualWebInstruct",
        "search engine",
        "question-answer pairs",
        "visual QA pairs",
        "text QA pairs",
        "fine-tuned",
        "Llava-OV-mid",
        "MAmmoTH-VL",
        "MAmmoTH-VL2",
        "MMMU-Pro-std",
        "MathVerse",
        "DynaMath"
      ]
    },
    "publishedAt": "2025-03-13T13:32:48.000Z",
    "title": "VisualWebInstruct: Scaling up Multimodal Instruction Data through Web\n  Search",
    "summary": "Vision-Language Models have made significant progress on many\nperception-focused tasks, however, their progress on reasoning-focused tasks\nseem to be limited due to the lack of high-quality and diverse training data.\nIn this work, we aim to address the scarcity issue of reasoning-focused\nmultimodal datasets. We propose VisualWebInstruct - a novel approach that\nleverages search engine to create a diverse, and high-quality dataset spanning\nmultiple disciplines like math, physics, finance, chemistry, etc. Starting with\nmeticulously selected 30,000 seed images, we employ Google Image search to\nidentify websites containing similar images. We collect and process the HTMLs\nfrom over 700K unique URL sources. Through a pipeline of content extraction,\nfiltering and synthesis, we build a dataset of approximately 900K\nquestion-answer pairs, with 40% being visual QA pairs and the rest as text QA\npairs. Models fine-tuned on VisualWebInstruct demonstrate significant\nperformance gains: (1) training from Llava-OV-mid shows 10-20% absolute point\ngains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain.\nOur best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B\nparameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath\n(55.7%). These remarkable results highlight the effectiveness of our dataset in\nenhancing VLMs' reasoning capabilities for complex multimodal tasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/VBzj4fQkEBEzfx26BsANS.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10582.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10618",
      "authors": [
        {
          "_id": "67d3d2dec4a225b653154b3a",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3b",
          "name": "Rui Qian",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3c",
          "name": "Wenze Hu",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3d",
          "name": "Tsu-Jui Fu",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3e",
          "name": "Lezhi Li",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3f",
          "name": "Bowen Zhang",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b40",
          "name": "Alex Schwing",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b41",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b42",
          "name": "Yinfei Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:57:25.000Z",
      "submittedOnDailyAt": "2025-03-14T05:26:46.588Z",
      "title": "DiT-Air: Reevaluación de la eficiencia, estudio sobre la diseño de la generación de imágenes a partir del texto",
      "submittedOnDailyBy": {
        "_id": "656c2fa772c19de72367bd69",
        "avatarUrl": "/avatars/540bb3d8a2afe2ef927b80d895cae28b.svg",
        "isPro": false,
        "fullname": "Alex Yang",
        "user": "yyf86",
        "type": "user"
      },
      "summary": "En este estudio, se realizó una investigación experimental para la generación de imágenes a partir de texto utilizando Transformers de Difusión (DiTs), con un enfoque en la selección estructural, estrategias de condicionamiento de texto y protocolos de entrenamiento. Se evaluaron diferentes estructuras basadas en DiTs, y se comparó la estructura estándar DiTs, que procesa directamente el texto y el ruido de entrada. Se encontró que el rendimiento de la DiTs estándar es relativamente alto en comparación con modelos especializados, especialmente en términos de eficiencia de parámetros. Utilizando una estrategia de compartir parámetros por capas, se logró reducir el tamaño del modelo en un 66% en comparación con MMDiT, con un impacto mínimo en el rendimiento. Se presentó un análisis detallado de componentes importantes como el encoder de texto y los Variational Auto-Encoders (VAEs), y se introdujeron DiT-Air y DiT-Air-Lite. DiT-Air alcanzó el mejor rendimiento en GenEval y T2I CompBench utilizando ajustes micro de subestructuras y reward, mientras que DiT-Air-Lite muestra una alta competencia a pequeña escala, superando actualmente los modelos existentes.",
      "upvotes": 8,
      "discussionId": "67d3d302c4a225b6531556d6",
      "ai_keywords": [
        "Diffusion Transformers (DiTs)",
        "text-to-image generation",
        "architectural choices",
        "text-conditioning strategies",
        "training protocols",
        "PixArt-style",
        "MMDiT variants",
        "concatenated text and noise inputs",
        "parameter-efficiency",
        "layer-wise parameter sharing strategy",
        "Variational Auto-Encoders (VAEs)",
        "DiT-Air",
        "DiT-Air-Lite",
        "supervised and reward fine-tuning",
        "GenEval",
        "T2I CompBench",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-03-13T13:57:25.000Z",
    "title": "DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture\n  Design in Text to Image Generation",
    "summary": "In this work, we empirically study Diffusion Transformers (DiTs) for\ntext-to-image generation, focusing on architectural choices, text-conditioning\nstrategies, and training protocols. We evaluate a range of DiT-based\narchitectures--including PixArt-style and MMDiT variants--and compare them with\na standard DiT variant which directly processes concatenated text and noise\ninputs. Surprisingly, our findings reveal that the performance of standard DiT\nis comparable with those specialized models, while demonstrating superior\nparameter-efficiency, especially when scaled up. Leveraging the layer-wise\nparameter sharing strategy, we achieve a further reduction of 66% in model size\ncompared to an MMDiT architecture, with minimal performance impact. Building on\nan in-depth analysis of critical components such as text encoders and\nVariational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With\nsupervised and reward fine-tuning, DiT-Air achieves state-of-the-art\nperformance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly\ncompetitive, surpassing most existing models despite its compact size.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656c2fa772c19de72367bd69",
      "avatarUrl": "/avatars/540bb3d8a2afe2ef927b80d895cae28b.svg",
      "fullname": "Alex Yang",
      "name": "yyf86",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10589",
      "authors": [
        {
          "_id": "67d3adc5c14480a46038cecf",
          "name": "Yuwei Guo",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced0",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced1",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced2",
          "name": "Zhibei Ma",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced3",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced4",
          "name": "Zhenheng Yang",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced5",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced6",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:40:07.000Z",
      "submittedOnDailyAt": "2025-03-14T02:47:38.511Z",
      "title": "Generación de Películas mediante Ajuste de Contexto de Capítulos",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Recientemente, la tecnología de generación de imágenes ha permitido la creación de imágenes realistas en un solo segundo utilizando transformadores distribuidos escalables. Sin embargo, las imágenes mundialmente conocidas requieren coherencia visual y dinámica, lo que implica múltiples escenarios de diferentes perspectivas. En este artículo, se propone la introducción de la Long Context Tuning (LCT) para expandir el contexto de los modelos distribuidos previamente entrenados con imágenes de una sola perspectiva y para proporcionar un enfoque de entrenamiento directamente desde los datos para la coherencia en nivel de escenario. Nuestro método permite que cada perspectiva completa en su función de atención se expanda a todos los perspectivas dentro de cada escenario, combinando mapeo 3D y escenarios de ruido asincrónico, sin incluir parámetros adicionales. Después de LCT, el modelo con atención bidireccional permite la finalización eficiente con caching de KV para la generación automática y coherente. Las experimentaciones muestran que el modelo de una sola perspectiva después de LCT puede crear escenarios coherentes con múltiples perspectivas, demostrando capacidades novedosas como expansión estructural y interacción, abriendo caminos para la creación de contenido visual práctico. Para más detalles, consulte https://guoyww.github.io/projects/long-context-video/.",
      "upvotes": 6,
      "discussionId": "67d3adc7c14480a46038cf5e",
      "ai_keywords": [
        "scalable diffusion transformers",
        "context window",
        "scene-level consistency",
        "Long Context Tuning (LCT)",
        "full attention mechanisms",
        "interleaved 3D position embedding",
        "asynchronous noise strategy",
        "joint and auto-regressive shot generation",
        "bidirectional attention",
        "context-causal attention",
        "efficient KV-cache",
        "compositional generation",
        "interactive shot extension"
      ]
    },
    "publishedAt": "2025-03-13T13:40:07.000Z",
    "title": "Long Context Tuning for Video Generation",
    "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10437",
      "authors": [
        {
          "_id": "67d3adf57360ea908cf5f0bc",
          "name": "Wanhua Li",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0bd",
          "user": {
            "_id": "66f0d2036a483077eed42bfb",
            "avatarUrl": "/avatars/f7f3f726842c26b8e52c9bdd48774b8e.svg",
            "isPro": false,
            "fullname": "Renping Zhou",
            "user": "rpzhou",
            "type": "user"
          },
          "name": "Renping Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:08.159Z",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0be",
          "name": "Jiawei Zhou",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0bf",
          "name": "Yingwei Song",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c0",
          "name": "Johannes Herter",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c1",
          "name": "Minghan Qin",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c2",
          "name": "Gao Huang",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c3",
          "name": "Hanspeter Pfister",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/H5DUFDgeiid82tHVuW9cx.mp4"
      ],
      "publishedAt": "2025-03-13T14:58:22.000Z",
      "submittedOnDailyAt": "2025-03-14T02:49:34.475Z",
      "title": "4D LangSplat: 4D Lenguaje Gaussian Splat Implementado por Diversificación en Modelos de Lenguaje",
      "submittedOnDailyBy": {
        "_id": "658bb7e47459b6e471b9d2e6",
        "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
        "isPro": false,
        "fullname": "Wanhua Li",
        "user": "EthanTaylor",
        "type": "user"
      },
      "summary": "El aprendizaje del dominio de lenguaje 4D es esencial para lograr consultas lingüísticas abiertas y adaptables al tiempo en un escenario dinámico. LangSplat logró precisión y eficiencia en un escenario 3D estático basándose en características de CLIP transformadas en representaciones gaussianas 3D, pero CLIP está diseñado para tareas de texto-imagen estáticas, por lo que no puede comprender las acciones temporales de un video. En entornos reales, la significación de objetos cambia dinámicamente a lo largo del tiempo. Para construir un dominio de lenguaje 4D con precisión, es necesario obtener las características vídeo de objetos que se corresponden con las correspondientes imágenes de píxeles. Para resolver este problema, se propone 4D LangSplat. 4D LangSplat se centra en el aprendizaje del dominio de lenguaje 4D para procesar consultas abiertas y adaptables al tiempo o independientes del tiempo de manera eficiente en escenarios dinámicos. 4D LangSplat evita el aprendizaje del dominio de lenguaje a partir de características visuales y utiliza textos generados a partir de capturas vídeos para entrenar directamente a Modelos de Lenguaje Multimodal Grandes (MLLMs). En particular, proponemos el método de prompt de video, que utiliza prompts visuales y textuales para generar profundos y altamente calidad capturas temporales en MLLMs. Estas capturas son utilizadas con modelos de lenguaje grandes para realizar consultas abiertas que soporten las características de los objetos y se comunican a través de espacios de embbeding compartidos. Los objetos en el escenario 4D muestran movimientos suaves entre estados. Por lo tanto, proponemos una red de deformación de estados para modelar efectivamente los cambios temporales. A través de resultados en varios benchmarks, 4D LangSplat ha logrado resultados precisos y eficientes en consultas abiertas y adaptables al tiempo o independientes del tiempo.",
      "upvotes": 6,
      "discussionId": "67d3adf87360ea908cf5f182",
      "ai_keywords": [
        "4D language fields",
        "time-sensitive queries",
        "open-ended language queries",
        "LangSplat",
        "CLIP features",
        "3D Gaussian representations",
        "dynamic 4D fields",
        "temporal dynamics",
        "pixel-aligned",
        "object-wise video features",
        "4D LangSplat",
        "time-agnostic queries",
        "Multimodal Large Language Models (MLLMs)",
        "multimodal object-wise video prompting",
        "visual prompts",
        "text prompts",
        "detailed captions",
        "temporally consistent captions",
        "sentence embeddings",
        "shared embedding spaces",
        "status deformable network",
        "continuous changes"
      ]
    },
    "publishedAt": "2025-03-13T10:58:22.000Z",
    "title": "4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large\n  Language Models",
    "summary": "Learning 4D language fields to enable time-sensitive, open-ended language\nqueries in dynamic scenes is essential for many real-world applications. While\nLangSplat successfully grounds CLIP features into 3D Gaussian representations,\nachieving precision and efficiency in 3D static scenes, it lacks the ability to\nhandle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot\ncapture temporal dynamics in videos. Real-world environments are inherently\ndynamic, with object semantics evolving over time. Building a precise 4D\nlanguage field necessitates obtaining pixel-aligned, object-wise video\nfeatures, which current vision models struggle to achieve. To address these\nchallenges, we propose 4D LangSplat, which learns 4D language fields to handle\ntime-agnostic or time-sensitive open-vocabulary queries in dynamic scenes\nefficiently. 4D LangSplat bypasses learning the language field from vision\nfeatures and instead learns directly from text generated from object-wise video\ncaptions via Multimodal Large Language Models (MLLMs). Specifically, we propose\na multimodal object-wise video prompting method, consisting of visual and text\nprompts that guide MLLMs to generate detailed, temporally consistent,\nhigh-quality captions for objects throughout a video. These captions are\nencoded using a Large Language Model into high-quality sentence embeddings,\nwhich then serve as pixel-aligned, object-specific feature supervision,\nfacilitating open-vocabulary text queries through shared embedding spaces.\nRecognizing that objects in 4D scenes exhibit smooth transitions across states,\nwe further propose a status deformable network to model these continuous\nchanges over time effectively. Our results across multiple benchmarks\ndemonstrate that 4D LangSplat attains precise and efficient results for both\ntime-sensitive and time-agnostic open-vocabulary queries.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/H5DUFDgeiid82tHVuW9cx.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10437.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658bb7e47459b6e471b9d2e6",
      "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
      "fullname": "Wanhua Li",
      "name": "EthanTaylor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10357",
      "authors": [
        {
          "_id": "67d3da3ce0b767b3d0fae2a4",
          "user": {
            "_id": "63bbfd74141c7d395c471768",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
            "isPro": false,
            "fullname": "Viktor Moskvoretskii",
            "user": "VityaVitalich",
            "type": "user"
          },
          "name": "Viktor Moskvoretskii",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T07:38:54.624Z",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a5",
          "name": "Alina Lobanova",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a6",
          "name": "Ekaterina Neminova",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a7",
          "name": "Chris Biemann",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a8",
          "name": "Alexander Panchenko",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a9",
          "name": "Irina Nikishina",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63bbfd74141c7d395c471768/7wZapqnRg87iwXLqxiKQL.jpeg"
      ],
      "publishedAt": "2025-03-13T13:37:54.000Z",
      "submittedOnDailyAt": "2025-03-14T05:58:28.512Z",
      "title": "¿Ves algo como `cat.n.01`? Clasificación de imágenes generación de benchmark.",
      "submittedOnDailyBy": {
        "_id": "63bbfd74141c7d395c471768",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
        "isPro": false,
        "fullname": "Viktor Moskvoretskii",
        "user": "VityaVitalich",
        "type": "user"
      },
      "summary": "Este artículo investiga la posibilidad de generar imágenes mediante modelos de imágenes utilizando texto en un conjunto de prueba sin ejemplos (0shot set), explorando cómo se pueden crear imágenes que representan conceptos textualmes. Métodos basados en texto ya están establecidos para la extensión de texturalización, pero la posibilidad visual aún no ha sido explorada. En este contexto, proponemos un criterio de evaluación para la generación de imágenes textualmes. Este criterio evalúa la capacidad de los modelos para comprender conceptos textualmes y generar imágenes de alta calidad relacionadas. Incluye conocimientos generales, conceptos de WordNet aleatoriamente seleccionados y predicciones generadas por LLMs. Se evaluaron 12 modelos utilizando 9 nuevos textos relacionados con texturalización, comparándolos con un criterio de evaluación y retroalimentación humana. Además, se introdujo por primera vez una evaluación relativa utilizando retroalimentación de GPT-4 para la generación de imágenes. Los resultados de los experimentos muestran que la clasificación de los modelos es significativamente diferente a las tareas de T2I estándar. Playground-v2 y FLUX ocupan una posición superior en varios métricas y subconjuntos, mientras que el enfoque basado en búsqueda es inadecuado. Estos hallazgos claramente demuestran la posibilidad de la caleidoscopización automática de recursos de datos estructurados.",
      "upvotes": 6,
      "discussionId": "67d3da42e0b767b3d0fae455",
      "projectPage": "https://huggingface.co/collections/VityaVitalich/generated-image-wordnet-67d2c868ff1414ec2f8e0d3d",
      "ai_keywords": [
        "text-to-image models",
        "zero-shot setup",
        "taxonomy concepts",
        "text-based methods",
        "taxonomy enrichment",
        "comprehensive benchmark",
        "WordNet",
        "LLM generated predictions",
        "taxonomy-related text-to-image metrics",
        "pairwise evaluation",
        "GPT-4 feedback",
        "image generation",
        "ranking of models",
        "retrieval-based approach",
        "automating the curation of structured data resources"
      ]
    },
    "publishedAt": "2025-03-13T09:37:54.000Z",
    "title": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark",
    "summary": "This paper explores the feasibility of using text-to-image models in a\nzero-shot setup to generate images for taxonomy concepts. While text-based\nmethods for taxonomy enrichment are well-established, the potential of the\nvisual dimension remains unexplored. To address this, we propose a\ncomprehensive benchmark for Taxonomy Image Generation that assesses models'\nabilities to understand taxonomy concepts and generate relevant, high-quality\nimages. The benchmark includes common-sense and randomly sampled WordNet\nconcepts, alongside the LLM generated predictions. The 12 models are evaluated\nusing 9 novel taxonomy-related text-to-image metrics and human feedback.\nMoreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for\nimage generation. Experimental results show that the ranking of models differs\nsignificantly from standard T2I tasks. Playground-v2 and FLUX consistently\noutperform across metrics and subsets and the retrieval-based approach performs\npoorly. These findings highlight the potential for automating the curation of\nstructured data resources.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63bbfd74141c7d395c471768/7wZapqnRg87iwXLqxiKQL.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10357.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbfd74141c7d395c471768",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
      "fullname": "Viktor Moskvoretskii",
      "name": "VityaVitalich",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09642",
      "authors": [
        {
          "_id": "67d3ab8d032b54ab876cb7ec",
          "name": "Xiangyu Peng",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ed",
          "name": "Zangwei Zheng",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ee",
          "name": "Chenhui Shen",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ef",
          "name": "Tom Young",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f0",
          "name": "Xinying Guo",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f1",
          "name": "Binluo Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f2",
          "name": "Hang Xu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f3",
          "name": "Hongxin Liu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f4",
          "name": "Mingyan Jiang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f5",
          "name": "Wenjun Li",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f6",
          "name": "Yuhui Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f7",
          "name": "Anbang Ye",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f8",
          "name": "Gang Ren",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f9",
          "name": "Qianran Ma",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fa",
          "name": "Wanying Liang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fb",
          "name": "Xiang Lian",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fc",
          "name": "Xiwen Wu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fd",
          "name": "Yuting Zhong",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fe",
          "name": "Zhuangyan Li",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ff",
          "name": "Chaoyu Gong",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb800",
          "name": "Guojun Lei",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb801",
          "name": "Leijun Cheng",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb802",
          "name": "Limin Zhang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb803",
          "name": "Minghao Li",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb804",
          "name": "Ruijie Zhang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb805",
          "name": "Silan Hu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb806",
          "name": "Shijie Huang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb807",
          "name": "Xiaokang Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb808",
          "name": "Yuanheng Zhao",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb809",
          "name": "Yuqi Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb80a",
          "name": "Ziang Wei",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb80b",
          "name": "Yang You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T05:00:07.000Z",
      "submittedOnDailyAt": "2025-03-14T02:39:06.938Z",
      "title": "Open-Sora 2.0: $200,000 para entrenar un modelo de generación de vídeos a nivel comercial",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "El modelo de generación de vídeo ha experimentado una sorprendente transición en los últimos 12 meses. Aunque la calidad de los videos generados por AI ha mejorado, el tamaño del modelo ha aumentado, la cantidad de datos ha crecido y la demanda de cálculos de entrenamiento ha aumentado. En este informe, se presenta Open-Sora 2.0, un modelo de generación de vídeo a nivel comercial. Este modelo se entrenó a un costo de $200k, demostrando cómo se puede controlar significativamente el costo de entrenamiento de un modelo de alto rendimiento. Se detallan en profundidad todas las tecnologías que han contribuido a esta mejora eficiente, incluyendo la carga de datos, la arquitectura del modelo, la estrategia de entrenamiento y la optimización del sistema. Basándose en los resultados de evaluación humana y en los puntajes de VBench, Open-Sora 2.0 se compara con otros modelos de generación de vídeo mundialmente reconocidos, como el código abierto HunyuanVideo y el Runway Gen-3 Alpha, un modelo de código cerrado. El objetivo de Open-Sora 2.0 es convertirlo completamente en código abierto, democratizar el acceso a las tecnologías avanzadas de generación de vídeo y fomentar una amplia innovación y creatividad en la creación de contenido. Todos los recursos están disponibles en https://github.com/hpcaitech/Open-Sora.",
      "upvotes": 6,
      "discussionId": "67d3ab93032b54ab876cb9b0",
      "ai_keywords": [
        "video generation models",
        "data curation",
        "model architecture",
        "training strategy",
        "system optimization",
        "human evaluation",
        "VBench scores",
        "HunyuanVideo",
        "Runway Gen-3 Alpha",
        "open-source"
      ]
    },
    "publishedAt": "2025-03-12T01:00:07.000Z",
    "title": "Open-Sora 2.0: Training a Commercial-Level Video Generation Model in\n  $200k",
    "summary": "Video generation models have achieved remarkable progress in the past year.\nThe quality of AI video continues to improve, but at the cost of larger model\nsize, increased data quantity, and greater demand for training compute. In this\nreport, we present Open-Sora 2.0, a commercial-level video generation model\ntrained for only $200k. With this model, we demonstrate that the cost of\ntraining a top-performing video generation model is highly controllable. We\ndetail all techniques that contribute to this efficiency breakthrough,\nincluding data curation, model architecture, training strategy, and system\noptimization. According to human evaluation results and VBench scores,\nOpen-Sora 2.0 is comparable to global leading video generation models including\nthe open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By\nmaking Open-Sora 2.0 fully open-source, we aim to democratize access to\nadvanced video generation technology, fostering broader innovation and\ncreativity in content creation. All resources are publicly available at:\nhttps://github.com/hpcaitech/Open-Sora.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09642.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10630",
      "authors": [
        {
          "_id": "67d39f4828221b583a33be2c",
          "name": "Hang Yin",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be2d",
          "name": "Xiuwei Xu",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be2e",
          "name": "Lingqing Zhao",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be2f",
          "name": "Ziwei Wang",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be30",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be31",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:48.000Z",
      "submittedOnDailyAt": "2025-03-14T01:45:48.071Z",
      "title": "UniGoal: Objetivo hacia el que se dirige la Navegación de Objetivo-Orientada de Zero-shot",
      "submittedOnDailyBy": {
        "_id": "648ac65fd044b25978015634",
        "avatarUrl": "/avatars/2278a66fdc953220e9f8fc0ccce3ff00.svg",
        "isPro": false,
        "fullname": "Xiuwei Xu",
        "user": "xuxw98",
        "type": "user"
      },
      "summary": "En este artículo, se propone un marco generalizado para el aprendizaje supervisado orientado a objetivos en 0 dimensiones. Los métodos actuales de 0 dimensiones construyen un marco de inferencia basado en grandes modelos de lenguaje (LLM) para una tarea específica, lo que resulta en un flujo de trabajo muy complejo y no se puede generalizar entre diferentes objetivos. Para alinear con el propósito de un aprendizaje supervisado general en 0 dimensiones, se propone una representación gráfica consistente para unificar diferentes objetivos, que incluye categorías de objetos, imágenes de instancias y explicaciones textuales. Además, se transforman los escenarios que mantienen la observación de las salidas en línea a gráficos y utilizan esta representación consistente de escenario y objetivo para preservar más información estructural que la texto, permitiendo el uso de LLM basado en grafos. Específicamente, se realiza el ajuste de grafos entre el escenario y el objetivo en cada etapa de tiempo, y se proponen diferentes estrategias para explorar objetivos a largo plazo dependiendo del estado de ajuste. En el caso de un ajuste de 0 dimensiones, se exploran grafos parciales de objetivos repetidamente, mientras que en el caso de un ajuste parcial, se estima la posición del objetivo utilizando proyecciones de coordenadas y alineamientos de pares. Finalmente, se aplican correcciones al escenario y verificaciones de objetivo para lograr un ajuste completo. Además, se introduce la función de negación para permitir cambios adecuados en el orden. A través de experimentos ampliados en varios benchmarks, nuestro UniGoal logra alcanzar el mejor rendimiento 0 dimensiones en tres tareas de aprendizaje supervisado, superando tanto los métodos específicos de 0 dimensiones como el método general observado.",
      "upvotes": 5,
      "discussionId": "67d39f4928221b583a33be7f",
      "ai_keywords": [
        "universal zero-shot goal-oriented navigation",
        "zero-shot methods",
        "large language models (LLM)",
        "uniform graph representation",
        "object category",
        "instance image",
        "text description",
        "scene graph",
        "explicit graph-based reasoning",
        "graph matching",
        "long-term goal of exploration",
        "subgraph search",
        "coordinate projection",
        "anchor pair alignment",
        "scene graph correction",
        "goal verification",
        "blacklist mechanism",
        "UniGoal"
      ]
    },
    "publishedAt": "2025-03-13T13:59:48.000Z",
    "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
    "summary": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10630.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648ac65fd044b25978015634",
      "avatarUrl": "/avatars/2278a66fdc953220e9f8fc0ccce3ff00.svg",
      "fullname": "Xiuwei Xu",
      "name": "xuxw98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10460",
      "authors": [
        {
          "_id": "67d3a87df6ea55297c3cd071",
          "name": "Liang Wen",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd072",
          "name": "Yunke Cai",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd073",
          "name": "Fenrui Xiao",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd074",
          "name": "Xin He",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd075",
          "name": "Qi An",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd076",
          "name": "Zhenyu Duan",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd077",
          "name": "Yimin Du",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd078",
          "name": "Junchen Liu",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd079",
          "name": "Lifu Tang",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07a",
          "name": "Xiaowei Lv",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07b",
          "name": "Haosheng Zou",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07c",
          "name": "Yongchao Deng",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07d",
          "name": "Shousheng Jia",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07e",
          "name": "Xiangzheng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T15:29:22.000Z",
      "submittedOnDailyAt": "2025-03-14T02:25:27.538Z",
      "title": "Light-R1: La Curriculum SFT, DPO y RL muestran mejores resultados que el scratch desde una perspectiva de largo plazo en comparación con el COT.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "En este artículo se presentan los resultados de la investigación de la serie Light-R1. Se ha publicado el modelo, los datos y el código.\n\nPrimero, se centraremos en el inicio de los modelos de COT a largo plazo. En particular, se sostiene que es crucial comenzar con un modelo que no tiene capacidades de COT a largo plazo en su inicio. Usando un curriculum de entrenamiento que consiste en dos etapas de SFT y una política de parámetros reversas DPO, se entrenó el modelo Light-R1-32B a partir de Qwen2.5-32B-Instruct, demostrando un rendimiento superior en matemáticas en comparación con DeepSeek-R1-Distill-Qwen-32B. El modelo Light-R1-32B, entrenado solo con datos de matemáticas, demostró un fuerte rendimiento de generalización en otros campos.\n\nEn el siguiente paso, se afirma la importancia de la colección de 3k datos construida en la segunda etapa de SFT y se utilizó para fine-tuning el modelo DeepSeek-R1-Distilled, obteniendo un nuevo modelo de SOTA. El modelo Light-R1-32B-DS, tanto para el modelo de 7B como para el de 14B, presentó un rendimiento comparable al de QwQ-32B y DeepSeek-R1.\n\nAdemás, se intentó mejorar el rendimiento de los modelos de COT a largo plazo mediante el aprendizaje por refuerzo, en particular aplicando GRPO. El modelo final Light-R1-14B-DS se entrenó usando RL y alcanzó el rendimiento de SOTA en el campo de la matemática, con puntuaciones de 74.0 y 60.2 en las pruebas AIME24 y AIME25, respectivamente. Este modelo superó a los modelos de 32B como DeepSeek-R1-Distill-Llama-70B. Además, mostró comportamientos de acción esperados por RL, con un aumento tanto en la longitud de las respuestas como en los puntajes de recompensa.\n\nLa investigación de la serie Light-R1 demuestra el inicio de los modelos de COT a largo plazo, la artesanía de los datos de SFT y la publicación de un modelo de SOTA utilizando RL.",
      "upvotes": 5,
      "discussionId": "67d3a87ef6ea55297c3cd0d0",
      "ai_keywords": [
        "COT models",
        "curriculum training",
        "two-stage SFT",
        "semi-on-policy DPO",
        "long COT capabilities",
        "SOTA",
        "generalization",
        "3k dataset",
        "fine-tuning",
        "GRPO",
        "reasoning performance",
        "AIME24 & 25 scores",
        "response length",
        "reward score"
      ]
    },
    "publishedAt": "2025-03-13T11:29:22.000Z",
    "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond",
    "summary": "This paper presents our work on the Light-R1 series, with models, data, and\ncode all released.\n  We first focus on training long COT models from scratch, specifically\nstarting from models initially lacking long COT capabilities. Using a\ncurriculum training recipe consisting of two-stage SFT and semi-on-policy DPO,\nwe train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in\nsuperior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite\nbeing trained exclusively on math data, Light-R1-32B shows strong\ngeneralization across other domains. In the subsequent phase of this work, we\nhighlight the significant benefit of the 3k dataset constructed for the second\nSFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled\nmodels using this dataset, we obtain new SOTA models in 7B and 14B, while the\n32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.\n  Furthermore, we extend our work by applying reinforcement learning,\nspecifically GRPO, on long-COT models to further improve reasoning performance.\nWe successfully train our final Light-R1-14B-DS with RL, achieving SOTA\nperformance among 14B parameter models in math. With AIME24 & 25 scores of 74.0\nand 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and\nDeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected\nbehavior, showing simultaneous increase in response length and reward score.\n  The Light-R1 series of work validates training long-COT models from scratch,\nshowcases the art in SFT data and releases SOTA models from RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10460.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09905",
      "authors": [
        {
          "_id": "67d393660d51cf27930a5e5d",
          "user": {
            "_id": "67d3930e4d3a41ed9f7ac71e",
            "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
            "isPro": false,
            "fullname": "Allison Andreyev",
            "user": "allisonandreyev",
            "type": "user"
          },
          "name": "Allison Andreyev",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T02:25:50.445Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T23:50:35.000Z",
      "submittedOnDailyAt": "2025-03-14T00:57:22.699Z",
      "title": "OpenAI's Whisper Model Digitalization: A Relatively Analytical Approach",
      "submittedOnDailyBy": {
        "_id": "67d3930e4d3a41ed9f7ac71e",
        "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
        "isPro": false,
        "fullname": "Allison Andreyev",
        "user": "allisonandreyev",
        "type": "user"
      },
      "summary": "El modelo de reconocimiento automático de voz (ASR) desempeña un papel importante en diversas aplicaciones como captura de voz, traducción de idiomas y texto en línea. En este artículo, se estudian y comparan dos variantes del modelo Whisper: uno diseñado para el streaming de voz en línea y otro para la texto en línea en entornos offline. En particular, se observa que estos modelos pueden disminuir la confianza en la texto generado debido a errores de lectura o la generación de contenido virtual. Además, los grandes modelos presentan problemas de radiación cuando se procesan en dispositivos con restricciones de radiación. El artículo analiza las similitudes y diferencias entre tres modelos Whisper y realiza una evaluación cualitativa. Luego, se evalúa cuanto la estrategia de procesamiento de datos afecta la radiación y se evalúa la posibilidad de procesamiento en dispositivos de punto de corte. Usando el conjunto de datos LibriSpeech, se realizan análisis de la tasa de error de palabra (WER) y radiación de Whispercpp con tres métodos de procesamiento de datos: INT4, INT5 y INT8. Finalmente, se concluye que el procesamiento de datos puede reducir la radiación en un 19% y reducir el tamaño del modelo en un 45% mientras se mantiene la precisión de texto. Estos resultados proporcionan casos de uso óptimos para diferentes modelos Whisper y la posibilidad de procesamiento en dispositivos de punto de corte. Todo el código, conjuntos de datos y detalles de implementación están disponibles en el repositorio GitHub: https://github.com/allisonandreyev/WhisperQuantization.git",
      "upvotes": 5,
      "discussionId": "67d393670d51cf27930a5e98",
      "githubRepo": "https://github.com/allisonandreyev/WhisperQuantization",
      "ai_keywords": [
        "Whisper",
        "live speech streaming",
        "offline transcription",
        "hallucinated content",
        "latency",
        "deployment",
        "quantization",
        "LibriSpeech dataset",
        "word error rate (WER)",
        "whispercpp",
        "INT4",
        "INT5",
        "INT8",
        "speech recognition (ASR)",
        "transcription accuracy",
        "edge deployment"
      ]
    },
    "publishedAt": "2025-03-12T19:50:35.000Z",
    "title": "Quantization for OpenAI's Whisper Models: A Comparative Analysis",
    "summary": "Automated speech recognition (ASR) models have gained prominence for\napplications such as captioning, speech translation, and live transcription.\nThis paper studies Whisper and two model variants: one optimized for live\nspeech streaming and another for offline transcription. Notably, these models\nhave been found to generate hallucinated content, reducing transcription\nreliability. Furthermore, larger model variants exhibit increased latency and\npose challenges for deployment on resource-constrained devices. This study\nanalyzes the similarities and differences between three Whisper models,\nqualitatively examining their distinct capabilities. Next, this study\nquantifies the impact of model quantization on latency and evaluates its\nviability for edge deployment. Using the open source LibriSpeech dataset, this\npaper evaluates the word error rate (WER) along with latency analysis of\nwhispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that\nquantization reduces latency by 19\\% and model size by 45\\%, while preserving\ntranscription accuracy. These findings provide insights into the optimal use\ncases of different Whisper models and edge device deployment possibilities. All\ncode, datasets, and implementation details are available in a public GitHub\nrepository: https://github.com/allisonandreyev/WhisperQuantization.git",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67d3930e4d3a41ed9f7ac71e",
      "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
      "fullname": "Allison Andreyev",
      "name": "allisonandreyev",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09641",
      "authors": [
        {
          "_id": "67d3b008e18f86384bd33fa1",
          "name": "Junsong Chen",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa2",
          "name": "Shuchen Xue",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa3",
          "name": "Yuyang Zhao",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa4",
          "name": "Jincheng Yu",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa5",
          "user": {
            "_id": "5f7fbd813e94f16a85448745",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
            "isPro": false,
            "fullname": "Sayak Paul",
            "user": "sayakpaul",
            "type": "user"
          },
          "name": "Sayak Paul",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:06.327Z",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa6",
          "name": "Junyu Chen",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa7",
          "name": "Han Cai",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa8",
          "name": "Enze Xie",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa9",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T04:53:07.000Z",
      "submittedOnDailyAt": "2025-03-14T02:57:11.544Z",
      "title": "SANA-Sprint: 1 paso difuso y consistencia temporal experimental de transmisión",
      "submittedOnDailyBy": {
        "_id": "5f7fbd813e94f16a85448745",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
        "isPro": false,
        "fullname": "Sayak Paul",
        "user": "sayakpaul",
        "type": "user"
      },
      "summary": "En este artículo, se presenta el modelo SANA-Sprint eficiente y se implementa la metodología para generar imágenes a partir de textos de alta velocidad (T2I). El SANA-Sprint se construye sobre un modelo pre-entrenado, agregando una distillación híbrida para reducir significativamente el tiempo de cálculo (de 20 etapas a 1-4 etapas). Se presentan tres innovaciones clave: (1) se propone un enfoque sin entrenamiento y se aplica un modelo de ajuste de flores a la distillación de coincidencias en el tiempo continuo (sCM) para eliminar el alto costo de entrenamiento desde el principio y lograr una alta eficiencia de entrenamiento. La estrategia de distillación híbrida combina a sCM y a la potencial distillación de desafío relativo (LADD): sCM garantiza la harmonía entre el modelo de enseñanza y LADD mejora la confianza en la generación en un solo paso. (2) El SANA-Sprint logra generar de alta calidad en 1-4 etapas, eliminando el entrenamiento por etapas para mejorar la eficiencia. Esta innovación continua establece un nuevo paradigma en el trade-off entre velocidad y calidad, al alcanzar un rendimiento reciente de 7.59 FID y 0.74 GenEval en un solo paso (superando a FLUX-schnell con 7.94 FID y 0.71 GenEval, y siendo 10 veces más rápido en H100 (0.1s vs 1.1s)). Además, en H100, se logra un tiempo de 0.1s para la generación de imágenes T2I de 1024 x 1024 y 0.25s con ControlNet, demostrando las funciones y potencialidades de aplicaciones de consumo basadas en IA (AIPC). El código y los modelos pre-entrenados están publicados bajo un licencia de código abierto.",
      "upvotes": 5,
      "discussionId": "67d3b00be18f86384bd3408f",
      "ai_keywords": [
        "diffusion model",
        "text-to-image (T2I)",
        "pre-trained foundation model",
        "hybrid distillation",
        "flow-matching model",
        "continuous-time consistency distillation (sCM)",
        "latent adversarial distillation (LADD)",
        "unified step-adaptive model",
        "ControlNet",
        "real-time interactive image generation",
        "FID",
        "GenEval",
        "FLUX-schnell",
        "H100",
        "RTX 4090",
        "AI-powered consumer applications (AIPC)"
      ]
    },
    "publishedAt": "2025-03-12T00:53:07.000Z",
    "title": "SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency\n  Distillation",
    "summary": "This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast\ntext-to-image (T2I) generation. SANA-Sprint is built on a pre-trained\nfoundation model and augmented with hybrid distillation, dramatically reducing\ninference steps from 20 to 1-4. We introduce three key innovations: (1) We\npropose a training-free approach that transforms a pre-trained flow-matching\nmodel for continuous-time consistency distillation (sCM), eliminating costly\ntraining from scratch and achieving high training efficiency. Our hybrid\ndistillation strategy combines sCM with latent adversarial distillation (LADD):\nsCM ensures alignment with the teacher model, while LADD enhances single-step\ngeneration fidelity. (2) SANA-Sprint is a unified step-adaptive model that\nachieves high-quality generation in 1-4 steps, eliminating step-specific\ntraining and improving efficiency. (3) We integrate ControlNet with SANA-Sprint\nfor real-time interactive image generation, enabling instant visual feedback\nfor user interaction. SANA-Sprint establishes a new Pareto frontier in\nspeed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID\nand 0.74 GenEval in only 1 step - outperforming FLUX-schnell (7.94 FID / 0.71\nGenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s\n(T2I) and 0.25s (ControlNet) latency for 1024 x 1024 images on H100, and 0.31s\n(T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for\nAI-powered consumer applications (AIPC). Code and pre-trained models will be\nopen-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09641.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f7fbd813e94f16a85448745",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
      "fullname": "Sayak Paul",
      "name": "sayakpaul",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 586
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10637",
      "authors": [
        {
          "_id": "67d3a2a6977358f62157977d",
          "user": {
            "_id": "636daf1b56c0762cfda074b5",
            "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
            "isPro": false,
            "fullname": "Rohit Gandikota",
            "user": "RohitGandikota",
            "type": "user"
          },
          "name": "Rohit Gandikota",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:16.089Z",
          "hidden": false
        },
        {
          "_id": "67d3a2a6977358f62157977e",
          "name": "David Bau",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:56.000Z",
      "submittedOnDailyAt": "2025-03-14T02:03:33.686Z",
      "title": "Utilización de modelos distribuidos para la diversidad y el control",
      "submittedOnDailyBy": {
        "_id": "636daf1b56c0762cfda074b5",
        "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
        "isPro": false,
        "fullname": "Rohit Gandikota",
        "user": "RohitGandikota",
        "type": "user"
      },
      "summary": "El modelo de dificultad destilado presenta un gran desafío fundamental: una significativa disminución de la diversidad de los ejemplos en comparación con los modelos básicos. En este estudio, demostramos que, a pesar de esta reducción, los modelos destilados mantienen la representación de los conceptos básicos del modelo básico. Esto abre la posibilidad de que estructuras de control entrenadas en el modelo básico (como Concept Sliders y LoRAs) puedan ser transferidas a los modelos destilados sin ser ignoradas, y viceversa, manteniendo el mismo efecto. La conservación de estas estructuras de representación permitió iniciar una investigación sobre la pérdida de diversidad durante el proceso de destilación.\n\nIntroducimos el DT-Visualization, un instrumento de análisis y depuración que permite visualizar el comportamiento del modelo en las etapas intermedias para predecir el output final. A través del DT-Visualization, identificamos retroalimentación generada, discontinuidades y observamos que las fases iniciales de dificultad determinan de manera inequívoca la diversidad del output, mientras que las fases posteriores se concentran en la precisión de detalles. Desde esta perspectiva, proponemos un enfoque híbrido de inferencia llamado \"Diversity-Difficulty Distillation\", que permite que el modelo básico se convierta en un modelo destilado eficiente utilizando solo las fases iniciales cruciales. Esta simple modificación recupera la función de diversidad desde el modelo básico y supera a niveles sorprendentes, manteniendo al mismo tiempo la eficiencia computacional de la inferencia destilada y evitando la necesidad de entrenamiento adicional o cambios en el modelo.\n\nEl código y los datos del estudio están disponibles en https://distillation.baulab.info.",
      "upvotes": 4,
      "discussionId": "67d3a2ab977358f6215798fc",
      "projectPage": "https://distillation.baulab.info",
      "githubRepo": "https://github.com/rohitgandikota/distillation",
      "ai_keywords": [
        "distilled diffusion models",
        "sample diversity",
        "base models",
        "Concept Sliders",
        "LoRAs",
        "control distillation",
        "representational structure",
        "diversity collapse",
        "Diffusion Target (DT) Visualization",
        "intermediate steps",
        "generation artifacts",
        "inconsistencies",
        "diffusion timesteps",
        "diversity distillation",
        "hybrid inference approach"
      ]
    },
    "publishedAt": "2025-03-13T13:59:56.000Z",
    "title": "Distilling Diversity and Control in Diffusion Models",
    "summary": "Distilled diffusion models suffer from a critical limitation: reduced sample\ndiversity compared to their base counterparts. In this work, we uncover that\ndespite this diversity loss, distilled models retain the fundamental concept\nrepresentations of base models. We demonstrate control distillation - where\ncontrol mechanisms like Concept Sliders and LoRAs trained on base models can be\nseamlessly transferred to distilled models and vice-versa, effectively\ndistilling control without any retraining. This preservation of\nrepresentational structure prompted our investigation into the mechanisms of\ndiversity collapse during distillation. To understand how distillation affects\ndiversity, we introduce Diffusion Target (DT) Visualization, an analysis and\ndebugging tool that reveals how models predict final outputs at intermediate\nsteps. Through DT-Visualization, we identify generation artifacts,\ninconsistencies, and demonstrate that initial diffusion timesteps\ndisproportionately determine output diversity, while later steps primarily\nrefine details. Based on these insights, we introduce diversity distillation -\na hybrid inference approach that strategically employs the base model for only\nthe first critical timestep before transitioning to the efficient distilled\nmodel. Our experiments demonstrate that this simple modification not only\nrestores the diversity capabilities from base to distilled models but\nsurprisingly exceeds it, while maintaining nearly the computational efficiency\nof distilled inference, all without requiring additional training or model\nmodifications. Our code and data are available at\nhttps://distillation.baulab.info",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10637.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636daf1b56c0762cfda074b5",
      "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
      "fullname": "Rohit Gandikota",
      "name": "RohitGandikota",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10615",
      "authors": [
        {
          "_id": "67d3aa3f2f42ed5552e8ea0c",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea0d",
          "name": "Xiaoxuan He",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea0e",
          "name": "Hongkun Pan",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea0f",
          "name": "Xiyan Jiang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea10",
          "name": "Yan Deng",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea11",
          "name": "Xingtao Yang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea12",
          "name": "Haoyu Lu",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea13",
          "name": "Dacheng Yin",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea14",
          "name": "Fengyun Rao",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea15",
          "name": "Minfeng Zhu",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea16",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea17",
          "name": "Wei Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:56:05.000Z",
      "submittedOnDailyAt": "2025-03-14T02:32:20.499Z",
      "title": "R1-Onevision: Formación Crossmodal para la Inferencia Multimodal Extendida",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje general tienen un excelente rendimiento en tareas de contexto compleja debido a su fuerte lógica. Sin embargo, la integración de información visual y textual en el teoría multimodal lógica sigue siendo un gran desafío. Los modelos de etiquetado visual actuales enfrentan dificultades para analizar de manera efectiva el contenido visual y aplicar lógica, y frecuentemente no muestran el mejor desempeño en tareas lógicas complejas. Además, la escasez de marcos de referencia detallados impide una evaluación precisa de la capacidad de teoría multimodal lógica. En este artículo, se presenta R1-Onevision, un modelo de teoría multimodal lógica. Este modelo ha sido diseñado para conectar el reconocimiento visual con la lógica profunda. Para lograrlo, se propone una pipeline de lógica multimodal para convertir imágenes en una representación de contexto formal. Este pipeline se utiliza para construir el conjunto de datos R1-Onevision y proporcionar etiquetaciones multimodal lógicas detalladas en diferentes áreas. Además, el modelo R1-Onevision se ha optimizado mediante ajustes micro de normalización y aprendizaje por refuerzo, lo que ha permitido aumentar su lógica y su capacidad de generalización. Para evaluar la capacidad de teoría multimodal lógica de manera integral, se presenta el marco de referencia R1-Onevision-Bench, que se adapta a los estándares educativos humanos desde la escuela primaria hasta la universidad. Los resultados de los experimentos muestran que R1-Onevision ha alcanzado los mejores rendimientos, superando a modelos como GPT-4o y Qwen2.5-VL en los benchmarks de teoría multimodal lógica complejos.",
      "upvotes": 4,
      "discussionId": "67d3aa422f42ed5552e8eaee",
      "ai_keywords": [
        "multimodal reasoning",
        "cross-modal reasoning",
        "formal textural representations",
        "R1-Onevision dataset",
        "supervised fine-tuning",
        "reinforcement learning",
        "R1-Onevision-Bench",
        "GPT-4o",
        "Qwen2.5-VL"
      ]
    },
    "publishedAt": "2025-03-13T13:56:05.000Z",
    "title": "R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization",
    "summary": "Large Language Models have demonstrated remarkable reasoning capability in\ncomplex textual tasks. However, multimodal reasoning, which requires\nintegrating visual and textual information, remains a significant challenge.\nExisting visual-language models often struggle to effectively analyze and\nreason visual content, resulting in suboptimal performance on complex reasoning\ntasks. Moreover, the absence of comprehensive benchmarks hinders the accurate\nassessment of multimodal reasoning capabilities. In this paper, we introduce\nR1-Onevision, a multimodal reasoning model designed to bridge the gap between\nvisual perception and deep reasoning. To achieve this, we propose a cross-modal\nreasoning pipeline that transforms images into formal textural representations,\nenabling precise language-based reasoning. Leveraging this pipeline, we\nconstruct the R1-Onevision dataset which provides detailed, step-by-step\nmultimodal reasoning annotations across diverse domains. We further develop the\nR1-Onevision model through supervised fine-tuning and reinforcement learning to\ncultivate advanced reasoning and robust generalization abilities. To\ncomprehensively evaluate multimodal reasoning performance across different\ngrades, we introduce R1-Onevision-Bench, a benchmark aligned with human\neducational stages, covering exams from junior high school to university and\nbeyond. Experimental results show that R1-Onevision achieves state-of-the-art\nperformance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple\nchallenging multimodal reasoning benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10615.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10568",
      "authors": [
        {
          "_id": "67d3a952687a7a8a4963a030",
          "user": {
            "_id": "64e86fbd0c2413c3571ef7a6",
            "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
            "isPro": false,
            "fullname": "Haopeng Li",
            "user": "hp-l33",
            "type": "user"
          },
          "name": "Haopeng Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:10.236Z",
          "hidden": false
        },
        {
          "_id": "67d3a952687a7a8a4963a031",
          "name": "Jinyue Yang",
          "hidden": false
        },
        {
          "_id": "67d3a952687a7a8a4963a032",
          "name": "Guoqi Li",
          "hidden": false
        },
        {
          "_id": "67d3a952687a7a8a4963a033",
          "name": "Huan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:19:51.000Z",
      "submittedOnDailyAt": "2025-03-14T02:32:07.479Z",
      "title": "Generación de Imágenes por Auto-Regresión con Decodificación Paralela y Randomización",
      "submittedOnDailyBy": {
        "_id": "64e86fbd0c2413c3571ef7a6",
        "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
        "isPro": false,
        "fullname": "Haopeng Li",
        "user": "hp-l33",
        "type": "user"
      },
      "summary": "Introducing ARPG. ARPG es un nuevo modelo de regresión funcional visual que aborda las limitaciones propias de la aproximación secuencial de LasTAR actual, permitiendo generación paralela aleatoria para mejorar la eficiencia de inferencia y evitar la carga de generalización Zero-Shot. Nuestra principal perspectiva es que para modelar secuencias aleatorias efectivamente, es necesario tener guías explícitas para determinar la ubicación de los etiquetas de predicción. Por lo tanto, proponemos un nuevo marco de guías que separan la guía de ubicación del contenido y codifican ambos en pares de consulta y clave-valor. Integrando esta guía directamente en una estructura de atención causal, nuestro enfoque permite entrenamiento y generación en secuencias completamente aleatorias, eliminando la necesidad de atención bidireccional. De esta manera, ARPG generaliza naturalmente en tareas Zero-Shot como la entrada de imágenes, OOP y expandidos de reso-cuenta. Además, al realizar la etapa de muestreo aleatoria en 64 pasos, ARPG logra reducir en más del 75% el consumo de memoria, alcanzando un aumento de 20 veces en la cantidad de transacciones en el marco de referencia de ImageNet-1K 256, al igual que los modelos de regresión funcional automática recientes de tamaño representativo.",
      "upvotes": 4,
      "discussionId": "67d3a957687a7a8a4963a179",
      "projectPage": "https://hp-l33.github.io/projects/arpg",
      "githubRepo": "https://github.com/hp-l33/ARPG",
      "ai_keywords": [
        "autoregressive model",
        "randomized parallel generation",
        "raster-order approaches",
        "inference efficiency",
        "zero-shot generalization",
        "sequential, predefined token generation order",
        "guided decoding framework",
        "positional guidance",
        "content representation",
        "queries",
        "key-value pairs",
        "causal attention mechanism",
        "fully random-order training",
        "bidirectional attention",
        "image inpainting",
        "outpainting",
        "resolution expansion",
        "parallel inference",
        "ImageNet-1K 256 benchmark",
        "FID",
        "sampling steps",
        "throughput",
        "memory consumption"
      ]
    },
    "publishedAt": "2025-03-13T13:19:51.000Z",
    "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
    "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10568.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e86fbd0c2413c3571ef7a6",
      "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
      "fullname": "Haopeng Li",
      "name": "hp-l33",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10391",
      "authors": [
        {
          "_id": "67d39679ea264394acf948ad",
          "name": "Yufan Deng",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948ae",
          "name": "Xun Guo",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948af",
          "name": "Yizhi Wang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b0",
          "name": "Jacob Zhiyuan Fang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b1",
          "name": "Angtian Wang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b2",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:29.782Z",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b3",
          "name": "Yiding Yang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b4",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b5",
          "name": "Haibin Huang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b6",
          "name": "Chongyang Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T14:07:58.000Z",
      "submittedOnDailyAt": "2025-03-14T01:07:59.040Z",
      "title": "CINEMA: Generación de múltiples subtítulos de vídeo mediante MLLM basada en guía",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "La generación de videos está experimentando una sorprendente revolución gracias al surgimiento de modelos de aprendizaje profundo, especialmente los modelos distribuidos. Los métodos anteriores se han fallado en generar altas calidades de videos desde proyectos de texto o solo imágenes. Sin embargo, la generación de videos personalizados para varios temas sigue siendo un gran desafío y ha sido poco explorado. Este problema radica en la complejidad de sintetizar videos que incluyan diversos temas y mantener coherencia temporal y espacial. Los enfoques actuales principalmente dependen de que las imágenes de tema se correspondan con los palabros clave de proyectos de texto, lo que genera incertidumbre y dificulta la modelación efectiva de las relaciones entre temas. En este artículo, proponemos un nuevo marco de trabajo para la generación de videos de múltiples temas coherentes mediante la extensión de modelos grandes de lenguaje multimodal (MLLM). Nuestro enfoque evita la necesidad de una correspondencia clara entre imágenes de tema y entidades textuales, reduciendo la incertidumbre y el esfuerzo de anotación. Al interpretar las relaciones entre temas, nuestro método promueve la escalabilidad y permite entrenar con grandes conjuntos de datos variados. Además, nuestro marco es condicional con respecto a la cantidad de temas variables, proporcionando mayor flexibilidad en la creación de contenido personalizado. Mediante evaluaciones extendidas, demostramos que nuestro enfoque significativamente mejora la coherencia de temas y la coherencia de todo el video, abriéndose caminos para aplicaciones avanzadas en la narrativa, interacción multimedia y la generación de videos personalizados.",
      "upvotes": 4,
      "discussionId": "67d3967aea264394acf94915",
      "ai_keywords": [
        "diffusion models",
        "multimodal large language model (MLLM)"
      ]
    },
    "publishedAt": "2025-03-13T10:07:58.000Z",
    "title": "CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance",
    "summary": "Video generation has witnessed remarkable progress with the advent of deep\ngenerative models, particularly diffusion models. While existing methods excel\nin generating high-quality videos from text prompts or single images,\npersonalized multi-subject video generation remains a largely unexplored\nchallenge. This task involves synthesizing videos that incorporate multiple\ndistinct subjects, each defined by separate reference images, while ensuring\ntemporal and spatial consistency. Current approaches primarily rely on mapping\nsubject images to keywords in text prompts, which introduces ambiguity and\nlimits their ability to model subject relationships effectively. In this paper,\nwe propose CINEMA, a novel framework for coherent multi-subject video\ngeneration by leveraging Multimodal Large Language Model (MLLM). Our approach\neliminates the need for explicit correspondences between subject images and\ntext entities, mitigating ambiguity and reducing annotation effort. By\nleveraging MLLM to interpret subject relationships, our method facilitates\nscalability, enabling the use of large and diverse datasets for training.\nFurthermore, our framework can be conditioned on varying numbers of subjects,\noffering greater flexibility in personalized content creation. Through\nextensive evaluations, we demonstrate that our approach significantly improves\nsubject consistency, and overall video coherence, paving the way for advanced\napplications in storytelling, interactive media, and personalized video\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 34
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10636",
      "authors": [
        {
          "_id": "67d39ae1faaad4ed2df1cc61",
          "user": {
            "_id": "63041b541dd5d3c62486c294",
            "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
            "isPro": false,
            "fullname": "Ho Kei Cheng",
            "user": "hkchengrex",
            "type": "user"
          },
          "name": "Ho Kei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:19.361Z",
          "hidden": false
        },
        {
          "_id": "67d39ae1faaad4ed2df1cc62",
          "name": "Alexander Schwing",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63041b541dd5d3c62486c294/ZHc69zwDiWkyl0cNnp5zn.png"
      ],
      "publishedAt": "2025-03-13T17:59:56.000Z",
      "submittedOnDailyAt": "2025-03-14T01:27:30.007Z",
      "title": "Condición del hechizo: Analizar y mejorar la optimización de la entrega del flujo base de flujo condicional.",
      "submittedOnDailyBy": {
        "_id": "63041b541dd5d3c62486c294",
        "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
        "isPro": false,
        "fullname": "Ho Kei Cheng",
        "user": "hkchengrex",
        "type": "user"
      },
      "summary": "Minibatch óptimo transmisión siempre ordena la ruta del flujo. Esto permite reducir la cantidad de pasos de integración y la complejidad de la solución numérica de ecuaciones de derivadas numéricas durante la prueba, reduciendo así la cantidad de cálculos. Sin embargo, en casos condicionales, el óptimo transmisión minibatch no es suficiente. Esto se debe a que el mapeo óptimo de transmisión básico ignora las condiciones y una tendencia condicional surge durante el entrenamiento. En contraste, durante la prueba, no se puede acceder a una distribución inicial sesgada y se muestran muestras desde todas las distribuciones iniciales sin sesgo. Este gap entre el entrenamiento y la prueba deteriora el rendimiento de las funciones. Para mitigar este gap, proponemos el óptimo transmisión condicional C^2OT. C^2OT agrega pesos condicionales a la matriz de costo al calcular el mapeo óptimo de transmisión. Las experimentaciones se realizaron en 8gaussians-to-moons, CIFAR-10, ImageNet-32x32 y ImageNet-256x256, y se adaptan tanto a condiciones discretas como continuas. Nuestro método muestra un mejor rendimiento en general dentro de diferentes barridos de funciones evaluadas, comparado con los baselines actuales. El código está disponible en https://hkchengrex.github.io/C2OT.",
      "upvotes": 3,
      "discussionId": "67d39ae4faaad4ed2df1cd42",
      "projectPage": "https://hkchengrex.com/C2OT/",
      "githubRepo": "https://github.com/hkchengrex/C2OT",
      "ai_keywords": [
        "minibatch optimal transport",
        "flow matching",
        "integration steps",
        "numerical solvers",
        "ordinary differential equation",
        "optimal transport mapping",
        "conditional optimal transport",
        "C^2OT",
        "cost matrix",
        "optimal transport assignment",
        "discrete conditions",
        "continuous conditions",
        "8gaussians-to-moons",
        "CIFAR-10",
        "ImageNet-32x32",
        "ImageNet-256x256",
        "function evaluation budgets"
      ]
    },
    "publishedAt": "2025-03-13T13:59:56.000Z",
    "title": "The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation",
    "summary": "Minibatch optimal transport coupling straightens paths in unconditional flow\nmatching. This leads to computationally less demanding inference as fewer\nintegration steps and less complex numerical solvers can be employed when\nnumerically solving an ordinary differential equation at test time. However, in\nthe conditional setting, minibatch optimal transport falls short. This is\nbecause the default optimal transport mapping disregards conditions, resulting\nin a conditionally skewed prior distribution during training. In contrast, at\ntest time, we have no access to the skewed prior, and instead sample from the\nfull, unbiased prior distribution. This gap between training and testing leads\nto a subpar performance. To bridge this gap, we propose conditional optimal\ntransport C^2OT that adds a conditional weighting term in the cost matrix when\ncomputing the optimal transport assignment. Experiments demonstrate that this\nsimple fix works with both discrete and continuous conditions in\n8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method\nperforms better overall compared to the existing baselines across different\nfunction evaluation budgets. Code is available at\nhttps://hkchengrex.github.io/C2OT",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63041b541dd5d3c62486c294/ZHc69zwDiWkyl0cNnp5zn.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10636.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63041b541dd5d3c62486c294",
      "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
      "fullname": "Ho Kei Cheng",
      "name": "hkchengrex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 25
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10072",
      "authors": [
        {
          "_id": "67d390de29a092bdbb0a2aeb",
          "user": {
            "_id": "6331c3f618711776b468e9ec",
            "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
            "isPro": false,
            "fullname": "Mia Mohammad Imran",
            "user": "imranraad",
            "type": "user"
          },
          "name": "Mia Mohammad Imran",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T02:14:33.331Z",
          "hidden": false
        },
        {
          "_id": "67d390de29a092bdbb0a2aec",
          "name": "Jaydeb Sarker",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T05:39:29.000Z",
      "submittedOnDailyAt": "2025-03-14T00:44:11.345Z",
      "title": "\"Nada hecho\": Análisis de la investigación de errores en el informe de errores",
      "submittedOnDailyBy": {
        "_id": "6331c3f618711776b468e9ec",
        "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
        "isPro": false,
        "fullname": "Mia Mohammad Imran",
        "user": "imranraad",
        "type": "user"
      },
      "summary": "El carácter de los reportes de errores es un problema grave que afecta a la estructura cooperativa de la desarrollo de software abierto. Los reportes de errores son cruciales para identificar y solucionar defectos, pero su propia naturaleza puede llevar a interacciones caracterizadas por una emocionalidad excesiva y un contexto rico, lo que puede llevar a un comportamiento excesivamente emotivo. Este estudio analizó 203 reportes de errores en GitHub, incluyendo 81 que presentaban características de carácter excesivamente emotivo (81 reportes de carácter excesivamente emotivo). Nuestros hallazgos muestran que los errores graves y la mala priorización, la insatisfacción con herramientas que no funcionan y la falta de comunicación profesional son frecuentes causas de este tipo de comportamiento. Estas interacciones caracterizadas por excesivo carácter pueden perder oportunidades para generar productivos debates y obtener resultados efectivos. Nuestros hallazgos iniciales proporcionan una propuesta concreta e implementable para mejorar la resolución de errores, centrada en la reducción del carácter excesivamente emotivo.",
      "upvotes": 2,
      "discussionId": "67d390df29a092bdbb0a2b2d",
      "projectPage": "https://zenodo.org/records/15015619"
    },
    "publishedAt": "2025-03-13T01:39:29.000Z",
    "title": "\"Silent Is Not Actually Silent\": An Investigation of Toxicity on Bug\n  Report Discussion",
    "summary": "Toxicity in bug report discussions poses significant challenges to the\ncollaborative dynamics of open-source software development. Bug reports are\ncrucial for identifying and resolving defects, yet their inherently\nproblem-focused nature and emotionally charged context make them susceptible to\ntoxic interactions. This study explores toxicity in GitHub bug reports through\na qualitative analysis of 203 bug threads, including 81 toxic ones. Our\nfindings reveal that toxicity frequently arises from misaligned perceptions of\nbug severity and priority, unresolved frustrations with tools, and lapses in\nprofessional communication. These toxic interactions not only derail productive\ndiscussions but also reduce the likelihood of actionable outcomes, such as\nlinking issues with pull requests. Our preliminary findings offer actionable\nrecommendations to improve bug resolution by mitigating toxicity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10072.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6331c3f618711776b468e9ec",
      "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
      "fullname": "Mia Mohammad Imran",
      "name": "imranraad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10291",
      "authors": [
        {
          "_id": "67d3cbea16d1ecea57ed096c",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed096d",
          "name": "Zhangwei Gao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed096e",
          "name": "Lianjie Chen",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed096f",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0970",
          "name": "Jinguo Zhu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0971",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0972",
          "name": "Yangzhou Liu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0973",
          "name": "Yue Cao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0974",
          "name": "Shenglong Ye",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0975",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0976",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0977",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0978",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0979",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed097a",
          "name": "Wenhai Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VPQRFEXv78LkPf2h5dgXm.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/w91GHKp_sv2u4U-8VFLOn.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/rG_h65J1RHcO0Lkbo5rg4.png"
      ],
      "publishedAt": "2025-03-13T12:03:37.000Z",
      "submittedOnDailyAt": "2025-03-14T06:40:16.854Z",
      "title": "VisualPRM: Razones diversas para modelar un proceso de recompensa efectivo de modelos",
      "submittedOnDailyBy": {
        "_id": "619507e7b74b6c591f794340",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
        "isPro": false,
        "fullname": "Weiyun Wang",
        "user": "Weiyun1025",
        "type": "user"
      },
      "summary": "VisualPRM es un modelo de proceso de recompensa multimodelo (PRM) con 8B parámetros. Este modelo mejora las capacidades actuales de los modelos multimodelo de lenguaje (MLLMs) utilizando una estrategia de evaluación Best-of-N (BoN) entre diferentes escalas y familias de modelos. En particular, nuestro modelo mejora el rendimiento de 3 MLLMs y 4 escalas de modelo. También logró mejorar en 5.9 puntos en 7 benchmarks multimodelo para el modelo de alta capacidad, InternVL2.5-78B. Los resultados de los experimentos muestran que nuestro modelo supera a los modelos de recompensa de resultados y a la auto-consistencia en la evaluación BoN. Además, para fomentar la entrenamiento del modelo multimodelo PRM, construimos el conjunto de datos multimodelo proceso subconjunto VisualPRM400K utilizando una pipeline de datos automatizada. También proponemos el benchmark VisualProcessBench, que tiene etiquetas de precisión etapa a etapa explicadas por humanos, para evaluar la capacidad de nuestro modelo de detectar errores en las tareas de razonamiento multimodelo. Nuestra investigación contribuye a la desarrollo futuro de los modelos multimodelo de lenguaje y fomenta su progreso adicional. Nuestro modelo, datos y benchmarks están disponibles en https://internvl.github.io/blog/2025-03-13-VisualPRM/.",
      "upvotes": 1,
      "discussionId": "67d3cbed16d1ecea57ed0a75",
      "projectPage": "https://internvl.github.io/blog/2025-03-13-VisualPRM/",
      "githubRepo": "https://github.com/OpenGVLab/InternVL",
      "ai_keywords": [
        "Process Reward Model (PRM)",
        "Multimodal Large Language Models (MLLMs)",
        "Best-of-N (BoN)",
        "multimodal reasoning benchmarks",
        "Automated data pipeline",
        "VisualPRM400K",
        "VisualProcessBench",
        "human-annotated step-wise correctness labels",
        "multimodal PRMs",
        "erroneous steps in multimodal reasoning tasks"
      ]
    },
    "publishedAt": "2025-03-13T08:03:37.000Z",
    "title": "VisualPRM: An Effective Process Reward Model for Multimodal Reasoning",
    "summary": "We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM)\nwith 8B parameters, which improves the reasoning abilities of existing\nMultimodal Large Language Models (MLLMs) across different model scales and\nfamilies with Best-of-N (BoN) evaluation strategies. Specifically, our model\nimproves the reasoning performance of three types of MLLMs and four different\nmodel scales. Even when applied to the highly capable InternVL2.5-78B, it\nachieves a 5.9-point improvement across seven multimodal reasoning benchmarks.\nExperimental results show that our model exhibits superior performance compared\nto Outcome Reward Models and Self-Consistency during BoN evaluation. To\nfacilitate the training of multimodal PRMs, we construct a multimodal process\nsupervision dataset VisualPRM400K using an automated data pipeline. For the\nevaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with\nhuman-annotated step-wise correctness labels, to measure the abilities of PRMs\nto detect erroneous steps in multimodal reasoning tasks. We hope that our work\ncan inspire more future research and contribute to the development of MLLMs.\nOur model, data, and benchmark are released in\nhttps://internvl.github.io/blog/2025-03-13-VisualPRM/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VPQRFEXv78LkPf2h5dgXm.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/w91GHKp_sv2u4U-8VFLOn.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/rG_h65J1RHcO0Lkbo5rg4.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10291.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619507e7b74b6c591f794340",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
      "fullname": "Weiyun Wang",
      "name": "Weiyun1025",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09368",
      "authors": [
        {
          "_id": "67d2ca4be4696fda20bac029",
          "user": {
            "_id": "656c8721e8bf55919a9732c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
            "isPro": false,
            "fullname": "Nikolai",
            "user": "Nikolai10",
            "type": "user"
          },
          "name": "Nikolai Körber",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:57:39.634Z",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02a",
          "name": "Eduard Kromer",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02b",
          "name": "Andreas Siebert",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02c",
          "name": "Sascha Hauke",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02d",
          "name": "Daniel Mueller-Gritschneder",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02e",
          "name": "Björn Schuller",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T13:14:51.000Z",
      "submittedOnDailyAt": "2025-03-14T07:51:16.414Z",
      "title": "PerCoV2: Mejora en la compresión visual de imágenes de alta resolución bajo bitrate mediante modelado de imágenes de mascarillas de héroes ocultos",
      "submittedOnDailyBy": {
        "_id": "656c8721e8bf55919a9732c5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
        "isPro": false,
        "fullname": "Nikolai",
        "user": "Nikolai10",
        "type": "user"
      },
      "summary": "PerCoV2 es un nuevo sistema de compresión visual de imágenes de bajo bitrate abierto diseñado para aplicaciones limitadas como banner y storarjan. Basado en la investigación previa de Careil, PerCoV2 se extiende en la ecosistema de Stable Diffusion 3, mejorando la eficiencia de la codificación de entropía mediante la modelación explícita de la distribución discreta de ultra-subpixel. Esto permite una comparación detallada con métodos automáticos de regreso de juegos (VAR y MaskGIT) y una validación en el marco de un gran benchmark de MSCOCO-30k. PerCoV2 supera a los estudios previos en tres aspectos: (i) reduce aún más el bitrate mientras mejora la precisión de las imágenes, manteniendo una calidad visual competitiva, (ii) presenta un modo generativo híbrido para reducir el bitrate, y (iii) se construye utilizando solo componentes publicados. Los códigos y modelos entrenados están disponibles en https://github.com/Nikolai10/PerCoV2.",
      "upvotes": 1,
      "discussionId": "67d2ca50e4696fda20bac1a8",
      "githubRepo": "https://github.com/Nikolai10/PerCoV2",
      "ai_keywords": [
        "PerCoV2",
        "ultralow bit-rate",
        "perceptual image compression",
        "bandwidth-constrained applications",
        "Stable Diffusion 3",
        "entropy coding",
        "discrete hyper-latent image distribution",
        "autoregressive methods",
        "VAR",
        "MaskGIT",
        "MSCOCO-30k",
        "image fidelity",
        "perceptual quality",
        "hybrid generation mode",
        "public components"
      ]
    },
    "publishedAt": "2025-03-12T09:14:51.000Z",
    "title": "PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with\n  Implicit Hierarchical Masked Image Modeling",
    "summary": "We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image\ncompression system designed for bandwidth- and storage-constrained\napplications. Building upon prior work by Careil et al., PerCoV2 extends the\noriginal formulation to the Stable Diffusion 3 ecosystem and enhances entropy\ncoding efficiency by explicitly modeling the discrete hyper-latent image\ndistribution. To this end, we conduct a comprehensive comparison of recent\nautoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our\napproach on the large-scale MSCOCO-30k benchmark. Compared to previous work,\nPerCoV2 (i) achieves higher image fidelity at even lower bit-rates while\nmaintaining competitive perceptual quality, (ii) features a hybrid generation\nmode for further bit-rate savings, and (iii) is built solely on public\ncomponents. Code and trained models will be released at\nhttps://github.com/Nikolai10/PerCoV2.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09368.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656c8721e8bf55919a9732c5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
      "fullname": "Nikolai",
      "name": "Nikolai10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]