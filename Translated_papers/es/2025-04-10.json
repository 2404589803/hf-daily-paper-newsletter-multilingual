[
  {
    "paper": {
      "id": "2504.05741",
      "authors": [
        {
          "_id": "67f726dc0b5aa5777fd3a431",
          "user": {
            "_id": "66615c855fd9d736e670e0a9",
            "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
            "isPro": false,
            "fullname": "wangshuai",
            "user": "wangsssssss",
            "type": "user"
          },
          "name": "Shuai Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:44:49.192Z",
          "hidden": false
        },
        {
          "_id": "67f726dc0b5aa5777fd3a432",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "67f726dc0b5aa5777fd3a433",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "67f726dc0b5aa5777fd3a434",
          "user": {
            "_id": "62c77f4352d8ae531f5511f9",
            "avatarUrl": "/avatars/50198ccb02ccd286975a4613fbabee28.svg",
            "isPro": false,
            "fullname": "Limin Wang",
            "user": "lmwang",
            "type": "user"
          },
          "name": "Limin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T07:58:42.903Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-08T07:17:45.000Z",
      "submittedOnDailyAt": "2025-04-10T00:40:02.945Z",
      "title": "DDT: Separación DIFUJENTRANSFORMER",
      "submittedOnDailyBy": {
        "_id": "66615c855fd9d736e670e0a9",
        "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
        "isPro": false,
        "fullname": "wangshuai",
        "user": "wangsssssss",
        "type": "user"
      },
      "summary": "Los transformadores de difusión muestran una excelente calidad de generación, pero requieren de largas iteraciones de entrenamiento y varios pasos de inferencia. En cada etapa de ruido, los transformadores de difusión codifican la entrada con ruido para extraer componentes significativos de baja frecuencia y decodifican los componentes de alta frecuencia con el mismo módulo. Esta metodología plantea dificultades propias de optimización: para codificar componentes significativos de baja frecuencia, es necesario reducir los componentes de alta frecuencia, lo que genera tensión entre el codificado significativo y el decodificado de alta frecuencia. Para resolver este problema, proponemos un nuevo transformador de difusión, DDT-XL/2, que tiene una arquitectura separada para la extracción de componentes significativos y decodificación de velocidad. Nuestros experimentos muestran que al aumentar el tamaño del modelo, un codificador más potente mejora el rendimiento. En ImageNet 256x256, nuestro DDT-XL/2 alcanza una convergencia de entrenamiento aproximadamente 4 veces más rápido que los transformadores de difusión anteriores y alcanza un nuevo rendimiento óptimo. En ImageNet 512x512, nuestro DDT-XL/2 alcanza un nuevo valor de FID óptimo de 1.28. Además, como el desarrollo lento, la arquitectura separada mejora la velocidad de inferencia al compartir condiciones entre etapas de ruido adyacentes. Para minimizar el deterioro de la calidad, proponemos una nueva estrategia estadística y dinámica para determinar la mejor manera de compartir estas condiciones.",
      "upvotes": 34,
      "discussionId": "67f726dd0b5aa5777fd3a463",
      "githubRepo": "https://github.com/MCG-NJU/DDT"
    },
    "publishedAt": "2025-04-08T03:17:45.000Z",
    "title": "DDT: Decoupled Diffusion Transformer",
    "summary": "Diffusion transformers have demonstrated remarkable generation quality,\nalbeit requiring longer training iterations and numerous inference steps. In\neach denoising step, diffusion transformers encode the noisy inputs to extract\nthe lower-frequency semantic component and then decode the higher frequency\nwith identical modules. This scheme creates an inherent optimization dilemma:\nencoding low-frequency semantics necessitates reducing high-frequency\ncomponents, creating tension between semantic encoding and high-frequency\ndecoding. To resolve this challenge, we propose a new\n\\color{ddtD}ecoupled \\color{ddtD}iffusion\n\\color{ddtT}ransformer~(\\color{ddtDDT}), with a decoupled\ndesign of a dedicated condition encoder for semantic extraction alongside a\nspecialized velocity decoder. Our experiments reveal that a more substantial\nencoder yields performance improvements as model size increases. For ImageNet\n256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of\n{1.31 FID}~(nearly 4times faster training convergence compared to previous\ndiffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a\nnew state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our\ndecoupled architecture enhances inference speed by enabling the sharing\nself-condition between adjacent denoising steps. To minimize performance\ndegradation, we propose a novel statistical dynamic programming approach to\nidentify optimal sharing strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05741.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66615c855fd9d736e670e0a9",
      "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
      "fullname": "wangshuai",
      "name": "wangsssssss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07083",
      "authors": [
        {
          "_id": "67f72c452eec6ce5c8b9e8e6",
          "user": {
            "_id": "64de20c5808492ba6e65d124",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
            "isPro": false,
            "fullname": "Zhang Mengchen",
            "user": "Dubhe-zmc",
            "type": "user"
          },
          "name": "Mengchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:42.813Z",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8e7",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8e8",
          "user": {
            "_id": "65367c40061949598892dbdc",
            "avatarUrl": "/avatars/4baf27263841471cbd5f629a8b99424d.svg",
            "isPro": false,
            "fullname": "Jing Tan",
            "user": "jingtan",
            "type": "user"
          },
          "name": "Jing Tan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:01:54.122Z",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8e9",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:01:11.166Z",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8ea",
          "user": {
            "_id": "6694e583ac96ca2c17131505",
            "avatarUrl": "/avatars/6e7a31f257e36cf301da6f879dc0a122.svg",
            "isPro": false,
            "fullname": "Gordon Wetzstein",
            "user": "wetzste1",
            "type": "user"
          },
          "name": "Gordon Wetzstein",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:01:03.935Z",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8eb",
          "user": {
            "_id": "636317ed80c1a705a6eff396",
            "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
            "isPro": false,
            "fullname": "Dahua Lin",
            "user": "lindahua",
            "type": "user"
          },
          "name": "Dahua Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:00:57.092Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64de20c5808492ba6e65d124/b1N08r8EbruYc8Yapg4J9.qt"
      ],
      "publishedAt": "2025-04-09T17:56:01.000Z",
      "submittedOnDailyAt": "2025-04-10T01:13:43.884Z",
      "title": "GenDoP: Generador automático de entrenador de cámaras de retrato para directores de fotógrafía",
      "submittedOnDailyBy": {
        "_id": "64de20c5808492ba6e65d124",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
        "isPro": false,
        "fullname": "Zhang Mengchen",
        "user": "Dubhe-zmc",
        "type": "user"
      },
      "summary": "El diseño de la trayectoria de cámara desempeña un papel importante en la producción de películas y es una herramienta básica para transmitir la intención del director, mejorando así la transmisión de historias visuales. En la captura de películas, los directores de fotografía planifican con precisión el movimiento de la cámara para lograr frases de fotografía expresivas e intencionales. Sin embargo, la generación de la trayectoria de cámara está limitada por los métodos actuales: los enfoques tradicionales basados en optimización geométrica o sistemas de procesos diseñados directamente, y los métodos basados en aprendizaje recientes que tienden a la sesgo estructural y limitan la creatividad al no coincidir suficientemente con el contexto. En este artículo, se presenta una metodología para generar trayectorias de cámara artísticas y expresivas basada en un modelo de regresión automática que se basa en el conocimiento de los directores de fotografía. Primero, se presenta un grande base de datos multitipo que incluye 29K imágenes reales, DataDoP. Esta base de datos incorpora trayectorias de cámara de movimiento libre, mapas de profundidad, acciones específicas, interacciones con el espacio y detalladas capturas de la intención del director. A través de esta información detallada, se desarrolla un decoder automático y retroalimentado, Transformer, para generar movimientos de cámara ordenados y contextuales basados en guías de texto y entradas RGBD, lo que se denomina GenDoP. Los experimentos extendidos muestran que GenDoP proporciona una mayor posibilidad de control, una mejor ajuste de la trayectoria y una mayor estabilidad del movimiento en comparación con los métodos existentes. Creemos que esta metodología establecerá un nuevo estándar para la captura de películas basada en aprendizaje y estimamos que impulsará el desarrollo futuro de la control de cámaras y la producción cinematográfica. El sitio web del proyecto está disponible en https://kszpxxzmc.github.io/GenDoP/.",
      "upvotes": 16,
      "discussionId": "67f72c472eec6ce5c8b9e97b",
      "projectPage": "https://kszpxxzmc.github.io/GenDoP/",
      "githubRepo": "https://github.com/3DTopia/GenDoP"
    },
    "publishedAt": "2025-04-09T13:56:01.000Z",
    "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of\n  Photography",
    "summary": "Camera trajectory design plays a crucial role in video production, serving as\na fundamental tool for conveying directorial intent and enhancing visual\nstorytelling. In cinematography, Directors of Photography meticulously craft\ncamera movements to achieve expressive and intentional framing. However,\nexisting methods for camera trajectory generation remain limited: Traditional\napproaches rely on geometric optimization or handcrafted procedural systems,\nwhile recent learning-based methods often inherit structural biases or lack\ntextual alignment, constraining creative synthesis. In this work, we introduce\nan auto-regressive model inspired by the expertise of Directors of Photography\nto generate artistic and expressive camera trajectories. We first introduce\nDataDoP, a large-scale multi-modal dataset containing 29K real-world shots with\nfree-moving camera trajectories, depth maps, and detailed captions in specific\nmovements, interaction with the scene, and directorial intent. Thanks to the\ncomprehensive and diverse database, we further train an auto-regressive,\ndecoder-only Transformer for high-quality, context-aware camera movement\ngeneration based on text guidance and RGBD inputs, named GenDoP. Extensive\nexperiments demonstrate that compared to existing methods, GenDoP offers better\ncontrollability, finer-grained trajectory adjustments, and higher motion\nstability. We believe our approach establishes a new standard for\nlearning-based cinematography, paving the way for future advancements in camera\ncontrol and filmmaking. Our project website:\nhttps://kszpxxzmc.github.io/GenDoP/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64de20c5808492ba6e65d124/b1N08r8EbruYc8Yapg4J9.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07083.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64de20c5808492ba6e65d124",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
      "fullname": "Zhang Mengchen",
      "name": "Dubhe-zmc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07096",
      "authors": [
        {
          "_id": "67f72bb1f9d51b79dca06d0a",
          "user": {
            "_id": "635f46d1928a42bc95cfcf7c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f46d1928a42bc95cfcf7c/5KF8aLiDCJdl7B1SdJ-7V.png",
            "isPro": false,
            "fullname": "Jiacheng Liu",
            "user": "liujch1998",
            "type": "user"
          },
          "name": "Jiacheng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:44.913Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0b",
          "user": {
            "_id": "6675a65557208377a15f745b",
            "avatarUrl": "/avatars/361dc6d0919f4d4545ff4fdd005332b5.svg",
            "isPro": false,
            "fullname": "Taylor Blanton",
            "user": "taylorb",
            "type": "user"
          },
          "name": "Taylor Blanton",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:02:05.681Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0c",
          "user": {
            "_id": "623ca115a795593324c4353f",
            "avatarUrl": "/avatars/bf11fe728df2786d52ed4d2de12b48d3.svg",
            "isPro": false,
            "fullname": "Yanai Elazar",
            "user": "yanaiela",
            "type": "user"
          },
          "name": "Yanai Elazar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:02:12.016Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0d",
          "user": {
            "_id": "63a76d0de27a6dbd485fe863",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a76d0de27a6dbd485fe863/qJJwHOuvyQGq1o0KscOF_.jpeg",
            "isPro": false,
            "fullname": "Sewon Min",
            "user": "sewon",
            "type": "user"
          },
          "name": "Sewon Min",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:02:17.909Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0e",
          "user": {
            "_id": "6697093a37d2483826562c24",
            "avatarUrl": "/avatars/0e526b4be6db07e2485f7ef862080339.svg",
            "isPro": false,
            "fullname": "Chen",
            "user": "Yensung",
            "type": "user"
          },
          "name": "YenSung Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:02:26.950Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0f",
          "name": "Arnavi Chheda-Kothary",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d10",
          "name": "Huy Tran",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d11",
          "name": "Byron Bischoff",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d12",
          "name": "Eric Marsh",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d13",
          "name": "Michael Schmitz",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d14",
          "name": "Cassidy Trier",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d15",
          "user": {
            "_id": "65b1520bf7638a13a641a620",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b1520bf7638a13a641a620/KTauZL0kXlmYnbkI2lFBG.png",
            "isPro": false,
            "fullname": "Aaron Sarnat",
            "user": "aaronsarnat",
            "type": "user"
          },
          "name": "Aaron Sarnat",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:03:34.212Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d16",
          "name": "Jenna James",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d17",
          "name": "Jon Borchardt",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d18",
          "user": {
            "_id": "65316953791d5a2611426c20",
            "avatarUrl": "/avatars/e632a9a30a57f62d59f9fe42eba8fd7d.svg",
            "isPro": false,
            "fullname": "bailey kuehl",
            "user": "baileyk",
            "type": "user"
          },
          "name": "Bailey Kuehl",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:03:47.506Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d19",
          "name": "Evie Cheng",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1a",
          "user": {
            "_id": "66213c05e288b64070184cac",
            "avatarUrl": "/avatars/ded6a173e60722200b372b8b046fc359.svg",
            "isPro": false,
            "fullname": "Karen Farley",
            "user": "AI2Karen",
            "type": "user"
          },
          "name": "Karen Farley",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:03:58.149Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1b",
          "name": "Sruthi Sreeram",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1c",
          "user": {
            "_id": "65de20ad4e73a7dea7fb4f08",
            "avatarUrl": "/avatars/f3b0ad6cc9417e8ea3f0607fa62824d1.svg",
            "isPro": false,
            "fullname": "Taira Anderson",
            "user": "tairaa",
            "type": "user"
          },
          "name": "Taira Anderson",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:04:09.193Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1d",
          "name": "David Albright",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1e",
          "user": {
            "_id": "6024546dc1f3c79f98e4b384",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1612993792778-6024546dc1f3c79f98e4b384.jpeg",
            "isPro": false,
            "fullname": "Carissa Schoenick",
            "user": "CarissaS",
            "type": "user"
          },
          "name": "Carissa Schoenick",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:04:25.492Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1f",
          "user": {
            "_id": "5f04d8c45d08220171a0ad32",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f04d8c45d08220171a0ad32/uXEta6nqBabrUlAOXnS5g.jpeg",
            "isPro": false,
            "fullname": "Luca Soldaini",
            "user": "soldni",
            "type": "user"
          },
          "name": "Luca Soldaini",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:04:31.958Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d20",
          "user": {
            "_id": "60369745413a78f892e7339c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1636671879171-60369745413a78f892e7339c.png",
            "isPro": false,
            "fullname": "Dirk Groeneveld",
            "user": "dirkgr",
            "type": "user"
          },
          "name": "Dirk Groeneveld",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:04:40.029Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d21",
          "name": "Rock Yuren Pang",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d22",
          "user": {
            "_id": "641b4263abfce26bcf7b27de",
            "avatarUrl": "/avatars/e91b4205e4f74b0dd8c333c23203a924.svg",
            "isPro": false,
            "fullname": "Pang Wei Koh",
            "user": "pangwei",
            "type": "user"
          },
          "name": "Pang Wei Koh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:04:53.298Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d23",
          "name": "Noah A. Smith",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d24",
          "user": {
            "_id": "65b301f04c9e50e74a893954",
            "avatarUrl": "/avatars/f52366959f9e7613576603c0272ff2c5.svg",
            "isPro": false,
            "fullname": "Sophie Lebrecht",
            "user": "Lebrechts",
            "type": "user"
          },
          "name": "Sophie Lebrecht",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:05:06.279Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d25",
          "user": {
            "_id": "64d42729f63b01b7f676b176",
            "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
            "isPro": false,
            "fullname": "Yejin Choi",
            "user": "yejinchoinka",
            "type": "user"
          },
          "name": "Yejin Choi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:05:13.983Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d26",
          "name": "Hannaneh Hajishirzi",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d27",
          "user": {
            "_id": "6660d4c1818c5c5ca0f31266",
            "avatarUrl": "/avatars/1d2972894cb3b9df1900fdb162d9c364.svg",
            "isPro": false,
            "fullname": "alifarhadi ",
            "user": "alifarhadi051",
            "type": "user"
          },
          "name": "Ali Farhadi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:05:24.948Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d28",
          "user": {
            "_id": "6283f38567d336d3e5d5280e",
            "avatarUrl": "/avatars/d0a54aaec74a90b050e671c191b87a80.svg",
            "isPro": false,
            "fullname": "Jesse Dodge",
            "user": "JesseDodge",
            "type": "user"
          },
          "name": "Jesse Dodge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:05:31.942Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T17:59:35.000Z",
      "submittedOnDailyAt": "2025-04-10T00:54:36.448Z",
      "title": "OLMoTrace: 100 millones de tokens de entrenamiento se rastrean a través de la generación de modelos de lenguaje posterior.",
      "submittedOnDailyBy": {
        "_id": "635f46d1928a42bc95cfcf7c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f46d1928a42bc95cfcf7c/5KF8aLiDCJdl7B1SdJ-7V.png",
        "isPro": false,
        "fullname": "Jiacheng Liu",
        "user": "liujch1998",
        "type": "user"
      },
      "summary": "OLMoTrace es el primer sistema que retrocede en tiempo real el output de un modelo de lenguaje a todos los tokens de toda la data de entrenamiento de multillion. OLMoTrace detecta y muestra la exacta coincidencia entre el output del modelo de lenguaje y los fragmentos de documentos de los textos de entrenamiento. Este es un extensión de infani-gram, retornando resultados de búsqueda en pocos segundos. OLMoTrace ayuda a entender la acción del modelo de lenguaje a través de una visión de la data de entrenamiento. Este sistema presenta métodos de verificación de hechos, string-matching y investigación sobre la creatividad del modelo de lenguaje. OLMoTrace está disponible para uso público y es completamente abierto-source.",
      "upvotes": 14,
      "discussionId": "67f72bb3f9d51b79dca06d8c"
    },
    "publishedAt": "2025-04-09T13:59:35.000Z",
    "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training\n  Tokens",
    "summary": "We present OLMoTrace, the first system that traces the outputs of language\nmodels back to their full, multi-trillion-token training data in real time.\nOLMoTrace finds and shows verbatim matches between segments of language model\noutput and documents in the training text corpora. Powered by an extended\nversion of infini-gram (Liu et al., 2024), our system returns tracing results\nwithin a few seconds. OLMoTrace can help users understand the behavior of\nlanguage models through the lens of their training data. We showcase how it can\nbe used to explore fact checking, hallucination, and the creativity of language\nmodels. OLMoTrace is publicly available and fully open-source.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07096.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635f46d1928a42bc95cfcf7c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f46d1928a42bc95cfcf7c/5KF8aLiDCJdl7B1SdJ-7V.png",
      "fullname": "Jiacheng Liu",
      "name": "liujch1998",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.06514",
      "authors": [
        {
          "_id": "67f72e933eacf8888816f3b0",
          "user": {
            "_id": "64a8121e35fab7cd04c30ed0",
            "avatarUrl": "/avatars/48849b84703158772f1022932331b143.svg",
            "isPro": false,
            "fullname": "Chenrui Fan",
            "user": "Fcr09",
            "type": "user"
          },
          "name": "Chenrui Fan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:05:59.999Z",
          "hidden": false
        },
        {
          "_id": "67f72e933eacf8888816f3b1",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67f72e933eacf8888816f3b2",
          "user": {
            "_id": "65a52766215aabac489e3468",
            "avatarUrl": "/avatars/fe05e22cd7e12e961296426434e17c76.svg",
            "isPro": false,
            "fullname": "Lichao Sun",
            "user": "sunlichao137",
            "type": "user"
          },
          "name": "Lichao Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:06:12.092Z",
          "hidden": false
        },
        {
          "_id": "67f72e933eacf8888816f3b3",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:37.906Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/FrHRMBKuB2v57LZWVJPxi.png"
      ],
      "publishedAt": "2025-04-09T01:25:27.000Z",
      "submittedOnDailyAt": "2025-04-10T01:07:13.718Z",
      "title": "La falta de premisas puede llevar a un pensamiento excesivo: ¿se pierde la capacidad de pensamiento crítico el modelo de razonamiento?",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Hemos introducido un nuevo escenario en el cual la longitud de las respuestas de los modelos de LLMs aumenta significativamente cuando se entrenan mediante aprendizaje por refuerzo o supervisión, especialmente ante preguntas erróneas con un premiso microscópico (MiP). Esto lleva a una repetición y una ineficiencia en el pensamiento, y exacerba muchas partes de los problemas de pensamiento excesivo. Esta falla contraviene la \"ley de escala de prueba\" pero, hemos observado esta falla en varios conjuntos de datos incluyendo MiP, lo que indica una falta de pensamiento crítico y un exceso de pensamiento general. Increíblemente, los modelos de LLMs que no se han entrenado específicamente para hacer inferencia muestran un desempeño mucho mejor en el escenario de MiP, generando respuestas más cortas y identificando rápidamente preguntas erróneas. Esto refleja una importante limitación en el receta de entrenamiento actual de los modelos de inferencia, que no logra impulsar suficientemente el pensamiento eficiente. Para investigar más profundamente la causa de estas fallas, analizamos la longitud de la inferencia, patrones de pensamiento excesivo y la posición de pensamiento importante en diferentes modelos de LLMs. Además, demostramos que el pensamiento excesivo puede propagar las respuestas de los modelos de inferencia hacia el desvanecimiento. Estos resultados mejoran la comprensión del pensamiento excesivo y proporcionan nuevas perspectivas para mitigar los problemas.",
      "upvotes": 10,
      "discussionId": "67f72e943eacf8888816f3fa",
      "githubRepo": "https://github.com/tianyi-lab/MiP-Overthinking"
    },
    "publishedAt": "2025-04-08T21:25:27.000Z",
    "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?",
    "summary": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/FrHRMBKuB2v57LZWVJPxi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06514.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.04842",
      "authors": [
        {
          "_id": "67f72ca8353d129fc7bdd504",
          "name": "Mengchao Wang",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd505",
          "user": {
            "_id": "653b195c5f1703225b2fd571",
            "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
            "isPro": false,
            "fullname": "wangqiang",
            "user": "wangqiang9",
            "type": "user"
          },
          "name": "Qiang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:40.647Z",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd506",
          "user": {
            "_id": "63048ea19aef62c4013c77aa",
            "avatarUrl": "/avatars/b2b2243ccc63cfb5a3289bc2eb1d6293.svg",
            "isPro": false,
            "fullname": "fanjiang",
            "user": "fanjiang",
            "type": "user"
          },
          "name": "Fan Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:50:07.747Z",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd507",
          "name": "Yaqi Fan",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd508",
          "name": "Yunpeng Zhang",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd509",
          "name": "Yonggang Qi",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd50a",
          "name": "Kun Zhao",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd50b",
          "name": "Mu Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T08:56:01.000Z",
      "submittedOnDailyAt": "2025-04-10T00:58:44.876Z",
      "title": "FantasyTokking: Generación de Tapes Realistas a través de la Sintesis de Movimiento Coherente en Fotografías",
      "submittedOnDailyBy": {
        "_id": "653b195c5f1703225b2fd571",
        "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
        "isPro": false,
        "fullname": "wangqiang",
        "user": "wangqiang9",
        "type": "user"
      },
      "summary": "Crear un avatar animable realmente posible a partir de una simple foto está difícil. Los métodos actuales de acceso tienen dificultades para entender facilmente expresiones faciales superficiales, movimientos corporales relacionados y fondos dinámicos. Para resolver estos limitaciones, proponemos un nuevo marco de trabajo y utilizamos un modelo de transformador preentrenado para la difusión de videos para generar imágenes de alta calidad a partir de diálogos cooperativos. El núcleo de nuestro estudio es una estrategia de alineamiento de voz-visión en dos pasos. En el primer paso, utilizamos un esquema de entrenamiento a nivel de clip para ajustar movimientos llevados a cabo por el espacio completo de voz y asegurar que coinciden con imágenes de referencia, objetos de contexto y fondos. En el segundo paso, ajustamos movimientos de cabeza con precisión a nivel de frame y aseguramos la sincronización con el uso de máscaras de seguimiento de cabezas y señales de voz. Manteniendo la flexibilidad de la animación, reemplazamos la red de referencia común y centramos nuestro enfoque en el rostro, utilizando un módulo de atención de intercambio para mantener la coherencia del rostro en toda la video. Además, para controlar con precisión la intensidad de la expresión y el movimiento corporal, incluimos un módulo de regulación de intensidad del movimiento y controlamos no solo el movimiento de la cabeza, sino también el movimiento de las imágenes animables. Nuestros resultados experimentales ampliados muestran que nuestro enfoque proporciona una calidad más alta y realista, así como una mayor cooperación, intensidad del movimiento y flexibilidad de la animación. Nuestra página de proyecto está disponible en https://fantasy-amap.github.io/fantasy-talking/.",
      "upvotes": 7,
      "discussionId": "67f72cac353d129fc7bdd60f",
      "projectPage": "https://fantasy-amap.github.io/fantasy-talking/",
      "githubRepo": "https://github.com/Fantasy-AMAP/fantasy-talking"
    },
    "publishedAt": "2025-04-07T04:56:01.000Z",
    "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion Synthesis",
    "summary": "Creating a realistic animatable avatar from a single static portrait remains\nchallenging. Existing approaches often struggle to capture subtle facial\nexpressions, the associated global body movements, and the dynamic background.\nTo address these limitations, we propose a novel framework that leverages a\npretrained video diffusion transformer model to generate high-fidelity,\ncoherent talking portraits with controllable motion dynamics. At the core of\nour work is a dual-stage audio-visual alignment strategy. In the first stage,\nwe employ a clip-level training scheme to establish coherent global motion by\naligning audio-driven dynamics across the entire scene, including the reference\nportrait, contextual objects, and background. In the second stage, we refine\nlip movements at the frame level using a lip-tracing mask, ensuring precise\nsynchronization with audio signals. To preserve identity without compromising\nmotion flexibility, we replace the commonly used reference network with a\nfacial-focused cross-attention module that effectively maintains facial\nconsistency throughout the video. Furthermore, we integrate a motion intensity\nmodulation module that explicitly controls expression and body motion\nintensity, enabling controllable manipulation of portrait movements beyond mere\nlip motion. Extensive experimental results show that our proposed approach\nachieves higher quality with better realism, coherence, motion intensity, and\nidentity preservation. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04842.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b195c5f1703225b2fd571",
      "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
      "fullname": "wangqiang",
      "name": "wangqiang9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07089",
      "authors": [
        {
          "_id": "67f7676d0ab78ef7b16a820f",
          "user": {
            "_id": "6614fb3d5aed02b298a4b469",
            "avatarUrl": "/avatars/d0ddb4f989ad1a3f24128cc843347bde.svg",
            "isPro": false,
            "fullname": "yiting lu",
            "user": "yeeeeeyy",
            "type": "user"
          },
          "name": "Yiting Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T07:58:03.992Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8210",
          "user": {
            "_id": "64a3d1ddb3239f3e3892b24b",
            "avatarUrl": "/avatars/7ce585f5fc1d077fb1d70cc18c4da2c1.svg",
            "isPro": false,
            "fullname": "Jiakang Yuan",
            "user": "JiakangYuan",
            "type": "user"
          },
          "name": "Jiakang Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:08:31.119Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8211",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8212",
          "name": "Shitian Zhao",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8213",
          "user": {
            "_id": "66bb136002fd8eb58bc84ffb",
            "avatarUrl": "/avatars/122cb8f59c502392768099b3c2afe043.svg",
            "isPro": false,
            "fullname": "qinqi",
            "user": "Dakerqi",
            "type": "user"
          },
          "name": "Qi Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T08:06:06.570Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8214",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8215",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8216",
          "user": {
            "_id": "64a7c43ae940d769194055df",
            "avatarUrl": "/avatars/441ccadd62e039fb8cb112f138ed917d.svg",
            "isPro": false,
            "fullname": "Licheng Wen",
            "user": "Wayne-lc",
            "type": "user"
          },
          "name": "Licheng Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:09:07.684Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8217",
          "user": {
            "_id": "646f1bef075e11ca78da3bb7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f1bef075e11ca78da3bb7/gNS-ikyZXYeMrf4a7HTQE.jpeg",
            "isPro": false,
            "fullname": "Dongyang Liu (Chris Liu)",
            "user": "Cxxs",
            "type": "user"
          },
          "name": "Dongyang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:09:21.398Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8218",
          "name": "Yuewen Cao",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8219",
          "user": {
            "_id": "65b88b92e0bde92c176a888a",
            "avatarUrl": "/avatars/fc1cb54328ca93860e97fc73a3c1eb2f.svg",
            "isPro": false,
            "fullname": "Xiangchao Yan",
            "user": "yxc97",
            "type": "user"
          },
          "name": "Xiangchao Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:09:35.388Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821a",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821b",
          "user": {
            "_id": "643df87f7cd64d872cb9fabd",
            "avatarUrl": "/avatars/c53bfabcee08de448dde973915e8b31d.svg",
            "isPro": false,
            "fullname": "Botian Shi",
            "user": "friskit",
            "type": "user"
          },
          "name": "Botian Shi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:09:41.754Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821c",
          "name": "Tao Chen",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821d",
          "user": {
            "_id": "66d963e52e82d53d3b81031b",
            "avatarUrl": "/avatars/302dbffc033ff47813a2435a2cec02f1.svg",
            "isPro": false,
            "fullname": "Zhibo Chen",
            "user": "winhelp",
            "type": "user"
          },
          "name": "Zhibo Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:10:04.682Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821e",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821f",
          "user": {
            "_id": "643dfd235aafbdca3a5792c0",
            "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg",
            "isPro": false,
            "fullname": "Bo Zhang",
            "user": "BoZhang",
            "type": "user"
          },
          "name": "Bo Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T09:57:56.032Z",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8220",
          "name": "Peng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T17:58:58.000Z",
      "submittedOnDailyAt": "2025-04-10T05:22:13.319Z",
      "title": "OmniCaptioner: Con una captura, controla todo.",
      "submittedOnDailyBy": {
        "_id": "6614fb3d5aed02b298a4b469",
        "avatarUrl": "/avatars/d0ddb4f989ad1a3f24128cc843347bde.svg",
        "isPro": false,
        "fullname": "yiting lu",
        "user": "yeeeeeyy",
        "type": "user"
      },
      "summary": "OmniCaptioner es un marco de registro visual funcional que genera registros textuales detallados en diferentes áreas visuales. No está limitado a ciertos tipos de imágenes específicos (por ejemplo, imágenes naturales o visuogeométricas), por lo que proporciona una solución integrada para registrar imágenes naturales, texto visual (por ejemplo, posters, UI, libros), y visión estructurada (por ejemplo, documentos, tablas, gráficos). Convierte información de píxeles de alto nivel en representaciones textuales ricas, asegurando una coherencia entre el modelo visual y el modelo textual. A continuación, se muestran sus tres principales ventajas: (i) mejora en la inferencia visual mediante modelos de LLM, ya que los registros de largo contexto de los modelos de DeepSeek-R1 series permiten una inferencia efectiva en muchos escenarios entre modelos; (ii) mejora en la generación de imágenes, ya que los registros detallados mejoran la creación de imágenes o la transformación de imágenes; (iii) aprendizaje de registros visuales válidos, lo que permite converger más rápido con menor cantidad de datos. Confío en que las capacidades y aplicabilidad de OmniCaptioner brindarán una nueva perspectiva en la reconciliación entre modelos de lenguaje y visión.",
      "upvotes": 5,
      "discussionId": "67f767700ab78ef7b16a82d6"
    },
    "publishedAt": "2025-04-09T13:58:58.000Z",
    "title": "OmniCaptioner: One Captioner to Rule Them All",
    "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07089.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6614fb3d5aed02b298a4b469",
      "avatarUrl": "/avatars/d0ddb4f989ad1a3f24128cc843347bde.svg",
      "fullname": "yiting lu",
      "name": "yeeeeeyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07086",
      "authors": [
        {
          "_id": "67f75609b2d783993db63aba",
          "user": {
            "_id": "64ff3944f0d65cca9b867ed2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ff3944f0d65cca9b867ed2/jWnHkF4AUzh51MkC0UT6b.png",
            "isPro": false,
            "fullname": "Andreas Hochlehnert",
            "user": "libeanim",
            "type": "user"
          },
          "name": "Andreas Hochlehnert",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:07:58.732Z",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abb",
          "user": {
            "_id": "6556760b35f26c82c09a010f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6556760b35f26c82c09a010f/hNbwvXRBsKo6pqrHbXNzz.jpeg",
            "isPro": false,
            "fullname": "Hardik Bhatnagar",
            "user": "hrdkbhatnagar",
            "type": "user"
          },
          "name": "Hardik Bhatnagar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:08:04.922Z",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abc",
          "user": {
            "_id": "6304da46ce6b12280b1bd575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6304da46ce6b12280b1bd575/V96ocKW4HOoysAGxuAH1X.jpeg",
            "isPro": false,
            "fullname": "Vishaal Udandarao",
            "user": "vishaal27",
            "type": "user"
          },
          "name": "Vishaal Udandarao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:08:10.717Z",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abd",
          "user": {
            "_id": "62f3efefd6ba2ee26651f44a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660153837083-noauth.png",
            "isPro": false,
            "fullname": "Samuel Albanie",
            "user": "albanie",
            "type": "user"
          },
          "name": "Samuel Albanie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:08:16.809Z",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abe",
          "user": {
            "_id": "6464a0d41683d3c81f51924a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
            "isPro": false,
            "fullname": "Ameya Prabhu",
            "user": "AmeyaPrabhu",
            "type": "user"
          },
          "name": "Ameya Prabhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:24.192Z",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abf",
          "name": "Matthias Bethge",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T17:58:17.000Z",
      "submittedOnDailyAt": "2025-04-10T03:54:51.677Z",
      "title": "「Examinar el progreso de la teoría de los modelos de lenguaje con calma: errores de realización y largos」",
      "submittedOnDailyBy": {
        "_id": "6464a0d41683d3c81f51924a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
        "isPro": false,
        "fullname": "Ameya Prabhu",
        "user": "AmeyaPrabhu",
        "type": "user"
      },
      "summary": "En el campo de la investigación de la inteligencia artificial (IA), se espera que el próximo gran avance de los modelos de lenguaje (LM) se desarrolle rápidamente en los laboratorios académicos e industriales. Sin embargo, este avance va más allá de la metodología y muchas evaluaciones dependen de prácticas de marcadores que no tienen transparencia, robustez o base estadística. En este estudio, mediante experimentos precisos, se ha descubierto que los marcadores basados en razonamiento matemático son extremadamente sensibles a decisiones micro de implementación, como decodificación de parámetros, semillas aleatorias, formato de prompts, configuraciones de hardware y software. Los mejoramientos recientemente reportados en la literatura se presentan de manera incomprensible o con variaciones no reportadas, lo que dificulta su comprensión. Para resolver estos problemas, se propone un marco de evaluación estandarizado que incluye prácticas óptimas y estándares de reporte claramente definidos. Este marco permite reevaluar métodos recientes y demostrar que el enfoque de aprendizaje por refuerzo (RL) ha demostrado mayores mejoras que las afirmaciones previas, especialmente en pequeños marcadores donde el riesgo de sobreajuste es alto. Por otro lado, el enfoque de aprendizaje supervisado (SFT) ha mostrado una capacidad de expansión coherente. Para facilitar la reproducibilidad, se ha publicado el código, los prompts y los resultados de todos los marcadores, y se está construyendo una base más rigurosa para futuras investigaciones.",
      "upvotes": 5,
      "discussionId": "67f7560cb2d783993db63b6b"
    },
    "publishedAt": "2025-04-09T13:58:17.000Z",
    "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths\n  to Reproducibility",
    "summary": "Reasoning has emerged as the next major frontier for language models (LMs),\nwith rapid advances from both academic and industrial labs. However, this\nprogress often outpaces methodological rigor, with many evaluations relying on\nbenchmarking practices that lack transparency, robustness, or statistical\ngrounding. In this work, we conduct a comprehensive empirical study and find\nthat current mathematical reasoning benchmarks are highly sensitive to subtle\nimplementation choices - including decoding parameters, random seeds, prompt\nformatting, and even hardware and software-framework configurations.\nPerformance gains reported in recent studies frequently hinge on unclear\ncomparisons or unreported sources of variance. To address these issues, we\npropose a standardized evaluation framework with clearly defined best practices\nand reporting standards. Using this framework, we reassess recent methods and\nfind that reinforcement learning (RL) approaches yield only modest improvements\n- far below prior claims - and are prone to overfitting, especially on\nsmall-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)\nmethods show consistently stronger generalization. To foster reproducibility,\nwe release all code, prompts, and model outputs, for reasoning benchmarks,\nestablishing more rigorous foundations for future work.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07086.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6464a0d41683d3c81f51924a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
      "fullname": "Ameya Prabhu",
      "name": "AmeyaPrabhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07046",
      "authors": [
        {
          "_id": "67f74727353d129fc7c4be7a",
          "name": "Jifang Wang",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be7b",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be7c",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be7d",
          "user": {
            "_id": "639c379cdb7c5f35004066cb",
            "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
            "isPro": false,
            "fullname": "Zhenran Xu",
            "user": "imryanxu",
            "type": "user"
          },
          "name": "Zhenran Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T09:58:11.729Z",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be7e",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be7f",
          "name": "Yaowei Wang",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be80",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be81",
          "name": "Kaifu Zhang",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be82",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "67f74727353d129fc7c4be83",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/xXlL1RROzluluflNDOIRv.png"
      ],
      "publishedAt": "2025-04-09T17:04:14.000Z",
      "submittedOnDailyAt": "2025-04-10T07:56:53.441Z",
      "title": "Una evaluación de generación de imágenes condicionales mediante un único marco de salida de integración",
      "submittedOnDailyBy": {
        "_id": "639c379cdb7c5f35004066cb",
        "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
        "isPro": false,
        "fullname": "Zhenran Xu",
        "user": "imryanxu",
        "type": "user"
      },
      "summary": "La generación de imágenes condicionales ha recibido atención por su capacidad para proteger la privacidad de los contenidos. Sin embargo, este campo presenta desafíos en la creación de criterios de evaluación independientes de tareas y fiables y explicables. En este artículo, se presenta un framework efectivo llamado CIGEval, que permite realizar evaluaciones detalladas de la tarea de generación de imágenes condicionales. CIGEval se basa en una gran variedad de modelos multimodal (LMMs) y combina diversos instrumentos funcionales para construir un marco de evaluación detallado. Además, se integra el proceso de evaluación final de la entrenamiento, permitiendo que modelos de pequeño tamaño LMMs seleccionen automáticamente las herramientas adecuadas y realicen análisis detallados basados en las salidas de las herramientas. En experimentos representativos de 7 tareas de generación de imágenes condicionales, CIGEval (versión GPT-4o) alcanzó una correlación alta con la evaluación humana de 0.4625 y se asemejó estrechamente a la correlación entre evaluadores de 0.47. Además, al utilizar un LMM de 7B y implementar 2.3K procesos de entrenamiento, CIGEval superó los métodos más avanzados basados en GPT-4o. En un estudio de caso de la generación de imágenes con GPT-4o, CIGEval demostró su capacidad para identificar problemas subtiles en la coherencia del tema y la adherencia a las guías de control, mostrando grandes potenciales para una evaluación automática de nivel humano de confianza en la tarea de generación de imágenes.",
      "upvotes": 5,
      "discussionId": "67f7472b353d129fc7c4bf4b",
      "githubRepo": "https://github.com/HITsz-TMG/Agentic-CIGEval"
    },
    "publishedAt": "2025-04-09T13:04:14.000Z",
    "title": "A Unified Agentic Framework for Evaluating Conditional Image Generation",
    "summary": "Conditional image generation has gained significant attention for its ability\nto personalize content. However, the field faces challenges in developing\ntask-agnostic, reliable, and explainable evaluation metrics. This paper\nintroduces CIGEval, a unified agentic framework for comprehensive evaluation of\nconditional image generation tasks. CIGEval utilizes large multimodal models\n(LMMs) as its core, integrating a multi-functional toolbox and establishing a\nfine-grained evaluation framework. Additionally, we synthesize evaluation\ntrajectories for fine-tuning, empowering smaller LMMs to autonomously select\nappropriate tools and conduct nuanced analyses based on tool outputs.\nExperiments across seven prominent conditional image generation tasks\ndemonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625\nwith human assessments, closely matching the inter-annotator correlation of\n0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K\ntraining trajectories, CIGEval surpasses the previous GPT-4o-based\nstate-of-the-art method. Case studies on GPT-4o image generation highlight\nCIGEval's capability in identifying subtle issues related to subject\nconsistency and adherence to control guidance, indicating its great potential\nfor automating evaluation of image generation tasks with human-level\nreliability.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/xXlL1RROzluluflNDOIRv.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07046.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639c379cdb7c5f35004066cb",
      "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
      "fullname": "Zhenran Xu",
      "name": "imryanxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07092",
      "authors": [
        {
          "_id": "67f7826a8b50772851ccb603",
          "user": {
            "_id": "64198d7efdfc2970b350f48f",
            "avatarUrl": "/avatars/c0a0f30e1cbc22f1eb6bbc4549a5709c.svg",
            "isPro": false,
            "fullname": "Alexander Rubinstein",
            "user": "arubique",
            "type": "user"
          },
          "name": "Alexander Rubinstein",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:52:04.485Z",
          "hidden": false
        },
        {
          "_id": "67f7826a8b50772851ccb604",
          "user": {
            "_id": "6464a0d41683d3c81f51924a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
            "isPro": false,
            "fullname": "Ameya Prabhu",
            "user": "AmeyaPrabhu",
            "type": "user"
          },
          "name": "Ameya Prabhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:52:11.053Z",
          "hidden": false
        },
        {
          "_id": "67f7826a8b50772851ccb605",
          "name": "Matthias Bethge",
          "hidden": false
        },
        {
          "_id": "67f7826a8b50772851ccb606",
          "user": {
            "_id": "638a50450f10aa3064f03f23",
            "avatarUrl": "/avatars/0c068458e42950c851758a238225c3a6.svg",
            "isPro": false,
            "fullname": "Seong Joon Oh",
            "user": "coallaoh",
            "type": "user"
          },
          "name": "Seong Joon Oh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:52:30.797Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T17:59:05.000Z",
      "submittedOnDailyAt": "2025-04-10T07:04:07.299Z",
      "title": "Esa es una pregunta sobre el aprendizaje centrado en el objetivo.",
      "submittedOnDailyBy": {
        "_id": "6464a0d41683d3c81f51924a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
        "isPro": false,
        "fullname": "Ameya Prabhu",
        "user": "AmeyaPrabhu",
        "type": "user"
      },
      "summary": "El aprendizaje centrado en objetos (OCL) es un enfoque que busca obtener representaciones que separen objetos de otros objetos o fondos en un espacio de representación. Este enfoque apoya múltiples objetivos, como la generalización fuera de la distribución (OOD), la eficiencia en el uso de muestras y la modelación de entornos estructurados. Muchos estudios han desarrollado métodos para dividir objetos sin manejo en el espacio de representación y han utilizado la detección de objetos sin manejo para evaluar estos métodos. Sin embargo, recientemente, se ha logrado separar y representar objetos de manera independiente en el espacio de píxeles utilizando modelos de división eficientes en muestras. Esto ha permitido alcanzar un rendimiento notablemente alto en el benchmark de descubrimiento de objetos fuera de la distribución (OOD) con un modelo scalable y que puede adaptarse a la cantidad de slots. De esta manera, se ha logrado significativamente alcanzar el objetivo de OCL de obtener representaciones centradas en objetos.\n\nAdemás de este progreso, una de las cuestiones más importantes es la contribución que la capacidad de dividir objetos tiene para la generalización fuera de la distribución y otros objetivos de OCL. Para abordar esta cuestión, se ha investigado el desafío de generalización fuera de la distribución con objetos separados de fondos incorrectamente desde la perspectiva de OCL y se ha propuesto un nuevo conjunto de entrenamiento no realizado llamado \"Clasificación de clases centrales de objetos y aplicación de máscara (OCCAM)\". Esto ha demostrado que las representaciones basadas en la división de objetos individuales son superiores a los métodos de OCL basados en slots. Sin embargo, aún existen problemas en aplicaciones reales. La comunidad de OCL proporciona una herramienta box para la representación centrada en objetos scalable y se centra en aplicaciones prácticas y problemas básicos, como la comprensión del reconocimiento cognitivo de objetos humanos. El código está disponible en https://github.com/AlexanderRubinstein/OCCAM.",
      "upvotes": 3,
      "discussionId": "67f7826b8b50772851ccb64c"
    },
    "publishedAt": "2025-04-09T13:59:05.000Z",
    "title": "Are We Done with Object-Centric Learning?",
    "summary": "Object-centric learning (OCL) seeks to learn representations that only encode\nan object, isolated from other objects or background cues in a scene. This\napproach underpins various aims, including out-of-distribution (OOD)\ngeneralization, sample-efficient composition, and modeling of structured\nenvironments. Most research has focused on developing unsupervised mechanisms\nthat separate objects into discrete slots in the representation space,\nevaluated using unsupervised object discovery. However, with recent\nsample-efficient segmentation models, we can separate objects in the pixel\nspace and encode them independently. This achieves remarkable zero-shot\nperformance on OOD object discovery benchmarks, is scalable to foundation\nmodels, and can handle a variable number of slots out-of-the-box. Hence, the\ngoal of OCL methods to obtain object-centric representations has been largely\nachieved. Despite this progress, a key question remains: How does the ability\nto separate objects within a scene contribute to broader OCL objectives, such\nas OOD generalization? We address this by investigating the OOD generalization\nchallenge caused by spurious background cues through the lens of OCL. We\npropose a novel, training-free probe called Object-Centric\nClassification with Applied Masks (OCCAM), demonstrating that\nsegmentation-based encoding of individual objects significantly outperforms\nslot-based OCL methods. However, challenges in real-world applications remain.\nWe provide the toolbox for the OCL community to use scalable object-centric\nrepresentations, and focus on practical applications and fundamental questions,\nsuch as understanding object perception in human cognition. Our code is\navailable https://github.com/AlexanderRubinstein/OCCAM{here}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07092.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6464a0d41683d3c81f51924a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
      "fullname": "Ameya Prabhu",
      "name": "AmeyaPrabhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.06947",
      "authors": [
        {
          "_id": "67f78485cfcd3569910c99ab",
          "name": "Natalia Loukachevitch",
          "hidden": false
        },
        {
          "_id": "67f78485cfcd3569910c99ac",
          "name": "Natalia Tkachenko",
          "hidden": false
        },
        {
          "_id": "67f78485cfcd3569910c99ad",
          "name": "Anna Lapanitsyna",
          "hidden": false
        },
        {
          "_id": "67f78485cfcd3569910c99ae",
          "user": {
            "_id": "652cedbdf120598322ae358a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652cedbdf120598322ae358a/RrxrP0gtQus4SfNwfyAg_.jpeg",
            "isPro": false,
            "fullname": "Mikhail",
            "user": "RefalMachine",
            "type": "user"
          },
          "name": "Mikhail Tikhomirov",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-10T09:18:43.707Z",
          "hidden": false
        },
        {
          "_id": "67f78485cfcd3569910c99af",
          "user": {
            "_id": "64e62d11d27a8292c3637f86",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e62d11d27a8292c3637f86/aptDeBHpCJxcREj6KPLN1.jpeg",
            "isPro": false,
            "fullname": "Nicolay Rusnachenko",
            "user": "nicolay-r",
            "type": "user"
          },
          "name": "Nicolay Rusnachenko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T09:57:54.353Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/pVL1YkBNlHeaQUud0VmSt.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/1ph_RyOzjsRcMs_04naOR.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/ewSfbBvFUKclWyZTGCOiE.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/gbTZ0dZq7O6zWKcstfwWy.png"
      ],
      "publishedAt": "2025-04-09T14:54:00.000Z",
      "submittedOnDailyAt": "2025-04-10T07:29:49.621Z",
      "title": "Opinión NE-2024: Tuplas de Opinión Extraídas de Artículos de Prensa Rusa de 2024",
      "submittedOnDailyBy": {
        "_id": "64e62d11d27a8292c3637f86",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e62d11d27a8292c3637f86/aptDeBHpCJxcREj6KPLN1.jpeg",
        "isPro": false,
        "fullname": "Nicolay Rusnachenko",
        "user": "nicolay-r",
        "type": "user"
      },
      "summary": "En este artículo, se presenta la tarea común de evaluación de diálogo para la extracción de opiniones estructuradas en textos de noticias en ruso. El objetivo de la competencia es extraer tuplas de opiniones a partir de frases dadas. Estas tuplas están compuestas por el sujeto de la emoción, su objetivo, la expresión hacia el sujeto de la emoción y la emoción. En total, se recibieron más de 100 entradas para este desafío. Los participantes experimentaron principalmente con modelos de lenguaje grandes en formatos de 0-shot, few-shot y fine-tuning. El mejor resultado en la muestra de prueba fue obtenido mediante el fine-tuning de un modelo de lenguaje grande. Además, se compararon 30 modelos de profenet y 11 modelos de lenguaje abierto (entre 3 y 32 billones de parámetros) en configuraciones de 1-shot y 10-shot para encontrar el mejor modelo y profenet.",
      "upvotes": 3,
      "discussionId": "67f78486cfcd3569910c9a13",
      "projectPage": "https://codalab.lisn.upsaclay.fr/competitions/20244",
      "githubRepo": "https://github.com/dialogue-evaluation/RuOpinionNE-2024"
    },
    "publishedAt": "2025-04-09T10:54:00.000Z",
    "title": "RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts",
    "summary": "In this paper, we introduce the Dialogue Evaluation shared task on extraction\nof structured opinions from Russian news texts. The task of the contest is to\nextract opinion tuples for a given sentence; the tuples are composed of a\nsentiment holder, its target, an expression and sentiment from the holder to\nthe target. In total, the task received more than 100 submissions. The\nparticipants experimented mainly with large language models in zero-shot,\nfew-shot and fine-tuning formats. The best result on the test set was obtained\nwith fine-tuning of a large language model. We also compared 30 prompts and 11\nopen source language models with 3-32 billion parameters in the 1-shot and\n10-shot settings and found the best models and prompts.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/pVL1YkBNlHeaQUud0VmSt.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/1ph_RyOzjsRcMs_04naOR.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/ewSfbBvFUKclWyZTGCOiE.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/gbTZ0dZq7O6zWKcstfwWy.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06947.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e62d11d27a8292c3637f86",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e62d11d27a8292c3637f86/aptDeBHpCJxcREj6KPLN1.jpeg",
      "fullname": "Nicolay Rusnachenko",
      "name": "nicolay-r",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 123
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.04010",
      "authors": [
        {
          "_id": "67f766cb1879ad2f13bee3d1",
          "user": {
            "_id": "63c9f93cdfac8071d01ed56f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674180895772-noauth.jpeg",
            "isPro": false,
            "fullname": "Maksim Siniukov",
            "user": "havent-invented",
            "type": "user"
          },
          "name": "Maksim Siniukov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:10:42.914Z",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d2",
          "user": {
            "_id": "64a5d8219f3b568c202b3137",
            "avatarUrl": "/avatars/eef6fb7c70d272555a53183c0e50dbaf.svg",
            "isPro": false,
            "fullname": "Di Chang",
            "user": "Boese0601",
            "type": "user"
          },
          "name": "Di Chang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:10:49.652Z",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d3",
          "user": {
            "_id": "632b6c08ca316c73cd3e4d8c",
            "avatarUrl": "/avatars/a02aa14823dd729df0267a9b55779edd.svg",
            "isPro": false,
            "fullname": "Minh Tran",
            "user": "minhtran",
            "type": "user"
          },
          "name": "Minh Tran",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:10:55.537Z",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d4",
          "user": {
            "_id": "6605cfc7b85b7b4ea506a33d",
            "avatarUrl": "/avatars/92abda656abf2298c9d28b9b2e3643a3.svg",
            "isPro": false,
            "fullname": "Hongkun Gong",
            "user": "hongkung",
            "type": "user"
          },
          "name": "Hongkun Gong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:11:01.857Z",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d5",
          "user": {
            "_id": "6541185dbd60d2bd193f7999",
            "avatarUrl": "/avatars/4ce4a0feff9bfbb87e9f40431718ba00.svg",
            "isPro": false,
            "fullname": "Ashutosh Chaubey",
            "user": "chaubeyG",
            "type": "user"
          },
          "name": "Ashutosh Chaubey",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:11:08.391Z",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d6",
          "user": {
            "_id": "65fcb99d383d3f256c3a92d2",
            "avatarUrl": "/avatars/b85d32f4d7a19816b8d499e05b173ad1.svg",
            "isPro": false,
            "fullname": "Mohammad Soleymani",
            "user": "msoleymani",
            "type": "user"
          },
          "name": "Mohammad Soleymani",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:11:15.030Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-05T01:19:46.000Z",
      "submittedOnDailyAt": "2025-04-10T05:06:42.197Z",
      "title": "Detalles del listado: Generación de vídeos auditivos de alta calidad con controlable distribución",
      "submittedOnDailyBy": {
        "_id": "64a5d8219f3b568c202b3137",
        "avatarUrl": "/avatars/eef6fb7c70d272555a53183c0e50dbaf.svg",
        "isPro": false,
        "fullname": "Di Chang",
        "user": "Boese0601",
        "type": "user"
      },
      "summary": "El movimiento de los escuchadores natural y mínimo es un problema abierto que permanece en la interacción a lo largo de tiempos prolongados. Los métodos existentes se basan principalmente en códigos de movimiento de baja dimensión para generar movimientos faciales, dependiendo de esto para realizar un renderizado realístico, lo que limita la precisión visual y la expresividad. Para resolver estas desafíos, presentamos DiTaiListener, un modelo de difusión vídeo con múltiples modalidades. Nuestro método comienza con DiTaiListener-Gen, que genera respuestas de escuchadores a pequeñas secciones de video a partir de la voz y el movimiento facial del interlocutor. Luego, DiTaiListener-Edit ajusta estos fragmentos de video a nivel de píxeles para lograr una transición sin interrupciones. Específicamente, DiTaiListener-Gen introduce un adaptador multimodal de series de tiempo causal (CTM-Adapter) para procesar los señales auditivas y visuales del interlocutor y generar imágenes de escuchadores utilizando un transformer de difusión (DiT). El CTM-Adapter integra los datos del interlocutor en un modo causal durante el proceso de generación de video, asegurando la continuidad temporal de la respuesta del escuchador. Para generar videos largos, introducimos DiTaiListener-Edit para combinar secciones de video de manera armoniosa y continua, asegurando que las expresiones faciales y la calidad de la imagen estén coherentes en el tiempo al combinar fragmentos de video generados por DiTaiListener-Gen. De manera dual, DiTaiListener ha alcanzado los mejores resultados en el conjunto de datos de referencia, mejorando significativamente la realismo (73.8% de mejora en FID en RealTalk) y la expresión del movimiento (6.1% de mejora en el índice FD en VICO). Las investigaciones de usuario han confirmado la excelencia de DiTaiListener, demostrando un desafío claro en aspectos de calidad, diversidad y suavidad frente a los competidores.",
      "upvotes": 3,
      "discussionId": "67f766ce1879ad2f13bee47a",
      "projectPage": "https://cv.maxi.su/DiTaiListener/"
    },
    "publishedAt": "2025-04-04T21:19:46.000Z",
    "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with\n  Diffusion",
    "summary": "Generating naturalistic and nuanced listener motions for extended\ninteractions remains an open problem. Existing methods often rely on\nlow-dimensional motion codes for facial behavior generation followed by\nphotorealistic rendering, limiting both visual fidelity and expressive\nrichness. To address these challenges, we introduce DiTaiListener, powered by a\nvideo diffusion model with multimodal conditions. Our approach first generates\nshort segments of listener responses conditioned on the speaker's speech and\nfacial motions with DiTaiListener-Gen. It then refines the transitional frames\nvia DiTaiListener-Edit for a seamless transition. Specifically,\nDiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener\nhead portrait generation by introducing a Causal Temporal Multimodal Adapter\n(CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter\nintegrates speakers' input in a causal manner into the video generation process\nto ensure temporally coherent listener responses. For long-form video\ngeneration, we introduce DiTaiListener-Edit, a transition refinement\nvideo-to-video diffusion model. The model fuses video segments into smooth and\ncontinuous videos, ensuring temporal consistency in facial expressions and\nimage quality when merging short video segments produced by DiTaiListener-Gen.\nQuantitatively, DiTaiListener achieves the state-of-the-art performance on\nbenchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion\nrepresentation (+6.1% in FD metric on VICO) spaces. User studies confirm the\nsuperior performance of DiTaiListener, with the model being the clear\npreference in terms of feedback, diversity, and smoothness, outperforming\ncompetitors by a significant margin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04010.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a5d8219f3b568c202b3137",
      "avatarUrl": "/avatars/eef6fb7c70d272555a53183c0e50dbaf.svg",
      "fullname": "Di Chang",
      "name": "Boese0601",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07081",
      "authors": [
        {
          "_id": "67f7823da630bcdabbd8b3eb",
          "name": "Gabriel Grand",
          "hidden": false
        },
        {
          "_id": "67f7823da630bcdabbd8b3ec",
          "name": "Joshua B. Tenenbaum",
          "hidden": false
        },
        {
          "_id": "67f7823da630bcdabbd8b3ed",
          "name": "Vikash K. Mansinghka",
          "hidden": false
        },
        {
          "_id": "67f7823da630bcdabbd8b3ee",
          "user": {
            "_id": "673cc08370644bb836283fec",
            "avatarUrl": "/avatars/b8c7f1a10ddf76dcd06398c59f553b61.svg",
            "isPro": false,
            "fullname": "Alexander Lew",
            "user": "alexanderlew",
            "type": "user"
          },
          "name": "Alexander K. Lew",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-10T08:33:02.433Z",
          "hidden": false
        },
        {
          "_id": "67f7823da630bcdabbd8b3ef",
          "name": "Jacob Andreas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T17:54:22.000Z",
      "submittedOnDailyAt": "2025-04-10T07:03:34.220Z",
      "title": "InternLM (书生·浦语)",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Según la teoría lógica durante el test, los modelos de lenguaje pueden realizar tareas complejas pero son lentos y costosos en búsquedas y planificación, además de ser causas de errores. Sin embargo, existe la posibilidad de que un modelo de lenguaje (LM) se alinee con los etapas precisas de una teoría lógica necesaria para su solución, aunque excela en la estructura abstracta. En este artículo, se presenta el método de \"autocontrol\". Este método consiste en que un modelo de planificación genera un programa de inferencia para responder a una tarea, que es luego ejecutado por un grupo de modelos de seguimiento. Nuestro enfoque permite escribir los modelos de lenguaje de manera recursiva en un orden de búsqueda, y guia la inferencia de los modelos de lenguaje con un nuevo formato lógico. En casos donde se utilizan pequeños modelos de seguimiento (por ejemplo, Llama-3.2-1B), DisCIPL es un modelo grande como GPT-4o o o1. Al separar la planificación de la ejecución, nuestro estudio explora el espacio de diseño para una estrategia de Monte Carlo de alta dimensión y paralela que supera la mejor muestración estándar de N, mostrando lo que es automáticamente implementable en los modelos de lenguaje actuales.",
      "upvotes": 1,
      "discussionId": "67f7823ea630bcdabbd8b42e"
    },
    "publishedAt": "2025-04-09T13:54:22.000Z",
    "title": "Self-Steering Language Models",
    "summary": "While test-time reasoning enables language models to tackle complex tasks,\nsearching or planning in natural language can be slow, costly, and error-prone.\nBut even when LMs struggle to emulate the precise reasoning steps needed to\nsolve a problem, they often excel at describing its abstract structure--both\nhow to verify solutions and how to search for them. This paper introduces\nDisCIPL, a method for \"self-steering\" LMs where a Planner model generates a\ntask-specific inference program that is executed by a population of Follower\nmodels. Our approach equips LMs with the ability to write recursive search\nprocedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models,\nincluding GPT-4o and o1, on challenging constrained generation tasks. In\ndecoupling planning from execution, our work opens up a design space of\nhighly-parallelized Monte Carlo inference strategies that outperform standard\nbest-of-N sampling, require no finetuning, and can be implemented automatically\nby existing LMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07081.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6621
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03886",
      "authors": [
        {
          "_id": "67f78d2850c25afaf8a1210f",
          "name": "Jianhao Zheng",
          "hidden": false
        },
        {
          "_id": "67f78d2850c25afaf8a12110",
          "name": "Zihan Zhu",
          "hidden": false
        },
        {
          "_id": "67f78d2850c25afaf8a12111",
          "name": "Valentin Bieri",
          "hidden": false
        },
        {
          "_id": "67f78d2850c25afaf8a12112",
          "name": "Marc Pollefeys",
          "hidden": false
        },
        {
          "_id": "67f78d2850c25afaf8a12113",
          "name": "Songyou Peng",
          "hidden": false
        },
        {
          "_id": "67f78d2850c25afaf8a12114",
          "name": "Iro Armeni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T19:19:40.000Z",
      "submittedOnDailyAt": "2025-04-10T07:53:22.145Z",
      "title": "WildGS-SLAM: SLAM basado en cámara única en entornos dinámicos",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "WildGS-SLAM introduce a powerful and efficient single-camera RGB-SLAM system. This system operates in dynamic environments by leveraging geometric mapping that considers uncertainty. Traditional SLAM systems assume static scenes, but our approach detects dynamic objects and enhances mapping and rendering performance by integrating depth and uncertainty information. We introduce the predicted uncertainty map. This map guides the removal of dynamic objects and is used in both mapping and charging. The uncertainty map improves reconstruction accuracy through dense bundle adjustment and Gaussian map optimization. WildGS-SLAM demonstrates artifact-free visual synthesis across multiple datasets, proving high performance compared to state-of-the-art methods in dynamic environments.",
      "upvotes": 1,
      "discussionId": "67f78d2e50c25afaf8a122c9"
    },
    "publishedAt": "2025-04-04T15:19:40.000Z",
    "title": "WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments",
    "summary": "We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system\ndesigned to handle dynamic environments by leveraging uncertainty-aware\ngeometric mapping. Unlike traditional SLAM systems, which assume static scenes,\nour approach integrates depth and uncertainty information to enhance tracking,\nmapping, and rendering performance in the presence of moving objects. We\nintroduce an uncertainty map, predicted by a shallow multi-layer perceptron and\nDINOv2 features, to guide dynamic object removal during both tracking and\nmapping. This uncertainty map enhances dense bundle adjustment and Gaussian map\noptimization, improving reconstruction accuracy. Our system is evaluated on\nmultiple datasets and demonstrates artifact-free view synthesis. Results\nshowcase WildGS-SLAM's superior performance in dynamic environments compared to\nstate-of-the-art methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03886.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6621
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.06958",
      "authors": [
        {
          "_id": "67f783fc2eec6ce5c8d18be3",
          "user": {
            "_id": "672f8a28c53c174f39b08ac1",
            "avatarUrl": "/avatars/9d865f757667de14381d7c4d7ba7e4c4.svg",
            "isPro": false,
            "fullname": "XINHAO LI",
            "user": "xinhaoli",
            "type": "user"
          },
          "name": "Xinhao Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:52:46.553Z",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18be4",
          "user": {
            "_id": "65499e5f2a292b3e2e5715a3",
            "avatarUrl": "/avatars/087b3e36dfb66e044265b856bab31657.svg",
            "isPro": false,
            "fullname": "ziang yan",
            "user": "Aurorana",
            "type": "user"
          },
          "name": "Ziang Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:52:52.960Z",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18be5",
          "user": {
            "_id": "63217b7231205fe84a9626ca",
            "avatarUrl": "/avatars/ed1e96c713c0b884adc87b8c12faa32c.svg",
            "isPro": false,
            "fullname": "Desen Meng",
            "user": "desenmeng",
            "type": "user"
          },
          "name": "Desen Meng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:53:00.046Z",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18be6",
          "user": {
            "_id": "666946fdec88f15e04db6022",
            "avatarUrl": "/avatars/84511d4cd2a6bdc229dd2b1057d4b2ab.svg",
            "isPro": false,
            "fullname": "Lu Dong",
            "user": "donglu",
            "type": "user"
          },
          "name": "Lu Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:53:11.431Z",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18be7",
          "user": {
            "_id": "660a7e1c3fbd33a1d0b0e233",
            "avatarUrl": "/avatars/ceff1231078115cae8f3f4f87d026963.svg",
            "isPro": false,
            "fullname": "Xiangyu Zeng",
            "user": "Lanxingxuan",
            "type": "user"
          },
          "name": "Xiangyu Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:53:25.534Z",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18be8",
          "user": {
            "_id": "65b9d9961fe588f824fde191",
            "avatarUrl": "/avatars/a9245958cc998a4b4b870bf2490fdaee.svg",
            "isPro": false,
            "fullname": "Yinan He",
            "user": "yinanhe",
            "type": "user"
          },
          "name": "Yinan He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:53:32.159Z",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18be9",
          "name": "Yali Wang",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18bea",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18beb",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67f783fc2eec6ce5c8d18bec",
          "user": {
            "_id": "643d4996482011f5f2be271f",
            "avatarUrl": "/avatars/134b8f5d44b85d55eaaa2bbe6c409917.svg",
            "isPro": false,
            "fullname": "limin wang",
            "user": "flyacht",
            "type": "user"
          },
          "name": "Limin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-10T08:53:52.538Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T15:09:27.000Z",
      "submittedOnDailyAt": "2025-04-10T07:11:25.167Z",
      "title": "VideoChat-R1: Mejora la reconocimiento del espacio-tiempo mediante aprendizaje por refuerzo.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "El desarrollo reciente del aprendizaje reforzado ha mejorado significativamente las capacidades de los modelos multimodales de lenguaje y imágenes (MLLMs). Métodos como la optimización de políticas en grupos (GRPO) y estructuras de recompensa basadas en reglas han demostrado resultados deseados en el campo de las oraciones e imágenes, pero su aplicación en la comprensión de videos está limitada. En este artículo, exploramos sistemáticamente la aprendizaje reforzado con ajustes micro (RFT) utilizando GRPO, con el objetivo de mejorar la percepción temporal-espacial mientras mantenemos las capacidades generales. Nuestros experimentos muestran que el RFT demostra un alto rendimiento de datos en la mejora de las tareas específicas. Desarrollamos VideoChat-R1, un potente MLLM de video que mejora la percepción temporal-espacial en tareas de este tipo, manteniendo su capacidad de chat sin perder. Comparado con Qwen2.5-VL-7B, VideoChat-R1 obtiene mejoras significativas en tareas como detección de tiempo (+31.8) y seguimiento de objetos (+31.2). Además, también muestra mejoras notables en benchmarks generales como Video MME (+0.9), MVBench (+1.0) y Test de Percepción (+0.9). Nuestros hallazgos subrayan la posibilidad de que el RFT puede mejorar las tareas específicas de los MLLMs de video. Nuestra investigación espera que futuros estudios de aprendizaje reforzado proporcionen más datos para los MLLMs de video, lo que podría mejorar aún más sus capacidades.",
      "upvotes": 0,
      "discussionId": "67f783fd2eec6ce5c8d18c2e"
    },
    "publishedAt": "2025-04-09T11:09:27.000Z",
    "title": "VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement\n  Fine-Tuning",
    "summary": "Recent advancements in reinforcement learning have significantly advanced the\nreasoning capabilities of multimodal large language models (MLLMs). While\napproaches such as Group Relative Policy Optimization (GRPO) and rule-based\nreward mechanisms demonstrate promise in text and image domains, their\napplication to video understanding remains limited. This paper presents a\nsystematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video\nMLLMs, aiming to enhance spatio-temporal perception while maintaining general\ncapabilities. Our experiments reveal that RFT is highly data-efficient for\ntask-specific improvements. Through multi-task RFT on spatio-temporal\nperception objectives with limited samples, we develop VideoChat-R1, a powerful\nvideo MLLM that achieves state-of-the-art performance on spatio-temporal\nperception tasks without sacrificing chat ability, while exhibiting emerging\nspatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1\nboosts performance several-fold in tasks like temporal grounding (+31.8) and\nobject tracking (+31.2). Additionally, it significantly improves on general QA\nbenchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9).\nOur findings underscore the potential of RFT for specialized task enhancement\nof Video MLLMs. We hope our work offers valuable insights for future RL\nresearch in video MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06958.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6621
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.05287",
      "authors": [
        {
          "_id": "67f6b394b67801f1ab494709",
          "user": {
            "_id": "67f6b0fd2142abc30f1a193e",
            "avatarUrl": "/avatars/abc81a1bef1055da378c780d435dcc0a.svg",
            "isPro": false,
            "fullname": "Hui Zhang",
            "user": "ethHuiZhang",
            "type": "user"
          },
          "name": "Hui Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:45:15.453Z",
          "hidden": false
        },
        {
          "_id": "67f6b394b67801f1ab49470a",
          "name": "Zijian Wu",
          "hidden": false
        },
        {
          "_id": "67f6b394b67801f1ab49470b",
          "name": "Linyi Huang",
          "hidden": false
        },
        {
          "_id": "67f6b394b67801f1ab49470c",
          "name": "Sammy Christen",
          "hidden": false
        },
        {
          "_id": "67f6b394b67801f1ab49470d",
          "name": "Jie Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:38:19.000Z",
      "submittedOnDailyAt": "2025-04-10T06:35:15.115Z",
      "title": "RobustDexGrasp: Observación de manos firmes y seguras para una visión general de objetos comunes",
      "submittedOnDailyBy": {
        "_id": "67f6b0fd2142abc30f1a193e",
        "avatarUrl": "/avatars/abc81a1bef1055da378c780d435dcc0a.svg",
        "isPro": false,
        "fullname": "Hui Zhang",
        "user": "ethHuiZhang",
        "type": "user"
      },
      "summary": "Capturar fuertes diferencias en observaciones múltiples de diferentes objetos es la base de los dobles robots. En estudios previos, se utilizaron objetos completamente observables, guía de expertos o posturas estáticas para capturar diferencias, pero estos limitaban la extensibilidad y la adaptación a la confusión externa. En este artículo, se propone un marco de aprendizaje por refuerzo que permite capturar diferencias dinámicas de objetos no vistos en 0-shot. Además, puede realizar acciones adaptativas a la confusión externa. Utilizando una representación de objetos concentrada, se extraen características de forma, se enfatizan características locales relacionadas con la interacción y se mejora la robustez frente a cambios de forma e incertidumbres. Para responder a la confusión con observaciones limitadas, se propone un estado de aprendizaje mixto de Kalman. Primero, se utiliza retroalimentación visual tactil con distribución de tiempo privilegiada para inducir el aprendizaje de la política mediante aprendizaje simulado, y luego se pasa al aprendizaje por refuerzo para aprender acciones adaptativas frente a ruido de observación y randomización dinámica. Los experimentos muestran que la extensibilidad para capturar diferencias en posturas sin confusión de objetos no vistos se ha fortalecido, logrando un éxito del 97.0% con 247,786 objetos de simulación y un éxito del 94.6% con 512 objetos reales. Además, se pueden realizar evaluaciones cuantitativas y cualitativas que demuestran su robustez frente a diversas confusiones. Página del proyecto: https://zdchan.github.io/Robust_DexGrasp/",
      "upvotes": 0,
      "discussionId": "67f6b399b67801f1ab49487f",
      "projectPage": "https://zdchan.github.io/Robust_DexGrasp/"
    },
    "publishedAt": "2025-04-07T13:38:19.000Z",
    "title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects from\n  Single-view Perception",
    "summary": "Robust grasping of various objects from single-view perception is fundamental\nfor dexterous robots. Previous works often rely on fully observable objects,\nexpert demonstrations, or static grasping poses, which restrict their\ngeneralization ability and adaptability to external disturbances. In this\npaper, we present a reinforcement-learning-based framework that enables\nzero-shot dynamic dexterous grasping of a wide range of unseen objects from\nsingle-view perception, while performing adaptive motions to external\ndisturbances. We utilize a hand-centric object representation for shape feature\nextraction that emphasizes interaction-relevant local shapes, enhancing\nrobustness to shape variance and uncertainty. To enable effective hand\nadaptation to disturbances with limited observations, we propose a mixed\ncurriculum learning strategy, which first utilizes imitation learning to\ndistill a policy trained with privileged real-time visual-tactile feedback, and\ngradually transfers to reinforcement learning to learn adaptive motions under\ndisturbances caused by observation noises and dynamic randomization. Our\nexperiments demonstrate strong generalization in grasping unseen objects with\nrandom poses, achieving success rates of 97.0% across 247,786 simulated objects\nand 94.6% across 512 real objects. We also demonstrate the robustness of our\nmethod to various disturbances, including unobserved object movement and\nexternal forces, through both quantitative and qualitative evaluations. Project\nPage: https://zdchan.github.io/Robust_DexGrasp/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05287.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67f6b0fd2142abc30f1a193e",
      "avatarUrl": "/avatars/abc81a1bef1055da378c780d435dcc0a.svg",
      "fullname": "Hui Zhang",
      "name": "ethHuiZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]