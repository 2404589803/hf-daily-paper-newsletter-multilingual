[
  {
    "paper": {
      "id": "2504.08685",
      "authors": [
        {
          "_id": "67fc6ffc59b22e7c34d64c2e",
          "name": "Team Seawead",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c2f",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c30",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c31",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c32",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c33",
          "name": "Zhibei Ma",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c34",
          "name": "Haoyuan Guo",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c35",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c36",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c37",
          "name": "Sen Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c38",
          "name": "Feng Cheng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c39",
          "name": "Feilong Zuo Xuejiao Zeng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3a",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3b",
          "name": "Fangyuan Kong",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3c",
          "name": "Zhiwu Qing",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3d",
          "name": "Fei Xiao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3e",
          "name": "Meng Wei",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3f",
          "name": "Tuyen Hoang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c40",
          "name": "Siyu Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c41",
          "name": "Peihao Zhu",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c42",
          "name": "Qi Zhao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c43",
          "name": "Jiangqiao Yan",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c44",
          "name": "Liangke Gui",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c45",
          "name": "Sheng Bi",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c46",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c47",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c48",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c49",
          "name": "Huixia Li",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4a",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4b",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4c",
          "user": {
            "_id": "636a4e4fa55bbbdf8c877667",
            "avatarUrl": "/avatars/efdb68c56a4a44fdac52750c07a6cc35.svg",
            "isPro": false,
            "fullname": "Ling Feng",
            "user": "lingff",
            "type": "user"
          },
          "name": "Feng Ling",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:31.964Z",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4d",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4e",
          "name": "Houmin Wei",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4f",
          "name": "Huafeng Kuang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c50",
          "name": "Jerry Duncan",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c51",
          "name": "Junda Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c52",
          "name": "Junru Zheng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c53",
          "name": "Li Sun",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c54",
          "name": "Manlin Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c55",
          "name": "Renfei Sun",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c56",
          "name": "Xiaobin Zhuang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c57",
          "name": "Xiaojie Li",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c58",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c59",
          "name": "Xuyan Chi",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5a",
          "name": "Yanghua Peng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5b",
          "name": "Yuping Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5c",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5d",
          "name": "Zhongkai Zhao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5e",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5f",
          "name": "Zuquan Song",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c60",
          "user": {
            "_id": "6421183b69a2c2933882d652",
            "avatarUrl": "/avatars/66813a8fa22915087cccd4dbfb945ca7.svg",
            "isPro": false,
            "fullname": "Zhenheng Yang",
            "user": "zhenheny",
            "type": "user"
          },
          "name": "Zhenheng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:34.053Z",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c61",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c62",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c63",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a5cba3bea0116f8f7187a7/Zl6Xq55_4dviWTCMwOptb.jpeg"
      ],
      "publishedAt": "2025-04-11T16:46:20.000Z",
      "submittedOnDailyAt": "2025-04-14T00:55:27.428Z",
      "title": "Algoritmo-7B: Entrenamiento eficiente de modelos basados en la generación de vídeos",
      "submittedOnDailyBy": {
        "_id": "64a5cba3bea0116f8f7187a7",
        "avatarUrl": "/avatars/97b3bad82e359c57d1ab7d6e3aa7748b.svg",
        "isPro": false,
        "fullname": "Lu Jiang",
        "user": "roadjiang",
        "type": "user"
      },
      "summary": "En este informe técnico se propone estrategias eficientes para el entrenamiento de modelos basados en la generación de vídeos. Se utiliza un modelo de tamaño interno llamado 'Seaweed-7B', que tiene aproximadamente 700 millones de parámetros (7B), entrenado en un total de 665,000 horas de GPU H100 en un formato reducido. Inclusive cuando los recursos computacionales son limitados, 'Seaweed-7B' muestra un rendimiento competitivo frente a modelos grandes de generación de vídeos, lo que es especialmente importante en entornos con restricciones de recursos. En este informe técnico se presentan de manera central los decisiones de diseño clave que contribuyen al mejoramiento de la performance del modelo Difu-Jetion de tamaño interno. Se realizan dos observaciones experimentales: 1) 'Seaweed-7B' puede compararse y superar el rendimiento de modelos grandes entrenados con grandes recursos de GPU. 2) Modelos con una fuerte capacidad de generalización pueden adaptarse efectivamente a una amplia gama de aplicaciones de vídeo. Para más información, se puede visitar la página del proyecto en https://seaweed.video/.",
      "upvotes": 57,
      "discussionId": "67fc700159b22e7c34d64d78",
      "projectPage": "https://seaweed.video/"
    },
    "publishedAt": "2025-04-11T12:46:20.000Z",
    "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
    "summary": "This technical report presents a cost-efficient strategy for training a video\ngeneration foundation model. We present a mid-sized research model with\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\nresources, Seaweed-7B demonstrates highly competitive performance compared to\ncontemporary video generation models of much larger size. Design choices are\nespecially crucial in a resource-constrained setting. This technical report\nhighlights the key design decisions that enhance the performance of the\nmedium-sized diffusion model. Empirically, we make two observations: (1)\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\ntrained on substantially greater GPU resources, and (2) our model, which\nexhibits strong generalization ability, can be effectively adapted across a\nwide range of downstream applications either by lightweight fine-tuning or\ncontinue training. See the project page at https://seaweed.video/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a5cba3bea0116f8f7187a7/Zl6Xq55_4dviWTCMwOptb.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08685.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "64a5cba3bea0116f8f7187a7",
      "avatarUrl": "/avatars/97b3bad82e359c57d1ab7d6e3aa7748b.svg",
      "fullname": "Lu Jiang",
      "name": "roadjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08736",
      "authors": [
        {
          "_id": "67fc8e37864dfcbd93d3b802",
          "user": {
            "_id": "668125557b50b433cda2a211",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
            "isPro": false,
            "fullname": "Tianwei Xiong",
            "user": "YuuTennYi",
            "type": "user"
          },
          "name": "Tianwei Xiong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:29.330Z",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b803",
          "name": "Jun Hao Liew",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b804",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b805",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b806",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:26.140Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T17:59:58.000Z",
      "submittedOnDailyAt": "2025-04-14T02:57:04.488Z",
      "title": "GigaTok: 30억 파라미터의 시각 토크나이저를 확장하여 자동 복원 이미지 생성에 적용",
      "submittedOnDailyBy": {
        "_id": "668125557b50b433cda2a211",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
        "isPro": false,
        "fullname": "Tianwei Xiong",
        "user": "YuuTennYi",
        "type": "user"
      },
      "summary": "En la generación de imágenes por auto-regresión (AR), el tokenizador visual comprese las imágenes en un conjunto de potenciales distribuidos para facilitar el aprendizaje eficiente de la generación visual mediante la predicción de los siguientes tokens. Aunque la calidad de reconstrucción de las imágenes puede mejorarse al expandir el tokenizador visual, a menudo se observa una deterioro en la calidad de la generación en la parte posterior. Esto es un problema no suficientemente abordado en la literatura actual. En respuesta a este desafío, proponemos GigaTok, la primera aproximación que busca mejorar simultáneamente la calidad de reconstrucción, generación y representación de imágenes al expandir el tokenizador visual. Se ha reconocido que la expansión del espacio potencial es la principal causa del contratiempo entre reconstrucción y generación. Para mitigar esto, proponemos una normalización semántica que alinea las características del tokenizador con aquellas del encoder previamente preprocesado, lo que ayuda a evitar la excesiva complejidad del espacio potencial y mejora tanto la reconstrucción como la generación automática de AR. Basándonos en esta normalización, examinamos tres estrategias principales para expandir el tokenizador: (1) utilizar un tokenizador 1D para mejorar la escalabilidad, (2) priorizar la escalabilidad del decodificador al expandirlo tanto como el encoder, y (3) mejorar la estabilidad del entrenamiento del tokenizador de escalas de berry utilizando la pérdida de entropía. La expansión de GigaTok a 30 mil millones de parámetros logra la mejor calidad en reconstrucción, generación y representación de AR.",
      "upvotes": 23,
      "discussionId": "67fc8e38864dfcbd93d3b836",
      "projectPage": "https://silentview.github.io/GigaTok/",
      "githubRepo": "https://github.com/SilentView/GigaTok"
    },
    "publishedAt": "2025-04-11T13:59:58.000Z",
    "title": "GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for\n  Autoregressive Image Generation",
    "summary": "In autoregressive (AR) image generation, visual tokenizers compress images\ninto compact discrete latent tokens, enabling efficient training of downstream\nautoregressive models for visual generation via next-token prediction. While\nscaling visual tokenizers improves image reconstruction quality, it often\ndegrades downstream generation quality -- a challenge not adequately addressed\nin existing literature. To address this, we introduce GigaTok, the first\napproach to simultaneously improve image reconstruction, generation, and\nrepresentation learning when scaling visual tokenizers. We identify the growing\ncomplexity of latent space as the key factor behind the reconstruction vs.\ngeneration dilemma. To mitigate this, we propose semantic regularization, which\naligns tokenizer features with semantically consistent features from a\npre-trained visual encoder. This constraint prevents excessive latent space\ncomplexity during scaling, yielding consistent improvements in both\nreconstruction and downstream autoregressive generation. Building on semantic\nregularization, we explore three key practices for scaling tokenizers:(1) using\n1D tokenizers for better scalability, (2) prioritizing decoder scaling when\nexpanding both encoder and decoder, and (3) employing entropy loss to stabilize\ntraining for billion-scale tokenizers. By scaling to 3 space billion\nparameters, GigaTok achieves state-of-the-art performance in reconstruction,\ndownstream AR generation, and downstream AR representation quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08736.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668125557b50b433cda2a211",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
      "fullname": "Tianwei Xiong",
      "name": "YuuTennYi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08388",
      "authors": [
        {
          "_id": "67fc7367df5f5d1e87c14c6a",
          "name": "Junliang Guo",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6b",
          "name": "Yang Ye",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6c",
          "name": "Tianyu He",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6d",
          "name": "Haoyu Wu",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6e",
          "name": "Yushu Jiang",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6f",
          "name": "Tim Pearce",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c70",
          "name": "Jiang Bian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T09:41:04.000Z",
      "submittedOnDailyAt": "2025-04-14T02:07:06.500Z",
      "title": "MineWorld: Modelo de mundo interactivo de tiempo real de Minecraft abierto fuente",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "El modelado del mundo es una tarea importante que permite que las inteligencias artificiales interactúen efectivamente con los seres humanos y funcionen en entornos dinámicos. En este artículo, proponemos un nuevo modelo de mundo interactivo en secuencia, llamado \"MineWorld\", para el juego de sandbox \"Minecraft\" utilizando un enfoque abierto y de acceso abierto a través de un \"Test Box\". \"MineWorld\" utiliza un auto-regressivo de retroalimentación visual para generar las siguientes escenas a partir de la percepción visual y las acciones. Específicamente, utilizamos tokenización de imágenes y acciones para convertir la percepción visual y las acciones en IDs discretos, lo que se combinan para configurar el input del modelo. Posteriormente, mediante la predicción de tokenización futura, se entrena el modelo para representar de manera rica el estado del juego y las relaciones entre el estado y las acciones. Durante la inferencia, desarrollamos un nuevo algoritmo de decodificación paralela que predice simultáneamente las repeticiones espaciales de cada frame, permitiendo a los jugadores interactuar de manera secuencial y a modelos de diferentes escalas generar entre 4 y 7 frames. En la evaluación, proponemos nuevos métricas para evaluar la calidad visual de las nuevas escenas y la adaptabilidad de las acciones, demostrando la importancia del modelo de mundo. En evaluaciones detalladas, el efecto de \"MineWorld\" supera significativamente a modelos de mundo basados en ramificación de versiones ordenadas. Los códigos y modelos se han publicado.",
      "upvotes": 12,
      "discussionId": "67fc7367df5f5d1e87c14ca6",
      "githubRepo": "https://github.com/microsoft/MineWorld"
    },
    "publishedAt": "2025-04-11T05:41:04.000Z",
    "title": "MineWorld: a Real-Time and Open-Source Interactive World Model on\n  Minecraft",
    "summary": "World modeling is a crucial task for enabling intelligent agents to\neffectively interact with humans and operate in dynamic environments. In this\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\nan open-ended sandbox game which has been utilized as a common testbed for\nworld modeling. MineWorld is driven by a visual-action autoregressive\nTransformer, which takes paired game scenes and corresponding actions as input,\nand generates consequent new scenes following the actions. Specifically, by\ntransforming visual game scenes and actions into discrete token ids with an\nimage tokenizer and an action tokenizer correspondingly, we consist the model\ninput with the concatenation of the two kinds of ids interleaved. The model is\nthen trained with next token prediction to learn rich representations of game\nstates as well as the conditions between states and actions simultaneously. In\ninference, we develop a novel parallel decoding algorithm that predicts the\nspatial redundant tokens in each frame at the same time, letting models in\ndifferent scales generate 4 to 7 frames per second and enabling real-time\ninteractions with game players. In evaluation, we propose new metrics to assess\nnot only visual quality but also the action following capacity when generating\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\nbased world models significantly. The code and model have been released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08388.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.07963",
      "authors": [
        {
          "_id": "67f86da6ac109135e18e150f",
          "name": "Shoufa Chen",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1510",
          "name": "Chongjian Ge",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1511",
          "name": "Shilong Zhang",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1512",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1513",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6412a33900634c4fe9873652/tQXTl59CIq5IxKj_5dk7f.jpeg"
      ],
      "publishedAt": "2025-04-10T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-14T04:05:17.975Z",
      "title": "PixelFlow: Modelo generativo en el espacio de píxeles utilizando flujos",
      "submittedOnDailyBy": {
        "_id": "6412a33900634c4fe9873652",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
        "isPro": false,
        "fullname": "Shoufa Chen",
        "user": "ShoufaChen",
        "type": "user"
      },
      "summary": "PixelFlow es un conjunto de modelos de generación de imágenes que operan directamente en el espacio de píxeles real, en contraste con los modelos de espacio potencial comunes. Este enfoque reduce la necesidad de entrenar previamente un Variational Autoencoder (VAE) y permite que el modelo sea entrenable de principio a fin. Mediante un modelado eficiente jerárquico, PixelFlow puede reducir los costos de cálculo en el espacio de píxeles real. En el benchmark de generación de imágenes condicionadas por clases de ImageNet de 256x256, el valor de FID es de 1.98. Los resultados de imagenes generados a partir de textos de alta calidad demuestran que PixelFlow excel en la calidad, artesanía y control semántico de las imágenes. Esperamos que este nuevo paradigma abra nuevas oportunidades y inspiraciones para las próximas generaciones de modelos de generación de imágenes. Los códigos y modelos están disponibles en https://github.com/ShoufaChen/PixelFlow.",
      "upvotes": 8,
      "discussionId": "67f86da7ac109135e18e154b",
      "githubRepo": "https://github.com/ShoufaChen/PixelFlow"
    },
    "publishedAt": "2025-04-10T13:59:56.000Z",
    "title": "PixelFlow: Pixel-Space Generative Models with Flow",
    "summary": "We present PixelFlow, a family of image generation models that operate\ndirectly in the raw pixel space, in contrast to the predominant latent-space\nmodels. This approach simplifies the image generation process by eliminating\nthe need for a pre-trained Variational Autoencoder (VAE) and enabling the whole\nmodel end-to-end trainable. Through efficient cascade flow modeling, PixelFlow\nachieves affordable computation cost in pixel space. It achieves an FID of 1.98\non 256times256 ImageNet class-conditional image generation benchmark. The\nqualitative text-to-image results demonstrate that PixelFlow excels in image\nquality, artistry, and semantic control. We hope this new paradigm will inspire\nand open up new opportunities for next-generation visual generation models.\nCode and models are available at https://github.com/ShoufaChen/PixelFlow.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6412a33900634c4fe9873652/tQXTl59CIq5IxKj_5dk7f.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07963.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6412a33900634c4fe9873652",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
      "fullname": "Shoufa Chen",
      "name": "ShoufaChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08600",
      "authors": [
        {
          "_id": "67fc9e72b2383c63dc413dcb",
          "name": "Peixian Ma",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dcc",
          "user": {
            "_id": "6575a625b951d40e7a4d8685",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
            "isPro": false,
            "fullname": "zhuangxialie",
            "user": "ZhuangXialie",
            "type": "user"
          },
          "name": "Xialie Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:23.810Z",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dcd",
          "name": "Chengjin Xu",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dce",
          "name": "Xuhui Jiang",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dcf",
          "name": "Ran Chen",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dd0",
          "name": "Jian Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T15:01:30.000Z",
      "submittedOnDailyAt": "2025-04-14T04:07:17.501Z",
      "title": "SQL-R1: Entrenamiento de un modelo de inferencia de SQL a partir de lenguaje natural mediante aprendizaje reforzado",
      "submittedOnDailyBy": {
        "_id": "6575a625b951d40e7a4d8685",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
        "isPro": false,
        "fullname": "zhuangxialie",
        "user": "ZhuangXialie",
        "type": "user"
      },
      "summary": "La conversión de la naturaleza a SQL (NL2SQL) transforma consultas en naturaleza en sentencias de SQL estructuradas para facilitar interacción intuitiva con bases de datos. Ha habido un desarrollo reciente en la mejora de la interacción humano-computadora dentro de aplicaciones de bases de datos, pero sigue existiendo problemas significativos en la capacidad de inferencia para juntares múltiples de tablas y para las consultas ocultas en escenarios complejos. Actualmente, se utiliza principalmente ajustes micro-personalizados (SFT) para entrenar modelos NL2SQL, pero estos métodos tienen limitaciones en la adaptación y comprensión en nuevos entornos (como financieros o de salud). Para mejorar la eficiencia de inferencia de modelos NL2SQL en situaciones complejas, se introduce SQL-R1, un nuevo modelo de inferencia NL2SQL. Este modelo se entrenó utilizando algoritmos de aprendizaje por refuerzo (RL). Se diseñó una función de recompensa basada en aprendizaje por refuerzo específica para el trabajo NL2SQL y se analizó el impacto inicial de los efectos de la eficiencia del aprendizaje por refuerzo. Además, se logró lograr una precisión competitiva con un pequeño conjunto de datos sintéticos de NL2SQL y se exploraron nuevas formas de ingeniería de datos para mejorar esta precisión. En experimentos previos, SQL-R1 alcanzó una precisión del 88.6% en VIEWER y 66.6% en BIRD.",
      "upvotes": 6,
      "discussionId": "67fc9e73b2383c63dc413e19"
    },
    "publishedAt": "2025-04-11T11:01:30.000Z",
    "title": "SQL-R1: Training Natural Language to SQL Reasoning Model By\n  Reinforcement Learning",
    "summary": "Natural Language to SQL (NL2SQL) enables intuitive interactions with\ndatabases by transforming natural language queries into structured SQL\nstatements. Despite recent advancements in enhancing human-computer interaction\nwithin database applications, significant challenges persist, particularly\nregarding the inference performance in complex scenarios involving multi-table\njoins and nested queries. Current methodologies primarily utilize supervised\nfine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and\ninterpretability in new environments (e.g., finance and healthcare). In order\nto enhance the reasoning performance of the NL2SQL model in the above complex\nsituations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the\nreinforcement learning (RL) algorithms. We design a specialized RL-based reward\nfunction tailored for NL2SQL tasks and discussed the impact of cold start on\nthe effectiveness of intensive training. In addition, we achieve competitive\naccuracy using only a tiny amount of synthetic NL2SQL data for augmented\ntraining and further explore data engineering for RL. In existing experiments,\nSQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider\nand BIRD, respectively, only using the 7B base model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6575a625b951d40e7a4d8685",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
      "fullname": "zhuangxialie",
      "name": "ZhuangXialie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.07405",
      "authors": [
        {
          "_id": "67fa383909d06d0501a5e34e",
          "user": {
            "_id": "660d844462d63ad0009a9859",
            "avatarUrl": "/avatars/6822fc1a82c64dbfd40b88080a5fb1ae.svg",
            "isPro": false,
            "fullname": "Linyan Huang",
            "user": "DevLinyan",
            "type": "user"
          },
          "name": "Linyan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-13T19:24:47.680Z",
          "hidden": false
        },
        {
          "_id": "67fa383909d06d0501a5e34f",
          "name": "Haonan Lin",
          "hidden": false
        },
        {
          "_id": "67fa383909d06d0501a5e350",
          "name": "Yanning Zhou",
          "hidden": false
        },
        {
          "_id": "67fa383909d06d0501a5e351",
          "name": "Kaiwen Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T02:58:22.000Z",
      "submittedOnDailyAt": "2025-04-14T00:54:04.513Z",
      "title": "FlexIP: Control Dinámico de Procesión (Prescription) y Personalidad (Personality) para la Generación de Imágenes Personalizadas para el Usuario",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "El rápido desarrollo de modelos de generación 2D ha llevado a que el mantenimiento de la reconocimiento de temas y la posibilidad de editar de manera variada sean puntos de enfoque importantes de investigación. Los métodos actuales generalmente presentan un compromiso especial entre protección de la reconocimiento y la regulación de la representación de personas. Presentamos FlexIP, un nuevo marco de trabajo que incluye un Adapter de personalización para operaciones de estilo y un Adapter de preservación para mantener la reconocimiento. Este marco de trabajo inyecta una estructura de control directo en los modelos generativos y permite un control dinámico de los ajustes de pesos durante la inferencia, facilitando un control flexible de los parámetros. Los resultados de los experimentos muestran que nuestro enfoque supera los límites de los métodos de valoración y puede realizar una protección de la reconocimiento de alto nivel y apoyar la generación de representaciones de personas más diversas (página del proyecto: https://flexip-tech.github.io/flexip/).",
      "upvotes": 6,
      "discussionId": "67fa383c09d06d0501a5e3ef",
      "projectPage": "https://flexip-tech.github.io/flexip"
    },
    "publishedAt": "2025-04-09T22:58:22.000Z",
    "title": "FlexIP: Dynamic Control of Preservation and Personality for Customized\n  Image Generation",
    "summary": "With the rapid advancement of 2D generative models, preserving subject\nidentity while enabling diverse editing has emerged as a critical research\nfocus. Existing methods typically face inherent trade-offs between identity\npreservation and personalized manipulation. We introduce FlexIP, a novel\nframework that decouples these objectives through two dedicated components: a\nPersonalization Adapter for stylistic manipulation and a Preservation Adapter\nfor identity maintenance. By explicitly injecting both control mechanisms into\nthe generative model, our framework enables flexible parameterized control\nduring inference through dynamic tuning of the weight adapter. Experimental\nresults demonstrate that our approach breaks through the performance\nlimitations of conventional methods, achieving superior identity preservation\nwhile supporting more diverse personalized generation capabilities (Project\nPage: https://flexip-tech.github.io/flexip/).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07405.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08366",
      "authors": [
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3b",
          "name": "Sauradip Nag",
          "hidden": false
        },
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3c",
          "name": "Daniel Cohen-Or",
          "hidden": false
        },
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3d",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3e",
          "name": "Ali Mahdavi-Amiri",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T09:01:09.000Z",
      "submittedOnDailyAt": "2025-04-14T00:36:54.457Z",
      "title": "In-2-4D: 2 imágenes de punto de vista único a la generación indirecta en 4D",
      "submittedOnDailyBy": {
        "_id": "6399ab3296ce14c5dcf4ccbf",
        "avatarUrl": "/avatars/89aeedc96f73d76f7a6da96454f40fd2.svg",
        "isPro": false,
        "fullname": "Sauradip Nag",
        "user": "sauradip",
        "type": "user"
      },
      "summary": "Proponemos una nueva problemática llamada \"In-2-4D\" en relación con la generación de indirectividad 4D (es decir, 3D + movimiento) a partir de configuraciones de entrada mínimas: capturamos dos estados de movimiento diferentes de un objeto a partir de dos imágenes en solo un vistazo. Cuando las dos imágenes representan el inicio y el final del movimiento, nuestro objetivo es generar y reconstruir el movimiento en 4D. Utilizamos modelos de interfaz de video para predecir el movimiento, evitando así interpretaciones inciertas de movimiento entre grandes intervalos de frames. Para resolver esto, señalamos puntos clave visualmente cercanos al estado de entrada y buscamos movimientos notables para generar frames suaves entre ellos, utilizando un enfoque jerárquico. En cada frame, la representación 3D de los puntos clave se construye utilizando un suavizado gaussiano. La estructura temporal de los frames permite deformar el movimiento gaussianamente, mejorando la consistencia temporal y la precisión del movimiento 3D. Para mejorar esta consistencia y precisión, expandimos la atención de auto-difusión de múltiples vistas y aplicamos una normalización de deformaciones rígidas en cada etapa temporal. Finalmente, los segmentos de movimiento 3D generados independientemente se integran mediante un campo de deformación de bordes para que coincidan con la interfaz de video, asegurando un contexto suave y continuo. En experimentos detallados, tanto cualitativos como cuantitativos, y en un escenario de usuario, mostramos los efectos de nuestro método y sus componentes. El sitio web del proyecto está disponible en https://in-2-4d.github.io/.",
      "upvotes": 4,
      "discussionId": "67fc6d9374c3c0f0d6f24e12",
      "projectPage": "https://in-2-4d.github.io/",
      "githubRepo": "https://github.com/sauradip/In-2-4D"
    },
    "publishedAt": "2025-04-11T05:01:09.000Z",
    "title": "In-2-4D: Inbetweening from Two Single-View Images to 4D Generation",
    "summary": "We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion)\ninbetweening from a minimalistic input setting: two single-view images\ncapturing an object in two distinct motion states. Given two images\nrepresenting the start and end states of an object in motion, our goal is to\ngenerate and reconstruct the motion in 4D. We utilize a video interpolation\nmodel to predict the motion, but large frame-to-frame motions can lead to\nambiguous interpretations. To overcome this, we employ a hierarchical approach\nto identify keyframes that are visually close to the input states and show\nsignificant motion, then generate smooth fragments between them. For each\nfragment, we construct the 3D representation of the keyframe using Gaussian\nSplatting. The temporal frames within the fragment guide the motion, enabling\ntheir transformation into dynamic Gaussians through a deformation field. To\nimprove temporal consistency and refine 3D motion, we expand the self-attention\nof multi-view diffusion across timesteps and apply rigid transformation\nregularization. Finally, we merge the independently generated 3D motion\nsegments by interpolating boundary deformation fields and optimizing them to\nalign with the guiding video, ensuring smooth and flicker-free transitions.\nThrough extensive qualitative and quantitiave experiments as well as a user\nstudy, we show the effectiveness of our method and its components. The project\npage is available at https://in-2-4d.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08366.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6399ab3296ce14c5dcf4ccbf",
      "avatarUrl": "/avatars/89aeedc96f73d76f7a6da96454f40fd2.svg",
      "fullname": "Sauradip Nag",
      "name": "sauradip",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08716",
      "authors": [
        {
          "_id": "67fca0ca05cd5b5035123b7e",
          "name": "Wissam Antoun",
          "hidden": false
        },
        {
          "_id": "67fca0ca05cd5b5035123b7f",
          "name": "Benoît Sagot",
          "hidden": false
        },
        {
          "_id": "67fca0ca05cd5b5035123b80",
          "name": "Djamé Seddah",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T17:29:35.000Z",
      "submittedOnDailyAt": "2025-04-14T04:20:01.034Z",
      "title": "ModernBERT y DeBERTaV3? Análisis de la influencia de la arquitectura y el datos en el rendimiento de los modelos encoder de Transformer.",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "En el modelo de predicción, se han introducido desarrollos estructurales con el objetivo de mejorar la eficiencia y la performance, como el deBERTaV3 y el modernBERT, entre otros modelos de codificadores pre-entrenados de tensores. Los autores del modernBERT han reportado mejoras en el rendimiento en mayoría de los benchmarks frente al deBERTaV3, pero, por falta de comparación con datos de entrenamiento no publicados y conjuntos de datos compartidos, es difícil determinar si estas mejoras son debidas a mejoras estructurales o a diferencias en los datos de entrenamiento. En este estudio, se utilizaron conjuntos de datos como el de modernBERT para entrenar el modernBERT y se realizó una investigación controlada para separar el impacto de la diseño del modelo. Finalmente, la generación de los modelos anteriores supera en efectos de muestra y en el rendimiento general de los benchmarks, pero el principal ventaja del modernBERT es su velocidad de entrenamiento y inferencia. Sin embargo, el nuevo modelo propuesto ofrece mejoras estructurales significativas en comparación con BERT y RoBERTa. Además, los datos de entrenamiento de alta calidad aceleran la convergencia, pero no tienen un impacto significativo en el rendimiento final. Esto muestra la posibilidad de sesgos en los benchmarks. Estos hallazgos demuestran la importancia de distinguir entre datos de entrenamiento y innovaciones estructurales en la evaluación de modelos de entrenamiento previo.",
      "upvotes": 3,
      "discussionId": "67fca0ca05cd5b5035123ba6"
    },
    "publishedAt": "2025-04-11T13:29:35.000Z",
    "title": "ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\n  Transformer Encoder Models Performance",
    "summary": "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce\narchitectural advancements aimed at improving efficiency and performance.\nAlthough the authors of ModernBERT report improved performance over DeBERTaV3\non several benchmarks, the lack of disclosed training data and the absence of\ncomparisons using a shared dataset make it difficult to determine whether these\ngains are due to architectural improvements or differences in training data. In\nthis work, we conduct a controlled study by pretraining ModernBERT on the same\ndataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of\nmodel design. Our results show that the previous model generation remains\nsuperior in sample efficiency and overall benchmark performance, with\nModernBERT's primary advantage being faster training and inference speed.\nHowever, the new proposed model still provides meaningful architectural\nimprovements compared to earlier models such as BERT and RoBERTa. Additionally,\nwe observe that high-quality pre-training data accelerates convergence but does\nnot significantly improve final performance, suggesting potential benchmark\nsaturation. These findings show the importance of disentangling pretraining\ndata from architectural innovations when evaluating transformer models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08716.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2491
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08192",
      "authors": [
        {
          "_id": "67fcb3584a92187863e732d5",
          "name": "Aashiq Muhamed",
          "hidden": false
        },
        {
          "_id": "67fcb3584a92187863e732d6",
          "name": "Jacopo Bonato",
          "hidden": false
        },
        {
          "_id": "67fcb3584a92187863e732d7",
          "name": "Mona Diab",
          "hidden": false
        },
        {
          "_id": "67fcb3584a92187863e732d8",
          "name": "Virginia Smith",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T01:24:03.000Z",
      "submittedOnDailyAt": "2025-04-14T05:34:15.388Z",
      "title": "SAEs Pueden Mejorar la Desaprendizaje: Codificador Autónomo Dinámico\nLínea de Protección para el Aprendizaje de Precisión en LLM",
      "submittedOnDailyBy": {
        "_id": "64755a83e0b188d3cb2579d8",
        "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
        "isPro": false,
        "fullname": "Aashiq Muhamed",
        "user": "aashiqmuhamed",
        "type": "user"
      },
      "summary": "La red neuronal de unidades es un potencial método para mejorar la seguridad de los modelos LLM al eliminar conocimientos insatisfactorios. Sin embargo, los métodos de unidades basados en gradiente que se utilizan generalmente enfrentan problemas como alto costo computacional, inestabilidad de parámetros, disminución de la capacidad de unidades en secuencia, vulnerabilidad a ataques de re-entrenamiento, baja efectividad de datos y falta de interpretabilidad. El desactivador de activaciones (sparsity out-weights) permite unidades basadas en activaciones específicas y mejora estos aspectos, pero los métodos existentes son menos efectivos que los basados en gradiente. En este artículo, se contrarrestan los resultados previos y muestra que el SAE mejora significativamente el unidades cuando se utiliza dinámicamente. Se presenta el Dynamic Denoising Autoencoder Guardrails (DSG). El DSG es un nuevo método para el unidades de precisión utilizando selección de características básicas y un clasificador dinámico de características. Según los resultados de las pruebas, el DSG supera significativamente los principales métodos de unidades y equilibra bien entre olvidar y ayudar. El DSG resuelve los principales desafíos de los métodos basados en gradiente, ofrece un alto rendimiento de unidades en secuencia, resistencia a ataques de re-entrenamiento, mejora de la efectividad de los datos (incluyendo conjuntos de 0 shot), y proporciona interpretabilidad en el unidades.",
      "upvotes": 2,
      "discussionId": "67fcb3594a92187863e732fa"
    },
    "publishedAt": "2025-04-10T21:24:03.000Z",
    "title": "SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder\n  Guardrails for Precision Unlearning in LLMs",
    "summary": "Machine unlearning is a promising approach to improve LLM safety by removing\nunwanted knowledge from the model. However, prevailing gradient-based\nunlearning methods suffer from issues such as high computational costs,\nhyperparameter instability, poor sequential unlearning capability,\nvulnerability to relearning attacks, low data efficiency, and lack of\ninterpretability. While Sparse Autoencoders are well-suited to improve these\naspects by enabling targeted activation-based unlearning, prior approaches\nunderperform gradient-based methods. This work demonstrates that, contrary to\nthese earlier findings, SAEs can significantly improve unlearning when employed\ndynamically. We introduce Dynamic DAE Guardrails (DSG), a novel\nmethod for precision unlearning that leverages principled feature selection and\na dynamic classifier. Our experiments show DSG substantially outperforms\nleading unlearning methods, achieving superior forget-utility trade-offs. DSG\naddresses key drawbacks of gradient-based approaches for unlearning -- offering\nenhanced computational efficiency and stability, robust performance in\nsequential unlearning, stronger resistance to relearning attacks, better data\nefficiency including zero-shot settings, and more interpretable unlearning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08192.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64755a83e0b188d3cb2579d8",
      "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
      "fullname": "Aashiq Muhamed",
      "name": "aashiqmuhamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01883",
      "authors": [
        {
          "_id": "67fcb50ea69150c25fb4b645",
          "name": "Aashiq Muhamed",
          "hidden": false
        },
        {
          "_id": "67fcb50ea69150c25fb4b646",
          "name": "Mona Diab",
          "hidden": false
        },
        {
          "_id": "67fcb50ea69150c25fb4b647",
          "name": "Virginia Smith",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T16:40:43.000Z",
      "submittedOnDailyAt": "2025-04-14T05:41:34.796Z",
      "title": "CoRAG: Generación de Datos de Signatura de Colaboración",
      "submittedOnDailyBy": {
        "_id": "64755a83e0b188d3cb2579d8",
        "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
        "isPro": false,
        "fullname": "Aashiq Muhamed",
        "user": "aashiqmuhamed",
        "type": "user"
      },
      "summary": "El modelo de Revisión y Aumento de la Gente (RAG) es un modelo especializado en tareas de concentración de conocimientos y muestra un excelente rendimiento incluso bajo las restricciones de la entrenamiento de imágenes de características. Presentamos CoRAG. CoRAG es un marco de trabajo que extiende el RAG, permitiendo a los usuarios compartir un almacenamiento de pasos para entrenar un modelo en conjunto. En cuanto a la evaluación de CoRAG, presentamos CRAB. CRAB es un marco de evaluación para respuestas a preguntas de dominio abierto en la identidad de la aprendizaje compartido. Nuestros experimentos muestran que CoRAG puede aprender compartido con un escalador de recursos bajo, alineando los modelos RAG entrenados localmente y obteniendo un excelente rendimiento. El análisis realizado revela la importancia de los pasos compartidos relevantes dentro del almacenamiento compartido, la sorprendente beneficio de los pasos irrelevantes, y la posibilidad de que los pasos negativos afecten negativamente el rendimiento. Esto introduce nuevas consideraciones en el RAG compartido: el uso de un conocimiento enriquecido compartido y el riesgo de comportamientos nocivos derivados de pasos nocivos de otros usuarios. Nuestros hallazgos subrayan la posibilidad de CoRAG y identifican las principales cuestiones de diseño y las posibles rutas de investigación futura.",
      "upvotes": 2,
      "discussionId": "67fcb510a69150c25fb4b6b1"
    },
    "publishedAt": "2025-04-02T12:40:43.000Z",
    "title": "CoRAG: Collaborative Retrieval-Augmented Generation",
    "summary": "Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive\ntasks, especially under few-shot learning constraints. We introduce CoRAG, a\nframework extending RAG to collaborative settings, where clients jointly train\na shared model using a collaborative passage store. To evaluate CoRAG, we\nintroduce CRAB, a benchmark for collaborative homogeneous open-domain question\nanswering. Our experiments demonstrate that CoRAG consistently outperforms both\nparametric collaborative learning methods and locally trained RAG models in\nlow-resource scenarios. Further analysis reveals the critical importance of\nrelevant passages within the shared store, the surprising benefits of\nincorporating irrelevant passages, and the potential for hard negatives to\nnegatively impact performance. This introduces a novel consideration in\ncollaborative RAG: the trade-off between leveraging a collectively enriched\nknowledge base and the potential risk of incorporating detrimental passages\nfrom other clients. Our findings underscore the viability of CoRAG, while also\nhighlighting key design challenges and promising avenues for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01883.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64755a83e0b188d3cb2579d8",
      "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
      "fullname": "Aashiq Muhamed",
      "name": "aashiqmuhamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.05303",
      "authors": [
        {
          "_id": "67fcbbe8daf0cf6803943949",
          "user": {
            "_id": "6492bf9681d93008eb33f167",
            "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
            "isPro": false,
            "fullname": "Sai Kumar Dwivedi",
            "user": "saidwivedi",
            "type": "user"
          },
          "name": "Sai Kumar Dwivedi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:12.532Z",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394a",
          "name": "Dimitrije Antić",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394b",
          "name": "Shashank Tripathi",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394c",
          "name": "Omid Taheri",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394d",
          "name": "Cordelia Schmid",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394e",
          "name": "Michael J. Black",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394f",
          "name": "Dimitrios Tzionas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:59:33.000Z",
      "submittedOnDailyAt": "2025-04-14T06:14:23.936Z",
      "title": "InteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2D base models\n\nInteractVLM: 3D interaction inference from 2",
      "submittedOnDailyBy": {
        "_id": "6492bf9681d93008eb33f167",
        "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
        "isPro": false,
        "fullname": "Sai Kumar Dwivedi",
        "user": "saidwivedi",
        "type": "user"
      },
      "summary": "InteractVLM es un nuevo método para inferir los puntos de contacto 3D de seres humanos o objetos desde una sola imagen de entorno natural. Esto permite reconstruir exactamente la estructura común 3D de seres humanos y objetos. Sin embargo, se enfrentan desafíos complejos debido a obstrucciones, incertidumbre de profundidad y cambios extremos en la forma de los objetos. Los métodos actuales obtienen anóticaciones 3D de contacto o etiquetado de movimientos de manos largos y costosos a partir de sistemas de detección de movimiento 3D. Esto limita la escalabilidad y la capacidad de generalización. Para resolver estos problemas, InteractVLM utiliza el conocimiento visual de modelos de lenguaje visual (VLMs) amplios y fine-tunea con datos limitados de contacto 3D. Sin embargo, aplicar directamente estos modelos es complejo, ya que implica realizar lógica en 2D. Por lo tanto, InteractVLM introduce un nuevo módulo Render-Localize-Lift. Este módulo (1) inserta las superficies del cuerpo y de los objetos en el espacio 2D a través de varias imágenes, (2) entrena un nuevo modelo para estimar nuevas posiciones de imágenes en 2D para inferir el contacto, y (3) los realiza en 3D. Además, InteractVLM propone un nuevo desafío de estimación de contacto humano semántico. Esto permite predecir el contacto humano basado en la semántica de los objetos y modelar una interacción rica. InteractVLM supera los métodos actuales en la inferencia de contacto y la reconstrucción 3D, y apoya la reconstrucción 3D a partir de imágenes de entorno natural. El código y los modelos están disponibles en https://interactvlm.is.tue.mpg.de.",
      "upvotes": 0,
      "discussionId": "67fcbbeadaf0cf68039439b9",
      "projectPage": "https://interactvlm.is.tue.mpg.de/",
      "githubRepo": "https://github.com/saidwivedi/InteractVLM"
    },
    "publishedAt": "2025-04-07T13:59:33.000Z",
    "title": "InteractVLM: 3D Interaction Reasoning from 2D Foundational Models",
    "summary": "We introduce InteractVLM, a novel method to estimate 3D contact points on\nhuman bodies and objects from single in-the-wild images, enabling accurate\nhuman-object joint reconstruction in 3D. This is challenging due to occlusions,\ndepth ambiguities, and widely varying object shapes. Existing methods rely on\n3D contact annotations collected via expensive motion-capture systems or\ntedious manual labeling, limiting scalability and generalization. To overcome\nthis, InteractVLM harnesses the broad visual knowledge of large Vision-Language\nModels (VLMs), fine-tuned with limited 3D contact data. However, directly\napplying these models is non-trivial, as they reason only in 2D, while\nhuman-object contact is inherently 3D. Thus we introduce a novel\nRender-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D\nspace via multi-view rendering, (2) trains a novel multi-view localization\nmodel (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D.\nAdditionally, we propose a new task called Semantic Human Contact estimation,\nwhere human contact predictions are conditioned explicitly on object semantics,\nenabling richer interaction modeling. InteractVLM outperforms existing work on\ncontact estimation and also facilitates 3D reconstruction from an in-the wild\nimage. Code and models are available at https://interactvlm.is.tue.mpg.de.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05303.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6492bf9681d93008eb33f167",
      "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
      "fullname": "Sai Kumar Dwivedi",
      "name": "saidwivedi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05262",
      "authors": [
        {
          "_id": "67fcc9a980568c7ef6180dcb",
          "name": "Yang Yan",
          "hidden": false
        },
        {
          "_id": "67fcc9a980568c7ef6180dcc",
          "name": "Yu Lu",
          "hidden": false
        },
        {
          "_id": "67fcc9a980568c7ef6180dcd",
          "name": "Renjun Xu",
          "hidden": false
        },
        {
          "_id": "67fcc9a980568c7ef6180dce",
          "name": "Zhenzhong Lan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T16:57:10.000Z",
      "submittedOnDailyAt": "2025-04-14T07:11:56.706Z",
      "title": "Los modelos de lenguaje de nivel de Dipendencia en LLMs no realmente entienden la suma básica. Comparación de aprendizaje de reglas y memorización.",
      "submittedOnDailyBy": {
        "_id": "62ce6dd785cfd21c04c7e6f5",
        "avatarUrl": "/avatars/89837a5dea6e2d753d59caad142bed4a.svg",
        "isPro": false,
        "fullname": "ZhenzhongLan",
        "user": "DannyLan",
        "type": "user"
      },
      "summary": "Aunque los puntajes del marco de referencia son altos, los modelos de lenguaje de gran escala (LLMs) fallan en problemas sencillos y se plantean preguntas importantes: ¿aprenden los principios de la matemática los LLMs o solo dependen de la memoria de patrones? En lugar de diseñar marcos de referencia más complejos que los últimos estudios, se investigan actualmente dos características fundamentales utilizando la suma de dos números enteros (de 0 a 2^64): la intercambiabilidad (A+B=B+A) y la generalización estructural (usando mapeos de fórmulas, por ejemplo, 7→y). Los mejores LLMs actuales alcanzan una precisión del 73.8-99.8% en la suma de números, pero su rendimiento en mapeos de fórmulas cae a 7.5% o menos, fallando en la generalización de las reglas aprendidas. Además, se observa un escalado no lineal del rendimiento con el número de números y la destrucción frecuente de la intercambiabilidad (1,700 o más casos de A+B≠B+A). La provisión clara de la regla de suma causa un descenso promedio del 81.2% en el rendimiento, mientras que las explicaciones automáticas mantienen la precisión del modelo, mostrando que el procesamiento de cálculos de los LLMs no se alinea con las reglas humanas. Nuestro hallazgo es que los LLMs actuales dependen de patrones de memoria para aprender realmente las reglas, revelando limitaciones estructurales y la necesidad de nuevas aproximaciones.",
      "upvotes": 0,
      "discussionId": "67fcc9aa80568c7ef6180e24"
    },
    "publishedAt": "2025-04-07T12:57:10.000Z",
    "title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\n  vs. Memorization in Large Language Models",
    "summary": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n(0 to 2^{64}), probing two core properties: commutativity (A+B=B+A) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., 7\nrightarrow y). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to leq7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of A+B neq B+A) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05262.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ce6dd785cfd21c04c7e6f5",
      "avatarUrl": "/avatars/89837a5dea6e2d753d59caad142bed4a.svg",
      "fullname": "ZhenzhongLan",
      "name": "DannyLan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]