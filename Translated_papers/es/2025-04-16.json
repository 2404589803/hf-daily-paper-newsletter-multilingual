[
  {
    "paper": {
      "id": "2504.08672",
      "authors": [
        {
          "_id": "67fcb7294a92187863e805ee",
          "user": {
            "_id": "64e6cf78ecce34cb442dc889",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
            "isPro": false,
            "fullname": "Fangzhi Xu",
            "user": "xufangzhi",
            "type": "user"
          },
          "name": "Fangzhi Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:16.537Z",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805ef",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f0",
          "name": "Chang Ma",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f1",
          "name": "Haiteng Zhao",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f2",
          "user": {
            "_id": "6064a0eeb1703ddba0d458b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
            "isPro": false,
            "fullname": "Qiushi",
            "user": "QiushiSun",
            "type": "user"
          },
          "name": "Qiushi Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:24:08.107Z",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f3",
          "name": "Kanzhi Cheng",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f4",
          "name": "Junxian He",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f5",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f6",
          "name": "Zhiyong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T16:26:23.000Z",
      "submittedOnDailyAt": "2025-04-16T05:46:28.754Z",
      "title": "Genio: Generalizado y un marco de aprendizaje automático completo sin maquinaria. Además, está adaptado a razones avanzadas.",
      "submittedOnDailyBy": {
        "_id": "64e6cf78ecce34cb442dc889",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
        "isPro": false,
        "fullname": "Fangzhi Xu",
        "user": "xufangzhi",
        "type": "user"
      },
      "summary": "El desarrollo de la teoría de los LLM ha despertado un gran interés. Sin embargo, la tecnología actual de entrenamiento posterior es limitada en términos de resultados subóptimos.",
      "upvotes": 35,
      "discussionId": "67fcb72a4a92187863e8061b",
      "projectPage": "https://github.com/xufangzhi/Genius",
      "githubRepo": "https://github.com/xufangzhi/Genius",
      "ai_keywords": [
        "self-training framework",
        "Genius",
        "stepwise foresight re-sampling strategy",
        "advantage-calibrated optimization (ACO) loss function"
      ]
    },
    "publishedAt": "2025-04-11T12:26:23.000Z",
    "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
    "summary": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6cf78ecce34cb442dc889",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
      "fullname": "Fangzhi Xu",
      "name": "xufangzhi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10481",
      "authors": [
        {
          "_id": "67fdc1b41d1bc292f7b9358e",
          "user": {
            "_id": "64e18e9ec20c27fcc8df384e",
            "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
            "isPro": false,
            "fullname": "Ding Chen",
            "user": "Hush-cd",
            "type": "user"
          },
          "name": "Ding Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:22.449Z",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b9358f",
          "user": {
            "_id": "6455ff584095c967f9a847bb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6455ff584095c967f9a847bb/A5wjtWsudC73fLVmgASBr.jpeg",
            "isPro": false,
            "fullname": "Qingchen Yu",
            "user": "Duguce",
            "type": "user"
          },
          "name": "Qingchen Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:45.275Z",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93590",
          "name": "Pengyuan Wang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93591",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93592",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93593",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93594",
          "name": "Xinchi Li",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93595",
          "name": "Minchuan Yang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93596",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:59:36.000Z",
      "submittedOnDailyAt": "2025-04-16T00:53:50.942Z",
      "title": "xVerify: Herramienta de Verificación de Respuestas para Evaluar Modelos de Razonamiento Eficiente",
      "submittedOnDailyBy": {
        "_id": "64e18e9ec20c27fcc8df384e",
        "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
        "isPro": false,
        "fullname": "Ding Chen",
        "user": "Hush-cd",
        "type": "user"
      },
      "summary": "Con la actualización del modelo o1, se han aparecido modelos de inferencia que aplican pasos de tiempo y generan respuestas que incluyen complejas inferencias, pasos intermedios y autopercepción. Estos modelos son insuficientemente evaluados por métodos existentes debido a que no pueden determinar si la salida del modelo es exactamente la respuesta correcta, ni extraer la respuesta final de respuestas largas y complejas. Para resolver estos problemas, se propone una función de verificación de respuestas eficiente para la evaluación de modelos de inferencia llamada xVerify. xVerify muestra una capacidad fuerte en la evaluación de igualdad y está diseñado para efectivamente determinar si las respuestas generadas por modelos de inferencia son iguales a la respuesta correcta para diversas preguntas. Para la entrenamiento y evaluación de xVerify, se utilizaron pares de preguntas y respuestas generados en diferentes datasets, así como un conjunto de datos específico para la evaluación de modelos de inferencia llamado VAR. Se garantizaron la precisión de los etiquetadores mediante diversos procesos de notas de reuniones. Se entrenaron modelos de xVerify de diferentes tamaños basándose en el conjunto de datos VAR. En experimentos de evaluación con conjuntos de prueba y de generalización, todos los modelos de xVerify superaron el F1 score y la precisión del 95%. En particular, la versión de tamaño mínimo, xVerify-0.5B-I, superó a todos los métodos de evaluación, excepto GPT-4o, mientras que xVerify-3B-Ib superó a GPT-4o en el rendimiento general. Estos resultados demuestran la eficacia y capacidad de generalización de xVerify.",
      "upvotes": 28,
      "discussionId": "67fdc1b51d1bc292f7b935e8",
      "githubRepo": "https://github.com/IAAR-Shanghai/xVerify",
      "ai_keywords": [
        "reasoning models",
        "o1 model",
        "slow thinking strategies",
        "complex reasoning",
        "intermediate steps",
        "self-reflection",
        "evaluation methods",
        "LLM output",
        "reference answer",
        "final answer",
        "xVerify",
        "equivalence judgment",
        "VAR dataset",
        "multi-round annotation process",
        "F1 scores",
        "xVerify-0.5B-I",
        "xVerify-3B-Ib",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-04-14T13:59:36.000Z",
    "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
    "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10481.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e18e9ec20c27fcc8df384e",
      "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
      "fullname": "Ding Chen",
      "name": "Hush-cd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10465",
      "authors": [
        {
          "_id": "67ff26c3414c03ebc1d42529",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252a",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:53.598Z",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252b",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252c",
          "name": "Yanwei Li",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252d",
          "name": "Weixian Lei",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252e",
          "name": "Xueqing Deng",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252f",
          "name": "Shihao Chen",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d42530",
          "name": "Shunping Ji",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d42531",
          "name": "Jiashi Feng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:52:22.000Z",
      "submittedOnDailyAt": "2025-04-16T02:11:29.898Z",
      "title": "Pixel-SAIL: Un Transformer único para lograr la comprensión basada en píxeles",
      "submittedOnDailyBy": {
        "_id": "63958b4414513eaf9029ebf1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
        "isPro": false,
        "fullname": "Xiangtai Li",
        "user": "LXT",
        "type": "user"
      },
      "summary": "Los modelos de lenguaje multimodal de alta dimensión (MLLMs) alcanzan un desempeño sorprendente en tareas de comprensión a nivel de píxeles. Sin embargo, todos los estudios dependen de funciones adicionales como el encoder visual CLIP y el segmentación espléndido, lo que limita la expansión del modelo. En este trabajo, se busca explorar un MLLM simplificado en altas dimensiones sin introducir nuevas funciones. Este trabajo se basa en el diseño reciente de un único transformer como modelo de visión-lenguaje unificado (SAIL). En esta investigación, se utilizan transformers para entrenar tokens visuales y textos juntos. Pixel-SAIL presenta un único transformer para tareas de MLLM a nivel de píxeles. Específicamente, se realizan tres mejoras técnicas en el baseline básico. Primero, se diseña un módulo de upsampling entrenable para refinar las características de los tokens visuales. Luego, se propone una estrategia de injección de pronósticos visuales para que el transformer único pueda comprender entradas de pronósticos visuales y obtener beneficios de la fusión inicial de los embeddings de pronósticos visuales y tokens visuales. Finalmente, se introduce una estrategia de desarrollo experiencial para mejorar la capacidad de extracción de características precisas del segmentador visual. Además, se presenta el benchmark detallado de comprensión de píxeles (PerBench) colectado con el método de mano-a-mano. Este benchmark incluye tres tareas: explicación de objetos detallados, respuesta a preguntas basadas en pronósticos visuales y segmentación de referencias de texto visual. Se realizan experimentos ampliados para los cuatro referencias de segmentación, un benchmark de pronósticos visuales y nuestro PerBench, mostrando que nuestro Pixel-SAIL obtiene resultados mejores o menos mal comparado con un sistema más simple. Los códigos y modelos están disponibles en https://github.com/magic-research/Sa2VA.",
      "upvotes": 21,
      "discussionId": "67ff26c6414c03ebc1d425de",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "pixel-level understanding",
        "vision encoder (CLIP)",
        "segmentation experts",
        "single transformer as a unified vision-language model (SAIL)",
        "pixel-wise MLLM tasks",
        "learnable upsampling module",
        "visual prompt injection",
        "visual prompt embeddings",
        "vision expert distillation",
        "pixel understanding benchmark (PerBench)",
        "detailed object description",
        "visual prompt-based question answering",
        "visual-text referring segmentation",
        "referring segmentation benchmarks",
        "visual prompt benchmark"
      ]
    },
    "publishedAt": "2025-04-14T13:52:22.000Z",
    "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding",
    "summary": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for\nfine-grained pixel-level understanding tasks. However, all the works rely\nheavily on extra components, such as vision encoder (CLIP), segmentation\nexperts, leading to high system complexity and limiting model scaling. In this\nwork, our goal is to explore a highly simplified MLLM without introducing extra\ncomponents. Our work is motivated by the recent works on Single trAnsformer as\na unified vIsion-Language Model (SAIL) design, where these works jointly learn\nvision tokens and text tokens in transformers. We present Pixel-SAIL, a single\ntransformer for pixel-wise MLLM tasks. In particular, we present three\ntechnical improvements on the plain baseline. First, we design a learnable\nupsampling module to refine visual token features. Secondly, we propose a novel\nvisual prompt injection strategy to enable the single transformer to understand\nvisual prompt inputs and benefit from the early fusion of visual prompt\nembeddings and vision tokens. Thirdly, we introduce a vision expert\ndistillation strategy to efficiently enhance the single transformer's\nfine-grained feature extraction capability. In addition, we have collected a\ncomprehensive pixel understanding benchmark (PerBench), using a manual check.\nIt includes three tasks: detailed object description, visual prompt-based\nquestion answering, and visual-text referring segmentation. Extensive\nexperiments on four referring segmentation benchmarks, one visual prompt\nbenchmark, and our PerBench show that our Pixel-SAIL achieves comparable or\neven better results with a much simpler pipeline. Code and model will be\nreleased at https://github.com/magic-research/Sa2VA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10465.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63958b4414513eaf9029ebf1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
      "fullname": "Xiangtai Li",
      "name": "LXT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10337",
      "authors": [
        {
          "_id": "67fddae99a03686367721718",
          "user": {
            "_id": "6471a24381ded91f253ceb1c",
            "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
            "isPro": false,
            "fullname": "Wesley Shi",
            "user": "WesleyShi",
            "type": "user"
          },
          "name": "Wenlei Shi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-15T06:43:40.277Z",
          "hidden": false
        },
        {
          "_id": "67fddae99a03686367721719",
          "name": "Xing Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T15:46:33.000Z",
      "submittedOnDailyAt": "2025-04-16T00:52:23.733Z",
      "title": "Haymider: Prueba de escalado para la validación de la reconocimiento de la generación",
      "submittedOnDailyBy": {
        "_id": "6471a24381ded91f253ceb1c",
        "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
        "isPro": false,
        "fullname": "Wesley Shi",
        "user": "WesleyShi",
        "type": "user"
      },
      "summary": "El sistema AI es capaz de generar y mantener conocimiento, así como de verificar dicho conocimiento. Las recientes investigaciones sobre la razonamiento de cadena de pensamiento (Chain-of-Thought) han demostrado que los modelos de lenguaje grandes (LLM) tienen un gran potencial para resolver problemas competitivos, pero su capacidad de verificación es débil y no ha sido suficientemente explorada. En este artículo, proponemos Heimdall, un modelo de LLM que realiza la verificación de la razonamiento de cadena de pensamiento. Heimdall puede evaluar precisamente la exactitud de las soluciones. Utilizando aprendizaje por refuerzo como enfoque principal, Hemdall mejoró la precisión de la verificación en problemas matemáticos competitivos, aumentando la precisión del 62.5% a 94.5%. Al aplicar técnicas de re-sampling para escalabilidad, la precisión alcanzó un 97.5%. De acuerdo con la evaluación humana, Heimdall muestra una impresionante capacidad de generalización y detecta correctamente muchos problemas de demostraciones matemáticas difíciles que no fueron incluidos durante el entrenamiento. Además, proponemos la verificación pesimista (Pessimistic Verification), que expande las funciones de Heimdall para la escalabilidad de la resolución de problemas. La verificación pesimista evalúa soluciones proporcionadas por un modelo de solución y selecciona aquellas con la menor confianza. Cuando DeepSeek-R1-Distill-Qwen-32B se utilizó como modelo de solución, la verificación pesimista mejoró la precisión de la resolución en AIME2025 del 54.2% a 70.0%, y con 16 veces más bucles de cálculo, alcanzó un 83.3%. Con el uso de un modelo de solución potente como 2.5 Pro, la precisión alcanzó un 93.0%. Finalmente, construimos un prototipo de un sistema automático de descubrimiento de conocimiento. En este sistema, el primer fuente propone el problema, el segundo proporciona la solución y el tercero la verifica. Utilizamos el primer componente de la herramienta de trabajo de synthesis de datos de NuminaMath y Heimdall detecta eficazmente los problemas dentro del conjunto de datos, revelando que casi la mitad de los datos necesitan corrección. Esto refleja profundamente las recientes investigaciones de reducción de NuminaMath.",
      "upvotes": 21,
      "discussionId": "67fddaea9a03686367721776",
      "ai_keywords": [
        "Chain-of-Thought reasoning",
        "LLMs (Large Language Models)",
        "Heimdall",
        "long CoT verification",
        "pure reinforcement learning",
        "synthetic math problems",
        "human evaluation",
        "generalization capabilities",
        "Pessimistic Verification",
        "DeepSeek-R1-Distill-Qwen-32B",
        "AIME2025",
        "Gemini 2.5 Pro",
        "solution accuracy",
        "automatic knowledge discovery system",
        "ternary system",
        "NuminaMath",
        "data synthesis",
        "data records",
        "flawed data"
      ]
    },
    "publishedAt": "2025-04-14T11:46:33.000Z",
    "title": "Heimdall: test-time scaling on the generative verification",
    "summary": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10337.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6471a24381ded91f253ceb1c",
      "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
      "fullname": "Wesley Shi",
      "name": "WesleyShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11346",
      "authors": [
        {
          "_id": "67ff18961dc5d56fdd6ca724",
          "name": "Yu Gao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca725",
          "name": "Lixue Gong",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca726",
          "name": "Qiushan Guo",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca727",
          "name": "Xiaoxia Hou",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca728",
          "name": "Zhichao Lai",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca729",
          "name": "Fanshi Li",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72a",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72b",
          "name": "Xiaochen Lian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72c",
          "name": "Chao Liao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72d",
          "name": "Liyang Liu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72e",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72f",
          "name": "Yichun Shi",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca730",
          "name": "Shiqi Sun",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca731",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca732",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca733",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca734",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca735",
          "name": "Xuanda Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca736",
          "name": "Xun Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca737",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca738",
          "name": "Guofeng Wu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca739",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73a",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73b",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73c",
          "name": "Zhonghua Zhai",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73d",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73e",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73f",
          "name": "Yuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca740",
          "name": "Shijia Zhao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca741",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca742",
          "name": "Weilin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:19:07.000Z",
      "submittedOnDailyAt": "2025-04-16T01:10:39.295Z",
      "title": "Certainly! Here is the translation of the provided text into Spanish:\n\n**Reporte Técnico de la Tecnología Seedream 3.0**\n\n(Note: The original text \"Seedream 3.0 技术报告\" has been translated to \"Reporte Técnico de la Tecnología Seedream 3.0\" to maintain the professional and accurate tone requested.)",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Zongshimeng 3.0 es un modelo basado en la generación de imágenes bilingüe de alto rendimiento en chino y inglés. Para resolver los problemas existentes en Zongshimeng 2.0, el proyecto realizó una serie de mejoras técnicas en todo el proceso, desde la construcción de datos hasta la implementación del modelo. En particular, se abordaron problemas como la respuesta a patrones complejos, la generación de tipografías detalladas, la inadecuación de la calidad de la imagen y las limitaciones de resolución de la imagen. El desarrollo de Zongshimeng 3.0 se basa en la mejora de todo el proceso desde la construcción de datos hasta la implementación del modelo. En la capa de datos, se utilizaron paradigmas de entrenamiento para defectos y un marco de muestreo de datos colaborativo de doble eje para duplicar el conjunto de datos. Además, durante el entrenamiento previo, se emplearon métodos efectivos como entrenamiento mixto de lenguajes, RoPE de modalidades cruzadas, pérdida de correspondencia de representaciones y muestreo de pasos de tiempo para el rango. En la etapa de aprendizaje profundo, se utilizó un SFT con comentarios artísticos diversos y un modelo de recompensa basado en VLM extendido para generar salidas que se ajusten a las preferencias humanas. Además, Zongshimeng 3.0 desarrolló un nuevo patrón de aceleración. Usando un valor esperado de ruido consistente y un muestreo de pasos de tiempo para la importancia, se logró mantener la calidad de la imagen mientras aumentó la velocidad en 4 o 8 veces. Zongshimeng 3.0 muestra una mejora clara frente a Zongshimeng 2.0, especialmente en la mejora de la capacidad de renderización de textos chinos complejos, lo cual es crucial para la generación de tipografía. Además, proporciona una salida de alta resolución, permitiendo la generación de imágenes de alta calidad.",
      "upvotes": 20,
      "discussionId": "67ff189c1dc5d56fdd6ca8e0",
      "projectPage": "https://team.doubao.com/zh/tech/seedream3_0",
      "ai_keywords": [
        "mixed-resolution training",
        "cross-modality RoPE",
        "representation alignment loss",
        "resolution-aware timestep sampling",
        "SFT (Supervised Fine-Tuning)",
        "VLM (Vision Language Model)",
        "consistent noise expectation",
        "importance-aware timestep sampling"
      ]
    },
    "publishedAt": "2025-04-15T12:19:07.000Z",
    "title": "Seedream 3.0 Technical Report",
    "summary": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11346.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 47
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11442",
      "authors": [
        {
          "_id": "67ff1387e1bfbb6bdd79ab72",
          "user": {
            "_id": "628b671f8fb67b90658613f7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653303027789-noauth.jpeg",
            "isPro": false,
            "fullname": "Leon Guertler",
            "user": "LeonGuertler",
            "type": "user"
          },
          "name": "Leon Guertler",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:02.481Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab73",
          "user": {
            "_id": "653879fbf5f5016df355d010",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653879fbf5f5016df355d010/hU3mrTOw3DQ8auUGoQceW.jpeg",
            "isPro": false,
            "fullname": "Bobby Cheng",
            "user": "bobbycxy",
            "type": "user"
          },
          "name": "Bobby Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:06.026Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab74",
          "user": {
            "_id": "636681feaa6a4af6073ba73e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636681feaa6a4af6073ba73e/u_0moYNu6as9Sszp-ej95.png",
            "isPro": true,
            "fullname": "Simon Yu",
            "user": "simonycl",
            "type": "user"
          },
          "name": "Simon Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:08.368Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab75",
          "user": {
            "_id": "635e3a76106f984574c36409",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
            "isPro": false,
            "fullname": "Bo Liu",
            "user": "Benjamin-eecs",
            "type": "user"
          },
          "name": "Bo Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:46:16.265Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab76",
          "name": "Leshem Choshen",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab77",
          "name": "Cheston Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:55:20.000Z",
      "submittedOnDailyAt": "2025-04-16T01:06:25.215Z",
      "title": "TextArena\n\nEsta traducción es del contenido de TextArena traducido al español. TextArena es un plataforma para procesar y traducir textos. Esta plataforma puede traducir textos a diferentes idiomas según las solicitudes de los usuarios. TextArena analiza el texto de los usuarios, proporciona resultados de traducción precisos, y ofrece funciones avanzadas de traducción que permiten entender y aplicar diferentes lenguajes y reglas gramaticales. TextArena proporciona una variedad de herramientas y funciones necesarias para la traducción, ayudando a que las tareas de traducción de los usuarios sean eficientes y precisas.",
      "submittedOnDailyBy": {
        "_id": "635e3a76106f984574c36409",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
        "isPro": false,
        "fullname": "Bo Liu",
        "user": "Benjamin-eecs",
        "type": "user"
      },
      "summary": "TextArena es un conjunto de juegos de texto competitivos abierto-source para el entrenamiento y evaluación de las acciones de agentes en modelos de lenguaje de gran escala (LLMs). Incluye más de 57 entornos únicos (incluyendo configuraciones de un solo jugador, jugadores de recorrido y multijugador), permitiendo a los humanos o a otros modelos proporcionados evaluar fácilmente las capacidades del modelo a través de un sistema de juego en línea. Ofrece puntuaciones en tiempo real de TrueSkill. Es conocido que los benchmarks tradicionales tienden a faltar en la evaluación de habilidades sociales dinámicas (por ejemplo, Negajo, Mindsink, Desgin), y TextArena puede resolver estas deficiencias. Se diseña con base en investigación, comunidad y extensibilidad, priorizando la adición de nuevos juegos, cambios en el marco, pruebas de modelos, juegos entre modelos y entrenamiento de modelos. Para obtener una descripción detallada, incluyendo entornos, juegos, ranking y ejemplos, visite https://github.com/LeonGuertler/TextArena y https://www.textarena.ai/.",
      "upvotes": 18,
      "discussionId": "67ff1388e1bfbb6bdd79abbe",
      "projectPage": "https://textarena.ai/",
      "githubRepo": "https://github.com/LeonGuertler/TextArena",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "TrueSkill scores",
        "negotiation",
        "theory of mind",
        "deception",
        "dynamic social skills"
      ]
    },
    "publishedAt": "2025-04-15T13:55:20.000Z",
    "title": "TextArena",
    "summary": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11442.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "635e3a76106f984574c36409",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
      "fullname": "Bo Liu",
      "name": "Benjamin-eecs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10766",
      "authors": [
        {
          "_id": "67ff114a3026f8abc4bf7e43",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e44",
          "name": "Yanhong Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e45",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e46",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:46:19.086Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T23:53:47.000Z",
      "submittedOnDailyAt": "2025-04-16T00:40:29.697Z",
      "title": "Los datos de instrucción y los datos de razón de qué tan afectan después del entrenamiento: calidad de los datos desde la perspectiva de cada capa",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "El aprendizaje posterior de los modelos de lenguaje grandes (LLMs) está evolucionando hacia problemas de lógica complejos orientados a guías, pero la comprensión de cómo afectan de manera dinámica diferentes datos en el ajuste es principalmente explorada. En este artículo, proponemos un análisis espectral de gradientes por capas basado en datos de guía y lógica de baja/alta calidad resultantes del aprendizaje posterior de los LLMs. Nuestro análisis muestra que los indicadores preliminares constituidos por evaluaciones de datos (por ejemplo, IFD, InsTag, Dificultad, Reward) se pueden interpretar en términos de las características del espectro de los valores propios calculados a partir de la decomposición de valores propios (SVD). En particular, los datos de alta calidad generalmente muestran un norma nuclear más baja y un ranking efectivo más alto, lo que demostra que un ranking efectivo tiene una mejor robustez y resolución para detectar pequeñas diferencias de calidad, como el norma nuclear. Por ejemplo, los datos de lógica muestran un ranking efectivo significativamente más alto que los datos de guía, y tienen una estructura de gradiente más rica para manejar problemas complejos. Las experimentaciones también muestran que modelos de la misma familia comparten patrón de gradientes similares, mientras que modelos de diferente familia muestran diferencias significativas. Esta investigación proporciona una visión consistente sobre el impacto de la calidad de los datos en el aprendizaje, claramente demuestra la interacción entre la calidad de los datos y la estabilidad del aprendizaje, y ofrece una nueva perspectiva para el desarrollo de estrategias de exploración de datos en el aprendizaje posterior.",
      "upvotes": 16,
      "discussionId": "67ff11503026f8abc4bf7fed",
      "githubRepo": "https://github.com/MingLiiii/Gradient_Unified",
      "ai_keywords": [
        "spectral analysis",
        "layer-wise gradients",
        "low/high-quality instruction",
        "reasoning data",
        "IFD",
        "InsTag",
        "Difficulty",
        "Reward",
        "singular value decomposition (SVD)",
        "nuclear norms",
        "effective ranks",
        "gradient structures",
        "training stability"
      ]
    },
    "publishedAt": "2025-04-14T19:53:47.000Z",
    "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients",
    "summary": "As the post-training of large language models (LLMs) advances from\ninstruction-following to complex reasoning tasks, understanding how different\ndata affect finetuning dynamics remains largely unexplored. In this paper, we\npresent a spectral analysis of layer-wise gradients induced by low/high-quality\ninstruction and reasoning data for LLM post-training. Our analysis reveals that\nwidely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and\nReward, can be explained and unified by spectral properties computed from\ngradients' singular value decomposition (SVD). Specifically, higher-quality\ndata are usually associated with lower nuclear norms and higher effective\nranks. Notably, effective rank exhibits better robustness and resolution than\nnuclear norm in capturing subtle quality differences. For example, reasoning\ndata achieves substantially higher effective ranks than instruction data,\nimplying richer gradient structures on more complex tasks. Our experiments also\nhighlight that models within the same family share similar gradient patterns\nregardless of their sizes, whereas different model families diverge\nsignificantly. Providing a unified view on the effects of data quality across\ninstruction and reasoning data, this work illuminates the interplay between\ndata quality and training stability, shedding novel insights into developing\nbetter data exploration strategies for post-training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10766.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10462",
      "authors": [
        {
          "_id": "67ff2aa6a0346c2e622afdb2",
          "name": "Weixian Lei",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb3",
          "name": "Jiacong Wang",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb4",
          "name": "Haochen Wang",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb5",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:51.715Z",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb6",
          "name": "Jun Hao Liew",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb7",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb8",
          "name": "Zilong Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:50:20.000Z",
      "submittedOnDailyAt": "2025-04-16T02:27:49.065Z",
      "title": "La simplicidad de la escalabilidad: un análisis experimental del aprendizaje de visiones y run-grayes realizado mediante un solo Transformer.",
      "submittedOnDailyBy": {
        "_id": "63958b4414513eaf9029ebf1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
        "isPro": false,
        "fullname": "Xiangtai Li",
        "user": "LXT",
        "type": "user"
      },
      "summary": "En este artículo, se integran la codificación de píxeles y la interpretación de lenguaje en una sola arquitectura mediante un grande modelo de lenguaje de transformadores monomodal unificado, SAIL. A diferencia de los modelos modularizados como los líneas de modelo actuales, SAIL no necesita un encoder visual independiente de un modelo de transformadores visuais (ViT) previamente entrenado, proporcionando una arquitectura de diseño más mínimo. En lugar de introducir nuevos componentes arquitectónicos, SAIL aplica una estructura de atención mixta y un encoding de posición monomodal adaptados a las características de los modelos visuales y textuales. Se compara sistemáticamente las características de SAIL, como su escalabilidad, patrón de flujo de información multicapa, y capacidad de representación visual, con los modelos modularizados de MLLM. Al escalar tanto los datos de entrenamiento como el tamaño del modelo, SAIL logra un rendimiento comparable a los modelos modularizados de MLLM. En particular, la eliminación de los componentes previamente entrenados de ViT mejora la escalabilidad de SAIL y cambia considerablemente el patrón de flujo de información multicapa. Además, SAIL muestra una fuerte capacidad de representación visual, obteniendo resultados similares a los de ViT-22B en tareas visuales como la segmentación semántica. Los códigos y modelos están disponibles en https://github.com/bytedance/SAIL.",
      "upvotes": 12,
      "discussionId": "67ff2aa7a0346c2e622afe08",
      "ai_keywords": [
        "single transformer",
        "unified multimodal large language model (MLLM)",
        "raw pixel encoding",
        "language decoding",
        "vision transformer (ViT)",
        "mix-attention mechanisms",
        "multimodal positional encodings",
        "scalability",
        "cross-modal information flow patterns",
        "visual representation capabilities",
        "semantic segmentation",
        "ViT-22B"
      ]
    },
    "publishedAt": "2025-04-14T13:50:20.000Z",
    "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer",
    "summary": "This paper introduces SAIL, a single transformer unified multimodal large\nlanguage model (MLLM) that integrates raw pixel encoding and language decoding\nwithin a singular architecture. Unlike existing modular MLLMs, which rely on a\npre-trained vision transformer (ViT), SAIL eliminates the need for a separate\nvision encoder, presenting a more minimalist architecture design. Instead of\nintroducing novel architectural components, SAIL adapts mix-attention\nmechanisms and multimodal positional encodings to better align with the\ndistinct characteristics of visual and textual modalities. We systematically\ncompare SAIL's properties-including scalability, cross-modal information flow\npatterns, and visual representation capabilities-with those of modular MLLMs.\nBy scaling both training data and model size, SAIL achieves performance\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\nenhances SAIL's scalability and results in significantly different cross-modal\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\nrepresentation capabilities, achieving results on par with ViT-22B in vision\ntasks such as semantic segmentation. Code and models are available at\nhttps://github.com/bytedance/SAIL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10462.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63958b4414513eaf9029ebf1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
      "fullname": "Xiangtai Li",
      "name": "LXT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10559",
      "authors": [
        {
          "_id": "67ff1df03b42083b37219456",
          "name": "Keyu Duan",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219457",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219458",
          "name": "Xin Mao",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219459",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945a",
          "name": "Changyu Chen",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945b",
          "name": "Qiguang Chen",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945c",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945d",
          "user": {
            "_id": "6214e4ee1e35c843d42d1f88",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
            "isPro": true,
            "fullname": "Longxu Dou",
            "user": "dreamerdeo",
            "type": "user"
          },
          "name": "Longxu Dou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:00.444Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T14:53:56.000Z",
      "submittedOnDailyAt": "2025-04-16T01:34:03.069Z",
      "title": "Método para lograr entrenamiento de modelos de recompensa de procesos efectivos mediante aprendizaje químico activo",
      "submittedOnDailyBy": {
        "_id": "6214e4ee1e35c843d42d1f88",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
        "isPro": true,
        "fullname": "Longxu Dou",
        "user": "dreamerdeo",
        "type": "user"
      },
      "summary": "El modelo de recompensa de proceso (PRMs) proporciona subejecciones estadísticas en las etapas para modelos de lenguaje grandes (LLMs), pero presenta desafíos en la escalación de las explicaciones del conjunto de datos de entrenamiento tanto para los humanos como para los LLMs. Para superar esta limitación, proponemos un enfoque de aprendizaje activo. ActPRM selecciona de manera activa los muestras más inciertas para reducir los costos de etiquetado. Durante el entrenamiento, ActPRM evalúa la incertidumbre posterior a la propagación y deja solo los datos con alto riesgo basado en esta incertidumbre. Posteriormente, se utiliza un modelo de inferencia con costos asociados más altos para etiquetar estos datos. Se calcula la pérdida con respecto a la etiqueta y se actualizan los pesos de ActPRM. Comparado con la micro-ajuste de la teoría de la probabilidad, ActPRM logra reducir en un 50% las explicaciones en un entorno de aprendizaje activo basado en conjuntos (POOL), al mismo tiempo que presenta un rendimiento relativamente mejor. Excediendo la eficiencia de las explicaciones, utilizamos ActPRM para filtrar un cargador de datos lógicos matemáticos de más de 1M, dejando solo el 60% de los datos. Posteriormente, el entrenamiento basado en este conjunto de datos seleccionado permite al PRM de nuevas tecnologías más recientes (SOTA) alcanzar los niveles de ProcessBench (75.0%) y PRMBench (65.5%).",
      "upvotes": 8,
      "discussionId": "67ff1df23b42083b372194a8",
      "githubRepo": "https://github.com/sail-sg/ActivePRM",
      "ai_keywords": [
        "active learning",
        "ActPRM",
        "uncertainty estimation",
        "labeling costs",
        "vanilla fine-tuning",
        "pool-based active learning",
        "annotation efficiency",
        "math reasoning trajectories",
        "state-of-the-art (SOTA)",
        "ProcessBench",
        "PRMBench"
      ]
    },
    "publishedAt": "2025-04-14T10:53:56.000Z",
    "title": "Efficient Process Reward Model Training via Active Learning",
    "summary": "Process Reward Models (PRMs) provide step-level supervision to large language\nmodels (LLMs), but scaling up training data annotation remains challenging for\nboth humans and LLMs. To address this limitation, we propose an active learning\napproach, ActPRM, which proactively selects the most uncertain samples for\ntraining, substantially reducing labeling costs. During training, we use the\nPRM to estimate uncertainty after the forward pass, retaining only highly\nuncertain data. A capable yet costly reasoning model then labels this data.\nThen we compute the loss with respect to the labels and update the PRM's\nweights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active\nlearning setting, demonstrating that ActPRM reduces 50% annotation, but\nachieving the comparable or even better performance. Beyond annotation\nefficiency, we further advance the actively trained PRM by filtering over 1M+\nmath reasoning trajectories with ActPRM, retaining 60% of the data. A\nsubsequent training on this selected dataset yields a new state-of-the-art\n(SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same\nsized models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10559.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6214e4ee1e35c843d42d1f88",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
      "fullname": "Longxu Dou",
      "name": "dreamerdeo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11427",
      "authors": [
        {
          "_id": "67ff1cc4372d6790b1b7da90",
          "name": "Yanrui Bin",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da91",
          "name": "Wenbo Hu",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da92",
          "name": "Haoyuan Wang",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da93",
          "name": "Xinya Chen",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da94",
          "name": "Bing Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/dKoipt1ASgjiyDt33avVt.mp4"
      ],
      "publishedAt": "2025-04-15T17:39:07.000Z",
      "submittedOnDailyAt": "2025-04-16T01:28:48.790Z",
      "title": "NormalCrafter: Aprendizaje de vectores de rumba para sincronizar secuencias temporales en películas\nProyección dispersa hacia el frente",
      "submittedOnDailyBy": {
        "_id": "657a7458afbb0117ba15c59f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
        "isPro": false,
        "fullname": "Wenbo Hu",
        "user": "wbhu-tc",
        "type": "user"
      },
      "summary": "La estimación de la superficie Norrma es fundamental para múltiples aplicaciones de visión por computadora. Aunque se ha dedicado mucho esfuerzo a escenarios de imágenes estáticas, garantizar la consistencia temporal en la estimación de Norrma basada en vídeo es un desafío complejo. En lugar de agregar elementos temporales a métodos existentes, presentamos NormalCrafter, que explota las predicciones temporales inherentes al modelo de difusión de vídeo. Proponemos la Regularización de Características Semánticas (SFR) para asegurar una estimación de Norrma de alta calidad en toda la secuencia, recomendando que el modelo se concentre en las características significativas específicas de cada fotograma. Además, introducimos un protocolo de entrenamiento en dos etapas que utiliza tanto el espacio potencial como el espacio de píxeles, con el objetivo de mantener la precisión espacial mientras se mantiene un contexto temporal a largo plazo. Una evaluación ampliada demuestra el efecto de nuestro método y muestra que, a partir de múltiples vídeos, nuestro enfoque genera secuencias de Norrma temporalmente consistentes con detalles complejos, demostrando su efectividad.",
      "upvotes": 5,
      "discussionId": "67ff1cc5372d6790b1b7daee",
      "projectPage": "https://normalcrafter.github.io/",
      "githubRepo": "https://github.com/Binyr/NormalCrafter",
      "ai_keywords": [
        "video diffusion models",
        "Semantic Feature Regularization (SFR)",
        "latent space",
        "pixel space",
        "temporal coherence",
        "spatial accuracy",
        "long temporal context",
        "temporally consistent",
        "intricate details"
      ]
    },
    "publishedAt": "2025-04-15T13:39:07.000Z",
    "title": "NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors",
    "summary": "Surface normal estimation serves as a cornerstone for a spectrum of computer\nvision applications. While numerous efforts have been devoted to static image\nscenarios, ensuring temporal coherence in video-based normal estimation remains\na formidable challenge. Instead of merely augmenting existing methods with\ntemporal components, we present NormalCrafter to leverage the inherent temporal\npriors of video diffusion models. To secure high-fidelity normal estimation\nacross sequences, we propose Semantic Feature Regularization (SFR), which\naligns diffusion features with semantic cues, encouraging the model to\nconcentrate on the intrinsic semantics of the scene. Moreover, we introduce a\ntwo-stage training protocol that leverages both latent and pixel space learning\nto preserve spatial accuracy while maintaining long temporal context. Extensive\nevaluations demonstrate the efficacy of our method, showcasing a superior\nperformance in generating temporally consistent normal sequences with intricate\ndetails from diverse videos.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/dKoipt1ASgjiyDt33avVt.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a7458afbb0117ba15c59f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
      "fullname": "Wenbo Hu",
      "name": "wbhu-tc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11343",
      "authors": [
        {
          "_id": "67ff2d4a86e7ad2b4bea1349",
          "name": "Wei Xiong",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134a",
          "name": "Jiarui Yao",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134b",
          "name": "Yuhui Xu",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134c",
          "name": "Bo Pang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134d",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134e",
          "name": "Doyen Sahoo",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134f",
          "name": "Junnan Li",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1350",
          "name": "Nan Jiang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1351",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1352",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1353",
          "name": "Hanze Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:15:02.000Z",
      "submittedOnDailyAt": "2025-04-16T02:39:25.160Z",
      "title": "Un enfoque minimalista en la inferencia de modelos de lenguaje grandes: reafortamiento experimental desde muestreo de rechazo",
      "submittedOnDailyBy": {
        "_id": "643e59806db6ba8c5ee123f3",
        "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
        "isPro": false,
        "fullname": "Wei Xiong",
        "user": "weqweasdas",
        "type": "user"
      },
      "summary": "El aprendizaje por refuerzo (RL) ha adquirido una posición principal como enfoque principal para ajustar grandes modelos de lenguaje (LLMs) para enfrentar tareas complejas de razonamiento. Recientemente, métodos como GRPO han demostrado éxitos experimentales en la entrenamiento de modelos como DeepSeek-R1, aunque su comprensión de los efectos es incipiente. En este artículo, se reevalúa GRPO y se analizan sus componentes esenciales desde la perspectiva de algoritmos de aprendizaje por refuerzo como el gradiente de políticas. Sorprendentemente, el simple RAFT, basado en muestras de rechazo, logra rendimientos comparables a GRPO y PPO, ya que se entrena solo con muestras que reciben recompensas positivas. No es efectivo de normalización, pero uno de los principales beneficios de GRPO es eliminar respuestas completamente erróneas a consultas. Desde esta perspectiva, se propone una versión mínima de expansión del gradiente de políticas que filtra tanto respuestas completamente erróneas como correctas, llamado Reinforce-Rej. Reinforce-Rej mejora la eficiencia y estabilidad de la divergencia de Kullback-Leibler (KL), y se utiliza como una alternativa efectiva y ligera en comparación con algoritmos RL complejos. Se adopta RAFT como un fuerte base de referencia interpretable, y se argumenta en favor de un diseño más principiano en lugar de utilizar muestras negativas indiferentemente. Nuestros hallazgos proporcionan guías para la investigación futura sobre el procesamiento post-compensatorio de LLMs.",
      "upvotes": 4,
      "discussionId": "67ff2d4b86e7ad2b4bea1381",
      "ai_keywords": [
        "GRPO",
        "DeepSeek-R1",
        "reinforcement learning (RL)",
        "fine-tuning",
        "large language models (LLMs)",
        "complex reasoning tasks",
        "RAFT",
        "positively rewarded samples",
        "policy gradient",
        "KL efficiency"
      ]
    },
    "publishedAt": "2025-04-15T12:15:02.000Z",
    "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce",
    "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11343.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643e59806db6ba8c5ee123f3",
      "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
      "fullname": "Wei Xiong",
      "name": "weqweasdas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.10188",
      "authors": [
        {
          "_id": "67fe602166c0e8f3c2df22a9",
          "user": {
            "_id": "649d59cec6b4fdd84ebe0d47",
            "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
            "isPro": false,
            "fullname": "Deyuan Liu",
            "user": "SempraETY",
            "type": "user"
          },
          "name": "Deyuan Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T16:46:56.270Z",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22aa",
          "name": "Peng Sun",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22ab",
          "name": "Xufeng Li",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22ac",
          "name": "Tao Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T12:43:17.000Z",
      "submittedOnDailyAt": "2025-04-16T03:03:14.152Z",
      "title": "\"Aprovechamiento de representaciones internas mediante entrenamiento eficiente de modelos generativos\"",
      "submittedOnDailyBy": {
        "_id": "649d59cec6b4fdd84ebe0d47",
        "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
        "isPro": false,
        "fullname": "Deyuan Liu",
        "user": "SempraETY",
        "type": "user"
      },
      "summary": "Los modelos de difusión muestran un excelente rendimiento en la generación de datos de alta dimensión, aunque se han identificado deficiencias en la eficiencia de entrenamiento y la calidad de la representación cuando se comparan con métodos de reconocimiento automático. Hemos identificado las principales deficiencias: la escasez del uso de altas calidad y significativas representaciones durante el entrenamiento, que particularmente retrasa la velocidad de convergencia. Mediante un análisis sistemático, hemos descubierto que principalmente en las regiones de procesamiento de representación, las capas iniciales aprenden patrones de significado y estructura hasta que la generación se complete. Para abordar estas deficiencias, proponemos el Embedded Representation Warmup (ERW). El ERW inicializa los módulos ERW con representaciones de alta calidad y previamente entrenadas, similar a un WARMUP. Este WARMUP ayuda a iniciar el entrenamiento de las representaciones de manera que se minimice la inhibición y se acelere la convergencia, mejorando el rendimiento. De acuerdo con análisis teóricos, el efecto del ERW se confirma por la precisa integración de ciertos capos de la red neuronal en el área de procesamiento de representación. Además, el ERW realmente mejora la calidad de las representaciones, lo cual se demuestra por una aceleración real de la velocidad de convergencia: experimentalmente, comparado con el método óptimo actual REPA, el ERW acelera el entrenamiento en un 40 veces. El código está disponible en https://github.com/LINs-lab/ERW.",
      "upvotes": 4,
      "discussionId": "67fe602266c0e8f3c2df2334",
      "projectPage": "https://lins-lab.github.io/ERW/",
      "githubRepo": "https://github.com/LINs-lab/ERW",
      "ai_keywords": [
        "diffusion models",
        "high-dimensional data",
        "self-supervised methods",
        "high-quality representations",
        "semantic representations",
        "structural pattern learning",
        "Embedded Representation Warmup (ERW)",
        "warmup",
        "early layers",
        "representation processing region",
        "pretrained representations",
        "convergence",
        "training convergence",
        "representation quality",
        "REPA"
      ]
    },
    "publishedAt": "2025-04-14T08:43:17.000Z",
    "title": "Efficient Generative Model Training via Embedded Representation Warmup",
    "summary": "Diffusion models excel at generating high-dimensional data but fall short in\ntraining efficiency and representation quality compared to self-supervised\nmethods. We identify a key bottleneck: the underutilization of high-quality,\nsemantically rich representations during training notably slows down\nconvergence. Our systematic analysis reveals a critical representation\nprocessing region -- primarily in the early layers -- where semantic and\nstructural pattern learning takes place before generation can occur. To address\nthis, we propose Embedded Representation Warmup (ERW), a plug-and-play\nframework where in the first stage we get the ERW module serves as a warmup\nthat initializes the early layers of the diffusion model with high-quality,\npretrained representations. This warmup minimizes the burden of learning\nrepresentations from scratch, thereby accelerating convergence and boosting\nperformance. Our theoretical analysis demonstrates that ERW's efficacy depends\non its precise integration into specific neural network layers -- termed the\nrepresentation processing region -- where the model primarily processes and\ntransforms feature representations for later generation. We further establish\nthat ERW not only accelerates training convergence but also enhances\nrepresentation quality: empirically, our method achieves a 40times\nacceleration in training speed compared to REPA, the current state-of-the-art\nmethods. Code is available at https://github.com/LINs-lab/ERW.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10188.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d59cec6b4fdd84ebe0d47",
      "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
      "fullname": "Deyuan Liu",
      "name": "SempraETY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11447",
      "authors": [
        {
          "_id": "67ff1026f8afab940cc23f88",
          "name": "An Zhaol",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f89",
          "name": "Shengyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8a",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8b",
          "name": "Zejian Li",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8c",
          "name": "Jiale Wu",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8d",
          "name": "Haoran Xu",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8e",
          "name": "AnYang Wei",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8f",
          "name": "Perry Pengyun GU Lingyun Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:57:13.000Z",
      "submittedOnDailyAt": "2025-04-16T00:35:02.754Z",
      "title": "La completación eficiente de un 3D LiDAR escritorio mediante optimización directa de preferencias utilizando Difusión Destilación",
      "submittedOnDailyBy": {
        "_id": "63943c882b9483beb473ec25",
        "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
        "isPro": false,
        "fullname": "Shengyuan Zhang",
        "user": "SYZhang0805",
        "type": "user"
      },
      "summary": "El uso de modelos de dispersión en el final de un ciclo de LiDAR está limitado por la velocidad lenta de muestreo de la dispersión. Se reduce la velocidad de muestreo para ahorrar puntos, pero se pierde el rendimiento. Después del entrenamiento con directa optimización de políticas (DPO), se mejora el rendimiento usando datos de preferencia. En este artículo, se propone un nuevo marco de economía de dispersión llamado \"Distillation-DPO\", que tiene una disposición de preferencias. Primero, se generan ciclos de LiDAR completos con combinaciones de ruido iniciales diferentes. Luego, se establecen pares de muestras de ganadores y perdedores utilizando una medida de evaluación de LiDAR como preferencia. Esta configuración es razonable y efectiva, ya que muchos indicadores de LiDAR tienen información pero no pueden ser optimizados directamente. Además, \"Distillation-DPO\" optimiza el modelo estudiante utilizando la diferencia de funciones de puntuación entre el modelo docente y el estudiante. Este proceso se repite hasta convergencia. Experimentos amplios comparando el modelo de dispersión de LiDAR completo de estado de límite con \"Distillation-DPO\" muestran que el último realiza ciclos de mayor calidad y acelera la velocidad de completación en más de cinco veces. Nuestro método intenta aplicar el aprendizaje de preferencias para economía. Nuestro código está disponible en https://github.com/happyw1nd/DistillationDPO.",
      "upvotes": 3,
      "discussionId": "67ff1027f8afab940cc23fd4",
      "ai_keywords": [
        "diffusion models",
        "LiDAR scene completion",
        "score distillation",
        "direct policy optimization (DPO)",
        "preference alignment",
        "student model",
        "paired completion scenes",
        "LiDAR scene evaluation metrics",
        "winning and losing sample pairs",
        "score functions",
        "preference learning"
      ]
    },
    "publishedAt": "2025-04-15T13:57:13.000Z",
    "title": "Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion",
    "summary": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11447.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63943c882b9483beb473ec25",
      "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
      "fullname": "Shengyuan Zhang",
      "name": "SYZhang0805",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11326",
      "authors": [
        {
          "_id": "67ff25b765b52d1b69c1f6c1",
          "user": {
            "_id": "67ff29ecbf6889a333c69c7a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
            "isPro": false,
            "fullname": "Henghui Ding",
            "user": "HenghuiDing",
            "type": "user"
          },
          "name": "Henghui Ding",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T03:55:17.825Z",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c2",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c3",
          "name": "Nikhila Ravi",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c4",
          "name": "Shuting He",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c5",
          "name": "Yunchao Wei",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c6",
          "name": "Song Bai",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c7",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c8",
          "name": "Kehuan Song",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c9",
          "name": "Xinglin Xie",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6ca",
          "name": "Kexin Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cb",
          "name": "Licheng Jiao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cc",
          "name": "Lingling Li",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cd",
          "name": "Shuyuan Yang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6ce",
          "name": "Xuqiang Cao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cf",
          "name": "Linnan Zhao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d0",
          "name": "Jiaxuan Zhao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d1",
          "name": "Fang Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d2",
          "name": "Mengjiao Wang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d3",
          "name": "Junpei Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d4",
          "name": "Xu Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d5",
          "name": "Yuting Yang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d6",
          "name": "Mengru Ma",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d7",
          "name": "Hao Fang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d8",
          "name": "Runmin Cong",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d9",
          "name": "Xiankai Lu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6da",
          "name": "Zhiyang Che",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6db",
          "name": "Wei Zhan",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6dc",
          "name": "Tianming Liang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6dd",
          "name": "Haichao Jiang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6de",
          "name": "Wei-Shi Zheng",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6df",
          "name": "Jian-Fang Hu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e0",
          "name": "Haobo Yuan",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e1",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:58.429Z",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e2",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e3",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e4",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:02:47.000Z",
      "submittedOnDailyAt": "2025-04-16T02:26:49.281Z",
      "title": "PVUW 2025 Reporte de Desafío: Avances en la Comprensión de Video de Vida Natural a Nivel de Pixeles",
      "submittedOnDailyBy": {
        "_id": "67ff29ecbf6889a333c69c7a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
        "isPro": false,
        "fullname": "Henghui Ding",
        "user": "HenghuiDing",
        "type": "user"
      },
      "summary": "Este informe proporciona una descripción detallada de los cuatro concursos de Pixel-level Video Understanding in the Wild (PVUW) que se celebrarán junto con la CVPR de 2025. Este informe resume los resultados de los concursos y contiene información sobre el modo de participación y las direcciones de investigación futuras. Los concursos presentan dos tramos. Uno se centra en el segmentación de objetos en videos de pantalla compleja, denominado MOSE, mientras que el otro trabaja en la segmentación de video basada en lenguaje, guiada por el movimiento. Ambos tramos presentan nuevos y más difíciles conjuntos de datos para reflejar más precisamente el mundo real. A través de evaluaciones y análisis detallados, los concursos proporcionan una guía valiosa sobre las tecnologías de segmentación de videos complejos y los tendencias emergentes. Para obtener más información, consulte https://pvuw.github.io/.",
      "upvotes": 3,
      "discussionId": "67ff25b865b52d1b69c1f736"
    },
    "publishedAt": "2025-04-15T12:02:47.000Z",
    "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild",
    "summary": "This report provides a comprehensive overview of the 4th Pixel-level Video\nUnderstanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025.\nIt summarizes the challenge outcomes, participating methodologies, and future\nresearch directions. The challenge features two tracks: MOSE, which focuses on\ncomplex scene video object segmentation, and MeViS, which targets\nmotion-guided, language-based video segmentation. Both tracks introduce new,\nmore challenging datasets designed to better reflect real-world scenarios.\nThrough detailed evaluation and analysis, the challenge offers valuable\ninsights into the current state-of-the-art and emerging trends in complex video\nsegmentation. More information can be found on the workshop website:\nhttps://pvuw.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11326.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ff29ecbf6889a333c69c7a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
      "fullname": "Henghui Ding",
      "name": "HenghuiDing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.06949",
      "authors": [
        {
          "_id": "67ff12ea58ed263257af79b5",
          "name": "Zhixuan Lin",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b6",
          "name": "Johan Obando-Ceron",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b7",
          "user": {
            "_id": "66906c4e37eadb9c577984d3",
            "avatarUrl": "/avatars/b81765472942fdf94c0ee885ca62df2d.svg",
            "isPro": false,
            "fullname": "Owen He",
            "user": "littleowen",
            "type": "user"
          },
          "name": "Xu Owen He",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T02:16:11.020Z",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b8",
          "name": "Aaron Courville",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T14:57:55.000Z",
      "submittedOnDailyAt": "2025-04-16T01:28:49.703Z",
      "title": "Adaptive Computation Pruning for the Forgetting Transformer",
      "submittedOnDailyBy": {
        "_id": "6694cc1009326cb83f2d11bb",
        "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
        "isPro": false,
        "fullname": "Zhixuan Lin",
        "user": "zhixuan-lin",
        "type": "user"
      },
      "summary": "El recientemente propuesto Forgetting Transformer (FoX) inserta un gate de olvido en la softmax attention, demostrando un rendimiento consistente en comparación con el Transformer estándar basado en RoPE. En particular, muchos de los cabezas de atención de FoX tienen un olvido rápido, y el output en cada etapa temporal depende principalmente del contexto local. Basándonos en esta observación, proponemos la Adaptive Computation Pruning (ACP) para FoX. La ACP elimina de forma dinámica los cálculos relacionados con una fuerte disminución de la dependencia entre entrada y salida mediante el uso de un gate de olvido. La ACP utiliza un valor de umbral de reducción dinámico para hacer que los pesos de atención reducidos no sean visibles a nivel micro. La ACP se aplica en el entrenamiento previo de modelos de lenguaje utilizando FoX, reduciendo aproximadamente el 70% de las FLOP de la softmax attention para diferentes tamaños de modelo y longitudes de contexto. Se puede obtener mejoras del 10% a 35% en los pasos de transducción de tareas de entrenamiento. Además, las longitudes de contexto más largas pueden obtener mayores reducciones de costo de cálculo. Esta mejora en velocidad se implementa de manera que no se vea afectado por una pérdida de rendimiento. Además, hemos realizado diversas analíticas, como la revisión de patrones de reducción y el análisis de la distribución de reducción de FLOP en diferentes cabezas de atención, proporcionando una profunda comprensión del método. El código está disponible en https://github.com/zhixuan-lin/arctic-fox.",
      "upvotes": 3,
      "discussionId": "67ff12eb58ed263257af79fc",
      "ai_keywords": [
        "Forgetting Transformer (FoX)",
        "forget gate",
        "softmax attention",
        "RoPE-based Transformer",
        "Adaptive Computation Pruning (ACP)",
        "input-output dependencies",
        "pruning threshold",
        "FLOPs",
        "training throughput",
        "pruning patterns"
      ]
    },
    "publishedAt": "2025-04-09T10:57:55.000Z",
    "title": "Adaptive Computation Pruning for the Forgetting Transformer",
    "summary": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate\ninto softmax attention and has shown consistently better or on-par performance\ncompared to the standard RoPE-based Transformer. Notably, many attention heads\nin FoX tend to forget quickly, causing their output at each timestep to rely\nprimarily on the local context. Based on this observation, we propose Adaptive\nComputation Pruning (ACP) for FoX, a method that dynamically prunes\ncomputations involving input-output dependencies that are strongly decayed by\nthe forget gate. This is achieved using a dynamically set pruning threshold\nthat ensures that the pruned attention weights remain negligible. We apply ACP\nto language model pretraining with FoX and show it consistently reduces the\nnumber of FLOPs in softmax attention by around 70% across different model sizes\nand context lengths, resulting in a roughly 10% to 35% improvement in training\nthroughput. Furthermore, longer context lengths yield greater computational\nsavings. All these speed improvements are achieved without any performance\ndegradation. We also perform several analyses to provide deeper insights into\nour method, such as examining the pruning patterns and analyzing the\ndistribution of FLOP savings across different attention heads. Our code is\navailable at https://github.com/zhixuan-lin/arctic-fox.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6694cc1009326cb83f2d11bb",
      "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
      "fullname": "Zhixuan Lin",
      "name": "zhixuan-lin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11456",
      "authors": [
        {
          "_id": "67ff79b3d68757d92e9c168e",
          "name": "Zhiwei He",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c168f",
          "name": "Tian Liang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1690",
          "name": "Jiahao Xu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1691",
          "name": "Qiuzhi Liu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1692",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1693",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1694",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1695",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1696",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1697",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1698",
          "name": "Zhuosheng Zhang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1699",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c169a",
          "name": "Zhaopeng Tu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c169b",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c169c",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:59:51.000Z",
      "submittedOnDailyAt": "2025-04-16T08:09:13.393Z",
      "title": "DeepMath-103K: Un conjunto de datos matemáticos de gran escala, complejo, no contaminado y verificable, que fomenta el desarrollo de la lógica.",
      "submittedOnDailyBy": {
        "_id": "60107b385ac3e86b3ea4fc34",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
        "isPro": true,
        "fullname": "Daniel van Strien",
        "user": "davanstrien",
        "type": "user"
      },
      "summary": "La capacidad matemática y lógica compleja es un importante marco de referencia para la inteligencia artificial. La aplicación del aprendizaje por refuerzo (RL) en modelos de lenguaje grande (LLM) ha demostrado resultados deseables, pero su desarrollo está limitado por la escasez de grandes conjuntos de datos, la insuficiencia de estas para ser adecuadas para el aprendizaje por refuerzo y la falta de formatos de respuestas provablemente válidas, además de que no se puede eliminar la contaminación causada por los marcos de evaluación. Para superar estos limitaciones, se presenta DeepMath-103K. DeepMath-103K es un nuevo conjunto de datos de aproximadamente 103K problemas matemáticos diseñado especialmente para entrenar modelos de alta lógica utilizando RL. Este conjunto de datos ha sido ajustado con precisión a través de análisis de datos de fuente, la eliminación de contaminación en múltiples marcos de referencia y la filtración de niveles de dificultad alto (principalmente niveles 5-9). Cada problema incluye una respuesta final provablemente válida, lo que permite el aprendizaje basado en reglas y incluye tres soluciones generadas por R1 diferentes, lo que lo adapta a diferentes paradigmas de entrenamiento (por ejemplo, aprendizaje observacional y saltos). Extiende el alcance de temas matemáticos y fomenta el desarrollo de lógica generalizable. Los modelos entrenados con DeepMath-103K muestran significativas mejoras en los benchmarks matemáticos difíciles y han sido probados. DeepMath-103K se ha lanzado públicamente con el objetivo de fomentar el desarrollo de una comunidad que contribuya a la construcción de sistemas de lógica artificial más potentes: https://github.com/zwhe99/DeepMath.",
      "upvotes": 2,
      "discussionId": "67ff79b4d68757d92e9c16e1",
      "githubRepo": "https://github.com/zwhe99/DeepMath",
      "ai_keywords": [
        "reinforcement learning",
        "large-scale dataset",
        "mathematical problems",
        "training data",
        "verifiable answer formats",
        "decontamination",
        "benchmark",
        "curriculum learning",
        "rule-based RL",
        "supervised fine-tuning",
        "distillation",
        "generalizable reasoning",
        "AI reasoning systems"
      ]
    },
    "publishedAt": "2025-04-15T13:59:51.000Z",
    "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning",
    "summary": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11456.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60107b385ac3e86b3ea4fc34",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
      "fullname": "Daniel van Strien",
      "name": "davanstrien",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 586
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11001",
      "authors": [
        {
          "_id": "67ff4bdb1dc5d56fdd7a1bc4",
          "user": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "isPro": false,
            "fullname": "Alan Dao",
            "user": "alandao",
            "type": "user"
          },
          "name": "Alan Dao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T06:19:08.903Z",
          "hidden": false
        },
        {
          "_id": "67ff4bdb1dc5d56fdd7a1bc5",
          "name": "Thinh Le",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/s2uxHvFhyqjBXMY5wkDLy.mp4"
      ],
      "publishedAt": "2025-04-15T09:18:21.000Z",
      "submittedOnDailyAt": "2025-04-16T04:49:41.555Z",
      "title": "ReZero: Método de intentar mejorar la capacidad de búsqueda de un LLM utilizando el monomisismo",
      "submittedOnDailyBy": {
        "_id": "62d7b2339b629105a5d6888a",
        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
        "isPro": false,
        "fullname": "Alan Dao",
        "user": "alandao",
        "type": "user"
      },
      "summary": "La RAG (Retrieval-Augmented Generation) se utiliza para mejorar el rendimiento de los modelos de lenguaje grandes (LLM) en tareas de agrupación de conocimientos, pero depende significativamente de la calidad de los primeros keywords de búsqueda. Los métodos actuales generalmente utilizan aprendizaje por refuerzo (RL) para centrarse en la configuración de los keywords de búsqueda o en los resultados de la búsqueda, pero no promueven explícitamente la búsqueda de nuevos keywords después de un fracaso de búsqueda inicial. Presentamos un nuevo marco de RL llamado ReZero (Reducing Zero-shot Learning). ReZero otorga una recompensa directa para la búsqueda de nuevos keywords después de un fracaso inicial, induciendo al LLM a explorar keywords alternativos. ReZero logra un rendimiento de precisión del 46.88% más que el estándar de 25%, mejorando la robustez del LLM en escenarios complejos de búsqueda de información donde los primeros keywords de búsqueda no son suficientes.",
      "upvotes": 2,
      "discussionId": "67ff4bdc1dc5d56fdd7a1c36",
      "githubRepo": "https://github.com/menloresearch/ReZero",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "Large Language Model (LLM)",
        "knowledge-intensive tasks",
        "Reinforcement Learning (RL)",
        "query formulation",
        "reasoning over results",
        "ReZero (Retry-Zero)",
        "persistence",
        "search query",
        "unsuccessful attempt",
        "alternative queries",
        "robustness",
        "information-seeking scenarios"
      ]
    },
    "publishedAt": "2025-04-15T05:18:21.000Z",
    "title": "ReZero: Enhancing LLM search ability by trying one-more-time",
    "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/s2uxHvFhyqjBXMY5wkDLy.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d7b2339b629105a5d6888a",
      "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
      "fullname": "Alan Dao",
      "name": "alandao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10049",
      "authors": [
        {
          "_id": "67ff5b335cf0fe153845d1c9",
          "user": {
            "_id": "60d35154d7b174177faabd55",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
            "isPro": false,
            "fullname": "Théo Gigant",
            "user": "gigant",
            "type": "user"
          },
          "name": "Théo Gigant",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:49.462Z",
          "hidden": false
        },
        {
          "_id": "67ff5b335cf0fe153845d1ca",
          "name": "Camille Guinaudeau",
          "hidden": false
        },
        {
          "_id": "67ff5b335cf0fe153845d1cb",
          "name": "Frédéric Dufaux",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T09:55:01.000Z",
      "submittedOnDailyAt": "2025-04-16T05:55:17.253Z",
      "title": "Resumen de la representación multimodal y estructural sobre el impacto de la modalidad y la estructura",
      "submittedOnDailyBy": {
        "_id": "60d35154d7b174177faabd55",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
        "isPro": false,
        "fullname": "Théo Gigant",
        "user": "gigant",
        "type": "user"
      },
      "summary": "Los modelos Vision-Language Models (VLMs) pueden procesar varias formas de información visual y textual: texto, imágenes, combinación de texto y imágenes, o aproximadamente 1 hora de video. En este estudio, se realiza un análisis cualitativo y cuantitativo detallado sobre la auto-resumen de presentaciones multimodales utilizando VLMs. En estas experimentaciones, se propone una estrategia adecuada para generar resumenes ricos de documentos multimodales utilizando VLMs. Además, se demuestra que es beneficioso comparar slides extraídos de videos de streaming con los videos originales y que las representaciones estructuradas obtenidas de combinaciones de slides y texto ofrecen el mejor desempeño. Finalmente, se ofrecen consejos para reflexionar y mejorar sobre las características de la interacción cruzada entre diferentes modalidades de presentaciones.",
      "upvotes": 2,
      "discussionId": "67ff5b355cf0fe153845d215",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "automatic summarization",
        "multimodal presentations",
        "text-heavy multimodal documents",
        "input-length budgets",
        "slides",
        "video stream",
        "raw video",
        "structured representation",
        "interleaved slides",
        "transcript",
        "cross-modal interactions"
      ]
    },
    "publishedAt": "2025-04-14T05:55:01.000Z",
    "title": "Summarization of Multimodal Presentations with Vision-Language Models:\n  Study of the Effect of Modalities and Structure",
    "summary": "Vision-Language Models (VLMs) can process visual and textual information in\nmultiple formats: texts, images, interleaved texts and images, or even\nhour-long videos. In this work, we conduct fine-grained quantitative and\nqualitative analyses of automatic summarization of multimodal presentations\nusing VLMs with various representations as input. From these experiments, we\nsuggest cost-effective strategies for generating summaries from text-heavy\nmultimodal documents under different input-length budgets using VLMs. We show\nthat slides extracted from the video stream can be beneficially used as input\nagainst the raw video, and that a structured representation from interleaved\nslides and transcript provides the best performance. Finally, we reflect and\ncomment on the nature of cross-modal interactions in multimodal presentations\nand share suggestions to improve the capabilities of VLMs to understand\ndocuments of this nature.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10049.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d35154d7b174177faabd55",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
      "fullname": "Théo Gigant",
      "name": "gigant",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08846",
      "authors": [
        {
          "_id": "67ff25f33026f8abc4c4a10d",
          "name": "Mostafa Faghih Shojaei",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a10e",
          "name": "Rahul Gulati",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a10f",
          "name": "Benjamin A. Jasperson",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a110",
          "name": "Shangshang Wang",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a111",
          "user": {
            "_id": "6729342804227d5ea3b283c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
            "isPro": false,
            "fullname": "Simone Cimolato",
            "user": "simocimolato",
            "type": "user"
          },
          "name": "Simone Cimolato",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:55.794Z",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a112",
          "user": {
            "_id": "672ad19141a93b8e140e8689",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VRMw7_PIE5QoD_eWP0hOp.png",
            "isPro": false,
            "fullname": "Dangli Cao",
            "user": "Dinzhenzhenzhu",
            "type": "user"
          },
          "name": "Dangli Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:14.535Z",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a113",
          "name": "Willie Neiswanger",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a114",
          "user": {
            "_id": "67859d73e670c62966ba5767",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mx3fE-YPsjsxpYsBh_DDn.png",
            "isPro": false,
            "fullname": "Krishna Garikipati",
            "user": "garikipati",
            "type": "user"
          },
          "name": "Krishna Garikipati",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-16T05:32:49.978Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T01:26:34.000Z",
      "submittedOnDailyAt": "2025-04-16T07:21:37.722Z",
      "title": "AI Universidad: Plataforma de Inserción Alimento para la Clase de Ciencia en Classland",
      "submittedOnDailyBy": {
        "_id": "6729342804227d5ea3b283c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
        "isPro": false,
        "fullname": "Simone Cimolato",
        "user": "simocimolato",
        "type": "user"
      },
      "summary": "AI Universidad (AI-U) proporciona un flexible marco para la distribución de contenido de cursos utilizando IA, adaptándose a las prácticas de enseñanza de los profesores. El enfoque es que AI-U utilize la tecnología de RAG (Redacción de Contenido de Búsqueda) para fines de aprendizaje para ajustar grandes modelos de lenguaje (LLM) y generar respuestas que se ajusten a videos de clases, notas y libros. Para un curso de FEM (Método de Elementos Finitos) de alto nivel, propone un escenario de estudio que incluye la construcción sistemática de datos de entrenamiento, la adaptación de un modelo de LLM abierto por Low-Rank Adaptation (LoRA), y la optimización de respuestas mediante RAG. La combinación de la similitud cosínea, evaluaciones basadas en LLM y revisiones por expertos demuestra la fuerte adecuación de los contenidos de curso. Además, se ha desarrollado un prototipo de aplicación web \"https://my-ai-university.com\" que enlaza secciones específicas de contenido de curso y videos de acceso abierto con etiquetas de tiempo, fortaleciendo el enfoque basado en seguimiento. El modelo de experto supera la similitud cosínea con referencia en 86% de los casos de prueba y el evaluador de LLM demostró un rendimiento superior al modelo básico Llama 3.2 en casi 4 de cada 5 pruebas. AI-U ofrece una aproximación escalable para la educación de la discriminación de IA, promoviendo su introducción en la educación superior. Aquí se presenta el marco para un curso de FEM, pero este es un ejemplo específico dentro de un contexto más amplio de la adaptación de LLM para contenidos de investigación científica.",
      "upvotes": 2,
      "discussionId": "67ff25f43026f8abc4c4a16c",
      "projectPage": "https://my-ai-university.com",
      "githubRepo": "https://github.com/my-ai-university/finite-element-method",
      "ai_keywords": [
        "large language model (LLM)",
        "retrieval-augmented generation (RAG)",
        "finite-element-method (FEM)",
        "Low-Rank Adaptation (LoRA)",
        "RAG-based synthesis",
        "cosine similarity",
        "LLM-based assessment",
        "traceability",
        "base Llama 3.2 model"
      ]
    },
    "publishedAt": "2025-04-10T21:26:34.000Z",
    "title": "AI-University: An LLM-based platform for instructional alignment to\n  scientific classrooms",
    "summary": "We introduce AI University (AI-U), a flexible framework for AI-driven course\ncontent delivery that adapts to instructors' teaching styles. At its core, AI-U\nfine-tunes a large language model (LLM) with retrieval-augmented generation\n(RAG) to generate instructor-aligned responses from lecture videos, notes, and\ntextbooks. Using a graduate-level finite-element-method (FEM) course as a case\nstudy, we present a scalable pipeline to systematically construct training\ndata, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and\noptimize its responses through RAG-based synthesis. Our evaluation - combining\ncosine similarity, LLM-based assessment, and expert review - demonstrates\nstrong alignment with course materials. We also have developed a prototype web\napplication, available at https://my-ai-university.com, that enhances\ntraceability by linking AI-generated responses to specific sections of the\nrelevant course material and time-stamped instances of the open-access video\nlectures. Our expert model is found to have greater cosine similarity with a\nreference on 86% of test cases. An LLM judge also found our expert model to\noutperform the base Llama 3.2 model approximately four times out of five. AI-U\noffers a scalable approach to AI-assisted education, paving the way for broader\nadoption in higher education. Here, our framework has been presented in the\nsetting of a class on FEM - a subject that is central to training PhD and\nMaster students in engineering science. However, this setting is a particular\ninstance of a broader context: fine-tuning LLMs to research content in science.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6729342804227d5ea3b283c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
      "fullname": "Simone Cimolato",
      "name": "simocimolato",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09454",
      "authors": [
        {
          "_id": "67ff6f8c661b74d0050d2774",
          "name": "Weinan Jia",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2775",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2776",
          "name": "Nan Chen",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2777",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2778",
          "name": "Zhendong Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T06:33:28.000Z",
      "submittedOnDailyAt": "2025-04-16T07:21:45.180Z",
      "title": "D^2iT: Librería de Distribución Dinámica de Canales Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seguidores Seg",
      "submittedOnDailyBy": {
        "_id": "630636bcd37ce67e0e4d1d42",
        "avatarUrl": "/avatars/c3ba695d339eaaf9e810bfa0d9a7689a.svg",
        "isPro": false,
        "fullname": "Mengqi Huang",
        "user": "CoreloneH",
        "type": "user"
      },
      "summary": "El modelo DiPiFusion es reconocido por su capacidad de generar imágenes de alta calidad. Basado en el excelente rendimiento y escalabilidad de la arquitectura DiT Transformer, aunque en el proceso de DiPiFusion se aplica una compresión fija en diferentes áreas de la imagen, ignorando así la densidad de información natural y variada que puede existir en esas áreas. Sin embargo, una compresión muy alta limita la calidad local de las imágenes, mientras que una compresión muy baja aumenta la complejidad computacional, destruye la coherencia global y afecta la calidad de las imágenes finales. Para resolver estas limitaciones, proponemos un nuevo marco de trabajo de 2 etapas que reconocen la importancia de diferentes áreas y compresionan de manera dinámica para mejorar la eficiencia y eficacia de la generación de imágenes. Primero, el VAE Dinámico (DVAE) utiliza diferentes tasas de muestreo para encoder diferentes áreas de la imagen, generando códigos potenciales más precisos y naturales. Luego, el DiT Transformer Dinámico (D^2iT) combina un nuevo enfoque con el Transformer de grado dinámico y el Transformer de contenido dinámico para predecir una mezcla de ruidos que incluyen el \"core-els\" (áreas suaves con pocos códigos potenciales) y los \"finit-els\" (áreas detalladas con muchos códigos potenciales), generando así imágenes. La combinación del gran predecido de ruido y la definición de áreas detalladas permite alcanzar una integración de la coherencia global y la calidad local de las imágenes. Mediante experimentos detallados en diferentes tareas de generación, hemos comprobado la efectividad de nuestro enfoque. El código está disponible en https://github.com/jiawn-creator/Dynamic-DiT.",
      "upvotes": 1,
      "discussionId": "67ff6f8f661b74d0050d28a9",
      "ai_keywords": [
        "Diffusion models",
        "Diffusion Transformer (DiT)",
        "compression",
        "image regions",
        "information densities",
        "local realism",
        "computational complexity",
        "global consistency",
        "generated images",
        "Dynamic VAE (DVAE)",
        "hierarchical encoder",
        "downsampling rates",
        "latent codes",
        "Dynamic Diffusion Transformer (D$^2$iT)",
        "multi-grained noise",
        "coarse-grained",
        "fine-grained",
        "Dynamic Grain Transformer",
        "Dynamic Content Transformer",
        "rough prediction",
        "detailed regions correction"
      ]
    },
    "publishedAt": "2025-04-13T02:33:28.000Z",
    "title": "D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation",
    "summary": "Diffusion models are widely recognized for their ability to generate\nhigh-fidelity images. Despite the excellent performance and scalability of the\nDiffusion Transformer (DiT) architecture, it applies fixed compression across\ndifferent image regions during the diffusion process, disregarding the\nnaturally varying information densities present in these regions. However,\nlarge compression leads to limited local realism, while small compression\nincreases computational complexity and compromises global consistency,\nultimately impacting the quality of generated images. To address these\nlimitations, we propose dynamically compressing different image regions by\nrecognizing the importance of different regions, and introduce a novel\ntwo-stage framework designed to enhance the effectiveness and efficiency of\nimage generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical\nencoder to encode different image regions at different downsampling rates,\ntailored to their specific information densities, thereby providing more\naccurate and natural latent codes for the diffusion process. (2) Dynamic\nDiffusion Transformer (D^2iT) at second stage generates images by predicting\nmulti-grained noise, consisting of coarse-grained (less latent code in smooth\nregions) and fine-grained (more latent codes in detailed regions), through an\nnovel combination of the Dynamic Grain Transformer and the Dynamic Content\nTransformer. The strategy of combining rough prediction of noise with detailed\nregions correction achieves a unification of global consistency and local\nrealism. Comprehensive experiments on various generation tasks validate the\neffectiveness of our approach. Code will be released at\nhttps://github.com/jiawn-creator/Dynamic-DiT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630636bcd37ce67e0e4d1d42",
      "avatarUrl": "/avatars/c3ba695d339eaaf9e810bfa0d9a7689a.svg",
      "fullname": "Mengqi Huang",
      "name": "CoreloneH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]