[
  {
    "paper": {
      "id": "2501.15368",
      "authors": [
        {
          "_id": "67986c6822990ae89bb71fb9",
          "name": "Yadong Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fba",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbb",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbc",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbd",
          "name": "Song Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbe",
          "name": "Tianpeng Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbf",
          "name": "Zehuan Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc0",
          "name": "Lijun Liu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc1",
          "name": "Lingfeng Ming",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc2",
          "name": "Guosheng Dong",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc3",
          "name": "Da Pan",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc4",
          "name": "Chong Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc5",
          "name": "Yuanbo Fang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc6",
          "name": "Dongdong Kuang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc7",
          "name": "Mingrui Wang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc8",
          "name": "Chenglin Zhu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc9",
          "name": "Youwei Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fca",
          "name": "Hongyu Guo",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fcb",
          "name": "Fengyu Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fcc",
          "name": "Yuran Wang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fcd",
          "name": "Bowen Ding",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fce",
          "name": "Wei Song",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fcf",
          "name": "Xu Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd0",
          "name": "Yuqi Huo",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd1",
          "name": "Zheng Liang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd2",
          "name": "Shusen Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd3",
          "name": "Xin Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd4",
          "name": "Shuai Zhao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd5",
          "name": "Linchu Xiong",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd6",
          "name": "Yozhen Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd7",
          "name": "Jiahui Ye",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd8",
          "name": "Wenhao Lu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd9",
          "name": "Bowen Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fda",
          "name": "Yan Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fdb",
          "name": "Yaqi Zhou",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fdc",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fdd",
          "name": "Lei Su",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fde",
          "name": "Hongda Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fdf",
          "name": "Fuzhong Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe0",
          "name": "Xuezhen Dong",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe1",
          "name": "Na Nie",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe2",
          "name": "Zhiying Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe3",
          "name": "Bin Xiao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe4",
          "name": "Ting Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe5",
          "name": "Shunya Dang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe6",
          "name": "Ping Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe7",
          "name": "Yijia Sun",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe8",
          "name": "Jincheng Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe9",
          "name": "Jinjie Yang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fea",
          "name": "Xionghai Lin",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71feb",
          "name": "Zhi Ma",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fec",
          "name": "Kegeng Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fed",
          "name": "Jia li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fee",
          "name": "Aiyuan Yang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fef",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff0",
          "name": "Jianqiang Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff1",
          "name": "Xiaoxi Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff2",
          "name": "Guangwei Ai",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff3",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff4",
          "name": "Yicong Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff5",
          "name": "Xiaoqin Huang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff6",
          "name": "Kun Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff7",
          "name": "Wenjing Luo",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff8",
          "name": "Yifei Duan",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff9",
          "name": "Lingling Zhu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffa",
          "name": "Ran Xiao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffb",
          "name": "Zhe Su",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffc",
          "name": "Jiani Pu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffd",
          "name": "Dian Wang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffe",
          "name": "Xu Jia",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fff",
          "name": "Tianyu Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72000",
          "name": "Mengyu Ai",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72001",
          "name": "Mang Wang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72002",
          "name": "Yujing Qiao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72003",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72004",
          "name": "Yanjun Shen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72005",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72006",
          "name": "Miao Zhen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72007",
          "name": "Yijie Zhou",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72008",
          "name": "Mingyang Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72009",
          "name": "Fei Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200a",
          "name": "Chenzheng Zhu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200b",
          "name": "Keer Lu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200c",
          "name": "Yaqi Zhao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200d",
          "name": "Hao Liang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200e",
          "name": "Youquan Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200f",
          "name": "Yanzhao Qin",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72010",
          "name": "Linzhuang Sun",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72011",
          "name": "Jianhua Xu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72012",
          "name": "Haoze Sun",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72013",
          "name": "Mingan Lin",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72014",
          "name": "Zenan Zhou",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72015",
          "name": "Weipeng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-26T02:19:03.000Z",
      "title": "Certainly! Aquí está el texto traducido al español, manteniendo la profundidad y la precisión:\n\n**Informe Técnico Baichuan-Omni-1.5**\n\n(Tenga en cuenta que, aunque no haya solicitado ninguna explicación o texto adicional, he asegurado la precisión y la profundidad del traducción, sin añadir ningún texto extra.)",
      "summary": "Baichuan-Omni-1.5 tiene la capacidad de comprender diferentes modalidades y además proporciona la capacidad de generar audio. Para mantener las funciones del modelo y permitir una interacción fluida entre diferentes modalidades, se optimizaron tres aspectos importantes. Primero, se construyó un sistema de limpieza y síntesis de datos multimodal para recopilar cerca de 500B de datos de alta calidad (texto, audio, visión). Luego, se diseñó un tokenizer de audio (Baichuan-Audio-Tokenizer) para extraer información significativa y acústica del audio, mejorando las funciones sin necesidad de diferencias entre el audio y las imágenes. Finalmente, se diseñó una estrategia de entrenamiento multiescala para integrar de manera gradual la armamentación multimodal y el entrenamiento final de tareas, asegurando un simple entrenamiento efectivo entre todas las modalidades. Baichuan-Omni-1.5 tiene funciones específicas para diferentes modalidades en comparación con modelos modernos (GPT4o-mini, MiniCPM-o 2.6, etc.). En particular, obtuvo resultados competitivos en diferentes benchmarks médicos comparado con Qwen2-VL-72B.",
      "upvotes": 18,
      "discussionId": "67986c6b22990ae89bb720aa"
    },
    "publishedAt": "2025-01-28T00:34:49.721Z",
    "title": "Baichuan-Omni-1.5 Technical Report",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15368.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.15383",
      "authors": [
        {
          "_id": "67986c83b5e71350993d28eb",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ec",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ed",
          "name": "Chengyuan Li",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ee",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ef",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f0",
          "name": "Haoyan Huang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f1",
          "name": "Jiandong Jiang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f2",
          "name": "Jianhong Tu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f3",
          "name": "Jianwei Zhang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f4",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f5",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f6",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f7",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f8",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f9",
          "name": "Mei Li",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fa",
          "name": "Minmin Sun",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fb",
          "name": "Qin Zhu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fc",
          "name": "Rui Men",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fd",
          "name": "Tao He",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fe",
          "name": "Weijia Xu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ff",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2900",
          "name": "Wenyuan Yu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2901",
          "name": "Xiafei Qiu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2902",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2903",
          "name": "Xinlong Yang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2904",
          "name": "Yong Li",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2905",
          "name": "Zhiying Xu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2906",
          "name": "Zipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-26T03:47:25.000Z",
      "title": "Qwen2.5-1M Reporte Técnico\n\n(Nota: Este traducción es simplemente una traducción del texto en inglés al español. No se garantiza que todos los contenidos sean traducciones precisas o que transmitan contenidos técnicos correctos.)",
      "summary": "Introduzco la serie de modelos Qwen2.5-1M. Esta serie de modelos extiende la longitud de contexto a un millón de tokens. En comparación con las versiones anteriores de 128K, se ha mejorado significativamente la capacidad de procesamiento de largo contexto. Se utilizan principales tecnologías como la síntesis de datos largos, el entrenamiento de protocolos avanzados y la optimización de control de subgrupos multinivel para mejorar eficazmente la capacidad de largo contexto y reducir los costos de entrenamiento.\n\nPara promover la utilización de modelos de largo contexto en más usuarios, proporcionamos un marco de inferencia y lo hacemos abierto-source. Este marco de inferencia incluye un método de extensión de contexto que puede expandir la longitud del contexto del modelo por al menos cuatro veces sin necesidad de entrenamiento adicional. Para reducir los costos de inferencia, implementamos métodos de atención esparsa y optimización de prefiltering basado en comas, así como también un método de precisión esparsa. Además, detallamos la optimización del motor de inferencia, incluyendo optimización de kernels, paralelización de pipa y optimización de programación, lo que mejora significativamente el rendimiento general de la inferencia.\n\nCon este marco de inferencia, se puede obtener un aumento en la velocidad de prefiltering entre 3 y 7 veces en situaciones de contexto de un millón de tokens. Este marco de inferencia proporciona una solución eficiente y potente para el desarrollo de aplicaciones que requieren procesamiento de largo contexto utilizando modelos abierto-source.\n\nActualmente, la serie Qwen2.5-1M incluye modelos abierto-source como Qwen2.5-7B-Instruct-1M y Qwen2.5-14B-Instruct-1M, así como modelos accesibles a través de API como Qwen2.5-Turbo. Según evaluaciones, los modelos Qwen2.5-1M mejoran significativamente en tareas de largo contexto sin perder la eficiencia en escenarios de corto contexto. En particular, el modelo Qwen2.5-14B-Instruct-1M supera a GPT-4o-mini en tareas de largo contexto y soporta un contexto de largo de ocho veces.",
      "upvotes": 7,
      "discussionId": "67986c84b5e71350993d2974"
    },
    "publishedAt": "2025-01-28T00:35:46.871Z",
    "title": "Qwen2.5-1M Technical Report",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15383.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.15570",
      "authors": [
        {
          "_id": "679843ae7d7b7f8196c61ab7",
          "name": "Lin Yueyu",
          "hidden": false
        },
        {
          "_id": "679843ae7d7b7f8196c61ab8",
          "name": "Li Zhiyuan",
          "hidden": false
        },
        {
          "_id": "679843ae7d7b7f8196c61ab9",
          "name": "Peter Yue",
          "hidden": false
        },
        {
          "_id": "679843ae7d7b7f8196c61aba",
          "user": {
            "_id": "6176b32847ee6431f632981e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg",
            "isPro": false,
            "fullname": "IvanD",
            "user": "xiaol",
            "type": "user"
          },
          "name": "Liu Xiao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-01-28T02:44:02.658Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-26T15:56:56.000Z",
      "title": "ARWKV: No se necesita entrenamiento previo, modelo de lenguaje basado en RNN-Attention derivado de Transformer.",
      "summary": "Hasta ahora como se ha conocido, el modelo de atención híbrido 2D y 3D en la arquitectura de Dahead supera a los modelos Transformer y Linear RNN. Estas investigaciones se centraron principalmente en la reducción de la complejidad KV y en la mejora de la eficiencia. Se presentan en Qwen 2.5 modelos basados en la serie de modelos de atención RWKV-7, entrenados para mejorar la expresión. Estos modelos aumentan la capacidad de representación de los RNN y muestran un mejor seguimiento de estado que los Transformers. El QRWK 32B se basa en la arquitectura RWKV-6 y se diseñó para mantener el rendimiento de Qwen 2.5, limitando el tiempo de procesamiento de conocimientos a 8 horas utilizando 16 GPUs AMD MI300X. Se explica cómo se utiliza en el proceso de entrenamiento un LLM, así como cómo se facilita la propagación de conocimientos desde grandes a pequeños LLMs, compartiendo feedback sobre la construcción de modelos de base fuertes. Esta es una investigación en curso que se actualiza continuamente. Los checkpoints de modelo y el código fuente están disponibles en las siguientes URLs.\n\nhttps://github.com/yynil/RWKVInside\nhttps://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1",
      "upvotes": 3,
      "discussionId": "679843af7d7b7f8196c61b21"
    },
    "publishedAt": "2025-01-28T03:02:56.062Z",
    "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15570.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6176b32847ee6431f632981e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg",
      "fullname": "IvanD",
      "name": "xiaol",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 81
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.16142",
      "authors": [
        {
          "_id": "67986cbc7dbf69e4e38539b7",
          "name": "Scott Fujimoto",
          "hidden": false
        },
        {
          "_id": "67986cbc7dbf69e4e38539b8",
          "name": "Pierluca D'Oro",
          "hidden": false
        },
        {
          "_id": "67986cbc7dbf69e4e38539b9",
          "name": "Amy Zhang",
          "hidden": false
        },
        {
          "_id": "67986cbc7dbf69e4e38539ba",
          "name": "Yuandong Tian",
          "hidden": false
        },
        {
          "_id": "67986cbc7dbf69e4e38539bb",
          "name": "Michael Rabbat",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T15:36:37.000Z",
      "title": "Modelo sin modelo de reacción química en estudio",
      "summary": "El aprendizaje por refuerzo (RL) promete no solo aproximaciones sino también un marco de trabajo para la resolución de problemas. De manera práctica, los algoritmos de RL están adaptados a ciertos marcos de referencia y dependen de parámetros de configuración y la elección del algoritmo. Recientemente, los métodos de RL basados en modelos potentes han mostrado resultados sorprendentes en varios marcos de referencia, pero esto limita su aplicabilidad en un amplio espectro de dominios. En este artículo, se intenta encontrar un algoritmo de RL profundo sin modelos que pueda manejar amplios dominios y configuraciones de problemas. Para lograrlo, se utilizan representaciones basadas en modelos y objetos más cercanos utilizados en RL basado en modelos, y se aproximan las funciones de valor linealmente para evitar los costos relacionados con la planificación o cálculo de trayectorias. Nuestro algoritmo, MR.Q, se evalua en varios marcos de referencia generales con un solo conjunto de parámetros iniciales, demostrando una competencia con los límites de dominio específicos y los límites generales, y proporciona una etapa concreta para la construcción de un algoritmo de RL profundo sin modelos.",
      "upvotes": 2,
      "discussionId": "67986cbf7dbf69e4e3853a89"
    },
    "publishedAt": "2025-01-28T00:36:09.186Z",
    "title": "Towards General-Purpose Model-Free Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.15369",
      "authors": [
        {
          "_id": "6798706dabdc35456a92212d",
          "name": "Chuanyang Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-26T02:34:58.000Z",
      "title": "iFormer: Integración de ConvNet y Transformer en aplicaciones móviles",
      "summary": "Aquí se presenta la familia de nuevas redes híbridas de visión móvil para optimizar la latencia y la precisión en aplicaciones móviles, llamadas \"iFormer\". iFormer integra efectivamente la capacidad de representación local rápida y la capacidad de modelado global eficiente. La interacción local se realiza al modificar la red convolucional estándar (ConvNeXt), permitiendo la diseño de redes móviles más ligeras. La nueva atención de modelado móvil reduce la carga de memoria de la MHA y mejora la capacidad de representación global dinámica mediante una estructura de modelado eficiente. Se realizaron experimentos detallados y iFormer demostró un excelente desempeño en diferentes tareas que superan a las redes más ligeras actuales. En particular, alcanzó una precisión de 80.4% en Top-1 en ImageNet-1k, con una latencia de 1.10ms en iPhone 13. Además, superó a MobileNetV4, proporcionando un aumento claro en tareas como detección de objetos, segmentación de instancias y segmentación de etiquetas en ADE20k, manteniendo baja la latencia incluso con entradas de alta resolución en dispositivos móviles.",
      "upvotes": 1,
      "discussionId": "6798706eabdc35456a92215a"
    },
    "publishedAt": "2025-01-28T00:51:51.263Z",
    "title": "iFormer: Integrating ConvNet and Transformer for Mobile Application",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15369.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16295",
      "authors": [
        {
          "_id": "67986cd6bdc99911a989b0a5",
          "name": "Weixin Liang",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0a6",
          "name": "Junhong Shen",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0a7",
          "name": "Genghan Zhang",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0a8",
          "name": "Ning Dong",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0a9",
          "name": "Luke Zettlemoyer",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0aa",
          "name": "Lili Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T18:35:05.000Z",
      "title": "Mixture-of-Mamba: Utilizando señales complejas para extender modelos de espacios de estado multimodales",
      "summary": "Los modelos de estado espacio (SSMs) han aparecido como una alternativa eficiente a los Transformers, pero están limitados en el rendimiento de entrenamiento multimodelo debido a que no pueden explotar las características individuales de cada modelo. En este trabajo, proponemos una nueva arquitectura SSM llamada Mixture-of-Mamba, que introduce señales ocultas modelo-específicas. Este método introduce la esparsidad patrón utilizando la parametrización modelo-específica de bloques Mamba. Basándonos en el Mixture-of-Transformers (W. Liang et al., arXiv:2411.04996; 2024), extendemos los beneficios de las señales ocultas modelo-específicas en SSMs mientras mantenemos la eficiencia computacional. La Mixture-of-Mamba evalúa tres configuraciones de entrenamiento multimodelo en marcos extensos que incluyen texto, imágenes continuas y texto: Transfusion (texto indirecto y tokens de imagen continuo con pérdida de difusión), Chameleon (texto indirecto y tokens de imagen discreto) y un modelo que incluye lenguaje. La Mixture-of-Mamba alcanza el mismo valor de pérdida en las etapas iniciales de entrenamiento y contribuye significativamente a la reducción de costos computacionales. En el entorno de Transfusion, con una escala de 1.4B, se alcanza una pérdida de imágenes equivalente con 34.76% de FLOP de entrenamiento. En el entorno de Chameleon, con la misma escala, se alcanza una pérdida de imágenes similar con 42.50% de FLOP y una pérdida de texto similar con 65.40% de FLOP. En los tres entornos, con la misma escala, se alcanza una pérdida de lenguaje con 24.80% de FLOP. En un estudio de ablación, se realizan análisis y se observa que los cambios individuales tienen menor efecto que las interpretaciones comunes. Estos resultados demuestran que las señales ocultas modelo-específicas son una base sólida para la diseño efectivo, expandiendo el impacto de los SSMs sobre los Transformers y estableciendo nuevos estándares de comparación en el entrenamiento multimodelo. El código está disponible en https://github.com/Weixin-Liang/Mixture-of-Mamba.",
      "upvotes": 1,
      "discussionId": "67986cd7bdc99911a989b0ea"
    },
    "publishedAt": "2025-01-28T00:36:31.841Z",
    "title": "Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.14912",
      "authors": [
        {
          "_id": "67986d764fccd4b95149db0b",
          "name": "Juan Ramirez",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db0c",
          "name": "Ignacio Hounie",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db0d",
          "name": "Juan Elenter",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db0e",
          "name": "Jose Gallego-Posada",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db0f",
          "name": "Meraj Hashemizadeh",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db10",
          "name": "Alejandro Ribeiro",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db11",
          "name": "Simon Lacoste-Julien",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-24T20:39:38.000Z",
      "title": "Feasible Learning",
      "summary": "Introducing Feasible Learning (FL). FL es un paradigma de aprendizaje centrado en muestras que se desarrolla al abordar el problema de limitar la pérdida de cada muestra de entrenamiento. Comparado con el marco clásico de Empirical Risk Minimization (ERM), el ERM optimiza el rendimiento promedio, mientras que FL exige un rendimiento satisfactorio para cada punto de datos. Un modelo que satisface una secuencia de rendimientos específicas es reconocido como una solución justificada en FL, y la elección del algoritmo de optimización y su dinámica desempeñan un papel crucial en la caracterización de la solución obtenida. En particular, se está investigando un enfoque subjetivo para reevaluar la importancia de cada muestra durante el aprendizaje, antes de abordar un problema real. Para superar las dificultades de establecer secuencias significativas, se introduce una flexibilidad en FL que incluye variables de slack mínimas, lo que facilita la resolución de problemas como la clasificación de imágenes, la regresión de edad y el optimización de preferencias en grandes modelos de lenguaje. En análisis experimentales que incluyen clasificación de imágenes, regresión de edad y optimización de preferencias en grandes modelos de lenguaje, los modelos entrenados con FL presentan un mayor impacto en el rendimiento promedio en comparación con ERM, pero también mejoran el rendimiento tail-biased mientras se aprenden de los datos.",
      "upvotes": 0,
      "discussionId": "67986d784fccd4b95149db6b"
    },
    "publishedAt": "2025-01-28T00:39:11.423Z",
    "title": "Feasible Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.14912.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  }
]