[
  {
    "paper": {
      "id": "2501.15368",
      "authors": [
        {
          "_id": "67986c6822990ae89bb71fb9",
          "name": "Yadong Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fba",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbb",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbc",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbd",
          "name": "Song Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbe",
          "name": "Tianpeng Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbf",
          "name": "Zehuan Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc0",
          "name": "Lijun Liu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc1",
          "name": "Lingfeng Ming",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc2",
          "name": "Guosheng Dong",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc3",
          "name": "Da Pan",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc4",
          "name": "Chong Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc5",
          "name": "Yuanbo Fang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc6",
          "name": "Dongdong Kuang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc7",
          "name": "Mingrui Wang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc8",
          "name": "Chenglin Zhu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc9",
          "name": "Youwei Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fca",
          "name": "Hongyu Guo",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fcb",
          "name": "Fengyu Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fcc",
          "name": "Yuran Wang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fcd",
          "name": "Bowen Ding",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fce",
          "name": "Wei Song",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fcf",
          "name": "Xu Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd0",
          "name": "Yuqi Huo",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd1",
          "name": "Zheng Liang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd2",
          "name": "Shusen Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd3",
          "name": "Xin Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd4",
          "name": "Shuai Zhao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd5",
          "name": "Linchu Xiong",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd6",
          "name": "Yozhen Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd7",
          "name": "Jiahui Ye",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd8",
          "name": "Wenhao Lu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd9",
          "name": "Bowen Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fda",
          "name": "Yan Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fdb",
          "name": "Yaqi Zhou",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fdc",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fdd",
          "name": "Lei Su",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fde",
          "name": "Hongda Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fdf",
          "name": "Fuzhong Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe0",
          "name": "Xuezhen Dong",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe1",
          "name": "Na Nie",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe2",
          "name": "Zhiying Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe3",
          "name": "Bin Xiao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe4",
          "name": "Ting Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe5",
          "name": "Shunya Dang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe6",
          "name": "Ping Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe7",
          "name": "Yijia Sun",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe8",
          "name": "Jincheng Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe9",
          "name": "Jinjie Yang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fea",
          "name": "Xionghai Lin",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71feb",
          "name": "Zhi Ma",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fec",
          "name": "Kegeng Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fed",
          "name": "Jia li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fee",
          "name": "Aiyuan Yang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fef",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff0",
          "name": "Jianqiang Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff1",
          "name": "Xiaoxi Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff2",
          "name": "Guangwei Ai",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff3",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff4",
          "name": "Yicong Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff5",
          "name": "Xiaoqin Huang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff6",
          "name": "Kun Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff7",
          "name": "Wenjing Luo",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff8",
          "name": "Yifei Duan",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff9",
          "name": "Lingling Zhu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffa",
          "name": "Ran Xiao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffb",
          "name": "Zhe Su",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffc",
          "name": "Jiani Pu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffd",
          "name": "Dian Wang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffe",
          "name": "Xu Jia",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fff",
          "name": "Tianyu Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72000",
          "name": "Mengyu Ai",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72001",
          "name": "Mang Wang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72002",
          "name": "Yujing Qiao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72003",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72004",
          "name": "Yanjun Shen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72005",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72006",
          "name": "Miao Zhen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72007",
          "name": "Yijie Zhou",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72008",
          "name": "Mingyang Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72009",
          "name": "Fei Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200a",
          "name": "Chenzheng Zhu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200b",
          "name": "Keer Lu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200c",
          "name": "Yaqi Zhao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200d",
          "name": "Hao Liang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200e",
          "name": "Youquan Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200f",
          "name": "Yanzhao Qin",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72010",
          "name": "Linzhuang Sun",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72011",
          "name": "Jianhua Xu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72012",
          "name": "Haoze Sun",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72013",
          "name": "Mingan Lin",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72014",
          "name": "Zenan Zhou",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72015",
          "name": "Weipeng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-26T02:19:03.000Z",
      "title": "Baichuan-Omni-1.5 Reporte Técnico",
      "summary": "Baichuan-Omni-1.5 está compuesto por diversos modelos y, además de su capacidad para entender varios modelos, ofrece una capacidad de generación de voz cohesionada desde el principio hasta el final. Priorizamos tres aspectos importantes para mantener la integridad de todas las capacidades y realizar una interacción de alta calidad entre modelos: primero, construimos una pipeline de limpieza y síntesis de contenido para varios tipos de datos, recopilando aproximadamente 500B de datos de alta calidad (texto, voz, visual). Luego, diseñamos un tokenizador de voz (Baichuan-Audio-Tokenizer) para extraer información lingüística y acústica de las señales de voz, logrando una integración y extensión sin interferencia con los modelos de MLLM. Finalmente, diseñamos una estrategia de entrenamiento multi-etapa para integrar de manera efectiva la adaptación a diferentes modelos y tareas en diferentes etapas, asegurando una armonía entre todos los modelos. Baichuan-Omni-1.5 supera los modelos modernos (incluyendo GPT4o-mini y MiniCPM-o 2.6) y ha sido evaluado en detalle.",
      "upvotes": 18,
      "discussionId": "67986c6b22990ae89bb720aa"
    },
    "publishedAt": "2025-01-28T00:34:49.721Z",
    "title": "Baichuan-Omni-1.5 Technical Report",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15368.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.15383",
      "authors": [
        {
          "_id": "67986c83b5e71350993d28eb",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ec",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ed",
          "name": "Chengyuan Li",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ee",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ef",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f0",
          "name": "Haoyan Huang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f1",
          "name": "Jiandong Jiang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f2",
          "name": "Jianhong Tu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f3",
          "name": "Jianwei Zhang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f4",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f5",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f6",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f7",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f8",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f9",
          "name": "Mei Li",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fa",
          "name": "Minmin Sun",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fb",
          "name": "Qin Zhu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fc",
          "name": "Rui Men",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fd",
          "name": "Tao He",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fe",
          "name": "Weijia Xu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ff",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2900",
          "name": "Wenyuan Yu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2901",
          "name": "Xiafei Qiu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2902",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2903",
          "name": "Xinlong Yang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2904",
          "name": "Yong Li",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2905",
          "name": "Zhiying Xu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2906",
          "name": "Zipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-26T03:47:25.000Z",
      "title": "Qwen2.5-1M Reporte Técnico\n\n(Nota: Este traducción es simplemente el resultado de traducir el inglés al español. Se mantiene la profundidad y precisión del original, sin agregar descripciones o texto adicional.)",
      "summary": "Se presenta la serie de modelos Qwen2.5-1M. Esta serie de modelos ha extendido la longitud de contexto a 1,000,000 tokens. En comparación con la versión anterior de 128K, el procesamiento de contextos largos ha mejorado significativamente. A través de la entrenamiento de contextos largos y el aprendizaje posterior, se ha mejorado efectivamente la capacidad de contexto largo y se ha reducido los costos de entrenamiento. Se ha mejorado la capacidad de contexto largo y reducido los costos de entrenamiento mediante técnicas principales como la síntesis de datos largos, el aprendizaje de predicción refinado y la ajuste de retroalimentación manual multinivel.\n\nPara promover la utilización de modelos de contexto largo basado en un usuario mundial, proporcionamos un marco de inferencia y publicamos el código fuente. Este marco de inferencia incluye un método de inferencia de contexto largo que puede extender la longitud de contexto del modelo más de cuatro veces, excepto para el aprendizaje adicional. Para reducir los costos de inferencia, se ha implementado el método de atención rara y el pre-procesado de campos bloque, y se ha implementado retroalimentación para mejorar la raridad en escenarios de batch. Además, se explica detalladamente la optimización del motor de inferencia, y se han aplicado optimizaciones de kernel, paralelización de pipeline y optimización de programación, lo que ha mejorado significativamente la performance de inferencia global. Al utilizar este marco de inferencia, se puede obtener un aumento de velocidad de pre-procesado de campos bloque entre 3 y 7 veces para un contexto de 1,000,000 tokens. Este marco de inferencia proporciona una solución eficiente y potente para el desarrollo de aplicaciones que requieren el procesamiento de contextos largos.\n\nActualmente, la serie Qwen2.5-1M incluye modelos abiertos de código fuente como Qwen2.5-7B-Instruct-1M y Qwen2.5-14B-Instruct-1M, así como el modelo Qwen2.5-Turbo accesible a través de la API. Según evaluaciones, los modelos Qwen2.5-1M han mejorado significativamente en tareas de contexto largo sin perder la eficiencia en escenarios de contexto corto. En particular, el modelo Qwen2.5-14B-Instruct-1M tiene una ventaja clara sobre GPT-4o-mini, proporcionando soporte para un contexto de ocho veces más largo.",
      "upvotes": 7,
      "discussionId": "67986c84b5e71350993d2974"
    },
    "publishedAt": "2025-01-28T00:35:46.871Z",
    "title": "Qwen2.5-1M Technical Report",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15383.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.15570",
      "authors": [
        {
          "_id": "679843ae7d7b7f8196c61ab7",
          "name": "Lin Yueyu",
          "hidden": false
        },
        {
          "_id": "679843ae7d7b7f8196c61ab8",
          "name": "Li Zhiyuan",
          "hidden": false
        },
        {
          "_id": "679843ae7d7b7f8196c61ab9",
          "name": "Peter Yue",
          "hidden": false
        },
        {
          "_id": "679843ae7d7b7f8196c61aba",
          "user": {
            "_id": "6176b32847ee6431f632981e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg",
            "isPro": false,
            "fullname": "IvanD",
            "user": "xiaol",
            "type": "user"
          },
          "name": "Liu Xiao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-01-28T02:44:02.658Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-26T15:56:56.000Z",
      "title": "ARWKV: No se necesita aprendizaje, modelo de lenguaje basado en Attention de RNN nacido de Transformer",
      "summary": "Hasta ahora como se ha conocido, modelos de atención Gavalaide CoverBick y SubCoverBick combinados en múltiples arquitecturas de cabezas superan a los modelos Transformer y Linear RNN, centrandose en la reducción de complejidad KV y la mejora de la eficiencia. Además, se presentan las series de modelos basados en atención RWKV-7, desde Qwen 2.5, que promueven la investigación de representación. Estos modelos elevan la representación de los RNN y tienen como objetivo superar la capacidad de seguimiento del Transformer. Además, QRWK 32B, basada en la arquitectura RWKV-6, utilizando 16 proteínas 300X GPU, limita el tiempo de procesamiento de conocimiento a 8 horas mientras mantiene el rendimiento de Qwen 2.5, permitiendo así reducir significativamente el tiempo de procesamiento de conocimiento. De hecho, el proceso de distillación no está limitado a la clase de modelos de LLM y facilita la transmisión de conocimiento de un LLM grande a uno pequeño. Se compartirán los detalles de este proceso y retroalimentación sobre la construcción de modelos de base potentes. Esta es una tarea en curso que se actualizará continuamente. Los checkpoints de modelos y código fuente están disponibles en la URL proporcionada.",
      "upvotes": 3,
      "discussionId": "679843af7d7b7f8196c61b21"
    },
    "publishedAt": "2025-01-28T03:02:56.062Z",
    "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15570.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6176b32847ee6431f632981e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg",
      "fullname": "IvanD",
      "name": "xiaol",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 81
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.16142",
      "authors": [
        {
          "_id": "67986cbc7dbf69e4e38539b7",
          "name": "Scott Fujimoto",
          "hidden": false
        },
        {
          "_id": "67986cbc7dbf69e4e38539b8",
          "name": "Pierluca D'Oro",
          "hidden": false
        },
        {
          "_id": "67986cbc7dbf69e4e38539b9",
          "name": "Amy Zhang",
          "hidden": false
        },
        {
          "_id": "67986cbc7dbf69e4e38539ba",
          "name": "Yuandong Tian",
          "hidden": false
        },
        {
          "_id": "67986cbc7dbf69e4e38539bb",
          "name": "Michael Rabbat",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T15:36:37.000Z",
      "title": "Versión en español:\n\nHacia Aprendizaje de Refuerzo Sin Modelo de Propósito General",
      "summary": "El aprendizaje por refuerzo (RL) promete un marco para resolver problemas recientes. De hecho, los algoritmos de RL han demostrado su efectividad en ciertos marcos de prueba, dependiendo en su mejora de parámetros iniciales ajustados y la elección del algoritmo. Recientemente, los métodos de RL basados en modelos han mostrado resultados impresionantes en todo el conjunto de pruebas debido a su complejidad y tiempo de ejecución lento, lograndolo mediante el uso de objetos de tarea basados en modelos para reducir los costos asociados con la planificación y lógica de cálculo. En este artículo, se busca explorar algoritmos profundos de RL sin modelos que sean uniformes en diferentes áreas y configuraciones de problemas. Para lograrlo, se utiliza una representación basada en modelos, se aproxima la función de valor con líneas lineales y se reducen los costos asociados con la planificación y lógica de cálculo mediante el uso de objetos de tarea más detallados en el RL basado en modelos. Nuestro algoritmo, MR.Q, se evaluó en diferentes marcos de prueba generales con un solo conjunto de parámetros iniciales, mostrando una competencia con baselines especializados en áreas y generales, y proporcionando una etapa concreta para la construcción de un RL profundo sin modelos.",
      "upvotes": 2,
      "discussionId": "67986cbf7dbf69e4e3853a89"
    },
    "publishedAt": "2025-01-28T00:36:09.186Z",
    "title": "Towards General-Purpose Model-Free Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.15369",
      "authors": [
        {
          "_id": "6798706dabdc35456a92212d",
          "name": "Chuanyang Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-26T02:34:58.000Z",
      "title": "iFormer: Integración de ConvNet y Transformer para aplicaciones móviles",
      "summary": "Presentamos una nueva familia de redes visuales híbridas móviles, denominada iFormer, diseñada para optimizar el tiempo de respuesta y la precisión en aplicaciones móviles. iFormer integra efectivamente la capacidad de representación local rápida de redes de aprendizaje profundo con la capacidad de modelado global eficiente de auto-atención. La interacción local se extrae transformando la red de convolución estándar ConvNeXt para diseñar una red móvil más ligera. Introducimos una nueva modulación de atención móvil para eliminar las operaciones memoria-costosas de la MHA y mejorar la capacidad de representación global dinámica mediante un mecanismo de modulación eficiente. Hemos realizado experimentos exhaustivos para demostrar que iFormer supera a redes ligeras existentes en diversas tareas. En particular, alcanza una precisión de Top-1 de 80.4% en ImageNet-1k y muestra un desempeño notable al procesar altas resoluciones de entrada en tiempos de respuesta de 1.10 ms en iPhone 13. Además, ha mostrado significativas mejoras en tareas de detección de objetos, segmentación de instancias y segmentación semántica en ADE20k, manteniendo bajos tiempos de respuesta en dispositivos móviles.",
      "upvotes": 1,
      "discussionId": "6798706eabdc35456a92215a"
    },
    "publishedAt": "2025-01-28T00:51:51.263Z",
    "title": "iFormer: Integrating ConvNet and Transformer for Mobile Application",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15369.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16295",
      "authors": [
        {
          "_id": "67986cd6bdc99911a989b0a5",
          "name": "Weixin Liang",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0a6",
          "name": "Junhong Shen",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0a7",
          "name": "Genghan Zhang",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0a8",
          "name": "Ning Dong",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0a9",
          "name": "Luke Zettlemoyer",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0aa",
          "name": "Lili Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T18:35:05.000Z",
      "title": "Mixture-of-Mamba: Método de fortalecimiento de un modelo estado espacial multimodelo con características de sparseness en la modalidad",
      "summary": "Los Modelos de Espacio Estado (SSMs) surgen como una nueva metodología para modelar secuencias, eficientemente reemplazando a los Transformers, pero su rendimiento en aprendizaje de múltiples modelos está limitado porque no se pueden explotar las características únicas de cada modelo. En este artículo, proponemos una nueva arquitectura SSM llamada \"Mixture-of-Mamba\" para aprovechar las características únicas de cada modelo. Esta arquitectura se basa en Mixture-of-Transformers (W. Liang et al., arXiv:2411.04996; 2024) y busca expandir las ventajas de las características únicas de cada modelo en SSMs, manteniendo la eficiencia computacional. La Mixture-of-Mamba evaluó tres configuraciones de aprendizaje de múltiples modelos: Transfusion (texto y tokens de imágenes continuas), Chameleon (texto y tokens de imágenes binarias), y el incluso de voz. En las fases iniciales de entrenamiento, logró alcanzar valores de pérdida iguales y observó un significativo descenso en el costo computacional. En el entorno Transfusion, con un tamaño de 1.4B, se alcanzó una pérdida similar a la imagen con 34.76% de FLOP. En Chameleon, con el mismo tamaño, se alcanzó una pérdida similar a la imagen con 42.50% de FLOP y una pérdida similar a la texto con 65.40% de FLOP. En la configuración de tres modelos, con el mismo tamaño, se alcanzó una pérdida similar a la voz con 24.80% de FLOP. Estos resultados contribuyen a diseñar una arquitectura eficiente que extiende a SSMs, explotando las características únicas de cada modelo, y establece un nuevo estándar de prueba para el aprendizaje de múltiples modelos. El código está disponible en https://github.com/Weixin-Liang/Mixture-of-Mamba.",
      "upvotes": 1,
      "discussionId": "67986cd7bdc99911a989b0ea"
    },
    "publishedAt": "2025-01-28T00:36:31.841Z",
    "title": "Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.14912",
      "authors": [
        {
          "_id": "67986d764fccd4b95149db0b",
          "name": "Juan Ramirez",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db0c",
          "name": "Ignacio Hounie",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db0d",
          "name": "Juan Elenter",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db0e",
          "name": "Jose Gallego-Posada",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db0f",
          "name": "Meraj Hashemizadeh",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db10",
          "name": "Alejandro Ribeiro",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db11",
          "name": "Simon Lacoste-Julien",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-24T20:39:38.000Z",
      "title": "Feasible Learning\n\nEsto significa el significado de aprendizaje práctico.",
      "summary": "Introducing the Sample-Centric Learning Paradigm through Feasible Learning (FL). In this paradigm, the model trains by solving the feasibility problem of limiting the loss of each training sample. Compared to the general Empirical Risk Minimization (ERM) framework, which optimizes average performance, FL demands performance for every individual data point. Models that satisfy a specified performance slope are recognized as valid solutions in FL, playing a crucial role in the selection of optimization algorithms and the characteristics of dynamic solutions. Particularly, research is conducted on an approach that dynamically reevaluates the importance of each sample during actual training. Introducing exceptions to FL that include minimal norm slack variables helps address significant challenges in setting meaningful slopes in practical settings. Experimental analyses, including image classification, age prediction, and preference optimization of large-scale language models, show that while FL models have a slight impact on average performance compared to ERM, they demonstrate that models trained with FL learn from data and improve bias, thereby validating the advantages of FL.",
      "upvotes": 0,
      "discussionId": "67986d784fccd4b95149db6b"
    },
    "publishedAt": "2025-01-28T00:39:11.423Z",
    "title": "Feasible Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.14912.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  }
]