[
  {
    "paper": {
      "id": "2503.13358",
      "authors": [
        {
          "_id": "67dd2ed0d2550735426e7b6f",
          "user": {
            "_id": "64a42977250bfdecd9570a9e",
            "avatarUrl": "/avatars/df5d7cf159e6bb9e961e1c77d1b89d36.svg",
            "isPro": false,
            "fullname": "Daniil Selikhanovych",
            "user": "apryc1",
            "type": "user"
          },
          "name": "Daniil Selikhanovych",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-21T09:18:55.946Z",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b70",
          "name": "David Li",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b71",
          "name": "Aleksei Leonov",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b72",
          "name": "Nikita Gushchin",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b73",
          "name": "Sergei Kushneriuk",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b74",
          "name": "Alexander Filippov",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b75",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b76",
          "name": "Iaroslav Koshelev",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b77",
          "name": "Alexander Korotin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T16:44:08.000Z",
      "submittedOnDailyAt": "2025-03-21T07:50:23.779Z",
      "title": "Un diseño de superresolución de imágenes mediante el shift diferencial residual una sola pasada",
      "submittedOnDailyBy": {
        "_id": "64a42977250bfdecd9570a9e",
        "avatarUrl": "/avatars/df5d7cf159e6bb9e961e1c77d1b89d36.svg",
        "isPro": false,
        "fullname": "Daniil Selikhanovych",
        "user": "apryc1",
        "type": "user"
      },
      "summary": "Modelos de Deep Learning para Super Resolución (SR) pueden elevar la calidad visual, pero presentan un problema de alto costo computacional. Se han desarrollado diversos métodos para hacer que estos modelos de SR se ejecuten rápidamente, pero algunos no pueden generar detalles visuales realistas o pueden mal construir la estructura. Para resolver estos problemas, proponemos un nuevo método de entrenamiento para el modelo de SR basado en Deep Learning llamado ResShift. Nuestro método se basa en entrenar una red neuronal estudiante a la misma altura que el modelo de entrenamiento. RSD implementa la derretización en un solo paso y muestra un rendimiento que supera significativamente al modelo de entrenamiento. Nuestro método de entrenamiento supera a otros métodos de entrenamiento para ResShift (SinSR) y alcanza niveles comparables a los más recientes métodos de entrenamiento de Deep Learning para SR. Comparado con métodos de SR que utilizan modelos de predicción de texto a imagen, RSD ofrece una calidad visual competitiva y mejora su respuesta a imágenes de entrada con calidad baja, al mismo tiempo que reduce el número de parámetros y la uso de memoria GPU. Presentamos resultados de experimentos en conjuntos de datos reales y sintéticos como RealSR, RealSet65, DRealSR, ImageNet y DIV2K.",
      "upvotes": 39,
      "discussionId": "67dd2ed7d2550735426e7d7f",
      "ai_keywords": [
        "diffusion models",
        "super-resolution (SR)",
        "ResShift",
        "distillation method",
        "fake ResShift model",
        "single-step restoration",
        "SinSR",
        "perceptual quality",
        "degraded input images",
        "parameters",
        "GPU memory"
      ]
    },
    "publishedAt": "2025-03-17T12:44:08.000Z",
    "title": "One-Step Residual Shifting Diffusion for Image Super-Resolution via\n  Distillation",
    "summary": "Diffusion models for super-resolution (SR) produce high-quality visual\nresults but require expensive computational costs. Despite the development of\nseveral methods to accelerate diffusion-based SR models, some (e.g., SinSR)\nfail to produce realistic perceptual details, while others (e.g., OSEDiff) may\nhallucinate non-existent structures. To overcome these issues, we present RSD,\na new distillation method for ResShift, one of the top diffusion-based SR\nmodels. Our method is based on training the student network to produce such\nimages that a new fake ResShift model trained on them will coincide with the\nteacher model. RSD achieves single-step restoration and outperforms the teacher\nby a large margin. We show that our distillation method can surpass the other\ndistillation-based method for ResShift - SinSR - making it on par with\nstate-of-the-art diffusion-based SR distillation methods. Compared to SR\nmethods based on pre-trained text-to-image models, RSD produces competitive\nperceptual quality, provides images with better alignment to degraded input\nimages, and requires fewer parameters and GPU memory. We provide experimental\nresults on various real-world and synthetic datasets, including RealSR,\nRealSet65, DRealSR, ImageNet, and DIV2K.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13358.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a42977250bfdecd9570a9e",
      "avatarUrl": "/avatars/df5d7cf159e6bb9e961e1c77d1b89d36.svg",
      "fullname": "Daniil Selikhanovych",
      "name": "apryc1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16419",
      "authors": [
        {
          "_id": "67dcdbfc71027d42fa46e3f2",
          "name": "Yang Sui",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f3",
          "name": "Yu-Neng Chuang",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f4",
          "name": "Guanchu Wang",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f5",
          "name": "Jiamu Zhang",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f6",
          "name": "Tianyi Zhang",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f7",
          "name": "Jiayi Yuan",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f8",
          "name": "Hongyi Liu",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f9",
          "name": "Andrew Wen",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3fa",
          "name": "Shaochen",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3fb",
          "name": "Zhong",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3fc",
          "name": "Hanjie Chen",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3fd",
          "name": "Xia Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:38.000Z",
      "submittedOnDailyAt": "2025-03-21T01:56:58.604Z",
      "title": "STOP・OVER・TENSION: INVESTIGACIÓN SOBRE LAS RAZONES EFICIENTES DE LOS MODELOS DE IDIOMAS A GRANDES ESCALAS",
      "submittedOnDailyBy": {
        "_id": "63787b13500186f250ba377c",
        "avatarUrl": "/avatars/96bb6051662109e9cd25e6df7d738f17.svg",
        "isPro": false,
        "fullname": "yangsui",
        "user": "yangsui",
        "type": "user"
      },
      "summary": "Los grandes modelos de lenguaje (LLMs) muestran una excelente capacidad en tareas complejas. El desarrollo reciente de los grandes modelos de lógica (LRMs), especialmente OpenAI o1 y DeepSeek-R1, ha mejorado el rendimiento en áreas lógicas de sistemas 2, como matemáticas o programación. Estos modelos han sido fortalecidos mediante la técnica de aprendizaje supervisado (SFT) y aprendizaje por refuerzo (RL), fortaleciendo el razonamiento por filosofía de la mente (CoT). Sin embargo, las secuencias largas de CoT pueden usar excesivamente la capacidad de cálculo, lo que se conoce como \"overtuning\". Este artículo investiga sistemáticamente el desarrollo actual de la lógica eficiente de los LLMs y ofrece una investigación estructurada para explorar posibles soluciones. En general, se basa en la estructura propia de los LLMs y clasifica los avances actuales en algunas direcciones importantes: (1) lógica eficiente basada en el modelo, considerando la conversión de modelos de lógica larga a modelos de lógica más sencillos y la aprendizaje directo de modelos de lógica eficiente; (2) lógica eficiente basada en la salida de lógica, con el objetivo de reducir dinámicamente las etapas y longitud de las razones durante la inferencia; (3) lógica eficiente basada en los input prompts, intentando fortalecer la eficiencia de la lógica basándose en las características de los input prompts (por ejemplo, control de dificultad o longitud). Además, se discuten el aprendizaje de modelos de lógica basado en datos eficientes, la capacidad lógica de modelos pequeños, métodos de evaluación y marcos de referencia.",
      "upvotes": 27,
      "discussionId": "67dcdbfd71027d42fa46e439",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Large Reasoning Models (LRMs)",
        "OpenAI o1",
        "DeepSeek-R1",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "Chain-of-Thought (CoT) reasoning",
        "overthinking phenomenon",
        "model-based efficient reasoning",
        "reasoning output-based efficient reasoning",
        "input prompts-based efficient reasoning",
        "efficient data",
        "small language models",
        "evaluation methods",
        "benchmarking"
      ]
    },
    "publishedAt": "2025-03-20T13:59:38.000Z",
    "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63787b13500186f250ba377c",
      "avatarUrl": "/avatars/96bb6051662109e9cd25e6df7d738f17.svg",
      "fullname": "yangsui",
      "name": "yangsui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16302",
      "authors": [
        {
          "_id": "67dce2d2068292e7ef79b3dd",
          "name": "Zeqiang Lai",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3de",
          "name": "Yunfei Zhao",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3df",
          "name": "Zibo Zhao",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e0",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e1",
          "name": "Fuyun Wang",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e2",
          "name": "Huiwen Shi",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e3",
          "name": "Xianghui Yang",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e4",
          "name": "Qinxiang Lin",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e5",
          "name": "Jinwei Huang",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e6",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e7",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e8",
          "name": "Chunchao Guo",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e9",
          "name": "Xiangyu Yue",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63044b89eedc089484c995ad/ukrgJYM5cBYEzAo7b9J7U.mp4"
      ],
      "publishedAt": "2025-03-20T16:23:44.000Z",
      "submittedOnDailyAt": "2025-03-21T02:25:30.177Z",
      "title": "Utilizamos un modelo de deep learning de Vecset para realizar la generación de formas de alta velocidad.",
      "submittedOnDailyBy": {
        "_id": "63044b89eedc089484c995ad",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
        "isPro": false,
        "fullname": "Zeqiang Lai",
        "user": "ZeqiangLai",
        "type": "user"
      },
      "summary": "La generación de formas 3D ha avanzado significativamente a través del desarrollo de distribuciones \"nativas\" 3D, con el modelo de distribución Vecset (VDM) desempeñando un papel esencial. Las últimas innovaciones han demostrado resultados excelentes en la generación de formas 3D de alta resolución, pero el VDM enfrenta desafíos en la generación rápida. Para resolver estos problemas, hemos identificado que la aceleración de la muestra de distribuciones y la validación del área de VAE son causas clave. Para abordar estos desafíos, proponemos un marco sistemático que acelera FlashVDM, VAE y DiT. En DiT, FlashVDM utiliza una nueva técnica de entrenamiento progresivo y aprendizaje por transferencia para permitir una muestra flexible de distribuciones en 5 etapas, asegurando un alto nivel de calidad. En VAE, introducimos una validación de vecset con Adaptive KV Selection, Decodificación Volumetrica Hierárquica y Diseño de Red Neuronal Eficiente, que utilizan la localidad y la sparseness de las superficies de formas de vecset para reducir significativamente los FLOPs y minimizar el overhead de validación. Al aplicar FlashVDM en Hunyuan3D-2, obtenemos Hunyuan3D-2 Turbo. Mediante evaluaciones sistemáticas, nuestro modelo supera significativamente los métodos de generación 3D rápida existentes y alcanza un rendimiento relativo superior, reduciendo el tiempo de reconstrucción en más de 45 veces y el tiempo de generación en más de 32 veces. El código y el modelo están disponibles en la siguiente URL.\nhttps://github.com/Tencent/FlashVDM",
      "upvotes": 23,
      "discussionId": "67dce2d6068292e7ef79b556",
      "githubRepo": "https://github.com/Tencent/FlashVDM",
      "ai_keywords": [
        "3D diffusion",
        "Vecset Diffusion Model (VDM)",
        "diffusion sampling",
        "VAE",
        "DiT",
        "Progressive Flow Distillation",
        "lightning vecset decoder",
        "Adaptive KV Selection",
        "Hierarchical Volume Decoding",
        "Efficient Network Design",
        "FLOPs",
        "decoding overhead",
        "Hunyuan3D-2",
        "Hunyuan3D-2 Turbo"
      ]
    },
    "publishedAt": "2025-03-20T12:23:44.000Z",
    "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
    "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63044b89eedc089484c995ad/ukrgJYM5cBYEzAo7b9J7U.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16302.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63044b89eedc089484c995ad",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
      "fullname": "Zeqiang Lai",
      "name": "ZeqiangLai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14487",
      "authors": [
        {
          "_id": "67da83d1b05eff6d87a41f81",
          "user": {
            "_id": "662887715d246621f33d2ce6",
            "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg",
            "isPro": false,
            "fullname": "Shi Minglei",
            "user": "MingleiShi",
            "type": "user"
          },
          "name": "Minglei Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:43:55.360Z",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f82",
          "name": "Ziyang Yuan",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f83",
          "name": "Haotian Yang",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f84",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f85",
          "name": "Mingwu Zheng",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f86",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f87",
          "name": "Wenliang Zhao",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f88",
          "name": "Wenzhao Zheng",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f89",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f8a",
          "name": "Jiwen Lu",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f8b",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f8c",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f8d",
          "name": "Kun Gai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:57:07.000Z",
      "submittedOnDailyAt": "2025-03-21T06:35:23.843Z",
      "title": "DiffMoE: Selección Dinámica de Tokenes para Transformadores de Difusión Escalables",
      "submittedOnDailyBy": {
        "_id": "662887715d246621f33d2ce6",
        "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg",
        "isPro": false,
        "fullname": "Shi Minglei",
        "user": "MingleiShi",
        "type": "user"
      },
      "summary": "El modelo de difusión funciona con éxito en varias tareas de generación de imágenes, pero su procesamiento de entrada está limitado por condiciones o niveles de ruido. Para resolver estos limites, proponemos un nuevo enfoque llamado DiffMoE. DiffMoE introduce una agrupación de tokens globales a nivel de batch y permite que los expositores accedan a la distribución de tokens global durante el entrenamiento, lo que estimula el comportamiento de expositores especializados. Utiliza un predictor que asigna recursos de computación dinámicamente basados en el nivel de ruido y la complejidad de la muestra. Los resultados de evaluación detallados muestran que DiffMoE alcanza los mejores rendimientos en el marco de ImageNet, superando significativamente a arquitecturas densas con tres veces más parámetros activos y a otros enfoques de MoE, manteniendo un número de parámetros activos igual. Nuestro enfoque es efectivo en tareas más complejas que las generaciones condicionadas por clase y también en tareas más complejas como la generación de imágenes a partir de texto, demostrando una amplia aplicabilidad en diversos modelos de DiffMoE. Página del proyecto: https://shiml20.github.io/DiffMoE/",
      "upvotes": 19,
      "discussionId": "67da83d3b05eff6d87a42049",
      "projectPage": "https://shiml20.github.io/DiffMoE/",
      "githubRepo": "https://github.com/KwaiVGI/DiffMoE",
      "ai_keywords": [
        "diffusion models",
        "ImageNet",
        "batch-level global token pool",
        "experts",
        "global token distributions",
        "capacity predictor",
        "computational resources",
        "noise levels",
        "sample complexity",
        "class-conditional generation",
        "text-to-image generation"
      ]
    },
    "publishedAt": "2025-03-18T13:57:07.000Z",
    "title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers",
    "summary": "Diffusion models have demonstrated remarkable success in various image\ngeneration tasks, but their performance is often limited by the uniform\nprocessing of inputs across varying conditions and noise levels. To address\nthis limitation, we propose a novel approach that leverages the inherent\nheterogeneity of the diffusion process. Our method, DiffMoE, introduces a\nbatch-level global token pool that enables experts to access global token\ndistributions during training, promoting specialized expert behavior. To\nunleash the full potential of the diffusion process, DiffMoE incorporates a\ncapacity predictor that dynamically allocates computational resources based on\nnoise levels and sample complexity. Through comprehensive evaluation, DiffMoE\nachieves state-of-the-art performance among diffusion models on ImageNet\nbenchmark, substantially outperforming both dense architectures with 3x\nactivated parameters and existing MoE approaches while maintaining 1x activated\nparameters. The effectiveness of our approach extends beyond class-conditional\ngeneration to more challenging tasks such as text-to-image generation,\ndemonstrating its broad applicability across different diffusion model\napplications. Project Page: https://shiml20.github.io/DiffMoE/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14487.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "662887715d246621f33d2ce6",
      "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg",
      "fullname": "Shi Minglei",
      "name": "MingleiShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16416",
      "authors": [
        {
          "_id": "67dd1d595fd14aedd30bb94a",
          "name": "Asaf Yehudai",
          "hidden": false
        },
        {
          "_id": "67dd1d595fd14aedd30bb94b",
          "name": "Lilach Eden",
          "hidden": false
        },
        {
          "_id": "67dd1d595fd14aedd30bb94c",
          "name": "Alan Li",
          "hidden": false
        },
        {
          "_id": "67dd1d595fd14aedd30bb94d",
          "name": "Guy Uziel",
          "hidden": false
        },
        {
          "_id": "67dd1d595fd14aedd30bb94e",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "67dd1d595fd14aedd30bb94f",
          "name": "Roy Bar-Haim",
          "hidden": false
        },
        {
          "_id": "67dd1d595fd14aedd30bb950",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "67dd1d595fd14aedd30bb951",
          "name": "Michal Shmueli-Scheuer",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:23.000Z",
      "submittedOnDailyAt": "2025-03-21T06:34:12.447Z",
      "title": "Investigación de Evaluación de Agentes Basados en LLM",
      "submittedOnDailyBy": {
        "_id": "638324f862badff43269e588",
        "avatarUrl": "/avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg",
        "isPro": false,
        "fullname": "Asaf Yehudai",
        "user": "Asaf-Yehudai",
        "type": "user"
      },
      "summary": "El advenimiento de agentes basados en LLM significa una transición del paradigma de la IA, permitiendo que sistemas de planificación automática planifiquen estratégicamente, presenten razones, utilizen herramientas, mantengan memoria y interactuar de manera dinámica con entornos. Este artículo ofrece una primera investigación detallada sobre los métodos de evaluación de estas capacidades crecientes. Analizamos sistemáticamente en cuatro aspectos importantes: capacidades básicas del agente, marcos de referencia propios de aplicaciones, marcos de referencia generales para agentes, y los marcos de evaluación utilizados para agentes. En la análisis se destacan tendencias hacia evaluaciones realistas y difíciles, así como la tendencia hacia marcos de referencia actualizados continuamente. Además, se identifican defectos cruciales para futuros estudios. En particular, se necesitan métodos de evaluación eficientes en términos de costo, seguros, robustos y escalables. Esta investigación mapea rápidamente el camino de la evaluación de agentes, destaca las tendencias en el campo, identifica limitaciones actuales y propone direcciones para futuros estudios.",
      "upvotes": 17,
      "discussionId": "67dd1d5a5fd14aedd30bb999",
      "ai_keywords": [
        "LLM-based agents",
        "planning",
        "tool use",
        "self-reflection",
        "memory",
        "evaluation benchmarks",
        "evaluation frameworks",
        "fundamental agent capabilities",
        "application-specific benchmarks",
        "web agents",
        "software engineering agents",
        "scientific agents",
        "conversational agents",
        "generalist agents",
        "cost-efficiency",
        "safety",
        "robustness",
        "fine-grained evaluation methods",
        "scalable evaluation methods"
      ]
    },
    "publishedAt": "2025-03-20T13:59:23.000Z",
    "title": "Survey on Evaluation of LLM-based Agents",
    "summary": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16416.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638324f862badff43269e588",
      "avatarUrl": "/avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg",
      "fullname": "Asaf Yehudai",
      "name": "Asaf-Yehudai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15558",
      "authors": [
        {
          "_id": "67dcadafb2cd7d4f3a266037",
          "name": "NVIDIA",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266039",
          "name": "Alisson Azzolini",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603a",
          "name": "Hannah Brandon",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603b",
          "name": "Prithvijit Chattopadhyay",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603c",
          "name": "Huayu Chen",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603d",
          "name": "Jinju Chu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603e",
          "name": "Yin Cui",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603f",
          "name": "Jenna Diamond",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266040",
          "name": "Yifan Ding",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266041",
          "name": "Francesco Ferroni",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266042",
          "name": "Rama Govindaraju",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266043",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266044",
          "name": "Siddharth Gururani",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266045",
          "name": "Imad El Hanafi",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266046",
          "name": "Zekun Hao",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266047",
          "name": "Jacob Huffman",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266048",
          "name": "Jingyi Jin",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266049",
          "name": "Brendan Johnson",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604a",
          "name": "Rizwan Khan",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604b",
          "name": "George Kurian",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604c",
          "name": "Elena Lantz",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604d",
          "name": "Nayeon Lee",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604e",
          "name": "Zhaoshuo Li",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604f",
          "name": "Xuan Li",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266050",
          "name": "Tsung-Yi Lin",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266051",
          "name": "Yen-Chen Lin",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266052",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266053",
          "name": "Andrew Mathau",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266054",
          "name": "Yun Ni",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266055",
          "name": "Lindsey Pavao",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266056",
          "name": "Wei Ping",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266057",
          "name": "David W. Romero",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266058",
          "name": "Misha Smelyanskiy",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266059",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605a",
          "name": "Lyne Tchapmi",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605b",
          "name": "Andrew Z. Wang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605c",
          "name": "Boxin Wang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605d",
          "name": "Haoxiang Wang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605e",
          "name": "Fangyin Wei",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605f",
          "name": "Jiashu Xu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266060",
          "name": "Yao Xu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266061",
          "name": "Xiaodong Yang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266062",
          "name": "Zhuolin Yang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266063",
          "name": "Xiaohui Zeng",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266064",
          "name": "Zhe Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T22:06:58.000Z",
      "submittedOnDailyAt": "2025-03-21T01:09:51.519Z",
      "title": "Cosmos-Reason1: Conceptos básicos de la física hasta inferencias experientes",
      "submittedOnDailyBy": {
        "_id": "649f05367b57fab3a5b27c8b",
        "avatarUrl": "/avatars/749eca8ba898685c90c305f4e3549ba1.svg",
        "isPro": false,
        "fullname": "Yin Cui",
        "user": "richardaecn",
        "type": "user"
      },
      "summary": "Los sistemas de IA físicos requieren la capacidad de reconocer y comprender el mundo físico y ejecutar acciones complejas. En este artículo, proponemos el modelo Cosmos-Reason1, que utiliza procesos de inferencia de la inteligencia artificial de larga cadena para generar juicios específicos en lenguaje natural (por ejemplo, acciones futuras) a través de la comprensión del mundo físico. Primero, definiremos las capacidades esenciales para la inferencia física y centraremos nuestro enfoque en la comprensión común física y la inferencia específica. Para representar la comprensión común física, utilizaremos una ontología heurística sobre los conceptos básicos de espacio, tiempo y física. Para la inferencia específica, requeriremos una ontología bidimensional para expandir varias especificaciones físicas. Basándonos en estas capacidades, desarrollaremos dos modelos de lenguaje de alto nivel de multimodalidad: Cosmos-Reason1-8B y Cosmos-Reason1-56B. Los datos se entrenan en cuatro etapas: preentrenamiento visual, entrenamiento de subconjuntos generales (SFT), SFT de IA física y retuning de IA física mediante aprendizaje por refuerzo (RL). En la evaluación del modelo, construiremos detallados benchmarks basados en nuestra ontología para la comprensión común física y la inferencia específica. Los resultados de la evaluación muestran notables mejoras debido al SFT y el aprendizaje por refuerzo de la IA física. Para fomentar el desarrollo de la IA física, proporcionaremos nuestro código y modelos preentrenados bajo la licencia NVIDIA Open Model License de forma gratuita.",
      "upvotes": 14,
      "discussionId": "67dcadb1b2cd7d4f3a2660f4",
      "githubRepo": "https://github.com/nvidia-cosmos/cosmos-reason1",
      "ai_keywords": [
        "hierarchical ontology",
        "two-dimensional ontology",
        "multimodal large language models",
        "vision pre-training",
        "long chain-of-thought reasoning",
        "Physical AI SFT",
        "Physical AI reinforcement learning",
        "embodied reasoning",
        "physical common sense"
      ]
    },
    "publishedAt": "2025-03-18T18:06:58.000Z",
    "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning",
    "summary": "Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data\nand train our models in four stages: vision pre-training, general supervised\nfine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL)\nas the post-training. To evaluate our models, we build comprehensive benchmarks\nfor physical common sense and embodied reasoning according to our ontologies.\nEvaluation results show that Physical AI SFT and reinforcement learning bring\nsignificant improvements. To facilitate the development of Physical AI, we will\nmake our code and pre-trained models available under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-reason1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15558.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649f05367b57fab3a5b27c8b",
      "avatarUrl": "/avatars/749eca8ba898685c90c305f4e3549ba1.svg",
      "fullname": "Yin Cui",
      "name": "richardaecn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16212",
      "authors": [
        {
          "_id": "67dcd33626989570158ce8cf",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d0",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d1",
          "name": "Zhuoshi Pan",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d2",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d3",
          "name": "Honglin Lin",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d4",
          "name": "Chenlin Ming",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d5",
          "name": "Xin Gao",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d6",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d7",
          "name": "Rui Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T15:00:41.000Z",
      "submittedOnDailyAt": "2025-03-21T01:18:55.765Z",
      "title": "La capacidad de resolver problemas matemáticos mediante la fusión de LLM: un enfoque innovador para mejorar la resolución de problemas matemáticos.",
      "submittedOnDailyBy": {
        "_id": "6397f6081323f19c578f142e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
        "isPro": false,
        "fullname": "QizhiPei",
        "user": "QizhiPei",
        "type": "user"
      },
      "summary": "Los grandes modelos de lenguaje (LLMs) han demostrado un desarrollo sorprendente en el campo de la lógica matemática. Se espera que la expansión de datos mejore la capacidad de resolver problemas matemáticos; sin embargo, la forma actual de abordarlo está principalmente limitada a cambios a nivel de instancia, fallando en mejorar los métodos de resolución que utilizan la estructura relacional propia de la conocida matemática. También se observa que la perfección en matemáticas en el proceso de aprendizaje humano se desarrolla a través de contactos sistemáticos con conceptos conectados. Por ello, se presenta un nuevo marco de trabajo llamado MathFusion. Este marco tiene como objetivo mejorar la lógica a través de los problemas. MathFusion implementa tres estrategias de fusión: 1. Fusión secuencial conecta problemas relacionados para modelar las relaciones de dependencia de los métodos de resolución. 2. Fusión paralela combina problemas similares para fortalecer la comprensión de conceptos. 3. Fusión condicional genera problemas selectivos relacionados con el contexto para mejorar la flexibilidad lógica. Aplicando estas estrategias, se genera un nuevo conjunto de datos (MathFusionQA) y se fine-tunen modelos (DeepSeekMath-7B, Mistral-7B, Llama3-8B). Los resultados experimentales muestran que MathFusion logra un gran avance en la lógica matemática, con un aumento de precisión en 18.0 puntos en diferentes benchmarks, y necesita solo 45K indicaciones adicionales sin perder eficiencia en el uso de datos. Esto representa un gran avance frente a la forma de acceso a un solo comando. El conjunto de datos, modelos y código están disponibles para uso público. https://github.com/QizhiPei/mathfusion",
      "upvotes": 13,
      "discussionId": "67dcd33726989570158ce90a",
      "githubRepo": "https://github.com/QizhiPei/MathFusion",
      "ai_keywords": [
        "MathFusion",
        "cross-problem instruction synthesis",
        "sequential fusion",
        "parallel fusion",
        "conditional fusion",
        "MathFusionQA",
        "DeepSeekMath-7B",
        "Mistral-7B",
        "Llama3-8B",
        "mathematical reasoning",
        "data efficiency"
      ]
    },
    "publishedAt": "2025-03-20T11:00:41.000Z",
    "title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion",
    "summary": "Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, MathFusionQA, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16212.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6397f6081323f19c578f142e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
      "fullname": "QizhiPei",
      "name": "QizhiPei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16257",
      "authors": [
        {
          "_id": "67dd2cbabf4c007db3bc0b76",
          "name": "Keda Tao",
          "hidden": false
        },
        {
          "_id": "67dd2cbabf4c007db3bc0b77",
          "name": "Haoxuan You",
          "hidden": false
        },
        {
          "_id": "67dd2cbabf4c007db3bc0b78",
          "name": "Yang Sui",
          "hidden": false
        },
        {
          "_id": "67dd2cbabf4c007db3bc0b79",
          "name": "Can Qin",
          "hidden": false
        },
        {
          "_id": "67dd2cbabf4c007db3bc0b7a",
          "name": "Huan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T15:52:43.000Z",
      "submittedOnDailyAt": "2025-03-21T07:41:12.399Z",
      "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language Models\n\n1.x-Bit KV Cache Quantization Plug-and-Play se aplica a los modelos de lenguaje grandes de video.",
      "submittedOnDailyBy": {
        "_id": "62b624f3b52bef716e248fd7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b624f3b52bef716e248fd7/AllcccKH-eBWduA8KVnOQ.png",
        "isPro": false,
        "fullname": "Huan Wang",
        "user": "Huan-WhoRegisteredMyName",
        "type": "user"
      },
      "summary": "VideoLLMs muestra la capacidad de procesar largos videos y analizar complejas causas y razones. Sin embargo, los videos frames contienen miles de tokens visuales que significan que el cache Key-Value (KV) aumenta significativamente la demanda de memoria y provoca notables retrasos en la inferencia y el uso de memoria. La optimización de consultas en el cache KV se utiliza ampliamente para abordar este problema. En este artículo, se muestra que la optimización de consultas en el cache KV con 2 bits no significativamente daña el rendimiento del modelo, y se descubre que la limitación de la optimización del cache KV con un número de bits más bajo no ha sido explorada. Para remediar esto, se introduce un método de optimización de consultas en el cache KV utilizando adelantamiento y retroceso, llamado VidKV. Específicamente, (1) se propone una estrategia de optimización de consultas en el cache KV mixta en la dirección canal, realizando optimización de 2 bits ideal para los canales ideales y combinando FFT con optimización de 1 bit para los canales comunes. (2) Se implementa optimización de consultas en el cache KV de 1.58 bits para los valores, seleccionando de manera selectiva los tokens visuales de mayor importancia para equilibrar mejor la precisión y el rendimiento del modelo. Un punto clave es que VidKV difiere de los métodos de optimización de consultas en el cache KV propuestos en los estudios previos y muestra que el cache de valores en VideoLLMs necesita ser optimizado por canales. Experimentalmente, se verificó en seis marcos de prueba de LLaVA-OV-7B y Qwen2.5-VL-7B que VidKV puede compresar el cache KV a una precisión de 1.5 bits y 1.58 bits frente a FP16, sin afectar significativamente el rendimiento.",
      "upvotes": 11,
      "discussionId": "67dd2cbebf4c007db3bc0cc4",
      "githubRepo": "https://github.com/KD-TAO/VidKV",
      "ai_keywords": [
        "large language models (LLMs)",
        "Video large language models (VideoLLMs)",
        "video frames",
        "key-value (KV) cache",
        "memory requirements",
        "inference speed",
        "KV cache quantization",
        "2-bit KV quantization",
        "VidKV",
        "mixed-precision quantization",
        "channel dimension",
        "anomalous channels",
        "1-bit quantization",
        "FFT",
        "1.58-bit quantization",
        "semantically salient visual tokens",
        "per-channel fashion",
        "per-token fashion",
        "LLaVA-OV-7B",
        "Qwen2.5-VL-7B",
        "benchmarks",
        "FP16 counterparts"
      ]
    },
    "publishedAt": "2025-03-20T11:52:43.000Z",
    "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
    "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16257.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b624f3b52bef716e248fd7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b624f3b52bef716e248fd7/AllcccKH-eBWduA8KVnOQ.png",
      "fullname": "Huan Wang",
      "name": "Huan-WhoRegisteredMyName",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16365",
      "authors": [
        {
          "_id": "67dcdc98e406e84ea880ccab",
          "name": "Muyao Li",
          "hidden": false
        },
        {
          "_id": "67dcdc98e406e84ea880ccac",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67dcdc98e406e84ea880ccad",
          "name": "Kaichen He",
          "hidden": false
        },
        {
          "_id": "67dcdc98e406e84ea880ccae",
          "name": "Xiaojian Ma",
          "hidden": false
        },
        {
          "_id": "67dcdc98e406e84ea880ccaf",
          "name": "Yitao Liang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/QK0LQhSbfUSqe9FuDmxJu.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/VHbOI8bWxJLDd1aOBjoRT.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/xBWzJPEtxSx-8agJJeIXc.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/y2QTnsahG2XEhyG35nhab.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/W_gV_JuPtKvZjdI9Wv1Jb.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/YDIIWH_7nTy1Xm2pTRt5B.mp4"
      ],
      "publishedAt": "2025-03-20T17:21:58.000Z",
      "submittedOnDailyAt": "2025-03-21T01:59:54.135Z",
      "title": "JARVIS-VLA: Utilizamos un grande modelo de lenguaje visuolingüístico post-entrenado para jugar a los juegos con teclado y ratón.",
      "submittedOnDailyBy": {
        "_id": "642e8c99c1b0f8e4e76bcaab",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e8c99c1b0f8e4e76bcaab/BOs9r0P9KyT9pEba9v0H4.png",
        "isPro": false,
        "fullname": "Zihao Wang",
        "user": "zhwang4ai",
        "type": "user"
      },
      "summary": "Recientemente, las políticas de decisión basadas en acciones han recibido atención en entornos abiertos. El modelo de acción visual long-range (VLA) ha demostrado resultados deseados en tareas de políticas de decisión al haber sido entrenado previamente con un gran conjunto de datos web. Sin embargo, la mayoría de los estudios previos se centran principalmente en la entrenamiento posterior a la acción, dejando poco atención a la mejora de la propia arquitectura de modelo. En este contexto, presentamos una nueva metodología llamada \"entrenamiento después de la acción para acciones\", que utiliza guías de visión y lenguaje para mejorar automáticamente modelos de visión y lenguaje (VLMs). Esta mejora mejora las capacidades de conocimiento mundial, reconocimiento visual y reconocimiento espacial en entornos abiertos. Basándonos en este paradigma de entrenamiento posterior, obtuvimos el primer modelo VLA en Minecraft. Este modelo puede seguir instrucciones humanas en más de 1000 tareas elementales diferentes. Nuestros experimentos muestran que el entrenamiento posterior en tareas no tarotísticas mejora significativamente la capacidad de los agentes basados en acciones en un 40% aproximado. Además, nuestro enfoque supera las políticas tradicionales de aprendizaje por refuerzo en Minecraft y alcanza los mejores rendimientos. Hemos abierto nuestro código, modelos y conjuntos de datos como recursos abiertos, proporcionando una base para la investigación continua. El sitio web del proyecto está disponible en https://craftjarvis.github.io/JarvisVLA.",
      "upvotes": 10,
      "discussionId": "67dcdc9ce406e84ea880ce67",
      "projectPage": "https://craftjarvis.github.io/JarvisVLA/",
      "githubRepo": "https://github.com/CraftJarvis/JarvisVLA",
      "ai_keywords": [
        "Visual Language Action (VLA) models",
        "Visual Language Models (VLMs)",
        "self-supervised manner",
        "world knowledge",
        "visual recognition",
        "spatial grounding",
        "atomic tasks",
        "crafting",
        "smelting",
        "cooking",
        "mining",
        "killing",
        "non-trajectory tasks",
        "imitation learning-based policies"
      ]
    },
    "publishedAt": "2025-03-20T13:21:58.000Z",
    "title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play\n  Visual Games with Keyboards and Mouse",
    "summary": "Recently, action-based decision-making in open-world environments has gained\nsignificant attention. Visual Language Action (VLA) models, pretrained on\nlarge-scale web datasets, have shown promise in decision-making tasks. However,\nprevious work has primarily focused on action post-training, often neglecting\nenhancements to the foundational model itself. In response, we introduce a\nnovel approach, Act from Visual Language Post-Training, which refines Visual\nLanguage Models (VLMs) through visual and linguistic guidance in a\nself-supervised manner. This enhancement improves the models' capabilities in\nworld knowledge, visual recognition, and spatial grounding in open-world\nenvironments. Following the above post-training paradigms, we obtain the first\nVLA models in Minecraft that can follow human instructions on over 1k different\natomic tasks, including crafting, smelting, cooking, mining, and killing. Our\nexperiments demonstrate that post-training on non-trajectory tasks leads to a\nsignificant 40% improvement over the best agent baseline on a diverse set of\natomic tasks. Furthermore, we demonstrate that our approach surpasses\ntraditional imitation learning-based policies in Minecraft, achieving\nstate-of-the-art performance. We have open-sourced the code, models, and\ndatasets to foster further research. The project page can be found in\nhttps://craftjarvis.github.io/JarvisVLA.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/QK0LQhSbfUSqe9FuDmxJu.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/VHbOI8bWxJLDd1aOBjoRT.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/xBWzJPEtxSx-8agJJeIXc.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/y2QTnsahG2XEhyG35nhab.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/W_gV_JuPtKvZjdI9Wv1Jb.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/YDIIWH_7nTy1Xm2pTRt5B.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16365.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e8c99c1b0f8e4e76bcaab",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e8c99c1b0f8e4e76bcaab/BOs9r0P9KyT9pEba9v0H4.png",
      "fullname": "Zihao Wang",
      "name": "zhwang4ai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16356",
      "authors": [
        {
          "_id": "67dcd18ad2550735425351bf",
          "name": "Yunzhi Yao",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c0",
          "name": "Jizhan Fang",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c1",
          "name": "Jia-Chen Gu",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c2",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c3",
          "name": "Shumin Deng",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c4",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c5",
          "name": "Nanyun Peng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/aC9fMp8dvcRIzvms4CAu_.png"
      ],
      "publishedAt": "2025-03-20T17:14:34.000Z",
      "submittedOnDailyAt": "2025-03-21T01:12:07.189Z",
      "title": "CaKE: La edición de circuitos facilita el aprendizaje de conocimientos generales.",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "La Edición de Conocimiento (KE) permite modificar información antigua o incorrecta en modelos de lenguaje de gran escala (LLMs). Los métodos existentes de KE pueden actualizar información individual, pero generalizar estas actualizaciones a tareas de inferencia multi-paso es difícil. Al analizar los caminos neuronales (neural pathways) y observar cómo LLMs realizan inferencia basada en conocimiento, se ha confirmado que el enfoque actual de KE local entre capas (como MEMIT y WISE) tiene dificultades para integrar efectivamente la información actualizada en estos caminos neuronales. Para resolver estos límites, se propone CaKE (Circuit-aware Knowledge Editing). CaKE promueve el desarrollo de caminos neuronales adecuados para nuevas informaciones, utilizando datos personalizados estratégicamente basados en un análisis centrado en la causa, y permite la integración efectiva de la información actualizada en LLMs. Los resultados de los experimentos muestran que CaKE permite usar la información actualizada de manera modificada en tareas relacionadas y mejora la precisión de inferencia multi-paso en el conjunto de datos MQuAKE en un promedio de 20% más que los métodos de KE actuales. El código y los datos de CaKE están disponibles en https://github.com/zjunlp/CaKE.",
      "upvotes": 9,
      "discussionId": "67dcd18bd255073542535223",
      "githubRepo": "https://github.com/zjunlp/CaKE",
      "ai_keywords": [
        "Knowledge Editing (KE)",
        "large language models (LLMs)",
        "multi-hop reasoning tasks",
        "reasoning circuits",
        "neural pathways",
        "knowledge-based inference",
        "MEMIT",
        "WISE",
        "layer-localized KE approaches",
        "CaKE (Circuit-aware Knowledge Editing)",
        "strategically curated data",
        "circuits-based analysis",
        "MQuAKE dataset"
      ]
    },
    "publishedAt": "2025-03-20T13:14:34.000Z",
    "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
    "summary": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/aC9fMp8dvcRIzvms4CAu_.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16356.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16418",
      "authors": [
        {
          "_id": "67dcd0fe1f94b594ef4f3e8e",
          "name": "Liming Jiang",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e8f",
          "name": "Qing Yan",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e90",
          "name": "Yumin Jia",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e91",
          "name": "Zichuan Liu",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e92",
          "name": "Hao Kang",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e93",
          "name": "Xin Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:34.000Z",
      "submittedOnDailyAt": "2025-03-21T02:41:07.989Z",
      "title": "Infinite Yu: Mantener la identidad mientras reorganizar fotos de manera flexible",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "InfU es una de las tecnologías más avanzadas de Transformadores de Difusión (DiTs), un marco avanzado que permite la generación de imágenes de alta calidad y alta flexibilidad con protección de identidad. InfU se esfuerza para resolver problemas como la insuficiencia de similitud del reconocimiento, la disminución de la correspondencia entre texto y imagen, la calidad y la artesanía de la generación, entre otros. Uno de los componentes clave de InfU es el modelo basado en DiT, donde se inyecta características del reconocimiento a través de una conexión residual, lo que permite mejorar la similitud del reconocimiento mientras mantiene la capacidad de generación. Además, InfU utiliza una estrategia de entrenamiento multinivel y métodos para mejorar la correspondencia entre texto y imagen, así como la calidad de las imágenes, incluyendo el problema de copia y pega de rostros, utilizando datos de múltiples ejemplos de un solo texto (SPMS) para entrenamiento previo y ajuste de habitación (SFT). Los resultados de los experimentos muestran que InfU supera los estándares existentes y logra performances más avanzadas. Además, el diseño de portafolio de InfU mantiene la correspondencia con diversos métodos y contribuye a una comunidad más amplia.",
      "upvotes": 8,
      "discussionId": "67dcd1001f94b594ef4f3f44",
      "ai_keywords": [
        "Diffusion Transformers (DiTs)",
        "FLUX",
        "InfiniteYou (InfU)",
        "InfuseNet",
        "residual connections",
        "synthetic single-person-multiple-sample (SPMS) data",
        "pretraining",
        "supervised fine-tuning (SFT)"
      ]
    },
    "publishedAt": "2025-03-20T13:59:34.000Z",
    "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
    "summary": "Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16418.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6415
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16322",
      "authors": [
        {
          "_id": "67dce4c10784200359ab2494",
          "name": "Ruonan Yu",
          "hidden": false
        },
        {
          "_id": "67dce4c10784200359ab2495",
          "name": "Songhua Liu",
          "hidden": false
        },
        {
          "_id": "67dce4c10784200359ab2496",
          "name": "Zhenxiong Tan",
          "hidden": false
        },
        {
          "_id": "67dce4c10784200359ab2497",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T16:44:43.000Z",
      "submittedOnDailyAt": "2025-03-21T02:36:06.925Z",
      "title": "Transformación Sencilla del Patrón de Edad en Alta Resolución",
      "submittedOnDailyBy": {
        "_id": "6486fb33570a419f41a882e4",
        "avatarUrl": "/avatars/860a42074439a23c629cd23851ae4da6.svg",
        "isPro": false,
        "fullname": "Ruonan Yu",
        "user": "roseannelexie",
        "type": "user"
      },
      "summary": "El modelo de diseno de imágenes de alta resolución ha experimentado un desarrollo notable en los últimos años. Sin embargo, la entrenamiento de modelos para la generación de imágenes de alta resolución puede resultar particularmente difícil cuando se limitan los datos de entrenamiento y los recursos computacionales. En este artículo, se investigan estos problemas prácticos desde dos perspectivas clave: la eficiencia de los datos y la eficiencia de los parámetros, y se propone un conjunto de pautas clave llamado URAE, que se conoce como \"Ultra-Resolution Adapter\". Se ha probado teóricamente y experimentalmente que los datos sintéticos generados por modelos de enseñanza pueden significativamente acelerar la convergencia del entrenamiento. Además, en casos donde los datos sintéticos no son utilizables, los resultados muestran que los modelos con menos parámetros pueden demostrar un desempeño significativamente mejor que los modelos de línear de baja densidad, manteniendo así una buena eficiencia. Además, se muestra que para modelos que utilizan pautas libres de clases (como FLUX), la desactivación de las rondas de guia es crucial para mantener el rendimiento. Los experimentos expandidos demuestran que URAE logra un rendimiento de generación de 2K comparable a FLUX1.1[Pro] Ultra, y establece un nuevo estándar de prueba para la generación de 4K con 3K muestras y 2K iteraciones. El código está disponible en https://github.com/Huage001/URAE.",
      "upvotes": 7,
      "discussionId": "67dce4c50784200359ab25dc",
      "ai_keywords": [
        "text-to-image diffusion models",
        "high-resolution image generation",
        "training data",
        "computational resources",
        "data efficiency",
        "synthetic data",
        "teacher models",
        "training convergence",
        "parameter efficiency",
        "weight matrices",
        "low-rank adapters",
        "guidance distillation",
        "FLUX",
        "classifier-free guidance",
        "guidance scale",
        "2K-generation performance",
        "FLUX1.1",
        "4K-resolution generation"
      ]
    },
    "publishedAt": "2025-03-20T12:44:43.000Z",
    "title": "Ultra-Resolution Adaptation with Ease",
    "summary": "Text-to-image diffusion models have achieved remarkable progress in recent\nyears. However, training models for high-resolution image generation remains\nchallenging, particularly when training data and computational resources are\nlimited. In this paper, we explore this practical problem from two key\nperspectives: data and parameter efficiency, and propose a set of key\nguidelines for ultra-resolution adaptation termed URAE. For data\nefficiency, we theoretically and empirically demonstrate that synthetic data\ngenerated by some teacher models can significantly promote training\nconvergence. For parameter efficiency, we find that tuning minor components of\nthe weight matrices outperforms widely-used low-rank adapters when synthetic\ndata are unavailable, offering substantial performance gains while maintaining\nefficiency. Additionally, for models leveraging guidance distillation, such as\nFLUX, we show that disabling classifier-free guidance, i.e., setting\nthe guidance scale to 1 during adaptation, is crucial for satisfactory\nperformance. Extensive experiments validate that URAE achieves comparable\n2K-generation performance to state-of-the-art closed-source models like FLUX1.1\n[Pro] Ultra with only 3K samples and 2K iterations, while setting new\nbenchmarks for 4K-resolution generation. Codes are available\nhttps://github.com/Huage001/URAE{here}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16322.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6486fb33570a419f41a882e4",
      "avatarUrl": "/avatars/860a42074439a23c629cd23851ae4da6.svg",
      "fullname": "Ruonan Yu",
      "name": "roseannelexie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16057",
      "authors": [
        {
          "_id": "67dd04563b4c256a9809cc96",
          "name": "Yike Yuan",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc97",
          "name": "Ziyu Wang",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc98",
          "name": "Zihao Huang",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc99",
          "name": "Defa Zhu",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc9a",
          "name": "Xun Zhou",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc9b",
          "name": "Jingyi Yu",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc9c",
          "name": "Qiyang Min",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T11:45:08.000Z",
      "submittedOnDailyAt": "2025-03-21T04:50:45.905Z",
      "title": "Experto Ras: Estrategia de Escalado Básico por Experto Mixto y Formato Transjordano",
      "submittedOnDailyBy": {
        "_id": "667505f4361b960c79e35486",
        "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
        "isPro": false,
        "fullname": "Defa Zhu",
        "user": "mathfinder",
        "type": "user"
      },
      "summary": "El modelo de difusión ha adquirido la posición de referencia en la generación visual. Con este éxito, la integración del método de Mixture of Experts (MoE) proporciona una mejora significativa en la escalabilidad y el rendimiento del modelo. En este artículo, se presenta Race-DiT, un nuevo modelo MoE. Este modelo es un transformador de difusión con una estrategia de ruteo flexible y una carrera de Expertos. Mediante la competencia entre tokens y Expertos para seleccionar la candidata más probable, el modelo puede asignar de manera dinámica a los Expertos los tokens de mayor importancia. Además, para abordar los problemas de entrenamiento de capas superficiales, se propone la normalización de cada capa y la pérdida de semejanza del ruteo para prevenir la desaparición del modo. Los experimentos extendidos en ImageNet demuestran la efectividad de nuestro enfoque, mostrando tanto la mejora en el rendimiento como la expectativa de escalabilidad.",
      "upvotes": 7,
      "discussionId": "67dd045a3b4c256a9809cdb1",
      "ai_keywords": [
        "diffusion models",
        "Mixture of Experts (MoE)",
        "Race-DiT",
        "diffusion transformers",
        "Expert Race",
        "tokens",
        "experts",
        "per-layer regularization",
        "router similarity loss",
        "mode collapse",
        "ImageNet"
      ]
    },
    "publishedAt": "2025-03-20T07:45:08.000Z",
    "title": "Expert Race: A Flexible Routing Strategy for Scaling Diffusion\n  Transformer with Mixture of Experts",
    "summary": "Diffusion models have emerged as mainstream framework in visual generation.\nBuilding upon this success, the integration of Mixture of Experts (MoE) methods\nhas shown promise in enhancing model scalability and performance. In this\npaper, we introduce Race-DiT, a novel MoE model for diffusion transformers with\na flexible routing strategy, Expert Race. By allowing tokens and experts to\ncompete together and select the top candidates, the model learns to dynamically\nassign experts to critical tokens. Additionally, we propose per-layer\nregularization to address challenges in shallow layer learning, and router\nsimilarity loss to prevent mode collapse, ensuring better expert utilization.\nExtensive experiments on ImageNet validate the effectiveness of our approach,\nshowcasing significant performance gains while promising scaling properties.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16057.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667505f4361b960c79e35486",
      "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
      "fullname": "Defa Zhu",
      "name": "mathfinder",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16421",
      "authors": [
        {
          "_id": "67dcd5913713a0e1da19bbe5",
          "name": "Quanhao Li",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbe6",
          "name": "Zhen Xing",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbe7",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbe8",
          "name": "Hui Zhang",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbe9",
          "name": "Qi Dai",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbea",
          "name": "Zuxuan Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:42.000Z",
      "submittedOnDailyAt": "2025-03-21T01:28:06.140Z",
      "title": "MagicMotion: Secreto de la guía de trajejequetico para la generación de vídeos controlables",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "El desarrollo reciente en la generación de imágenes ha mejorado considerablemente en la continuidad espacial y temporal. Al mismo tiempo, ha surgido la generación de imágenes con control de rutas, permitiendo el control preciso del movimiento de objetos a través de rutas espaciales bien definidas. Sin embargo, los métodos existentes enfrentan dificultades al controlar el movimiento de objetos complejos y múltiples, así como problemas como la adaptabilidad de las rutas, la consistencia de los objetos y la pérdida de calidad visual. Además, estas técnicas están limitadas en su aplicación en diferentes escenarios debido a que su forma de control de rutas es única. Además, no existen dataset y marcos de referencia publicados específicos para la generación de imágenes con control de rutas, lo que impide una buena entrenamiento y evaluación sistemática. Para abordar estos problemas, hemos desarrollado un nuevo framework de generación de imágenes a partir de imágenes llamado MagicMotion, que permite el control de rutas y utiliza tres niveles de condiciones (máscara, caja de bordes, caja espesora) para mover objetos de manera fluida y mantener su consistencia y calidad visual. Al proporcionar una imagen de entrada y una ruta, MagicMotion moviliza los objetos de acuerdo con la ruta definida, manteniendo su consistencia y calidad visual. Además, proporcionamos MagicData, un grande dataset de imágenes con control de rutas, junto con una pipeline de anotación y filtrado automatizados. También presentamos MagicBench, un marco de referencia detallado para evaluar la calidad de las imágenes y la precisión del control de rutas en función del número de objetos. Los experimentos expandidos demuestran que MagicMotion supera a los métodos anteriores en varios métricas. Nuestro sitio web del proyecto está disponible públicamente. https://quanhaol.github.io/magicmotion-site.",
      "upvotes": 6,
      "discussionId": "67dcd5953713a0e1da19bd51",
      "projectPage": "https://quanhaol.github.io/magicmotion-site",
      "ai_keywords": [
        "trajectory-controllable video generation",
        "dense conditions",
        "sparse conditions",
        "masks",
        "bounding boxes",
        "sparse boxes",
        "object consistency",
        "MagicMotion",
        "MagicData",
        "MagicBench"
      ]
    },
    "publishedAt": "2025-03-20T13:59:42.000Z",
    "title": "MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance",
    "summary": "Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16421.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6415
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16428",
      "authors": [
        {
          "_id": "67dcd7a53c21e084fe58c3a8",
          "name": "Ruyi Xu",
          "hidden": false
        },
        {
          "_id": "67dcd7a53c21e084fe58c3a9",
          "name": "Guangxuan Xiao",
          "hidden": false
        },
        {
          "_id": "67dcd7a53c21e084fe58c3aa",
          "user": {
            "_id": "63797f727df2fefdcaf3ff7e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668906853549-noauth.jpeg",
            "isPro": false,
            "fullname": "Song",
            "user": "songhan",
            "type": "user"
          },
          "name": "Haofeng Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-21T03:06:14.875Z",
          "hidden": false
        },
        {
          "_id": "67dcd7a53c21e084fe58c3ab",
          "name": "Junxian Guo",
          "hidden": false
        },
        {
          "_id": "67dcd7a53c21e084fe58c3ac",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:58.000Z",
      "submittedOnDailyAt": "2025-03-21T01:36:33.593Z",
      "title": "XAttention: Se utiliza un escoring en contradiagonal para la atención en bloque espacio.",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Los modelos Transformer de largo contexto (LCTMs) son importantes en aplicaciones reales, pero su complejidad bidimensional requiere un alto costo de cálculo. El enfoque de espacio de bloques ayuda a mitigar este costo al concentrar cálculos en áreas importantes, pero los métodos existentes tienen dificultades para mantener un equilibrio entre precisión y eficiencia debido a los altos costos asociados con la evaluación de la importancia de los bloques. En este artículo, se presenta XAttention, un plugin y un marco base que utilizan la metodología de atención esparsa para acelerar significativamente la inferencia de modelos Transformer de largo contexto. La innovación principal de XAttention es la comprensión de que la suma de los elementos en la diagonal de la matriz de atención (es decir, desde la esquina inferior izquierda hasta la superior derecha) puede representar una fuerte representación del valor de los bloques. Esto permite la identificación y eliminación precisa de bloques no relevantes, logrando altas espesitudes y una notable aceleración de la inferencia. Se realizaron evaluaciones detalladas en diferentes marcos de referencia de largo contexto como RULER, LongBench (lenguaje), VideoMME (comprensión de videos) y VBench (generación de imágenes), demostrando que XAttention logra alcanzar la precisión de una atención completa mientras reduciendo significativamente el costo de cálculo. XAttention muestra un acelero de 13.5 veces en la computación de atención y demostra su capacidad para liberar el potencial práctico de la atención en bloques, facilitando la introducción de rendimientos eficientes y sustanciales en aplicaciones reales. El código está disponible en https://github.com/mit-han-lab/x-attention.",
      "upvotes": 5,
      "discussionId": "67dcd7a63c21e084fe58c422",
      "ai_keywords": [
        "Long-Context Transformer Models (LCTMs)",
        "attention's quadratic complexity",
        "block-sparse attention",
        "block importance",
        "XAttention",
        "sparse attention",
        "attention matrix",
        "antidiagonal values",
        "block importance proxy",
        "precision identification",
        "block pruning",
        "high sparsity",
        "inference acceleration",
        "RULER benchmark",
        "LongBench benchmark",
        "VideoMME benchmark",
        "VBench benchmark",
        "video understanding",
        "video generation"
      ]
    },
    "publishedAt": "2025-03-20T13:59:58.000Z",
    "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
    "summary": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16428.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6415
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16422",
      "authors": [
        {
          "_id": "67dcd5da7f5c5665205b11c0",
          "name": "Yuheng Yuan",
          "hidden": false
        },
        {
          "_id": "67dcd5da7f5c5665205b11c1",
          "name": "Qiuhong Shen",
          "hidden": false
        },
        {
          "_id": "67dcd5da7f5c5665205b11c2",
          "name": "Xingyi Yang",
          "hidden": false
        },
        {
          "_id": "67dcd5da7f5c5665205b11c3",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/O7634-Dy0NBVM_UZzq5Nm.mp4"
      ],
      "publishedAt": "2025-03-20T17:59:44.000Z",
      "submittedOnDailyAt": "2025-03-21T01:29:12.177Z",
      "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
      "submittedOnDailyBy": {
        "_id": "634cfebc350bcee9bed20a4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
        "isPro": false,
        "fullname": "Xingyi Yang",
        "user": "adamdad",
        "type": "user"
      },
      "summary": "4D Gaussian Splatting (4DGS) es un método reciente para la representación dinámica de espacios. Sin embargo, para alcanzar altas calidades, presenta problemas de espacio de almacenamiento grande y velocidad gráfica lenta. En este artículo, se investigan estos problemas y se identifican dos principales causas de la complejidad temporal. (Q1) Gaussianes cortos: 4DGS utiliza muchos Gaussianes con un rango temporal corto para representar dinámicamente, lo que genera una cantidad excesiva de Gaussianes. (Q2) Gaussianes inactivos: en la velocidad gráfica, los Gaussianes que contribuyen a cada frame son pocos, y todos los Gaussianes se reinician, lo que provoca un sobrecargo computacional ineficiente. Para resolver estas complejidades, se propone 4DGS-1K. 4DGS-1K funciona en más de 1000 FPS en GPUs modernos. Para (Q1), se introduce el puntaje de cambio espectral-temporal para reducir los Gaussianes cortos y inducir a 4DGS a representar movimientos dinámicos con Gaussianes de largo rango de tiempo. Para (Q2), se almacena una mascara de Gaussianes activas en los frames consecutivos para reducir significativamente los cálculos ineficientes en tiempo gráfico. En comparación con la versión original de 4DGS, 4DGS-1K requiere 41 veces menos espacio de almacenamiento, tiene una velocidad de reinicio 9 veces más rápida y mantiene la calidad visual. Para más detalles, consulte el sitio web del proyecto (https://4DGS-1K.github.io).",
      "upvotes": 5,
      "discussionId": "67dcd5e17f5c5665205b1422",
      "ai_keywords": [
        "4D Gaussian Splatting (4DGS)",
        "temporal redundancy",
        "Short-Lifespan Gaussians",
        "inactive Gaussians",
        "rasterization",
        "Spatial-Temporal Variation Score",
        "4DGS-1K",
        "FPS",
        "modern GPUs",
        "storage",
        "rasterization speed",
        "visual quality"
      ]
    },
    "publishedAt": "2025-03-20T13:59:44.000Z",
    "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
    "summary": "4D Gaussian Splatting (4DGS) has recently gained considerable attention as a\nmethod for reconstructing dynamic scenes. Despite achieving superior quality,\n4DGS typically requires substantial storage and suffers from slow rendering\nspeed. In this work, we delve into these issues and identify two key sources of\ntemporal redundancy. (Q1) Short-Lifespan Gaussians: 4DGS uses a large\nportion of Gaussians with short temporal span to represent scene dynamics,\nleading to an excessive number of Gaussians. (Q2) Inactive Gaussians:\nWhen rendering, only a small subset of Gaussians contributes to each frame.\nDespite this, all Gaussians are processed during rasterization, resulting in\nredundant computation overhead. To address these redundancies, we present\n4DGS-1K, which runs at over 1000 FPS on modern GPUs. For Q1, we\nintroduce the Spatial-Temporal Variation Score, a new pruning criterion that\neffectively removes short-lifespan Gaussians while encouraging 4DGS to capture\nscene dynamics using Gaussians with longer temporal spans. For Q2, we store a\nmask for active Gaussians across consecutive frames, significantly reducing\nredundant computations in rendering. Compared to vanilla 4DGS, our method\nachieves a 41times reduction in storage and 9times faster rasterization\nspeed on complex dynamic scenes, while maintaining comparable visual quality.\nPlease see our project page at https://4DGS-1K.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/O7634-Dy0NBVM_UZzq5Nm.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16422.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634cfebc350bcee9bed20a4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
      "fullname": "Xingyi Yang",
      "name": "adamdad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16413",
      "authors": [
        {
          "_id": "67dccb8ca33f11a56567bd61",
          "name": "Xueyan Zou",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd62",
          "name": "Yuchen Song",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd63",
          "name": "Ri-Zhao Qiu",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd64",
          "name": "Xuanbin Peng",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd65",
          "name": "Jianglong Ye",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd66",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd67",
          "name": "Xiaolong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:12.000Z",
      "submittedOnDailyAt": "2025-03-21T00:52:49.431Z",
      "title": "M3: Modelo de Memoria Espectral Tridimensional de Modo Doble",
      "submittedOnDailyBy": {
        "_id": "62520988818a5dc29ab91d6f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671670066375-62520988818a5dc29ab91d6f.png",
        "isPro": false,
        "fullname": "Xueyan Zou",
        "user": "xueyanz",
        "type": "user"
      },
      "summary": "Aquí se presenta el 3D Espectral Multi-Model Memory (M3). M3 es un sistema de memoria multi-modelo diseñado para almacenar información espectral estática obtenida de fuentes de video y ayudar a la reconocimiento visual. M3 integra la tecnología de 3D gauss splitting y el modelo básico para renderizar representaciones de características que incluyen conocimientos de diferentes estructuras específicas. En investigaciones previas, se identificaron dos problemas importantes: (1) las restricciones computacionales para almacenar las grandes cantidades de características de los primitivos gaussianos y (2) la necesidad de ajustar o perder información entre las características combinadas y el modelo básico. Para enfrentar estos problemas, M3 propone componentes principales espectrales y resumenes de la acción de memoria gaussiana, lo que permite un entrenamiento y inferencia eficientes. Para probar la efectividad de M3, se realizan evaluaciones detalladas de la similitud de características y tareas posteriores, y se visualiza el flujo de píxeles de la acción de memoria gaussiana. Nuestro enfoque incluye modelos básicos como modelos de lenguaje visual (VLMs), modelos de reconocimiento, modelos grandes multi-modelo y modelos de lenguaje grande (LMMs/LLMs). Además, demostramos la aplicación real de M3 en espectrales interiores de robots, aplicando su campo de características. En particular, M3 se presenta como el primer estudio que resuelve los problemas fundamentales de compresión de características 3D, afirmando que resolve este problema.",
      "upvotes": 5,
      "discussionId": "67dccb92a33f11a56567bf43",
      "ai_keywords": [
        "3D Spatial MultiModal Memory (M3)",
        "3D Gaussian Splatting",
        "feature representations",
        "granularities",
        "principal scene components",
        "Gaussian memory attention",
        "feature splatting",
        "computational constraints",
        "high-dimensional features",
        "Gaussian primitive",
        "misalignment",
        "information loss",
        "distilled features",
        "foundation models",
        "vision-language models (VLMs)",
        "perception models",
        "large multimodal and language models (LMMs/LLMs)",
        "feature field",
        "quadruped robot",
        "core compression challenges",
        "3D feature distillation"
      ]
    },
    "publishedAt": "2025-03-20T13:59:12.000Z",
    "title": "M3: 3D-Spatial MultiModal Memory",
    "summary": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16413.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62520988818a5dc29ab91d6f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671670066375-62520988818a5dc29ab91d6f.png",
      "fullname": "Xueyan Zou",
      "name": "xueyanz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16188",
      "authors": [
        {
          "_id": "67dce5afbabeda89ca6071c3",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67dce5afbabeda89ca6071c4",
          "name": "Shitian Zhao",
          "hidden": false
        },
        {
          "_id": "67dce5afbabeda89ca6071c5",
          "name": "Jike Zhong",
          "hidden": false
        },
        {
          "_id": "67dce5afbabeda89ca6071c6",
          "name": "Yuxiang Lai",
          "hidden": false
        },
        {
          "_id": "67dce5afbabeda89ca6071c7",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T14:37:45.000Z",
      "submittedOnDailyAt": "2025-03-21T03:47:03.083Z",
      "title": "CLS-RL: Aprendizaje de refuerzo basado en reglas para la clasificación de imágenes",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "La clasificación de clases es una de las tareas fundamentales en el aprendizaje automático. Según recientes estudios, los modelos de lenguaje y imágenes (MLLMs) mostraron un rendimiento deficiente en la clasificación de clases de imágenes inicialmente, pero mejoraron significativamente su rendimiento al fine-tunar con una cantidad adecuada de datos, lo que los posicionó para compararse con los mejores modelos de clasificación de clases. Sin embargo, la obtención de grandes conjuntos de datos estándarizados es costosamente poco rentable. En este artículo, investigamos el fine-tuning de la clasificación de clases de MLLMs con pocos datos. Encontramos que el entrenamiento de retroalimentación simple (SFT) puede causar problemas de sobreajuste estricto y que su rendimiento puede disminuir en comparación con el enfoque de 0 shot. Para enfrentar estas desafíos, basándonos en el éxito reciente del aprendizaje por refuerzo basado en reglas, proponemos el aprendizaje por refuerzo de clasificación de clases (CLS-RL). CLS-RL ajusta MLLMs utilizando señales probadas como recompensa. Demostramos que CLS-RL es superior a SFT en múltiples conjuntos de datos y que su precisión promedio es significativamente mayor que el modelo de 0 shot, incluso cuando se entrena desde el principio con nuevos conjuntos de datos. Además, se observó un fenómeno llamado \"liberdad de pensamiento\" en CLS-RL. Al fine-tunar un conjunto de datos, el rendimiento del modelo en otros conjuntos también se mejoró, incluso cuando la distribución de los conjuntos o los nombres de las clases eran diferentes. Esto demuestra que los métodos de aprendizaje por refuerzo pueden enseñar a los modelos una comprensión básica de la clasificación de clases. Finalmente, basándonos en los recientes estudios sobre el pensamiento durante la inferencia, revisamos el proceso de \"pensamiento\" durante el fine-tuning y consideramos si es necesario y si puede afectar negativamente el rendimiento. En consecuencia, proponemos un módulo CLS-RL No-Thinking que minimiza el proceso de pensamiento durante el fine-tuning, ajustando la precisión de igualdad y reduciendo el tiempo de fine-tuning significativamente, lo que mejora tanto el rendimiento dentro del modelo como su capacidad de generalización.",
      "upvotes": 5,
      "discussionId": "67dce5b1babeda89ca607241",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "few-shot MLLM classification fine-tuning",
        "SFT",
        "overfitting issues",
        "zero-shot approach",
        "CLS-RL",
        "verifiable signals",
        "reward",
        "free-lunch phenomenon",
        "base-to-new",
        "few-shot learning",
        "RL-based methods",
        "inference time thinking",
        "thinking process",
        "No-Thinking-CLS-RL",
        "equality accuracy reward"
      ]
    },
    "publishedAt": "2025-03-20T10:37:45.000Z",
    "title": "CLS-RL: Image Classification with Rule-Based Reinforcement Learning",
    "summary": "Classification is a core task in machine learning. Recent research has shown\nthat although Multimodal Large Language Models (MLLMs) are initially poor at\nimage classification, fine-tuning them with an adequate amount of data can\nsignificantly enhance their performance, making them comparable to SOTA\nclassification models. However, acquiring large-scale labeled data is\nexpensive. In this paper, we explore few-shot MLLM classification fine-tuning.\nWe found that SFT can cause severe overfitting issues and may even degrade\nperformance over the zero-shot approach. To address this challenge, inspired by\nthe recent successes in rule-based reinforcement learning, we propose CLS-RL,\nwhich uses verifiable signals as reward to fine-tune MLLMs. We discovered that\nCLS-RL outperforms SFT in most datasets and has a much higher average accuracy\non both base-to-new and few-shot learning setting. Moreover, we observed a\nfree-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular\ndataset, their performance on other distinct datasets may also improve over\nzero-shot models, even if those datasets differ in distribution and class\nnames. This suggests that RL-based methods effectively teach models the\nfundamentals of classification. Lastly, inspired by recent works in inference\ntime thinking, we re-examine the `thinking process' during fine-tuning, a\ncritical aspect of RL-based methods, in the context of visual classification.\nWe question whether such tasks require extensive thinking process during\nfine-tuning, proposing that this may actually detract from performance. Based\non this premise, we introduce the No-Thinking-CLS-RL method, which minimizes\nthinking processes during training by setting an equality accuracy reward. Our\nfindings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL\nmethod achieves superior in-domain performance and generalization capabilities\nthan CLS-RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16188.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13657",
      "authors": [
        {
          "_id": "67dc4391f618f3c7ba6a3b6d",
          "name": "Mert Cemri",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b6e",
          "name": "Melissa Z. Pan",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b6f",
          "name": "Shuyi Yang",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b70",
          "name": "Lakshya A. Agrawal",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b71",
          "name": "Bhavya Chopra",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b72",
          "name": "Rishabh Tiwari",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b73",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b74",
          "name": "Aditya Parameswaran",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b75",
          "name": "Dan Klein",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b76",
          "name": "Kannan Ramchandran",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b77",
          "name": "Matei Zaharia",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b78",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b79",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T19:04:38.000Z",
      "submittedOnDailyAt": "2025-03-21T07:07:51.154Z",
      "title": "Los motivos por los que el sistema de LLM Multi-Agent falla:\n\n- Cuando hay una interacción insuficiente o no adecuada entre los Agentes.\n- Cuando un Agente no realiza su función correctamente.\n- Cuando un Agente realiza mal las funciones de otros Agentes o afecta a sus tareas.\n- Cuando los Agentes experimentan problemas con la precisión y calidad de los datos.\n- Cuando los modelos entrenados no son suficientemente adecuados para el entorno real.\n- Cuando los Agentes no pueden adaptarse a los cambios en el entorno.\n- Cuando los Agentes no cumplen con los requisitos del usuario.\n- Cuando el sistema no mantiene la estabilidad y la estabilidad.\n- Cuando el sistema disminuye su rendimiento.\n- Cuando el sistema se enfrenta a amenazas de seguridad.\n- Cuando el sistema no es eficiente.\n- Cuando el sistema no es expandible.\n- Cuando el sistema no es mantenible.\n- Cuando el sistema no es amigable con los usuarios.\n- Cuando el sistema no satisface a los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.\n- Cuando el sistema no genera confianza en los usuarios.",
      "submittedOnDailyBy": {
        "_id": "5ff5d596f244529b3ec0fb89",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png",
        "isPro": false,
        "fullname": "Philipp Schmid",
        "user": "philschmid",
        "type": "user"
      },
      "summary": "En un sistema multi-agente (MAS), el entusiasmo de los mayores números de agentes inteligentes (LLM) para realizar tareas en conjunto aumenta, sin embargo, su capacidad para mejorar el rendimiento en benchmarks es limitada al menos comparado con un marco de trabajo de un solo agente. Esta diferencia requiere un análisis de los problemas de eficiencia en un MAS.\n\nEste artículo proporciona el primer estudio detallado sobre los problemas de un MAS. Se analizan 5 de los mejores frameworks de MAS que procesan más de 150 tareas, y se realizan los análisis a través de 6 expertos. Se identifican 14 modos de fallo únicos y se propone una traqueonomía detallada para aplicarlos. Esta traqueonomía fue aprobada por 3 expertos, con un puntaje de Kappa de Cohen de 0.88. Estos pequeños modos de fallo se clasifican en tres categorías: (i) fallos en especificaciones y diseño del sistema, (ii) disposición entre agentes, y (iii) verificación y finalización de tareas. Para apoyar la evaluación escalable, se integran MASFT y LLM-as-a-Judge. Además, se investiga cómo mejorar las especificaciones de los roles de los agentes y aumentar la trayectoria de entrenamiento para prevenir los fallos identificados. Este artículo demuestra que los fallos identificados requieren soluciones complejas y presenta un programa claro para futuras investigaciones. Se publican los datasets y los analistas de LLM en este artículo.",
      "upvotes": 5,
      "discussionId": "67dc4392f618f3c7ba6a3be9",
      "githubRepo": "https://github.com/multi-agent-systems-failure-taxonomy/MASFT",
      "ai_keywords": [
        "Multi-Agent Systems (MAS)",
        "LLM agents",
        "performance gains",
        "single-agent frameworks",
        "failure modes",
        "Cohen's Kappa score",
        "specification and system design failures",
        "inter-agent misalignment",
        "task verification and termination",
        "LLM-as-a-Judge",
        "orchestration strategies"
      ]
    },
    "publishedAt": "2025-03-17T15:04:38.000Z",
    "title": "Why Do Multi-Agent LLM Systems Fail?",
    "summary": "Despite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM\nagents collaborate to accomplish tasks, their performance gains across popular\nbenchmarks remain minimal compared to single-agent frameworks. This gap\nhighlights the need to analyze the challenges hindering MAS effectiveness.\n  In this paper, we present the first comprehensive study of MAS challenges. We\nanalyze five popular MAS frameworks across over 150 tasks, involving six expert\nhuman annotators. We identify 14 unique failure modes and propose a\ncomprehensive taxonomy applicable to various MAS frameworks. This taxonomy\nemerges iteratively from agreements among three expert annotators per study,\nachieving a Cohen's Kappa score of 0.88. These fine-grained failure modes are\norganized into 3 categories, (i) specification and system design failures, (ii)\ninter-agent misalignment, and (iii) task verification and termination. To\nsupport scalable evaluation, we integrate MASFT with LLM-as-a-Judge. We also\nexplore if identified failures could be easily prevented by proposing two\ninterventions: improved specification of agent roles and enhanced orchestration\nstrategies. Our findings reveal that identified failures require more complex\nsolutions, highlighting a clear roadmap for future research. We open-source our\ndataset and LLM annotator.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13657.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5ff5d596f244529b3ec0fb89",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png",
      "fullname": "Philipp Schmid",
      "name": "philschmid",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 811
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16397",
      "authors": [
        {
          "_id": "67dd1227046f2c38458e9588",
          "name": "Nikita Starodubcev",
          "hidden": false
        },
        {
          "_id": "67dd1227046f2c38458e9589",
          "name": "Denis Kuznedelev",
          "hidden": false
        },
        {
          "_id": "67dd1227046f2c38458e958a",
          "name": "Artem Babenko",
          "hidden": false
        },
        {
          "_id": "67dd1227046f2c38458e958b",
          "name": "Dmitry Baranchuk",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6410d3a4cfbe9c4400233d1e/lkzM32YzNPrhP9ESkaUpF.png"
      ],
      "publishedAt": "2025-03-20T17:54:02.000Z",
      "submittedOnDailyAt": "2025-03-21T08:05:39.256Z",
      "title": "Simulación de difusión con escala de ruido en tratamientos térmicos",
      "submittedOnDailyBy": {
        "_id": "6410d3a4cfbe9c4400233d1e",
        "avatarUrl": "/avatars/9e1c4e0ea5f6964c90013f6e19c41db1.svg",
        "isPro": false,
        "fullname": "nikita",
        "user": "quickjkee",
        "type": "user"
      },
      "summary": "SwD es una forma de destilación paso a paso para los DMs. SwD implementa un generador de etapas de dificultad basado en la predicción de próximos pasos efectivamente. Específicamente, SwD se ha desarrollado a partir de la comprensión de procesos difíciles recientes y el auto-regresso de Shifrulal oculto. Se supone que los DMs comienzan la generación desde la baja resolución de datos, escalando gradualmente las muestras a mayores niveles de ruido, minimizando pérdidas de rendimiento y reduciendo costos de cálculo. SwD naturalmente integra estas ideas en la forma de destilación basada en ajuste de distribuciones, introduciendo un nuevo tipo de pérdida de patch para enriquecer la familia de métodos de ajuste de distribuciones. SwD muestra claramente, a través de métricas de evaluación automática y investigaciones de preferencia humana, que puede mejorar significativamente el rendimiento de modelos de dificultad en la conversión de texto a imagen, al tener tiempos de inferencia similares a dos etapas de resolución completa, bajo el mismo coste de cálculo.",
      "upvotes": 4,
      "discussionId": "67dd1229046f2c38458e9617",
      "projectPage": "https://yandex-research.github.io/swd/",
      "githubRepo": "https://github.com/yandex-research/swd",
      "ai_keywords": [
        "scale-wise distillation",
        "diffusion models",
        "next-scale prediction",
        "implicit spectral autoregression",
        "denoising",
        "computational costs",
        "distribution matching",
        "patch loss",
        "text-to-image diffusion models",
        "inference times",
        "computation budget",
        "automated metrics",
        "human preference studies"
      ]
    },
    "publishedAt": "2025-03-20T13:54:02.000Z",
    "title": "Scale-wise Distillation of Diffusion Models",
    "summary": "We present SwD, a scale-wise distillation framework for diffusion models\n(DMs), which effectively employs next-scale prediction ideas for\ndiffusion-based few-step generators. In more detail, SwD is inspired by the\nrecent insights relating diffusion processes to the implicit spectral\nautoregression. We suppose that DMs can initiate generation at lower data\nresolutions and gradually upscale the samples at each denoising step without\nloss in performance while significantly reducing computational costs. SwD\nnaturally integrates this idea into existing diffusion distillation methods\nbased on distribution matching. Also, we enrich the family of distribution\nmatching approaches by introducing a novel patch loss enforcing finer-grained\nsimilarity to the target distribution. When applied to state-of-the-art\ntext-to-image diffusion models, SwD approaches the inference times of two full\nresolution steps and significantly outperforms the counterparts under the same\ncomputation budget, as evidenced by automated metrics and human preference\nstudies.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6410d3a4cfbe9c4400233d1e/lkzM32YzNPrhP9ESkaUpF.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16397.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6410d3a4cfbe9c4400233d1e",
      "avatarUrl": "/avatars/9e1c4e0ea5f6964c90013f6e19c41db1.svg",
      "fullname": "nikita",
      "name": "quickjkee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16278",
      "authors": [
        {
          "_id": "67dcc98b54dcfdc1fd17d9b6",
          "name": "Shuqi Lu",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9b7",
          "name": "Haowei Lin",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9b8",
          "name": "Lin Yao",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9b9",
          "name": "Zhifeng Gao",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9ba",
          "name": "Xiaohong Ji",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9bb",
          "name": "Weinan E",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9bc",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9bd",
          "name": "Guolin Ke",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T16:07:04.000Z",
      "submittedOnDailyAt": "2025-03-21T00:36:39.769Z",
      "title": "Uni-3DAR: Integración y Comprensión de Multidimensional Generación y Automático Reconstrucción sobre Tokenes de Ciudades Comprimidos",
      "submittedOnDailyBy": {
        "_id": "6348de0c62c668c7b48d83c9",
        "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
        "isPro": false,
        "fullname": "Guolin Ke",
        "user": "guolinke",
        "type": "user"
      },
      "summary": "Recientemente, el desarrollo de modelos de lenguaje grande y su extensión multimodal han demostrado la eficiencia en la generación y comprensión mediante predicción automática de palabras. Sin embargo, la generación y comprensión de estructuras tridimensionales (3D GU) en la inteligencia artificial científica no se ha estudiado principalmente mediante la predicción automática de palabras, sino que ha evolucionado de manera independiente. Para remediar esto, presentamos el marco de trabajo unificado Uni-3DAR. Este framework integra de manera seamless las tareas de 3D GU a través de la predicción automática de palabras. El núcleo de Uni-3DAR es la nueva tokenización capaz de comprimir espacios tridimensionales a través de ópticas y explotar la especificidad de estructuras tridimensionales. Posteriormente, se aplica una tokenización adicional para identificar detalles de estructura, como la especie de átomos y las coordenadas espaciales precisas de la estructura tridimensional microscópica. Además, se proponen dos optimizaciones para mejorar la eficiencia y eficacia: una estrategia de compresión de secuencias de tokens ópticas en dos etapas que reducen la longitud en un 8-fold, y otra estructura de predicción de siguiente token con mascaras que ajustan a las posiciones de tokens variables, lo que significativamente mejora el rendimiento del modelo. Combinando estas estrategias, Uni-3DAR integra diversas tareas de 3D GU en un solo framework de predicción automática de palabras. Se han realizado amplios experimentos en tareas de 3D GU microscópicas, como moléculas, proteínas, poliméros y cristales, demostrando su eficiencia y extensibilidad. En particular, Uni-3DAR supera significativamente a los modelos más avanzados previos, alcanzando un mejoramiento relativo del 256% y una velocidad de inferencia 21.8 veces más rápida. El código está disponible en https://github.com/dptech-corp/Uni-3DAR.",
      "upvotes": 4,
      "discussionId": "67dcc98c54dcfdc1fd17da0f",
      "githubRepo": "https://github.com/dptech-corp/Uni-3DAR",
      "ai_keywords": [
        "hierarchical tokenization",
        "octree",
        "two-level subtree compression strategy",
        "masked next-token prediction mechanism",
        "Uni-3DAR",
        "3D GU (3D generation and understanding)",
        "autoregressive prediction",
        "state-of-the-art diffusion models"
      ]
    },
    "publishedAt": "2025-03-20T12:07:04.000Z",
    "title": "Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens",
    "summary": "Recent advancements in large language models and their multi-modal extensions\nhave demonstrated the effectiveness of unifying generation and understanding\nthrough autoregressive next-token prediction. However, despite the critical\nrole of 3D structural generation and understanding ({3D GU}) in AI for science,\nthese tasks have largely evolved independently, with autoregressive methods\nremaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified\nframework that seamlessly integrates {3D GU} tasks via autoregressive\nprediction. At its core, Uni-3DAR employs a novel hierarchical tokenization\nthat compresses 3D space using an octree, leveraging the inherent sparsity of\n3D structures. It then applies an additional tokenization for fine-grained\nstructural details, capturing key attributes such as atom types and precise\nspatial coordinates in microscopic 3D structures. We further propose two\noptimizations to enhance efficiency and effectiveness. The first is a two-level\nsubtree compression strategy, which reduces the octree token sequence by up to\n8x. The second is a masked next-token prediction mechanism tailored for\ndynamically varying token positions, significantly boosting model performance.\nBy combining these strategies, Uni-3DAR successfully unifies diverse {3D GU}\ntasks within a single autoregressive framework. Extensive experiments across\nmultiple microscopic {3D GU} tasks, including molecules, proteins, polymers,\nand crystals, validate its effectiveness and versatility. Notably, Uni-3DAR\nsurpasses previous state-of-the-art diffusion models by a substantial margin,\nachieving up to 256\\% relative improvement while delivering inference speeds up\nto 21.8x faster. The code is publicly available at\nhttps://github.com/dptech-corp/Uni-3DAR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6348de0c62c668c7b48d83c9",
      "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
      "fullname": "Guolin Ke",
      "name": "guolinke",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15567",
      "authors": [
        {
          "_id": "67dcc734067589b43b19af7a",
          "name": "Yanchen Luo",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7b",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7c",
          "name": "Yi Zhao",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7d",
          "name": "Sihang Li",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7e",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7f",
          "name": "Tat-Seng Chua",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af80",
          "name": "Xiang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T08:56:13.000Z",
      "submittedOnDailyAt": "2025-03-21T00:33:21.187Z",
      "title": "「Acerca del espacio potencial propio de la modelación de difusión potencial molecular 3D」",
      "submittedOnDailyBy": {
        "_id": "64f04a28f3cd962c21726459",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/MOTc7SWbzc4jdJbMcWMcK.jpeg",
        "isPro": false,
        "fullname": "LuoYanchen",
        "user": "lyc0930",
        "type": "user"
      },
      "summary": "La generación de moléculas 3D es crucial en la descubrimiento de fármacos y la ciencia de los materiales, requiriendo modelos que puedan procesar complejas multimodalidades, como la especie de átomos, la unión química y las coordenadas 3D. Uno de los principales desafíos es la integración de diferentes tipos de multimodalidades mientras se mantiene la simetría SE(3) de las coordenadas 3D. Para abordar esto, los métodos actuales generalmente mantienen espacios potenciales separados para cada tipo de multimodalidad, lo que reduce la eficiencia en el entrenamiento y la generación de muestras. En este trabajo, proponemos un Variational Auto-Encoder (UAE-3D) que codifica la información de moléculas 3D en secuencias de potenciales unificados. El UAE-3D es un VAE multimodalidad que mantiene un error de reconstrucción cercano a cero y el espacio potencial integrado elimina la complejidad de la multimodalidad y la simetría. Utilizamos un Transformer de Difusión (un modelo general de difusión que no tiene bias de inducción de moléculas) para generar potenciales y, a través de experimentos en los conjuntos de datos GEOM-Drugs y QM9, demostramos que nuestro método mejora significativamente los nuevos benchmarks y logra la eficiencia y calidad en la generación de nuevas moléculas 3D de manera dinámica y condicional.",
      "upvotes": 4,
      "discussionId": "67dcc735067589b43b19afd9",
      "ai_keywords": [
        "SE(3) equivariance",
        "latent spaces",
        "multi-modal VAE",
        "latent sequences",
        "unified latent space",
        "latent diffusion modeling",
        "Diffusion Transformer",
        "GEOM-Drugs dataset",
        "QM9 dataset",
        "de novo molecule generation",
        "conditional molecule generation"
      ]
    },
    "publishedAt": "2025-03-19T04:56:13.000Z",
    "title": "Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling",
    "summary": "3D molecule generation is crucial for drug discovery and material science,\nrequiring models to process complex multi-modalities, including atom types,\nchemical bonds, and 3D coordinates. A key challenge is integrating these\nmodalities of different shapes while maintaining SE(3) equivariance for 3D\ncoordinates. To achieve this, existing approaches typically maintain separate\nlatent spaces for invariant and equivariant modalities, reducing efficiency in\nboth training and sampling. In this work, we propose Unified\nVariational Auto-Encoder for 3D Molecular Latent\nDiffusion Modeling (UAE-3D), a multi-modal VAE that compresses 3D\nmolecules into latent sequences from a unified latent space, while maintaining\nnear-zero reconstruction error. This unified latent space eliminates the\ncomplexities of handling multi-modality and equivariance when performing latent\ndiffusion modeling. We demonstrate this by employing the Diffusion\nTransformer--a general-purpose diffusion model without any molecular inductive\nbias--for latent generation. Extensive experiments on GEOM-Drugs and QM9\ndatasets demonstrate that our method significantly establishes new benchmarks\nin both de novo and conditional 3D molecule generation, achieving\nleading efficiency and quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15567.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f04a28f3cd962c21726459",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/MOTc7SWbzc4jdJbMcWMcK.jpeg",
      "fullname": "LuoYanchen",
      "name": "lyc0930",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10625",
      "authors": [
        {
          "_id": "67dacb439c49701f604e4257",
          "name": "Lingteng Qiu",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e4258",
          "name": "Xiaodong Gu",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e4259",
          "name": "Peihao Li",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425a",
          "user": {
            "_id": "64d0d72e15b26cc7f704a60f",
            "avatarUrl": "/avatars/f450013fb2b204f1422e614629914248.svg",
            "isPro": true,
            "fullname": "Qi Zuo",
            "user": "DyrusQZ",
            "type": "user"
          },
          "name": "Qi Zuo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-20T10:46:05.121Z",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425b",
          "name": "Weichao Shen",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425c",
          "name": "Junfei Zhang",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425d",
          "name": "Kejie Qiu",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425e",
          "name": "Weihao Yuan",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425f",
          "name": "Guanying Chen",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e4260",
          "name": "Zilong Dong",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e4261",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64d0d72e15b26cc7f704a60f/tiDonOVoU7mFVv3w6qgcR.mp4"
      ],
      "publishedAt": "2025-03-13T17:59:21.000Z",
      "submittedOnDailyAt": "2025-03-21T05:52:51.662Z",
      "title": "LHM: Modelo de reconstrucción de personajes dinámicos en una sola imagen a velocidades extremas",
      "submittedOnDailyBy": {
        "_id": "64d0d72e15b26cc7f704a60f",
        "avatarUrl": "/avatars/f450013fb2b204f1422e614629914248.svg",
        "isPro": true,
        "fullname": "Qi Zuo",
        "user": "DyrusQZ",
        "type": "user"
      },
      "summary": "La reconstrucción 3D de personas es una tecnología que permite reconstruir personas 3D en movimiento a partir de una sola fotografía, resuelviendo problemas complejos debido a la geometría, la morfología y la incertidumbre de la deformación. El desarrollo reciente de la reconstrucción 3D de personas ha centrado principalmente en modelos humanos fijos, utilizando escaneos 3D sintéticos para entrenamiento, lo que limita su capacidad de generalización. Por otro lado, el enfoque basado en optimización requiere procesos de entrenamiento precisos con alta precisión, pero también controladas condiciones de captura y bajo consumo computacional. En respuesta a este desafío, se propone el LHM (Modelo de Reconstrucción Humana Animable de Grandes Escalas) para lograr reconstrucciones fijas eficientes. El LHM puede inferir un avatar de alta precisión, representado por una distribución gaussiana 3D, en una sola ruta de propagación. Nuestro modelo extiende la arquitectura de transformadores multimodal, codificando efectivamente características de la posición del cuerpo humano y de las imágenes utilizando un mecanismo de atención, permitiendo la almacenamiento de detalles geométricos y texturales de los vestidos. Además, se propone una técnica de codificación piramidal de características de caras para mantener la reconocimiento facial y realizar una reconstrucción detallada de la cabeza. En experimentos extensos, nuestro LHM genera personas moviles significativamente con una precisión superior a los métodos existentes, generando resultados comprobables en tiempos de aproximadamente un segundo por frame, excluyendo el post-procesado de caras y manos.",
      "upvotes": 4,
      "discussionId": "67dacb499c49701f604e4454",
      "ai_keywords": [
        "3D Gaussian splatting",
        "multimodal transformer architecture",
        "attention mechanism",
        "head feature pyramid encoding scheme"
      ]
    },
    "publishedAt": "2025-03-13T13:59:21.000Z",
    "title": "LHM: Large Animatable Human Reconstruction Model from a Single Image in\n  Seconds",
    "summary": "Animatable 3D human reconstruction from a single image is a challenging\nproblem due to the ambiguity in decoupling geometry, appearance, and\ndeformation. Recent advances in 3D human reconstruction mainly focus on static\nhuman modeling, and the reliance of using synthetic 3D scans for training\nlimits their generalization ability. Conversely, optimization-based video\nmethods achieve higher fidelity but demand controlled capture conditions and\ncomputationally intensive refinement processes. Motivated by the emergence of\nlarge reconstruction models for efficient static reconstruction, we propose LHM\n(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars\nrepresented as 3D Gaussian splatting in a feed-forward pass. Our model\nleverages a multimodal transformer architecture to effectively encode the human\nbody positional features and image features with attention mechanism, enabling\ndetailed preservation of clothing geometry and texture. To further boost the\nface identity preservation and fine detail recovery, we propose a head feature\npyramid encoding scheme to aggregate multi-scale features of the head regions.\nExtensive experiments demonstrate that our LHM generates plausible animatable\nhuman in seconds without post-processing for face and hands, outperforming\nexisting methods in both reconstruction accuracy and generalization ability.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64d0d72e15b26cc7f704a60f/tiDonOVoU7mFVv3w6qgcR.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d0d72e15b26cc7f704a60f",
      "avatarUrl": "/avatars/f450013fb2b204f1422e614629914248.svg",
      "fullname": "Qi Zuo",
      "name": "DyrusQZ",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16252",
      "authors": [
        {
          "_id": "67dcc7b29c17514cb9815abd",
          "name": "Zhaowei Liu",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815abe",
          "name": "Xin Guo",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815abf",
          "name": "Fangqi Lou",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac0",
          "name": "Lingfeng Zeng",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac1",
          "name": "Jinyi Niu",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac2",
          "name": "Zixuan Wang",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac3",
          "name": "Jiajie Xu",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac4",
          "name": "Weige Cai",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac5",
          "name": "Ziwei Yang",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac6",
          "name": "Xueqian Zhao",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac7",
          "name": "Chao Li",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac8",
          "name": "Sheng Xu",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac9",
          "name": "Dezhi Chen",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815aca",
          "name": "Yun Chen",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815acb",
          "name": "Zuo Bai",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815acc",
          "name": "Liwen Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T15:46:18.000Z",
      "submittedOnDailyAt": "2025-03-21T00:54:01.337Z",
      "title": "Fin-R1: Aprendizaje de modelos de lenguaje de gran escala mediante el aprendizaje de reglas financieras",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "El lenguaje de mayor capacidad para procesar tareas financieras complejas requiere una profunda exploración de sus habilidades. En particular, se presenta Fin-R1, un lenguaje de mayor capacidad diseñado específicamente para la industria financiera. Fin-R1 se construyó utilizando una arquitectura de dos etapas basada en DeepSeek-R1, y se utilizó un conjunto de datos de lógica financiera purificado. A través de entrenamiento por Supervisado y por Reinforcement Learning, Fin-R1 alcanzó un rendimiento cercano al de DeepSeek-R1 en una gama de tareas de lógica financiera de diferentes tipos, con un tamaño de 700 millones de parámetros. En las evaluaciones de FinQA y ConvFinQA, Fin-R1 logró el mejor rendimiento y superó a otros modelos. Fin-R1 muestra una fuerza de lógica y capacidad de toma de decisiones potentes para abordar diversos problemas en el campo financiero. El código está disponible en https://github.com/SUFE-AIFLM-Lab/Fin-R1.",
      "upvotes": 3,
      "discussionId": "67dcc7b69c17514cb9815c1d",
      "githubRepo": "https://github.com/SUFE-AIFLM-Lab/Fin-R1",
      "ai_keywords": [
        "reasoning large language models",
        "two-stage architecture",
        "financial reasoning dataset",
        "DeepSeek-R1",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "state-of-the-art (SOTA)",
        "FinQA",
        "ConvFinQA"
      ]
    },
    "publishedAt": "2025-03-20T11:46:18.000Z",
    "title": "Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning",
    "summary": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16252.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6415
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15851",
      "authors": [
        {
          "_id": "67dcff844aa37abf77ae7338",
          "name": "Zhou Zhenglin",
          "hidden": false
        },
        {
          "_id": "67dcff844aa37abf77ae7339",
          "name": "Ma Fan",
          "hidden": false
        },
        {
          "_id": "67dcff844aa37abf77ae733a",
          "name": "Fan Hehe",
          "hidden": false
        },
        {
          "_id": "67dcff844aa37abf77ae733b",
          "name": "Chua Tat-Seng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T05:07:46.000Z",
      "submittedOnDailyAt": "2025-03-21T04:27:49.029Z",
      "title": "Zero-1-to-A: Creación de imágenes de rostros moviles a partir de una sola imagen de 0-shot",
      "submittedOnDailyBy": {
        "_id": "6425318d175bd2952281065e",
        "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
        "isPro": false,
        "fullname": "ZhenglinZhou",
        "user": "zhenglin",
        "type": "user"
      },
      "summary": "La generación de modelos de imágenes animadas requiere generalmente datos complejos. Una solución natural para reducir la demanda de datos es utilizar métodos de generación de modelos de imágenes estáticas sin datos existentes. Para ello, utilizamos el Score Designer Sampling (SDS) incluyendo modelos diferenciales preentrenados. Sin embargo, diseñar directamente modelos de imágenes 4D desde vídeos diferenciales puede resultar en resultados sobre-smozados debido a la incertidumbre espacial y temporal del vídeo generado. Para resolver este problema, proponemos el potente métodología Zero-1-to-A. Este método utiliza modelos de vídeo diferenciales para sintetizar datos espaciales y temporales coherentes para la reconstrucción de modelos de imágenes 4D, reduciendo así la demanda de datos. En particular, Zero-1-to-A construye paso a paso un conjunto de datos de vídeo, optimiza modelos de imágenes animadas de manera gradual y mantiene la coherencia mientras la calidad de los modelos animados aumenta armoníacamente durante el proceso de entrenamiento. Este aprendizaje paso a paso se configura en dos etapas: 1) el aprendizaje de coherencia espacial se realiza con expresiones fijas y se entrena desde arriba hacia abajo visualmente. 2) el aprendizaje de coherencia temporal se realiza con vistas fijas y con expresiones desafiadas, permitiendo la creación de modelos de imágenes 4D de manera sencilla. Los experimentos extendidos muestran que Zero-1-to-A mejora la confiabilidad, la calidad de la animación y la velocidad de renderización en comparación con los métodos basados en diferenciales actuales, proporcionando una solución para la generación de modelos de imágenes vivos. El código está disponible para uso público: https://github.com/ZhenglinZhou/Zero-1-to-A.",
      "upvotes": 3,
      "discussionId": "67dcff884aa37abf77ae7415",
      "ai_keywords": [
        "diffusion models",
        "score distillation sampling (SDS)",
        "pseudo ground-truth outputs",
        "video diffusion",
        "spatial consistency",
        "temporal consistency",
        "Zero-1-to-A",
        "front-to-side views",
        "progressive learning",
        "spatial consistency learning",
        "temporal consistency learning",
        "4D avatars",
        "avatar quality",
        "fidelity",
        "animation quality",
        "rendering speed"
      ]
    },
    "publishedAt": "2025-03-20T01:07:46.000Z",
    "title": "Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video\n  Diffusion",
    "summary": "Animatable head avatar generation typically requires extensive data for\ntraining. To reduce the data requirements, a natural solution is to leverage\nexisting data-free static avatar generation methods, such as pre-trained\ndiffusion models with score distillation sampling (SDS), which align avatars\nwith pseudo ground-truth outputs from the diffusion model. However, directly\ndistilling 4D avatars from video diffusion often leads to over-smooth results\ndue to spatial and temporal inconsistencies in the generated video. To address\nthis issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial\nand temporal consistency dataset for 4D avatar reconstruction using the video\ndiffusion model. Specifically, Zero-1-to-A iteratively constructs video\ndatasets and optimizes animatable avatars in a progressive manner, ensuring\nthat avatar quality increases smoothly and consistently throughout the learning\nprocess. This progressive learning involves two stages: (1) Spatial Consistency\nLearning fixes expressions and learns from front-to-side views, and (2)\nTemporal Consistency Learning fixes views and learns from relaxed to\nexaggerated expressions, generating 4D avatars in a simple-to-complex manner.\nExtensive experiments demonstrate that Zero-1-to-A improves fidelity, animation\nquality, and rendering speed compared to existing diffusion-based methods,\nproviding a solution for lifelike avatar creation. Code is publicly available\nat: https://github.com/ZhenglinZhou/Zero-1-to-A.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15851.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6425318d175bd2952281065e",
      "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
      "fullname": "ZhenglinZhou",
      "name": "zhenglin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16375",
      "authors": [
        {
          "_id": "67dd269625f9991caf94c667",
          "name": "Han-Hung Lee",
          "hidden": false
        },
        {
          "_id": "67dd269625f9991caf94c668",
          "name": "Qinghong Han",
          "hidden": false
        },
        {
          "_id": "67dd269625f9991caf94c669",
          "name": "Angel X. Chang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6313c7a754e6e5d9f0fa4d81/U4GisQW1kt0WXR-0HiBLv.mp4"
      ],
      "publishedAt": "2025-03-20T17:37:43.000Z",
      "submittedOnDailyAt": "2025-03-21T07:15:44.580Z",
      "title": "NuiScene: Investigación sobre la generación eficiente de paisajes exteriores infinitos",
      "submittedOnDailyBy": {
        "_id": "6313c7a754e6e5d9f0fa4d81",
        "avatarUrl": "/avatars/0b0e5f09171b87d61260225b2021aa98.svg",
        "isPro": true,
        "fullname": "HAN-HUNG LEE",
        "user": "rexleeppp",
        "type": "user"
      },
      "summary": "En este artículo, se revisa la tarea de generar amplias ubicaciones exteriores en el ámbito de las ciudades y edificios de alto rendimiento. La principal enfocación de los estudios previos se centra en la generación de lugares interiores, mientras que la generación de lugares exteriores presenta problemas propios, como la amplia variación de altura y la rápida generación de grandes paisajes. En respuesta a esto, proponemos un enfoque eficiente que incluye efectivamente los calks de los lugares en un conjunto de vectores coherentes, comparandolo con la estructuración espacial latina utilizada en los métodos existentes, para mejorar la compresión y el rendimiento. Además, para lograr la generación ilimitada, se entrena un modelo de abierto 펫 explícito, mejorando la complejidad frente a los métodos de resampling, y eliminando pasos adicionales de difusión para acelerar la velocidad de generación. Para apoyar esta tarea, se seleccionan conjuntos de lugares de alta calidad y se realiza preprocesamiento adecuado para la entrenamiento conjunto. En particular, entrenando lugares con estilos diferentes, nuestro modelo puede mezclar casas rurales y escritorios urbanos en el mismo lugar, demostrando que nuestro proceso de selección puede utilizar lugares diferentes en el entrenamiento conjunto.",
      "upvotes": 2,
      "discussionId": "67dd269c25f9991caf94c87b",
      "projectPage": "https://3dlg-hcvc.github.io/NuiScene/",
      "githubRepo": "https://github.com/3dlg-hcvc/NuiScene",
      "ai_keywords": [
        "outpainting model",
        "explicit outpainting",
        "diffusion steps"
      ]
    },
    "publishedAt": "2025-03-20T13:37:43.000Z",
    "title": "NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes",
    "summary": "In this paper, we explore the task of generating expansive outdoor scenes,\nranging from castles to high-rises. Unlike indoor scene generation, which has\nbeen a primary focus of prior work, outdoor scene generation presents unique\nchallenges, including wide variations in scene heights and the need for a\nmethod capable of rapidly producing large landscapes. To address this, we\npropose an efficient approach that encodes scene chunks as uniform vector sets,\noffering better compression and performance than the spatially structured\nlatents used in prior methods. Furthermore, we train an explicit outpainting\nmodel for unbounded generation, which improves coherence compared to prior\nresampling-based inpainting schemes while also speeding up generation by\neliminating extra diffusion steps. To facilitate this task, we curate\nNuiScene43, a small but high-quality set of scenes, preprocessed for joint\ntraining. Notably, when trained on scenes of varying styles, our model can\nblend different environments, such as rural houses and city skyscrapers, within\nthe same scene, highlighting the potential of our curation process to leverage\nheterogeneous scenes for joint training.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313c7a754e6e5d9f0fa4d81/U4GisQW1kt0WXR-0HiBLv.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16375.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313c7a754e6e5d9f0fa4d81",
      "avatarUrl": "/avatars/0b0e5f09171b87d61260225b2021aa98.svg",
      "fullname": "HAN-HUNG LEE",
      "name": "rexleeppp",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16194",
      "authors": [
        {
          "_id": "67dcf6375fd14aedd3005237",
          "name": "Ziyao Guo",
          "hidden": false
        },
        {
          "_id": "67dcf6375fd14aedd3005238",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67dcf6375fd14aedd3005239",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T14:41:29.000Z",
      "submittedOnDailyAt": "2025-03-21T03:46:49.486Z",
      "title": "En CoreAST, el manejo de manejo para mejorar la capacidad de restauración automática de imágenes mediante manejo.",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "Los modelos automáticos de regresión han demostrado un éxito notable en la generación de imágenes al aplicar métodos de predicción de secuencias en el modelado de lenguajes. Sin embargo, para aplicar estos métodos a las imágenes, es necesario convertir los datos de píxeles absolutos en números utilizando técnicas de codificación vectorial como VQ-VAE. Las investigaciones recientes han dirigido su esfuerzo hacia la creación de códigobooks más grandes para reducir los errores de conversión numérica. Sin embargo, este enfoque aumenta la complejidad del modelado automático de regresión al aumentar el tamaño de las palabras. El objetivo de este artículo es encontrar una solución que permita disfrutar de los beneficios de un códigobook grande sin complicar el modelado automático de regresión. Mediante una investigación experimental, se ha demostrado que tokens con la misma representación de códigobook tienden a generar imágenes con efectos similares y que el códigobook grande muestra una excesiva repetitividad. Desde esta perspectiva, se propone asignar etiquetas breves similares para predecir el avance de los tokens a través del curso hacia el fondo. Nuestro marco de trabajo está constituido por dos etapas: (1) un modelo automático de regresión que predice etiquetas breves para cada token en la secuencia, y (2) un modelo auxiliar que predice todas las etiquetas detalladas simultáneamente basadas en las etiquetas breves. Los resultados de las experimentaciones en ImageNet muestran la excelencia de nuestro método, con un aumento promedio del 59 puntos en la tasa de cambio frente a los referenciales. Además, añadiendo un paso de inferencia, nuestro enfoque logra una velocidad de muestreo más rápida.",
      "upvotes": 2,
      "discussionId": "67dcf6375fd14aedd300527b",
      "ai_keywords": [
        "autoregressive models",
        "image generation",
        "sequential prediction",
        "language modeling",
        "VQ-VAE",
        "vector quantization",
        "codebooks",
        "token",
        "codeword representations",
        "coarse to fine (CTF)",
        "inference step",
        "Inception Score",
        "sampling speeds"
      ]
    },
    "publishedAt": "2025-03-20T10:41:29.000Z",
    "title": "Improving Autoregressive Image Generation through Coarse-to-Fine Token\n  Prediction",
    "summary": "Autoregressive models have shown remarkable success in image generation by\nadapting sequential prediction techniques from language modeling. However,\napplying these approaches to images requires discretizing continuous pixel data\nthrough vector quantization methods like VQ-VAE. To alleviate the quantization\nerrors that existed in VQ-VAE, recent works tend to use larger codebooks.\nHowever, this will accordingly expand vocabulary size, complicating the\nautoregressive modeling task. This paper aims to find a way to enjoy the\nbenefits of large codebooks without making autoregressive modeling more\ndifficult. Through empirical investigation, we discover that tokens with\nsimilar codeword representations produce similar effects on the final generated\nimage, revealing significant redundancy in large codebooks. Based on this\ninsight, we propose to predict tokens from coarse to fine (CTF), realized by\nassigning the same coarse label for similar tokens. Our framework consists of\ntwo stages: (1) an autoregressive model that sequentially predicts coarse\nlabels for each token in the sequence, and (2) an auxiliary model that\nsimultaneously predicts fine-grained labels for all tokens conditioned on their\ncoarse labels. Experiments on ImageNet demonstrate our method's superior\nperformance, achieving an average improvement of 59 points in Inception Score\ncompared to baselines. Notably, despite adding an inference step, our approach\nachieves faster sampling speeds.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16194.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16055",
      "authors": [
        {
          "_id": "67dd0d7c68dc6463747e6cac",
          "name": "Abdelrahman Elsayed",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6cad",
          "name": "Sarim Hashmi",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6cae",
          "name": "Mohammed Elseiagy",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6caf",
          "name": "Hu Wang",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6cb0",
          "name": "Mohammad Yaqub",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6cb1",
          "name": "Ibrahim Almakky",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T11:42:41.000Z",
      "submittedOnDailyAt": "2025-03-21T05:26:30.501Z",
      "title": "SALT: Adaptación de los valores de características mediante transformación de bajo grado",
      "submittedOnDailyBy": {
        "_id": "62676a94dacab364889bb36c",
        "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
        "isPro": false,
        "fullname": "SARIM HASHMI",
        "user": "Sarim-Hash",
        "type": "user"
      },
      "summary": "Como alternativa a las complejas características de la segmentación de imágenes médicas, especialmente para comprender las características detalladas y propias del dominio, es necesario un modelo que pueda identificar estos aspectos. Los grandes modelos base pueden ser cambiados con libertad, pero el costo asociado al ajuste de estos modelos puede ser un obstáculo significativo. Métodos como la Low-Rank Adaptation (LoRA) en los métodos de Fine-Tuning de Parámetros Eficientes (PEFT) pueden actualizar eficientemente los pesos del modelo, pero pueden fallar en comprender las partes más delicadas y propias del dominio. Por otro lado, los métodos basados en la Decomposición de Valores Singulares (SVD) que incluyen todos los ejes pueden realizar actualizaciones detalladas modificando todos los valores singulares, pero pueden ser inflexibles y su rendimiento puede variar entre conjuntos de datos. Proponemos la Singular Value Adaptation with Low-Rank Transformation (SALT). Este método utiliza parámetros escalables y escalas para seleccionar y cambiar de manera selectiva los valores singulares más influyentes, mientras que los demás ejes se actualizan con una transformación de bajo rango. Esta aproximación híbrida combina los beneficios de LoRA y SVD, permitiendo efectivos ajustes sin necesidad de aumentar la tamaño o la profundidad del modelo. En evaluaciones en cinco difíciles conjuntos de datos médicos, SALT supera a PEFT (LoRA y SVD) en términos de Dice entre 2% y 5%, y puede realizar fuertes ajustes en entornos de recursos bajos con solo 3.9% de parámetros aprendibles. El código de SALT está disponible en la siguiente URL: https://github.com/BioMedIA-MBZUAI/SALT",
      "upvotes": 2,
      "discussionId": "67dd0d7e68dc6463747e6d03",
      "ai_keywords": [
        "SALT",
        "Singular Value Adaptation with Low-Rank Transformation",
        "Low-Rank Adaptation",
        "LoRA",
        "Singular Value Decomposition",
        "SVD",
        "Dice",
        "Parameter-Efficient Fine-Tuning",
        "trainable parameters"
      ]
    },
    "publishedAt": "2025-03-20T07:42:41.000Z",
    "title": "SALT: Singular Value Adaptation with Low-Rank Transformation",
    "summary": "The complex nature of medical image segmentation calls for models that are\nspecifically designed to capture detailed, domain-specific features. Large\nfoundation models offer considerable flexibility, yet the cost of fine-tuning\nthese models remains a significant barrier. Parameter-Efficient Fine-Tuning\n(PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model\nweights with low-rank matrices but may suffer from underfitting when the chosen\nrank is insufficient to capture domain-specific nuances. Conversely, full-rank\nSingular Value Decomposition (SVD) based methods provide comprehensive updates\nby modifying all singular values, yet they often lack flexibility and exhibit\nvariable performance across datasets. We propose SALT (Singular Value\nAdaptation with Low-Rank Transformation), a method that selectively adapts the\nmost influential singular values using trainable scale and shift parameters\nwhile complementing this with a low-rank update for the remaining subspace.\nThis hybrid approach harnesses the advantages of both LoRA and SVD, enabling\neffective adaptation without relying on increasing model size or depth.\nEvaluated on 5 challenging medical datasets, ranging from as few as 20 samples\nto 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in\nDice with only 3.9% trainable parameters, demonstrating robust adaptation even\nin low-resource settings. The code for SALT is available at:\nhttps://github.com/BioMedIA-MBZUAI/SALT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62676a94dacab364889bb36c",
      "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
      "fullname": "SARIM HASHMI",
      "name": "Sarim-Hash",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16031",
      "authors": [
        {
          "_id": "67dcc5e41f94b594ef4c0312",
          "user": {
            "_id": "651692d718f3a57f869a5a0a",
            "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
            "isPro": false,
            "fullname": "Sai Kartheek Reddy",
            "user": "UVSKKR",
            "type": "user"
          },
          "name": "Sai Kartheek Reddy Kasu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-21T01:51:18.039Z",
          "hidden": false
        },
        {
          "_id": "67dcc5e41f94b594ef4c0313",
          "name": "Shankar Biradar",
          "hidden": false
        },
        {
          "_id": "67dcc5e41f94b594ef4c0314",
          "name": "Sunil Saumya",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T10:58:02.000Z",
      "submittedOnDailyAt": "2025-03-21T00:28:13.365Z",
      "title": "Humor Deceptivo: Sobre la Excellencia, Sobre las Afirmaciones y la Excellencia: Una Colección de Datos Benchmark Multilingüe Sintético para Conectar los Conceptos",
      "submittedOnDailyBy": {
        "_id": "651692d718f3a57f869a5a0a",
        "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
        "isPro": false,
        "fullname": "Sai Kartheek Reddy",
        "user": "UVSKKR",
        "type": "user"
      },
      "summary": "En este artículo, se presenta el conjunto de datos de ruido exagerado (DHD). Este conjunto de datos es una nueva fuente de recursos para investigar el ruido en proposiciones exageradas e información negativa. En la era en la que la información negativa se extiende, es crucial comprender por qué el ruido actúa de manera falsa. DHD consiste en un conjunto de comentarios con ruido generados a partir de notas exageradas producidas por el modelo ChatGPT-4o. Cada instancia se asigna un nivel de satire desde un nivel leve (1) hasta un nivel alto de satire (3), y se clasifica en cinco categorías diferentes de ruido: ruido negro, irónico, comentarios sociales, juego de palabras y ruido sin sentido. Este conjunto de datos se ha expandido en varios idiomas, incluyendo inglés, turco, hindi, canadiense, tamil y versiones mixtas de estos idiomas con inglés (Te-En, Hi-En, Ka-En, Ta-En). Esto hace de DHD una herramienta valiosa para benchmarking multilingüe. La introducción de DHD establece una estructura básica para el análisis del ruido en contextos falsos, y abre nuevas direcciones de investigación sobre cómo el ruido interactúa con la información negativa y cómo esta interacción y la propagación afectan a la comprensión. Esta propuesta de conjunto de datos establece una base línea fuerte y proporciona una base para futuras investigaciones que pueden utilizarse como un benchmark.",
      "upvotes": 2,
      "discussionId": "67dcc5e41f94b594ef4c0353"
    },
    "publishedAt": "2025-03-20T06:58:02.000Z",
    "title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging\n  Fabricated Claims with Humorous Content",
    "summary": "This paper presents the Deceptive Humor Dataset (DHD), a novel resource for\nstudying humor derived from fabricated claims and misinformation. In an era of\nrampant misinformation, understanding how humor intertwines with deception is\nessential. DHD consists of humor-infused comments generated from false\nnarratives, incorporating fabricated claims and manipulated information using\nthe ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging\nfrom 1 for subtle satire to 3 for high-level satire and classified into five\ndistinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and\nAbsurdity. The dataset spans multiple languages including English, Telugu,\nHindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En,\nTa-En), making it a valuable multilingual benchmark. By introducing DHD, we\nestablish a structured foundation for analyzing humor in deceptive contexts,\npaving the way for a new research direction that explores how humor not only\ninteracts with misinformation but also influences its perception and spread. We\nestablish strong baselines for the proposed dataset, providing a foundation for\nfuture research to benchmark and advance deceptive humor detection models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16031.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "651692d718f3a57f869a5a0a",
      "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
      "fullname": "Sai Kartheek Reddy",
      "name": "UVSKKR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.15855",
      "authors": [
        {
          "_id": "67dd072f1c182b6168eaa004",
          "name": "Hyojun Go",
          "hidden": false
        },
        {
          "_id": "67dd072f1c182b6168eaa005",
          "name": "Byeongjun Park",
          "hidden": false
        },
        {
          "_id": "67dd072f1c182b6168eaa006",
          "name": "Hyelin Nam",
          "hidden": false
        },
        {
          "_id": "67dd072f1c182b6168eaa007",
          "name": "Byung-Hoon Kim",
          "hidden": false
        },
        {
          "_id": "67dd072f1c182b6168eaa008",
          "name": "Hyungjin Chung",
          "hidden": false
        },
        {
          "_id": "67dd072f1c182b6168eaa009",
          "name": "Changick Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T05:26:09.000Z",
      "submittedOnDailyAt": "2025-03-21T07:39:17.497Z",
      "title": "VideoRFSplat: Creación de 3D gaussian splats a partir de texto en nivel de espacio\nDescripción de frecuencias de pose y varianza angular\n\nVideoRFSplat: Creación de 3D gaussian splats a partir de texto directo en nivel de kernel\nDescripción de parejas de frecuencias y varianza angular asociadas",
      "submittedOnDailyBy": {
        "_id": "649f65a4ca03a1a35e3dac14",
        "avatarUrl": "/avatars/b0dcd8ad795b1e666ee247b2ac024d53.svg",
        "isPro": false,
        "fullname": "Hyojun GO",
        "user": "HJGO",
        "type": "user"
      },
      "summary": "Se propone el modelo de video RFSplat. Este método utiliza modelos de generación de video para crear directamente modelos 3D gaussianos realistas a partir de texto en un escenario real-time sin limitaciones. Se generan diferentes posiciones de cámara y una expansión espacial ilimitada, y para garantizar la generalización para cualquier texto de proyecto, los métodos anteriores se ajustaban a modelos de generación 2D. Sin embargo, estos métodos sufren de instabilidades entre modelos al modelar juntos. Por lo tanto, se necesita un modelo adicional para estabilizar el entrenamiento y la inferencia. En este artículo, se propone una arquitectura y un estado de la técnica de muestreo que modelan juntos diferentes posiciones de cámara y expansión espacial al ajustar un modelo de generación de video. La idea clave es la conexión de un bloque de comunicación entre un modelo de generación de video predeterminado y un modelo de generación de posiciones específicas, lo que forma una arquitectura de doble flujo. En esta disección, se reduce la interferencia entre modelos de posición e imagen. Además, se propone un estado de la técnica de muestreo de análisis rápido y se elimina rápidamente el ruido de las posiciones de cámara comparativamente a las imágenes, lo que permite que posiciones de cámara sin ruido condicionen rápidamente la generación de diferentes imágenes, reduciendo la incertidumbre mutua y mejorando la consistencia entre modalidades. El modelo de video RFSplat, entrenado en grandes conjuntos de datos de real-time como RealEstate10K, MVImgNet, DL3DV-10K y ACID, genera resultados excelentes sin necesidad de ajustes, en comparación con métodos que dependen de la muestreo de score distil.",
      "upvotes": 2,
      "discussionId": "67dd07351c182b6168eaa1fc",
      "ai_keywords": [
        "VideoRFSplat",
        "text-to-3D model",
        "video generation model",
        "realistic 3D Gaussian Splatting (3DGS)",
        "camera poses",
        "unbounded real-world scenes",
        "multi-view images",
        "dual-stream architecture",
        "pose generation model",
        "communication blocks",
        "asynchronous sampling strategy",
        "denoising",
        "mutual ambiguity",
        "cross-modal consistency",
        "RealEstate10K",
        "MVImgNet",
        "DL3DV-10K",
        "ACID",
        "score distillation sampling"
      ]
    },
    "publishedAt": "2025-03-20T01:26:09.000Z",
    "title": "VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting\n  Generation with Flexible Pose and Multi-View Joint Modeling",
    "summary": "We propose VideoRFSplat, a direct text-to-3D model leveraging a video\ngeneration model to generate realistic 3D Gaussian Splatting (3DGS) for\nunbounded real-world scenes. To generate diverse camera poses and unbounded\nspatial extent of real-world scenes, while ensuring generalization to arbitrary\ntext prompts, previous methods fine-tune 2D generative models to jointly model\ncamera poses and multi-view images. However, these methods suffer from\ninstability when extending 2D generative models to joint modeling due to the\nmodality gap, which necessitates additional models to stabilize training and\ninference. In this work, we propose an architecture and a sampling strategy to\njointly model multi-view images and camera poses when fine-tuning a video\ngeneration model. Our core idea is a dual-stream architecture that attaches a\ndedicated pose generation model alongside a pre-trained video generation model\nvia communication blocks, generating multi-view images and camera poses through\nseparate streams. This design reduces interference between the pose and image\nmodalities. Additionally, we propose an asynchronous sampling strategy that\ndenoises camera poses faster than multi-view images, allowing rapidly denoised\nposes to condition multi-view generation, reducing mutual ambiguity and\nenhancing cross-modal consistency. Trained on multiple large-scale real-world\ndatasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms\nexisting text-to-3D direct generation methods that heavily depend on post-hoc\nrefinement via score distillation sampling, achieving superior results without\nsuch refinement.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15855.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649f65a4ca03a1a35e3dac14",
      "avatarUrl": "/avatars/b0dcd8ad795b1e666ee247b2ac024d53.svg",
      "fullname": "Hyojun GO",
      "name": "HJGO",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15451",
      "authors": [
        {
          "_id": "67dd03fb2672aa3643252e8c",
          "user": {
            "_id": "65220fedc709aaca9aa63061",
            "avatarUrl": "/avatars/36e56199fa8c98ff3430636567a0ed62.svg",
            "isPro": false,
            "fullname": "Lixing Xiao",
            "user": "lxxiao",
            "type": "user"
          },
          "name": "Lixing Xiao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-21T06:19:01.806Z",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e8d",
          "name": "Shunlin Lu",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e8e",
          "name": "Huaijin Pi",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e8f",
          "name": "Ke Fan",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e90",
          "name": "Liang Pan",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e91",
          "name": "Yueer Zhou",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e92",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e93",
          "name": "Xiaowei Zhou",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e94",
          "name": "Sida Peng",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e95",
          "name": "Jingbo Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T17:32:24.000Z",
      "submittedOnDailyAt": "2025-03-21T05:04:41.437Z",
      "title": "MovementStreamer: Streaming de Movimientos utilizando un Modelo de Regresión Automática en Espacios Potenciales Causales basados en Difusión",
      "submittedOnDailyBy": {
        "_id": "65220fedc709aaca9aa63061",
        "avatarUrl": "/avatars/36e56199fa8c98ff3430636567a0ed62.svg",
        "isPro": false,
        "fullname": "Lixing Xiao",
        "user": "lxxiao",
        "type": "user"
      },
      "summary": "Este artículo aborda el desafío de generar acciones humanas en tiempo real basadas en texto, pidiéndoles a los modelos la predicción de la siguiente posición humana a partir de movimientos históricos y textos de entrada. Los métodos actuales enfrentan dificultades para lograr la generación de acciones en tiempo real. Por ejemplo, modelos difusivos están limitados por una longitud de acción predefinida, mientras que los métodos basados en GPT sufren de pérdida de información causada por tokens no causales y la acumulación de errores en la generación automática de acciones a largo plazo. Para resolver estos problemas, proponemos un nuevo marco de trabajo que integra un modelo de auto-regresión probabilística y espacios de potenciales causales continuos. Los potenciales continuos reducen la pérdida de información causada por tokens y efectivamente mitigan la acumulación de errores en la generación automática a largo plazo. Además, establecemos una relación causal temporal entre los potenciales actuales y los movimientos históricos, lo que permite que nuestro modelo maximice la información disponible para realizar una interpretación precisa de las acciones en tiempo real. Los experimentos demuestran que nuestro método supera los enfoques actuales y proporciona una mayor variedad de aplicaciones, incluyendo múltiples generaciones, creaciones a largo plazo y combinaciones dinámicas de acciones. Página del proyecto: https://zju3dv.github.io/MotionStreamer/",
      "upvotes": 2,
      "discussionId": "67dd03fd2672aa3643252f2a",
      "projectPage": "https://zju3dv.github.io/MotionStreamer/",
      "ai_keywords": [
        "diffusion models",
        "streaming motion generation",
        "human pose prediction",
        "historical motions",
        "incoming texts",
        "GPT-based methods",
        "continuous causal latent space",
        "probabilistic autoregressive model",
        "information loss",
        "error accumulation",
        "temporal causal dependencies",
        "online motion decoding",
        "multi-round generation",
        "long-term generation",
        "dynamic motion composition"
      ]
    },
    "publishedAt": "2025-03-19T13:32:24.000Z",
    "title": "MotionStreamer: Streaming Motion Generation via Diffusion-based\n  Autoregressive Model in Causal Latent Space",
    "summary": "This paper addresses the challenge of text-conditioned streaming motion\ngeneration, which requires us to predict the next-step human pose based on\nvariable-length historical motions and incoming texts. Existing methods\nstruggle to achieve streaming motion generation, e.g., diffusion models are\nconstrained by pre-defined motion lengths, while GPT-based methods suffer from\ndelayed response and error accumulation problem due to discretized non-causal\ntokenization. To solve these problems, we propose MotionStreamer, a novel\nframework that incorporates a continuous causal latent space into a\nprobabilistic autoregressive model. The continuous latents mitigate information\nloss caused by discretization and effectively reduce error accumulation during\nlong-term autoregressive generation. In addition, by establishing temporal\ncausal dependencies between current and historical motion latents, our model\nfully utilizes the available information to achieve accurate online motion\ndecoding. Experiments show that our method outperforms existing approaches\nwhile offering more applications, including multi-round generation, long-term\ngeneration, and dynamic motion composition. Project Page:\nhttps://zju3dv.github.io/MotionStreamer/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15451.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65220fedc709aaca9aa63061",
      "avatarUrl": "/avatars/36e56199fa8c98ff3430636567a0ed62.svg",
      "fullname": "Lixing Xiao",
      "name": "lxxiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14237",
      "authors": [
        {
          "_id": "67db7a33b1d42828a18fd0c8",
          "name": "Chenting Wang",
          "hidden": false
        },
        {
          "_id": "67db7a33b1d42828a18fd0c9",
          "name": "Kunchang Li",
          "hidden": false
        },
        {
          "_id": "67db7a33b1d42828a18fd0ca",
          "name": "Tianxiang Jiang",
          "hidden": false
        },
        {
          "_id": "67db7a33b1d42828a18fd0cb",
          "name": "Xiangyu Zeng",
          "hidden": false
        },
        {
          "_id": "67db7a33b1d42828a18fd0cc",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67db7a33b1d42828a18fd0cd",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T13:15:58.000Z",
      "submittedOnDailyAt": "2025-03-21T07:23:35.664Z",
      "title": "\"Métodos para hacer flexible la entrenamiento: formas de hacer una distribución eficiente de videos con modelos de vídeo\"",
      "submittedOnDailyBy": {
        "_id": "62aafa49f29ff279b51f0182",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62aafa49f29ff279b51f0182/rQx8QFQGOY2qIhqJ8zSRj.jpeg",
        "isPro": false,
        "fullname": "yinanhe",
        "user": "ynhe",
        "type": "user"
      },
      "summary": "Una de las más populares formas de entrenamiento de tokens de video es la muestreo de un número fijo de tokens en espacios de tiempo predefinidos, lo que dificulta la optimización de precisión y cantidad de cálculos debido a la característica de redundancia propia de los videos. Además, no puede adaptarse a los cambios en el algoritmo de cálculo de tareas posteriores, lo que impide la aplicación en el mundo real de los modelos más potentes. Por ello, proponemos un nuevo escenario de prueba \"optimización de tokens\" para optimizar la información de entrada de los videos. Este método consiste en seleccionar tokens adecuados en videos muestreados, optimizando un conjunto limitado de tokens de entrada para obtener la información de entrada óptima. En este contexto, proponemos una nueva herramienta de aplicación \"Flux\". Esta herramienta permite crear un gráfico de muestreo flexible y diseñada para aplicarse fácilmente en muchos marcos de entrenamiento de videos. De esta manera, se puede significativamente mejorar la robustez del modelo, y además, el costo adicional es casi nulo. Integrando Flux en entrenamientos de videos a gran escala, FluxViT establece un rendimiento innovador en diferentes tareas con un costo estándar. En particular, al usar solo un cuarto de los tokens, mediante la optimización de tokens, se logra una reducción del costo aproximadamente del 90% en comparación con los modelos más recientes. Todos los modelos y datos están disponibles en https://github.com/OpenGVLab/FluxViT.",
      "upvotes": 2,
      "discussionId": "67db7a35b1d42828a18fd11f",
      "ai_keywords": [
        "token optimization",
        "spatiotemporal grid",
        "token selection",
        "Flux augmentation tool",
        "FluxViT",
        "video pre-training"
      ]
    },
    "publishedAt": "2025-03-18T09:15:58.000Z",
    "title": "Make Your Training Flexible: Towards Deployment-Efficient Video Models",
    "summary": "Popular video training methods mainly operate on a fixed number of tokens\nsampled from a predetermined spatiotemporal grid, resulting in sub-optimal\naccuracy-computation trade-offs due to inherent video redundancy. They also\nlack adaptability to varying computational budgets for downstream tasks,\nhindering applications of the most competitive model in real-world scenes. We\nthus propose a new test setting, Token Optimization, for maximized input\ninformation across budgets, which optimizes the size-limited set of input\ntokens through token selection from more suitably sampled videos. To this end,\nwe propose a novel augmentation tool termed Flux. By making the sampling grid\nflexible and leveraging token selection, it is easily adopted in most popular\nvideo training frameworks, boosting model robustness with nearly no additional\ncost. We integrate Flux in large-scale video pre-training, and the resulting\nFluxViT establishes new state-of-the-art results across extensive tasks at\nstandard costs. Notably, with 1/4 tokens only, it can still match the\nperformance of previous state-of-the-art models with Token Optimization,\nyielding nearly 90\\% savings. All models and data are available at\nhttps://github.com/OpenGVLab/FluxViT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14237.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62aafa49f29ff279b51f0182",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62aafa49f29ff279b51f0182/rQx8QFQGOY2qIhqJ8zSRj.jpeg",
      "fullname": "yinanhe",
      "name": "ynhe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12689",
      "authors": [
        {
          "_id": "67dcc2e10df3501c657ef478",
          "name": "Hengjia Li",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef479",
          "name": "Lifan Jiang",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47a",
          "name": "Xi Xiao",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47b",
          "name": "Tianyang Wang",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47c",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47d",
          "name": "Boxi Wu",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47e",
          "name": "Deng Cai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T23:15:09.000Z",
      "submittedOnDailyAt": "2025-03-21T02:25:48.893Z",
      "title": "MagicID: Optimización de la preferencia híbrida - AID coincidencia y almacenamiento dinámico para la personalización de vídeos",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Video Identity Customization genera el video de alta precisión basándose en la imagen de referencia del usuario, con el objetivo de mantener la identidad y la movilidad coincidentes. Sin embargo, existen dos principales problemas en el enfoque actual: la disminución de la identidad y la pérdida de movilidad durante el entrenamiento, lo cual se atribuye principalmente a la entrenamiento tradicional de auto-reconstrucción basado en imágenes estáticas. Para resolver estos problemas, presentamos un nuevo marco de trabajo llamado \"MagicID\", que mantiene la identidad y la movilidad ricas en acuerdo con las preferencias del usuario. Específicamente, proponemos la construcción de datos de video pareadas que incluyen identidad explícita y compensación dinámica, y evitamos el enfoque tradicional de auto-reconstrucción. Además, para resolver las restricciones de datos de preferencias del usuario, introducimos un estádio de muestreo híbrido que prioriza la preservación de la identidad a partir de videos generados a partir de imágenes estáticas, y mejora la calidad de las acciones dinámicas de los videos generados utilizando un método de muestreo basado en límites superiores. Utilizando estas parejas de orientación híbrida, se optimiza el modelo según las diferencias de compensación. Los experimentos extendidos muestran que MagicID logra con éxito la identidad coincidente y la movilidad natural, presentando resultados notablemente mejores que los actuales.",
      "upvotes": 2,
      "discussionId": "67dcc2e50df3501c657ef56b",
      "projectPage": "https://echopluto.github.io/MagicID-project/",
      "githubRepo": "https://github.com/EchoPluto/MagicID",
      "ai_keywords": [
        "pairwise preference video data",
        "identity and dynamic rewards",
        "preference learning",
        "hybrid sampling strategy",
        "static videos",
        "Frontier-based sampling method",
        "reward differences",
        "customized preferences",
        "identity-preserving",
        "dynamic motion quality",
        "high-fidelity videos",
        "consistent identity",
        "natural dynamics"
      ]
    },
    "publishedAt": "2025-03-16T19:15:09.000Z",
    "title": "MagicID: Hybrid Preference Optimization for ID-Consistent and\n  Dynamic-Preserved Video Customization",
    "summary": "Video identity customization seeks to produce high-fidelity videos that\nmaintain consistent identity and exhibit significant dynamics based on users'\nreference images. However, existing approaches face two key challenges:\nidentity degradation over extended video length and reduced dynamics during\ntraining, primarily due to their reliance on traditional self-reconstruction\ntraining with static images. To address these issues, we introduce\nMagicID, a novel framework designed to directly promote the\ngeneration of identity-consistent and dynamically rich videos tailored to user\npreferences. Specifically, we propose constructing pairwise preference video\ndata with explicit identity and dynamic rewards for preference learning,\ninstead of sticking to the traditional self-reconstruction. To address the\nconstraints of customized preference data, we introduce a hybrid sampling\nstrategy. This approach first prioritizes identity preservation by leveraging\nstatic videos derived from reference images, then enhances dynamic motion\nquality in the generated videos using a Frontier-based sampling method. By\nutilizing these hybrid preference pairs, we optimize the model to align with\nthe reward differences between pairs of customized preferences. Extensive\nexperiments show that MagicID successfully achieves consistent identity and\nnatural dynamics, surpassing existing methods across various metrics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12689.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 35
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16219",
      "authors": [
        {
          "_id": "67dd1a9cfa598c90d14e9b47",
          "user": {
            "_id": "645b663eca5d8a297712f2e1",
            "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
            "isPro": false,
            "fullname": "Quy-Anh Dang",
            "user": "quyanh",
            "type": "user"
          },
          "name": "Quy-Anh Dang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-21T07:52:47.629Z",
          "hidden": false
        },
        {
          "_id": "67dd1a9cfa598c90d14e9b48",
          "name": "Chris Ngo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/EIt1JuoKkqWkGY1Pied64.png"
      ],
      "publishedAt": "2025-03-20T15:13:23.000Z",
      "submittedOnDailyAt": "2025-03-21T07:03:47.115Z",
      "title": "La eficacia y ineficacia de la aprendizaje por refuerzo en la inferencia lógica de pequeños modelos de lenguaje: ¿Qué es efectivo y qué no es efectivo?",
      "submittedOnDailyBy": {
        "_id": "645b663eca5d8a297712f2e1",
        "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
        "isPro": false,
        "fullname": "Quy-Anh Dang",
        "user": "quyanh",
        "type": "user"
      },
      "summary": "Para mejorar la capacidad de inferencia de los grandes modelos de lenguaje (LLMs), se requieren grandes recursos computacionales y amplios conjuntos de datos, lo que limita su acceso en entornos con recursos restringidos. En este estudio, se revisa la posibilidad de mejorar la inferencia de pequeños LLMs mediante el aprendizaje por refuerzo (RL), con un enfoque específico en el modelo de 150 billones de parámetros, DeepSeek-R1-Distill-Qwen-1.5B. Se utilizaron 4 nodos de NVIDIA A40 GPU (48GB VRAM por nodo) para entrenar el modelo en 24 horas bajo condiciones estrictas. Se aplicó el algoritmo de Policy Optimization Group Relative (GRPO) y se generaron conjuntos de datos matemáticos organizados para realizar tres experimentos. Estos resultados demostraron que la tasa de respuestas correctas en AMC23 aumentó desde 63% a 80%, y en AIME24 alcanzó un 46.7%, mejorando significativamente la eficiencia de la inferencia con un costo superior a miles de dólares para un conjunto de 7,000 muestras. Sin embargo, se presentaron problemas de estabilidad de optimización a largo plazo y limitaciones de longitud. Estos hallazgos demuestran la efectividad de la micro-ajuste basado en RL para pequeños LLMs y la posibilidad de una alternativa costo-eficiente para un acceso a escala a grandes modelos. El código y conjuntos de datos del estudio se han publicado como recursos abiertos en GitHub, proporcionando vistas para la búsqueda de soluciones y estableciendo una base para la capacidad de inferencia escalable en entornos con recursos limitados. Todos pueden acceder a ellos en https://github.com/knoveleng/open-rs.",
      "upvotes": 1,
      "discussionId": "67dd1a9dfa598c90d14e9ba4",
      "githubRepo": "https://github.com/knoveleng/open-rs",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "Group Relative Policy Optimization (GRPO)",
        "mathematical reasoning dataset",
        "AMC23",
        "AIME24",
        "optimization instability",
        "RL-based fine-tuning",
        "scalable",
        "reasoning-capable LLMs"
      ]
    },
    "publishedAt": "2025-03-20T11:13:23.000Z",
    "title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't",
    "summary": "Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/EIt1JuoKkqWkGY1Pied64.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16219.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b663eca5d8a297712f2e1",
      "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
      "fullname": "Quy-Anh Dang",
      "name": "quyanh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13834",
      "authors": [
        {
          "_id": "67dcf6456b575dc3179e05a2",
          "name": "JuneHyoung Kwon",
          "hidden": false
        },
        {
          "_id": "67dcf6456b575dc3179e05a3",
          "name": "MiHyeon Kim",
          "hidden": false
        },
        {
          "_id": "67dcf6456b575dc3179e05a4",
          "name": "Eunju Lee",
          "hidden": false
        },
        {
          "_id": "67dcf6456b575dc3179e05a5",
          "name": "Juhwan Choi",
          "hidden": false
        },
        {
          "_id": "67dcf6456b575dc3179e05a6",
          "name": "YoungBin Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T02:17:41.000Z",
      "submittedOnDailyAt": "2025-03-21T03:48:06.030Z",
      "title": "Seosoabarans: Seosojenerador, y el desbalanceo de la lengua visua en combinación con el desbalanceo de la lengua visua, se utilizan para mitigar el desbalanceo de la lengua visua.",
      "submittedOnDailyBy": {
        "_id": "65646b22ac9d3c2bd7b14788",
        "avatarUrl": "/avatars/0bf19dcfa568a694361fb3a63b999997.svg",
        "isPro": false,
        "fullname": "Juhwan Choi",
        "user": "c-juhwan",
        "type": "user"
      },
      "summary": "La Visión Language (VL) modelo muestra un potente rendimiento en diversas tareas. Sin embargo, estos modelos generalmente utilizan un directorio específico de modelo para realizar predicciones, generando una \"inclinación hacia el mejor modelo\", un efecto de sesgo. Este sesgo afecta particularmente la eficiencia cuando un modelo es dañado. En este estudio, analizamos el impacto de esta inclinación y demostramos teóricamente que la convergencia equilibrada de la pérdida puede ser impidida por gradientes o diferencias de tamaño de gradiente inadecuados. Basándonos en estas observaciones, proponemos un nuevo marco de trabajo llamado BalGrad para mitigar esta inclinación. Nuestro enfoque incluye el re-peso de gradientes entre modelos, la regulación del gradiente de la dispersión de Kullback-Leibler basada en la contribución de cada modelo, y la proyección de gradientes entre modelos. Los experimentos en los conjuntos de datos UPMC Food-101, Hateful Memes y MM-IMDb confirman que BalGrad efectivamente mitiga la dependencia excesiva de predicciones en un directorio específico de modelo.",
      "upvotes": 1,
      "discussionId": "67dcf64a6b575dc3179e0754",
      "ai_keywords": [
        "dominant modality bias",
        "unaligned gradients",
        "gradient magnitudes",
        "inter-modality gradient reweighting",
        "KL divergence",
        "inter-task gradient projection"
      ]
    },
    "publishedAt": "2025-03-17T22:17:41.000Z",
    "title": "See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language\n  Balance to Mitigate Dominant Modality Bias",
    "summary": "Vision-language (VL) models have demonstrated strong performance across\nvarious tasks. However, these models often rely on a specific modality for\npredictions, leading to \"dominant modality bias.'' This bias significantly\nhurts performance, especially when one modality is impaired. In this study, we\nanalyze model behavior under dominant modality bias and theoretically show that\nunaligned gradients or differences in gradient magnitudes prevent balanced\nconvergence of the loss. Based on these findings, we propose a novel framework,\nBalGrad to mitigate dominant modality bias. Our approach includes\ninter-modality gradient reweighting, adjusting the gradient of KL divergence\nbased on each modality's contribution, and inter-task gradient projection to\nalign task directions in a non-conflicting manner. Experiments on UPMC\nFood-101, Hateful Memes, and MM-IMDb datasets confirm that BalGrad effectively\nalleviates over-reliance on specific modalities when making predictions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13834.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65646b22ac9d3c2bd7b14788",
      "avatarUrl": "/avatars/0bf19dcfa568a694361fb3a63b999997.svg",
      "fullname": "Juhwan Choi",
      "name": "c-juhwan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]